{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "TorchNLP",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "kolloldas",
                "owner_type": "User",
                "name": "torchnlp",
                "url": "https://github.com/kolloldas/torchnlp",
                "stars": 238,
                "pushed_at": "2019-12-07 17:29:08+00:00",
                "created_at": "2018-02-18 15:48:28+00:00",
                "language": "Python",
                "description": "Easy to use NLP library built on PyTorch and TorchText",
                "license": "Apache License 2.0",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "9f3d5f43e6f17ee4e13d370f3ab44f470c4f2751",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kolloldas/torchnlp/blob/master/.gitignore"
                    }
                },
                "size": 95
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "261eeb9e9f8b2b4b0d119366dda99c6fd7d35c64",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kolloldas/torchnlp/blob/master/LICENSE"
                    }
                },
                "size": 11357
            },
            {
                "type": "code",
                "name": "data",
                "sha": "ce5cb190b3aebaa174b75a093e11b1c1359f3340",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kolloldas/torchnlp/tree/master/data"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "docker",
                "sha": "739cf809a3d26c74856eb04b89e52128fbbebd89",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kolloldas/torchnlp/tree/master/docker"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "pytest.ini",
                "sha": "de19c9f0b7133138e09e72785b1c56cefae528ee",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kolloldas/torchnlp/blob/master/pytest.ini"
                    }
                },
                "size": 26
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "ba85df78003b1834fc4857fec8bdcd8cce36bb32",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kolloldas/torchnlp/blob/master/requirements.txt"
                    }
                },
                "size": 185
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "46ae66e4309bcd830a514920520fe4420b9e9769",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kolloldas/torchnlp/blob/master/setup.py"
                    }
                },
                "size": 491
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "08175964dbb27e5f8630c16d74b112f422b482e7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kolloldas/torchnlp/tree/master/tests"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "torchnlp",
                "sha": "35634c23f4f22ae881b0d41a9c3b7c2fbf39b634",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kolloldas/torchnlp/tree/master/torchnlp"
                    }
                },
                "num_files": 7
            }
        ]
    },
    "authors": [
        {
            "name": "Kollol Das",
            "github_id": "kolloldas"
        },
        {
            "name": "Aleksas Pielikis",
            "github_id": "aleksas"
        },
        {
            "name": "JudeLee (Dongyub Lee)",
            "email": "dongyub63@gmail.com",
            "github_id": "JudeLee19"
        }
    ],
    "tags": [
        "nlp",
        "machine-learning",
        "crf",
        "pytorch",
        "torchtext",
        "transformer"
    ],
    "description": "Easy to use NLP library built on PyTorch and TorchText",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/kolloldas/torchnlp",
            "stars": 238,
            "issues": true,
            "readme": "# TorchNLP\nTorchNLP is a deep learning library for NLP tasks. Built on PyTorch and TorchText, it is an attempt to provide reusable components that work across tasks. Currently it can be used for Named Entity Recognition (NER) and Chunking tasks with a Bidirectional LSTM CRF model and a Transformer network model. It can support any dataset which uses the [CoNLL 2003 format](https://www.clips.uantwerpen.be/conll2003/ner/). More tasks will be added shortly\n\n## High Level Workflow\n1. Define the NLP task\n2. Extend the `Model` class and implement the `forward()` and `loss()` methods to return predictions and loss respectively\n3. Use the `HParams` class to easily define the hyperparameters for the model\n4. Define a data function to return dataset iterators, vocabularies etc using [TorchText](https://github.com/pytorch/text) API. Check conll.py for an example\n5. Set up the `Evaluator` and `Trainer` classes to use the model, dataset iterators and metrics. Check ner.py for details\n6. Run the trainer for desired number of epochs along with an early stopping criteria\n7. Use the evaluator to evaluate the trained model on a specific dataset split\n8. Run inference on the trained model using available input processors\n\n## Boilerplate Components\n* `Model`: Handles loading and saving of models as well as the associated hyperparameters\n* `HParams`: Generic class to define hyperparameters. Can be persisted\n* `Trainer`: Train a given model on a dataset. Supports features like predefined learning rate decay schedules and early stopping\n* `Evaluator`: Evaluates the model on a dataset and multiple predefined or custom metrics. \n* `get_input_processor_words`: Use during inference to quickly convert input strings into a format that can be processed by a model\n\n## Available Models\n* `transformer.Encoder`, `transformer.Decoder`: Transfomer network implementation from [Attention is all you need](https://arxiv.org/abs/1706.03762)\n* `CRF`: Conditional Random Field layer which can be used as the final output\n* `TransformerTagger`: Sequence tagging model implemented using the Transformer network and CRF\n* `BiLSTMTagger`: Sequence tagging model implemented using bidirectional LSTMs and CRF\n\n## Installation\nTorchNLP requires a minimum of Python 3.5 and PyTorch 0.4.0 to run. Check [Pytorch](http://pytorch.org/) for the installation steps. \nClone this repository and install other dependencies like TorchText:\n```\npip install -r requirements.txt\n```\nGo to the root of the project and check for integrity with PyTest:\n```\npytest\n```\nInstall this project:\n```\npython setup.py\n```\n\n## Usage\nTorchNLP is designed to be used inside the python interpreter to make it easier to experiment without typing cumbersome command line arguments. \n\n**NER Task**\n\nThe NER task can be run on any dataset that confirms to the [CoNLL 2003](https://www.clips.uantwerpen.be/conll2003/ner/) format. To use the CoNLL 2003 NER dataset place the dataset files in the following directory structure within your workspace root:\n```\n.data\n  |\n  |---conll2003\n          |\n          |---eng.train.txt\n          |---eng.testa.txt\n          |---eng.testb.txt\n```\n`eng.testa.txt` is used the validation dataset and `eng.testb.txt` is used as the test dataset.\n\nStart the NER module in the python shell which sets up the imports:\n```\npython -i -m torchnlp.ner\n```\n```\nTask: Named Entity Recognition\n\nAvailable models:\n-------------------\nTransformerTagger\n\n    Sequence tagger using the Transformer network (https://arxiv.org/pdf/1706.03762.pdf)\n    Specifically it uses the Encoder module. For character embeddings (per word) it uses\n    the same Encoder module above which an additive (Bahdanau) self-attention layer is added\n\nBiLSTMTagger\n\n    Sequence tagger using bidirectional LSTM. For character embeddings per word\n    uses (unidirectional) LSTM\n\n\nAvailable datasets:\n-------------------\n    conll2003: Conll 2003 (Parser only. You must place the files)\n\n>>>\n```\n\nTrain the [Transformer](https://arxiv.org/abs/1706.03762) model on the CoNLL 2003 dataset:\n```\n>>> train('ner-conll2003', TransformerTagger, conll2003)\n```\nThe first argument is the task name. You need to use the same task name during evaluation and inference. By default the train function will use the F1 metric with a window of 5 epochs to perform early stopping. To change the early stopping criteria set the `PREFS` global variable as follows:\n```\n>>> PREFS.early_stopping='lowest_3_loss'\n```\nThis will now use validation loss as the stopping criteria with a window of 3 epochs. The model files are saved under *taskname-modelname* directory. In this case it is *ner-conll2003-TransformerTagger*\n\nEvaluate the trained model on the *testb* dataset split:\n```\n>>> evaluate('ner-conll2003', TransformerTagger, conll2003, 'test')\n```\nIt will display metrics like accuracy, sequence accuracy, F1 etc\n\nRun the trained model interactively for the ner task:\n```\n>>> interactive('ner-conll2003', TransformerTagger)\n...\nCtrl+C to quit\n> Tom went to New York\nI-PER O O I-LOC I-LOC\n```\nYou can similarly train the bidirectional LSTM CRF model by using the `BiLSTMTagger` class.\nCustomizing hyperparameters is quite straight forward. Let's look at the hyperparameters for `TransformerTagger`:\n```\n>>> h2 = hparams_transformer_ner()\n>>> h2\n\nHyperparameters:\n filter_size=128\n optimizer_adam_beta2=0.98\n learning_rate=0.2\n learning_rate_warmup_steps=500\n input_dropout=0.2\n embedding_size_char=16\n dropout=0.2\n hidden_size=128\n optimizer_adam_beta1=0.9\n embedding_size_word=300\n max_length=256\n attention_dropout=0.2\n relu_dropout=0.2\n batch_size=100\n num_hidden_layers=1\n attention_value_channels=0\n attention_key_channels=0\n use_crf=True\n embedding_size_tags=100\n learning_rate_decay=noam_step\n embedding_size_char_per_word=100\n num_heads=4\n filter_size_char=64\n ```\n Now let's disable the CRF layer:\n ```\n >>> h2.update(use_crf=False)\n\nHyperparameters:\n filter_size=128\n optimizer_adam_beta2=0.98\n learning_rate=0.2\n learning_rate_warmup_steps=500\n input_dropout=0.2\n embedding_size_char=16\n dropout=0.2\n hidden_size=128\n optimizer_adam_beta1=0.9\n embedding_size_word=300\n max_length=256\n attention_dropout=0.2\n relu_dropout=0.2\n batch_size=100\n num_hidden_layers=1\n attention_value_channels=0\n attention_key_channels=0\n use_crf=False\n embedding_size_tags=100\n learning_rate_decay=noam_step\n embedding_size_char_per_word=100\n num_heads=4\n filter_size_char=64\n ```\n Use it to re-train the model:\n ```\n >>> train('ner-conll2003-nocrf', TransformerTagger, conll2003, hparams=h2)\n ```\n Along with the model the hyperparameters are also saved so there is no need to pass the `HParams` object during evaluation. Also note that by default it will not overwrite any existing model directories (will rename instead). To change that behavior set the PREFS variable:\n ```\n >>> PREFS.overwrite_model_dir = True\n ```\n The `PREFS` variable is automatically persisted in `prefs.json`\n \n **Chunking Task**\n \n The [CoNLL 2000](https://www.clips.uantwerpen.be/conll2000/chunking/) dataset is available for the Chunking task. The dataset is automatically downloaded from the public repository so you don't need to manually download it.\n\nStart the Chunking task:\n```\npython -i -m torchnlp.chunk\n```\nTrain the [Transformer](https://arxiv.org/abs/1706.03762) model:\n```\n>>> train('chunk-conll2000', TransformerTagger, conll2000)\n```\nThere is no validation partition provided in the repository hence 10% of the training set is used for validation.\n\nEvaluate the model on the test set:\n```\n>>> evaluate('chunk-conll2000', TransformerTagger, conll2000, 'test')\n```\n\n ## Standalone Use\n The `transformer.Encoder`, `transformer.Decoder` and `CRF` modules can be independently imported as they only depend on PyTorch:\n ```\n from torchnlp.modules.transformer import Encoder\n from torchnlp.modules.transformer import Decoder\n from torchnlp.modules.crf import CRF\n ```\nPlease refer to the comments within the source code for more details on the usage\n",
            "readme_url": "https://github.com/kolloldas/torchnlp",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Attention Is All You Need",
            "arxiv": "1706.03762",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.03762v5",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999815105514595,
        "task": "Machine Translation",
        "task_prob": 0.97087027233939
    }
}