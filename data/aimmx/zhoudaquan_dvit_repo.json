{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "DeepViT",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "zhoudaquan",
                "owner_type": "User",
                "name": "dvit_repo",
                "url": "https://github.com/zhoudaquan/dvit_repo",
                "stars": 96,
                "pushed_at": "2021-12-18 15:03:53+00:00",
                "created_at": "2021-03-24 00:59:04+00:00",
                "language": "Python",
                "license": "MIT License",
                "frameworks": [
                    "MXNet",
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "aad570ccdfd1b096d2fa78a0bd6338d3948c74a3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/blob/master/LICENSE"
                    }
                },
                "size": 1067
            },
            {
                "type": "code",
                "name": "MANIFEST.in",
                "sha": "4f2d15846513429725906870ff23b04370a41e86",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/blob/master/MANIFEST.in"
                    }
                },
                "size": 34
            },
            {
                "type": "code",
                "name": "attn_visualize.py",
                "sha": "dde3642ec29b2a36125004473a0514165b9e2ff8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/blob/master/attn_visualize.py"
                    }
                },
                "size": 848
            },
            {
                "type": "code",
                "name": "avg_checkpoints.py",
                "sha": "a692122481bc29f2216ea3ef7685760b7fa096db",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/blob/master/avg_checkpoints.py"
                    }
                },
                "size": 4590
            },
            {
                "type": "code",
                "name": "clean_checkpoint.py",
                "sha": "94f184d1b6eee26a2564a96b3d2452f24d9f2c7a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/blob/master/clean_checkpoint.py"
                    }
                },
                "size": 3333
            },
            {
                "type": "code",
                "name": "convert",
                "sha": "9af9034fbb63f57faf145768b421514d390ab4f9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/tree/master/convert"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "dist_utils",
                "sha": "9d16e136ef8ff7388a1d15c86c8aace9f16b8d4d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/tree/master/dist_utils"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "distributed_train.sh",
                "sha": "1985669e25b858964ce04b9796865b78d9870663",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/blob/master/distributed_train.sh"
                    }
                },
                "size": 108
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "d3dff95bb82ca753c5637021e9c22dee137316a0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/tree/master/docs"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "eval.sh",
                "sha": "bd25934cda904b8a21fce166a37c0cb4f031c066",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/blob/master/eval.sh"
                    }
                },
                "size": 90
            },
            {
                "type": "code",
                "name": "figures",
                "sha": "f1e8c8df58eea06a29124422e1acf9acc3b80c1f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/tree/master/figures"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "hubconf.py",
                "sha": "70fed79a27a254dbe5fc32a6ffb41f2fded08ef2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/blob/master/hubconf.py"
                    }
                },
                "size": 105
            },
            {
                "type": "code",
                "name": "inference.py",
                "sha": "16d199443e0707b8967970ecc6fbf5f5b670bc9b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/blob/master/inference.py"
                    }
                },
                "size": 5057
            },
            {
                "type": "code",
                "name": "lib_deit",
                "sha": "1dc80a57e3b6df7cdcd20c5fe898911e2177b18c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/tree/master/lib_deit"
                    }
                },
                "num_files": 16
            },
            {
                "type": "code",
                "name": "loss",
                "sha": "f1453b5ad8774a4ebf0540e53c3d82ce57be8389",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/tree/master/loss"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "models",
                "sha": "d520194712bcd17f7d5cb933ca82c059abaf57aa",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/tree/master/models"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "results",
                "sha": "e6c0981130aadbbbf5b05343371e96f4a893782c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/tree/master/results"
                    }
                },
                "num_files": 16
            },
            {
                "type": "code",
                "name": "run.sh",
                "sha": "9dc36cba13e51fd2d8b3c2c07e2e64bb5ef764c5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/blob/master/run.sh"
                    }
                },
                "size": 77
            },
            {
                "type": "code",
                "name": "scripts",
                "sha": "f9b365ab3db44635aff0eb8b63dfc9462d218e69",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/tree/master/scripts"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "3eea9d29c5e45c29fe7b5a8174418a7b302fdde6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/blob/master/setup.py"
                    }
                },
                "size": 1723
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "4ecfcc1dfab4616db748a52042f1d2f3e314f017",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/tree/master/tests"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "timm",
                "sha": "caf07bf7cceb687e8e3bcef2fa074e2fafbe6841",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/tree/master/timm"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "92a343f1dd95f54ca732d25082c7cdb104e5eee6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/blob/master/train.py"
                    }
                },
                "size": 38033
            },
            {
                "type": "code",
                "name": "utils",
                "sha": "131505d48ca8489b1c0e600fd71441905558aa2e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/tree/master/utils"
                    }
                },
                "num_files": 11
            },
            {
                "type": "code",
                "name": "validate.py",
                "sha": "5a0d388c15b3c8e4b52237f69ca45876f66009b3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zhoudaquan/dvit_repo/blob/master/validate.py"
                    }
                },
                "size": 14277
            }
        ]
    },
    "authors": [
        {
            "name": "zhoudaquan",
            "github_id": "zhoudaquan"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/zhoudaquan/dvit_repo",
            "stars": 96,
            "issues": true,
            "readme": "# DeepViT\n\nThis repo is the official implementation of [\"DeepViT: Towards Deeper Vision Transformer\"](https://arxiv.org/abs/2103.11886). The repo is based on the timm library (https://github.com/rwightman/pytorch-image-models) by [Ross Wightman](https://github.com/rwightman)\n\n## Introduction\n\n**Deep Vision Transformer** is initially described in [arxiv](https://arxiv.org/abs/2103.11886), which observes the attention collapese phenomenon when training deep vision transformers: In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6% on ImageNet.\n\n<p align=\"center\">\n<img src=\"https://github.com/zhoudaquan/DeepViT_ICCV21/blob/master/figures/performance_comparison.png\" | width=500>\n</p>\n\n## 2. DeepViT Models\n\n1. Comparison with ViTs\n\n| Model        | Re-attention | Top1 Acc (%) | #params | #Similar Blocks |  Checkpoint | Attention Map |\n| :---         |   :---:         |  :---:   |  :---:  | :---: |  :---:   | :---:   | \n| ViT-16       |  NA  |   78.88   |  24.5M  | 5 | [download here](comming soon)| |\n| DeepViT-16   |  FC  |   79.10   |  24.5M  | 0  | [download here](comming soon)|  |\n| ViT-24       |  NA  |   79.35   |  36.3M  | 11  | [download here](comming soon)|  |\n| DeepViT-24   |  FC  |   79.99   |  36.3M  | 0  | [download here](https://drive.google.com/file/d/1lPmKdNMPJq-mq-4FivqIY4JNAPoQHYxV/view?usp=sharing)|  |\n| ViT-32       | NA   |   79.27   |  48.1M  | 15 | [download here](comming soon)  |  |\n| DeepViT-32 | FC   |   80.90   |  48.1M  | 0  | [download here](https://drive.google.com/file/d/1kUBrAbAQfMT5FQJprITRxDtOE6ECZM0M/view?usp=sharing) |  |\n\n2. DeepViTs with CNNs for patch processing and optimized training hyper-parameters\n\n| Model        | Re-attention | Top1 Acc (%) | #Params | Image Size |  Checkpoint | \n| :---         |   :---:         |  :---:   |  :---:  | :---: |  :---:   | \n| DeepViT-16   |  FC + BN  |   82.3   |  24.5M  | 224  | [download here](https://drive.google.com/file/d/1evhBjla4Nth7LawW2PAXnGEcpv3YBWEC/view?usp=sharing)| \n| DeepViT-32   |  FC + BN  |   83.1   |  48.1M  | 224  | [download here](https://drive.google.com/file/d/1MTRm0NK_sPGRSyuD_hHJ1Zgw2diEaKLR/view?usp=sharing)| \n| DeepViT-32 | FC  + BN  |   84.25   |  48.1M  | 384  | [download here](comming soon) | \n\n## Evaluation\nTo evaluate a pre-trained DeepViT models on ImageNet val run:\n\n```\nbash eval.sh\n```\n## Attention Map Visualization\nTo visualize the self-attention map, first save the attention map matrix into a pickle file and run \n\n```\npython attn_visualize.py\n```\n\nThe pickle file of the [vit baseline model](https://drive.google.com/file/d/1s3oJreoeJZKpxbUpgQXAfiEb13r6af7H/view?usp=sharing) can be downloaded [here](https://drive.google.com/file/d/1rE659WiR775gj-44Ez6mTnuWGrvb5RPA/view?usp=sharing). The visualization rsults is shown below:\n\n<div align=\"center\">\n  <img width=\"100%\" alt=\"Self-attention map visualization a Vision Transformer with 16x16 patches trained with basic training hyper-parameter\" src=\"https://github.com/zhoudaquan/dvit_repo/blob/master/figures/attention_map_visualization.png\">\n</div>\n\n## Citing DeepVit\n\n```\n@article{zhou2021deepvit,\n  title={DeepViT: Towards Deeper Vision Transformer},\n  author={Zhou, Daquan and Kang, Bingyi and Jin, Xiaojie and Yang, Linjie and Lian, Xiaochen and Hou, Qibin and Feng, Jiashi},\n  journal={arXiv preprint arXiv:2103.11886},\n  year={2021}\n}\n```\n\n\n\n\n",
            "readme_url": "https://github.com/zhoudaquan/dvit_repo",
            "frameworks": [
                "MXNet",
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "DeepViT: Towards Deeper Vision Transformer",
            "arxiv": "2103.11886",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.11886v4",
            "abstract": "Vision transformers (ViTs) have been successfully applied in image\nclassification tasks recently. In this paper, we show that, unlike convolution\nneural networks (CNNs)that can be improved by stacking more convolutional\nlayers, the performance of ViTs saturate fast when scaled to be deeper. More\nspecifically, we empirically observe that such scaling difficulty is caused by\nthe attention collapse issue: as the transformer goes deeper, the attention\nmaps gradually become similar and even much the same after certain layers. In\nother words, the feature maps tend to be identical in the top layers of deep\nViT models. This fact demonstrates that in deeper layers of ViTs, the\nself-attention mechanism fails to learn effective concepts for representation\nlearning and hinders the model from getting expected performance gain. Based on\nabove observation, we propose a simple yet effective method, named\nRe-attention, to re-generate the attention maps to increase their diversity at\ndifferent layers with negligible computation and memory cost. The pro-posed\nmethod makes it feasible to train deeper ViT models with consistent performance\nimprovements via minor modification to existing ViT models. Notably, when\ntraining a deep ViT model with 32 transformer blocks, the Top-1 classification\naccuracy can be improved by 1.6% on ImageNet. Code is publicly available at\nhttps://github.com/zhoudaquan/dvit_repo.",
            "authors": [
                "Daquan Zhou",
                "Bingyi Kang",
                "Xiaojie Jin",
                "Linjie Yang",
                "Xiaochen Lian",
                "Zihang Jiang",
                "Qibin Hou",
                "Jiashi Feng"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9390883556064996,
        "task": "Visual Question Answering",
        "task_prob": 0.35677218024265567
    }
}