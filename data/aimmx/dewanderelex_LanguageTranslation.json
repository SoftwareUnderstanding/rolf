{
    "visibility": {
        "visibility": "public"
    },
    "name": "Language Translation with deep learning",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "dewanderelex",
                "owner_type": "User",
                "name": "LanguageTranslation",
                "url": "https://github.com/dewanderelex/LanguageTranslation",
                "stars": 0,
                "pushed_at": "2019-06-23 08:45:58+00:00",
                "created_at": "2019-06-23 08:41:44+00:00",
                "language": "Python",
                "frameworks": [
                    "Keras"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "48693b11557def4a6d3cfbf7a3d98b8fd01b61bc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dewanderelex/LanguageTranslation/blob/master/.gitignore"
                    }
                },
                "size": 45
            },
            {
                "type": "code",
                "name": "char2encoding.pkl",
                "sha": "26c747492f45e5718970ce4483f457dbaad1a3de",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dewanderelex/LanguageTranslation/blob/master/char2encoding.pkl"
                    }
                },
                "size": 3354
            },
            {
                "type": "code",
                "name": "predictionTranslation.py",
                "sha": "33e864ddf00516db8e4760ef6b5492cf03671bed",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dewanderelex/LanguageTranslation/blob/master/predictionTranslation.py"
                    }
                },
                "size": 843
            },
            {
                "type": "code",
                "name": "training.py",
                "sha": "110cd1ed99831612ff47207850d496eed1c4f9f0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dewanderelex/LanguageTranslation/blob/master/training.py"
                    }
                },
                "size": 1216
            },
            {
                "type": "code",
                "name": "util.py",
                "sha": "0ffa2cd19ab2ea5dc97c0df1ea77b98893e82141",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dewanderelex/LanguageTranslation/blob/master/util.py"
                    }
                },
                "size": 11592
            },
            {
                "type": "code",
                "name": "vie1.txt",
                "sha": "675a0cd49d4e27fd04e42eadb339888d8080b18c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dewanderelex/LanguageTranslation/blob/master/vie1.txt"
                    }
                },
                "size": 2778424
            }
        ]
    },
    "trained_model": {
        "binaries": [
            {
                "type": "binary",
                "name": "decoder_modelPredTranslation.h5",
                "sha": "7c33688dc1acf85fce429b90330f938dc06ee56e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dewanderelex/LanguageTranslation/blob/master/decoder_modelPredTranslation.h5"
                    }
                },
                "size": 20660616
            },
            {
                "type": "binary",
                "name": "encoder_modelPredTranslation.h5",
                "sha": "a1ee43db80d0c25732be7e6cd3fafbde0dcc088b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dewanderelex/LanguageTranslation/blob/master/encoder_modelPredTranslation.h5"
                    }
                },
                "size": 18362352
            }
        ]
    },
    "authors": [
        {
            "name": "Alexander Nguyen",
            "github_id": "dewanderelex"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/dewanderelex/LanguageTranslation",
            "stars": 0,
            "issues": true,
            "readme": "# Language Translation with deep learning \n\n## Project purpose\n\nFor this project we build a RNN sequence-to-sequence learning in Keras to translate a language A to a language B.\n\n## Language and Dataset\n\nSince I am french, I choose to translate english to french. However our system is pretty general and accepts any other language pair (e.g. english/french). By defauft, we use ANKI dataset which can be easy download [there](http://www.manythings.org/anki/)\n\n## What is Sequence-to-sequence learning ?\n\nSequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain to sequences in another domain. It works as following:\n\n1. We start with input sequences from a domain (e.g. English sentences) and correspding target sequences from another domain\n    (e.g. French sentences).\n2. An encoder LSTM turns input sequences to 2 state vectors (we keep the last LSTM state and discard the outputs).\n\n3. A decoder LSTM is trained to turn the target sequences into the same sequence but offset by one timestep in the future,     a training process called \"teacher forcing\" in this context.  Is uses as initial state the state vectors from the encoder.     Effectively, the decoder learns to generate `targets[t+1...]` given `targets[...t]`, conditioned on the input sequence.\n\t\n4. In inference mode, when we want to decode unknown input sequences, we:\n    * Encode the input sequence into state vectors\n    * Start with a target sequence of size 1 (just the start-of-sequence character)\n    *\tFeed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character\n    * Sample the next character using these predictions (we simply use argmax).\n    * Append the sampled character to the target sequence\n    * Repeat until we generate the end-of-sequence character or we hit the character limit.\n\t\nFor more information, please check these papers:\n\n\t* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n    * [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n\n## How to use it\n\n1. Downlow you training dataset\n2. Update path and the number of training example\n3. Run ```python3 training.py ```\t\n4. Prediction with ```python3 predictionTranslation.py```\n\t\n## LSTM or GRU\n\nBy default, the model runs with LSTM cell (long short term memory), but we also provide the user the opportunity to use instead GRU cell. (GRU cell only include 1 gate which is meke the training faster)\n\n## Downloading weights\n\nWe trained this model on the complete English/French dataset. The all training takes weeks. But we got promising results after 18 h of training (20 epoch). You can download our weights [there](https://drive.google.com/open?id=12s5KVDXex1Icy5FeFMLtQ2ADuWupzG_u)\n\n## Our result \n\nFor sure, our system is far from being as accurate as Google Transle. But after 20 epoch only, it reconnizes accurately short sentences.\n\nExample of output:\n\n``` Input sentence: I love you.``` \n\n``` Decoded sentence: Je t'aime !``` \n\nIt is accurate.\n\n``` Input sentence: We studied.``` \n\n``` Decoded sentence: Nous \u00e9tudions.``` \n\nIt is accurate.\n\n``` Input sentence: I slept well. ```\n\n``` Decoded sentence: J'ai dormi toute la journ\u00e9e. ```\n\nSame meaning, but the translation is not fully accurate. The right translation would be \"j'ai bien dormi\"\n\n``` Input sentence: He worked a lot.``` \n``` Decoded sentence: Il a travaill\u00e9 pour un homme riche.``` \n\nThe translation is not correct.\n\n## Conclusion\n\nTo conclude, our network learnt the basic concept of english/french, but it still requires two things:\n\n1. A longer training time\n2. A deeper architecture, such as more LSTM cells\n\n\n\n\n\n\n\n\n\n",
            "readme_url": "https://github.com/dewanderelex/LanguageTranslation",
            "frameworks": [
                "Keras"
            ]
        }
    ],
    "references": [
        {
            "title": "Sequence to Sequence Learning with Neural Networks",
            "arxiv": "1409.3215",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.3215v3",
            "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\na vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT'14 dataset, the translations produced by\nthe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\nnot have difficulty on long sentences. For comparison, a phrase-based SMT\nsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on\nthis task. The LSTM also learned sensible phrase and sentence representations\nthat are sensitive to word order and are relatively invariant to the active and\nthe passive voice. Finally, we found that reversing the order of the words in\nall source sentences (but not target sentences) improved the LSTM's performance\nmarkedly, because doing so introduced many short term dependencies between the\nsource and the target sentence which made the optimization problem easier.",
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V. Le"
            ]
        },
        {
            "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
            "arxiv": "1406.1078",
            "year": 2014,
            "url": "http://arxiv.org/abs/1406.1078v3",
            "abstract": "In this paper, we propose a novel neural network model called RNN\nEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNN\nencodes a sequence of symbols into a fixed-length vector representation, and\nthe other decodes the representation into another sequence of symbols. The\nencoder and decoder of the proposed model are jointly trained to maximize the\nconditional probability of a target sequence given a source sequence. The\nperformance of a statistical machine translation system is empirically found to\nimprove by using the conditional probabilities of phrase pairs computed by the\nRNN Encoder-Decoder as an additional feature in the existing log-linear model.\nQualitatively, we show that the proposed model learns a semantically and\nsyntactically meaningful representation of linguistic phrases.",
            "authors": [
                "Kyunghyun Cho",
                "Bart van Merrienboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999999873120792,
        "task": "Machine Translation",
        "task_prob": 0.9821498218250286
    }
}