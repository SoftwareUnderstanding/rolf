{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "THIS IS A REPLICA COPY FROM bert-as-service by Han Xiao, that uses the pre-trained model from BERT  google-research",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "jageshmaharjan",
                "owner_type": "User",
                "name": "BERT_Service",
                "url": "https://github.com/jageshmaharjan/BERT_Service",
                "stars": 1,
                "pushed_at": "2019-10-06 14:39:20+00:00",
                "created_at": "2018-11-17 08:42:25+00:00",
                "language": "Jupyter Notebook",
                "license": "MIT License",
                "frameworks": [
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "eae222075835e9c5a394af0c12d375cf0cb51c7e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/tree/master/.github"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "=",
                "sha": "b6ab08eda42cf7984538f56a7f1e84926cc8ce00",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/blob/master/="
                    }
                },
                "size": 176
            },
            {
                "type": "code",
                "name": "BERT_sentence_similarity.ipynb",
                "sha": "d5d2fc6669412a329fa1fee4c876acab04d9ed71",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/blob/master/BERT_sentence_similarity.ipynb"
                    }
                },
                "size": 133609
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "84c511105424346e0ff3da88d9c5450030e04e63",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/blob/master/LICENSE"
                    }
                },
                "size": 1065
            },
            {
                "type": "code",
                "name": "Word2Vec.ipynb",
                "sha": "61c8557ef3be49b4535ba8c2afa4fa8b1993f7b6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/blob/master/Word2Vec.ipynb"
                    }
                },
                "size": 135478
            },
            {
                "type": "code",
                "name": "__init__.py",
                "sha": "deb595886763f9a857969cdef312feea9e63a9bf",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/blob/master/__init__.py"
                    }
                },
                "size": 615
            },
            {
                "type": "code",
                "name": "app.py",
                "sha": "ba8146ade508fdf7c5f8858f05e7c0e34f883a4b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/blob/master/app.py"
                    }
                },
                "size": 1250
            },
            {
                "type": "code",
                "name": "azure-pipelines.yml",
                "sha": "a22a98cac19eaa0151f152aa6503e6edda27cd67",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/blob/master/azure-pipelines.yml"
                    }
                },
                "size": 2202
            },
            {
                "type": "code",
                "name": "benchmark.py",
                "sha": "9d215cf1de866f08131ffd7ef27f6badb07873d8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/blob/master/benchmark.py"
                    }
                },
                "size": 3013
            },
            {
                "type": "code",
                "name": "bert",
                "sha": "19b6ae4c11eef73fa41b7b52f8ffc4d6b04bfe7f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/tree/master/bert"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "client_example.py",
                "sha": "b1aed5c0ad8d86a0fa8770fd5d1b31cc5f764a9d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/blob/master/client_example.py"
                    }
                },
                "size": 603
            },
            {
                "type": "code",
                "name": "docker",
                "sha": "2ce9a1d66e67e8771748bb3de9c1fbca932b183f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/tree/master/docker"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "gpu_env.py",
                "sha": "0f28b1bf1c1b3d06b26655dee209c2a27aa1b430",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/blob/master/gpu_env.py"
                    }
                },
                "size": 752
            },
            {
                "type": "code",
                "name": "helper.py",
                "sha": "9fdec9fdfb5947db756ae0dc5cf14ac2854406f2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/blob/master/helper.py"
                    }
                },
                "size": 726
            },
            {
                "type": "code",
                "name": "manifests",
                "sha": "87738f662b43dd380b25dec83a67a241595a046c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/tree/master/manifests"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "requirements.client.txt",
                "sha": "fc6deded14c6d03f4105102796025b5fed4f7dd9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/blob/master/requirements.client.txt"
                    }
                },
                "size": 90
            },
            {
                "type": "code",
                "name": "requirements.gpu.txt",
                "sha": "9a8e5ce1a42efedcf721ca83a84816bcc027ed6e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/blob/master/requirements.gpu.txt"
                    }
                },
                "size": 214
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "9f5cf3c8907f801aff738df2633b5846b2b73444",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/blob/master/requirements.txt"
                    }
                },
                "size": 214
            },
            {
                "type": "code",
                "name": "service",
                "sha": "4a18a9bfa7c422bb91f65bae722381b98c95a13e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jageshmaharjan/BERT_Service/tree/master/service"
                    }
                },
                "num_files": 3
            }
        ]
    },
    "authors": [
        {
            "name": "jagesh maharjan (\u9a6c\u5bb6\u6770)",
            "email": "jageshmaharjan@hotmail.com",
            "github_id": "jageshmaharjan"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/jageshmaharjan/BERT_Service",
            "stars": 1,
            "issues": true,
            "readme": "## THIS IS A REPLICA COPY FROM bert-as-service by Han Xiao, that uses the pre-trained model from BERT  google-research\n\n# bert-as-service\n\n[![Python: 3.6](https://img.shields.io/badge/Python-3.6-brightgreen.svg)](https://opensource.org/licenses/MIT)    [![Tensorflow: 1.10](https://img.shields.io/badge/Tensorflow-1.10-brightgreen.svg)](https://opensource.org/licenses/MIT)  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nUsing BERT model as a sentence encoding service, i.e. mapping a variable-length sentence to a fixed-length vector.\n\n<img src=\".github/demo.gif\" width=\"600\">\n\nAuthor: Han Xiao [https://hanxiao.github.io](https://hanxiao.github.io)\n\n[BERT code of this repo](bert/) is forked from the [original BERT repo]((https://github.com/google-research/bert)) with necessary modification, [especially in extract_features.py](bert/extract_features.py).\n\n\n## What is it?\n\n**BERT**: [Developed by Google](https://github.com/google-research/bert), BERT is a method of pre-training language representations. It leverages an enormous amount of plain text data publicly available on the web and is trained in an unsupervised manner. Pre-training a BERT model is a fairly expensive yet one-time procedure for each language. Fortunately, Google released several pre-trained models where [you can download from here](https://github.com/google-research/bert#pre-trained-models).\n\n\n**Sentence Encoding/Embedding**: sentence encoding is a upstream task required in many NLP applications, e.g. sentiment analysis, text classification. The goal is to represent a variable length sentence into a fixed length vector, each element of which should \"encode\" some semantics of the original sentence.\n\n**Finally, this repo**: This repo uses BERT as the sentence encoder and hosts it as a service via ZeroMQ, allowing you to map sentences into fixed-length representations in just two lines of code. \n\n## Highlights\n\n- :telescope: **State-of-the-art**: based on pretrained 12/24-layer models released by Google AI, which is considered as a milestone in the NLP community.\n- :zap: **Fast**: 2000 sentence/s on a single Tesla M40 24GB with `max_seq_len=40`.\n- :traffic_light: **Concurrency**: support single-server-multi-client.\n- :smiley: **Easy-to-use**: require only two lines of code to get sentence encoding once the server is set up.\n\n## Requirements\n\n- Python >= 3.5 (Python 2 is NOT supported!)\n- Tensorflow >= 1.10\n\nThese two requirements MUST be satisfied. For other dependent packages, please refere to `requirments.txt`  and `requirments.client.txt`.\n\n## Usage\n\n#### 1. Download a Pre-trained BERT Model\nDownload a model from [here](https://github.com/google-research/bert#pre-trained-models), then uncompress the zip file into some folder, say `/tmp/english_L-12_H-768_A-12/`\n\nYou can use all models listed, including `BERT-Base, Multilingual` and `BERT-Base, Chinese`.\n\n\n#### 2. Start a BERT service\n```bash\npython app.py -num_worker=4 -model_dir /tmp/english_L-12_H-768_A-12/\n```\nThis will start a service with four workers, meaning that it can handel up to four **concurrent** requests. (These workers are behind a simple load balancer.)\n\n#### 3. Use Client to Encode Sentences\n> NOTE: please make sure your project includes [`client.py`](service/client.py), as we need to import `BertClient` class from this file. This is the **only file** that you will need as a client. You don't even need Tensorflow on client.\n\nNow you can use pretrained BERT to encode sentences in your Python code simply as follows:\n```python\nfrom service.client import BertClient\nec = BertClient()\nec.encode(['First do it', 'then do it right', 'then do it better'])\n```\nThis will return a python object with type `List[List[float]]`, each element of the outer `List` is the fixed representation of a sentence.\n\n### Using BERT Service Remotely\nOne can also start the service on one (GPU) machine and call it from another (CPU) machine as follows\n\n```python\n# on another CPU machine\nfrom service.client import BertClient\nec = BertClient(ip='xx.xx.xx.xx', port=5555)  # ip address of the GPU machine\nec.encode(['First do it', 'then do it right', 'then do it better'])\n```\n\n> NOTE: please make sure your project includes [`client.py`](service/client.py), as we need to import `BertClient` class from this file. Again, this is the **only file** that you need as a client. You don't even need Tensorflow. Please refer to [`requirements.client.txt`](requirements.client.txt) for the dependency on the client side.\n \n### Run service on Nvidia Docker\n```bash\ndocker build -t bert-as-service -f ./docker/Dockerfile .\nNUM_WORKER=1\nPATH_MODEL=<path of your model>\ndocker run --runtime nvidia -dit -p 5555:5555 -v $PATH_MODEL:/model -t bert-as-service $NUM_WORKER\n```\n\n## QA on Technical Details\n\n**Q:** Where do you get the fixed representation? Did you do pooling or something?\n\n**A:** I take the second-to-last hidden layer of all of the tokens in the sentence and do average pooling. See [the function I added to the modeling.py](bert/modeling.py#L236)\n\n**Q:** Why not use the hidden state of the first token, i.e. the `[CLS]`?\n\n**A:** Because a pre-trained model is not fine-tuned on any downstream tasks yet. In this case, the hidden state of `[CLS]` is not a good sentence representation. If later you fine-tune the model, you may [use `get_pooled_output()` to get the fixed length representation](bert/modeling.py#L224) as well.\n\n**Q:** Why not the last hidden layer? Why second-to-last?\n\n**A:** The last layer is too closed to the target functions (i.e. masked language model and next sentence prediction) during pre-training, therefore may be biased to those targets.\n\n**Q:** Could I use other pooling techniques?\n\n**A:** For sure. Just follows [`get_sentence_encoding()` I added to the modeling.py](bert/modeling.py#L236). Note that, if you introduce new `tf.variables` to the graph, then you need to train those variables before using the model. You may also want to check [some pooling techniques I mentioned in my blog post](https://hanxiao.github.io/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/#pooling-block).\n\n**Q:** Can I start multiple clients and send requests to one server simultaneously?\n\n**A:** Yes! That's the purpose of this repo. In fact you can start as many clients as you want. One server can handle all of them (given enough time).\n\n**Q:** How many requests can one service handle concurrently?\n\n**A:** The maximum number of concurrent requests is determined by `num_worker` in `app.py`. If you a sending more than `num_worker` requests concurrently, the new requests will be temporally stored in a queue until a free worker becomes available.\n\n**Q:** So one request means one sentence?\n\n**A:** No. One request means a list of sentences sent from a client. Think the size of a request as the batch size. A request may contain 256, 512 or 1024 sentences. The optimal size of a request is often determined empirically. One large request can certainly improve the GPU utilization, yet it also increases the overhead of transmission. You may run `python client_example.py` for a simple benchmark.\n\n**Q:** How about the speed? Is it fast enough for production?\n\n**A:** It highly depends on the `max_seq_len` and the size of a request. On a single Tesla M40 24GB with `max_seq_len=40`, you should get about 2000 samples per second using a 12-layer BERT. In general, I'd suggest smaller `max_seq_len` (25) and larger request size (512/1024).\n\n**Q:** Did you benchmark the efficiency?\n\n**A:** Yes. See [Benchmark](#Benchmark).\n\nTo reproduce the results, please run [`python benchmark.py`](benchmark.py).\n\n**Q:** What is backend based on?\n\n**A:** [ZeroMQ](http://zeromq.org/).\n\n**Q:** Do I need Tensorflow on the client side?\n\n**A:** No. Think of `BertClient` as a general feature extractor, whose output can be fed to *any* ML models, e.g. `scikit-learn`, `pytorch`, `tensorflow`. The only file that client need is [`client.py`](service/client.py). Copy this file to your project and import it, then you are ready to go.\n\n**Q:** Can I use multilingual BERT model provided by Google?\n\n**A:** Yes.\n\n**Q:** Can I use my own fine-tuned BERT model?\n\n**A:** Yes. Make sure you have the following three items in `model_dir`:\n                             \n- A TensorFlow checkpoint (`bert_model.ckpt`) containing the pre-trained weights (which is actually 3 files).\n- A vocab file (`vocab.txt`) to map WordPiece to word id.\n- A config file (`bert_config.json`) which specifies the hyperparameters of the model.\n\n**Q:** Can I run it in python 2?\n\n**A:** No.\n\n\n## Benchmark\n\nBenchmark was done on Tesla M40 24GB, experiments were repeated 10 times and the average value is reported. \n\nTo reproduce the results, please run\n```bash\npython benchmark.py\n```\n\n### Single GPU Single Client\nCommon arguments across all experiments are:\n\n| Parameter         | Value |\n|-------------------|-------|\n| num_worker        | 1     |\n| max_seq_len       | 40    |\n| client_batch_size | 2048  |\n| max_batch_size    | 256   |\n| num_client        | 1     |\n\n#### Speed wrt. `max_seq_len`\n\n| max_seq_len | seqs/s |\n|-------------|------------|\n| 20          | 2530       |\n| 40          | 2042       |\n| 80          | 1060       |\n\n#### Speed wrt. `client_batch_size`\n\n| client_batch_size | seqs/s |\n|-------------------|-------|\n| 256               | 520   |\n| 512               | 1037  |\n| 1024              | 2065  |\n| 2048              | 2021  |\n| 4096              | 2013  |\n\n#### Speed wrt. `max_batch_size`\n\n| max_batch_size | seqs/s |\n|----------------|-------|\n| 32             | 2025  |\n| 64             | 2020  |\n| 128            | 1963  |\n| 256            | 2058  |\n| 512            | 2047  |\n\n### Single GPU Multiple Client\n\n#### Speed wrt. `num_client`\n| num_client | seqs/s |\n|------------|-------|\n| 2          | 1048  |\n| 4          | 775   |\n| 8          | 534   |\n| 16         | 350   |\n| 32         | 217   |\n",
            "readme_url": "https://github.com/jageshmaharjan/BERT_Service",
            "frameworks": [
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "ZeroMQ",
            "url": "http://zeromq.org/"
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9982254971900405,
        "task": "Machine Translation",
        "task_prob": 0.9643508287897341
    }
}