{
    "visibility": {
        "visibility": "public",
        "license": "GNU General Public License v3.0"
    },
    "name": "CheXNet-with-localization",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "thtang",
                "owner_type": "User",
                "name": "CheXNet-with-localization",
                "url": "https://github.com/thtang/CheXNet-with-localization",
                "stars": 274,
                "pushed_at": "2021-10-12 22:51:50+00:00",
                "created_at": "2018-01-17 07:22:37+00:00",
                "language": "Python",
                "description": "Weakly Supervised Learning for Findings Detection in Medical Images",
                "license": "GNU General Public License v3.0",
                "frameworks": [
                    "scikit-learn",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "94a9ed024d3859793618152ea559a168bbcbb5e2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/thtang/CheXNet-with-localization/blob/master/LICENSE"
                    }
                },
                "size": 35147
            },
            {
                "type": "code",
                "name": "_config.yml",
                "sha": "c7418817439b2f071c93a4a6cee831e996123c0b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/thtang/CheXNet-with-localization/blob/master/_config.yml"
                    }
                },
                "size": 25
            },
            {
                "type": "code",
                "name": "deepQ_24.zip",
                "sha": "3e3d93aae832e7ff68647ce553c0486d771ce279",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/thtang/CheXNet-with-localization/blob/master/deepQ_24.zip"
                    }
                },
                "size": 26323604
            },
            {
                "type": "code",
                "name": "deepQ_25.zip",
                "sha": "9f13e764064e060360cc84e18f3db1050fe985cc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/thtang/CheXNet-with-localization/blob/master/deepQ_25.zip"
                    }
                },
                "size": 26324182
            },
            {
                "type": "code",
                "name": "denseNet_localization.py",
                "sha": "6d6379ce11cfa06a9eff83ba20cae3faaaab0bbc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/thtang/CheXNet-with-localization/blob/master/denseNet_localization.py"
                    }
                },
                "size": 9969
            },
            {
                "type": "code",
                "name": "find_bbox_size.py",
                "sha": "a1494f201a11fbf78cf84f1d87cd5baa80463913",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/thtang/CheXNet-with-localization/blob/master/find_bbox_size.py"
                    }
                },
                "size": 4713
            },
            {
                "type": "code",
                "name": "iou.py",
                "sha": "c35e11531a07657d7c471b334169979a8f393fbd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/thtang/CheXNet-with-localization/blob/master/iou.py"
                    }
                },
                "size": 5143
            },
            {
                "type": "code",
                "name": "model",
                "sha": "a35b14b33daff16688164f19fbbafc5b23c7a152",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/thtang/CheXNet-with-localization/tree/master/model"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "output",
                "sha": "c0d6ad096c739c8be0bed8fb100f6cadae4f72dc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/thtang/CheXNet-with-localization/tree/master/output"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "preprocessing.py",
                "sha": "2a96479c2eddbe532e9c15f188200c65aeb13d48",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/thtang/CheXNet-with-localization/blob/master/preprocessing.py"
                    }
                },
                "size": 3060
            },
            {
                "type": "code",
                "name": "report.pdf",
                "sha": "3bfccae2859e55f76867f8509cda48cfecf1b6e6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/thtang/CheXNet-with-localization/blob/master/report.pdf"
                    }
                },
                "size": 766202
            },
            {
                "type": "code",
                "name": "requirements3.txt",
                "sha": "9402e5257c7032e987b4ea00aa7ef3dcbab96c08",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/thtang/CheXNet-with-localization/blob/master/requirements3.txt"
                    }
                },
                "size": 93
            },
            {
                "type": "code",
                "name": "thresholds.npy",
                "sha": "ef4502b7680b922197aa7d48e13dcb43c79bdbba",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/thtang/CheXNet-with-localization/blob/master/thresholds.npy"
                    }
                },
                "size": 112
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "3310c91660da74bfb5c078acc8bfaa6390404170",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/thtang/CheXNet-with-localization/blob/master/train.py"
                    }
                },
                "size": 7175
            }
        ]
    },
    "authors": [
        {
            "name": "T.H. Tang",
            "email": "thtang@nlg.csie.ntu.edu.tw",
            "github_id": "thtang"
        },
        {
            "name": "Minhuan Fu",
            "email": "r06922030@ntu.edu.tw",
            "github_id": "mhfu0"
        }
    ],
    "tags": [
        "chexnet",
        "densenet",
        "nih",
        "chest-xray-images",
        "chestxray14",
        "weakly-supervised-learning",
        "pytorch",
        "bounding-boxes",
        "medical-imaging",
        "chest-x-ray8",
        "deep-learning"
    ],
    "description": "Weakly Supervised Learning for Findings Detection in Medical Images",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/thtang/CheXNet-with-localization",
            "stars": 274,
            "issues": true,
            "readme": "# CheXNet-with-localization\nADLxMLDS 2017 fall final\n\nTeam:XD\n\n*\u9ec3\u6674 (R06922014), \u738b\u601d\u5091 (R06922019), \u66f9\u7217\u6587 (R06922022), \u5085\u654f\u6853 (R06922030), \u6e6f\u5fe0\u61b2 (R06946003)*\n## Weakly supervised localization :\nIn this task, we have to plot bounding boxes for each disease finding in a single chest X-ray without goundtruth (X, Y, width, height) in training set. The workflow is shown below:\n### Workflow :\n<img src=\"https://github.com/thtang/CheXNet-with-localization/blob/master/output/process_flow.png\">\n1) Predict findings\n2) Use the classifier to plot heatmap (Grad-CAM)\n3) Plot the bounding box base on Grad-CAM\n### Package : \n`Pytorch==0.2.0` &nbsp; `torchvision==0.2.0` &nbsp;` matplotlib`  &nbsp;` scikit-image==0.13.1` &nbsp;` opencv_python==3.4.0.12` &nbsp;` numpy==1.13.3` &nbsp;`matplotlib==2.1.1` &nbsp;`scipy==1.0.0` &nbsp; `sklearn==0.19.1` &nbsp;\n\n### Environment:\n* OS: Linux\n* Python 3.5\n* GPU: 1080 Ti\n* CPU: Xeon(R) E5-2667 v4\n* RAM: 500 GB\n### Experiments process:\n1) preprocessing:\n```\npython3 preprocessing.py [path of images folder] [path to data_entry] [path to bbox_list_path] [path to train_txt] [path to valid_txt] [path of preprocessed output (folder)]\n```\n\n2) training:\n```\npython3 train.py [path of preprocessed output (folder)]\n```\n\n3) local testing:\n```\npython3 denseNet_localization.py [path to test.txt] [path of images folder]\n```\n4) Output txt format: <br>\nAfter running denseNet_localization.py, you would get a txt file. The format is shown below:<br>\n```\n[image_path] [number_of_detection]\n[disease] [x] [y] [width] [height]\n[disease] [x] [y] [width] [height]\n...\n[image_path] [number_of_detection]\n[disease] [x] [y] [width] [height]\n[disease] [x] [y] [width] [height]\n...\n```\n------\nFor DeepQ platform testing:\n\nupload **deepQ_25.zip** to the platform. Then use following command:\n```\npython3 inference.py\n```\n------\n\n5) For **visualization**, please refers to [issue](https://github.com/thtang/CheXNet-with-localization/issues/9). Credit to [Sadam1195](https://github.com/Sadam1195).\n\n### Note :\nIn our .py script, I used the following script to assign the task running on GPU 0.<br>\n\n```\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n```\n### Model :\n<img src=\"https://github.com/thtang/CheXNet-with-localization/blob/master/output/multi_label_denseNet.png\">\n* Image is modified from Ref [2].\n\n### Result :\n*Prediction*<br>\n<img src=\"https://github.com/thtang/CheXNet-with-localization/blob/master/output/prediction.png\" width=\"320\"><br>\n*Heatmap per disease*\n![Alt Text](https://github.com/thtang/CheXNet-with-localization/blob/master/output/heatmap_per_class.jpg)\nVisualization of some heat maps with its ground-truth label (red) and its prediction\n(blue) selected from each disease class. (From top-left to bottom: Atelectasis, Cardiomegaly,\nEffusion, Infiltration, Mass, Nodule, Pneumonia and Pneumothorax)\n\n*Bounding Box per patient*\n![Alt Text](https://github.com/thtang/CheXNet-with-localization/blob/master/output/bb_select.JPG)\nVisualization of some images with its ground-truth label (red) and its prediction\n(blue) selected from each disease class.\n\n**Refers to the [report](https://github.com/thtang/CheXNet-with-localization/blob/master/report.pdf) for more experiment results.**\n## Reference:\n1. *ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases* [[Arxiv]](https://arxiv.org/pdf/1705.02315.pdf)\n2. *LEARNING TO DIAGNOSE FROM SCRATCH BY EXPLOITING DEPENDENCIES AMONG LABELS* [[Arxiv]](https://arxiv.org/pdf/1710.10501.pdf)\n3. *CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning* [[Arxiv]](https://arxiv.org/pdf/1711.05225.pdf)\n4. *Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization* [[Arxiv]](https://arxiv.org/pdf/1610.02391.pdf)\n\n## Contact:\nFeel free to contact me (thtang@nlg.csie.ntu.edu.tw) if you have any problem.\n",
            "readme_url": "https://github.com/thtang/CheXNet-with-localization",
            "frameworks": [
                "scikit-learn",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
            "arxiv": "1610.02391",
            "year": 2016,
            "url": "http://arxiv.org/abs/1610.02391v4",
            "abstract": "We propose a technique for producing \"visual explanations\" for decisions from\na large class of CNN-based models, making them more transparent. Our approach -\nGradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of\nany target concept, flowing into the final convolutional layer to produce a\ncoarse localization map highlighting important regions in the image for\npredicting the concept. Grad-CAM is applicable to a wide variety of CNN\nmodel-families: (1) CNNs with fully-connected layers, (2) CNNs used for\nstructured outputs, (3) CNNs used in tasks with multimodal inputs or\nreinforcement learning, without any architectural changes or re-training. We\ncombine Grad-CAM with fine-grained visualizations to create a high-resolution\nclass-discriminative visualization and apply it to off-the-shelf image\nclassification, captioning, and visual question answering (VQA) models,\nincluding ResNet-based architectures. In the context of image classification\nmodels, our visualizations (a) lend insights into their failure modes, (b) are\nrobust to adversarial images, (c) outperform previous methods on localization,\n(d) are more faithful to the underlying model and (e) help achieve\ngeneralization by identifying dataset bias. For captioning and VQA, we show\nthat even non-attention based models can localize inputs. We devise a way to\nidentify important neurons through Grad-CAM and combine it with neuron names to\nprovide textual explanations for model decisions. Finally, we design and\nconduct human studies to measure if Grad-CAM helps users establish appropriate\ntrust in predictions from models and show that Grad-CAM helps untrained users\nsuccessfully discern a 'stronger' nodel from a 'weaker' one even when both make\nidentical predictions. Our code is available at\nhttps://github.com/ramprs/grad-cam/, along with a demo at\nhttp://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.",
            "authors": [
                "Ramprasaath R. Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ]
        },
        {
            "title": "Learning to diagnose from scratch by exploiting dependencies among labels",
            "arxiv": "1710.10501",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.10501v2",
            "abstract": "The field of medical diagnostics contains a wealth of challenges which\nclosely resemble classical machine learning problems; practical constraints,\nhowever, complicate the translation of these endpoints naively into classical\narchitectures. Many tasks in radiology, for example, are largely problems of\nmulti-label classification wherein medical images are interpreted to indicate\nmultiple present or suspected pathologies. Clinical settings drive the\nnecessity for high accuracy simultaneously across a multitude of pathological\noutcomes and greatly limit the utility of tools which consider only a subset.\nThis issue is exacerbated by a general scarcity of training data and maximizes\nthe need to extract clinically relevant features from available samples --\nideally without the use of pre-trained models which may carry forward\nundesirable biases from tangentially related tasks. We present and evaluate a\npartial solution to these constraints in using LSTMs to leverage\ninterdependencies among target labels in predicting 14 pathologic patterns from\nchest x-rays and establish state of the art results on the largest publicly\navailable chest x-ray dataset from the NIH without pre-training. Furthermore,\nwe propose and discuss alternative evaluation metrics and their relevance in\nclinical practice.",
            "authors": [
                "Li Yao",
                "Eric Poblenz",
                "Dmitry Dagunts",
                "Ben Covington",
                "Devon Bernard",
                "Kevin Lyman"
            ]
        },
        {
            "title": "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning",
            "arxiv": "1711.05225",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.05225v3",
            "abstract": "We develop an algorithm that can detect pneumonia from chest X-rays at a\nlevel exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer\nconvolutional neural network trained on ChestX-ray14, currently the largest\npublicly available chest X-ray dataset, containing over 100,000 frontal-view\nX-ray images with 14 diseases. Four practicing academic radiologists annotate a\ntest set, on which we compare the performance of CheXNet to that of\nradiologists. We find that CheXNet exceeds average radiologist performance on\nthe F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and\nachieve state of the art results on all 14 diseases.",
            "authors": [
                "Pranav Rajpurkar",
                "Jeremy Irvin",
                "Kaylie Zhu",
                "Brandon Yang",
                "Hershel Mehta",
                "Tony Duan",
                "Daisy Ding",
                "Aarti Bagul",
                "Curtis Langlotz",
                "Katie Shpanskaya",
                "Matthew P. Lungren",
                "Andrew Y. Ng"
            ]
        },
        {
            "title": "ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases",
            "arxiv": "1705.02315",
            "year": 2017,
            "url": "http://arxiv.org/abs/1705.02315v5",
            "abstract": "The chest X-ray is one of the most commonly accessible radiological\nexaminations for screening and diagnosis of many lung diseases. A tremendous\nnumber of X-ray imaging studies accompanied by radiological reports are\naccumulated and stored in many modern hospitals' Picture Archiving and\nCommunication Systems (PACS). On the other side, it is still an open question\nhow this type of hospital-size knowledge database containing invaluable imaging\ninformatics (i.e., loosely labeled) can be used to facilitate the data-hungry\ndeep learning paradigms in building truly large-scale high precision\ncomputer-aided diagnosis (CAD) systems.\n  In this paper, we present a new chest X-ray database, namely \"ChestX-ray8\",\nwhich comprises 108,948 frontal-view X-ray images of 32,717 unique patients\nwith the text-mined eight disease image labels (where each image can have\nmulti-labels), from the associated radiological reports using natural language\nprocessing. Importantly, we demonstrate that these commonly occurring thoracic\ndiseases can be detected and even spatially-located via a unified\nweakly-supervised multi-label image classification and disease localization\nframework, which is validated using our proposed dataset. Although the initial\nquantitative results are promising as reported, deep convolutional neural\nnetwork based \"reading chest X-rays\" (i.e., recognizing and locating the common\ndisease patterns trained with only image-level labels) remains a strenuous task\nfor fully-automated high precision CAD systems. Data download link:\nhttps://nihcc.app.box.com/v/ChestXray-NIHCC",
            "authors": [
                "Xiaosong Wang",
                "Yifan Peng",
                "Le Lu",
                "Zhiyong Lu",
                "Mohammadhadi Bagheri",
                "Ronald M. Summers"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9801950810572722,
        "task": "Image Classification",
        "task_prob": 0.8917286398010759
    }
}