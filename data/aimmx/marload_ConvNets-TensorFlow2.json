{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "ConvNets-TensorFlow2",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "marload",
                "owner_type": "User",
                "name": "ConvNets-TensorFlow2",
                "url": "https://github.com/marload/ConvNets-TensorFlow2",
                "stars": 78,
                "pushed_at": "2020-05-05 13:49:57+00:00",
                "created_at": "2020-04-22 11:22:01+00:00",
                "language": "Python",
                "description": "\u26f5\ufe0f Implementation a variety of popular Image Classification Models using TensorFlow2. [ResNet, GoogLeNet, VGG, Inception-v3, Inception-v4, MobileNet, MobileNet-v2, ShuffleNet, ShuffleNet-v2, etc...]",
                "license": "Apache License 2.0",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "ff4aca4fd6b7cbe7bbff8f25bfc0a453cdf8aee0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/marload/ConvNets-TensorFlow2/blob/master/.gitignore"
                    }
                },
                "size": 31
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "261eeb9e9f8b2b4b0d119366dda99c6fd7d35c64",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/marload/ConvNets-TensorFlow2/blob/master/LICENSE"
                    }
                },
                "size": 11357
            },
            {
                "type": "code",
                "name": "assets",
                "sha": "5c7b490eab1b5eecf87c67c369078e12b07ee93e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/marload/ConvNets-TensorFlow2/tree/master/assets"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "553be73fbe41aa8f890b3bad211c1f486a014ed8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/marload/ConvNets-TensorFlow2/blob/master/main.py"
                    }
                },
                "size": 2748
            },
            {
                "type": "code",
                "name": "models",
                "sha": "d6259b20070a94891a3268218126afd06a160855",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/marload/ConvNets-TensorFlow2/tree/master/models"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "bd953fb1f186faebfc2367a63093257eca28a7e1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/marload/ConvNets-TensorFlow2/blob/master/utils.py"
                    }
                },
                "size": 3059
            }
        ]
    },
    "authors": [
        {
            "name": "Wansoo Kim",
            "email": "rladhkstn8@gmail.com",
            "github_id": "marload"
        }
    ],
    "tags": [
        "tensorflow",
        "machine-learning",
        "deep-learning",
        "resnet",
        "googlenet",
        "vgg",
        "inception-v3",
        "inception-v4",
        "mobilenet",
        "mobilenet-v2",
        "shufflenet",
        "shufflenet-v2"
    ],
    "description": "\u26f5\ufe0f Implementation a variety of popular Image Classification Models using TensorFlow2. [ResNet, GoogLeNet, VGG, Inception-v3, Inception-v4, MobileNet, MobileNet-v2, ShuffleNet, ShuffleNet-v2, etc...]",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/marload/ConvNets-TensorFlow2",
            "stars": 78,
            "issues": true,
            "readme": "![TF Depend](https://img.shields.io/badge/TensorFlow-2.1-orange) ![License Badge](https://img.shields.io/badge/license-Apache%202-green)<br>\n\n<p align=\"center\">\n  <img width=\"150\" src=\"./assets/logo.png\">\n</p>\n\n<h2 align=center>Convolutional Nets in TensorFlow2</h2>\n\n[ConvNets-TensorFlow2](https://github.com/marload/ConvNetsRL-TensorFlow2) is a repository that implements a variety of popular Deep Convolutional Network Architectures using [TensorFlow2](https://tensorflow.org). The core of this repository is intuitive code and concise architecture. If you are a user of TensorFlow2 and want to study various and popular CNN architectures, this repository will be the best choice to study. ConvNets-TensorFlow2 is continuously updated and managed. This repository has been very much influenced by [Cifar100-pytorch](https://github.com/weiaicunzai/pytorch-cifar100).\n\n## Usage\n```bash\n$ python main.py \n    --nets={NETS} \n    --batch_size={BATCH_SIZE} \n    --lr={LEARNING_RATE} \n    --epochs={EPOCHS}\n```\n\n## Models\n\n- [VGG](#vgg)\n- [GoogLeNet](#googlenet)\n- [ResNet](#resnet)\n- [DenseNet](#densenet)\n- [InceptionV3](#inceptionv3)\n- [InceptionV4](#inceptionv4)\n- [MobileNet](#mobilenet)\n- [MobileNetV2](#mobilenetv2)\n- [Squeezenet](#squeezenet)\n- [SENet](#senet)\n- [ShuffleNet](#shufflenet)\n- [CondenseNet](#condenseNet)\n- [Xcention](#xception)\n- [PreActResNet](#preactresnet)\n- [ResAttNet](#resattnet)\n- [ResNeXt](#resnext)\n- [PolyNet](#polynet)\n- [PyramidNet](#pyramidnet)\n\n\n<hr>\n\n<a name='vgg'></a>\n\n### VGG\n\n**Paper** [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)<br>\n**Author** Karen Simonyan, Andrew Zissermanr<br>\n**Code** [VGG.py](https://github.com/marload/ConvNets-TensorFlow2/blob/master/models/VGG.py)\n<br><br>\n**Model Options**\n\n```bash\n--nets {VGG11 or VGG13 or VGG16 or VGG19}\n```\n\n<hr>\n\n<a name='googlenet'></a>\n\n### GoogLeNet\n\n**Paper** [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)<br>\n**Author** Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich<br>\n**Code** [GoogLeNet.py](https://github.com/marload/ConvNets-TensorFlow2/blob/master/models/GoogLeNet.py)\n<br><br>\n**Model Options**\n\n```bash\n--nets {GoogLeNet}\n```\n\n<hr>\n\n<a name='resnet'></a>\n\n### ResNet\n\n**Paper** [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)<br>\n**Author** Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun<br>\n**Code** [ResNet.py](https://github.com/marload/ConvNets-TensorFlow2/blob/master/models/ResNet.py)\n<br><br>\n**Model Options**\n\n```bash\n--nets {ResNet18 or ResNet34 ResNet50 ResNet101 ResNet 152}\n```\n\n<hr>\n<a name='densenet'></a>\n\n### DenseNet\n\n**Paper** [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)<br>\n**Author** Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger\n<br>\n**Code** [DenseNet.py](https://github.com/marload/ConvNets-TensorFlow2/blob/master/models/DenseNet.py)\n<br><br>\n**Model Options**\n\n```bash\n--nets {DenseNet121 or DenseNet169 or DenseNet201 or DenseNet161}\n```\n\n<hr>\n\n\n<a name='inceptionv3'></a>\n\n### InceptionV3\n\n**Paper** [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)<br>\n**Author** Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna\n<br>\n**Code** [InceptionV3.py](https://github.com/marload/ConvNets-TensorFlow2/blob/master/models/InceptionV3.py)\n<br><br>\n**Model Options**\n\n```bash\n--nets {InceptionV3}\n```\n\n<hr>\n\n<a name='inceptionv4'></a>\n\n### InceptionV4\n\n**Paper** [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261)<br>\n**Author** Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n<br>\n**Code** [InceptionV4.py](https://github.com/marload/ConvNets-TensorFlow2/blob/master/models/InceptionV4.py)\n<br><br>\n**Model Options**\n\n```bash\n--nets {InceptionV4}\n```\n\n<hr>\n\n<a name='mobilenet'></a>\n\n### MobileNet\n\n**Paper** [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)<br>\n**Author** Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam\n<br>\n**Code** [MobileNet.py](https://github.com/marload/ConvNets-TensorFlow2/blob/master/models/MobileNet.py)\n<br><br>\n**Model Options**\n\n```bash\n--nets {MobileNet}\n```\n\n<hr>\n\n<a name='mobilenetv2'></a>\n\n### MobileNetV2\n\n**Paper** [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)<br>\n**Author** Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen\n<br>\n**Code** [MobileNetV2.py](https://github.com/marload/ConvNets-TensorFlow2/blob/master/models/MobileNetV2.py)\n<br><br>\n**Model Options**\n\n```bash\n--nets {MobileNetV2}\n```\n\n<hr>\n\n<a name='squeezenet'></a>\n\n### SqueezeNet\n\n**Paper** [SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size](https://arxiv.org/abs/1602.07360)<br>\n**Author** Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer\n<br>\n**Code** [SqueezeNet.py](https://github.com/marload/ConvNets-TensorFlow2/blob/master/models/SqueezeNet.py)\n<br><br>\n**Model Options**\n\n```bash\n--nets {SqueezeNet}\n```\n\n<hr>\n\n<a name='SENet'></a>\n\n### SENet\n\n**Paper** [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)<br>\n**Author** Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu\n<br>\n**Code** [SEResNet.py](https://github.com/marload/ConvNets-TensorFlow2/blob/master/models/SEResNet.py)\n<br><br>\n**Model Options**\n\n```bash\n--nets {SEResNet18 or SEResNet34 or SEResNet50 or SEResNet101 or SEResNet152}\n```\n\n<hr>\n\n<a name='shufflenet'></a>\n\n### ShuffleNet\n\n**Paper** [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](https://arxiv.org/abs/1707.01083)<br>\n**Author** Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun\n<br>\n**Code** Coming Soon\n<br><br>\n**Model Options**\n\n```bash\n// Coming Soon\n```\n\n<hr>\n\n<a name='condensenet'></a>\n\n### CondenseNet\n\n**Paper** [CondenseNet: An Efficient DenseNet using Learned Group Convolutions](https://arxiv.org/abs/1711.09224)<br>\n**Author** Gao Huang, Shichen Liu, Laurens van der Maaten, Kilian Q. Weinberger\n<br>\n**Code** Coming Soon\n<br><br>\n**Model Options**\n\n```bash\n// Coming Soon\n```\n\n<hr>\n\n<a name='xception'></a>\n\n### Xception\n\n**Paper** [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)<br>\n**Author** Fran\u00e7ois Chollet\n<br>\n**Code** Coming Soon\n<br><br>\n**Model Options**\n\n```bash\n// Coming Soon\n```\n\n<hr>\n\n<a name='preactresnet'></a>\n\n### PreActResNet\n\n**Paper** [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)<br>\n**Author** Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n<br>\n**Code** Coming Soon\n<br><br>\n**Model Options**\n\n```bash\n// Coming Soon\n```\n\n<hr>\n\n<a name='resattnet'></a>\n\n### ResAttNet\n\n**Paper** [Residual Attention Network for Image Classification](https://arxiv.org/abs/1704.06904)<br>\n**Author** Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang\n<br>\n**Code** Coming Soon\n<br><br>\n**Model Options**\n\n```bash\n// Coming Soon\n```\n\n<hr>\n\n<a name='polynet'></a>\n\n### PolyNet\n\n**Paper** [PolyNet: A Pursuit of Structural Diversity in Very Deep Networks](https://arxiv.org/abs/1611.05725)<br>\n**Author** Xingcheng Zhang, Zhizhong Li, Chen Change Loy, Dahua Lin\n<br>\n**Code** Coming Soon\n<br><br>\n**Model Options**\n\n```bash\n// Coming Soon\n```\n\n<hr>\n\n<a name='pyramidnet'></a>\n\n### PyramidNet\n\n**Paper** [Deep Pyramidal Residual Networks](https://arxiv.org/abs/1610.02915)<br>\n**Author** Dongyoon Han, Jiwhan Kim, Junmo Kim\n<br>\n**Code** Coming Soon\n<br><br>\n**Model Options**\n\n```bash\n// Coming Soon\n```\n\n<hr>\n\n## Reference\n\n- https://github.com/weiaicunzai/pytorch-cifar100\n- https://github.com/tensorflow/tensorflow\n",
            "readme_url": "https://github.com/marload/ConvNets-TensorFlow2",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Rethinking the Inception Architecture for Computer Vision",
            "arxiv": "1512.00567",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.00567v3",
            "abstract": "Convolutional networks are at the core of most state-of-the-art computer\nvision solutions for a wide variety of tasks. Since 2014 very deep\nconvolutional networks started to become mainstream, yielding substantial gains\nin various benchmarks. Although increased model size and computational cost\ntend to translate to immediate quality gains for most tasks (as long as enough\nlabeled data is provided for training), computational efficiency and low\nparameter count are still enabling factors for various use cases such as mobile\nvision and big-data scenarios. Here we explore ways to scale up networks in\nways that aim at utilizing the added computation as efficiently as possible by\nsuitably factorized convolutions and aggressive regularization. We benchmark\nour methods on the ILSVRC 2012 classification challenge validation set\ndemonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6%\ntop-5 error for single frame evaluation using a network with a computational\ncost of 5 billion multiply-adds per inference and with using less than 25\nmillion parameters. With an ensemble of 4 models and multi-crop evaluation, we\nreport 3.5% top-5 error on the validation set (3.6% error on the test set) and\n17.3% top-1 error on the validation set.",
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jonathon Shlens",
                "Zbigniew Wojna"
            ]
        },
        {
            "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices",
            "arxiv": "1707.01083",
            "year": 2017,
            "url": "http://arxiv.org/abs/1707.01083v2",
            "abstract": "We introduce an extremely computation-efficient CNN architecture named\nShuffleNet, which is designed specially for mobile devices with very limited\ncomputing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new\noperations, pointwise group convolution and channel shuffle, to greatly reduce\ncomputation cost while maintaining accuracy. Experiments on ImageNet\nclassification and MS COCO object detection demonstrate the superior\nperformance of ShuffleNet over other structures, e.g. lower top-1 error\n(absolute 7.8%) than recent MobileNet on ImageNet classification task, under\nthe computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet\nachieves ~13x actual speedup over AlexNet while maintaining comparable\naccuracy.",
            "authors": [
                "Xiangyu Zhang",
                "Xinyu Zhou",
                "Mengxiao Lin",
                "Jian Sun"
            ]
        },
        {
            "title": "Going Deeper with Convolutions",
            "arxiv": "1409.4842",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.4842v1",
            "abstract": "We propose a deep convolutional neural network architecture codenamed\n\"Inception\", which was responsible for setting the new state of the art for\nclassification and detection in the ImageNet Large-Scale Visual Recognition\nChallenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the\nimproved utilization of the computing resources inside the network. This was\nachieved by a carefully crafted design that allows for increasing the depth and\nwidth of the network while keeping the computational budget constant. To\noptimize quality, the architectural decisions were based on the Hebbian\nprinciple and the intuition of multi-scale processing. One particular\nincarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22\nlayers deep network, the quality of which is assessed in the context of\nclassification and detection.",
            "authors": [
                "Christian Szegedy",
                "Wei Liu",
                "Yangqing Jia",
                "Pierre Sermanet",
                "Scott Reed",
                "Dragomir Anguelov",
                "Dumitru Erhan",
                "Vincent Vanhoucke",
                "Andrew Rabinovich"
            ]
        },
        {
            "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
            "arxiv": "1704.04861",
            "year": 2017,
            "url": "http://arxiv.org/abs/1704.04861v1",
            "abstract": "We present a class of efficient models called MobileNets for mobile and\nembedded vision applications. MobileNets are based on a streamlined\narchitecture that uses depth-wise separable convolutions to build light weight\ndeep neural networks. We introduce two simple global hyper-parameters that\nefficiently trade off between latency and accuracy. These hyper-parameters\nallow the model builder to choose the right sized model for their application\nbased on the constraints of the problem. We present extensive experiments on\nresource and accuracy tradeoffs and show strong performance compared to other\npopular models on ImageNet classification. We then demonstrate the\neffectiveness of MobileNets across a wide range of applications and use cases\nincluding object detection, finegrain classification, face attributes and large\nscale geo-localization.",
            "authors": [
                "Andrew G. Howard",
                "Menglong Zhu",
                "Bo Chen",
                "Dmitry Kalenichenko",
                "Weijun Wang",
                "Tobias Weyand",
                "Marco Andreetto",
                "Hartwig Adam"
            ]
        },
        {
            "title": "Identity Mappings in Deep Residual Networks",
            "arxiv": "1603.05027",
            "year": 2016,
            "url": "http://arxiv.org/abs/1603.05027v3",
            "abstract": "Deep residual networks have emerged as a family of extremely deep\narchitectures showing compelling accuracy and nice convergence behaviors. In\nthis paper, we analyze the propagation formulations behind the residual\nbuilding blocks, which suggest that the forward and backward signals can be\ndirectly propagated from one block to any other block, when using identity\nmappings as the skip connections and after-addition activation. A series of\nablation experiments support the importance of these identity mappings. This\nmotivates us to propose a new residual unit, which makes training easier and\nimproves generalization. We report improved results using a 1001-layer ResNet\non CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.\nCode is available at: https://github.com/KaimingHe/resnet-1k-layers",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "Deep Pyramidal Residual Networks",
            "arxiv": "1610.02915",
            "year": 2016,
            "url": "http://arxiv.org/abs/1610.02915v4",
            "abstract": "Deep convolutional neural networks (DCNNs) have shown remarkable performance\nin image classification tasks in recent years. Generally, deep neural network\narchitectures are stacks consisting of a large number of convolutional layers,\nand they perform downsampling along the spatial dimension via pooling to reduce\nmemory usage. Concurrently, the feature map dimension (i.e., the number of\nchannels) is sharply increased at downsampling locations, which is essential to\nensure effective performance because it increases the diversity of high-level\nattributes. This also applies to residual networks and is very closely related\nto their performance. In this research, instead of sharply increasing the\nfeature map dimension at units that perform downsampling, we gradually increase\nthe feature map dimension at all units to involve as many locations as\npossible. This design, which is discussed in depth together with our new\ninsights, has proven to be an effective means of improving generalization\nability. Furthermore, we propose a novel residual unit capable of further\nimproving the classification accuracy with our new network architecture.\nExperiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown\nthat our network architecture has superior generalization ability compared to\nthe original residual networks. Code is available at\nhttps://github.com/jhkim89/PyramidNet}",
            "authors": [
                "Dongyoon Han",
                "Jiwhan Kim",
                "Junmo Kim"
            ]
        },
        {
            "title": "CondenseNet: An Efficient DenseNet using Learned Group Convolutions",
            "arxiv": "1711.09224",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.09224v2",
            "abstract": "Deep neural networks are increasingly used on mobile devices, where\ncomputational resources are limited. In this paper we develop CondenseNet, a\nnovel network architecture with unprecedented efficiency. It combines dense\nconnectivity with a novel module called learned group convolution. The dense\nconnectivity facilitates feature re-use in the network, whereas learned group\nconvolutions remove connections between layers for which this feature re-use is\nsuperfluous. At test time, our model can be implemented using standard group\nconvolutions, allowing for efficient computation in practice. Our experiments\nshow that CondenseNets are far more efficient than state-of-the-art compact\nconvolutional networks such as MobileNets and ShuffleNets.",
            "authors": [
                "Gao Huang",
                "Shichen Liu",
                "Laurens van der Maaten",
                "Kilian Q. Weinberger"
            ]
        },
        {
            "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size",
            "arxiv": "1602.07360",
            "year": 2016,
            "url": "http://arxiv.org/abs/1602.07360v4",
            "abstract": "Recent research on deep neural networks has focused primarily on improving\naccuracy. For a given accuracy level, it is typically possible to identify\nmultiple DNN architectures that achieve that accuracy level. With equivalent\naccuracy, smaller DNN architectures offer at least three advantages: (1)\nSmaller DNNs require less communication across servers during distributed\ntraining. (2) Smaller DNNs require less bandwidth to export a new model from\nthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on\nFPGAs and other hardware with limited memory. To provide all of these\nadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet\nachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques we are able to compress\nSqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n  The SqueezeNet architecture is available for download here:\nhttps://github.com/DeepScale/SqueezeNet",
            "authors": [
                "Forrest N. Iandola",
                "Song Han",
                "Matthew W. Moskewicz",
                "Khalid Ashraf",
                "William J. Dally",
                "Kurt Keutzer"
            ]
        },
        {
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
            "arxiv": "1409.1556",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.1556v6",
            "abstract": "In this work we investigate the effect of the convolutional network depth on\nits accuracy in the large-scale image recognition setting. Our main\ncontribution is a thorough evaluation of networks of increasing depth using an\narchitecture with very small (3x3) convolution filters, which shows that a\nsignificant improvement on the prior-art configurations can be achieved by\npushing the depth to 16-19 weight layers. These findings were the basis of our\nImageNet Challenge 2014 submission, where our team secured the first and the\nsecond places in the localisation and classification tracks respectively. We\nalso show that our representations generalise well to other datasets, where\nthey achieve state-of-the-art results. We have made our two best-performing\nConvNet models publicly available to facilitate further research on the use of\ndeep visual representations in computer vision.",
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ]
        },
        {
            "title": "Residual Attention Network for Image Classification",
            "arxiv": "1704.06904",
            "year": 2017,
            "url": "http://arxiv.org/abs/1704.06904v1",
            "abstract": "In this work, we propose \"Residual Attention Network\", a convolutional neural\nnetwork using attention mechanism which can incorporate with state-of-art feed\nforward network architecture in an end-to-end training fashion. Our Residual\nAttention Network is built by stacking Attention Modules which generate\nattention-aware features. The attention-aware features from different modules\nchange adaptively as layers going deeper. Inside each Attention Module,\nbottom-up top-down feedforward structure is used to unfold the feedforward and\nfeedback attention process into a single feedforward process. Importantly, we\npropose attention residual learning to train very deep Residual Attention\nNetworks which can be easily scaled up to hundreds of layers. Extensive\nanalyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the\neffectiveness of every module mentioned above. Our Residual Attention Network\nachieves state-of-the-art object recognition performance on three benchmark\ndatasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and\nImageNet (4.8% single model and single crop, top-5 error). Note that, our\nmethod achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69%\nforward FLOPs comparing to ResNet-200. The experiment also demonstrates that\nour network is robust against noisy labels.",
            "authors": [
                "Fei Wang",
                "Mengqing Jiang",
                "Chen Qian",
                "Shuo Yang",
                "Cheng Li",
                "Honggang Zhang",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ]
        },
        {
            "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
            "arxiv": "1801.04381",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.04381v4",
            "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that\nimproves the state of the art performance of mobile models on multiple tasks\nand benchmarks as well as across a spectrum of different model sizes. We also\ndescribe efficient ways of applying these mobile models to object detection in\na novel framework we call SSDLite. Additionally, we demonstrate how to build\nmobile semantic segmentation models through a reduced form of DeepLabv3 which\nwe call Mobile DeepLabv3.\n  The MobileNetV2 architecture is based on an inverted residual structure where\nthe input and output of the residual block are thin bottleneck layers opposite\nto traditional residual models which use expanded representations in the input\nan MobileNetV2 uses lightweight depthwise convolutions to filter features in\nthe intermediate expansion layer. Additionally, we find that it is important to\nremove non-linearities in the narrow layers in order to maintain\nrepresentational power. We demonstrate that this improves performance and\nprovide an intuition that led to this design. Finally, our approach allows\ndecoupling of the input/output domains from the expressiveness of the\ntransformation, which provides a convenient framework for further analysis. We\nmeasure our performance on Imagenet classification, COCO object detection, VOC\nimage segmentation. We evaluate the trade-offs between accuracy, and number of\noperations measured by multiply-adds (MAdd), as well as the number of\nparameters",
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ]
        },
        {
            "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
            "arxiv": "1610.02357",
            "year": 2016,
            "url": "http://arxiv.org/abs/1610.02357v3",
            "abstract": "We present an interpretation of Inception modules in convolutional neural\nnetworks as being an intermediate step in-between regular convolution and the\ndepthwise separable convolution operation (a depthwise convolution followed by\na pointwise convolution). In this light, a depthwise separable convolution can\nbe understood as an Inception module with a maximally large number of towers.\nThis observation leads us to propose a novel deep convolutional neural network\narchitecture inspired by Inception, where Inception modules have been replaced\nwith depthwise separable convolutions. We show that this architecture, dubbed\nXception, slightly outperforms Inception V3 on the ImageNet dataset (which\nInception V3 was designed for), and significantly outperforms Inception V3 on a\nlarger image classification dataset comprising 350 million images and 17,000\nclasses. Since the Xception architecture has the same number of parameters as\nInception V3, the performance gains are not due to increased capacity but\nrather to a more efficient use of model parameters.",
            "authors": [
                "Fran\u00e7ois Chollet"
            ]
        },
        {
            "title": "Squeeze-and-Excitation Networks",
            "arxiv": "1709.01507",
            "year": 2017,
            "url": "http://arxiv.org/abs/1709.01507v4",
            "abstract": "The central building block of convolutional neural networks (CNNs) is the\nconvolution operator, which enables networks to construct informative features\nby fusing both spatial and channel-wise information within local receptive\nfields at each layer. A broad range of prior research has investigated the\nspatial component of this relationship, seeking to strengthen the\nrepresentational power of a CNN by enhancing the quality of spatial encodings\nthroughout its feature hierarchy. In this work, we focus instead on the channel\nrelationship and propose a novel architectural unit, which we term the\n\"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise\nfeature responses by explicitly modelling interdependencies between channels.\nWe show that these blocks can be stacked together to form SENet architectures\nthat generalise extremely effectively across different datasets. We further\ndemonstrate that SE blocks bring significant improvements in performance for\nexisting state-of-the-art CNNs at slight additional computational cost.\nSqueeze-and-Excitation Networks formed the foundation of our ILSVRC 2017\nclassification submission which won first place and reduced the top-5 error to\n2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%.\nModels and code are available at https://github.com/hujie-frank/SENet.",
            "authors": [
                "Jie Hu",
                "Li Shen",
                "Samuel Albanie",
                "Gang Sun",
                "Enhua Wu"
            ]
        },
        {
            "title": "PolyNet: A Pursuit of Structural Diversity in Very Deep Networks",
            "arxiv": "1611.05725",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.05725v2",
            "abstract": "A number of studies have shown that increasing the depth or width of\nconvolutional networks is a rewarding approach to improve the performance of\nimage recognition. In our study, however, we observed difficulties along both\ndirections. On one hand, the pursuit for very deep networks is met with a\ndiminishing return and increased training difficulty; on the other hand,\nwidening a network would result in a quadratic growth in both computational\ncost and memory demand. These difficulties motivate us to explore structural\ndiversity in designing deep networks, a new dimension beyond just depth and\nwidth. Specifically, we present a new family of modules, namely the\nPolyInception, which can be flexibly inserted in isolation or in a composition\nas replacements of different parts of a network. Choosing PolyInception modules\nwith the guidance of architectural efficiency can improve the expressive power\nwhile preserving comparable computational cost. The Very Deep PolyNet, designed\nfollowing this direction, demonstrates substantial improvements over the\nstate-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2,\nit reduces the top-5 validation error on single crops from 4.9% to 4.25%, and\nthat on multi-crops from 3.7% to 3.45%.",
            "authors": [
                "Xingcheng Zhang",
                "Zhizhong Li",
                "Chen Change Loy",
                "Dahua Lin"
            ]
        },
        {
            "title": "Densely Connected Convolutional Networks",
            "arxiv": "1608.06993",
            "year": 2016,
            "url": "http://arxiv.org/abs/1608.06993v5",
            "abstract": "Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high performance. Code and\npre-trained models are available at https://github.com/liuzhuang13/DenseNet .",
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens van der Maaten",
                "Kilian Q. Weinberger"
            ]
        },
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
            "arxiv": "1602.07261",
            "year": 2016,
            "url": "http://arxiv.org/abs/1602.07261v2",
            "abstract": "Very deep convolutional networks have been central to the largest advances in\nimage recognition performance in recent years. One example is the Inception\narchitecture that has been shown to achieve very good performance at relatively\nlow computational cost. Recently, the introduction of residual connections in\nconjunction with a more traditional architecture has yielded state-of-the-art\nperformance in the 2015 ILSVRC challenge; its performance was similar to the\nlatest generation Inception-v3 network. This raises the question of whether\nthere are any benefit in combining the Inception architecture with residual\nconnections. Here we give clear empirical evidence that training with residual\nconnections accelerates the training of Inception networks significantly. There\nis also some evidence of residual Inception networks outperforming similarly\nexpensive Inception networks without residual connections by a thin margin. We\nalso present several new streamlined architectures for both residual and\nnon-residual Inception networks. These variations improve the single-frame\nrecognition performance on the ILSVRC 2012 classification task significantly.\nWe further demonstrate how proper activation scaling stabilizes the training of\nvery wide residual Inception networks. With an ensemble of three residual and\none Inception-v4, we achieve 3.08 percent top-5 error on the test set of the\nImageNet classification (CLS) challenge",
            "authors": [
                "Christian Szegedy",
                "Sergey Ioffe",
                "Vincent Vanhoucke",
                "Alex Alemi"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999999988909996,
        "task": "Image Classification",
        "task_prob": 0.7463072139251994
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "COCO"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "SVHN"
            }
        ]
    }
}