{
    "visibility": {
        "visibility": "public"
    },
    "name": "Python Code Generation",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "akashe",
                "owner_type": "User",
                "name": "Python-Code-Generation",
                "url": "https://github.com/akashe/Python-Code-Generation",
                "stars": 1,
                "pushed_at": "2021-05-28 22:49:42+00:00",
                "created_at": "2021-03-01 19:33:28+00:00",
                "language": "Jupyter Notebook",
                "description": "Using language model with attention mechanisms to write python code from an English prompt.",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".idea",
                "sha": "0409a630fb800e9b7b1a3fbed2065453de70e0f1",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/tree/main/.idea"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "Conala_with_original_data.ipynb",
                "sha": "784163a01fd8c4965ea5f751db0268435769741a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/blob/main/Conala_with_original_data.ipynb"
                    }
                },
                "size": 75449
            },
            {
                "type": "code",
                "name": "Conala_with_original_data_with_python_embeddings.ipynb",
                "sha": "c112aa81c1c03e7a2d006d3bb2b59adf0162916c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/blob/main/Conala_with_original_data_with_python_embeddings.ipynb"
                    }
                },
                "size": 93831
            },
            {
                "type": "code",
                "name": "Embedding_layer_for_python.ipynb",
                "sha": "65be93d6dc153ed3fb555717d930f4cb17b34c4b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/blob/main/Embedding_layer_for_python.ipynb"
                    }
                },
                "size": 8485
            },
            {
                "type": "code",
                "name": "Notebook_with_model_code_improved_for_jit_script.ipynb",
                "sha": "47c5c542d86eabbf291374bdbdfdd831cf06e6fd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/blob/main/Notebook_with_model_code_improved_for_jit_script.ipynb"
                    }
                },
                "size": 86959
            },
            {
                "type": "code",
                "name": "Original_data_with_penalty.ipynb",
                "sha": "1cc5175940517c67a0bf383b9cf3a644b551817e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/blob/main/Original_data_with_penalty.ipynb"
                    }
                },
                "size": 759541
            },
            {
                "type": "code",
                "name": "Python_Embeddings_on_CoNaLa_mined_data.ipynb",
                "sha": "0096ac05c788f96f63a8a76b78b5bc08763b964a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/blob/main/Python_Embeddings_on_CoNaLa_mined_data.ipynb"
                    }
                },
                "size": 37112
            },
            {
                "type": "code",
                "name": "Vanilla_Enocder_Decoder_Architecture.ipynb",
                "sha": "2901c82ee2470f08ed82fcb94463985e7d65a72f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/blob/main/Vanilla_Enocder_Decoder_Architecture.ipynb"
                    }
                },
                "size": 62276
            },
            {
                "type": "code",
                "name": "Vanilla_Enocder_Decoder_Architecture_with_character_wise_decoder_vocab.ipynb",
                "sha": "853d16fa80319b7f63809540fd670279dca2b6d5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/blob/main/Vanilla_Enocder_Decoder_Architecture_with_character_wise_decoder_vocab.ipynb"
                    }
                },
                "size": 72919
            },
            {
                "type": "code",
                "name": "commands_for_torchserve_deployment.txt",
                "sha": "ba6a4f058606f0db3550a8cd8c89ada0e3946f1e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/blob/main/commands_for_torchserve_deployment.txt"
                    }
                },
                "size": 660
            },
            {
                "type": "code",
                "name": "custom_handler_for_deployment.py",
                "sha": "ac5bc7025ac5081139066487639fc59d4a9ff51d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/blob/main/custom_handler_for_deployment.py"
                    }
                },
                "size": 5951
            },
            {
                "type": "code",
                "name": "data",
                "sha": "b18f979c24c87f1ce045f4b5b76f8bfad5bfd7de",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/tree/main/data"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "data_processing.py",
                "sha": "a7574aa1d6b540258885bb9abacaa76222b0f7f7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/blob/main/data_processing.py"
                    }
                },
                "size": 9036
            },
            {
                "type": "code",
                "name": "model.py",
                "sha": "99bfb28e3e82edeac9c4103660b8a8fa6c179df1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/blob/main/model.py"
                    }
                },
                "size": 13376
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "ff684b460d9ad1be06cf319a8edbe674ba2e3f07",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/akashe/Python-Code-Generation/blob/main/test.py"
                    }
                },
                "size": 1403
            }
        ]
    },
    "authors": [
        {
            "name": "akashe",
            "github_id": "akashe"
        }
    ],
    "tags": [
        "big-code",
        "source-code-modelling",
        "source-code-generator",
        "python-source-code-generation",
        "program-generation",
        "english-to-python"
    ],
    "description": "Using language model with attention mechanisms to write python code from an English prompt.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/akashe/Python-Code-Generation",
            "stars": 1,
            "issues": true,
            "readme": "# Python Code Generation\n\n#### Observations:\nThe code in the repository doesn't create the right code for any possible python question. It is trained on a small data and creates a good program related to the questions present in the data. However, it still doesn't understand indentation properly for very long programs. After a ':' model correctly predicts the amount of indentation for the line but fails to capture which statements to keep in which indent specially for a very long program.\n\n\n#### Steps taken for data cleaning\n 1. Remove all comments \\# and '''...''': This is to reduce the vocab size and make the problem simpler for the model\n 2. Replace tabs with \"    \": This helps to keep same indentation scheme in the file. Specially for cases with indentation scheme as 4-3-2 spaces.\n 3. Replacing multiple line declarations of variables: We use python's own [tokenizer](https://docs.python.org/3/library/tokenize.html). It was creating problems with multiline declarations.  \n  \n\n#### Steps for data processing:\n1. Removing duplicate question answer pairs: Original data had many duplicate questions and python codes submitted by as same assignment by different team members. After removing duplicate pairs, the total unique question answer pair we about 3100 as compared to 4600+ original pairs.\n2. Tokenization: As said earlier, we used python's own tokenizer. There was a problem with it though. It took strings like the ones present in print('Akash Kumar') as a seperate string token 'Akash Kumar'. This unnecessarily increase vocab size. So tokenized these strings as characters to increase models verstality.\n3. Formatting with proper indentation: Data had multiple indentation schemes. We identify the indent required and finally replace it with corresponding '\\t' to keep sequence length smaller.\n\n#### Loss Function:\nPrimarily we used Cross Entropy as our loss function. We experimented with an [additional penalty](https://github.com/akashe/Python-Code-Generation/blob/main/Original_data_with_penalty.ipynb) for code that fails execution but:\n1. The new penalty based loss function make the training really slow because for each output in a batch we had to execute the script to get the result.\n2. Model didn't learn. Since there is no way for the parameters to find gradients wrt to actual execution of the scripts, we multiplied it as a separate constant to the loss value. This changes the gradient value and naturally it didn't work. But we tried to see if we can atleast have some rudimentary learning which we can adjust with the punishment_constant we chose as a hyperparameter.\n\n#### Python Embeddings:\nWe created [python embeddings](https://github.com/akashe/Python-Code-Generation/blob/main/Python_Embeddings_on_CoNaLa_mined_data.ipynb) using [CONALA mined dataset](https://conala-corpus.github.io/). The dataset consists of 590763 snippets of python. We train Decoder only transformer architecture and train it in an autoregressive manner. The task is simple to predict the next word given an input token. We train embeddings for a total of 15018 tokens which we got after using pythons in built tokenizer on the CONALA mined dataset. \n\n#### Data Expansion:\nIn addition to 3100 examples from the original data we add additional 2800 examples from conala-train and conala-test datasets. The datasets are of same format with a natural language prompt for a python code and the corresponding python snippet.\n\n#### Architecture:\nArchitecture is same as mentioned in the paper [\"Attention is all you need.\"](https://arxiv.org/abs/1706.03762). It's an encoder decoder model with the natural language prompt processed by the encoder and code generation by the decoder using multi-headed and self attention.\n\n#### Evaluation metrics:\nWe used Rouge-L metric which matches the longest subsequence. In code, there is a fixed structure with snippets following each other to build on previous snippets. In machine translation, the same words can come in the beginning and at the end to form the same meaning so n-grams based evaluation metric makes sense.\nSince in the code, n-grams presenting anywhere doesn't make sense, we chose [ROUGE-L metric](https://www.aclweb.org/anthology/P04-1077.pdf). It gives score according to matching of the longest common subsequence in target and output codes. We get a maximum of 15.8 ROUGE-L score on the validation set. Refer [this file](https://github.com/akashe/Python-Code-Generation/blob/main/Conala_with_original_data_with_python_embeddings.ipynb) for code.\n\n#### Experiments:\nHere are the different experiments we did, and their corresponding files.\n\n1. [Vanilla encoder-decoder architecture with word vocab](https://github.com/akashe/Python-Code-Generation/blob/main/Vanilla_Enocder_Decoder_Architecture.ipynb):\n    The first experiment with a simple encoder-decoder architecture with python tokenizer and no pretrained embeddings.\n\n2. [Vanilla encoder-decoder architecture with char vocab](https://github.com/akashe/Python-Code-Generation/blob/main/Vanilla_Enocder_Decoder_Architecture_with_character_wise_decoder_vocab.ipynb):\n    In this file, we do similar things as above we just used a char vocab for the decoder. We realized, that decoder outputs didn't have space between statements like 'def gdc(x,y)'\n   \n3. [Penalty for wrong execution](https://github.com/akashe/Python-Code-Generation/blob/main/Original_data_with_penalty.ipynb):\n   As discussed earlier, the model didn't train well with an extra execution penalty because it deflected gradients from their true direction. \n\n4. [Training python embeddings](https://github.com/akashe/Python-Code-Generation/blob/main/Python_Embeddings_on_CoNaLa_mined_data.ipynb):\n    In this file, we trained decoder embeddings for python tokens using 590763 instances of mined data in conala-mined dataset. The embeddings along with their corresponding vocab are present in the data folder.\n\n5. [Conala data with original data](https://github.com/akashe/Python-Code-Generation/blob/main/Conala_with_original_data.ipynb):\n    Similar to details in 1. Here, we trained our model on more data from conala train and test files from CONALA dataset.    \n\n6. [Conala data with original with python Embeddings](https://github.com/akashe/Python-Code-Generation/blob/main/Conala_with_original_data_with_python_embeddings.ipynb):\n    We use this file to report our results. We train on total 5937 unique question answer pairs along with the pretrained embeddings we got from 4.\n   \n#### Example Outputs:\nCheck the [file here for example outputs](https://github.com/akashe/Python-Code-Generation/blob/main/data/example_output.txt) for better formatting. Refer [this file](https://github.com/akashe/Python-Code-Generation/blob/main/Conala_with_original_data_with_python_embeddings.ipynb) for code.\n\n#### Attention graphs between text and python code \n\nReference paper:\nhttps://arxiv.org/pdf/2002.05442.pdf\n",
            "readme_url": "https://github.com/akashe/Python-Code-Generation",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Attention Is All You Need",
            "arxiv": "1706.03762",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.03762v5",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ]
        },
        {
            "title": "Deep Learning for Source Code Modeling and Generation: Models, Applications and Challenges",
            "arxiv": "2002.05442",
            "year": 2020,
            "url": "http://arxiv.org/abs/2002.05442v1",
            "abstract": "Deep Learning (DL) techniques for Natural Language Processing have been\nevolving remarkably fast. Recently, the DL advances in language modeling,\nmachine translation and paragraph understanding are so prominent that the\npotential of DL in Software Engineering cannot be overlooked, especially in the\nfield of program learning. To facilitate further research and applications of\nDL in this field, we provide a comprehensive review to categorize and\ninvestigate existing DL methods for source code modeling and generation. To\naddress the limitations of the traditional source code models, we formulate\ncommon program learning tasks under an encoder-decoder framework. After that,\nwe introduce recent DL mechanisms suitable to solve such problems. Then, we\npresent the state-of-the-art practices and discuss their challenges with some\nrecommendations for practitioners and researchers as well.",
            "authors": [
                "Triet H. M. Le",
                "Hao Chen",
                "M. Ali Babar"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "CONALA mined dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://conala-corpus.github.io/"
                    }
                }
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999238308265963,
        "task": "Machine Translation",
        "task_prob": 0.9703768562646037
    }
}