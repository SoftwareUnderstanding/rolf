{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "Deepvoice3_pytorch",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "r9y9",
                "owner_type": "User",
                "name": "deepvoice3_pytorch",
                "url": "https://github.com/r9y9/deepvoice3_pytorch",
                "stars": 1689,
                "pushed_at": "2020-10-31 16:15:53+00:00",
                "created_at": "2017-10-31 12:31:44+00:00",
                "language": "Python",
                "description": "PyTorch implementation of convolutional neural networks-based text-to-speech synthesis models",
                "license": "Other",
                "frameworks": [
                    "NLTK",
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "0251b6739429f6bd6c98c39ceac0b127dc2fed9c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/tree/master/.github"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "720d69576a6d8a67e5c52c045d638f1e532681ba",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/.gitignore"
                    }
                },
                "size": 2409
            },
            {
                "type": "code",
                "name": ".gitmodules",
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/.gitmodules"
                    }
                },
                "size": 0
            },
            {
                "type": "code",
                "name": ".travis.yml",
                "sha": "3fdea81de8133bc9ef370aeb9401c5542e5ca988",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/.travis.yml"
                    }
                },
                "size": 1319
            },
            {
                "type": "code",
                "name": "LICENSE.md",
                "sha": "0f387640d5a6ea85a601d1f6196925ea07b2f97e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/LICENSE.md"
                    }
                },
                "size": 1329
            },
            {
                "type": "code",
                "name": "MANIFEST.in",
                "sha": "83fcc2c6cc2555e67a5f56a835c06d6c6d2890da",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/MANIFEST.in"
                    }
                },
                "size": 29
            },
            {
                "type": "code",
                "name": "appveyor.yml",
                "sha": "8a79977d40d19dfc19d367cde58b54aa89a1693b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/appveyor.yml"
                    }
                },
                "size": 852
            },
            {
                "type": "code",
                "name": "assets",
                "sha": "57ad8cc5c56aa365155ff87ddcd1c240a95bf91c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/tree/master/assets"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "audio.py",
                "sha": "d7749ed34225c99d8b779264afd2ac4af7fad1fb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/audio.py"
                    }
                },
                "size": 2445
            },
            {
                "type": "code",
                "name": "compute_timestamp_ratio.py",
                "sha": "a91f6437fe53d82657bb6e0f847179fe0579610d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/compute_timestamp_ratio.py"
                    }
                },
                "size": 1674
            },
            {
                "type": "code",
                "name": "deepvoice3_pytorch",
                "sha": "fd7c7444b976d2c37c40a9ca284d18e4677d9632",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/tree/master/deepvoice3_pytorch"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "e9abb5cf5a45c027d1f1888eeadafdb399b62fba",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/tree/master/docs"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "dump_hparams_to_json.py",
                "sha": "d67e88d3e864fdff9536acf19cc9909e989c0c71",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/dump_hparams_to_json.py"
                    }
                },
                "size": 725
            },
            {
                "type": "code",
                "name": "gentle_web_align.py",
                "sha": "856deec6de7d16af2586679f5b713b53a6c7a722",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/gentle_web_align.py"
                    }
                },
                "size": 6164
            },
            {
                "type": "code",
                "name": "hparams.py",
                "sha": "70a439c42abc2a4efe44ee49e9a30d4670d0f302",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/hparams.py"
                    }
                },
                "size": 5022
            },
            {
                "type": "code",
                "name": "json_meta.py",
                "sha": "2e654e9a5c72b6c9b85f970e94fc949f92d99bdf",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/json_meta.py"
                    }
                },
                "size": 9377
            },
            {
                "type": "code",
                "name": "jsut.py",
                "sha": "f206cb786f496369483b231b46f0c5983d93c4f1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/jsut.py"
                    }
                },
                "size": 2281
            },
            {
                "type": "code",
                "name": "ljspeech.py",
                "sha": "8f68a8058118b97f72d3c76858e4a9882350cdc3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/ljspeech.py"
                    }
                },
                "size": 3013
            },
            {
                "type": "code",
                "name": "lrschedule.py",
                "sha": "fb556118ddd4b4a907dd605b4696ee08b4d239bf",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/lrschedule.py"
                    }
                },
                "size": 1119
            },
            {
                "type": "code",
                "name": "nikl_m.py",
                "sha": "7c5149878ada36da2c36520464955fbbf071a67c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/nikl_m.py"
                    }
                },
                "size": 3408
            },
            {
                "type": "code",
                "name": "nikl_preprocess",
                "sha": "60dc56830bdab40e4d6c3ddda87e037b71f571d1",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/tree/master/nikl_preprocess"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "nikl_s.py",
                "sha": "6b1830f730a5aa808afc2cb07fba66a5938c4e51",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/nikl_s.py"
                    }
                },
                "size": 3256
            },
            {
                "type": "code",
                "name": "preprocess.py",
                "sha": "d76de83ff0232dff83109eb514c92dc350bce42c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/preprocess.py"
                    }
                },
                "size": 2022
            },
            {
                "type": "code",
                "name": "presets",
                "sha": "279bfef4624cdaf7afd5f286ceba183c39564687",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/tree/master/presets"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "release.sh",
                "sha": "1bedfbfd3168a2ff243a419108afc7449e47a574",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/release.sh"
                    }
                },
                "size": 632
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "3203c866477fac5132a0f629f28fb262663594ef",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/setup.py"
                    }
                },
                "size": 2769
            },
            {
                "type": "code",
                "name": "synthesis.py",
                "sha": "1cd55ec5ce1a44b158899de08ad413f0d68d43f0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/synthesis.py"
                    }
                },
                "size": 6207
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "b2698162040a2ed5da4b3c9d5dfe1a5fc86d6fcb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/tree/master/tests"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "tox.ini",
                "sha": "2de9af465e9fbaf781b3ec917dc8ce19ea169012",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/tox.ini"
                    }
                },
                "size": 184
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "6781a730e3acad611bb550490f9364f0cef327f1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/train.py"
                    }
                },
                "size": 38196
            },
            {
                "type": "code",
                "name": "vctk.py",
                "sha": "5fd9ff496aca793d3c0ebe5f5d212052d56b1128",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/vctk.py"
                    }
                },
                "size": 2864
            },
            {
                "type": "code",
                "name": "vctk_preprocess",
                "sha": "b9f35db597476d9acaf59d5398f8ee098d367254",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/r9y9/deepvoice3_pytorch/tree/master/vctk_preprocess"
                    }
                },
                "num_files": 5
            }
        ]
    },
    "authors": [
        {
            "name": "Ryuichi Yamamoto",
            "email": "zryuichi@gmail.com",
            "github_id": "r9y9"
        },
        {
            "name": "Hyeongkeun Kim",
            "email": "engiecat3.14@gmail.com",
            "github_id": "engiecat"
        },
        {
            "name": "Ilya Lasy",
            "github_id": "Misterion777"
        },
        {
            "name": "Kevron Rees",
            "email": "kevron.m.rees@intel.com",
            "github_id": "tripzero"
        },
        {
            "name": "kokimame",
            "github_id": "kokimame"
        },
        {
            "name": "lzala",
            "github_id": "lzala"
        },
        {
            "name": "Ra\u00fal",
            "github_id": "jraulhernandezi"
        },
        {
            "name": "Steve Fischer",
            "github_id": "sjfischr"
        },
        {
            "name": "Amila Rathnayake ",
            "email": "amilamad@gmail.com",
            "github_id": "amilamad"
        },
        {
            "name": "gisforgirard",
            "github_id": "gisforgirard"
        },
        {
            "name": "homink",
            "github_id": "homink"
        }
    ],
    "tags": [
        "tts",
        "speech-synthesis",
        "end-to-end",
        "speech-processing",
        "machine-learning",
        "pytorch",
        "python",
        "multi-speaker"
    ],
    "description": "PyTorch implementation of convolutional neural networks-based text-to-speech synthesis models",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/r9y9/deepvoice3_pytorch",
            "stars": 1689,
            "issues": true,
            "readme": "![alt text](assets/banner.jpg)\n\n# Deepvoice3_pytorch\n\n[![PyPI](https://img.shields.io/pypi/v/deepvoice3_pytorch.svg)](https://pypi.python.org/pypi/deepvoice3_pytorch)\n[![Build Status](https://travis-ci.org/r9y9/deepvoice3_pytorch.svg?branch=master)](https://travis-ci.org/r9y9/deepvoice3_pytorch)\n[![Build status](https://ci.appveyor.com/api/projects/status/8eurjakfaofbr24k?svg=true)](https://ci.appveyor.com/project/r9y9/deepvoice3-pytorch)\n[![DOI](https://zenodo.org/badge/108992863.svg)](https://zenodo.org/badge/latestdoi/108992863)\n\nPyTorch implementation of convolutional networks-based text-to-speech synthesis models:\n\n1. [arXiv:1710.07654](https://arxiv.org/abs/1710.07654): Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning.\n2. [arXiv:1710.08969](https://arxiv.org/abs/1710.08969): Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention.\n\nAudio samples are available at https://r9y9.github.io/deepvoice3_pytorch/.\n\n## Folks\n\n- https://github.com/hash2430/dv3_world: DeepVoice3 with WORLD vocoder support. [#166](https://github.com/r9y9/deepvoice3_pytorch/issues/166)\n\n## Online TTS demo\n\nNotebooks supposed to be executed on https://colab.research.google.com are available:\n\n- [DeepVoice3: Multi-speaker text-to-speech demo](https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/DeepVoice3_multi_speaker_TTS_en_demo.ipynb)\n- [DeepVoice3: Single-speaker text-to-speech demo](https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/DeepVoice3_single_speaker_TTS_en_demo.ipynb)\n\n## Highlights\n\n- Convolutional sequence-to-sequence model with attention for text-to-speech synthesis\n- Multi-speaker and single speaker versions of DeepVoice3\n- Audio samples and pre-trained models\n- Preprocessor for [LJSpeech (en)](https://keithito.com/LJ-Speech-Dataset/), [JSUT (jp)](https://sites.google.com/site/shinnosuketakamichi/publication/jsut) and [VCTK](http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html) datasets, as well as [carpedm20/multi-speaker-tacotron-tensorflow](https://github.com/carpedm20/multi-Speaker-tacotron-tensorflow) compatible custom dataset (in JSON format)\n- Language-dependent frontend text processor for English and Japanese\n\n### Samples\n\n- [Ja Step000380000 Predicted](https://soundcloud.com/user-623907374/ja-step000380000-predicted)\n- [Ja Step000370000 Predicted](https://soundcloud.com/user-623907374/ja-step000370000-predicted)\n- [Ko_single Step000410000 Predicted](https://soundcloud.com/user-623907374/ko-step000410000-predicted)\n- [Ko_single Step000400000 Predicted](https://soundcloud.com/user-623907374/ko-step000400000-predicted)\n- [Ko_multi Step001680000 Predicted](https://soundcloud.com/user-623907374/step001680000-predicted)\n- [Ko_multi Step001700000 Predicted](https://soundcloud.com/user-623907374/step001700000-predicted)\n\n## Pretrained models\n\n**NOTE**: pretrained models are not compatible to master. To be updated soon.\n\n | URL | Model      | Data     | Hyper paramters                                  | Git commit | Steps  |\n |-----|------------|----------|--------------------------------------------------|----------------------|--------|\n | [link](https://www.dropbox.com/s/5ucl9remrwy5oeg/20180505_deepvoice3_checkpoint_step000640000.pth?dl=0) | DeepVoice3 | LJSpeech | [link](https://www.dropbox.com/s/0ck82unm0bo0rxd/20180505_deepvoice3_ljspeech.json?dl=0) | [abf0a21](https://github.com/r9y9/deepvoice3_pytorch/tree/abf0a21f83aeb451b918f867bc23378f1e2e608b)| 640k |\n |  [link](https://www.dropbox.com/s/1y8bt6bnggbzzlp/20171129_nyanko_checkpoint_step000585000.pth?dl=0)   | Nyanko     | LJSpeech | `builder=nyanko,preset=nyanko_ljspeech`     | [ba59dc7](https://github.com/r9y9/deepvoice3_pytorch/tree/ba59dc75374ca3189281f6028201c15066830116) | 585k |\n  |  [link](https://www.dropbox.com/s/uzmtzgcedyu531k/20171222_deepvoice3_vctk108_checkpoint_step000300000.pth?dl=0)   | Multi-speaker DeepVoice3     | VCTK | `builder=deepvoice3_multispeaker,preset=deepvoice3_vctk`     | [0421749](https://github.com/r9y9/deepvoice3_pytorch/tree/0421749af908905d181f089f06956fddd0982d47) | 300k + 300k |\n\nTo use pre-trained models, it's highly recommended that you are on the **specific git commit** noted above. i.e.,\n\n```\ngit checkout ${commit_hash}\n```\n\nThen follow the \"Synthesize from a checkpoint\" section in the README of the specific git commit. Please notice that the latest development version of the repository may not work.\n\nYou could try for example:\n\n```\n# pretrained model (20180505_deepvoice3_checkpoint_step000640000.pth)\n# hparams (20180505_deepvoice3_ljspeech.json)\ngit checkout 4357976\npython synthesis.py --preset=20180505_deepvoice3_ljspeech.json \\\n  20180505_deepvoice3_checkpoint_step000640000.pth \\\n  sentences.txt \\\n  output_dir\n```\n\n## Notes on hyper parameters\n\n- Default hyper parameters, used during preprocessing/training/synthesis stages, are turned for English TTS using LJSpeech dataset. You will have to change some of parameters if you want to try other datasets. See `hparams.py` for details.\n- `builder` specifies which model you want to use. `deepvoice3`, `deepvoice3_multispeaker` [1] and `nyanko` [2] are surpprted.\n- Hyper parameters described in DeepVoice3 paper for single speaker didn't work for LJSpeech dataset, so I changed a few things. Add dilated convolution, more channels, more layers and add guided attention loss, etc. See code for details. The changes are also applied for multi-speaker model.\n- Multiple attention layers are hard to learn. Empirically, one or two (first and last) attention layers seems enough.\n- With guided attention (see https://arxiv.org/abs/1710.08969), alignments get monotonic more quickly and reliably if we use multiple attention layers. With guided attention, I can confirm five attention layers get monotonic, though I cannot get speech quality improvements.\n- Binary divergence (described in https://arxiv.org/abs/1710.08969) seems stabilizes training particularly for deep (> 10 layers) networks.\n- Adam with step lr decay works. However, for deeper networks, I find Adam + noam's lr scheduler is more stable.\n\n## Requirements\n\n- Python >= 3.5\n- CUDA >= 8.0\n- PyTorch >= v1.0.0\n- [nnmnkwii](https://github.com/r9y9/nnmnkwii) >= v0.0.11\n- [MeCab](http://taku910.github.io/mecab/) (Japanese only)\n\n## Installation\n\nPlease install packages listed above first, and then\n\n```\ngit clone https://github.com/r9y9/deepvoice3_pytorch && cd deepvoice3_pytorch\npip install -e \".[bin]\"\n```\n\n## Getting started\n\n### Preset parameters\n\nThere are many hyper parameters to be turned depends on what model and data you are working on. For typical datasets and models, parameters that known to work good (**preset**) are provided in the repository. See `presets` directory for details. Notice that\n\n1. `preprocess.py`\n2. `train.py`\n3. `synthesis.py`\n\naccepts `--preset=<json>` optional parameter, which specifies where to load preset parameters. If you are going to use preset parameters, then you must use same `--preset=<json>` throughout preprocessing, training and evaluation. e.g.,\n\n```\npython preprocess.py --preset=presets/deepvoice3_ljspeech.json ljspeech ~/data/LJSpeech-1.0\npython train.py --preset=presets/deepvoice3_ljspeech.json --data-root=./data/ljspeech\n```\n\ninstead of\n\n```\npython preprocess.py ljspeech ~/data/LJSpeech-1.0\n# warning! this may use different hyper parameters used at preprocessing stage\npython train.py --preset=presets/deepvoice3_ljspeech.json --data-root=./data/ljspeech\n```\n\n### 0. Download dataset\n\n- LJSpeech (en): https://keithito.com/LJ-Speech-Dataset/\n- VCTK (en): http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html\n- JSUT (jp): https://sites.google.com/site/shinnosuketakamichi/publication/jsut\n- NIKL (ko) (**Need korean cellphone number to access it**): http://www.korean.go.kr/front/board/boardStandardView.do?board_id=4&mn_id=17&b_seq=464\n\n### 1. Preprocessing\n\nUsage:\n\n```\npython preprocess.py ${dataset_name} ${dataset_path} ${out_dir} --preset=<json>\n```\n\nSupported `${dataset_name}`s are:\n\n- `ljspeech` (en, single speaker)\n- `vctk` (en, multi-speaker)\n- `jsut` (jp, single speaker)\n- `nikl_m` (ko, multi-speaker)\n- `nikl_s` (ko, single speaker)\n\nAssuming you use preset parameters known to work good for LJSpeech dataset / DeepVoice3 and have data in `~/data/LJSpeech-1.0`, then you can preprocess data by:\n\n```\npython preprocess.py --preset=presets/deepvoice3_ljspeech.json ljspeech ~/data/LJSpeech-1.0/ ./data/ljspeech\n```\n\nWhen this is done, you will see extracted features (mel-spectrograms and linear spectrograms) in `./data/ljspeech`.\n\n#### 1-1. Building custom dataset. (using json_meta)\nBuilding your own dataset, with metadata in JSON format (compatible with [carpedm20/multi-speaker-tacotron-tensorflow](https://github.com/carpedm20/multi-Speaker-tacotron-tensorflow)) is currently supported.\nUsage:\n\n```\npython preprocess.py json_meta ${list-of-JSON-metadata-paths} ${out_dir} --preset=<json>\n```\nYou may need to modify pre-existing preset JSON file, especially `n_speakers`. For english multispeaker, start with `presets/deepvoice3_vctk.json`.\n\nAssuming you have dataset A (Speaker A) and dataset B (Speaker B), each described in the JSON metadata file `./datasets/datasetA/alignment.json` and `./datasets/datasetB/alignment.json`, then you can preprocess  data by:\n\n```\npython preprocess.py json_meta \"./datasets/datasetA/alignment.json,./datasets/datasetB/alignment.json\" \"./datasets/processed_A+B\" --preset=(path to preset json file)\n```\n\n#### 1-2. Preprocessing custom english datasets with long silence. (Based on [vctk_preprocess](vctk_preprocess/))\n\nSome dataset, especially automatically generated dataset may include long silence and undesirable leading/trailing noises, undermining the char-level seq2seq model.\n(e.g. VCTK, although this is covered in vctk_preprocess)\n\nTo deal with the problem, `gentle_web_align.py` will\n- **Prepare phoneme alignments for all utterances**\n- Cut silences during preprocessing\n\n`gentle_web_align.py` uses [Gentle](https://github.com/lowerquality/gentle), a kaldi based speech-text alignment tool. This accesses web-served Gentle application, aligns given sound segments with transcripts and converts the result to HTK-style label files, to be processed in `preprocess.py`. Gentle can be run in Linux/Mac/Windows(via Docker).\n\nPreliminary results show that while HTK/festival/merlin-based method in `vctk_preprocess/prepare_vctk_labels.py` works better on VCTK, Gentle is more stable with audio clips with ambient noise. (e.g. movie excerpts)\n\nUsage:\n(Assuming Gentle is running at `localhost:8567` (Default when not specified))\n1. When sound file and transcript files are saved in separate folders. (e.g. sound files are at `datasetA/wavs` and transcripts are at `datasetA/txts`)\n```\npython gentle_web_align.py -w \"datasetA/wavs/*.wav\" -t \"datasetA/txts/*.txt\" --server_addr=localhost --port=8567\n```\n\n2. When sound file and transcript files are saved in nested structure. (e.g. `datasetB/speakerN/blahblah.wav` and `datasetB/speakerN/blahblah.txt`)\n```\npython gentle_web_align.py --nested-directories=\"datasetB\" --server_addr=localhost --port=8567\n```\n**Once you have phoneme alignment for each utterance, you can extract features by running `preprocess.py`**\n\n### 2. Training\n\nUsage:\n\n```\npython train.py --data-root=${data-root} --preset=<json> --hparams=\"parameters you may want to override\"\n```\n\nSuppose you build a DeepVoice3-style model using LJSpeech dataset, then you can train your model by:\n\n```\npython train.py --preset=presets/deepvoice3_ljspeech.json --data-root=./data/ljspeech/\n```\n\nModel checkpoints (.pth) and alignments (.png) are saved in `./checkpoints` directory per 10000 steps by default.\n\n#### NIKL\n\nPleae check [this](https://github.com/homink/deepvoice3_pytorch/blob/master/nikl_preprocess/README.md) in advance and follow the commands below.\n\n```\npython preprocess.py nikl_s ${your_nikl_root_path} data/nikl_s --preset=presets/deepvoice3_nikls.json\n\npython train.py --data-root=./data/nikl_s --checkpoint-dir checkpoint_nikl_s --preset=presets/deepvoice3_nikls.json\n```\n\n### 4. Monitor with Tensorboard\n\nLogs are dumped in `./log` directory by default. You can monitor logs by tensorboard:\n\n```\ntensorboard --logdir=log\n```\n\n### 5. Synthesize from a checkpoint\n\nGiven a list of text, `synthesis.py` synthesize audio signals from trained model. Usage is:\n\n```\npython synthesis.py ${checkpoint_path} ${text_list.txt} ${output_dir} --preset=<json>\n```\n\nExample test_list.txt:\n\n```\nGenerative adversarial network or variational auto-encoder.\nOnce upon a time there was a dear little girl who was loved by every one who looked at her, but most of all by her grandmother, and there was nothing that she would not have given to the child.\nA text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module.\n```\n\n## Advanced usage\n\n### Multi-speaker model\n\nVCTK and NIKL are supported dataset for building a multi-speaker model.\n\n#### VCTK\nSince some audio samples in VCTK have long silences that affect performance, it's recommended to do phoneme alignment and remove silences according to [vctk_preprocess](vctk_preprocess/).\n\nOnce you have phoneme alignment for each utterance, you can extract features by:\n\n```\npython preprocess.py vctk ${your_vctk_root_path} ./data/vctk\n```\n\nNow that you have data prepared, then you can train a multi-speaker version of DeepVoice3 by:\n\n```\npython train.py --data-root=./data/vctk --checkpoint-dir=checkpoints_vctk \\\n   --preset=presets/deepvoice3_vctk.json \\\n   --log-event-path=log/deepvoice3_multispeaker_vctk_preset\n```\n\nIf you want to reuse learned embedding from other dataset, then you can do this instead by:\n\n```\npython train.py --data-root=./data/vctk --checkpoint-dir=checkpoints_vctk \\\n   --preset=presets/deepvoice3_vctk.json \\\n   --log-event-path=log/deepvoice3_multispeaker_vctk_preset \\\n   --load-embedding=20171213_deepvoice3_checkpoint_step000210000.pth\n```\n\nThis may improve training speed a bit.\n\n#### NIKL\n\nYou will be able to obtain cleaned-up audio samples in ../nikl_preprocoess. Details are found in [here](https://github.com/homink/speech.ko).\n\n\nOnce NIKL corpus is ready to use from the preprocessing, you can extract features by:\n\n```\npython preprocess.py nikl_m ${your_nikl_root_path} data/nikl_m\n```\n\nNow that you have data prepared, then you can train a multi-speaker version of DeepVoice3 by:\n\n```\npython train.py --data-root=./data/nikl_m  --checkpoint-dir checkpoint_nikl_m \\\n   --preset=presets/deepvoice3_niklm.json\n```\n\n### Speaker adaptation\n\nIf you have very limited data, then you can consider to try fine-turn pre-trained model. For example, using pre-trained model on LJSpeech, you can adapt it to data from VCTK speaker `p225` (30 mins) by the following command:\n\n```\npython train.py --data-root=./data/vctk --checkpoint-dir=checkpoints_vctk_adaptation \\\n    --preset=presets/deepvoice3_ljspeech.json \\\n    --log-event-path=log/deepvoice3_vctk_adaptation \\\n    --restore-parts=\"20171213_deepvoice3_checkpoint_step000210000.pth\"\n    --speaker-id=0\n```\n\nFrom my experience, it can get reasonable speech quality very quickly rather than training the model from scratch.\n\nThere are two important options used above:\n\n- `--restore-parts=<N>`: It specifies where to load model parameters. The differences from the option `--checkpoint=<N>` are 1) `--restore-parts=<N>` ignores all invalid parameters, while `--checkpoint=<N>` doesn't. 2) `--restore-parts=<N>` tell trainer to start from 0-step, while `--checkpoint=<N>` tell trainer to continue from last step. `--checkpoint=<N>` should be ok if you are using exactly same model and continue to train, but it would be useful if you want to customize your model architecture and take advantages of pre-trained model.\n- `--speaker-id=<N>`: It specifies what speaker of data is used for training. This should only be specified if you are using multi-speaker dataset. As for VCTK, speaker id is automatically assigned incrementally (0, 1, ..., 107) according to the `speaker_info.txt` in the dataset.\n\nIf you are training multi-speaker model, speaker adaptation will only work **when `n_speakers` is identical**.\n\n## Trouble shooting\n\n### [#5](https://github.com/r9y9/deepvoice3_pytorch/issues/5) RuntimeError: main thread is not in main loop\n\n\nThis may happen depending on backends you have for matplotlib. Try changing backend for matplotlib and see if it works as follows:\n\n```\nMPLBACKEND=Qt5Agg python train.py ${args...}\n```\n\nIn [#78](https://github.com/r9y9/deepvoice3_pytorch/pull/78#issuecomment-385327057), engiecat reported that changing the backend of matplotlib from Tkinter(TkAgg) to PyQt5(Qt5Agg) fixed the problem.\n\n## Sponsers\n\n- https://github.com/echelon\n\n## Acknowledgements\n\nPart of code was adapted from the following projects:\n\n- https://github.com/keithito/tacotron\n- https://github.com/facebookresearch/fairseq-py\n\nBanner and logo created by [@jraulhernandezi](https://github.com/jraulhernandezi) ([#76](https://github.com/r9y9/deepvoice3_pytorch/issues/76))\n",
            "readme_url": "https://github.com/r9y9/deepvoice3_pytorch",
            "frameworks": [
                "NLTK",
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention",
            "arxiv": "1710.08969",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.08969v2",
            "abstract": "This paper describes a novel text-to-speech (TTS) technique based on deep\nconvolutional neural networks (CNN), without use of any recurrent units.\nRecurrent neural networks (RNN) have become a standard technique to model\nsequential data recently, and this technique has been used in some cutting-edge\nneural TTS techniques. However, training RNN components often requires a very\npowerful computer, or a very long time, typically several days or weeks. Recent\nother studies, on the other hand, have shown that CNN-based sequence synthesis\ncan be much faster than RNN-based techniques, because of high\nparallelizability. The objective of this paper is to show that an alternative\nneural TTS based only on CNN alleviate these economic costs of training. In our\nexperiment, the proposed Deep Convolutional TTS was sufficiently trained\novernight (15 hours), using an ordinary gaming PC equipped with two GPUs, while\nthe quality of the synthesized speech was almost acceptable.",
            "authors": [
                "Hideyuki Tachibana",
                "Katsuya Uenoyama",
                "Shunsuke Aihara"
            ]
        },
        {
            "title": "Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning",
            "arxiv": "1710.07654",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.07654v3",
            "abstract": "We present Deep Voice 3, a fully-convolutional attention-based neural\ntext-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural\nspeech synthesis systems in naturalness while training ten times faster. We\nscale Deep Voice 3 to data set sizes unprecedented for TTS, training on more\nthan eight hundred hours of audio from over two thousand speakers. In addition,\nwe identify common error modes of attention-based speech synthesis networks,\ndemonstrate how to mitigate them, and compare several different waveform\nsynthesis methods. We also describe how to scale inference to ten million\nqueries per day on one single-GPU server.",
            "authors": [
                "Wei Ping",
                "Kainan Peng",
                "Andrew Gibiansky",
                "Sercan O. Arik",
                "Ajay Kannan",
                "Sharan Narang",
                "Jonathan Raiman",
                "John Miller"
            ]
        }
    ],
    "domain": {
        "domain_type": "Speech",
        "domain_prob": 0.9819022527517391
    }
}