{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "RLax",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "deepmind",
                "owner_type": "Organization",
                "name": "rlax",
                "url": "https://github.com/deepmind/rlax",
                "stars": 767,
                "pushed_at": "2022-03-21 10:05:50+00:00",
                "created_at": "2020-02-18 07:14:59+00:00",
                "language": "Python",
                "license": "Apache License 2.0",
                "frameworks": []
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "22da15e2a4dcd3fd89b599c670c5cb45ca0741af",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/deepmind/rlax/tree/master/.github"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "c815ebbbca4a9b9c277801a203f54fa0522f3450",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/deepmind/rlax/blob/master/.gitignore"
                    }
                },
                "size": 177
            },
            {
                "type": "code",
                "name": ".pylintrc",
                "sha": "f744d2009c290ca103da0e57d0b8f1444b594312",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/deepmind/rlax/blob/master/.pylintrc"
                    }
                },
                "size": 14359
            },
            {
                "type": "code",
                "name": ".readthedocs.yaml",
                "sha": "f003f390ba927553f4f5329aca7dadfff2a9b701",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/deepmind/rlax/blob/master/.readthedocs.yaml"
                    }
                },
                "size": 382
            },
            {
                "type": "code",
                "name": "CONTRIBUTING.md",
                "sha": "b3ca14a5e841c42037cd7e59d16b55dbbdcf7535",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/deepmind/rlax/blob/master/CONTRIBUTING.md"
                    }
                },
                "size": 1322
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "d645695673349e3947e8e5ae42332d0ac3164cd7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/deepmind/rlax/blob/master/LICENSE"
                    }
                },
                "size": 11358
            },
            {
                "type": "code",
                "name": "MANIFEST.in",
                "sha": "763a1b6881229053c74bd8d2a8a49e405bffd576",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/deepmind/rlax/blob/master/MANIFEST.in"
                    }
                },
                "size": 79
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "59009d426677b9daa1bb8e5ebc28b8d361b805e9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/deepmind/rlax/tree/master/docs"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "examples",
                "sha": "ee56cdee943c5cb838ad4fdc98cfd8cc4df71622",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/deepmind/rlax/tree/master/examples"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "requirements",
                "sha": "67253a1e424c9b24145767e5960fcece8892b882",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/deepmind/rlax/tree/master/requirements"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "rlax",
                "sha": "01d9eec14e3e7602f878215e66406d3c1cace3a1",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/deepmind/rlax/tree/master/rlax"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "37737927458b3977e681819b958b1a2a9aa452ff",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/deepmind/rlax/blob/master/setup.py"
                    }
                },
                "size": 2772
            },
            {
                "type": "code",
                "name": "test.sh",
                "sha": "efc29a0df39fc02a040de8caf59e14e2aba76acb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/deepmind/rlax/blob/master/test.sh"
                    }
                },
                "size": 2413
            }
        ]
    },
    "authors": [
        {
            "name": "Kate Baumli",
            "email": "baumli@google.com",
            "github_id": "katebaumli"
        },
        {
            "name": "Matteo Hessel",
            "github_id": "mtthss"
        },
        {
            "name": "Iurii Kemaev",
            "github_id": "hbq1"
        },
        {
            "name": "John Q",
            "github_id": "jqdm"
        },
        {
            "name": "dbudden",
            "github_id": "dbudden"
        },
        {
            "name": "Surya Bhupatiraju",
            "email": "surya95@gmail.com",
            "github_id": "suryabhupa"
        },
        {
            "name": "akssri",
            "github_id": "akssri"
        },
        {
            "name": "Chris Hoyean Song",
            "email": "sjhshy@gmail.com",
            "github_id": "chris-chris"
        },
        {
            "name": "Peter Hawkins",
            "github_id": "hawkinsp"
        },
        {
            "name": "Tom Hennigan",
            "email": "tomhennigan@gmail.com",
            "github_id": "tomhennigan"
        },
        {
            "name": "Hamza Merzi\u0107",
            "github_id": "hamzamerzic"
        },
        {
            "name": "Kristian Holsheimer",
            "email": "kristian.holsheimer@gmail.com",
            "github_id": "KristianHolsheimer"
        },
        {
            "name": "Rebecca Chen",
            "github_id": "rchen152"
        },
        {
            "name": "joao guilherme",
            "email": "joaoguilhermearujo@gmail.com",
            "github_id": "joaogui1"
        },
        {
            "name": "\u8a00\u8449",
            "email": "funaox@gmail.com",
            "github_id": "GitHub30"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/deepmind/rlax",
            "stars": 767,
            "issues": true,
            "readme": "# RLax\n\n![CI status](https://github.com/deepmind/rlax/workflows/ci/badge.svg)\n![docs](https://readthedocs.org/projects/rlax/badge/?version=latest)\n![pypi](https://img.shields.io/pypi/v/rlax)\n\nRLax (pronounced \"relax\") is a library built on top of JAX that exposes\nuseful building blocks for implementing reinforcement learning agents. Full\ndocumentation can be found at\n [rlax.readthedocs.io](https://rlax.readthedocs.io/en/latest/index.html).\n\n## Installation\n\nRLax can be installed with pip directly from github, with the following command:\n\n`pip install git+git://github.com/deepmind/rlax.git`.\n\nor from PyPI:\n\n`pip install rlax`\n\nAll RLax code may then be just in time compiled for different hardware\n(e.g. CPU, GPU, TPU) using `jax.jit`.\n\nIn order to run the `examples/` you will also need to clone the repo and\ninstall the additional requirements:\n[optax](https://github.com/deepmind/optax),\n[haiku](https://github.com/deepmind/haiku), and\n[bsuite](https://github.com/deepmind/bsuite).\n\n## Content\n\nThe operations and functions provided are not complete algorithms, but\nimplementations of reinforcement learning specific mathematical operations that\nare needed when building fully-functional agents capable of learning:\n\n* Values, including both state and action-values;\n* Values for Non-linear generalizations of the Bellman equations.\n* Return Distributions, aka distributional value functions;\n* General Value Functions, for cumulants other than the main reward;\n* Policies, via policy-gradients in both continuous and discrete action spaces.\n\nThe library supports both on-policy and off-policy learning (i.e. learning from\ndata sampled from a policy different from the agent's policy).\n\nSee file-level and function-level doc-strings for the documentation of these\nfunctions and for references to the papers that introduced and/or used them.\n\n## Usage\n\nSee `examples/` for examples of using some of the functions in RLax to\nimplement a few simple reinforcement learning agents, and demonstrate learning\non BSuite's version of the Catch environment (a common unit-test for\nagent development in the reinforcement learning literature):\n\nOther examples of JAX reinforcement learning agents using `rlax` can be found in\n[bsuite](https://github.com/deepmind/bsuite/tree/master/bsuite/baselines).\n\n## Background\n\nReinforcement learning studies the problem of a learning system (the *agent*),\nwhich must learn to interact with the universe it is embedded in (the\n*environment*).\n\nAgent and environment interact on discrete steps. On each step the agent selects\nan *action*, and is provided in return a (partial) snapshot of the state of the\nenvironment (the *observation*), and a scalar feedback signal (the *reward*).\n\nThe behaviour of the agent is characterized by a probability distribution over\nactions, conditioned on past observations of the environment (the *policy*). The\nagents seeks a policy that, from any given step, maximises the discounted\ncumulative reward that will be collected from that point onwards (the *return*).\n\nOften the agent policy or the environment dynamics itself are stochastic. In\nthis case the return is a random variable, and the optimal agent's policy is\ntypically more precisely specified as a policy that maximises the expectation of\nthe return (the *value*), under the agent's and environment's stochasticity.\n\n## Reinforcement Learning Algorithms\n\nThere are three prototypical families of reinforcement learning algorithms:\n\n1.  those that estimate the value of states and actions, and infer a policy by\n    *inspection* (e.g. by selecting the action with highest estimated value)\n2.  those that learn a model of the environment (capable of predicting the\n    observations and rewards) and infer a policy via *planning*.\n3.  those that parameterize a policy that can be directly *executed*,\n\nIn any case, policies, values or models are just functions. In deep\nreinforcement learning such functions are represented by a neural network.\nIn this setting, it is common to formulate reinforcement learning updates as\ndifferentiable pseudo-loss functions (analogously to (un-)supervised learning).\nUnder automatic differentiation, the original update rule is recovered.\n\nNote however, that in particular, the updates are only valid if the input data\nis sampled in the correct manner. For example, a policy gradient loss is only\nvalid if the input trajectory is an unbiased sample from the current policy;\ni.e. the data are on-policy. The library cannot check or enforce such\nconstraints. Links to papers describing how each operation is used are however\nprovided in the functions' doc-strings.\n\n## Naming Conventions and Developer Guidelines\n\nWe define functions and operations for agents interacting with a single stream\nof experience. The JAX construct `vmap` can be used to apply these same\nfunctions to batches (e.g. to support *replay* and *parallel* data generation).\n\nMany functions consider policies, actions, rewards, values, in consecutive\ntimesteps in order to compute their outputs. In this case the suffix `_t` and\n`tm1` is often to clarify on which step each input was generated, e.g:\n\n*   `q_tm1`: the action value in the `source` state of a transition.\n*   `a_tm1`: the action that was selected in the `source` state.\n*   `r_t`: the resulting rewards collected in the `destination` state.\n*   `discount_t`: the `discount` associated with a transition.\n*   `q_t`: the action values in the `destination` state.\n\nExtensive testing is provided for each function. All tests should also verify\nthe output of `rlax` functions when compiled to XLA using `jax.jit` and when\nperforming batch operations using `jax.vmap`.\n\n## Citing RLax\n\nRLax is part of the [DeepMind JAX Ecosystem], to cite RLax please use\nthe [DeepMind JAX Ecosystem citation].\n\n[DeepMind JAX Ecosystem]: https://deepmind.com/blog/article/using-jax-to-accelerate-our-research \"DeepMind JAX Ecosystem\"\n[DeepMind JAX Ecosystem citation]: https://github.com/deepmind/jax/blob/main/deepmind2020jax.txt \"Citation\"\n\n",
            "readme_url": "https://github.com/deepmind/rlax",
            "frameworks": []
        }
    ],
    "domain": {
        "domain_type": "Playing Games",
        "domain_prob": 0.9878094196759415
    }
}