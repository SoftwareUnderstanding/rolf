{
    "visibility": {
        "visibility": "public",
        "license": "BSD 3-Clause \"New\" or \"Revised\" License"
    },
    "name": "Fixup",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "hongyi-zhang",
                "owner_type": "User",
                "name": "Fixup",
                "url": "https://github.com/hongyi-zhang/Fixup",
                "stars": 135,
                "pushed_at": "2019-06-09 17:00:25+00:00",
                "created_at": "2019-03-15 02:43:21+00:00",
                "language": "Python",
                "description": "A Re-implementation of Fixed-update Initialization",
                "license": "BSD 3-Clause \"New\" or \"Revised\" License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "894a44cc066a027465cd26d634948d56d13af9af",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hongyi-zhang/Fixup/blob/master/.gitignore"
                    }
                },
                "size": 1203
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "913e173b97444fb5af5337bca4616c8e96fe68ab",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hongyi-zhang/Fixup/blob/master/LICENSE"
                    }
                },
                "size": 1500
            },
            {
                "type": "code",
                "name": "cifar",
                "sha": "89503446100c6d9af0eea226e406a99d88cb095b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hongyi-zhang/Fixup/tree/master/cifar"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "fairseq",
                "sha": "7a2ca69cbd12d896966b2db6fb23dca9853051ce",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hongyi-zhang/Fixup/tree/master/fairseq"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "imagenet",
                "sha": "9475e32a9f5f7d66de52b7aca8064f9828e87b54",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hongyi-zhang/Fixup/tree/master/imagenet"
                    }
                },
                "num_files": 3
            }
        ]
    },
    "authors": [
        {
            "name": "hongyi-zhang",
            "github_id": "hongyi-zhang"
        },
        {
            "name": "Tian Jin",
            "email": "tianjin@mit.edu",
            "github_id": "tjingrant"
        }
    ],
    "tags": [
        "fixup",
        "batchnorm",
        "resnet"
    ],
    "description": "A Re-implementation of Fixed-update Initialization",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/hongyi-zhang/Fixup",
            "stars": 135,
            "issues": true,
            "readme": "# Fixup\n**A Re-implementation of Fixed-update Initialization (https://arxiv.org/abs/1901.09321). *(requires Pytorch 1.0)***\n\n**Cite as:**\n\n*Hongyi Zhang, Yann N. Dauphin, Tengyu Ma. Fixup Initialization: Residual Learning Without Normalization. 7th International Conference on Learning Representations (ICLR 2019).*\n\n----\n## ResNet for CIFAR-10\nThe default arguments will train a ResNet-110 (https://arxiv.org/abs/1512.03385) with Fixup + Mixup (https://arxiv.org/abs/1710.09412).\n\n*Example:*\n\nThe following script will train a ResNet-32 model (https://arxiv.org/abs/1512.03385) on GPU 0 with Fixup and no Mixup (alpha=0), with weight decay 5e-4 and (the default) learning rate 0.1 and batch size 128.\n```\nCUDA_VISIBLE_DEVICES=0 python cifar_train.py -a fixup_resnet32 --sess benchmark_a0d5e4lr01 --seed 11111 --alpha 0. --decay 5e-4\n```\n\n----\n## ResNet for ImageNet\nImageNet models with training scripts are now available. (Thanks @tjingrant for help!) \n\nTop-1 accuracy for ResNet-50 at Epoch 100 with Mixup (alpha=0.7) is around 76.0%.\n\n----\n## Transformer for machine translation\nTransformer model with Fixup (instead of layer normalization) is available. To run the experiments, you will need to download and install the fairseq library (the provided code was tested on an earlier version: https://github.com/pytorch/fairseq/tree/5d00e8eea2644611f397d05c6c8f15083388b8b4). You can then copy the files into corresponding folders.\n\nAn example script `run.sh` is provided to run the IWSLT experiments described in the paper. For more information, please refer to the instructions in fairseq repo (https://github.com/pytorch/fairseq/tree/5d00e8eea2644611f397d05c6c8f15083388b8b4/examples/translation).\n",
            "readme_url": "https://github.com/hongyi-zhang/Fixup",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "Fixup Initialization: Residual Learning Without Normalization",
            "arxiv": "1901.09321",
            "year": 2019,
            "url": "http://arxiv.org/abs/1901.09321v2",
            "abstract": "Normalization layers are a staple in state-of-the-art deep neural network\narchitectures. They are widely believed to stabilize training, enable higher\nlearning rate, accelerate convergence and improve generalization, though the\nreason for their effectiveness is still an active research topic. In this work,\nwe challenge the commonly-held beliefs by showing that none of the perceived\nbenefits is unique to normalization. Specifically, we propose fixed-update\ninitialization (Fixup), an initialization motivated by solving the exploding\nand vanishing gradient problem at the beginning of training via properly\nrescaling a standard initialization. We find training residual networks with\nFixup to be as stable as training with normalization -- even for networks with\n10,000 layers. Furthermore, with proper regularization, Fixup enables residual\nnetworks without normalization to achieve state-of-the-art performance in image\nclassification and machine translation.",
            "authors": [
                "Hongyi Zhang",
                "Yann N. Dauphin",
                "Tengyu Ma"
            ]
        },
        {
            "title": "mixup: Beyond Empirical Risk Minimization",
            "arxiv": "1710.09412",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.09412v2",
            "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors\nsuch as memorization and sensitivity to adversarial examples. In this work, we\npropose mixup, a simple learning principle to alleviate these issues. In\nessence, mixup trains a neural network on convex combinations of pairs of\nexamples and their labels. By doing so, mixup regularizes the neural network to\nfavor simple linear behavior in-between training examples. Our experiments on\nthe ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show\nthat mixup improves the generalization of state-of-the-art neural network\narchitectures. We also find that mixup reduces the memorization of corrupt\nlabels, increases the robustness to adversarial examples, and stabilizes the\ntraining of generative adversarial networks.",
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N. Dauphin",
                "David Lopez-Paz"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999993771310272,
        "task": "Object Detection",
        "task_prob": 0.7812610723860052
    }
}