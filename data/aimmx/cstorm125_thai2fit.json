{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "thai2fit (formerly thai2vec)",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "cstorm125",
                "owner_type": "User",
                "name": "thai2fit",
                "url": "https://github.com/cstorm125/thai2fit",
                "stars": 180,
                "pushed_at": "2021-01-09 14:29:57+00:00",
                "created_at": "2018-01-25 11:15:34+00:00",
                "language": "Jupyter Notebook",
                "description": "ULMFit Language Modeling, Text Feature Extraction and Text Classification in Thai Language. Created as part of pyThaiNLP",
                "license": "MIT License",
                "frameworks": [
                    "scikit-learn"
                ]
            },
            {
                "type": "code",
                "name": ".gitattributes",
                "sha": "dfd065ffa10e5a4fd97727fef9c8ae5d74dc0e52",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cstorm125/thai2fit/blob/master/.gitattributes"
                    }
                },
                "size": 169
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "bafb7c699453f87f29481c039878f953b06d1c06",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cstorm125/thai2fit/blob/master/.gitignore"
                    }
                },
                "size": 1434
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "4011e7d4ae540e48583056775b4a451227fda3f0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cstorm125/thai2fit/blob/master/LICENSE"
                    }
                },
                "size": 1063
            },
            {
                "type": "code",
                "name": "images",
                "sha": "3850b92bb571d455d22abe27f816bc0e71f8f4cc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cstorm125/thai2fit/tree/master/images"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "thwiki_lm",
                "sha": "d380545666aad6d88d6c7312cd087a7eae2c9320",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cstorm125/thai2fit/tree/master/thwiki_lm"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "wongnai_cls",
                "sha": "fcdd27909f49159df1e308ef7256be07841a41e0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cstorm125/thai2fit/tree/master/wongnai_cls"
                    }
                },
                "num_files": 4
            }
        ]
    },
    "authors": [
        {
            "name": "Charin",
            "github_id": "cstorm125"
        },
        {
            "name": "Wannaphong Phatthiyaphaibun",
            "email": "email@wannaphong.com",
            "github_id": "wannaphong"
        }
    ],
    "tags": [],
    "description": "ULMFit Language Modeling, Text Feature Extraction and Text Classification in Thai Language. Created as part of pyThaiNLP",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/cstorm125/thai2fit",
            "stars": 180,
            "issues": true,
            "readme": "# thai2fit (formerly thai2vec)\nULMFit Language Modeling, Text Feature Extraction and Text Classification in Thai Language.\nCreated as part of [pyThaiNLP](https://github.com/PyThaiNLP/) with [ULMFit](https://arxiv.org/abs/1801.06146) implementation from [fast.ai](http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html)\n\nModels and word embeddings can also be downloaded via [Dropbox](https://www.dropbox.com/sh/lgd8wf5h0eoehzr/AACD0ZnpOiMKQq1N94WmfV-Va?dl=1).\n\nWe pretrained a language model with 60,005 embeddings on [Thai Wikipedia Dump](https://dumps.wikimedia.org/thwiki/latest/thwiki-latest-pages-articles.xml.bz2) (perplexity of 28.71067) and text classification (micro-averaged F-1 score of 0.60322 on 5-label classification problem. Benchmarked to 0.5109 by [fastText](fasttext.cc) and 0.4976 by LinearSVC on [Wongnai Challenge: Review Rating Prediction](https://www.kaggle.com/c/wongnai-challenge-review-rating-prediction). The language model can also be used to extract text features for other downstream tasks.\n\n![random word vectors](https://github.com/cstorm125/thai2fit/blob/master/images/random.png?raw=true)\n\n# Dependencies\n* Python>=3.6\n* PyTorch>=1.0\n* fastai>=1.0.38\n\n# Version History\n\n## v0.1\n\n* Pretrained language model based on Thai Wikipedia with the perplexity of 46.61\n* Pretrained word embeddings (.vec) with 51,556 tokens and 300 dimensions\n* Classification benchmark of 94.4% accuracy compared to 65.2% by [fastText](https://fasttext.cc/) for 4-label classification of [BEST](https://thailang.nectec.or.th/best/)\n\n## v0.2\n\n* Refactored to use `fastai.text` instead of `torchtext`\n* Pretrained word embeddings (.vec and .bin) with 60,000 tokens and 300 dimensions (`word2vec_examples.ipynb`)\n* Classification benchmark of 0.60925 micro-averaged F1 score compared to 0.49366 by [fastText](https://fasttext.cc/) and 0.58139 by competition winner for 5-label classification of [Wongnai Challenge: Review Rating Prediction](https://www.kaggle.com/c/wongnai-challenge-review-rating-prediction) (`ulmfit_wongnai.ipynb`)\n* Text feature extraction for other downstream tasks such as clustering (`ulmfit_ec.ipynb`)\n\n## v0.3\n* Repo name changed to `thai2fit` in order to avoid confusion since this is ULMFit not word2vec implementation\n* Migrate to Pytorch 1.0 and fastai 1.0 API\n* Add QRNN-based models; inference time drop by 50% on average\n* Pretrained language model based on Thai Wikipedia with the perplexity of 46.04264 (20% validation) and 23.32722 (1% validation) (`pretrain_wiki.ipynb`)\n* Pretrained word embeddings (.vec and .bin) with 60,000 tokens and 400 dimensions (`word2vec_examples.ipynb`) based on QRNN\n* Classification benchmark of 0.60925 micro-averaged F1 score compared to 0.49366 by [fastText](https://fasttext.cc/) and 0.58139 by competition winner for 5-label classification of [Wongnai Challenge: Review Rating Prediction](https://www.kaggle.com/c/wongnai-challenge-review-rating-prediction) (`ulmfit_wongnai.ipynb`)\n* LSTM weights are copied from v0.2 according to guideline provided in [fastai forum](https://forums.fast.ai/t/migrate-ulmfit-weights-trained-using-fastai-0-7-to-fastai-1-0/35100)\n```\nI remember someone doing a script but I can\u2019t find it. For both, you just have to map the old names of the weights to the new ones. Note that:\n\nin language models, there is a bias in the decoder in fastai v1 that you probably won\u2019t have\nin the classifier, the order you see for the layers is artificial (it\u2019s the pytorch representation that takes the things in the order you put them in __init__ when not using Sequential) but the two models (old and new) apply batchnorm, dropout and linear in the same order\ntokenizing is done differently in fastai v1, so you may have to fine-tune your models again (we add an xxmaj token for words beginning with a capital for instance)\nfor weight dropout, you want the weights you have put both in '0.rnns.0.module.weight_hh_l0' and 0.rnns.0.weight_hh_l0_raw (the second one is copied to the first with dropout applied anyway)\n```\n\n## v0.31\n* Support fastai>=1.0.38\n* Pretrained [Thai Wikipedia Dump](https://dumps.wikimedia.org/thwiki/latest/thwiki-latest-pages-articles.xml.bz2) with the same training scheme as [ulmfit-multilingual](https://github.com/n-waves/ulmfit-multilingual)\n* Remove QRNN models due to inferior performance\n* Classification benchmarks now include for [wongnai-corpus](https://github.com/wongnai/wongnai-corpus) (See `wongnai_cls`), [prachathai-67k](https://github.com/PyThaiNLP/prachathai-67k) (See `prachathai_cls`), and [wisesight-sentiment](https://github.com/cstorm125/wisesight-sentiment) (See `wisesight_cls`)\n\n## v0.32\n* Better text cleaning rules resulting in [Thai Wikipedia Dump](https://dumps.wikimedia.org/thwiki/latest/thwiki-latest-pages-articles.xml.bz2) pretrained perplexity of 28.71067.\n\n## v0.4 (In Progress)\n* Replace AWD-LSTM/QRNN with tranformers-based models\n* Named-entity recognition\n\n# Text Classification\n\nWe trained the [ULMFit model](https://arxiv.org/abs/1801.06146) implemented by`thai2fit` for text classification. We use [Wongnai Challenge: Review Rating Prediction](https://www.kaggle.com/c/wongnai-challenge-review-rating-prediction) as our benchmark as it is the only sizeable and publicly available text classification dataset at the time of writing (June 21, 2018). It has 39,999 reviews for training and validation, and 6,203 reviews for testing. \n\nWe achieved validation perplexity at 35.75113 and validation micro F1 score at 0.598 for five-label classification. Micro F1 scores for public and private leaderboards are 0.59313 and 0.60322 respectively, which are state-of-the-art as of the time of writing (February 27, 2019). FastText benchmark based on their own [pretrained embeddings](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md) has the performance of 0.50483 and 0.49366 for public and private leaderboards respectively. See `ulmfit_wongnai.ipynb` for more details.\n\n# Text Feature Extraction\n\nThe pretrained language model of `thai2fit` can be used to convert Thai texts into vectors, after which said vectors can be used for various machine learning tasks such as classification, clustering, translation, question answering and so on. The idea is to train a language model that \"understands\" the texts then extract certain vectors that the model \"thinks\" represents the texts we want. You can access this functionality easily via [pythainlp](https://github.com/pyThaiNLP/pythainlp/)\n\n```\nfrom pythainlp.ulmfit import *\ndocument_vector('\u0e27\u0e31\u0e19\u0e19\u0e35\u0e49\u0e27\u0e31\u0e19\u0e14\u0e35\u0e1b\u0e35\u0e43\u0e2b\u0e21\u0e48',learn,data)\n>> array([ 0.066298,  0.307813,  0.246051,  0.008683, ..., -0.058363,  0.133258, -0.289954, -1.770246], dtype=float32)\n```\n\n# Language Modeling\n\n\nThe goal of this notebook is to train a language model using the [fast.ai](http://www.fast.ai/) version of [AWD LSTM Language Model](https://arxiv.org/abs/1708.02182), with data from [Thai Wikipedia Dump](https://dumps.wikimedia.org/thwiki/latest/thwiki-latest-pages-articles.xml.bz2) last updated February 17, 2019. Using 40M/200k/200k tokens of train-validation-test split, we achieved validation perplexity of **27.81627 with 60,004 embeddings at 400 dimensions**, compared to state-of-the-art as of October 27, 2018 at **42.41 for English WikiText-2 by [Yang et al (2018)](https://arxiv.org/abs/1711.03953)**. To the best of our knowledge, there is no comparable research in Thai language at the point of writing (February 17, 2019). See `thwiki_lm` for more details.\n\n# Word Embeddings\n\nWe use the embeddings from `v0.1` since it was trained specifically for word2vec as opposed to latter versions which garner to classification. The `thai2vec.bin` 51,556 word embeddings of 300 dimensions, in descending order by their frequencies (See `thai2vec.vocab`). The files are in word2vec format readable by `gensim`. Most common applications include word vector visualization, word arithmetic, word grouping, cosine similarity and sentence or document vectors. For sample code, see `thwiki_lm/word2vec_examples.ipynb`.\n\n## Word Arithmetic\n\nYou can do simple \"arithmetic\" with words based on the word vectors such as:\n* \u0e1c\u0e39\u0e49\u0e2b\u0e0d\u0e34\u0e07 (female) + \u0e23\u0e32\u0e0a\u0e32 (king) - \u0e1c\u0e39\u0e49\u0e0a\u0e32\u0e22 (male) = \u0e23\u0e32\u0e0a\u0e34\u0e19\u0e35 (queen)\n* \u0e2b\u0e38\u0e49\u0e19 (stock) - \u0e1e\u0e19\u0e31\u0e19 (gambling) = \u0e01\u0e34\u0e08\u0e01\u0e32\u0e23 (business)\n* \u0e2d\u0e40\u0e21\u0e23\u0e34\u0e01\u0e31\u0e19 (american) + \u0e1f\u0e38\u0e15\u0e1a\u0e2d\u0e25 (football) = \u0e40\u0e1a\u0e2a\u0e1a\u0e2d\u0e25 (baseball)\n\n![word arithmetic](https://github.com/cstorm125/thai2fit/blob/master/images/word_arithematic_queen.png?raw=true)\n\n## Word Grouping\n\nIt can also be used to do word groupings. For instance:\n* \u0e2d\u0e32\u0e2b\u0e32\u0e23\u0e40\u0e0a\u0e49\u0e32 \u0e2d\u0e32\u0e2b\u0e32\u0e23\u0e2a\u0e31\u0e15\u0e27\u0e4c \u0e2d\u0e32\u0e2b\u0e32\u0e23\u0e40\u0e22\u0e47\u0e19 \u0e2d\u0e32\u0e2b\u0e32\u0e23\u0e01\u0e25\u0e32\u0e07\u0e27\u0e31\u0e19 (breakfast animal-food dinner lunch) - \u0e2d\u0e32\u0e2b\u0e32\u0e23\u0e2a\u0e31\u0e15\u0e27\u0e4c (animal-food) is type of food whereas others are meals in the day\n* \u0e25\u0e39\u0e01\u0e2a\u0e32\u0e27 \u0e25\u0e39\u0e01\u0e2a\u0e30\u0e43\u0e20\u0e49 \u0e25\u0e39\u0e01\u0e40\u0e02\u0e22 \u0e1b\u0e49\u0e32 (duaghter daughter-in-law son-in-law aunt) - \u0e25\u0e39\u0e01\u0e2a\u0e32\u0e27 (daughter) is immediate family whereas others are not\n* \u0e01\u0e14 \u0e01\u0e31\u0e14 \u0e01\u0e34\u0e19 \u0e40\u0e04\u0e35\u0e49\u0e22\u0e27 (press bite eat chew) - \u0e01\u0e14 (press) is not verbs for the eating process\nNote that this could be relying on a different \"take\" than you would expect. For example, you could have answered \u0e25\u0e39\u0e01\u0e40\u0e02\u0e22 in the second example because it  is the one associated with male gender.\n\n![word grouping](https://github.com/cstorm125/thai2fit/blob/master/images/doesnt_match1.png?raw=true)\n\n## Cosine Similarity\n\nCalculate cosine similarity between two word vectors.\n\n* \u0e08\u0e35\u0e19 (China) and \u0e1b\u0e31\u0e01\u0e01\u0e34\u0e48\u0e07 (Beijing): 0.31359560752667964\n* \u0e2d\u0e34\u0e15\u0e32\u0e25\u0e35 (Italy) and \u0e42\u0e23\u0e21 (Rome): 0.42819627065839394\n* \u0e1b\u0e31\u0e01\u0e01\u0e34\u0e48\u0e07 (Beijing) and \u0e42\u0e23\u0e21 (Rome): 0.27347283956785434\n* \u0e08\u0e35\u0e19 (China) and \u0e42\u0e23\u0e21 (Rome): 0.02666692964073511\n* \u0e2d\u0e34\u0e15\u0e32\u0e25\u0e35 (Italy) and \u0e1b\u0e31\u0e01\u0e01\u0e34\u0e48\u0e07 (Beijing): 0.17900795797557473\n\n![cosine similarity](https://github.com/cstorm125/thai2fit/blob/master/images/cosin_sim_arrows.png?raw=true)\n\n# Citation\n\n```\n@software{charin_polpanumas_2021_4429691,\n  author       = {Charin Polpanumas and\n                  Wannaphong Phatthiyaphaibun},\n  title        = {thai2fit: Thai language Implementation of ULMFit},\n  month        = jan,\n  year         = 2021,\n  publisher    = {Zenodo},\n  version      = {v0.3},\n  doi          = {10.5281/zenodo.4429691},\n  url          = {https://doi.org/10.5281/zenodo.4429691}\n}\n```\n\n# NLP Workshop at Chiangmai University\n\n- [Getting Started with PyThaiNLP](https://github.com/PyThaiNLP/pythainlp/blob/dev/notebooks/pythainlp-get-started.ipynb)\n\n- [thai2fit slides](https://www.canva.com/design/DADc1jbD1Hk/Iz4eFFQlbEMqjn8r99M85w/view)\n\n- [Text Generation with Wiki Language Model](https://github.com/PyThaiNLP/pythainlp/blob/dev/notebooks/text_generation.ipynb)\n\n- [Word Vectors](https://github.com/cstorm125/thai2fit/blob/master/thwiki_lm/word2vec_examples.ipynb)\n\n- [Sentiment Analysis](https://github.com/PyThaiNLP/pythainlp/blob/dev/notebooks/sentiment_analysis.ipynb)\n\n- [PyThaiNLP tutorial](https://www.thainlp.org/pythainlp/tutorials/)\n\n- [pyThaiNLP documentation](https://www.thainlp.org/pythainlp/docs/2.0/)\n",
            "readme_url": "https://github.com/cstorm125/thai2fit",
            "frameworks": [
                "scikit-learn"
            ]
        }
    ],
    "references": [
        {
            "title": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model",
            "arxiv": "1711.03953",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.03953v4",
            "abstract": "We formulate language modeling as a matrix factorization problem, and show\nthat the expressiveness of Softmax-based models (including the majority of\nneural language models) is limited by a Softmax bottleneck. Given that natural\nlanguage is highly context-dependent, this further implies that in practice\nSoftmax with distributed word embeddings does not have enough capacity to model\nnatural language. We propose a simple and effective method to address this\nissue, and improve the state-of-the-art perplexities on Penn Treebank and\nWikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on\nthe large-scale 1B Word dataset, outperforming the baseline by over 5.6 points\nin perplexity.",
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Ruslan Salakhutdinov",
                "William W. Cohen"
            ]
        },
        {
            "title": "Universal Language Model Fine-tuning for Text Classification",
            "arxiv": "1801.06146",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.06146v5",
            "abstract": "Inductive transfer learning has greatly impacted computer vision, but\nexisting approaches in NLP still require task-specific modifications and\ntraining from scratch. We propose Universal Language Model Fine-tuning\n(ULMFiT), an effective transfer learning method that can be applied to any task\nin NLP, and introduce techniques that are key for fine-tuning a language model.\nOur method significantly outperforms the state-of-the-art on six text\nclassification tasks, reducing the error by 18-24% on the majority of datasets.\nFurthermore, with only 100 labeled examples, it matches the performance of\ntraining from scratch on 100x more data. We open-source our pretrained models\nand code.",
            "authors": [
                "Jeremy Howard",
                "Sebastian Ruder"
            ]
        },
        {
            "title": "Regularizing and Optimizing LSTM Language Models",
            "arxiv": "1708.02182",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.02182v1",
            "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2.",
            "authors": [
                "Stephen Merity",
                "Nitish Shirish Keskar",
                "Richard Socher"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "wongnai-corpus",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/wongnai/wongnai-corpus"
                    }
                }
            },
            {
                "name": "WikiText-2"
            },
            {
                "name": "Wikipedia"
            },
            {
                "name": "Penn Treebank"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999999761639201,
        "task": "Language Modelling",
        "task_prob": 0.9726553674988568
    }
}