{
    "visibility": {
        "visibility": "public"
    },
    "name": "Automatic-Translation",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "VladimirV99",
                "owner_type": "User",
                "name": "Automatic-Translation",
                "url": "https://github.com/VladimirV99/Automatic-Translation",
                "stars": 0,
                "pushed_at": "2021-09-14 22:05:59+00:00",
                "created_at": "2020-12-10 10:09:45+00:00",
                "language": "Jupyter Notebook",
                "description": "Automatic language translation using LSTM",
                "frameworks": [
                    "NLTK",
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "06a8b3a363616e55350179cd33f0c3ce9cae41f1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/VladimirV99/Automatic-Translation/blob/main/.gitignore"
                    }
                },
                "size": 151
            },
            {
                "type": "code",
                "name": "Automatic_Translation.ipynb",
                "sha": "fcf978e1c607ef6f5cdb752f92a3a80a8c4bbc78",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/VladimirV99/Automatic-Translation/blob/main/Automatic_Translation.ipynb"
                    }
                },
                "size": 60323
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "1b62703eddedcb0455d6898f71f947c7b8bb98f7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/VladimirV99/Automatic-Translation/tree/main/docs"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "environment.yml",
                "sha": "519a7fec2d71d3012ad587aab364e9420d2b1751",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/VladimirV99/Automatic-Translation/blob/main/environment.yml"
                    }
                },
                "size": 131
            }
        ]
    },
    "authors": [
        {
            "name": "Vladimir Vuksanovic",
            "github_id": "VladimirV99"
        }
    ],
    "tags": [
        "deep-learning",
        "tensorflow"
    ],
    "description": "Automatic language translation using LSTM",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/VladimirV99/Automatic-Translation",
            "stars": 0,
            "issues": true,
            "readme": "# Automatic-Translation\n\nAutomatic language translation using a sequence-to-sequence LSTM model\n\n## Required system packages\n\n- python\n- pip\n- graphviz\n\n## Required libraries\n\n- notebook\n- numpy\n- pandas\n- tensorflow\n- pydot\n- nltk\n- scikit-learn\n- matplotlib\n\nIf you have conda installed, you can create an evironment with all required packages installed by running the following commands\n```bash\nconda env create -f environment.yml\nconda activate translation\n```\n\n## Datasets\n\nEnglish word list: https://github.com/dwyl/english-words\n\nFrench word list: http://www.lexique.org/\n\nEnglish-to-French translation datasets: \n- http://www.manythings.org/anki/\n- https://www.tensorflow.org/datasets/catalog/wmt14_translate\n\n## Bibliography\n\n- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)\n- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)\n- [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf)\n- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n- https://nlp.stanford.edu/projects/glove/\n- https://google.github.io/seq2seq/\n- https://keras.io/examples/nlp/lstm_seq2seq/\n- https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/\n- https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/\n- https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n- https://medium.com/@d.salvaggio/sequence-to-sequence-architectures-ad6ff4451f84\n- https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639\n- https://towardsdatascience.com/neural-machine-translation-using-seq2seq-with-keras-c23540453c74\n",
            "readme_url": "https://github.com/VladimirV99/Automatic-Translation",
            "frameworks": [
                "NLTK",
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
            "arxiv": "1609.08144",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.08144v2",
            "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for\nautomated translation, with the potential to overcome many of the weaknesses of\nconventional phrase-based translation systems. Unfortunately, NMT systems are\nknown to be computationally expensive both in training and in translation\ninference. Also, most NMT systems have difficulty with rare words. These issues\nhave hindered NMT's use in practical deployments and services, where both\naccuracy and speed are essential. In this work, we present GNMT, Google's\nNeural Machine Translation system, which attempts to address many of these\nissues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder\nlayers using attention and residual connections. To improve parallelism and\ntherefore decrease training time, our attention mechanism connects the bottom\nlayer of the decoder to the top layer of the encoder. To accelerate the final\ntranslation speed, we employ low-precision arithmetic during inference\ncomputations. To improve handling of rare words, we divide words into a limited\nset of common sub-word units (\"wordpieces\") for both input and output. This\nmethod provides a good balance between the flexibility of \"character\"-delimited\nmodels and the efficiency of \"word\"-delimited models, naturally handles\ntranslation of rare words, and ultimately improves the overall accuracy of the\nsystem. Our beam search technique employs a length-normalization procedure and\nuses a coverage penalty, which encourages generation of an output sentence that\nis most likely to cover all the words in the source sentence. On the WMT'14\nEnglish-to-French and English-to-German benchmarks, GNMT achieves competitive\nresults to state-of-the-art. Using a human side-by-side evaluation on a set of\nisolated simple sentences, it reduces translation errors by an average of 60%\ncompared to Google's phrase-based production system.",
            "authors": [
                "Yonghui Wu",
                "Mike Schuster",
                "Zhifeng Chen",
                "Quoc V. Le",
                "Mohammad Norouzi",
                "Wolfgang Macherey",
                "Maxim Krikun",
                "Yuan Cao",
                "Qin Gao",
                "Klaus Macherey",
                "Jeff Klingner",
                "Apurva Shah",
                "Melvin Johnson",
                "Xiaobing Liu",
                "\u0141ukasz Kaiser",
                "Stephan Gouws",
                "Yoshikiyo Kato",
                "Taku Kudo",
                "Hideto Kazawa",
                "Keith Stevens",
                "George Kurian",
                "Nishant Patil",
                "Wei Wang",
                "Cliff Young",
                "Jason Smith",
                "Jason Riesa",
                "Alex Rudnick",
                "Oriol Vinyals",
                "Greg Corrado",
                "Macduff Hughes",
                "Jeffrey Dean"
            ]
        },
        {
            "title": "Sequence to Sequence Learning with Neural Networks",
            "arxiv": "1409.3215",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.3215v3",
            "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\na vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT'14 dataset, the translations produced by\nthe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\nnot have difficulty on long sentences. For comparison, a phrase-based SMT\nsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on\nthis task. The LSTM also learned sensible phrase and sentence representations\nthat are sensitive to word order and are relatively invariant to the active and\nthe passive voice. Finally, we found that reversing the order of the words in\nall source sentences (but not target sentences) improved the LSTM's performance\nmarkedly, because doing so introduced many short term dependencies between the\nsource and the target sentence which made the optimization problem easier.",
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V. Le"
            ]
        },
        {
            "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
            "arxiv": "1406.1078",
            "year": 2014,
            "url": "http://arxiv.org/abs/1406.1078v3",
            "abstract": "In this paper, we propose a novel neural network model called RNN\nEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNN\nencodes a sequence of symbols into a fixed-length vector representation, and\nthe other decodes the representation into another sequence of symbols. The\nencoder and decoder of the proposed model are jointly trained to maximize the\nconditional probability of a target sequence given a source sequence. The\nperformance of a statistical machine translation system is empirically found to\nimprove by using the conditional probabilities of phrase pairs computed by the\nRNN Encoder-Decoder as an additional feature in the existing log-linear model.\nQualitatively, we show that the proposed model learns a semantically and\nsyntactically meaningful representation of linguistic phrases.",
            "authors": [
                "Kyunghyun Cho",
                "Bart van Merrienboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999999601915124,
        "task": "Machine Translation",
        "task_prob": 0.9875182638087608
    }
}