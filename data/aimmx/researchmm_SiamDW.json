{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Deeper and Wider Siamese Networks for Real-Time Visual Tracking",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "researchmm",
                "owner_type": "Organization",
                "name": "SiamDW",
                "url": "https://github.com/researchmm/SiamDW",
                "stars": 692,
                "pushed_at": "2021-05-18 03:53:58+00:00",
                "created_at": "2019-03-28 06:58:07+00:00",
                "language": "Python",
                "description": "[CVPR'19 Oral] Deeper and Wider Siamese Networks for Real-Time Visual Tracking",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "ddcc28430a07fe4e51818f52bcce9c01904ccc88",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/researchmm/SiamDW/blob/master/LICENSE"
                    }
                },
                "size": 1086
            },
            {
                "type": "code",
                "name": "demo",
                "sha": "56aa344838de993943798395fba2efa4a0c59295",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/researchmm/SiamDW/tree/master/demo"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "experiments",
                "sha": "83645fa7a484ec6f0e2f3276dbfa55d61827573b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/researchmm/SiamDW/tree/master/experiments"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "install_fc.sh",
                "sha": "2937c51d8f2ef6285205bfd2b00ae2cd7f5076a0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/researchmm/SiamDW/blob/master/install_fc.sh"
                    }
                },
                "size": 360
            },
            {
                "type": "code",
                "name": "install_rpn.sh",
                "sha": "cb58828097b2157bd4cba8027cea2ee04bbc9491",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/researchmm/SiamDW/blob/master/install_rpn.sh"
                    }
                },
                "size": 362
            },
            {
                "type": "code",
                "name": "lib",
                "sha": "c06aa8c7589950ea2b89ce037ab15307b0501fe0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/researchmm/SiamDW/tree/master/lib"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "siamese_tracking",
                "sha": "e31a3dd89abd0962dae5fef54c668a14058ae60b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/researchmm/SiamDW/tree/master/siamese_tracking"
                    }
                },
                "num_files": 12
            }
        ]
    },
    "authors": [
        {
            "name": "ZP ZHANG",
            "email": "zpzhang1995@gmail.com",
            "github_id": "JudasDie"
        },
        {
            "name": "Houwen Peng",
            "github_id": "penghouwen"
        },
        {
            "name": "Houwen Peng",
            "github_id": "hwpengms"
        },
        {
            "name": "Zhang Liliang",
            "email": "zhangll.level0@gmail.com",
            "github_id": "zhangliliang"
        },
        {
            "name": "hongyuan_yu",
            "email": "hongyuan_yu@yeah.net",
            "github_id": "hongyuanyu"
        }
    ],
    "tags": [
        "tracking"
    ],
    "description": "[CVPR'19 Oral] Deeper and Wider Siamese Networks for Real-Time Visual Tracking",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/researchmm/SiamDW",
            "stars": 692,
            "issues": true,
            "readme": "# Deeper and Wider Siamese Networks for Real-Time Visual Tracking\r\nWe are hiring research interns for visual tracking and neural architecture search projects: houwen.peng@microsoft.com\r\n\r\n## News\r\n- :trophy: **We are the Winner of [VOT-19 RGB-D](http://data.votchallenge.net/vot2019/vot2019_rgbt.pdf) challenge** [[codes and models]](https://github.com/researchmm/VOT2019)\r\n- :trophy: **We won the Runner-ups in VOT-19 Long-term and RGB-T challenges** [[codes and models]](https://github.com/researchmm/VOT2019)\r\n- :sunny::sunny: We add the results on **VOT-18**, **VOT-19**, **GOT10K**, **VISDRONE19**, and **LaSOT** datasets.\r\n- :sunny::sunny: The training and testing code of SiamFC+ and SiamRPN+ have been released.\r\n- :sunny::sunny: Our [paper](http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Deeper_and_Wider_Siamese_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.html) has been accepted by [CVPR2019](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Deeper_and_Wider_Siamese_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.pdf) (**Oral**).\r\n- :sunny::sunny: We provide a [parameter tuning toolkit](#TUNE-TOOLKIT) for siamese tracking framework.\r\n\r\n\r\n## Introduction\r\nSiamese networks have drawn great attention in visual tracking because of their balanced accuracy and speed.  However, the backbone network utilized in these trackers is still the classical AlexNet, which does not fully take advantage of the capability of modern deep neural networks. \r\n  \r\nOur proposals improve the performances of fully convolutional siamese trackers by,\r\n1) introducing CIR and CIR-D units to unveil the power of deeper and wider networks like [ResNet](https://arxiv.org/abs/1512.03385) and [Inceptipon](https://arxiv.org/abs/1409.4842); \r\n2) designing backbone networks according to the analysis on internal network factors (e.g. receptive field, stride, output feature size), which affect tracking performances.\r\n\r\n<div align=\"center\">\r\n  <img src=\"demo/vis.gif\" width=\"800px\" />\r\n  <!-- <p>Example SiamFC, SiamRPN and SiamMask outputs.</p> -->\r\n</div>\r\n\r\n<!-- :tada::tada: **Highlight !!**\r\nSiamese tracker is severely sensitive to hyper-parameter, which is a common sense in tracking field. Although significant progresses have been made in some works, the result is hard to reproduce. In this case, we provide a [parameter tuning toolkit]() to make our model being reproduced easily. We hope our efforts and supplies will be helpful to your work. -->\r\n\r\n## Main Results\r\n#### Main results on VOT and OTB\r\n| Models  | OTB13 | OTB15 | VOT15 | VOT16 | VOT17|\r\n| :------ | :------: | :------: | :------: | :------: | :------: | \r\n| Alex-FC      | 0.608 | 0.579 | 0.289 | 0.235 | 0.188 |\r\n| Alex-RPN     | -     | 0.637 | 0.349 | 0.344 | 0.244 |\r\n| CIResNet22-FC  | 0.663 | 0.644 | 0.318 | 0.303 | 0.234 |\r\n| CIResIncep22-FC| 0.662 | 0.642 | 0.310 | 0.295 | 0.236 |\r\n| CIResNext23-FC | 0.659 | 0.633 | 0.297 | 0.278 | 0.229 |\r\n| CIResNet22-RPN| 0.674 | 0.666 | 0.381 | 0.376 | 0.294 |\r\n\r\n#### Main results trained with GOT-10k (SiamFC)\r\n| Models  | OTB13 | OTB15 | VOT15 | VOT16 | VOT17|\r\n| :------ | :------: | :------: | :------: | :------: | :------: |\r\n| Alex-FC        |-      | -     | -     | -     |0.188     | \r\n| CIResNet22-FC  | 0.664 | 0.654 | 0.361 | 0.335 | 0.266| \r\n| CIResNet22W-FC | **0.689** | **0.674** | **0.368** | **0.352** | **0.269** |\r\n| CIResIncep22-FC| 0.673 | 0.650 | 0.332 | 0.305 | 0.251|\r\n| CIResNext22-FC | 0.668 | 0.651 | 0.336 | 0.304 | 0.246|\r\n| Raw Results | :paperclip: [OTB2013](https://pan.baidu.com/s/1HgkjUmnYl7qagIkz9u4r_A) | :paperclip: [OTB2015](https://pan.baidu.com/s/1ZgL4DQL57cuWfqxLFmUR1A)  | :paperclip: [VOT15](https://pan.baidu.com/s/1SGLcMWgrBuBT_kaXMdQBug)  | :paperclip: [VOT16](https://pan.baidu.com/s/12jmWEwo4tjbM4SHSKgULNw) |  :paperclip: [VOT17](https://pan.baidu.com/s/1UWQRE2VrJrONpj293el4Pw) |\r\n\r\n- Some reproduced results listed above are slightly better than the ones in the paper.\r\n- Recently we found that training on GOT10K dataset can achieve better performance for SiamFC. So we provide the results being trained on GOT10K.\r\n<!-- - Download pretrained on GOT10K [model](https://drive.google.com/file/d/1xvexXCUCB0gCYFnShj3NQ4Xuk52lLLtE/view?usp=sharing).  -->\r\n\r\n\r\n#### New added results\r\n| Benchmark | VOT18| VOT19 | GOT10K | VISDRONE19 | LaSOT | \r\n|:------: |:------: | :------: |  :------: | :------: | :------: | \r\n| Performance   | **0.270** | **0.242** | **0.416**  | **0.383** |**0.384**|\r\n| Raw Results | :paperclip: [VOT18](https://pan.baidu.com/s/1hKg-n4PTPL_VCEdxCrXMAA) | :paperclip: [VOT19](https://pan.baidu.com/s/1mwPrMJhi79_TO40RzTAwvQ) | :paperclip: [GOT10K](https://pan.baidu.com/s/10INTbmtfL-EdfkAmDQgcKw) |:paperclip: [VISDRONE](https://pan.baidu.com/s/17MLGaHEFEFG3yWUmLqJ7ig) | :paperclip: [LaSOT](https://drive.google.com/file/d/1Mu-tOpgMdfnlUGS1Tr6ugexmzhlqVUDu/view?usp=sharing) |\r\n\r\n- We add resutls of SiamFCRes22W on recent benchmarks.\r\n- Download pretrained on GOT10K [model](https://drive.google.com/file/d/1sfmy1nImNw_mstGLEOgpmJydQvSuaobI/view?usp=sharing) and [hyper-parameters](https://drive.google.com/file/d/1UAswI6Dd-TkbDa8oqRgSPPLUgiM9p--L/view?usp=sharing).\r\n\r\n\r\n#### Environment\r\nThe code is developed with Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz GPU: NVIDIA .GTX1080\r\n\r\n\r\n\r\n## Quick Start\r\n### Test\r\nSee details in [test.md](lib/tutorials/test.md)\r\n\r\n### Train\r\nSee details in [train.md](lib/tutorials/train.md)\r\n\r\n:cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud:\r\n\r\n## Citation\r\nIf any part of our paper and code is helpful to your work, please generously cite with:\r\n\r\n```\r\n@InProceedings{SiamDW_2019_CVPR,\r\nauthor = {Zhang, Zhipeng and Peng, Houwen},\r\ntitle = {Deeper and Wider Siamese Networks for Real-Time Visual Tracking},\r\nbooktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\r\nmonth = {June},\r\nyear = {2019}\r\n} \r\n```\r\n\r\n## License\r\nLicensed under an MIT license.\r\n\r\n\r\n\r\n",
            "readme_url": "https://github.com/researchmm/SiamDW",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "Going Deeper with Convolutions",
            "arxiv": "1409.4842",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.4842v1",
            "abstract": "We propose a deep convolutional neural network architecture codenamed\n\"Inception\", which was responsible for setting the new state of the art for\nclassification and detection in the ImageNet Large-Scale Visual Recognition\nChallenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the\nimproved utilization of the computing resources inside the network. This was\nachieved by a carefully crafted design that allows for increasing the depth and\nwidth of the network while keeping the computational budget constant. To\noptimize quality, the architectural decisions were based on the Hebbian\nprinciple and the intuition of multi-scale processing. One particular\nincarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22\nlayers deep network, the quality of which is assessed in the context of\nclassification and detection.",
            "authors": [
                "Christian Szegedy",
                "Wei Liu",
                "Yangqing Jia",
                "Pierre Sermanet",
                "Scott Reed",
                "Dragomir Anguelov",
                "Dumitru Erhan",
                "Vincent Vanhoucke",
                "Andrew Rabinovich"
            ]
        },
        {
            "year": "2019",
            "month": "June",
            "booktitle": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "title": "Deeper and Wider Siamese Networks for Real-Time Visual Tracking",
            "author": [
                "Zhang, Zhipeng",
                "Peng, Houwen"
            ],
            "ENTRYTYPE": "inproceedings",
            "ID": "SiamDW_2019_CVPR",
            "authors": [
                "Zhang, Zhipeng",
                "Peng, Houwen"
            ]
        },
        {
            "title": "[codes and models]",
            "url": "https://github.com/researchmm/VOT2019"
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999945134154979,
        "task": "Object Detection",
        "task_prob": 0.9690911477286083
    },
    "training": {
        "datasets": [
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "COCO"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ImageNet"
            }
        ]
    }
}