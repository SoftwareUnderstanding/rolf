{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "AdaIN-style",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "xunhuang1995",
                "owner_type": "User",
                "name": "AdaIN-style",
                "url": "https://github.com/xunhuang1995/AdaIN-style",
                "stars": 1220,
                "pushed_at": "2017-10-30 11:38:53+00:00",
                "created_at": "2017-03-20 16:16:37+00:00",
                "language": "Lua",
                "description": "Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization",
                "license": "MIT License",
                "frameworks": []
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "bee51dbbeb1c06d54b8b657cee924fc1ac82fae8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xunhuang1995/AdaIN-style/blob/master/.gitignore"
                    }
                },
                "size": 136
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "aef9eb50e378c59ef1598f92f727fcdd5a4b065d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xunhuang1995/AdaIN-style/blob/master/LICENSE"
                    }
                },
                "size": 1066
            },
            {
                "type": "code",
                "name": "examples",
                "sha": "27c9a829c4911a455277616e253cd86c1fb01fd4",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xunhuang1995/AdaIN-style/tree/master/examples"
                    }
                },
                "num_files": 24
            },
            {
                "type": "code",
                "name": "input",
                "sha": "6fb00214fa8764708b88726042bfccac19d5cc5b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xunhuang1995/AdaIN-style/tree/master/input"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "lib",
                "sha": "e48cc79c3367cb0d4a68929353203bcdb17fa672",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xunhuang1995/AdaIN-style/tree/master/lib"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "models",
                "sha": "2fb234da5ebc3adc1470a6b72b3a2f31379d193d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xunhuang1995/AdaIN-style/tree/master/models"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "styVid.sh",
                "sha": "361709d6954a146148c0897c3d5039e7526d24e6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xunhuang1995/AdaIN-style/blob/master/styVid.sh"
                    }
                },
                "size": 2016
            },
            {
                "type": "code",
                "name": "test.lua",
                "sha": "89bf1f341b551330b767b521056a50f073c029d4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xunhuang1995/AdaIN-style/blob/master/test.lua"
                    }
                },
                "size": 8676
            },
            {
                "type": "code",
                "name": "testVid.lua",
                "sha": "06ec7c8d16288711d4fe1dd67297186d02266dfe",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xunhuang1995/AdaIN-style/blob/master/testVid.lua"
                    }
                },
                "size": 13217
            },
            {
                "type": "code",
                "name": "train.lua",
                "sha": "b053fef8f07e0c4b238e7cad6b4cafab8d8531e3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xunhuang1995/AdaIN-style/blob/master/train.lua"
                    }
                },
                "size": 11740
            }
        ]
    },
    "authors": [
        {
            "name": "Xun Huang",
            "email": "xunh@nvidia.com",
            "github_id": "xunhuang1995"
        },
        {
            "name": "Elmar Hau\u00dfmann",
            "github_id": "elmarhaussmann"
        },
        {
            "name": "Srinivas",
            "github_id": "gsssrao"
        },
        {
            "name": "Alexander Kuranov",
            "github_id": "kuralex"
        }
    ],
    "tags": [
        "style-transfer",
        "neural-style",
        "generative-art",
        "generative-model",
        "deep-learning"
    ],
    "description": "Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/xunhuang1995/AdaIN-style",
            "stars": 1220,
            "issues": true,
            "readme": "# AdaIN-style\nThis repository contains the code (in [Torch](http://torch.ch/)) for the paper:\n\n[**Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization**](https://arxiv.org/abs/1703.06868)\n<br>\n[Xun Huang](http://www.cs.cornell.edu/~xhuang/),\n[Serge Belongie](http://blogs.cornell.edu/techfaculty/serge-belongie/)\n<br>\nICCV 2017 (**Oral**)\n\nThis paper proposes the first real-time style transfer algorithm that can transfer *arbitrary* new styles, in contrast to [a single style](https://arxiv.org/abs/1603.03417) or [32 styles](https://arxiv.org/abs/1610.07629). Our algorithm runs at 15 FPS with 512x512 images on a Pascal Titan X. This is around 720x speedup compared with the [original algorithm](https://arxiv.org/abs/1508.06576) of Gatys et al., without sacrificing any flexibility. We accomplish this with a novel *adaptive instance normalization* (AdaIN) layer, which is similar to [instance normalization](https://arxiv.org/abs/1701.02096) but with affine parameters adaptively computed from the feature representations of an arbitrary style image.\n<p align='center'>\n  <img src='examples/architecture.jpg' width=\"600px\">\n</p>\n\n## Examples\n<p align='center'>\n  <img src='examples/avril_cropped.jpg' width=\"140px\">\n  <img src='examples/impronte_d_artista_cropped.jpg' width=\"140px\">\n  <img src='examples/avril_stylized_impronte_d_artista.jpg' width=\"140px\">\n  <img src='examples/cornell_cropped.jpg' width=\"140px\">\n  <img src='examples/woman_with_hat_matisse_cropped.jpg' width=\"140px\">\n  <img src='examples/cornell_stylized_woman_with_hat_matisse.jpg' width=\"140px\">\n</p>\n\n<p align='center'>\n  <img src='examples/chicago_cropped.jpg' width=\"140px\">\n  <img src='examples/ashville_cropped.jpg' width=\"140px\">\n  <img src='examples/chicago_stylized_asheville.jpg' width=\"140px\">\n  <img src='examples/sailboat_cropped.jpg' width=\"140px\">\n  <img src='examples/sketch_cropped.png' width=\"140px\">\n  <img src='examples/sailboat_stylized_sketch.jpg' width=\"140px\">\n</p>\n\n<p align='center'>\n  <img src='examples/modern_cropped.jpg' width=\"140px\">\n  <img src='examples/goeritz_cropped.jpg' width=\"140px\">\n  <img src='examples/modern_stylized_goeritz.jpg' width=\"140px\">\n  <img src='examples/lenna_cropped.jpg' width=\"140px\">\n  <img src='examples/en_campo_gris_cropped.jpg', width=\"140px\">\n  <img src='examples/lenna_stylized_en_campo_gris.jpg' width=\"140px\">\n</p>\n\n## Dependencies\n* [torch7](https://github.com/torch/torch7)\n\nOptionally:\n* CUDA and cuDNN\n* [cunn](https://github.com/torch/cunn)\n* [torch.cudnn](https://github.com/soumith/cudnn.torch)\n* [ffmpeg](https://ffmpeg.org/) (for video)\n\n\n## Download\n```\nbash models/download_models.sh\n```\n\nThis command will download a pre-trained decoder as well as a modified VGG-19 network. Our style transfer network consists of the first few layers of VGG, an AdaIN layer, and the provided decoder.\n\n## Usage\n### Basic usage\nUse `-content` and `-style` to provide the respective path to the content and style image, for example:\n```\nth test.lua -content input/content/cornell.jpg -style input/style/woman_with_hat_matisse.jpg\n```\n\nYou can also run the code on directories of content and style images using `-contentDir` and `-styleDir`. It will save every possible combination of content and styles to the output directory.\n\n```\nth test.lua -contentDir input/content -styleDir input/style\n```\nSome other options:\n* `-crop`: Center crop both content and style images beforehand.\n* `-contentSize`: New (minimum) size for the content image. Keeping the original size if set to 0.\n* `-styleSize`: New (minimum) size for the content image. Keeping the original size if set to 0.\n\nTo see all available options, type:\n```\nth test.lua -help\n```\n\n### Content-style trade-off\nUse `-alpha` to adjust the degree of stylization. It should be a value between 0 and 1 (default). Example usage:\n```\nth test.lua -content input/content/chicago.jpg -style input/style/asheville.jpg -alpha 0.5 -crop\n```\n\nBy changing `-alpha`, you should be able to reproduce the following results.\n\n<p align='center'>\n  <img src='examples/style_weight.jpg' height=\"160px\">\n</p>\n\n### Transfer style but not color\nAdd `-preserveColor` to preserve the color of the content image. Example usage: \n```\nth test.lua -content input/content/newyork.jpg -style input/style/brushstrokes.jpg -contentSize 0 -styleSize 0 -preserveColor\n```\n\n<p align='center'>\n  <img src='input/content/newyork.jpg' height=\"200px\">\n  <img src='input/style/brushstrokes.jpg' height=\"200px\">\n</p>\n<p align='center'>\n  <img src='examples/newyork_brushstrokes_preservecolor.jpg' height=\"370px\">\n</p>\n\n### Style interpolation\nIt is possible to interpolate between several styles using `-styleInterpWeights ` that controls the relative weight of each style. Note that you also to need to provide the same number of style images separated be commas. Example usage:\n```\nth test.lua -content input/content/avril.jpg \\\n-style input/style/picasso_self_portrait.jpg,input/style/impronte_d_artista.jpg,input/style/trial.jpg,input/style/antimonocromatismo.jpg \\\n-styleInterpWeights 1,1,1,1 -crop\n```\nYou should be able to reproduce the following results shown in our paper by changing `-styleInterpWeights `.\n\n<p align='center'>\n  <img src='examples/style_interp.jpg' height=\"500px\">\n</p>\n\n### Spatial control\nUse `-mask` to provide the path to a binary foreground mask. You can transfer the foreground and background of the content image to different styles. Note that you also to need to provide two style images separated be comma, in which the first one is applied to foreground and the second one is applied to background. Example usage:\n```\nth test.lua -content input/content/blonde_girl.jpg -style input/style/woman_in_peasant_dress_cropped.jpg,input/style/mondrian_cropped.jpg \\\n-mask input/mask/mask.png -contentSize 0 -styleSize 0\n```\n\n<p align='center'>\n  <img src='examples/spatial_control.jpg' height=\"300px\">\n</p>\n\n### Video Stylization\nUse `styVid.sh` to process videos, example usage:\n```\nth testVid.lua -contentDir videoprocessing/${filename} -style ${styleimage} -outputDir videoprocessing/${filename}-${stylename}\n```\nThis generates 1 mp4 for each image present in ```style-dir-path```. Other video formats are also supported. To change other parameters like alpha, edit line 53 of ```styVid.sh```. An example video with some results can be seen [here](https://www.youtube.com/watch?v=vVkufidT0fc&t=1s) on youtube.\n\n## Training\n\n1. Download [MSCOCO images](http://mscoco.org/dataset/#download) and [Wikiart images](https://www.kaggle.com/c/painter-by-numbers).\n2. Use `th train.lua -contentDir COCO_TRAIN_DIR -styleDir WIKIART_TRAIN_DIR` to start training with default hyperparameters. Replace `COCO_TRAIN_DIR` with the path to COCO training images and `WIKIART_TRAIN_DIR` with the path to Wikiart training images. The default hyperparameters are the same as the ones used to train `decoder-content-similar.t7`. To reproduce the results from `decoder.t7`, add `-styleWeight 1e-1`.\n\n## Citation\n\nIf you find this code useful for your research, please cite the paper:\n\n```\n@inproceedings{huang2017adain,\n  title={Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization},\n  author={Huang, Xun and Belongie, Serge},\n  booktitle={ICCV},\n  year={2017}\n}\n```\n## Acknowledgement\n\nThis project is inspired by many existing style transfer methods and their open-source implementations, including:\n* [Image Style Transfer Using Convolutional Neural Networks](http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Gatys_Image_Style_Transfer_CVPR_2016_paper.html), Gatys et al. [[code](https://github.com/jcjohnson/neural-style) (by [Johnson](http://cs.stanford.edu/people/jcjohns/))]\n* [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155), [Johnson](http://cs.stanford.edu/people/jcjohns/) et al. [[code](https://github.com/jcjohnson/fast-neural-style)]\n* [Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis](https://arxiv.org/abs/1701.02096), [Ulyanov](https://dmitryulyanov.github.io/about/) et al. [[code](https://github.com/DmitryUlyanov/texture_nets)]\n* [A Learned Representation For Artistic Style](https://openreview.net/forum?id=BJO-BuT1g&noteId=BJO-BuT1g), [Dumoulin](http://vdumoulin.github.io/) et al. [[code](https://github.com/tensorflow/magenta/tree/master/magenta/models/image_stylization)]\n* [Fast Patch-based Style Transfer of Arbitrary Style](https://arxiv.org/abs/1612.04337), Chen and [Schmidt](http://www.cs.ubc.ca/~schmidtm/) [[code](https://github.com/rtqichen/style-swap)]\n* [Controlling Perceptual Factors in Neural Style Transfer](https://arxiv.org/abs/1611.07865), Gatys et al. [[code](https://github.com/leongatys/NeuralImageSynthesis)]\n\n## Contact\n\nIf you have any questions or suggestions about the paper, feel free to reach me (xh258@cornell.edu).\n",
            "readme_url": "https://github.com/xunhuang1995/AdaIN-style",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis",
            "arxiv": "1701.02096",
            "year": 2017,
            "url": "http://arxiv.org/abs/1701.02096v2",
            "abstract": "The recent work of Gatys et al., who characterized the style of an image by\nthe statistics of convolutional neural network filters, ignited a renewed\ninterest in the texture generation and image stylization problems. While their\nimage generation technique uses a slow optimization process, recently several\nauthors have proposed to learn generator neural networks that can produce\nsimilar outputs in one quick forward pass. While generator networks are\npromising, they are still inferior in visual quality and diversity compared to\ngeneration-by-optimization. In this work, we advance them in two significant\nways. First, we introduce an instance normalization module to replace batch\nnormalization with significant improvements to the quality of image\nstylization. Second, we improve diversity by introducing a new learning\nformulation that encourages generators to sample unbiasedly from the Julesz\ntexture ensemble, which is the equivalence class of all images characterized by\ncertain filter responses. Together, these two improvements take feed forward\ntexture synthesis and image stylization much closer to the quality of\ngeneration-via-optimization, while retaining the speed advantage.",
            "authors": [
                "Dmitry Ulyanov",
                "Andrea Vedaldi",
                "Victor Lempitsky"
            ]
        },
        {
            "title": "Controlling Perceptual Factors in Neural Style Transfer",
            "arxiv": "1611.07865",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.07865v2",
            "abstract": "Neural Style Transfer has shown very exciting results enabling new forms of\nimage manipulation. Here we extend the existing method to introduce control\nover spatial location, colour information and across spatial scale. We\ndemonstrate how this enhances the method by allowing high-resolution controlled\nstylisation and helps to alleviate common failure cases such as applying ground\ntextures to sky regions. Furthermore, by decomposing style into these\nperceptual factors we enable the combination of style information from multiple\nsources to generate new, perceptually appealing styles from existing ones. We\nalso describe how these methods can be used to more efficiently produce large\nsize, high-quality stylisation. Finally we show how the introduced control\nmeasures can be applied in recent methods for Fast Neural Style Transfer.",
            "authors": [
                "Leon A. Gatys",
                "Alexander S. Ecker",
                "Matthias Bethge",
                "Aaron Hertzmann",
                "Eli Shechtman"
            ]
        },
        {
            "title": "Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization",
            "arxiv": "1703.06868",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.06868v2",
            "abstract": "Gatys et al. recently introduced a neural algorithm that renders a content\nimage in the style of another image, achieving so-called style transfer.\nHowever, their framework requires a slow iterative optimization process, which\nlimits its practical application. Fast approximations with feed-forward neural\nnetworks have been proposed to speed up neural style transfer. Unfortunately,\nthe speed improvement comes at a cost: the network is usually tied to a fixed\nset of styles and cannot adapt to arbitrary new styles. In this paper, we\npresent a simple yet effective approach that for the first time enables\narbitrary style transfer in real-time. At the heart of our method is a novel\nadaptive instance normalization (AdaIN) layer that aligns the mean and variance\nof the content features with those of the style features. Our method achieves\nspeed comparable to the fastest existing approach, without the restriction to a\npre-defined set of styles. In addition, our approach allows flexible user\ncontrols such as content-style trade-off, style interpolation, color & spatial\ncontrols, all using a single feed-forward neural network.",
            "authors": [
                "Xun Huang",
                "Serge Belongie"
            ]
        },
        {
            "title": "Texture Networks: Feed-forward Synthesis of Textures and Stylized Images",
            "arxiv": "1603.03417",
            "year": 2016,
            "url": "http://arxiv.org/abs/1603.03417v1",
            "abstract": "Gatys et al. recently demonstrated that deep networks can generate beautiful\ntextures and stylized images from a single texture example. However, their\nmethods requires a slow and memory-consuming optimization process. We propose\nhere an alternative approach that moves the computational burden to a learning\nstage. Given a single example of a texture, our approach trains compact\nfeed-forward convolutional networks to generate multiple samples of the same\ntexture of arbitrary size and to transfer artistic style from a given image to\nany other image. The resulting networks are remarkably light-weight and can\ngenerate textures of quality comparable to Gatys~et~al., but hundreds of times\nfaster. More generally, our approach highlights the power and flexibility of\ngenerative feed-forward models trained with complex and expressive loss\nfunctions.",
            "authors": [
                "Dmitry Ulyanov",
                "Vadim Lebedev",
                "Andrea Vedaldi",
                "Victor Lempitsky"
            ]
        },
        {
            "title": "A Neural Algorithm of Artistic Style",
            "arxiv": "1508.06576",
            "year": 2015,
            "url": "http://arxiv.org/abs/1508.06576v2",
            "abstract": "In fine art, especially painting, humans have mastered the skill to create\nunique visual experiences through composing a complex interplay between the\ncontent and style of an image. Thus far the algorithmic basis of this process\nis unknown and there exists no artificial system with similar capabilities.\nHowever, in other key areas of visual perception such as object and face\nrecognition near-human performance was recently demonstrated by a class of\nbiologically inspired vision models called Deep Neural Networks. Here we\nintroduce an artificial system based on a Deep Neural Network that creates\nartistic images of high perceptual quality. The system uses neural\nrepresentations to separate and recombine content and style of arbitrary\nimages, providing a neural algorithm for the creation of artistic images.\nMoreover, in light of the striking similarities between performance-optimised\nartificial neural networks and biological vision, our work offers a path\nforward to an algorithmic understanding of how humans create and perceive\nartistic imagery.",
            "authors": [
                "Leon A. Gatys",
                "Alexander S. Ecker",
                "Matthias Bethge"
            ]
        },
        {
            "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution",
            "arxiv": "1603.08155",
            "year": 2016,
            "url": "http://arxiv.org/abs/1603.08155v1",
            "abstract": "We consider image transformation problems, where an input image is\ntransformed into an output image. Recent methods for such problems typically\ntrain feed-forward convolutional neural networks using a \\emph{per-pixel} loss\nbetween the output and ground-truth images. Parallel work has shown that\nhigh-quality images can be generated by defining and optimizing\n\\emph{perceptual} loss functions based on high-level features extracted from\npretrained networks. We combine the benefits of both approaches, and propose\nthe use of perceptual loss functions for training feed-forward networks for\nimage transformation tasks. We show results on image style transfer, where a\nfeed-forward network is trained to solve the optimization problem proposed by\nGatys et al in real-time. Compared to the optimization-based method, our\nnetwork gives similar qualitative results but is three orders of magnitude\nfaster. We also experiment with single-image super-resolution, where replacing\na per-pixel loss with a perceptual loss gives visually pleasing results.",
            "authors": [
                "Justin Johnson",
                "Alexandre Alahi",
                "Li Fei-Fei"
            ]
        },
        {
            "title": "A Learned Representation For Artistic Style",
            "arxiv": "1610.07629",
            "year": 2016,
            "url": "http://arxiv.org/abs/1610.07629v5",
            "abstract": "The diversity of painting styles represents a rich visual vocabulary for the\nconstruction of an image. The degree to which one may learn and parsimoniously\ncapture this visual vocabulary measures our understanding of the higher level\nfeatures of paintings, if not images in general. In this work we investigate\nthe construction of a single, scalable deep network that can parsimoniously\ncapture the artistic style of a diversity of paintings. We demonstrate that\nsuch a network generalizes across a diversity of artistic styles by reducing a\npainting to a point in an embedding space. Importantly, this model permits a\nuser to explore new painting styles by arbitrarily combining the styles learned\nfrom individual paintings. We hope that this work provides a useful step\ntowards building rich models of paintings and offers a window on to the\nstructure of the learned representation of artistic style.",
            "authors": [
                "Vincent Dumoulin",
                "Jonathon Shlens",
                "Manjunath Kudlur"
            ]
        },
        {
            "title": "Fast Patch-based Style Transfer of Arbitrary Style",
            "arxiv": "1612.04337",
            "year": 2016,
            "url": "http://arxiv.org/abs/1612.04337v1",
            "abstract": "Artistic style transfer is an image synthesis problem where the content of an\nimage is reproduced with the style of another. Recent works show that a\nvisually appealing style transfer can be achieved by using the hidden\nactivations of a pretrained convolutional neural network. However, existing\nmethods either apply (i) an optimization procedure that works for any style\nimage but is very expensive, or (ii) an efficient feedforward network that only\nallows a limited number of trained styles. In this work we propose a simpler\noptimization objective based on local matching that combines the content\nstructure and style textures in a single layer of the pretrained network. We\nshow that our objective has desirable properties such as a simpler optimization\nlandscape, intuitive parameter tuning, and consistent frame-by-frame\nperformance on video. Furthermore, we use 80,000 natural images and 80,000\npaintings to train an inverse network that approximates the result of the\noptimization. This results in a procedure for artistic style transfer that is\nefficient but also allows arbitrary content and style images.",
            "authors": [
                "Tian Qi Chen",
                "Mark Schmidt"
            ]
        },
        {
            "year": "2017",
            "booktitle": "ICCV",
            "author": [
                "Huang, Xun",
                "Belongie, Serge"
            ],
            "title": "Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization",
            "ENTRYTYPE": "inproceedings",
            "ID": "huang2017adain",
            "authors": [
                "Huang, Xun",
                "Belongie, Serge"
            ]
        },
        {
            "title": "torch7",
            "url": "https://github.com/torch/torch7"
        },
        {
            "title": "cunn",
            "url": "https://github.com/torch/cunn"
        },
        {
            "title": "torch.cudnn",
            "url": "https://github.com/soumith/cudnn.torch"
        },
        {
            "title": "ffmpeg",
            "url": "https://ffmpeg.org/"
        },
        {
            "title": "Image Style Transfer Using Convolutional Neural Networks",
            "url": "http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Gatys_Image_Style_Transfer_CVPR_2016_paper.html"
        },
        {
            "title": "A Learned Representation For Artistic Style",
            "url": "https://openreview.net/forum?id=BJO-BuT1g&noteId=BJO-BuT1g"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "COCO"
            },
            {
                "name": "MSCOCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999334905417818,
        "task": "Image-to-Image Translation",
        "task_prob": 0.6050281551476993
    }
}