{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "**CycleGAN-VC2-PyTorch**",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "jackaduma",
                "owner_type": "User",
                "name": "CycleGAN-VC2",
                "url": "https://github.com/jackaduma/CycleGAN-VC2",
                "stars": 274,
                "pushed_at": "2021-10-13 11:22:35+00:00",
                "created_at": "2020-05-13 11:29:35+00:00",
                "language": "Python",
                "description": "Voice Conversion by CycleGAN (\u8bed\u97f3\u514b\u9686/\u8bed\u97f3\u8f6c\u6362): CycleGAN-VC2",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "5639821eb1978fd1bd92e4fb219afafa8d0b2774",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jackaduma/CycleGAN-VC2/blob/master/LICENSE"
                    }
                },
                "size": 1063
            },
            {
                "type": "code",
                "name": "README.zh-CN.md",
                "sha": "8165cf680b5b9e2c3f2e6c9980c239ef88571c87",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jackaduma/CycleGAN-VC2/blob/master/README.zh-CN.md"
                    }
                },
                "size": 5094
            },
            {
                "type": "code",
                "name": "cache",
                "sha": "375e1b34dbd4480dc698731d2b799a67b12e7729",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jackaduma/CycleGAN-VC2/tree/master/cache"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "converted_sound",
                "sha": "8992d171729d656f0a0ad6f249a48e852beef193",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jackaduma/CycleGAN-VC2/tree/master/converted_sound"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "data",
                "sha": "b5bad7cc7108173fe33c2b2c092c9ae43077bf3d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jackaduma/CycleGAN-VC2/tree/master/data"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "misc",
                "sha": "ca1ed72b2f2427a542fc8fe4e293465bb6ebfb23",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jackaduma/CycleGAN-VC2/tree/master/misc"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "model_tf.py",
                "sha": "e4f9ea89a09e68d2e5bb723be7c477e456479880",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jackaduma/CycleGAN-VC2/blob/master/model_tf.py"
                    }
                },
                "size": 19580
            },
            {
                "type": "code",
                "name": "preprocess.py",
                "sha": "bde93677ba07ebfd9349d0f0bafed04f41f2eca0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jackaduma/CycleGAN-VC2/blob/master/preprocess.py"
                    }
                },
                "size": 6653
            },
            {
                "type": "code",
                "name": "preprocess_training.py",
                "sha": "c6909ce73cc73a7941045934d340b66b520ad711",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jackaduma/CycleGAN-VC2/blob/master/preprocess_training.py"
                    }
                },
                "size": 3807
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "61e70b34e9b97993d2c03dd7d5e3f6aa2264312b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jackaduma/CycleGAN-VC2/blob/master/requirements.txt"
                    }
                },
                "size": 26
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "3d03ae448413e4af4a5b107e9b49af2daff24ccd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jackaduma/CycleGAN-VC2/blob/master/train.py"
                    }
                },
                "size": 25595
            },
            {
                "type": "code",
                "name": "trainingDataset.py",
                "sha": "1db73b8d8859d8e142fdcc2551b39c438a3c14fd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jackaduma/CycleGAN-VC2/blob/master/trainingDataset.py"
                    }
                },
                "size": 2391
            }
        ]
    },
    "authors": [
        {
            "name": "Kun",
            "email": "jackaduma@gmail.com",
            "github_id": "jackaduma"
        },
        {
            "name": "Jeffery-zhang-nfls",
            "github_id": "Jeffery-zhang-nfls"
        }
    ],
    "tags": [
        "voice-conversion",
        "cyclegan-vc2",
        "cyclegan",
        "gan",
        "deeplearning",
        "voice-cloning",
        "pytorch-implementation",
        "cyclegan-vc",
        "speech-synthesis",
        "deep-learning",
        "pix2pix"
    ],
    "description": "Voice Conversion by CycleGAN (\u8bed\u97f3\u514b\u9686/\u8bed\u97f3\u8f6c\u6362): CycleGAN-VC2",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/jackaduma/CycleGAN-VC2",
            "stars": 274,
            "issues": true,
            "readme": "# **CycleGAN-VC2-PyTorch**\n\n[![standard-readme compliant](https://img.shields.io/badge/readme%20style-standard-brightgreen.svg?style=flat-square)](https://github.com/jackaduma/CycleGAN-VC2)\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://paypal.me/jackaduma?locale.x=zh_XC)\n\n[**\u4e2d\u6587\u8bf4\u660e**](./README.zh-CN.md) | [**English**](./README.md)\n\n------\n\nThis code is a **PyTorch** implementation for paper: [CycleGAN-VC2: Improved CycleGAN-based Non-parallel Voice Conversion](https://arxiv.org/abs/1904.04631]), a nice work on **Voice-Conversion/Voice Cloning**.\n\n- [x] Dataset\n  - [ ] VC\n  - [x] Chinese Male Speakers (S0913 from [AISHELL-Speech](https://openslr.org/33/) & [GaoXiaoSong: a Chinese star](https://en.wikipedia.org/wiki/Gao_Xiaosong))\n- [x] Usage\n  - [x] Training\n  - [x] Example \n- [ ] Demo\n- [x] Reference\n\n------\n\n## **Update**\n\n**2020.11.17**: fixed issues: re-implements the second step adverserial loss.\n\n**2020.08.27**: add the second step adverserial loss by [Jeffery-zhang-nfls](https://github.com/Jeffery-zhang-nfls)\n\n## **CycleGAN-VC2**\n\n### [**Project Page**](http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc2/index.html)\n\n\nTo advance the research on non-parallel VC, we propose CycleGAN-VC2, which is an improved version of CycleGAN-VC incorporating three new techniques: an improved objective (two-step adversarial losses), improved generator (2-1-2D CNN), and improved discriminator (Patch GAN).\n\n\n![network](http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc2/images/network.png \"network\")\n\n------\n\n**This repository contains:** \n\n1. [model code](model_tf.py) which implemented the paper.\n2. [audio preprocessing script](preprocess_training.py) you can use to create cache for [training data](data).\n3. [training scripts](train.py) to train the model.\n4. [Examples of Voice Conversion](converted_sound/) - converted result after training.\n\n------\n\n## **Table of Contents**\n\n- [**CycleGAN-VC2-PyTorch**](#cyclegan-vc2-pytorch)\n  - [**CycleGAN-VC2**](#cyclegan-vc2)\n    - [**Project Page**](#project-page)\n  - [**Table of Contents**](#table-of-contents)\n  - [**Requirement**](#requirement)\n  - [**Usage**](#usage)\n    - [**preprocess**](#preprocess)\n    - [**train**](#train)\n  - [**Pretrained**](#pretrained)\n  - [**Demo**](#demo)\n  - [**Reference**](#reference)\n  - [**Donation**](#donation) \n  - [**License**](#license)\n  \n------\n\n\n\n## **Requirement** \n\n```bash\npip install -r requirements.txt\n```\n## **Usage**\n\n### **preprocess**\n\n```python\npython preprocess_training.py\n```\nis short for\n\n```python\npython preprocess_training.py --train_A_dir ./data/S0913/ --train_B_dir ./data/gaoxiaosong/ --cache_folder ./cache/\n```\n\n\n### **train** \n```python\npython train.py\n```\n\nis short for\n\n```python\npython train.py --logf0s_normalization ./cache/logf0s_normalization.npz --mcep_normalization ./cache/mcep_normalization.npz --coded_sps_A_norm ./cache/coded_sps_A_norm.pickle --coded_sps_B_norm ./cache/coded_sps_B_norm.pickle --model_checkpoint ./model_checkpoint/ --resume_training_at ./model_checkpoint/_CycleGAN_CheckPoint --validation_A_dir ./data/S0913/ --output_A_dir ./converted_sound/S0913 --validation_B_dir ./data/gaoxiaosong/ --output_B_dir ./converted_sound/gaoxiaosong/\n```\n\n------\n\n## **Pretrained**\n\na pretrained model which converted between S0913 and GaoXiaoSong\n\ndownload from [Google Drive](https://drive.google.com/file/d/1iamizL98NWIPw4pw0nF-7b6eoBJrxEfj/view?usp=sharing) <735MB>\n\n------\n\n## **Demo**\n\nSamples:\n\n\n**reference speaker A:** [S0913(./data/S0913/BAC009S0913W0351.wav)](https://drive.google.com/file/d/14zU1mI8QtoBwb8cHkNdZiPmXI6Mj6pVW/view?usp=sharing)\n\n**reference speaker B:** [GaoXiaoSong(./data/gaoxiaosong/gaoxiaosong_1.wav)](https://drive.google.com/file/d/1s0ip6JwnWmYoWFcEQBwVIIdHJSqPThR3/view?usp=sharing)\n\n\n\n**speaker A's speech changes to speaker B's voice:** [Converted from S0913 to GaoXiaoSong (./converted_sound/S0913/BAC009S0913W0351.wav)](https://drive.google.com/file/d/1S4vSNGM-T0RTo_aclxRgIPkUJ7NEqmjU/view?usp=sharing)\n\n------\n\n## **Reference**\n1. **CycleGAN-VC2: Improved CycleGAN-based Non-parallel Voice Conversion**. [Paper](https://arxiv.org/abs/1904.04631), [Project](http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc2/index.html)\n2. Parallel-Data-Free Voice Conversion Using Cycle-Consistent Adversarial Networks. [Paper](https://arxiv.org/abs/1711.11293), [Project](http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc/)\n3. Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. [Paper](https://arxiv.org/abs/1703.10593), [Project](https://junyanz.github.io/CycleGAN/), [Code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\n4. Image-to-Image Translation with Conditional Adversarial Nets. [Paper](https://arxiv.org/abs/1611.07004), [Project](https://phillipi.github.io/pix2pix/), [Code](https://github.com/phillipi/pix2pix)\n\n------\n\n## Donation\nIf this project help you reduce time to develop, you can give me a cup of coffee :) \n\nAliPay(\u652f\u4ed8\u5b9d)\n<div align=\"center\">\n\t<img src=\"./misc/ali_pay.png\" alt=\"ali_pay\" width=\"400\" />\n</div>\n\nWechatPay(\u5fae\u4fe1)\n<div align=\"center\">\n    <img src=\"./misc/wechat_pay.png\" alt=\"wechat_pay\" width=\"400\" />\n</div>\n\n[![paypal](https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif)](https://paypal.me/jackaduma?locale.x=zh_XC)\n\n------\n\n## **License**\n\n[MIT](LICENSE) \u00a9 Kun\n",
            "readme_url": "https://github.com/jackaduma/CycleGAN-VC2",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
            "arxiv": "1703.10593",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.10593v7",
            "abstract": "Image-to-image translation is a class of vision and graphics problems where\nthe goal is to learn the mapping between an input image and an output image\nusing a training set of aligned image pairs. However, for many tasks, paired\ntraining data will not be available. We present an approach for learning to\ntranslate an image from a source domain $X$ to a target domain $Y$ in the\nabsence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$\nsuch that the distribution of images from $G(X)$ is indistinguishable from the\ndistribution $Y$ using an adversarial loss. Because this mapping is highly\nunder-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$\nand introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice\nversa). Qualitative results are presented on several tasks where paired\ntraining data does not exist, including collection style transfer, object\ntransfiguration, season transfer, photo enhancement, etc. Quantitative\ncomparisons against several prior methods demonstrate the superiority of our\napproach.",
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A. Efros"
            ]
        },
        {
            "title": "Parallel-Data-Free Voice Conversion Using Cycle-Consistent Adversarial Networks",
            "arxiv": "1711.11293",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.11293v2",
            "abstract": "We propose a parallel-data-free voice-conversion (VC) method that can learn a\nmapping from source to target speech without relying on parallel data. The\nproposed method is general purpose, high quality, and parallel-data free and\nworks without any extra data, modules, or alignment procedure. It also avoids\nover-smoothing, which occurs in many conventional statistical model-based VC\nmethods. Our method, called CycleGAN-VC, uses a cycle-consistent adversarial\nnetwork (CycleGAN) with gated convolutional neural networks (CNNs) and an\nidentity-mapping loss. A CycleGAN learns forward and inverse mappings\nsimultaneously using adversarial and cycle-consistency losses. This makes it\npossible to find an optimal pseudo pair from unpaired data. Furthermore, the\nadversarial loss contributes to reducing over-smoothing of the converted\nfeature sequence. We configure a CycleGAN with gated CNNs and train it with an\nidentity-mapping loss. This allows the mapping function to capture sequential\nand hierarchical structures while preserving linguistic information. We\nevaluated our method on a parallel-data-free VC task. An objective evaluation\nshowed that the converted feature sequence was near natural in terms of global\nvariance and modulation spectra. A subjective evaluation showed that the\nquality of the converted speech was comparable to that obtained with a Gaussian\nmixture model-based method under advantageous conditions with parallel and\ntwice the amount of data.",
            "authors": [
                "Takuhiro Kaneko",
                "Hirokazu Kameoka"
            ]
        },
        {
            "title": "CycleGAN-VC2: Improved CycleGAN-based Non-parallel Voice Conversion",
            "arxiv": "1904.04631",
            "year": 2019,
            "url": "http://arxiv.org/abs/1904.04631v1",
            "abstract": "Non-parallel voice conversion (VC) is a technique for learning the mapping\nfrom source to target speech without relying on parallel data. This is an\nimportant task, but it has been challenging due to the disadvantages of the\ntraining conditions. Recently, CycleGAN-VC has provided a breakthrough and\nperformed comparably to a parallel VC method without relying on any extra data,\nmodules, or time alignment procedures. However, there is still a large gap\nbetween the real target and converted speech, and bridging this gap remains a\nchallenge. To reduce this gap, we propose CycleGAN-VC2, which is an improved\nversion of CycleGAN-VC incorporating three new techniques: an improved\nobjective (two-step adversarial losses), improved generator (2-1-2D CNN), and\nimproved discriminator (PatchGAN). We evaluated our method on a non-parallel VC\ntask and analyzed the effect of each technique in detail. An objective\nevaluation showed that these techniques help bring the converted feature\nsequence closer to the target in terms of both global and local structures,\nwhich we assess by using Mel-cepstral distortion and modulation spectra\ndistance, respectively. A subjective evaluation showed that CycleGAN-VC2\noutperforms CycleGAN-VC in terms of naturalness and similarity for every\nspeaker pair, including intra-gender and inter-gender pairs.",
            "authors": [
                "Takuhiro Kaneko",
                "Hirokazu Kameoka",
                "Kou Tanaka",
                "Nobukatsu Hojo"
            ]
        },
        {
            "title": "Image-to-Image Translation with Conditional Adversarial Networks",
            "arxiv": "1611.07004",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.07004v3",
            "abstract": "We investigate conditional adversarial networks as a general-purpose solution\nto image-to-image translation problems. These networks not only learn the\nmapping from input image to output image, but also learn a loss function to\ntrain this mapping. This makes it possible to apply the same generic approach\nto problems that traditionally would require very different loss formulations.\nWe demonstrate that this approach is effective at synthesizing photos from\nlabel maps, reconstructing objects from edge maps, and colorizing images, among\nother tasks. Indeed, since the release of the pix2pix software associated with\nthis paper, a large number of internet users (many of them artists) have posted\ntheir own experiments with our system, further demonstrating its wide\napplicability and ease of adoption without the need for parameter tweaking. As\na community, we no longer hand-engineer our mapping functions, and this work\nsuggests we can achieve reasonable results without hand-engineering our loss\nfunctions either.",
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A. Efros"
            ]
        },
        {
            "title": "S0913(./data/S0913/BAC009S0913W0351.wav)",
            "url": "https://drive.google.com/file/d/14zU1mI8QtoBwb8cHkNdZiPmXI6Mj6pVW/view?usp=sharing"
        },
        {
            "title": "GaoXiaoSong(./data/gaoxiaosong/gaoxiaosong_1.wav)",
            "url": "https://drive.google.com/file/d/1s0ip6JwnWmYoWFcEQBwVIIdHJSqPThR3/view?usp=sharing"
        },
        {
            "title": "Converted from S0913 to GaoXiaoSong (./converted_sound/S0913/BAC009S0913W0351.wav)",
            "url": "https://drive.google.com/file/d/1S4vSNGM-T0RTo_aclxRgIPkUJ7NEqmjU/view?usp=sharing"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Wikipedia"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9998509453954919,
        "task": "Image-to-Image Translation",
        "task_prob": 0.9895389923346943
    }
}