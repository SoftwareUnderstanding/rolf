{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Reformer, the Efficient Transformer, in Pytorch",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "lucidrains",
                "owner_type": "User",
                "name": "reformer-pytorch",
                "url": "https://github.com/lucidrains/reformer-pytorch",
                "stars": 1703,
                "pushed_at": "2021-11-06 23:08:35+00:00",
                "created_at": "2020-01-09 20:42:37+00:00",
                "language": "Python",
                "description": "Reformer, the efficient Transformer, in Pytorch",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitattributes",
                "sha": "2f77e919cdc5d76d29bfc56dbc96e814aee0dc9a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/reformer-pytorch/blob/master/.gitattributes"
                    }
                },
                "size": 31
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "05e746b5a733ca27c94fcd43dd4a00e4c40302d7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/reformer-pytorch/tree/master/.github"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "a6d8e60a918e66ded4351a8383f65e63bb66559e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/reformer-pytorch/blob/master/.gitignore"
                    }
                },
                "size": 1975
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "145245790672b7d569096d08199b38a35d6f4d36",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/reformer-pytorch/blob/master/LICENSE"
                    }
                },
                "size": 1069
            },
            {
                "type": "code",
                "name": "examples",
                "sha": "19a2867e43a20ae2a02537ab501f4e83c2779540",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/reformer-pytorch/tree/master/examples"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "lsh_attention.png",
                "sha": "a906178ef0ab5e58a724ddbb056218ebb76e65c6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/reformer-pytorch/blob/master/lsh_attention.png"
                    }
                },
                "size": 126385
            },
            {
                "type": "code",
                "name": "pretraining",
                "sha": "ee9c7e456c4cd9446ed00c749f0b927c158f1670",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/reformer-pytorch/tree/master/pretraining"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "reformer_pytorch",
                "sha": "0bb5d258b4e5dc0fb091559036280283e308f6bc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/reformer-pytorch/tree/master/reformer_pytorch"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "setup.cfg",
                "sha": "9d5f797981b69a755547e7cf592e33a87675ad45",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/reformer-pytorch/blob/master/setup.cfg"
                    }
                },
                "size": 61
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "2078b85c0b6369f588db33e78c22f84e5ebc4aed",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/reformer-pytorch/blob/master/setup.py"
                    }
                },
                "size": 841
            }
        ]
    },
    "authors": [
        {
            "name": "Phil Wang",
            "email": "lucidrains@gmail.com",
            "github_id": "lucidrains"
        },
        {
            "name": "Abhishek",
            "email": "abhishekasdzxc@gmail.com",
            "github_id": "abhishekyana"
        },
        {
            "name": "Zachary Bloss",
            "email": "zacharybloss@gmail.com",
            "github_id": "zbloss"
        },
        {
            "name": "Small_yu",
            "github_id": "wangyu1997"
        },
        {
            "name": "gulby",
            "github_id": "gulby"
        },
        {
            "name": "Justin DuJardin",
            "github_id": "justindujardin"
        },
        {
            "name": "Pablo Pernias",
            "email": "pablo@pernias.com",
            "github_id": "pabloppp"
        },
        {
            "name": "Ilya Borovik",
            "email": "ilyaborovik97@gmail.com",
            "github_id": "ilya16"
        },
        {
            "name": "Emmanuel Noutahi",
            "email": "emmanuel.noutahi@hotmail.ca",
            "github_id": "maclandrol"
        },
        {
            "name": "hichiaty",
            "github_id": "hichiaty"
        }
    ],
    "tags": [
        "artificial-intelligence",
        "transformers",
        "attention-mechanism",
        "machine-learning",
        "pytorch"
    ],
    "description": "Reformer, the efficient Transformer, in Pytorch",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/lucidrains/reformer-pytorch",
            "stars": 1703,
            "issues": true,
            "readme": "## Reformer, the Efficient Transformer, in Pytorch\n[![PyPI version](https://badge.fury.io/py/reformer-pytorch.svg)](https://badge.fury.io/py/reformer-pytorch)\n\n<img src=\"./lsh_attention.png\" width=\"500\">\n\nThis is a Pytorch implementation of Reformer https://openreview.net/pdf?id=rkgNKkHtvB\n\nIt includes LSH attention, reversible network, and chunking. It has been validated with an auto-regressive task (enwik8).\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1am1DRl80Kd3o6n_4u3MomPzYS0NfdHAC) 32k tokens\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1awNgXYtjvUeXl1gS-v1iyDXTJJ-fyJIK) 81k tokens with half precision\n\n## Install\n\n```bash\n$ pip install reformer_pytorch\n```\n\n## Usage\n\nA simple Reformer language model\n\n```python\n# should fit in ~ 5gb - 8k tokens\n\nimport torch\nfrom reformer_pytorch import ReformerLM\n\nmodel = ReformerLM(\n    num_tokens= 20000,\n    dim = 1024,\n    depth = 12,\n    max_seq_len = 8192,\n    heads = 8,\n    lsh_dropout = 0.1,\n    ff_dropout = 0.1,\n    post_attn_dropout = 0.1,\n    layer_dropout = 0.1,  # layer dropout from 'Reducing Transformer Depth on Demand' paper\n    causal = True,        # auto-regressive or not\n    bucket_size = 64,     # average size of qk per bucket, 64 was recommended in paper\n    n_hashes = 4,         # 4 is permissible per author, 8 is the best but slower\n    emb_dim = 128,        # embedding factorization for further memory savings\n    dim_head = 64,        # be able to fix the dimension of each head, making it independent of the embedding dimension and the number of heads\n    ff_chunks = 200,      # number of chunks for feedforward layer, make higher if there are memory issues\n    attn_chunks = 8,      # process lsh attention in chunks, only way for memory to fit when scaling to 16k tokens\n    num_mem_kv = 128,       # persistent learned memory key values, from all-attention paper\n    full_attn_thres = 1024, # use full attention if context length is less than set value\n    reverse_thres = 1024,   # turn off reversibility for 2x speed for sequence lengths shorter or equal to the designated value\n    use_scale_norm = False,  # use scale norm from 'Transformers without tears' paper\n    use_rezero = False,      # remove normalization and use rezero from 'ReZero is All You Need'\n    one_value_head = False,  # use one set of values for all heads from 'One Write-Head Is All You Need'\n    weight_tie = False,           # tie parameters of each layer for no memory per additional depth\n    weight_tie_embedding = False, # use token embedding for projection of output, some papers report better results\n    n_local_attn_heads = 2,       # many papers suggest mixing local attention heads aids specialization and improves on certain tasks\n    pkm_layers = (4,7),           # specify layers to use product key memory. paper shows 1 or 2 modules near the middle of the transformer is best\n    pkm_num_keys = 128,           # defaults to 128, but can be increased to 256 or 512 as memory allows\n    use_full_attn = False    # only turn on this flag to override and turn on full attention for all sequence lengths. for comparison with LSH to show that it is working\n).cuda()\n\nx = torch.randint(0, 20000, (1, 8192)).long().cuda()\ny = model(x) # (1, 8192, 20000)\n```\n\nThe Reformer (just a stack of reversible LSH attention)\n\n```python\n# should fit in ~ 5gb - 8k embeddings\n\nimport torch\nfrom reformer_pytorch import Reformer\n\nmodel = Reformer(\n    dim = 512,\n    depth = 12,\n    heads = 8,\n    lsh_dropout = 0.1,\n    causal = True\n).cuda()\n\nx = torch.randn(1, 8192, 512).cuda()\ny = model(x) # (1, 8192, 512)\n```\n\nSelf Attention with LSH\n\n```python\nimport torch\nfrom reformer_pytorch import LSHSelfAttention\n\nattn = LSHSelfAttention(\n    dim = 128,\n    heads = 8,\n    bucket_size = 64,\n    n_hashes = 8,\n    causal = False\n)\n\nx = torch.randn(10, 1024, 128)\ny = attn(x) # (10, 1024, 128)\n```\n\nLSH (locality sensitive hashing) Attention\n\n```python\nimport torch\nfrom reformer_pytorch import LSHAttention\n\nattn = LSHAttention(\n    bucket_size = 64,\n    n_hashes = 16,\n    causal = True\n)\n\nqk = torch.randn(10, 1024, 128)\nv = torch.randn(10, 1024, 128)\n\nout, attn, buckets = attn(qk, v) # (10, 1024, 128)\n# attn contains the unsorted attention weights, provided return_attn is set to True (costly otherwise)\n# buckets will contain the bucket number (post-argmax) of each token of each batch\n```\n\n## Masking\n\nThis repository supports masks on the input sequence `input_mask (b x i_seq)`, the context sequence `context_mask (b x c_seq)`, as well as the rarely used full attention matrix itself `input_attn_mask (b x i_seq x i_seq)`, all made compatible with LSH attention. Masks are made of booleans where `False` denotes masking out prior to the softmax.\n\nThe causal triangular mask is all taken care of for you if you set `causal = True`.\n\n```python\nimport torch\nfrom reformer_pytorch import ReformerLM\n\nCONTEXT_LEN = 512\nSEQ_LEN = 8192\n\nmodel = ReformerLM(\n    num_tokens= 20000,\n    dim = 1024,\n    depth = 1,\n    max_seq_len = SEQ_LEN,\n    ff_chunks = 8,\n    causal = True\n)\n\nc = torch.randn(1, CONTEXT_LEN, 1024)\nx = torch.randint(0, 20000, (1, SEQ_LEN)).long()\n\ni_mask = torch.ones(1, SEQ_LEN).bool()\nc_mask = torch.ones(1, CONTEXT_LEN).bool()\n\ny = model(x, keys = c, input_mask = i_mask, context_mask = c_mask)\n# masking done correctly in LSH attention\n```\n\n## Positional Embeddings\n\nThe default positional embedding uses <a href=\"https://arxiv.org/abs/2104.09864\">rotary embeddings</a>.\n\nHowever, <a href=\"https://github.com/AranKomat\">Aran</a> has informed me that the Reformer team used axial position embeddings with great results on longer sequences.\n\nYou can turn on axial positional embedding and adjust the shape and dimension of the axial embeddings by following the instructions below.\n\n```python\nimport torch\nfrom reformer_pytorch import ReformerLM\n\nmodel = ReformerLM(\n    num_tokens= 20000,\n    dim = 1024,\n    depth = 12,\n    max_seq_len = 8192,\n    ff_chunks = 8,\n    attn_chunks = 2,\n    causal = True,\n    axial_position_emb = True,         # set this to True\n    axial_position_shape = (128, 64),  # the shape must multiply up to the max_seq_len (128 x 64 = 8192)\n)\n\nx = torch.randint(0, 20000, (1, 8192)).long()\ny = model(x) # (1, 8192, 20000)\n```\n\nIf you would rather use absolute positional embeddings, you can turn it on with `absolute_position_emb = True` flag on initialization.\n\n## Training\n\nSince version `0.17.0`, and some corrections to the reversible network, Reformer Pytorch is compatible with Microsoft's Deepspeed! If you have multiple local GPUs, you can follow the instructions / example <a href=\"https://github.com/lucidrains/reformer-pytorch/tree/master/examples/enwik8_deepspeed\">here</a>.\n\n## Examples\n\nA full Reformer sequence \u2192 sequence, say translation\n\n```python\nimport torch\nfrom reformer_pytorch import ReformerLM\n\nDE_SEQ_LEN = 4096\nEN_SEQ_LEN = 4096\n\nencoder = ReformerLM(\n    num_tokens = 20000,\n    emb_dim = 128,\n    dim = 1024,\n    depth = 12,\n    heads = 8,\n    max_seq_len = DE_SEQ_LEN,\n    fixed_position_emb = True,\n    return_embeddings = True # return output of last attention layer\n).cuda()\n\ndecoder = ReformerLM(\n    num_tokens = 20000,\n    emb_dim = 128,\n    dim = 1024,\n    depth = 12,\n    heads = 8,\n    max_seq_len = EN_SEQ_LEN,\n    fixed_position_emb = True,\n    causal = True\n).cuda()\n\nx  = torch.randint(0, 20000, (1, DE_SEQ_LEN)).long().cuda()\nyi = torch.randint(0, 20000, (1, EN_SEQ_LEN)).long().cuda()\n\nenc_keys = encoder(x)               # (1, 4096, 1024)\nyo = decoder(yi, keys = enc_keys)   # (1, 4096, 20000)\n```\n\nA full Reformer image \u2192 caption\n\n```python\nimport torch\nfrom torch.nn import Sequential\nfrom torchvision import models\nfrom reformer_pytorch import Reformer, ReformerLM\n\nresnet = models.resnet50(pretrained=True)\nresnet = Sequential(*list(resnet.children())[:-4])\n\nSEQ_LEN = 4096\n\nencoder = Reformer(\n    dim = 512,\n    depth = 6,\n    heads = 8,\n    max_seq_len = 4096\n)\n\ndecoder = ReformerLM(\n    num_tokens = 20000,\n    dim = 512,\n    depth = 6,\n    heads = 8,\n    max_seq_len = SEQ_LEN,\n    causal = True\n)\n\nx  = torch.randn(1, 3, 512, 512)\nyi = torch.randint(0, 20000, (1, SEQ_LEN)).long()\n\nvisual_emb = resnet(x)\nb, c, h, w = visual_emb.shape\nvisual_emb = visual_emb.view(1, c, h * w).transpose(1, 2) # nchw to nte\n\nenc_keys = encoder(visual_emb)\nyo = decoder(yi, keys = enc_keys) # (1, 4096, 20000)\n```\n\n## Reformer Encoder Decoder Architecture\n\n**There is a bug in versions < `0.21.0`. Please upgrade to at least the version specified for the working encoder / decoder Reformer.**\n\nBy popular demand, I have coded up a wrapper that removes a lot of the manual work in writing up a generic Reformer encoder / decoder architecture. To use, you would import the `ReformerEncDec` class. Encoder keyword arguments would be passed with a `enc_` prefix and decoder keyword arguments with `dec_`. The model dimension (`dim`) must be prefix free and will be shared between encoder and decoder. The framework will also take care of passing the encoder input mask to the decoder context mask, unless explicitly overridden.\n\n```python\nimport torch\nfrom reformer_pytorch import ReformerEncDec\n\nDE_SEQ_LEN = 4096\nEN_SEQ_LEN = 4096\n\nenc_dec = ReformerEncDec(\n    dim = 512,\n    enc_num_tokens = 20000,\n    enc_depth = 6,\n    enc_max_seq_len = DE_SEQ_LEN,\n    dec_num_tokens = 20000,\n    dec_depth = 6,\n    dec_max_seq_len = EN_SEQ_LEN\n).cuda()\n\ntrain_seq_in = torch.randint(0, 20000, (1, DE_SEQ_LEN)).long().cuda()\ntrain_seq_out = torch.randint(0, 20000, (1, EN_SEQ_LEN)).long().cuda()\ninput_mask = torch.ones(1, DE_SEQ_LEN).bool().cuda()\n\nloss = enc_dec(train_seq_in, train_seq_out, return_loss = True, enc_input_mask = input_mask)\nloss.backward()\n# learn\n\n# evaluate with the following\neval_seq_in = torch.randint(0, 20000, (1, DE_SEQ_LEN)).long().cuda()\neval_seq_out_start = torch.tensor([[0.]]).long().cuda() # assume 0 is id of start token\nsamples = enc_dec.generate(eval_seq_in, eval_seq_out_start, seq_len = EN_SEQ_LEN, eos_token = 1) # assume 1 is id of stop token\nprint(samples.shape) # (1, <= 1024) decode the tokens\n```\n\n## Product Key Memory\n\nTo see the benefits of using PKM, the learning rate of the values must be set higher than the rest of the parameters. (Recommended to be `1e-2`)\n\nYou can follow the instructions here to set it correctly https://github.com/lucidrains/product-key-memory#learning-rates\n\n## Customizing Feedforward\n\nBy default, the activation function is `GELU`. If you would like an alternative activation function, you can pass in the class to the keyword `ff_activation`.\n\n```python\nimport torch\nfrom reformer_pytorch import ReformerLM\nfrom torch import nn\n\nmodel = ReformerLM(\n    num_tokens= 20000,\n    dim = 512,\n    depth = 6,\n    max_seq_len = 8192,\n    ff_chunks = 8,\n    ff_dropout = 0.1,\n    ff_mult = 6,\n    ff_activation = nn.LeakyReLU,\n    ff_glu = True # use GLU in feedforward, from paper 'GLU Variants Improve Transformer'\n)\n\nx = torch.randint(0, 20000, (1, 8192)).long()\ny = model(x) # (1, 8192, 20000)\n```\n\n## Research\n\nTo access the attention weights and bucket distribution, simply wrap the instantiated model with the `Recorder` wrapper class.\n\n```python\nimport torch\nfrom reformer_pytorch import Reformer, Recorder\n\nmodel = Reformer(\n    dim = 512,\n    depth = 12,\n    max_seq_len = 8192,\n    heads = 8,\n    lsh_dropout = 0.1,\n    causal = True\n).cuda()\n\nmodel = Recorder(model)\n\nx = torch.randn(1, 8192, 512).cuda()\ny = model(x)\n\nmodel.recordings[0] # a list of attention weights and buckets for the first forward pass\n\nmodel.turn_off() # stop recording\nmodel.turn_on() # start recording\nmodel.clear() # clear the recordings\n\nmodel = model.eject() # recover the original model and remove all listeners\n```\n\n## Additional Helpers\n\nReformer comes with a slight drawback that the sequence must be neatly divisible by the bucket size * 2. I have provided a small helper tool that can help you auto-round the sequence length to the next best multiple.\n\n```python\nimport torch\nfrom reformer_pytorch import ReformerLM, Autopadder\n\nmodel = ReformerLM(\n    num_tokens= 20000,\n    dim = 1024,\n    depth = 12,\n    max_seq_len = 8192,\n    heads = 8,\n    lsh_dropout = 0.1,\n    causal = True,\n    bucket_size = 63,   # odd bucket size\n    num_mem_kv = 77     # odd memory key length\n).cuda()\n\nmodel = Autopadder(model)\n\nSEQ_LEN = 7777 # odd sequence length\nkeys = torch.randn(1, 137, 1024) # odd keys length\n\nx = torch.randint(0, 20000, (1, SEQ_LEN)).long().cuda()\ny = model(x, keys = keys) # (1, 7777, 20000)\n```\n\n## Helpers for training auto-regressive models\n\nA lot of users are only interested in an auto-regressive language model (like GPT-2). Here is a training wrapper to make it easy to both train and evaluate on arbitrarily lengthed sequences of encoded tokens. You will have to take care of the encoding and decoding yourself.\n\n```python\nimport torch\nfrom torch import randint\n\nfrom reformer_pytorch import ReformerLM\nfrom reformer_pytorch.generative_tools import TrainingWrapper\n\nmodel = ReformerLM(\n    num_tokens= 20000,\n    dim = 1024,\n    depth = 12,\n    max_seq_len = 4096,\n    lsh_dropout = 0.1,\n    causal = True,\n    full_attn_thres = 1024\n)\n\n# 0 is used for padding and no loss to be calculated on it\nmodel = TrainingWrapper(model, ignore_index = 0, pad_value = 0)\n\n# the wrapper can handle evenly packed sequences\nx_train = randint(0, 20000, (3, 357))\n\n# or if you have a list of uneven sequences, it will be padded for you\nx_train = [\n    randint(0, 20000, (120,)),\n    randint(0, 20000, (253,)),\n    randint(0, 20000, (846,))\n]\n\n# when training, set return_loss equal to True\nmodel.train()\nloss = model(x_train, return_loss = True)\nloss.backward()\n\n# when evaluating, just use the generate function, which will default to top_k sampling with temperature of 1.\ninitial = torch.tensor([[0]]).long() # assume 0 is start token\nsample = model.generate(initial, 100, temperature=1., filter_thres = 0.9, eos_token = 1) # assume end token is 1, or omit and it will sample up to 100\nprint(sample.shape) # (1, <=100) token ids\n```\n\n\n## Issues\n\n<a href=\"https://github.com/andreabac3\">Andrea</a> has uncovered that using O2 optimization level when training with mixed precision can lead to instability. Please use O1 instead, which can be set with the `amp_level` in Pytorch Lightning, or `opt_level` in Nvidia's Apex library.\n\n## Alternatives\n\n1. Routing Transformer - https://github.com/lucidrains/routing-transformer\n2. Sinkhorn Transformer - https://github.com/lucidrains/sinkhorn-transformer\n3. Performer - https://github.com/lucidrains/performer-pytorch\n4. Linear Transformer - https://github.com/lucidrains/linear-attention-transformer/\n5. Compressive Transformer - https://github.com/lucidrains/compressive-transformer-pytorch\n\n## Citations\n```bibtex\n@inproceedings{kitaev2020reformer,\n    title       = {Reformer: The Efficient Transformer},\n    author      = {Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},\n    booktitle   = {International Conference on Learning Representations},\n    year        = {2020},\n    url         = {https://openreview.net/forum?id=rkgNKkHtvB}\n}\n```\n\n```bibtex\n@article{DBLP:journals/corr/abs-1907-01470,\n    author    = {Sainbayar Sukhbaatar and\n               Edouard Grave and\n               Guillaume Lample and\n               Herv{\\'{e}} J{\\'{e}}gou and\n               Armand Joulin},\n    title     = {Augmenting Self-attention with Persistent Memory},\n    journal   = {CoRR},\n    volume    = {abs/1907.01470},\n    year      = {2019},\n    url       = {http://arxiv.org/abs/1907.01470}\n}\n```\n\n```bibtex\n@article{1910.05895,\n    author  = {Toan Q. Nguyen and Julian Salazar},\n    title   = {Transformers without Tears: Improving the Normalization of Self-Attention},\n    year    = {2019},\n    eprint  = {arXiv:1910.05895},\n    doi     = {10.5281/zenodo.3525484},\n}\n```\n\n```bibtex\n@inproceedings{fan2020reducing,\n    title     = {Reducing Transformer Depth on Demand with Structured Dropout},\n    author    = {Angela Fan and Edouard Grave and Armand Joulin},\n    booktitle = {International Conference on Learning Representations},\n    year      = {2020},\n    url       = {https://openreview.net/forum?id=SylO2yStDr}\n}\n```\n\n```bibtex\n@article{Shazeer2019FastTD,\n    title   = {Fast Transformer Decoding: One Write-Head is All You Need},\n    author  = {Noam Shazeer},\n    journal = {ArXiv},\n    year    = {2019},\n    volume  = {abs/1911.02150}\n}\n```\n\n```bibtex\n@misc{shazeer2020glu,\n    title   = {GLU Variants Improve Transformer},\n    author  = {Noam Shazeer},\n    year    = {2020},\n    url     = {https://arxiv.org/abs/2002.05202}    \n}\n```\n\n```bibtex\n@misc{roy*2020efficient,\n    title   = {Efficient Content-Based Sparse Attention with Routing Transformers},\n    author  = {Aurko Roy* and Mohammad Taghi Saffar* and David Grangier and Ashish Vaswani},\n    year    = {2020},\n    url     = {https://openreview.net/forum?id=B1gjs6EtDr}\n}\n```\n\n```bibtex\n@misc{bachlechner2020rezero,\n    title   = {ReZero is All You Need: Fast Convergence at Large Depth},\n    author  = {Thomas Bachlechner and Bodhisattwa Prasad Majumder and Huanru Henry Mao and Garrison W. Cottrell and Julian McAuley},\n    year    = {2020},\n    url     = {https://arxiv.org/abs/2003.04887}\n}\n```\n\n```bibtex\n@misc{lample2019large,\n    title   = {Large Memory Layers with Product Keys},\n    author  = {Guillaume Lample and Alexandre Sablayrolles and Marc'Aurelio Ranzato and Ludovic Denoyer and Herv\u00e9 J\u00e9gou},\n    year    = {2019},\n    eprint  = {1907.05242},\n    archivePrefix = {arXiv}\n}\n```\n\n```bibtex\n@misc{bhojanapalli2020lowrank,\n    title   = {Low-Rank Bottleneck in Multi-head Attention Models},\n    author  = {Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},\n    year    = {2020},\n    eprint  = {2002.07028}\n}\n```\n\n```bibtex\n@misc{dong2021attention,\n    title   = {Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth}, \n    author  = {Yihe Dong and Jean-Baptiste Cordonnier and Andreas Loukas},\n    year    = {2021},\n    eprint  = {2103.03404}\n}\n```\n\n```bibtex\n@misc{su2021roformer,\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},\n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n    year    = {2021},\n    eprint  = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{vaswani2017attention,\n    title   = {Attention Is All You Need},\n    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n    year    = {2017},\n    eprint  = {1706.03762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n[\u2665](https://www.youtube.com/watch?v=GUo2XuqMcCU)\n",
            "readme_url": "https://github.com/lucidrains/reformer-pytorch",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
            "arxiv": "2104.09864",
            "year": 2021,
            "url": "http://arxiv.org/abs/2104.09864v2",
            "abstract": "Position encoding in transformer architecture provides supervision for\ndependency modeling between elements at different positions in the sequence. We\ninvestigate various methods to encode positional information in\ntransformer-based language models and propose a novel implementation named\nRotary Position Embedding(RoPE). The proposed RoPE encodes absolute positional\ninformation with rotation matrix and naturally incorporates explicit relative\nposition dependency in self-attention formulation. Notably, RoPE comes with\nvaluable properties such as flexibility of being expand to any sequence\nlengths, decaying inter-token dependency with increasing relative distances,\nand capability of equipping the linear self-attention with relative position\nencoding. As a result, the enhanced transformer with rotary position embedding,\nor RoFormer, achieves superior performance in tasks with long texts. We release\nthe theoretical analysis along with some preliminary experiment results on\nChinese data. The undergoing experiment for English benchmark will soon be\nupdated.",
            "authors": [
                "Jianlin Su",
                "Yu Lu",
                "Shengfeng Pan",
                "Bo Wen",
                "Yunfeng Liu"
            ]
        },
        {
            "title": "ReZero is All You Need: Fast Convergence at Large Depth",
            "arxiv": "2003.04887",
            "year": 2020,
            "url": "http://arxiv.org/abs/2003.04887v2",
            "abstract": "Deep networks often suffer from vanishing or exploding gradients due to\ninefficient signal propagation, leading to long training times or convergence\ndifficulties. Various architecture designs, sophisticated residual-style\nnetworks, and initialization schemes have been shown to improve deep signal\npropagation. Recently, Pennington et al. used free probability theory to show\nthat dynamical isometry plays an integral role in efficient deep learning. We\nshow that the simplest architecture change of gating each residual connection\nusing a single zero-initialized parameter satisfies initial dynamical isometry\nand outperforms more complex approaches. Although much simpler than its\npredecessors, this gate enables training thousands of fully connected layers\nwith fast convergence and better test performance for ResNets trained on\nCIFAR-10. We apply this technique to language modeling and find that we can\neasily train 120-layer Transformers. When applied to 12 layer Transformers, it\nconverges 56% faster on enwiki8.",
            "authors": [
                "Thomas Bachlechner",
                "Bodhisattwa Prasad Majumder",
                "Huanru Henry Mao",
                "Garrison W. Cottrell",
                "Julian McAuley"
            ]
        },
        {
            "title": "GLU Variants Improve Transformer",
            "arxiv": "2002.05202",
            "year": 2020,
            "url": "http://arxiv.org/abs/2002.05202v1",
            "abstract": "Gated Linear Units (arXiv:1612.08083) consist of the component-wise product\nof two linear projections, one of which is first passed through a sigmoid\nfunction. Variations on GLU are possible, using different nonlinear (or even\nlinear) functions in place of sigmoid. We test these variants in the\nfeed-forward sublayers of the Transformer (arXiv:1706.03762)\nsequence-to-sequence model, and find that some of them yield quality\nimprovements over the typically-used ReLU or GELU activations.",
            "authors": [
                "Noam Shazeer"
            ]
        },
        {
            "title": "Augmenting Self-attention with Persistent Memory",
            "arxiv": "1907.01470",
            "year": 2019,
            "url": "http://arxiv.org/abs/1907.01470v1",
            "abstract": "Transformer networks have lead to important progress in language modeling and\nmachine translation. These models include two consecutive modules, a\nfeed-forward layer and a self-attention layer. The latter allows the network to\ncapture long term dependencies and are often regarded as the key ingredient in\nthe success of Transformers. Building upon this intuition, we propose a new\nmodel that solely consists of attention layers. More precisely, we augment the\nself-attention layers with persistent memory vectors that play a similar role\nas the feed-forward layer. Thanks to these vectors, we can remove the\nfeed-forward layer without degrading the performance of a transformer. Our\nevaluation shows the benefits brought by our model on standard character and\nword level language modeling benchmarks.",
            "authors": [
                "Sainbayar Sukhbaatar",
                "Edouard Grave",
                "Guillaume Lample",
                "Herve Jegou",
                "Armand Joulin"
            ]
        },
        {
            "url": "https://openreview.net/forum?id=rkgNKkHtvB",
            "year": "2020",
            "booktitle": "International Conference on Learning Representations",
            "author": [
                "Kitaev, Nikita",
                "Kaiser, Lukasz",
                "Levskaya, Anselm"
            ],
            "title": "Reformer: The Efficient Transformer",
            "ENTRYTYPE": "inproceedings",
            "ID": "kitaev2020reformer",
            "authors": [
                "Kitaev, Nikita",
                "Kaiser, Lukasz",
                "Levskaya, Anselm"
            ]
        },
        {
            "url": "http://arxiv.org/abs/1907.01470",
            "year": "2019",
            "volume": "abs/1907.01470",
            "journal": "CoRR",
            "title": "Augmenting Self-attention with Persistent Memory",
            "author": [
                "Sukhbaatar, Sainbayar",
                "Grave, Edouard",
                "Lample, Guillaume",
                "J{\\'{e}}gou, Herv{\\'{e}}",
                "Joulin, Armand"
            ],
            "ENTRYTYPE": "article",
            "ID": "DBLP:journals/corr/abs-1907-01470",
            "authors": [
                "Sukhbaatar, Sainbayar",
                "Grave, Edouard",
                "Lample, Guillaume",
                "J{\\'{e}}gou, Herv{\\'{e}}",
                "Joulin, Armand"
            ]
        },
        {
            "doi": "10.5281/zenodo.3525484",
            "eprint": "arXiv:1910.05895",
            "year": "2019",
            "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
            "author": [
                "Nguyen, Toan Q.",
                "Salazar, Julian"
            ],
            "ENTRYTYPE": "article",
            "ID": "1910.05895",
            "authors": [
                "Nguyen, Toan Q.",
                "Salazar, Julian"
            ]
        },
        {
            "url": "https://openreview.net/forum?id=SylO2yStDr",
            "year": "2020",
            "booktitle": "International Conference on Learning Representations",
            "author": [
                "Fan, Angela",
                "Grave, Edouard",
                "Joulin, Armand"
            ],
            "title": "Reducing Transformer Depth on Demand with Structured Dropout",
            "ENTRYTYPE": "inproceedings",
            "ID": "fan2020reducing",
            "authors": [
                "Fan, Angela",
                "Grave, Edouard",
                "Joulin, Armand"
            ]
        },
        {
            "volume": "abs/1911.02150",
            "year": "2019",
            "journal": "ArXiv",
            "author": [
                "Shazeer, Noam"
            ],
            "title": "Fast Transformer Decoding: One Write-Head is All You Need",
            "ENTRYTYPE": "article",
            "ID": "Shazeer2019FastTD",
            "authors": [
                "Shazeer, Noam"
            ]
        },
        {
            "url": "https://arxiv.org/abs/2002.05202",
            "year": "2020",
            "author": [
                "Shazeer, Noam"
            ],
            "title": "GLU Variants Improve Transformer",
            "ENTRYTYPE": "misc",
            "ID": "shazeer2020glu",
            "authors": [
                "Shazeer, Noam"
            ]
        },
        {
            "url": "https://openreview.net/forum?id=B1gjs6EtDr",
            "year": "2020",
            "author": [
                "Roy*, Aurko",
                "Saffar*, Mohammad Taghi",
                "Grangier, David",
                "Vaswani, Ashish"
            ],
            "title": "Efficient Content-Based Sparse Attention with Routing Transformers",
            "ENTRYTYPE": "misc",
            "ID": "roy*2020efficient",
            "authors": [
                "Roy*, Aurko",
                "Saffar*, Mohammad Taghi",
                "Grangier, David",
                "Vaswani, Ashish"
            ]
        },
        {
            "url": "https://arxiv.org/abs/2003.04887",
            "year": "2020",
            "author": [
                "Bachlechner, Thomas",
                "Majumder, Bodhisattwa Prasad",
                "Mao, Huanru Henry",
                "Cottrell, Garrison W.",
                "McAuley, Julian"
            ],
            "title": "ReZero is All You Need: Fast Convergence at Large Depth",
            "ENTRYTYPE": "misc",
            "ID": "bachlechner2020rezero",
            "authors": [
                "Bachlechner, Thomas",
                "Majumder, Bodhisattwa Prasad",
                "Mao, Huanru Henry",
                "Cottrell, Garrison W.",
                "McAuley, Julian"
            ]
        },
        {
            "archiveprefix": "arXiv",
            "eprint": "1907.05242",
            "year": "2019",
            "author": [
                "Lample, Guillaume",
                "Sablayrolles, Alexandre",
                "Ranzato, Marc'Aurelio",
                "Denoyer, Ludovic",
                "J\u00e9gou, Herv\u00e9"
            ],
            "title": "Large Memory Layers with Product Keys",
            "ENTRYTYPE": "misc",
            "ID": "lample2019large",
            "authors": [
                "Lample, Guillaume",
                "Sablayrolles, Alexandre",
                "Ranzato, Marc'Aurelio",
                "Denoyer, Ludovic",
                "J\u00e9gou, Herv\u00e9"
            ]
        },
        {
            "eprint": "2002.07028",
            "year": "2020",
            "author": [
                "Bhojanapalli, Srinadh",
                "Yun, Chulhee",
                "Rawat, Ankit Singh",
                "Reddi, Sashank J.",
                "Kumar, Sanjiv"
            ],
            "title": "Low-Rank Bottleneck in Multi-head Attention Models",
            "ENTRYTYPE": "misc",
            "ID": "bhojanapalli2020lowrank",
            "authors": [
                "Bhojanapalli, Srinadh",
                "Yun, Chulhee",
                "Rawat, Ankit Singh",
                "Reddi, Sashank J.",
                "Kumar, Sanjiv"
            ]
        },
        {
            "eprint": "2103.03404",
            "year": "2021",
            "author": [
                "Dong, Yihe",
                "Cordonnier, Jean-Baptiste",
                "Loukas, Andreas"
            ],
            "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth",
            "ENTRYTYPE": "misc",
            "ID": "dong2021attention",
            "authors": [
                "Dong, Yihe",
                "Cordonnier, Jean-Baptiste",
                "Loukas, Andreas"
            ]
        },
        {
            "primaryclass": "cs.CL",
            "archiveprefix": "arXiv",
            "eprint": "2104.09864",
            "year": "2021",
            "author": [
                "Su, Jianlin",
                "Lu, Yu",
                "Pan, Shengfeng",
                "Wen, Bo",
                "Liu, Yunfeng"
            ],
            "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
            "ENTRYTYPE": "misc",
            "ID": "su2021roformer",
            "authors": [
                "Su, Jianlin",
                "Lu, Yu",
                "Pan, Shengfeng",
                "Wen, Bo",
                "Liu, Yunfeng"
            ]
        },
        {
            "primaryclass": "cs.CL",
            "archiveprefix": "arXiv",
            "eprint": "1706.03762",
            "year": "2017",
            "author": [
                "Vaswani, Ashish",
                "Shazeer, Noam",
                "Parmar, Niki",
                "Uszkoreit, Jakob",
                "Jones, Llion",
                "Gomez, Aidan N.",
                "Kaiser, Lukasz",
                "Polosukhin, Illia"
            ],
            "title": "Attention Is All You Need",
            "ENTRYTYPE": "misc",
            "ID": "vaswani2017attention",
            "authors": [
                "Vaswani, Ashish",
                "Shazeer, Noam",
                "Parmar, Niki",
                "Uszkoreit, Jakob",
                "Jones, Llion",
                "Gomez, Aidan N.",
                "Kaiser, Lukasz",
                "Polosukhin, Illia"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9402095722259911,
        "task": "Machine Translation",
        "task_prob": 0.9445988746966426
    },
    "training": {
        "datasets": [
            {
                "name": "enwiki8"
            },
            {
                "name": "CIFAR-10"
            }
        ]
    }
}