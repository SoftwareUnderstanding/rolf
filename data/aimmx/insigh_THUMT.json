{
    "visibility": {
        "visibility": "public",
        "license": "BSD 3-Clause \"New\" or \"Revised\" License"
    },
    "name": "THUMT: An Open Source Toolkit for Neural Machine Translation",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "insigh",
                "owner_type": "User",
                "name": "THUMT",
                "url": "https://github.com/insigh/THUMT",
                "stars": 0,
                "pushed_at": "2018-07-03 03:07:05+00:00",
                "created_at": "2018-07-03 02:58:41+00:00",
                "language": "Python",
                "description": "THUMT: An Open Source Toolkit for Neural Machine Translation",
                "license": "BSD 3-Clause \"New\" or \"Revised\" License",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "5cb9b44eeff01aa4ed6d74baee160338dcb4e170",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/insigh/THUMT/blob/master/LICENSE"
                    }
                },
                "size": 1532
            },
            {
                "type": "code",
                "name": "UserManual.pdf",
                "sha": "c78810234cc2a0efa0cce192fb97d4b8cee03da4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/insigh/THUMT/blob/master/UserManual.pdf"
                    }
                },
                "size": 212825
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "f6459567dbbe56ad48a4106144d8c18e6bea533e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/insigh/THUMT/tree/master/docs"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "thumt",
                "sha": "40cb07720977d6d6b71519b5948c3f8a209910d3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/insigh/THUMT/tree/master/thumt"
                    }
                },
                "num_files": 8
            }
        ]
    },
    "tags": [],
    "description": "THUMT: An Open Source Toolkit for Neural Machine Translation",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/insigh/THUMT",
            "stars": 0,
            "issues": true,
            "readme": "# THUMT: An Open Source Toolkit for Neural Machine Translation\n## Contents\n* [Introduction](#introduction)\n* [Implementations](#implementations)\n* [License](#license)\n* [Citation](#citation)\n* [Development Team](#development-team)\n* [Contributors](#Contributors)\n* [Contact](#contact)\n\n## Introduction\n\nMachine translation is a natural language processing task that aims to translate natural languages using computers automatically. Recent several years have witnessed the rapid development of end-to-end neural machine translation, which has become the new mainstream method in practical MT systems.\n\nTHUMT is an open-source toolkit for neural machine translation developed by [the Natural Language Processing Group at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/site2/index.php?lang=en).\n\n\n## Implementations\nTHUMT has currently two main implementations:\n\n* [THUMT-TensorFlow](https://github.com/thumt/THUMT): a new implementation developed with [TensorFlow](https://github.com/tensorflow/tensorflow). It implements the sequence-to-sequence model (**Seq2Seq**) ([Sutskever et al., 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)), the standard attention-based model (**RNNsearch**) ([Bahdanau et al., 2014](https://arxiv.org/pdf/1409.0473.pdf)), and the Transformer model (**Transformer**) ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n\n* [THUMT-Theano](https://github.com/thumt/THUMT/tree/theano): the original project developed with [Theano](https://github.com/Theano/Theano), which is no longer updated because MLA put an end to [Theano](https://github.com/Theano/Theano). It implements the standard attention-based model (**RNNsearch**) ([Bahdanau et al., 2014](https://arxiv.org/pdf/1409.0473.pdf)), minimum risk training (**MRT**) ([Shen et al., 2016](http://nlp.csai.tsinghua.edu.cn/~ly/papers/acl2016_mrt.pdf)) for optimizing model parameters with respect to evaluation metrics, semi-supervised training (**SST**) ([Cheng et al., 2016](http://nlp.csai.tsinghua.edu.cn/~ly/papers/acl2016_semi.pdf)) for exploiting monolingual corpora to learn bi-directional translation models, and layer-wise relevance propagation (**LRP**) ([Ding et al., 2017](http://nlp.csai.tsinghua.edu.cn/~ly/papers/acl2017_dyz.pdf)) for visualizing and anlayzing RNNsearch.\n\n\nThe following table summarizes the features of two implementations:\n\n| Implementation | Model | Criterion | Optimizer | LRP |\n| :------------: | :---: | :--------------: | :--------------: | :----------------: |\n| Theano       |  RNNsearch | MLE, MRT, SST | SGD, AdaDelta, Adam | RNNsearch |   \n| TensorFlow   |  Seq2Seq, RNNsearch, Transformer | MLE| Adam |n/a |\n\nWe recommend using [THUMT-TensorFlow](https://github.com/thumt/THUMT), which delivers better translation performance than [THUMT-Theano](https://github.com/thumt/THUMT/tree/theano). We will keep adding new features to [THUMT-TensorFlow](https://github.com/thumt/THUMT).\n\n## License\n\nThe source code is dual licensed. Open source licensing is under the [BSD-3-Clause](https://opensource.org/licenses/BSD-3-Clause), which allows free use for research purposes. For commercial licensing, please email [thumt17@gmail.com](mailto:thumt17@gmail.com).\n\n## Citation\n\nPlease cite the following paper:\n\n> Jiacheng Zhang, Yanzhuo Ding, Shiqi Shen, Yong Cheng, Maosong Sun, Huanbo Luan, Yang Liu. 2017. [THUMT: An Open Source Toolkit for Neural Machine Translation](https://arxiv.org/abs/1706.06415). arXiv:1706.06415.\n\n## Development Team\n\nProject leaders: [Maosong Sun](http://www.thunlp.org/site2/index.php/zh/people?id=16), [Yang Liu](http://nlp.csai.tsinghua.edu.cn/~ly/), Huanbo Luan\n\nProject members: Jiacheng Zhang, Yanzhuo Ding, Shiqi Shen, Yong Cheng\n\n## Contributors \n* [Zhixing Tan](mailto:playinf@stu.xmu.edu.cn) (Xiamen University)\n\n## Contact\n\nIf you have questions, suggestions and bug reports, please email [thumt17@gmail.com](mailto:thumt17@gmail.com).\n",
            "readme_url": "https://github.com/insigh/THUMT",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
            "arxiv": "1409.0473",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.0473v7",
            "abstract": "Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition.",
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio"
            ]
        },
        {
            "title": "THUMT: An Open Source Toolkit for Neural Machine Translation",
            "arxiv": "1706.06415",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.06415v1",
            "abstract": "This paper introduces THUMT, an open-source toolkit for neural machine\ntranslation (NMT) developed by the Natural Language Processing Group at\nTsinghua University. THUMT implements the standard attention-based\nencoder-decoder framework on top of Theano and supports three training\ncriteria: maximum likelihood estimation, minimum risk training, and\nsemi-supervised training. It features a visualization tool for displaying the\nrelevance between hidden states in neural networks and contextual words, which\nhelps to analyze the internal workings of NMT. Experiments on Chinese-English\ndatasets show that THUMT using minimum risk training significantly outperforms\nGroundHog, a state-of-the-art toolkit for NMT.",
            "authors": [
                "Jiacheng Zhang",
                "Yanzhuo Ding",
                "Shiqi Shen",
                "Yong Cheng",
                "Maosong Sun",
                "Huanbo Luan",
                "Yang Liu"
            ]
        },
        {
            "title": "Attention Is All You Need",
            "arxiv": "1706.03762",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.03762v5",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ]
        },
        {
            "title": "THUMT-TensorFlow",
            "url": "https://github.com/thumt/THUMT"
        },
        {
            "title": "THUMT-Theano",
            "url": "https://github.com/thumt/THUMT/tree/theano"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "SST"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999950621014653,
        "task": "Machine Translation",
        "task_prob": 0.9896139270376781
    }
}