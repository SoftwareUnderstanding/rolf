{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "LAMB Optimizer (TensorFlow)",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "liuqiangict",
                "owner_type": "User",
                "name": "lamb_optimizer",
                "url": "https://github.com/liuqiangict/lamb_optimizer",
                "stars": 0,
                "pushed_at": "2019-10-13 23:06:56+00:00",
                "created_at": "2019-10-13 23:06:03+00:00",
                "language": "Jupyter Notebook",
                "description": "Optimizer of lamb",
                "license": "Apache License 2.0",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "FULL_RESULTS.md",
                "sha": "2b77f36ff20c7e731071deeffc69878f59b5bc9d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/liuqiangict/lamb_optimizer/blob/master/FULL_RESULTS.md"
                    }
                },
                "size": 3544
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "261eeb9e9f8b2b4b0d119366dda99c6fd7d35c64",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/liuqiangict/lamb_optimizer/blob/master/LICENSE"
                    }
                },
                "size": 11357
            },
            {
                "type": "code",
                "name": "algorithm.png",
                "sha": "f5838eaf679e20cf8712640f439ce506e66b86b6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/liuqiangict/lamb_optimizer/blob/master/algorithm.png"
                    }
                },
                "size": 110391
            },
            {
                "type": "code",
                "name": "mnist_tensorflow.ipynb",
                "sha": "30b17c0c24097fa235f8e51cb5c191e072e64e76",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/liuqiangict/lamb_optimizer/blob/master/mnist_tensorflow.ipynb"
                    }
                },
                "size": 35039
            },
            {
                "type": "code",
                "name": "optimization.py",
                "sha": "7a11e53d066589b7cb3e6a240de1ee814d6acb40",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/liuqiangict/lamb_optimizer/blob/master/optimization.py"
                    }
                },
                "size": 10805
            }
        ]
    },
    "tags": [],
    "description": "Optimizer of lamb",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/liuqiangict/lamb_optimizer",
            "stars": 0,
            "issues": true,
            "readme": "# LAMB Optimizer (TensorFlow)\nThis is a simple implementation of LAMB Optimizer, which appeared in the paper [**\"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes\"**](https://arxiv.org/abs/1904.00962v3). \n\nThe older name of the paper was [\"Reducing BERT Pre-Training Time from 3 Days to 76 Minutes\"](https://arxiv.org/abs/1904.00962v1)\n\n\n## Notes\n- **This is NOT an official implementation.**\n- LAMB optimizer changes slightly from arXiv v1 ~ v3.\n- We implement v3 version (which is the latest version on June, 2019.).\n- Some uncertain parts are clarified by consulting original authors (such as `scaling function`).\n\n\n## Algorithm\nLAMB optimizer is originally designed for large batch learning in neural networks, but could also used in small batch size as indicated by authors.\n\n![algorithm.png](https://github.com/ymcui/LAMB_Optimizer_TF/blob/master/algorithm.png)\n\n\n## Usage\nThe implementation is based on BERT [repository](https://github.com/google-research/bert), which uses `AdamWeightDecayOptimizer` (appears in [`optimization.py`](https://github.com/google-research/bert/blob/master/optimization.py)) for pre-training and fine-tuning.\n\n- Just use `LAMBOptimizer` as a regular optimizer in TensorFlow, similar to `Adam` or `AdamWeightDecayOptimizer`.\n- Find LAMB optimizer in `optimization.py`.\n- There is nothing special to tune other than initial `learning_rate`.\n\n\n## Results on MNIST\n- I don't have TPU Pod to test its scalability on BERT with large batch \ud83d\ude02, but tested on MNIST for verify its effectiveness.\n- All optimizers use an initial learning rate of **0.001** (default settings), and did **NOT** scale to the batch size (may bring another gain, but leave it for you to test).\n- All the experiments are done on NVIDIA TESLA T4.\n\nHere are the numbers on several three classical neural networks **(MLP, CNN, Bi-RNN, Bi-GRU, Bi-LSTM)** with different optimizers **(Adam, AdamW, LAMB)**. \n\nI only list results of batch={64, 128, 1024, 16384}. For full results, please see [`FULL_RESULTS.md`](https://github.com/ymcui/LAMB_Optimizer_TF/blob/master/FULL_RESULTS.md).\n\n\n### Batch=64\n| Optimizer | MLP | CNN | Bi-RNN | Bi-GRU | Bi-LSTM | Note | \n| :------ | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| Adam | 97.03 | 98.93 | 96.24 | 98.92 | **99.04** | Just ordinary Adam |\n| AdamW | 97.11 | 99.01 | 96.50 | **99.11** | **99.04** | Used in BERT |\n| **LAMB** | **98.27** | **99.33** | **97.73** | 98.83 | 98.94 | New optimizer for large batch |\n\n\n### Batch=128\n| Optimizer | MLP | CNN | Bi-RNN | Bi-GRU | Bi-LSTM | Note | \n| :------ | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| Adam | 96.38 | 98.76 | 97.73 | **99.08** | **99.09** | Just ordinary Adam |\n| AdamW | 96.57 | 98.72 | **98.05** | 98.96 | 99.00 | Used in BERT |\n| **LAMB** | **97.90** | **99.20** | 98.04 | 98.87 | 98.76 | New optimizer for large batch |\n\n\n### Batch=1024\n| Optimizer | MLP | CNN | Bi-RNN | Bi-GRU | Bi-LSTM | Note | \n| :------ | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| Adam | 93.05 | 97.92 | 98.10 | **98.94** | 98.67 | Just ordinary Adam |\n| AdamW | 93.67 | 98.00 | 98.19 | 98.86 | **98.82** | Used in BERT |\n| **LAMB** | **97.68** | **98.82** | **98.27** | 98.61 | 98.47 | New optimizer for large batch |\n\n\n### Batch=16384\n| Optimizer | MLP | CNN | Bi-RNN | Bi-GRU | Bi-LSTM | Note | \n| :------ | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| Adam | 88.46 | 95.06 | 95.98 | 97.81 | 97.74 | Just ordinary Adam |\n| AdamW | 91.46 | 96.57 | **96.34** | **98.45** | **98.39** | Used in BERT |\n| **LAMB** | **93.23** | **97.89** | 93.76 | 87.60 | 80.36 | New optimizer for large batch |\n\n\n### Several Conclusions\n**Note: The conclusions are only made by the results above.**\n\n- LAMB consistently outperforms `Adam` and `AdamW` in most of the times, and shows consistent results among different batch sizes.\n- LAMB shows big advantage than `Adam` and `AdamW` on large batch, showing its excellent scalability.\n- LAMB failed to outperform than `Adam` and `AdamW` on complex RNN-based models, despite batch size.\n\n\n## Reproducibility\nCheck [`mnist_tensorflow.ipynb`](https://github.com/ymcui/LAMB_Optimizer_TF/blob/master/mnist_tensorflow.ipynb) for details.\n\nNote: You know the GPU/TPU won't get exactly the same results even we use fixed random seed.\n\n\n## References\n- Large Batch Optimization for Deep Learning: Training BERT in 76 minutes. https://arxiv.org/abs/1904.00962v3\n- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. https://arxiv.org/abs/1810.04805\n\n## Issues\nFor help or issues, please submit a GitHub issue.\n",
            "readme_url": "https://github.com/liuqiangict/lamb_optimizer",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes",
            "arxiv": "1904.00962",
            "year": 2019,
            "url": "http://arxiv.org/abs/1904.00962v5",
            "abstract": "Training large deep neural networks on massive datasets is computationally\nvery challenging. There has been recent surge in interest in using large batch\nstochastic optimization methods to tackle this issue. The most prominent\nalgorithm in this line of research is LARS, which by employing layerwise\nadaptive learning rates trains ResNet on ImageNet in a few minutes. However,\nLARS performs poorly for attention models like BERT, indicating that its\nperformance gains are not consistent across tasks. In this paper, we first\nstudy a principled layerwise adaptation strategy to accelerate training of deep\nneural networks using large mini-batches. Using this strategy, we develop a new\nlayerwise adaptive large batch optimization technique called LAMB; we then\nprovide convergence analysis of LAMB as well as LARS, showing convergence to a\nstationary point in general nonconvex settings. Our empirical results\ndemonstrate the superior performance of LAMB across various tasks such as BERT\nand ResNet-50 training with very little hyperparameter tuning. In particular,\nfor BERT training, our optimizer enables use of very large batch sizes of 32868\nwithout any degradation of performance. By increasing the batch size to the\nmemory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to\njust 76 minutes (Table 1). The LAMB implementation is available at\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py",
            "authors": [
                "Yang You",
                "Jing Li",
                "Sashank Reddi",
                "Jonathan Hseu",
                "Sanjiv Kumar",
                "Srinadh Bhojanapalli",
                "Xiaodan Song",
                "James Demmel",
                "Kurt Keutzer",
                "Cho-Jui Hsieh"
            ]
        },
        {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "arxiv": "1810.04805",
            "year": 2018,
            "url": "http://arxiv.org/abs/1810.04805v2",
            "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).",
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "MNIST"
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "SQuAD"
            },
            {
                "name": "GLUE"
            },
            {
                "name": "MultiNLI"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9805268696653309,
        "task": "Question Answering",
        "task_prob": 0.9131920208574429
    }
}