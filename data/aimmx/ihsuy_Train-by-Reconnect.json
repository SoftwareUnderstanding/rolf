{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Train by Reconnect: Decoupling Locations of Weights from Their Values",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "ihsuy",
                "owner_type": "User",
                "name": "Train-by-Reconnect",
                "url": "https://github.com/ihsuy/Train-by-Reconnect",
                "stars": 8,
                "pushed_at": "2021-03-03 09:44:57+00:00",
                "created_at": "2020-10-22 14:30:17+00:00",
                "language": "Jupyter Notebook",
                "description": "Official code for the NeurIPS 2020 paper Train by Reconnect: Decoupling Locations of Weights from Their Values by Yushi Qiu and Reiji Suda.",
                "license": "MIT License",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "Images",
                "sha": "9336d6f0b9f17305724732e818c74be7e58dfb1b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ihsuy/Train-by-Reconnect/tree/main/Images"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "58c1d774b3a2ecac2f0a06e835a460e1b82e88b1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ihsuy/Train-by-Reconnect/blob/main/LICENSE"
                    }
                },
                "size": 1066
            },
            {
                "type": "code",
                "name": "NeurIPS Poster.pdf",
                "sha": "0bd35e6330cf49f79e6f5e6ca14a26296a5ec79b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ihsuy/Train-by-Reconnect/blob/main/NeurIPS Poster.pdf"
                    }
                },
                "size": 4384763
            },
            {
                "type": "code",
                "name": "notebooks",
                "sha": "0d691673c0b7e75405b99adc8d9350928fccb4e3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ihsuy/Train-by-Reconnect/tree/main/notebooks"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "pretrained",
                "sha": "bfb1f29ceaf9c5a0eb833932fe07c660d48e05fa",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ihsuy/Train-by-Reconnect/tree/main/pretrained"
                    }
                },
                "num_files": 11
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "a4245ba7eb4a066d0e76334b2f3362cd8b939acb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ihsuy/Train-by-Reconnect/blob/main/requirements.txt"
                    }
                },
                "size": 49
            },
            {
                "type": "code",
                "name": "train_by_reconnect",
                "sha": "77fb7ae3269c751c7bf5de53704accf842d830d3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ihsuy/Train-by-Reconnect/tree/main/train_by_reconnect"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "weight_profiles",
                "sha": "038d3165b0806161f6ffb540efcd3ed2882061fe",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ihsuy/Train-by-Reconnect/tree/main/weight_profiles"
                    }
                },
                "num_files": 13
            }
        ]
    },
    "authors": [
        {
            "name": "yushiqiu",
            "github_id": "ihsuy"
        }
    ],
    "tags": [],
    "description": "Official code for the NeurIPS 2020 paper Train by Reconnect: Decoupling Locations of Weights from Their Values by Yushi Qiu and Reiji Suda.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/ihsuy/Train-by-Reconnect",
            "stars": 8,
            "issues": true,
            "readme": "## Train by Reconnect: Decoupling Locations of Weights from Their Values\n\nThis repository contains the official code for the NeurIPS 2020 paper *Train by Reconnect: Decoupling Locations of Weights from Their Values* by Yushi Qiu and Reiji Suda.\n\n<p align=\"left\" style=\"float: left;\">\n  <img src=\"https://github.com/ihsuy/Train-by-Reconnect/blob/main/Images/perm4.gif?raw=true\" height=\"230\">\n</p> \n\n\n> **Train by Reconnect: Decoupling Locations of Weights from Their Values**<br>\n> Yushi Qiu and Reiji Suda <br>\n> The University of Tokyo\n>\n> **Abstract:** What makes untrained deep neural networks (DNNs) different from the trained performant ones? By zooming into the weights in well-trained DNNs, we found that it is the *location* of weights that holds most of the information encoded by the training. Motivated by this observation, we hypothesized that weights in DNNs trained using stochastic gradient-based methods can be separated into two dimensions: the location of weights, and their exact values. To assess our hypothesis, we propose a novel method called *lookahead permutation* (LaPerm) to train DNNs by reconnecting the weights. We empirically demonstrate LaPerm's versatility while producing extensive evidence to support our hypothesis: when the initial weights are random and dense, our method demonstrates speed and performance similar to or better than that of regular optimizers, e.g., *Adam*. When the initial weights are random and sparse (many zeros), our method changes the way neurons connect, achieving accuracy comparable to that of a well-trained dense network. When the initial weights share a single value, our method finds a weight agnostic neural network with far-better-than-chance accuracy.\n>\n\n\n## Dependencies\nCode in this repository requires:\n- Python 3.6 or higher\n- Tensorflow v2.1.0 or higher\nand the requirements highlighted in [requirements.txt](./requirements.txt)\n\n## Table of Contents\nThis repository contains the following contents:\n- **train_by_reconnect**: minimum code for reproducing main results mentioned in the paper. The code is commented and accompanied with working examples in [notebooks](./notebooks).\n    - [LaPerm.py](./train_by_reconnect/LaPerm.py)\n        - `LaPerm`: [Tensorflow](https://www.tensorflow.org/) implementation of LaPerm (Section 4).\n        - `LaPermTrainLoop`: A custom train loop that applies LaPerm to [tensorflow.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\n    - [weight_utils.py](./train_by_reconnect/weight_utils.py)\n        - `agnosticize`: Replace the weights in a model with a single shared value. (Section 5.5)\n        - `random_prune`: Randomly prune the model. (Section 5.4)\n    - [viz_utiles.py](./train_by_reconnect/viz_utils.py)\n        - `Profiler`: Plot weight profiles for a given model. (Section 2)\n        - `PermutationTracer`: Visualize and trace how the locations of weights has changed.       \n- **notebooks**: [Jupyter-notebooks](./notebooks) containing the model definitions and experiment configurations for reconducting or extending the experiments (training + evaluation). Detailed instructions can be found inside the notebooks.\n    - [`Conv2.ipynb`](./notebooks/Conv2.ipynb), [`Conv4.ipynb`](./notebooks/Conv4.ipynb), [`Conv13.ipynb`](./notebooks/Conv13.ipynb), [`Conv7.ipynb`](./notebooks/Conv7.ipynb), [`ResNet50.ipynb`]((./notebooks/ResNet50.ipynb)): For experiments mentioned in Section 5.1~5.4.\n    - [`F1_and_F2.ipynb`](./notebooks/F1_and_F2.ipynb): For experiments mentioned in Section 5.5.\n    - [`Weight_profiles.ipynb`](./notebooks/Weight_profiles.ipynb): For visualizations mentioned in Section 2.\n- **pretrain**: pre-train weights for main results mentioned in the paper. (For detailed model definitions, please refer to 'notebooks`)\n    | Models     | Top-1 | *p%* | *k* | Dataset | Section | Weights |\n    | ---------- |:-----:| ----:| ---:| -------:| -------:| -----------:| \n    | [Conv7](./pretrained/Conv7.h5)      | 99.72%| 0%   | 1200|   MNIST |     5.1 | He Uniform  |\n    | [Conv2](./pretrained/Conv2.h5)      | 78.21%| 0%   | 1000| CIFAR-10|5.2, 5.4 | He Uniform  |\n    | [Conv4](./pretrained/Conv4.h5)      | 89.17%| 0%   | 1000| CIFAR-10|5.2, 5.4 | He Uniform  |\n    | [Conv13](./pretrained/Conv13.h5)     | 92.21%| 0%   | 1000| CIFAR-10|5.2, 5.4 | He Uniform  |\n    | [ResNet50](./pretrained/resnet50.h5)   | 92.53%| 0%   |  400| CIFAR-10|     5.4 | He Uniform  |\n    | [ResNet50](./pretrained/resnet50_30.h5)   | 92.32%| 30%  |  800| CIFAR-10|     5.4 | He Uniform  |\n    | [ResNet50](./pretrained/resnet50_50.h5)   | 92.02%| 50%  |  800| CIFAR-10|     5.4 | He Uniform  |\n    | [ResNet50](./pretrained/resnet50_70.h5)   | 90.97%| 70%  |  800| CIFAR-10|     5.4 | He Uniform  |\n    | [F1](./pretrained/F1.h5)         | 85.46%| 40%  |  250|   MNIST |     5.5 | Shared 0.08 |\n    | [F2](./pretrained/F2.h5)         | 78.14%| 92%  |  250|   MNIST |     5.5 | Shared 0.03 |\n    \n    - ***p%***: Percentage of weights that are randomly pruned before training, e.g., *p*=10% meaning 90% of weights are remained non-zero. (Section 5.4)\n\n    - ***k***: Sync period used to perform the experiment. (Section 4)\n    - ***Weights***: Mechanism used to generate the random weights.\n        - He Uniform: [He et al. 2015](https://arxiv.org/abs/1502.01852)\n        - Shared 0.08: the weights are sampled from the set {0, 0.08}.\n        - Shared 0.03: the weights are sampled from the set {0, 0.03}.\n    - Datasets: [MNIST](http://yann.lecun.com/exdb/mnist/), [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html).\n\n\n## Steps to load the pre-trained weights\n1. Locate the weight's corresponding jupyter-notebook in [notebooks](./notebooks). For example, for the weight named `Conv7.h5`, please look for [Conv7.ipynb](./notebooks/Conv7.ipynb) for the model definition and experiment configurations.\n2. Define the `model` as demonstrated in the notebook.\n3. Load the weights to `model` by\n    ```python\n    model.load_weights('../pretrained/Conv7.h5')\n    ```\n---\n## Resources\n\nAll material related to our paper is available via the following links:\n\n| Resources                    | Link\n| :--------------           | :----------\n| Paper PDF | https://arxiv.org/abs/2003.02570\n| Project page | TBA\n| Notebooks to reproduce experiments | [Link Notebooks](./notebooks)\n| Source code | [Link Github](https://github.com/ihsuy/Train-by-Reconnect)\n| Summary video | TBA\n| Presentation slides | TBA\n| Poster | [Link](https://github.com/ihsuy/Train-by-Reconnect/blob/main/NeurIPS%20Poster.pdf)\n\n---\n## License\nMIT\n",
            "readme_url": "https://github.com/ihsuy/Train-by-Reconnect",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
            "arxiv": "1502.01852",
            "year": 2015,
            "url": "http://arxiv.org/abs/1502.01852v1",
            "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art\nneural networks. In this work, we study rectifier neural networks for image\nclassification from two aspects. First, we propose a Parametric Rectified\nLinear Unit (PReLU) that generalizes the traditional rectified unit. PReLU\nimproves model fitting with nearly zero extra computational cost and little\noverfitting risk. Second, we derive a robust initialization method that\nparticularly considers the rectifier nonlinearities. This method enables us to\ntrain extremely deep rectified models directly from scratch and to investigate\ndeeper or wider network architectures. Based on our PReLU networks\n(PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012\nclassification dataset. This is a 26% relative improvement over the ILSVRC 2014\nwinner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass\nhuman-level performance (5.1%, Russakovsky et al.) on this visual recognition\nchallenge.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "Train-by-Reconnect: Decoupling Locations of Weights from their Values",
            "arxiv": "2003.02570",
            "year": 2020,
            "url": "http://arxiv.org/abs/2003.02570v6",
            "abstract": "What makes untrained deep neural networks (DNNs) different from the trained\nperformant ones? By zooming into the weights in well-trained DNNs, we found it\nis the location of weights that hold most of the information encoded by the\ntraining. Motivated by this observation, we hypothesize that weights in\nstochastic gradient-based method trained DNNs can be separated into two\ndimensions: the locations of weights and their exact values. To assess our\nhypothesis, we propose a novel method named Lookahead Permutation (LaPerm) to\ntrain DNNs by reconnecting the weights. We empirically demonstrate the\nversatility of LaPerm while producing extensive evidence to support our\nhypothesis: when the initial weights are random and dense, our method\ndemonstrates speed and performance similar to or better than that of regular\noptimizers, e.g., Adam; when the initial weights are random and sparse (many\nzeros), our method changes the way neurons connect and reach accuracy\ncomparable to that of a well-trained fully initialized network; when the\ninitial weights share a single value, our method finds weight agnostic neural\nnetwork with far better-than-chance accuracy.",
            "authors": [
                "Yushi Qiu",
                "Reiji Suda"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "MNIST"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ImageNet"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9972092206461248,
        "task": "Image Classification",
        "task_prob": 0.8146482594191569
    }
}