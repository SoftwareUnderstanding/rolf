{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "ademxapp",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "itijyou",
                "owner_type": "User",
                "name": "ademxapp",
                "url": "https://github.com/itijyou/ademxapp",
                "stars": 336,
                "pushed_at": "2020-08-19 17:04:59+00:00",
                "created_at": "2016-11-24 06:45:36+00:00",
                "language": "Python",
                "description": "Code for https://arxiv.org/abs/1611.10080",
                "license": "Other",
                "frameworks": [
                    "MXNet"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "88f9e88bc99ba60f6760e60213690b450d5e015d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/itijyou/ademxapp/blob/master/.gitignore"
                    }
                },
                "size": 1121
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "98748a85b762f372917740905d09e0643db21049",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/itijyou/ademxapp/blob/master/LICENSE"
                    }
                },
                "size": 578
            },
            {
                "type": "code",
                "name": "data",
                "sha": "3206bba365b6111fedf95cc761e440f19d36af75",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/itijyou/ademxapp/tree/master/data"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "iclass",
                "sha": "73f93442c2bedc4746139b0572bb5eb32aa25583",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/itijyou/ademxapp/tree/master/iclass"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "issegm",
                "sha": "5458835e4a18fdccf9d54cbac6f0ab507ecac65c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/itijyou/ademxapp/tree/master/issegm"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "misc",
                "sha": "f822935f3391c63aa6ac0b5078de131d6e8bf19e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/itijyou/ademxapp/tree/master/misc"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "models",
                "sha": "30cb4435aa4a9007872789be3bf81ff16cdb7fb5",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/itijyou/ademxapp/tree/master/models"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "tools",
                "sha": "1e74ac5edd66ea541fb93fed9206713c561ca689",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/itijyou/ademxapp/tree/master/tools"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "util",
                "sha": "b9ba647f1238a12ebeb49469218a65bdf19babe4",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/itijyou/ademxapp/tree/master/util"
                    }
                },
                "num_files": 10
            }
        ]
    },
    "authors": [
        {
            "name": "Zifeng Wu",
            "email": "wuzifeng.buaa@gmail.com",
            "github_id": "itijyou"
        }
    ],
    "tags": [
        "resnet-38",
        "semantic-segmentation",
        "cityscapes"
    ],
    "description": "Code for https://arxiv.org/abs/1611.10080",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/itijyou/ademxapp",
            "stars": 336,
            "issues": true,
            "readme": "# ademxapp\n\nVisual applications by the University of Adelaide\n\nIn designing our Model A, we did not over-optimize its structure for efficiency unless it was neccessary, which led us to a high-performance model without non-trivial building blocks. Besides, by doing so, we anticipate this model and its trivial variants to perform well when they are finetuned for new tasks, considering their better spatial efficiency and larger model sizes compared to conventional [ResNet](https://arxiv.org/abs/1512.03385) models.\n\nIn this work, we try to find a proper depth for ResNets, without grid-searching the whole space, especially when it is too costly to do so, e.g., on the ILSVRC 2012 classification dataset.\nFor more details, refer to our report: [Wider or Deeper: Revisiting the ResNet Model for Visual Recognition](https://arxiv.org/abs/1611.10080).\n\nThis code is a refactored version of the one that we used in the competition, and has not yet been tested extensively, so feel free to open an issue if you find any problem.\n\nTo use, first install [MXNet](https://github.com/dmlc/mxnet).\n\n\n### Updates\n\n* Recent updates\n    + Model A1 trained on Cityscapes\n    + Model A1 trained on VOC\n    + Training code for semantic image segmentation\n    + Training code for image classification on ILSVRC 2012 (Still needs to be evaluated.)\n\n<!---\n* Planned\n-->\n\n* History\n    + Results on VOC using COCO for pre-training\n    + Fix the bug in testing resulted from changing the EPS in BatchNorm layers\n    + Model A1 for ADE20K trained using the *train* set with testing code\n    + Segmentation results with multi-scale testing on VOC and Cityscapes\n    + Model A and Model A1 for ILSVRC with testing code\n    + Segmentation results with single-scale testing on VOC and Cityscapes\n\n### Image classification\n\n#### Pre-trained models\n\n0. Download the ILSVRC 2012 classification val set [6.3GB](http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_val.tar), and put the extracted images into the directory:\n    ```\n    data/ilsvrc12/ILSVRC2012_val/\n    ```\n\n0. Download the models as below, and put them into the directory:\n    ```\n    models/\n    ```\n\n0. Check the classification performance of pre-trained models on the ILSVRC 2012 val set:\n    ```bash\n    python iclass/ilsvrc.py --data-root data/ilsvrc12 --output output --batch-images 10 --phase val --weights models/ilsvrc-cls_rna-a_cls1000_ep-0001.params --split val --test-scales 320 --gpus 0 --no-choose-interp-method --pool-top-infer-style caffe\n    \n    python iclass/ilsvrc.py --data-root data/ilsvrc12 --output output --batch-images 10 --phase val --weights models/ilsvrc-cls_rna-a1_cls1000_ep-0001.params --split val --test-scales 320 --gpus 0 --no-choose-interp-method\n    ```\n\nResults on the ILSVRC 2012 val set tested with a single scale (320, without flipping):\n\n    model|top-1 error (%)|top-5 error (%)|download\n    :---:|:---:|:---:|:---:\n    [Model A](https://cdn.rawgit.com/itijyou/ademxapp/master/misc/ilsvrc_model_a.pdf)|19.20|4.73|[aar](https://cloudstor.aarnet.edu.au/plus/index.php/s/V7dncO4H0ijzeRj)\n    [Model A1](https://cdn.rawgit.com/itijyou/ademxapp/master/misc/ilsvrc_model_a1.pdf)|19.54|4.75|[aar](https://cloudstor.aarnet.edu.au/plus/index.php/s/NOPhJ247fhVDnZH)\n\nNote: Due to a change of MXNet in padding at pooling layers, some of the computed feature maps in Model A will have different sizes from those stated in our report. However, this has no effect on Model A1, which always uses convolution layers (instead of pooling layers) for down-sampling. So, in most cases, just use Model A1, which was initialized from Model A, and tuned for 45k extra iterations.\n\n#### New models\n\n0. Find a machine with 4 devices, each with at least 11G memories.\n\n0. Download the ILSVRC 2012 classification train set [138GB](http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_train.tar), and put the extracted images into the directory:\n    ```\n    data/ilsvrc12/ILSVRC2012_train/\n    ```\n    with the following structure:\n    ```\n    ILSVRC2012_train\n    |-- n01440764\n    |-- n01443537\n    |-- ...\n    `-- n15075141\n    ```\n\n0. Train a new Model A from scratch, and check its performance:\n    ```bash\n    python iclass/ilsvrc.py --gpus 0,1,2,3 --data-root data/ilsvrc12 --output output --model ilsvrc-cls_rna-a_cls1000 --batch-images 256 --crop-size 224 --lr-type linear --base-lr 0.1 --to-epoch 90 --kvstore local --prefetch-threads 8 --prefetcher process --backward-do-mirror\n    \n    python iclass/ilsvrc.py --data-root data/ilsvrc12 --output output --batch-images 10 --phase val --weights output/ilsvrc-cls_rna-a_cls1000_ep-0090.params --split val --test-scales 320 --gpus 0\n    ```\n\n0. Tune a Model A1 from our released Model A, and check its performance:\n    ```bash\n    python iclass/ilsvrc.py --gpus 0,1,2,3 --data-root data/ilsvrc12 --output output --model ilsvrc-cls_rna-a1_cls1000_from-a --batch-images 256 --crop-size 224 --weights models/ilsvrc-cls_rna-a_cls1000_ep-0001.params --lr-type linear --base-lr 0.01 --to-epoch 9 --kvstore local --prefetch-threads 8 --prefetcher process --backward-do-mirror\n    \n    python iclass/ilsvrc.py --data-root data/ilsvrc12 --output output --batch-images 10 --phase val --weights output/model ilsvrc-cls_rna-a1_cls1000_from-a_ep-0009.params --split val --test-scales 320 --gpus 0\n    ```\n\n0. Or train a new Model A1 from scratch, and check its performance:\n    ```bash\n    python iclass/ilsvrc.py --gpus 0,1,2,3 --data-root data/ilsvrc12 --output output --model ilsvrc-cls_rna-a1_cls1000 --batch-images 256 --crop-size 224 --lr-type linear --base-lr 0.1 --to-epoch 90 --kvstore local --prefetch-threads 8 --prefetcher process --backward-do-mirror\n    \n    python iclass/ilsvrc.py --data-root data/ilsvrc12 --output output --batch-images 10 --phase val --weights output/ilsvrc-cls_rna-a1_cls1000_ep-0090.params --split val --test-scales 320 --gpus 0\n    ```\n\nIt cost more than 40 days on our workstation with 4 Maxwell GTX Titan cards. So, be patient or try smaller models as described in our report.\n\nNote: The best setting (*prefetch-threads* and *prefetcher*) for efficiency can vary depending on the circumstances (the provided CPUs, GPUs, and filesystem).\n\nNote: This code may not accurately reproduce our reported results, since there are subtle differences in implementation, e.g., different cropping strategies, interpolation methods, and padding strategies.\n\n\n### Semantic image segmentation\n\nWe show the effectiveness of our models (as pre-trained features) by semantic image segmenatation using **plain dilated FCNs** initialized from our models. Several A1 models tuned on the *train* set of PASCAL VOC, Cityscapes and ADE20K are available.\n\n* To use, download and put them into the directory:\n\n    ```\n    models/\n    ```\n\n#### PASCAL VOC 2012:\n\n0. Download the PASCAL VOC 2012 dataset [2GB](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar), and put the extracted images into the directory:\n    ```\n    data/VOCdevkit/VOC2012\n    ```\n    with the following structure:\n    ```\n    VOC2012\n    |-- JPEGImages\n    |-- SegmentationClass\n    `-- ...\n    ```\n\n0. Check the performance of the pre-trained models:\n    ```bash\n    python issegm/voc.py --data-root data/VOCdevkit --output output --phase val --weights models/voc_rna-a1_cls21_s8_ep-0001.params --split val --test-scales 500 --test-flipping --gpus 0\n    \n    python issegm/voc.py --data-root data/VOCdevkit --output output --phase val --weights models/voc_rna-a1_cls21_s8_coco_ep-0001.params --split val --test-scales 500 --test-flipping --gpus 0\n    ```\n\nResults on the *val* set:\n\n    model|training data|testing scale|mean IoU (%)|download\n    :---|:---:|:---:|:---:|:---:\n    Model A1, 2 conv.|VOC; SBD|500|80.84|[aar](https://cloudstor.aarnet.edu.au/plus/index.php/s/YqNptRcboMD44Kd)\n    Model A1, 2 conv.|VOC; SBD; COCO|500|82.86|[aar](https://cloudstor.aarnet.edu.au/plus/index.php/s/JKWePbLPlpfRDW4)\n\nResults on the *test* set:\n\n    model|training data|testing scale|mean IoU (%)\n    :---|:---:|:---:|:---:\n    Model A1, 2 conv.|VOC; SBD|500|[82.5](http://host.robots.ox.ac.uk:8080/anonymous/H0KLZK.html)\n    Model A1, 2 conv.|VOC; SBD|multiple|[83.1](http://host.robots.ox.ac.uk:8080/anonymous/BEWE9S.html)\n    Model A1, 2 conv.|VOC; SBD; COCO|multiple|[84.9](http://host.robots.ox.ac.uk:8080/anonymous/JU1PXP.html)\n\n#### Cityscapes:\n\n0. Download the [Cityscapes dataset](https://www.cityscapes-dataset.com/downloads/), and put the extracted images into the directory:\n    ```\n    data/cityscapes\n    ```\n    with the following structure:\n    ```\n    cityscapes\n    |-- gtFine\n    `-- leftImg8bit\n    ```\n\n0. Clone the official Cityscapes toolkit:\n    ```bash\n    git clone https://github.com/mcordts/cityscapesScripts.git data/cityscapesScripts\n    ```\n\n0. Check the performance of the pre-trained model:\n    ```bash\n    python issegm/voc.py --data-root data/cityscapes --output output --phase val --weights models/cityscapes_rna-a1_cls19_s8_ep-0001.params --split val --test-scales 2048 --test-flipping --gpus 0\n    ```\n\n0. Tune a Model A1, and check its performance:\n    ```bash\n    python issegm/voc.py --gpus 0,1,2,3 --split train --data-root data/cityscapes --output output --model cityscapes_rna-a1_cls19_s8 --batch-images 16 --crop-size 500 --origin-size 2048 --scale-rate-range 0.7,1.3 --weights models/ilsvrc-cls_rna-a1_cls1000_ep-0001.params --lr-type fixed --base-lr 0.0016 --to-epoch 140 --kvstore local --prefetch-threads 8 --prefetcher process --cache-images 0 --backward-do-mirror\n\n    python issegm/voc.py --gpus 0,1,2,3 --split train --data-root data/cityscapes --output output --model cityscapes_rna-a1_cls19_s8_x1-140 --batch-images 16 --crop-size 500 --origin-size 2048 --scale-rate-range 0.7,1.3 --weights output/cityscapes_rna-a1_cls19_s8_ep-0140.params --lr-type linear --base-lr 0.0008 --to-epoch 64 --kvstore local --prefetch-threads 8 --prefetcher process --cache-images 0 --backward-do-mirror\n\n    python issegm/voc.py --data-root data/cityscapes --output output --phase val --weights output/cityscapes_rna-a1_cls19_s8_x1-140_ep-0064.params --split val --test-scales 2048 --test-flipping --gpus 0\n    ```\n\nResults on the *val* set:\n\n    model|training data|testing scale|mean IoU (%)|download\n    :---|:---:|:---:|:---:|:---:\n    Model A1, 2 conv.|fine|1024x2048|78.08|[aar](https://cloudstor.aarnet.edu.au/plus/index.php/s/2hbvpro6J4XKVIu)\n\nResults on the *test* set:\n\n    model|training data|testing scale|class IoU (%)|class iIoU (%)| category IoU (%)| category iIoU(%)\n    :---|:---:|:---:|:---:|:---:|:---:|:---:\n    Model A2, 2 conv.|fine|1024x2048|78.4|59.1|90.9|81.1\n    Model A2, 2 conv.|fine|multiple|79.4|58.0|91.0|80.1\n    Model A2, 2 conv.|fine; coarse|1024x2048|79.9|59.7|91.2|80.8\n    Model A2, 2 conv.|fine; coarse|multiple|80.6|57.8|91.0|79.1\n\nFor more information, refer to the official [leaderboard](https://www.cityscapes-dataset.com/benchmarks/#pixel-level-results).\n\nNote: [Model A2](https://cdn.rawgit.com/itijyou/ademxapp/master/misc/places_model_a2.pdf) was initialized from Model A, and tuned for 45k extra iterations using the Places data in ILSVRC 2016.\n\n#### MIT Scene Parsing Benchmark (ADE20K):\n\n0. Download the [MIT Scene Parsing dataset](http://sceneparsing.csail.mit.edu/), and put the extracted images into the directory:\n    ```\n    data/ade20k/\n    ```\n    with the following structure:\n    ```\n    ade20k\n    |-- annotations\n    |   |-- training\n    |   `-- validation\n    `-- images\n        |-- testing\n        |-- training\n        `-- validation\n    ```\n\n0. Check the performance of the pre-trained model:\n    ```bash\n    python issegm/voc.py --data-root data/ade20k --output output --phase val --weights models/ade20k_rna-a1_cls150_s8_ep-0001.params --split val --test-scales 500 --test-flipping --test-steps 2 --gpus 0\n    ```\n\nResults on the *val* set:\n\n    model|testing scale|pixel accuracy (%)|mean IoU (%)|download\n    :---|:---:|:---:|:---:|:---:\n    [Model A1, 2 conv.](https://cdn.rawgit.com/itijyou/ademxapp/master/misc/ade20k_model_a1.pdf)|500|80.55|43.34|[aar](https://cloudstor.aarnet.edu.au/plus/index.php/s/E4JeZpmssK50kpn)\n\n\n### Citation\n\nIf you use this code or these models in your research, please cite:\n\n    @Misc{word.zifeng.2016,\n        author = {Zifeng Wu and Chunhua Shen and Anton van den Hengel},\n        title = {Wider or Deeper: {R}evisiting the ResNet Model for Visual Recognition},\n        year = {2016}\n        howpublished = {arXiv:1611.10080}\n    }\n\n\n### License\n\nThis code is only for academic purpose. For commercial purpose, please contact us.\n\n\n### Acknowledgement\n\nThis work is supported with supercomputing resources provided by the PSG cluster at NVIDIA and the Phoenix HPC service at the University of Adelaide.\n\n",
            "readme_url": "https://github.com/itijyou/ademxapp",
            "frameworks": [
                "MXNet"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "Wider or Deeper: Revisiting the ResNet Model for Visual Recognition",
            "arxiv": "1611.10080",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.10080v1",
            "abstract": "The trend towards increasingly deep neural networks has been driven by a\ngeneral observation that increasing depth increases the performance of a\nnetwork. Recently, however, evidence has been amassing that simply increasing\ndepth may not be the best way to increase performance, particularly given other\nlimitations. Investigations into deep residual networks have also suggested\nthat they may not in fact be operating as a single deep network, but rather as\nan ensemble of many relatively shallow networks. We examine these issues, and\nin doing so arrive at a new interpretation of the unravelled view of deep\nresidual networks which explains some of the behaviours that have been observed\nexperimentally. As a result, we are able to derive a new, shallower,\narchitecture of residual networks which significantly outperforms much deeper\nmodels such as ResNet-200 on the ImageNet classification dataset. We also show\nthat this performance is transferable to other problem domains by developing a\nsemantic segmentation approach which outperforms the state-of-the-art by a\nremarkable margin on datasets including PASCAL VOC, PASCAL Context, and\nCityscapes. The architecture that we propose thus outperforms its comparators,\nincluding very deep ResNets, and yet is more efficient in memory use and\nsometimes also in training time. The code and models are available at\nhttps://github.com/itijyou/ademxapp",
            "authors": [
                "Zifeng Wu",
                "Chunhua Shen",
                "Anton van den Hengel"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Cityscapes dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://www.cityscapes-dataset.com/downloads/"
                    }
                }
            },
            {
                "name": "MIT Scene Parsing dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "http://sceneparsing.csail.mit.edu/"
                    }
                }
            },
            {
                "name": "ILSVRC 2016"
            },
            {
                "name": "PASCAL VOC 2012"
            },
            {
                "name": "SBD"
            },
            {
                "name": "ADE20K"
            },
            {
                "name": "COCO"
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "Cityscapes"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "PASCAL Context"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999835937063344,
        "task": "Semantic Segmentation",
        "task_prob": 0.9532384102097822
    }
}