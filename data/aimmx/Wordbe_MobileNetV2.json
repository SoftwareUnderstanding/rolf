{
    "visibility": {
        "visibility": "public"
    },
    "name": "Quick Draw",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "Wordbe",
                "owner_type": "User",
                "name": "MobileNetV2",
                "url": "https://github.com/Wordbe/MobileNetV2",
                "stars": 0,
                "pushed_at": "2019-11-04 15:54:07+00:00",
                "created_at": "2019-05-19 14:17:08+00:00",
                "language": "Python",
                "description": "kaggle competition - Quick Draw : Light weight Deep learning",
                "frameworks": [
                    "Keras",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "test_and_submission.py",
                "sha": "6a0640e0fb69f0f5333776bc2f984dfc49094f0c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Wordbe/MobileNetV2/blob/master/test_and_submission.py"
                    }
                },
                "size": 1149
            },
            {
                "type": "code",
                "name": "train_kaggleQuickDraw.py",
                "sha": "3d0ecdf356575e1bf86a2805d1fb455ff351350f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Wordbe/MobileNetV2/blob/master/train_kaggleQuickDraw.py"
                    }
                },
                "size": 4511
            }
        ]
    },
    "authors": [
        {
            "name": "Seongho Jin",
            "github_id": "Wordbe"
        }
    ],
    "tags": [],
    "description": "kaggle competition - Quick Draw : Light weight Deep learning",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/Wordbe/MobileNetV2",
            "stars": 0,
            "issues": true,
            "readme": "# Quick Draw\n\nQuick, Draw! Doodle Recognition Challenge\nHow accurately can you identify a doodle?\n\n![](https://storage.googleapis.com/kaggle-media/competitions/quickdraw/what-does-a-bee-look-like-1.png)\n\nhttps://www.kaggle.com/c/quickdraw-doodle-recognition\n\n\nTrain with MobileNetV2\n\ntrain3401000.csv - train image 340 classes, 1000 each (70%)\n\nval3401000.csv - validation image 340 classes, 1000 each (30%)\n\n\n# MobileNetV2\n\nMobileNetV2: Inverted Residuals and Linear Bottlenecks\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen\n(Submitted on 13 Jan 2018 (v1), last revised 21 Mar 2019 (this version, v4))\n\nhttps://arxiv.org/abs/1801.04381\n\n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTzGfStNCy3jhIG-y7Re8CuvK90UpZp_x-uj9Hm4E7tEg8qqc781Q&s)\n\n\n\n\n\n# Geting Started\n  \n1. Set config\n\n2. Train the model\n\n3. Evaluate using test dataset\n",
            "readme_url": "https://github.com/Wordbe/MobileNetV2",
            "frameworks": [
                "Keras",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
            "arxiv": "1801.04381",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.04381v4",
            "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that\nimproves the state of the art performance of mobile models on multiple tasks\nand benchmarks as well as across a spectrum of different model sizes. We also\ndescribe efficient ways of applying these mobile models to object detection in\na novel framework we call SSDLite. Additionally, we demonstrate how to build\nmobile semantic segmentation models through a reduced form of DeepLabv3 which\nwe call Mobile DeepLabv3.\n  The MobileNetV2 architecture is based on an inverted residual structure where\nthe input and output of the residual block are thin bottleneck layers opposite\nto traditional residual models which use expanded representations in the input\nan MobileNetV2 uses lightweight depthwise convolutions to filter features in\nthe intermediate expansion layer. Additionally, we find that it is important to\nremove non-linearities in the narrow layers in order to maintain\nrepresentational power. We demonstrate that this improves performance and\nprovide an intuition that led to this design. Finally, our approach allows\ndecoupling of the input/output domains from the expressiveness of the\ntransformation, which provides a convenient framework for further analysis. We\nmeasure our performance on Imagenet classification, COCO object detection, VOC\nimage segmentation. We evaluate the trade-offs between accuracy, and number of\noperations measured by multiply-adds (MAdd), as well as the number of\nparameters",
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9995353865953878,
        "task": "Object Detection",
        "task_prob": 0.48449477380018596
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "COCO"
            }
        ]
    }
}