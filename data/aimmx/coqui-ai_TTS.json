{
    "visibility": {
        "visibility": "public",
        "license": "Mozilla Public License 2.0"
    },
    "name": "TTS",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "coqui-ai",
                "owner_type": "Organization",
                "name": "TTS",
                "url": "https://github.com/coqui-ai/TTS",
                "stars": 4367,
                "pushed_at": "2022-03-28 22:05:09+00:00",
                "created_at": "2020-05-20 15:45:28+00:00",
                "language": "Python",
                "description": "\ud83d\udc38\ud83d\udcac - a deep learning toolkit for Text-to-Speech, battle-tested in research and production",
                "license": "Mozilla Public License 2.0",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".cardboardlint.yml",
                "sha": "4a115a37cddb065c76afebc905476e650f53d085",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/.cardboardlint.yml"
                    }
                },
                "size": 111
            },
            {
                "type": "code",
                "name": ".dockerignore",
                "sha": "4032ec6b7c844f4835cb3bacb31387e55301d1f5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/.dockerignore"
                    }
                },
                "size": 5
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "c0703d7d8859785b112d848424fe0c50ad51dc0e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/tree/main/.github"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "f8d6e644f467f8e7f91d44fd25233d1e42b53146",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/.gitignore"
                    }
                },
                "size": 2013
            },
            {
                "type": "code",
                "name": ".pre-commit-config.yaml",
                "sha": "a70572dc9e2046e7c37317049aeb235736cce975",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/.pre-commit-config.yaml"
                    }
                },
                "size": 636
            },
            {
                "type": "code",
                "name": ".pylintrc",
                "sha": "d5f9c4909cb3fe0faeb41d4ec72764c1c69ec754",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/.pylintrc"
                    }
                },
                "size": 18355
            },
            {
                "type": "code",
                "name": ".readthedocs.yml",
                "sha": "946d363cff24913f01fffef6b5a2e868f99ad14b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/.readthedocs.yml"
                    }
                },
                "size": 469
            },
            {
                "type": "code",
                "name": "CODE_OF_CONDUCT.md",
                "sha": "b80639d63c29e902c547de347806651bcc9ad3b2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/CODE_OF_CONDUCT.md"
                    }
                },
                "size": 5488
            },
            {
                "type": "code",
                "name": "CODE_OWNERS.rst",
                "sha": "768b573911eae8aeb229de6f56039deb9a64ce27",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/CODE_OWNERS.rst"
                    }
                },
                "size": 4106
            },
            {
                "type": "code",
                "name": "CONTRIBUTING.md",
                "sha": "7175cf34808f4c1ac8ff7d352289f59de70aea1a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/CONTRIBUTING.md"
                    }
                },
                "size": 5417
            },
            {
                "type": "code",
                "name": "LICENSE.txt",
                "sha": "14e2f777f6c395e7e04ab4aa306bbcc4b0c1120e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/LICENSE.txt"
                    }
                },
                "size": 16726
            },
            {
                "type": "code",
                "name": "MANIFEST.in",
                "sha": "0d8b4b4ce4f3a940f9e31a7e49fa5d6134f04f89",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/MANIFEST.in"
                    }
                },
                "size": 310
            },
            {
                "type": "code",
                "name": "Makefile",
                "sha": "d04cd9761d3af685b905d1c7a00b0a9c0ab7ea2e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/Makefile"
                    }
                },
                "size": 2570
            },
            {
                "type": "code",
                "name": "TTS",
                "sha": "bf66666dde7ccad076f7817e898d52d5e331a66f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/tree/main/TTS"
                    }
                },
                "num_files": 11
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "4d4fb2186a27a887cfad2faca0c6a9faf347ff64",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/tree/main/docs"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "hubconf.py",
                "sha": "0c9c5930fcbf98962d3086e7537aa3941b191083",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/hubconf.py"
                    }
                },
                "size": 1892
            },
            {
                "type": "code",
                "name": "images",
                "sha": "7ec172b8283b254d2cbcf3dc44ad1af61e752c13",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/tree/main/images"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "notebooks",
                "sha": "609175d37c340376e8fa7529b84cff9c7da79ced",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/tree/main/notebooks"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "pyproject.toml",
                "sha": "0941a90681b87ee3e1872d0fccfaecb88254705f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/pyproject.toml"
                    }
                },
                "size": 598
            },
            {
                "type": "code",
                "name": "recipes",
                "sha": "310da2594dbc079fb40a281a57256d125c3bf043",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/tree/main/recipes"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "requirements.dev.txt",
                "sha": "c995f9e6a99cc2e2ccf89652cfd1e3335aee825c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/requirements.dev.txt"
                    }
                },
                "size": 41
            },
            {
                "type": "code",
                "name": "requirements.notebooks.txt",
                "sha": "65d3f642c9dcaf109cd8697beb8672f53a81dd59",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/requirements.notebooks.txt"
                    }
                },
                "size": 12
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "e3871874df03d1f6b49c8f2393388b61fddcae98",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/requirements.txt"
                    }
                },
                "size": 587
            },
            {
                "type": "code",
                "name": "run_bash_tests.sh",
                "sha": "feb9082bd3f566ca0b3636fc249bade741e19536",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/run_bash_tests.sh"
                    }
                },
                "size": 218
            },
            {
                "type": "code",
                "name": "setup.cfg",
                "sha": "2344c8b289944835fe78c6ecaf467c1197b8e2e7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/setup.cfg"
                    }
                },
                "size": 104
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "96173fecc90d3a90c879a7c96f519d2ec205625b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/blob/main/setup.py"
                    }
                },
                "size": 5171
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "2b6322403724422c31609ed2ce76ab1545478e91",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/coqui-ai/TTS/tree/main/tests"
                    }
                },
                "num_files": 12
            }
        ]
    },
    "authors": [
        {
            "name": "Eren G\u00f6lge",
            "email": "egolge@coqui.ai",
            "github_id": "erogol"
        },
        {
            "name": "Edresson Casanova",
            "email": "edresson1@gmail.com",
            "github_id": "Edresson"
        },
        {
            "name": "WeberJulian",
            "github_id": "WeberJulian"
        },
        {
            "name": "Alexander Korolev",
            "github_id": "lexkoro"
        },
        {
            "name": "Thomas Werkmeister",
            "email": "thomas@werkmeister.me",
            "github_id": "twerkmeister"
        },
        {
            "name": "Reuben Morais",
            "email": "reuben.morais@gmail.com",
            "github_id": "reuben"
        },
        {
            "name": "Thorsten M\u00fcller",
            "github_id": "thorstenMueller"
        },
        {
            "name": "Kirian Guiller",
            "github_id": "kirianguiller"
        },
        {
            "name": "Branislav Gerazov",
            "email": "gerazov@feit.ukim.edu.mk",
            "github_id": "gerazov"
        },
        {
            "name": "Thorben Hellweg",
            "github_id": "thllwg"
        },
        {
            "name": "J\u00f6rg Thalheim",
            "github_id": "Mic92"
        },
        {
            "name": "Ayush Chaurasia",
            "email": "ayush.chaurarsia@gmail.com",
            "github_id": "AyushExel"
        },
        {
            "name": "Katsuya Iida",
            "email": "katsuya.iida@gmail.com",
            "github_id": "kaiidams"
        },
        {
            "name": "Neil Stoker",
            "github_id": "nmstoker"
        },
        {
            "name": "Rishikesh (\u090b\u0937\u093f\u0915\u0947\u0936)",
            "email": "rishikksh20@gmail.com",
            "github_id": "rishikksh20"
        },
        {
            "name": "Michael Hansen",
            "email": "hansen.mike@gmail.com",
            "github_id": "synesthesiam"
        },
        {
            "name": "Adonis Pujols",
            "github_id": "adonispujols"
        },
        {
            "name": "bgerazov",
            "github_id": "bgerazov"
        },
        {
            "name": "mittimithai",
            "github_id": "mittimithai"
        },
        {
            "name": "Agrin Hilmkil",
            "github_id": "agrinh"
        },
        {
            "name": "Markus Toman",
            "github_id": "m-toman"
        },
        {
            "name": "geneing",
            "github_id": "geneing"
        },
        {
            "name": "Guy Elsmore-Paddock",
            "email": "guy.paddock@gmail.com",
            "github_id": "GuyPaddock"
        },
        {
            "name": "Martin Weinelt",
            "github_id": "mweinelt"
        },
        {
            "name": "QP Hou",
            "github_id": "houqp"
        },
        {
            "name": "dependabot[bot]",
            "github_id": "dependabot[bot]"
        },
        {
            "name": "jyegerlehner",
            "github_id": "jyegerlehner"
        },
        {
            "name": "a-froghyar",
            "github_id": "a-froghyar"
        },
        {
            "name": "Anand...",
            "github_id": "anand-371"
        },
        {
            "name": "Bajibabu Bollepalli",
            "github_id": "bajibabu"
        },
        {
            "name": "forcecore",
            "github_id": "forcecore"
        },
        {
            "name": "Sadam Hussain Memon",
            "email": "P146095@nu.edu.pk",
            "github_id": "Sadam1195"
        },
        {
            "name": "Tom Rochette",
            "email": "roctom@gmail.com",
            "github_id": "tomzx"
        },
        {
            "name": "Yves-Noel Weweler",
            "email": "y.weweler@gmail.com",
            "github_id": "yweweler"
        },
        {
            "name": "fatihkiralioglu",
            "github_id": "fatihkiralioglu"
        },
        {
            "name": "fijipants",
            "github_id": "fijipants"
        },
        {
            "name": "Ludev",
            "email": "luhavis.dev@gmail.com",
            "github_id": "luhavis"
        },
        {
            "name": "ravi-maithrey",
            "github_id": "ravi-maithrey"
        },
        {
            "name": "AXKuhta",
            "github_id": "AXKuhta"
        },
        {
            "name": "Aloento",
            "github_id": "Aloento"
        },
        {
            "name": "Baybars K\u00fclebi",
            "github_id": "gullabi"
        },
        {
            "name": "George",
            "github_id": "george-roussos"
        },
        {
            "name": "Han Xiao",
            "email": "han.xiao@jina.ai",
            "github_id": "hanxiao"
        },
        {
            "name": "Josh Meyer",
            "github_id": "JRMeyer"
        },
        {
            "name": "Kathy Reid",
            "email": "github@kathyreid.id.au",
            "github_id": "KathyReid"
        },
        {
            "name": "Mark Kockerbeck",
            "github_id": "xeb"
        },
        {
            "name": "Mozilla-GitHub-Standards",
            "github_id": "Mozilla-GitHub-Standards"
        },
        {
            "name": "Mo ",
            "github_id": "0xMilly"
        },
        {
            "name": "Noran Raskin",
            "github_id": "noranraskin"
        },
        {
            "name": "Peter R",
            "github_id": "PNRxA"
        },
        {
            "name": "\u2764\ufe0f",
            "github_id": "cuuupid"
        },
        {
            "name": "Seung-won Park",
            "email": "yyyyy@snu.ac.kr",
            "github_id": "seungwonpark"
        },
        {
            "name": "V",
            "github_id": "Vproject"
        },
        {
            "name": "Yorwba",
            "github_id": "Yorwba"
        },
        {
            "name": "Ayush Shridhar",
            "email": "ayush.shridhar1506@gmail.com",
            "github_id": "ayush-1506"
        },
        {
            "name": "Sivasurya Santhanam",
            "email": "sivasurya.in@gmail.com",
            "github_id": "chmodsss"
        },
        {
            "name": "Fabrizio",
            "github_id": "gnosly"
        },
        {
            "name": "kdavis-coqui",
            "github_id": "kdavis-coqui"
        },
        {
            "name": "krzim",
            "github_id": "krzim"
        },
        {
            "name": "logan hart",
            "github_id": "loganhart420"
        },
        {
            "name": "lokkelvin2",
            "github_id": "lokkelvin2"
        },
        {
            "name": "Max Bachmann",
            "email": "kontakt@maxbachmann.de",
            "github_id": "maxbachmann"
        },
        {
            "name": "r-dh",
            "github_id": "r-dh"
        },
        {
            "name": "repodiac",
            "github_id": "repodiac"
        },
        {
            "name": "richardburleigh",
            "github_id": "richardburleigh"
        },
        {
            "name": "shaun",
            "github_id": "shaun95"
        },
        {
            "name": "tset-tset-tset",
            "github_id": "tset-tset-tset"
        },
        {
            "name": "Lukasz Stolcman",
            "github_id": "lstolcman"
        }
    ],
    "tags": [
        "python",
        "text-to-speech",
        "deep-learning",
        "speech",
        "pytorch",
        "tts",
        "vocoder",
        "tacotron",
        "glow-tts",
        "melgan",
        "speaker-encoder",
        "hifigan",
        "speaker-encodings",
        "multi-speaker-tts",
        "tts-model",
        "speech-synthesis",
        "voice-cloning",
        "voice-synthesis"
    ],
    "description": "\ud83d\udc38\ud83d\udcac - a deep learning toolkit for Text-to-Speech, battle-tested in research and production",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/coqui-ai/TTS",
            "stars": 4367,
            "issues": true,
            "readme": "# <img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png\" height=\"56\"/>\n\n\ud83d\udc38TTS is a library for advanced Text-to-Speech generation. It's built on the latest research, was designed to achieve the best trade-off among ease-of-training, speed and quality.\n\ud83d\udc38TTS comes with pretrained models, tools for measuring dataset quality and already used in **20+ languages** for products and research projects.\n\n[![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/main.yml/badge.svg)](https://github.com/coqui-ai/TTS/actions)\n[![PyPI version](https://badge.fury.io/py/TTS.svg)](https://badge.fury.io/py/TTS)\n[![Covenant](https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667)](https://github.com/coqui-ai/TTS/blob/master/CODE_OF_CONDUCT.md)\n[![Downloads](https://pepy.tech/badge/tts)](https://pepy.tech/project/tts)\n[![DOI](https://zenodo.org/badge/265612440.svg)](https://zenodo.org/badge/latestdoi/265612440)\n\n[![Docs](<https://readthedocs.org/projects/tts/badge/?version=latest&style=plastic>)](https://tts.readthedocs.io/en/latest/)\n[![Gitter](https://badges.gitter.im/coqui-ai/TTS.svg)](https://gitter.im/coqui-ai/TTS?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n[![License](<https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg>)](https://opensource.org/licenses/MPL-2.0)\n\n\ud83d\udcf0 [**Subscribe to \ud83d\udc38Coqui.ai Newsletter**](https://coqui.ai/?subscription=true)\n\n\ud83d\udce2 [English Voice Samples](https://erogol.github.io/ddc-samples/) and [SoundCloud playlist](https://soundcloud.com/user-565970875/pocket-article-wavernn-and-tacotron2)\n\n\ud83d\udcc4 [Text-to-Speech paper collection](https://github.com/erogol/TTS-papers)\n\n<img src=\"https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2\" />\n\n## \ud83d\udcac Where to ask questions\nPlease use our dedicated channels for questions and discussion. Help is much more valuable if it's shared publicly so that more people can benefit from it.\n\n| Type                            | Platforms                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udea8 **Bug Reports**              | [GitHub Issue Tracker]                  |\n| \ud83c\udf81 **Feature Requests & Ideas** | [GitHub Issue Tracker]                  |\n| \ud83d\udc69\u200d\ud83d\udcbb **Usage Questions**          | [Github Discussions]                    |\n| \ud83d\uddef **General Discussion**       | [Github Discussions] or [Gitter Room]   |\n\n[github issue tracker]: https://github.com/coqui-ai/tts/issues\n[github discussions]: https://github.com/coqui-ai/TTS/discussions\n[gitter room]: https://gitter.im/coqui-ai/TTS?utm_source=share-link&utm_medium=link&utm_campaign=share-link\n[Tutorials and Examples]: https://github.com/coqui-ai/TTS/wiki/TTS-Notebooks-and-Tutorials\n\n\n## \ud83d\udd17 Links and Resources\n| Type                            | Links                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udcbc **Documentation**              | [ReadTheDocs](https://tts.readthedocs.io/en/latest/)\n| \ud83d\udcbe **Installation**               | [TTS/README.md](https://github.com/coqui-ai/TTS/tree/dev#install-tts)|\n| \ud83d\udc69\u200d\ud83d\udcbb **Contributing**               | [CONTRIBUTING.md](https://github.com/coqui-ai/TTS/blob/main/CONTRIBUTING.md)|\n| \ud83d\udccc **Road Map**                   | [Main Development Plans](https://github.com/coqui-ai/TTS/issues/378)\n| \ud83d\ude80 **Released Models**            | [TTS Releases](https://github.com/coqui-ai/TTS/releases) and [Experimental Models](https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models)|\n\n## \ud83e\udd47 TTS Performance\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png\" width=\"800\" /></p>\n\nUnderlined \"TTS*\" and \"Judy*\" are \ud83d\udc38TTS models\n<!-- [Details...](https://github.com/coqui-ai/TTS/wiki/Mean-Opinion-Score-Results) -->\n\n## Features\n- High-performance Deep Learning models for Text2Speech tasks.\n    - Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).\n    - Speaker Encoder to compute speaker embeddings efficiently.\n    - Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)\n- Fast and efficient model training.\n- Detailed training logs on the terminal and Tensorboard.\n- Support for Multi-speaker TTS.\n- Efficient, flexible, lightweight but feature complete `Trainer API`.\n- Released and ready-to-use models.\n- Tools to curate Text2Speech datasets under```dataset_analysis```.\n- Utilities to use and test your models.\n- Modular (but not too much) code base enabling easy implementation of new ideas.\n\n## Implemented Models\n### Text-to-Spectrogram\n- Tacotron: [paper](https://arxiv.org/abs/1703.10135)\n- Tacotron2: [paper](https://arxiv.org/abs/1712.05884)\n- Glow-TTS: [paper](https://arxiv.org/abs/2005.11129)\n- Speedy-Speech: [paper](https://arxiv.org/abs/2008.03802)\n- Align-TTS: [paper](https://arxiv.org/abs/2003.01950)\n- FastPitch: [paper](https://arxiv.org/pdf/2006.06873.pdf)\n- FastSpeech: [paper](https://arxiv.org/abs/1905.09263)\n\n### End-to-End Models\n- VITS: [paper](https://arxiv.org/pdf/2106.06103)\n\n### Attention Methods\n- Guided Attention: [paper](https://arxiv.org/abs/1710.08969)\n- Forward Backward Decoding: [paper](https://arxiv.org/abs/1907.09006)\n- Graves Attention: [paper](https://arxiv.org/abs/1910.10288)\n- Double Decoder Consistency: [blog](https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/)\n- Dynamic Convolutional Attention: [paper](https://arxiv.org/pdf/1910.10288.pdf)\n- Alignment Network: [paper](https://arxiv.org/abs/2108.10447)\n\n### Speaker Encoder\n- GE2E: [paper](https://arxiv.org/abs/1710.10467)\n- Angular Loss: [paper](https://arxiv.org/pdf/2003.11982.pdf)\n\n### Vocoders\n- MelGAN: [paper](https://arxiv.org/abs/1910.06711)\n- MultiBandMelGAN: [paper](https://arxiv.org/abs/2005.05106)\n- ParallelWaveGAN: [paper](https://arxiv.org/abs/1910.11480)\n- GAN-TTS discriminators: [paper](https://arxiv.org/abs/1909.11646)\n- WaveRNN: [origin](https://github.com/fatchord/WaveRNN/)\n- WaveGrad: [paper](https://arxiv.org/abs/2009.00713)\n- HiFiGAN: [paper](https://arxiv.org/abs/2010.05646)\n- UnivNet: [paper](https://arxiv.org/abs/2106.07889)\n\nYou can also help us implement more models.\n\n## Install TTS\n\ud83d\udc38TTS is tested on Ubuntu 18.04 with **python >= 3.6, < 3.9**.\n\nIf you are only interested in [synthesizing speech](https://tts.readthedocs.io/en/latest/inference.html) with the released \ud83d\udc38TTS models, installing from PyPI is the easiest option.\n\n```bash\npip install TTS\n```\n\nIf you plan to code or train models, clone \ud83d\udc38TTS and install it locally.\n\n```bash\ngit clone https://github.com/coqui-ai/TTS\npip install -e .[all,dev,notebooks]  # Select the relevant extras\n```\n\nIf you are on Ubuntu (Debian), you can also run following commands for installation.\n\n```bash\n$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a diffent OS.\n$ make install\n```\n\nIf you are on Windows, \ud83d\udc51@GuyPaddock wrote installation instructions [here](https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system).\n\n## Use TTS\n\n### Single Speaker Models\n\n- List provided models:\n\n    ```\n    $ tts --list_models\n    ```\n\n- Run TTS with default models:\n\n    ```\n    $ tts --text \"Text for TTS\"\n    ```\n\n- Run a TTS model with its default vocoder model:\n\n    ```\n    $ tts --text \"Text for TTS\" --model_name \"<language>/<dataset>/<model_name>\n    ```\n\n- Run with specific TTS and vocoder models from the list:\n\n    ```\n    $ tts --text \"Text for TTS\" --model_name \"<language>/<dataset>/<model_name>\" --vocoder_name \"<language>/<dataset>/<model_name>\" --output_path\n    ```\n\n- Run your own TTS model (Using Griffin-Lim Vocoder):\n\n    ```\n    $ tts --text \"Text for TTS\" --model_path path/to/model.pth.tar --config_path path/to/config.json --out_path output/path/speech.wav\n    ```\n\n- Run your own TTS and Vocoder models:\n    ```\n    $ tts --text \"Text for TTS\" --model_path path/to/config.json --config_path path/to/model.pth.tar --out_path output/path/speech.wav\n        --vocoder_path path/to/vocoder.pth.tar --vocoder_config_path path/to/vocoder_config.json\n    ```\n\n### Multi-speaker Models\n\n- List the available speakers and choose as <speaker_id> among them:\n\n    ```\n    $ tts --model_name \"<language>/<dataset>/<model_name>\"  --list_speaker_idxs\n    ```\n\n- Run the multi-speaker TTS model with the target speaker ID:\n\n    ```\n    $ tts --text \"Text for TTS.\" --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\"  --speaker_idx <speaker_id>\n    ```\n\n- Run your own multi-speaker TTS model:\n\n    ```\n    $ tts --text \"Text for TTS\" --out_path output/path/speech.wav --model_path path/to/config.json --config_path path/to/model.pth.tar --speakers_file_path path/to/speaker.json --speaker_idx <speaker_id>\n    ```\n\n## Directory Structure\n```\n|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)\n|- utils/           (common utilities.)\n|- TTS\n    |- bin/             (folder for all the executables.)\n      |- train*.py                  (train your target model.)\n      |- distribute.py              (train your TTS model using Multiple GPUs.)\n      |- compute_statistics.py      (compute dataset statistics for normalization.)\n      |- ...\n    |- tts/             (text to speech models)\n        |- layers/          (model layer definitions)\n        |- models/          (model definitions)\n        |- utils/           (model specific utilities.)\n    |- speaker_encoder/ (Speaker Encoder models.)\n        |- (same)\n    |- vocoder/         (Vocoder models.)\n        |- (same)\n```\n",
            "readme_url": "https://github.com/coqui-ai/TTS",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search",
            "arxiv": "2005.11129",
            "year": 2020,
            "url": "http://arxiv.org/abs/2005.11129v2",
            "abstract": "Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have\nbeen proposed to generate mel-spectrograms from text in parallel. Despite the\nadvantage, the parallel TTS models cannot be trained without guidance from\nautoregressive TTS models as their external aligners. In this work, we propose\nGlow-TTS, a flow-based generative model for parallel TTS that does not require\nany external aligner. By combining the properties of flows and dynamic\nprogramming, the proposed model searches for the most probable monotonic\nalignment between text and the latent representation of speech on its own. We\ndemonstrate that enforcing hard monotonic alignments enables robust TTS, which\ngeneralizes to long utterances, and employing generative flows enables fast,\ndiverse, and controllable speech synthesis. Glow-TTS obtains an\norder-of-magnitude speed-up over the autoregressive model, Tacotron 2, at\nsynthesis with comparable speech quality. We further show that our model can be\neasily extended to a multi-speaker setting.",
            "authors": [
                "Jaehyeon Kim",
                "Sungwon Kim",
                "Jungil Kong",
                "Sungroh Yoon"
            ]
        },
        {
            "title": "UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation",
            "arxiv": "2106.07889",
            "year": 2021,
            "url": "http://arxiv.org/abs/2106.07889v1",
            "abstract": "Most neural vocoders employ band-limited mel-spectrograms to generate\nwaveforms. If full-band spectral features are used as the input, the vocoder\ncan be provided with as much acoustic information as possible. However, in some\nmodels employing full-band mel-spectrograms, an over-smoothing problem occurs\nas part of which non-sharp spectrograms are generated. To address this problem,\nwe propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms\nin real time. Inspired by works in the field of voice activity detection, we\nadded a multi-resolution spectrogram discriminator that employs multiple linear\nspectrogram magnitudes computed using various parameter sets. Using full-band\nmel-spectrograms as input, we expect to generate high-resolution signals by\nadding a discriminator that employs spectrograms of multiple resolutions as the\ninput. In an evaluation on a dataset containing information on hundreds of\nspeakers, UnivNet obtained the best objective and subjective results among\ncompeting models for both seen and unseen speakers. These results, including\nthe best subjective score for text-to-speech, demonstrate the potential for\nfast adaptation to new speakers without a need for training from scratch.",
            "authors": [
                "Won Jang",
                "Dan Lim",
                "Jaesam Yoon",
                "Bongwan Kim",
                "Juntae Kim"
            ]
        },
        {
            "title": "Location-Relative Attention Mechanisms For Robust Long-Form Speech Synthesis",
            "arxiv": "1910.10288",
            "year": 2019,
            "url": "http://arxiv.org/abs/1910.10288v2",
            "abstract": "Despite the ability to produce human-level speech for in-domain text,\nattention-based end-to-end text-to-speech (TTS) systems suffer from text\nalignment failures that increase in frequency for out-of-domain text. We show\nthat these failures can be addressed using simple location-relative attention\nmechanisms that do away with content-based query/key comparisons. We compare\ntwo families of attention mechanisms: location-relative GMM-based mechanisms\nand additive energy-based mechanisms. We suggest simple modifications to\nGMM-based attention that allow it to align quickly and consistently during\ntraining, and introduce a new location-relative attention mechanism to the\nadditive energy-based family, called Dynamic Convolution Attention (DCA). We\ncompare the various mechanisms in terms of alignment speed and consistency\nduring training, naturalness, and ability to generalize to long utterances, and\nconclude that GMM attention and DCA can generalize to very long utterances,\nwhile preserving naturalness for shorter, in-domain utterances.",
            "authors": [
                "Eric Battenberg",
                "RJ Skerry-Ryan",
                "Soroosh Mariooryad",
                "Daisy Stanton",
                "David Kao",
                "Matt Shannon",
                "Tom Bagby"
            ]
        },
        {
            "title": "Multi-band MelGAN: Faster Waveform Generation for High-Quality Text-to-Speech",
            "arxiv": "2005.05106",
            "year": 2020,
            "url": "http://arxiv.org/abs/2005.05106v2",
            "abstract": "In this paper, we propose multi-band MelGAN, a much faster waveform\ngeneration model targeting to high-quality text-to-speech. Specifically, we\nimprove the original MelGAN by the following aspects. First, we increase the\nreceptive field of the generator, which is proven to be beneficial to speech\ngeneration. Second, we substitute the feature matching loss with the\nmulti-resolution STFT loss to better measure the difference between fake and\nreal speech. Together with pre-training, this improvement leads to both better\nquality and better training stability. More importantly, we extend MelGAN with\nmulti-band processing: the generator takes mel-spectrograms as input and\nproduces sub-band signals which are subsequently summed back to full-band\nsignals as discriminator input. The proposed multi-band MelGAN has achieved\nhigh MOS of 4.34 and 4.22 in waveform generation and TTS, respectively. With\nonly 1.91M parameters, our model effectively reduces the total computational\ncomplexity of the original MelGAN from 5.85 to 0.95 GFLOPS. Our Pytorch\nimplementation, which will be open-resourced shortly, can achieve a real-time\nfactor of 0.03 on CPU without hardware specific optimization.",
            "authors": [
                "Geng Yang",
                "Shan Yang",
                "Kai Liu",
                "Peng Fang",
                "Wei Chen",
                "Lei Xie"
            ]
        },
        {
            "title": "In defence of metric learning for speaker recognition",
            "arxiv": "2003.11982",
            "year": 2020,
            "url": "http://arxiv.org/abs/2003.11982v2",
            "abstract": "The objective of this paper is 'open-set' speaker recognition of unseen\nspeakers, where ideal embeddings should be able to condense information into a\ncompact utterance-level representation that has small intra-speaker and large\ninter-speaker distance.\n  A popular belief in speaker recognition is that networks trained with\nclassification objectives outperform metric learning methods. In this paper, we\npresent an extensive evaluation of most popular loss functions for speaker\nrecognition on the VoxCeleb dataset. We demonstrate that the vanilla triplet\nloss shows competitive performance compared to classification-based losses, and\nthose trained with our proposed metric learning objective outperform\nstate-of-the-art methods.",
            "authors": [
                "Joon Son Chung",
                "Jaesung Huh",
                "Seongkyu Mun",
                "Minjae Lee",
                "Hee Soo Heo",
                "Soyeon Choe",
                "Chiheon Ham",
                "Sunghwan Jung",
                "Bong-Jin Lee",
                "Icksang Han"
            ]
        },
        {
            "title": "SpeedySpeech: Efficient Neural Speech Synthesis",
            "arxiv": "2008.03802",
            "year": 2020,
            "url": "http://arxiv.org/abs/2008.03802v1",
            "abstract": "While recent neural sequence-to-sequence models have greatly improved the\nquality of speech synthesis, there has not been a system capable of fast\ntraining, fast inference and high-quality audio synthesis at the same time. We\npropose a student-teacher network capable of high-quality faster-than-real-time\nspectrogram synthesis, with low requirements on computational resources and\nfast training time. We show that self-attention layers are not necessary for\ngeneration of high quality audio. We utilize simple convolutional blocks with\nresidual connections in both student and teacher networks and use only a single\nattention layer in the teacher model. Coupled with a MelGAN vocoder, our\nmodel's voice quality was rated significantly higher than Tacotron 2. Our model\ncan be efficiently trained on a single GPU and can run in real time even on a\nCPU. We provide both our source code and audio samples in our GitHub\nrepository.",
            "authors": [
                "Jan Vainer",
                "Ond\u0159ej Du\u0161ek"
            ]
        },
        {
            "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
            "arxiv": "2010.05646",
            "year": 2020,
            "url": "http://arxiv.org/abs/2010.05646v2",
            "abstract": "Several recent work on speech synthesis have employed generative adversarial\nnetworks (GANs) to produce raw waveforms. Although such methods improve the\nsampling efficiency and memory usage, their sample quality has not yet reached\nthat of autoregressive and flow-based generative models. In this work, we\npropose HiFi-GAN, which achieves both efficient and high-fidelity speech\nsynthesis. As speech audio consists of sinusoidal signals with various periods,\nwe demonstrate that modeling periodic patterns of an audio is crucial for\nenhancing sample quality. A subjective human evaluation (mean opinion score,\nMOS) of a single speaker dataset indicates that our proposed method\ndemonstrates similarity to human quality while generating 22.05 kHz\nhigh-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We\nfurther show the generality of HiFi-GAN to the mel-spectrogram inversion of\nunseen speakers and end-to-end speech synthesis. Finally, a small footprint\nversion of HiFi-GAN generates samples 13.4 times faster than real-time on CPU\nwith comparable quality to an autoregressive counterpart.",
            "authors": [
                "Jungil Kong",
                "Jaehyeon Kim",
                "Jaekyoung Bae"
            ]
        },
        {
            "title": "Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram",
            "arxiv": "1910.11480",
            "year": 2019,
            "url": "http://arxiv.org/abs/1910.11480v2",
            "abstract": "We propose Parallel WaveGAN, a distillation-free, fast, and small-footprint\nwaveform generation method using a generative adversarial network. In the\nproposed method, a non-autoregressive WaveNet is trained by jointly optimizing\nmulti-resolution spectrogram and adversarial loss functions, which can\neffectively capture the time-frequency distribution of the realistic speech\nwaveform. As our method does not require density distillation used in the\nconventional teacher-student framework, the entire model can be easily trained.\nFurthermore, our model is able to generate high-fidelity speech even with its\ncompact architecture. In particular, the proposed Parallel WaveGAN has only\n1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster\nthan real-time on a single GPU environment. Perceptual listening test results\nverify that our proposed method achieves 4.16 mean opinion score within a\nTransformer-based text-to-speech framework, which is comparative to the best\ndistillation-based Parallel WaveNet system.",
            "authors": [
                "Ryuichi Yamamoto",
                "Eunwoo Song",
                "Jae-Min Kim"
            ]
        },
        {
            "title": "FastPitch: Parallel Text-to-speech with Pitch Prediction",
            "arxiv": "2006.06873",
            "year": 2020,
            "url": "http://arxiv.org/abs/2006.06873v2",
            "abstract": "We present FastPitch, a fully-parallel text-to-speech model based on\nFastSpeech, conditioned on fundamental frequency contours. The model predicts\npitch contours during inference. By altering these predictions, the generated\nspeech can be more expressive, better match the semantic of the utterance, and\nin the end more engaging to the listener. Uniformly increasing or decreasing\npitch with FastPitch generates speech that resembles the voluntary modulation\nof voice. Conditioning on frequency contours improves the overall quality of\nsynthesized speech, making it comparable to state-of-the-art. It does not\nintroduce an overhead, and FastPitch retains the favorable, fully-parallel\nTransformer architecture, with over 900x real-time factor for mel-spectrogram\nsynthesis of a typical utterance.",
            "authors": [
                "Adrian \u0141a\u0144cucki"
            ]
        },
        {
            "title": "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention",
            "arxiv": "1710.08969",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.08969v2",
            "abstract": "This paper describes a novel text-to-speech (TTS) technique based on deep\nconvolutional neural networks (CNN), without use of any recurrent units.\nRecurrent neural networks (RNN) have become a standard technique to model\nsequential data recently, and this technique has been used in some cutting-edge\nneural TTS techniques. However, training RNN components often requires a very\npowerful computer, or a very long time, typically several days or weeks. Recent\nother studies, on the other hand, have shown that CNN-based sequence synthesis\ncan be much faster than RNN-based techniques, because of high\nparallelizability. The objective of this paper is to show that an alternative\nneural TTS based only on CNN alleviate these economic costs of training. In our\nexperiment, the proposed Deep Convolutional TTS was sufficiently trained\novernight (15 hours), using an ordinary gaming PC equipped with two GPUs, while\nthe quality of the synthesized speech was almost acceptable.",
            "authors": [
                "Hideyuki Tachibana",
                "Katsuya Uenoyama",
                "Shunsuke Aihara"
            ]
        },
        {
            "title": "Tacotron: Towards End-to-End Speech Synthesis",
            "arxiv": "1703.10135",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.10135v2",
            "abstract": "A text-to-speech synthesis system typically consists of multiple stages, such\nas a text analysis frontend, an acoustic model and an audio synthesis module.\nBuilding these components often requires extensive domain expertise and may\ncontain brittle design choices. In this paper, we present Tacotron, an\nend-to-end generative text-to-speech model that synthesizes speech directly\nfrom characters. Given <text, audio> pairs, the model can be trained completely\nfrom scratch with random initialization. We present several key techniques to\nmake the sequence-to-sequence framework perform well for this challenging task.\nTacotron achieves a 3.82 subjective 5-scale mean opinion score on US English,\noutperforming a production parametric system in terms of naturalness. In\naddition, since Tacotron generates speech at the frame level, it's\nsubstantially faster than sample-level autoregressive methods.",
            "authors": [
                "Yuxuan Wang",
                "RJ Skerry-Ryan",
                "Daisy Stanton",
                "Yonghui Wu",
                "Ron J. Weiss",
                "Navdeep Jaitly",
                "Zongheng Yang",
                "Ying Xiao",
                "Zhifeng Chen",
                "Samy Bengio",
                "Quoc Le",
                "Yannis Agiomyrgiannakis",
                "Rob Clark",
                "Rif A. Saurous"
            ]
        },
        {
            "title": "Generalized End-to-End Loss for Speaker Verification",
            "arxiv": "1710.10467",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.10467v5",
            "abstract": "In this paper, we propose a new loss function called generalized end-to-end\n(GE2E) loss, which makes the training of speaker verification models more\nefficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike\nTE2E, the GE2E loss function updates the network in a way that emphasizes\nexamples that are difficult to verify at each step of the training process.\nAdditionally, the GE2E loss does not require an initial stage of example\nselection. With these properties, our model with the new loss function\ndecreases speaker verification EER by more than 10%, while reducing the\ntraining time by 60% at the same time. We also introduce the MultiReader\ntechnique, which allows us to do domain adaptation - training a more accurate\nmodel that supports multiple keywords (i.e. \"OK Google\" and \"Hey Google\") as\nwell as multiple dialects.",
            "authors": [
                "Li Wan",
                "Quan Wang",
                "Alan Papir",
                "Ignacio Lopez Moreno"
            ]
        },
        {
            "title": "MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis",
            "arxiv": "1910.06711",
            "year": 2019,
            "url": "http://arxiv.org/abs/1910.06711v3",
            "abstract": "Previous works (Donahue et al., 2018a; Engel et al., 2019a) have found that\ngenerating coherent raw audio waveforms with GANs is challenging. In this\npaper, we show that it is possible to train GANs reliably to generate high\nquality coherent waveforms by introducing a set of architectural changes and\nsimple training techniques. Subjective evaluation metric (Mean Opinion Score,\nor MOS) shows the effectiveness of the proposed approach for high quality\nmel-spectrogram inversion. To establish the generality of the proposed\ntechniques, we show qualitative results of our model in speech synthesis, music\ndomain translation and unconditional music synthesis. We evaluate the various\ncomponents of the model through ablation studies and suggest a set of\nguidelines to design general purpose discriminators and generators for\nconditional sequence synthesis tasks. Our model is non-autoregressive, fully\nconvolutional, with significantly fewer parameters than competing models and\ngeneralizes to unseen speakers for mel-spectrogram inversion. Our pytorch\nimplementation runs at more than 100x faster than realtime on GTX 1080Ti GPU\nand more than 2x faster than real-time on CPU, without any hardware specific\noptimization tricks.",
            "authors": [
                "Kundan Kumar",
                "Rithesh Kumar",
                "Thibault de Boissiere",
                "Lucas Gestin",
                "Wei Zhen Teoh",
                "Jose Sotelo",
                "Alexandre de Brebisson",
                "Yoshua Bengio",
                "Aaron Courville"
            ]
        },
        {
            "title": "WaveGrad: Estimating Gradients for Waveform Generation",
            "arxiv": "2009.00713",
            "year": 2020,
            "url": "http://arxiv.org/abs/2009.00713v2",
            "abstract": "This paper introduces WaveGrad, a conditional model for waveform generation\nwhich estimates gradients of the data density. The model is built on prior work\non score matching and diffusion probabilistic models. It starts from a Gaussian\nwhite noise signal and iteratively refines the signal via a gradient-based\nsampler conditioned on the mel-spectrogram. WaveGrad offers a natural way to\ntrade inference speed for sample quality by adjusting the number of refinement\nsteps, and bridges the gap between non-autoregressive and autoregressive models\nin terms of audio quality. We find that it can generate high fidelity audio\nsamples using as few as six iterations. Experiments reveal WaveGrad to generate\nhigh fidelity audio, outperforming adversarial non-autoregressive baselines and\nmatching a strong likelihood-based autoregressive baseline using fewer\nsequential operations. Audio samples are available at\nhttps://wavegrad.github.io/.",
            "authors": [
                "Nanxin Chen",
                "Yu Zhang",
                "Heiga Zen",
                "Ron J. Weiss",
                "Mohammad Norouzi",
                "William Chan"
            ]
        },
        {
            "title": "FastSpeech: Fast, Robust and Controllable Text to Speech",
            "arxiv": "1905.09263",
            "year": 2019,
            "url": "http://arxiv.org/abs/1905.09263v5",
            "abstract": "Neural network based end-to-end text to speech (TTS) has significantly\nimproved the quality of synthesized speech. Prominent methods (e.g., Tacotron\n2) usually first generate mel-spectrogram from text, and then synthesize speech\nfrom the mel-spectrogram using vocoder such as WaveNet. Compared with\ntraditional concatenative and statistical parametric approaches, neural network\nbased end-to-end models suffer from slow inference speed, and the synthesized\nspeech is usually not robust (i.e., some words are skipped or repeated) and\nlack of controllability (voice speed or prosody control). In this work, we\npropose a novel feed-forward network based on Transformer to generate\nmel-spectrogram in parallel for TTS. Specifically, we extract attention\nalignments from an encoder-decoder based teacher model for phoneme duration\nprediction, which is used by a length regulator to expand the source phoneme\nsequence to match the length of the target mel-spectrogram sequence for\nparallel mel-spectrogram generation. Experiments on the LJSpeech dataset show\nthat our parallel model matches autoregressive models in terms of speech\nquality, nearly eliminates the problem of word skipping and repeating in\nparticularly hard cases, and can adjust voice speed smoothly. Most importantly,\ncompared with autoregressive Transformer TTS, our model speeds up\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x.\nTherefore, we call our model FastSpeech.",
            "authors": [
                "Yi Ren",
                "Yangjun Ruan",
                "Xu Tan",
                "Tao Qin",
                "Sheng Zhao",
                "Zhou Zhao",
                "Tie-Yan Liu"
            ]
        },
        {
            "title": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions",
            "arxiv": "1712.05884",
            "year": 2017,
            "url": "http://arxiv.org/abs/1712.05884v2",
            "abstract": "This paper describes Tacotron 2, a neural network architecture for speech\nsynthesis directly from text. The system is composed of a recurrent\nsequence-to-sequence feature prediction network that maps character embeddings\nto mel-scale spectrograms, followed by a modified WaveNet model acting as a\nvocoder to synthesize timedomain waveforms from those spectrograms. Our model\nachieves a mean opinion score (MOS) of $4.53$ comparable to a MOS of $4.58$ for\nprofessionally recorded speech. To validate our design choices, we present\nablation studies of key components of our system and evaluate the impact of\nusing mel spectrograms as the input to WaveNet instead of linguistic, duration,\nand $F_0$ features. We further demonstrate that using a compact acoustic\nintermediate representation enables significant simplification of the WaveNet\narchitecture.",
            "authors": [
                "Jonathan Shen",
                "Ruoming Pang",
                "Ron J. Weiss",
                "Mike Schuster",
                "Navdeep Jaitly",
                "Zongheng Yang",
                "Zhifeng Chen",
                "Yu Zhang",
                "Yuxuan Wang",
                "RJ Skerry-Ryan",
                "Rif A. Saurous",
                "Yannis Agiomyrgiannakis",
                "Yonghui Wu"
            ]
        },
        {
            "title": "AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit Alignment",
            "arxiv": "2003.01950",
            "year": 2020,
            "url": "http://arxiv.org/abs/2003.01950v1",
            "abstract": "Targeting at both high efficiency and performance, we propose AlignTTS to\npredict the mel-spectrum in parallel. AlignTTS is based on a Feed-Forward\nTransformer which generates mel-spectrum from a sequence of characters, and the\nduration of each character is determined by a duration predictor.Instead of\nadopting the attention mechanism in Transformer TTS to align text to\nmel-spectrum, the alignment loss is presented to consider all possible\nalignments in training by use of dynamic programming. Experiments on the\nLJSpeech dataset show that our model achieves not only state-of-the-art\nperformance which outperforms Transformer TTS by 0.03 in mean option score\n(MOS), but also a high efficiency which is more than 50 times faster than\nreal-time.",
            "authors": [
                "Zhen Zeng",
                "Jianzong Wang",
                "Ning Cheng",
                "Tian Xia",
                "Jing Xiao"
            ]
        },
        {
            "title": "Forward-Backward Decoding for Regularizing End-to-End TTS",
            "arxiv": "1907.09006",
            "year": 2019,
            "url": "http://arxiv.org/abs/1907.09006v1",
            "abstract": "Neural end-to-end TTS can generate very high-quality synthesized speech, and\neven close to human recording within similar domain text. However, it performs\nunsatisfactory when scaling it to challenging test sets. One concern is that\nthe encoder-decoder with attention-based network adopts autoregressive\ngenerative sequence model with the limitation of \"exposure bias\" To address\nthis issue, we propose two novel methods, which learn to predict future by\nimproving agreement between forward and backward decoding sequence. The first\none is achieved by introducing divergence regularization terms into model\ntraining objective to reduce the mismatch between two directional models,\nnamely L2R and R2L (which generates targets from left-to-right and\nright-to-left, respectively). While the second one operates on decoder-level\nand exploits the future information during decoding. In addition, we employ a\njoint training strategy to allow forward and backward decoding to improve each\nother in an interactive process. Experimental results show our proposed methods\nespecially the second one (bidirectional decoder regularization), leads a\nsignificantly improvement on both robustness and overall naturalness, as\noutperforming baseline (the revised version of Tacotron2) with a MOS gap of\n0.14 in a challenging test, and achieving close to human quality (4.42 vs. 4.49\nin MOS) on general test.",
            "authors": [
                "Yibin Zheng",
                "Xi Wang",
                "Lei He",
                "Shifeng Pan",
                "Frank K. Soong",
                "Zhengqi Wen",
                "Jianhua Tao"
            ]
        },
        {
            "title": "One TTS Alignment To Rule Them All",
            "arxiv": "2108.10447",
            "year": 2021,
            "url": "http://arxiv.org/abs/2108.10447v1",
            "abstract": "Speech-to-text alignment is a critical component of neural textto-speech\n(TTS) models. Autoregressive TTS models typically use an attention mechanism to\nlearn these alignments on-line. However, these alignments tend to be brittle\nand often fail to generalize to long utterances and out-of-domain text, leading\nto missing or repeating words. Most non-autoregressive endto-end TTS models\nrely on durations extracted from external sources. In this paper we leverage\nthe alignment mechanism proposed in RAD-TTS as a generic alignment learning\nframework, easily applicable to a variety of neural TTS models. The framework\ncombines forward-sum algorithm, the Viterbi algorithm, and a simple and\nefficient static prior. In our experiments, the alignment learning framework\nimproves all tested TTS architectures, both autoregressive (Flowtron, Tacotron\n2) and non-autoregressive (FastPitch, FastSpeech 2, RAD-TTS). Specifically, it\nimproves alignment convergence speed of existing attention-based mechanisms,\nsimplifies the training pipeline, and makes the models more robust to errors on\nlong utterances. Most importantly, the framework improves the perceived speech\nsynthesis quality, as judged by human evaluators.",
            "authors": [
                "Rohan Badlani",
                "Adrian \u0141ancucki",
                "Kevin J. Shih",
                "Rafael Valle",
                "Wei Ping",
                "Bryan Catanzaro"
            ]
        },
        {
            "title": "High Fidelity Speech Synthesis with Adversarial Networks",
            "arxiv": "1909.11646",
            "year": 2019,
            "url": "http://arxiv.org/abs/1909.11646v2",
            "abstract": "Generative adversarial networks have seen rapid development in recent years\nand have led to remarkable improvements in generative modelling of images.\nHowever, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in\ngenerative modelling of audio signals such as human speech. To address this\npaucity, we introduce GAN-TTS, a Generative Adversarial Network for\nText-to-Speech. Our architecture is composed of a conditional feed-forward\ngenerator producing raw speech audio, and an ensemble of discriminators which\noperate on random windows of different sizes. The discriminators analyse the\naudio both in terms of general realism, as well as how well the audio\ncorresponds to the utterance that should be pronounced. To measure the\nperformance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean\nOpinion Score), as well as novel quantitative metrics (Fr\\'echet DeepSpeech\nDistance and Kernel DeepSpeech Distance), which we find to be well correlated\nwith MOS. We show that GAN-TTS is capable of generating high-fidelity speech\nwith naturalness comparable to the state-of-the-art models, and unlike\nautoregressive models, it is highly parallelisable thanks to an efficient\nfeed-forward generator. Listen to GAN-TTS reading this abstract at\nhttps://storage.googleapis.com/deepmind-media/research/abstract.wav.",
            "authors": [
                "Miko\u0142aj Bi\u0144kowski",
                "Jeff Donahue",
                "Sander Dieleman",
                "Aidan Clark",
                "Erich Elsen",
                "Norman Casagrande",
                "Luis C. Cobo",
                "Karen Simonyan"
            ]
        }
    ],
    "domain": {
        "domain_type": "Speech",
        "domain_prob": 0.9869810973015956
    },
    "training": {
        "datasets": [
            {
                "name": "Music domain"
            }
        ]
    }
}