{
    "visibility": {
        "visibility": "public"
    },
    "name": "finnlem",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "jmyrberg",
                "owner_type": "User",
                "name": "finnlem",
                "url": "https://github.com/jmyrberg/finnlem",
                "stars": 9,
                "pushed_at": "2020-09-10 12:31:30+00:00",
                "created_at": "2017-07-27 21:02:12+00:00",
                "language": "Python",
                "description": "Neural network based lemmatizer for Finnish language",
                "frameworks": [
                    "Keras",
                    "NLTK",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "5c7c495832487797645153e85c623d9d4daee37a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jmyrberg/finnlem/blob/master/.gitignore"
                    }
                },
                "size": 1197
            },
            {
                "type": "code",
                "name": "doc",
                "sha": "1c570ca49cc3e28546f700e7082ae4eeda8ff437",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jmyrberg/finnlem/tree/master/doc"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "89d4b218139a2797d669d94f56e1bbcb55165283",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jmyrberg/finnlem/blob/master/requirements.txt"
                    }
                },
                "size": 55
            },
            {
                "type": "code",
                "name": "src",
                "sha": "ad431e9b973ada638b05297765c27714247deb3c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jmyrberg/finnlem/tree/master/src"
                    }
                },
                "num_files": 9
            }
        ]
    },
    "authors": [
        {
            "name": "Jesse Myrberg",
            "email": "jesse.myrberg@gmail.com",
            "github_id": "jmyrberg"
        }
    ],
    "tags": [
        "neural-network",
        "seq2seq",
        "finnish",
        "natural-language-processing",
        "nlp",
        "news",
        "tensorflow",
        "lemmatization"
    ],
    "description": "Neural network based lemmatizer for Finnish language",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/jmyrberg/finnlem",
            "stars": 9,
            "issues": true,
            "readme": "# finnlem\n\n**finnlem** is a [neural network](https://en.wikipedia.org/wiki/Artificial_neural_network) based [lemmatizer](https://en.wikipedia.org/wiki/Lemmatisation) model for [Finnish language](https://en.wikipedia.org/wiki/Finnish_language).\n\nA trained neural network can map given Finnish words into their base form with quite reasonable accuracy. These are examples of the model output:\n```\n[ORIGINAL] --> [BASE FORM]\nKiinalaisessa --> kiinalainen\nosinkotulojen --> osinko#tulo\t\nRajoittavalla --> rajoittaa\nmultimediaopetusmateriaalia -->\tmulti#media#opetus#materiaali\nei-rasistisella\t--> ei-rasistinen\n```\nThe model is a [tensorflow](https://www.tensorflow.org) implementation of a [sequence-to-sequence](https://arxiv.org/abs/1406.1078) (Seq2Seq) recurrent neural network model. \nThis repository contains the code and data needed for training and making predictions with the model. The [datasets](src/data/datasets) contain over 2M samples in total.\n\n## Features\n![Tensorboard](doc/tensorboard.JPG)\n* Easy-to-use Python wrapper for sequence-to-sequence modeling\n* Automatical session handling, model checkpointing and logging\n* Support for tensorboard\n* Sequence-to-sequence model features: [Bahdanau](https://arxiv.org/abs/1409.0473) and [Luong](https://arxiv.org/abs/1508.04025) attention, residual connections, dropout, beamsearch decoding, ...\n\n## Installation\nYou should have the latest versions for (as of 7/2017):\n* keras\n* nltk\n* numpy\n* pandas\n* tensorflow (1.3.0 or greater, with CUDA 8.0 and cuDNN 6.0 or greater)\n* unidecode\n* sacremoses ([see issue regarding this](https://github.com/jmyrberg/finnlem/issues/1))\n\nAfter this, clone this repository to your local machine.\n\nUpdate 10.9.2020: You could also try to first clone and then run `pip install -r requirements.txt` at the root of this repository. This will install the latest versions of the required packages automatically, but notice that the very latest versions of some of the packages might nowadays be incompatible with the source code provided here. Feel free to make a pull request with fixed versions of the packages, in case you manage to run the source code successfully :)\n\n## Example usage\n\nThree-steps are required in order to get from zero to making predictions with a trained model:\n\n1. **Dictionary training**: Dictionary is created from training documents, which are processed the same way as the Seq2Seq model inputs later on.\n\tDictionary handles vocabulary/integer mappings required by Seq2Seq.\n2. **Model training**: Seq2Seq model is trained in batches with training documents that contain source and target.\n3. **Model decoding**: Unseen source documents are fed into Seq2Seq model, which makes predictions on the target.\n\n### Python ([See list of relevant Python API classes](doc/python_api.md))\n\nThe following is a simple example of using some of the features in the Python API.\nSee more detailed descriptions of functions and parameters available from the source code documentation.\n\n#### 1. Dictionary training - fit a dictionary with default parameters\n```python\nfrom dictionary import Dictionary\n\n# Documents to fit in dictionary\ndocs = ['abcdefghijklmnopqrstuvwxyz','\u00e5\u00e4\u00f6','@?*#-']\n\n# Create a new Dictionary object\nd = Dictionary()\n\n# Fit characters of each document\nd.fit(docs)\n\n# Save for later usage\nd.save('./data/dictionaries/lemmatizer.dict')\n```\n\n#### 2. Model training - create and train a Seq2Seq model with default parameters\n```python\nfrom model_wrappers import Seq2Seq\n\n# Create a new model\nmodel = Seq2Seq(model_dir='./data/models/lemmatizer,\n\t\t\t\tdict_path='./data/dictionaries/lemmatizer.dict')\n\n# Create some documents to train on\nsource_docs = ['koira','koiran','koiraa','koirana','koiraksi','koirassa']*128\ntarget_docs = ['koira','koira','koira','koira','koira','koira']*128\n\n# Train 100 batches, save checkpoint every 25th batch\nfor i in range(100):\n\tloss,global_step = model.train(source_docs, target_docs, save_every_n_batch=25)\n\tprint('Global step %d loss: %f' % (global_step,loss))\n```\n\t\t\n#### 3. Model decoding - make predictions on test data\n```python\ntest_docs = ['koiraa','koirana','koiraksi']\npred_docs = model.decode(test_docs)\nprint(pred_docs) # --> [['koira'],['koira'],['koira']]\n```\n\n\n### Command line ([See list of available commands here](doc/commands.md))\n\nThe following demonstrates the usage of command line for training and predicting from files.\n\n#### 1. Dictionary training - fit a dictionary with default parameters\n```\npython -m dict_train\n\t\t--dict-save-path ./data/dictionaries/lemmatizer.dict\n\t\t--dict-train-path ./data/dictionaries/lemmatizer.vocab\n```\nThe dictionary train path file(s) should contain one document per line ([example](src/data/dictionaries/lemmatizer.vocab)).\n\n#### 2. Model training - create and train a Seq2Seq model with default parameters\n```\npython -m model_train\n\t\t--model-dir ./data/models/lemmatizer\n\t\t--dict-path ./data/dictionaries/lemmatizer.dict\n\t\t--train-data-path ./data/datasets/lemmatizer_train.csv\n```\nThe model train and validation data path file(s) should contain one source and target document per line, \nseparated by a comma ([example](src/data/datasets/lemmatizer_validation.csv)).\n\t\t\n#### 3. Model decoding - make predictions on test data\n```\npython -m model_decode\n\t\t--model-dir ./data/models/lemmatizer\n\t\t--test-data-path ./data/datasets/lemmatizer_test.csv\n\t\t--decoded-data-path ./data/decoded/lemmatizer_decoded.csv\n```\nThe model test data path file(s) should contain either:\n* one source document per line, or\n* one source and target document per line, separated by a comma ([example](src/data/datasets/lemmatizer_test.csv))\n\n## Extensions\n* To use tensorboard, run command ```python -m tensorflow.tensorboard --logdir=model_dir```, \nwhere ```model_dir``` is the Seq2Seq model checkpoint folder.\n* The model was originally created for summarizing the Finnish news, by using news contents as the sources, and news titles as the targets.\nThis proved to be quite a difficult task due to rich morphology of Finnish language, and lack of computational resources. My first\napproach for tackling the morphology was to use the base forms for each word, which is what the model in this package does by default. However, \nusing this model to convert every word to their base form ended up being too slow to be used as an input for the second model in real time.\n\n\tIn the end, I decided to try the [Finnish SnowballStemmer from nltk](http://www.nltk.org/_modules/nltk/stem/snowball.html) in order to get the \"base words\", \n\tand started training the model with 100k vocabulary. After 36 hours of training with loss decreasing very slowly, I decided to stop, and keep this package as a character-level lemmatizer.\n\tHowever, in [model_wrappers.py](src/model_wrappers.py), there is a global variable *DOC_HANDLER_FUNC*, which enables one to change the preprocessing method easily from\n\tcharacters to words by setting ```DOC_HANDLER_FUNC='WORD'```. Try changing the variable, and/or write your own preprocessing function *doc_to_tokens*, if you'd like to \n\texperiment with the word-level model.\n\n\n## Acknowledgements and references\n* [JayParks/tf-seq2seq](https://github.com/JayParks/tf-seq2seq): Example sequence-to-sequence implementation in tensorflow\n* [Omorfi](https://github.com/flammie/omorfi): Finnish open source morphology tool\n* [FinnTreeBank](http://www.ling.helsinki.fi/kieliteknologia/tutkimus/treebank/): Source for datasets\n* [Finnish Dependency Parser](http://bionlp.utu.fi/finnish-parser.html): Source for datasets\n\t\t\n---\nJesse Myrberg (jesse.myrberg@gmail.com)",
            "readme_url": "https://github.com/jmyrberg/finnlem",
            "frameworks": [
                "Keras",
                "NLTK",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
            "arxiv": "1409.0473",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.0473v7",
            "abstract": "Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition.",
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio"
            ]
        },
        {
            "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
            "arxiv": "1406.1078",
            "year": 2014,
            "url": "http://arxiv.org/abs/1406.1078v3",
            "abstract": "In this paper, we propose a novel neural network model called RNN\nEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNN\nencodes a sequence of symbols into a fixed-length vector representation, and\nthe other decodes the representation into another sequence of symbols. The\nencoder and decoder of the proposed model are jointly trained to maximize the\nconditional probability of a target sequence given a source sequence. The\nperformance of a statistical machine translation system is empirically found to\nimprove by using the conditional probabilities of phrase pairs computed by the\nRNN Encoder-Decoder as an additional feature in the existing log-linear model.\nQualitatively, we show that the proposed model learns a semantically and\nsyntactically meaningful representation of linguistic phrases.",
            "authors": [
                "Kyunghyun Cho",
                "Bart van Merrienboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ]
        },
        {
            "title": "Effective Approaches to Attention-based Neural Machine Translation",
            "arxiv": "1508.04025",
            "year": 2015,
            "url": "http://arxiv.org/abs/1508.04025v5",
            "abstract": "An attentional mechanism has lately been used to improve neural machine\ntranslation (NMT) by selectively focusing on parts of the source sentence\nduring translation. However, there has been little work exploring useful\narchitectures for attention-based NMT. This paper examines two simple and\neffective classes of attentional mechanism: a global approach which always\nattends to all source words and a local one that only looks at a subset of\nsource words at a time. We demonstrate the effectiveness of both approaches\nover the WMT translation tasks between English and German in both directions.\nWith local attention, we achieve a significant gain of 5.0 BLEU points over\nnon-attentional systems which already incorporate known techniques such as\ndropout. Our ensemble model using different attention architectures has\nestablished a new state-of-the-art result in the WMT'15 English to German\ntranslation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over\nthe existing best system backed by NMT and an n-gram reranker.",
            "authors": [
                "Minh-Thang Luong",
                "Hieu Pham",
                "Christopher D. Manning"
            ]
        },
        {
            "title": "JayParks/tf-seq2seq",
            "url": "https://github.com/JayParks/tf-seq2seq"
        },
        {
            "title": "Omorfi",
            "url": "https://github.com/flammie/omorfi"
        },
        {
            "title": "FinnTreeBank",
            "url": "http://www.ling.helsinki.fi/kieliteknologia/tutkimus/treebank/"
        },
        {
            "title": "Finnish Dependency Parser",
            "url": "http://bionlp.utu.fi/finnish-parser.html"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "datasets",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/jmyrberg/finnlem/blob/master/src/data/datasets"
                    }
                }
            },
            {
                "name": "Wikipedia"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999989956237698,
        "task": "Machine Translation",
        "task_prob": 0.9877047052259907
    }
}