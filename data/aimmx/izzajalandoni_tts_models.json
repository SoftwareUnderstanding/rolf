{
    "visibility": {
        "visibility": "public",
        "license": "BSD 3-Clause \"New\" or \"Revised\" License"
    },
    "name": "tts_models",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "izzajalandoni",
                "owner_type": "User",
                "name": "tts_models",
                "url": "https://github.com/izzajalandoni/tts_models",
                "stars": 5,
                "pushed_at": "2019-12-17 03:57:53+00:00",
                "created_at": "2019-09-20 04:32:36+00:00",
                "language": "Jupyter Notebook",
                "description": "A compilation of Text-to-Speech Synthesis projects",
                "license": "BSD 3-Clause \"New\" or \"Revised\" License",
                "frameworks": [
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "2a718d63da7fd063f44577f27b87be38cc6f6761",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/LICENSE"
                    }
                },
                "size": 1518
            },
            {
                "type": "code",
                "name": "__pycache__",
                "sha": "3d44bdbe60d380ae91295a34453fd3a85882cac5",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/tree/master/__pycache__"
                    }
                },
                "num_files": 12
            },
            {
                "type": "code",
                "name": "audio_processing.py",
                "sha": "b5af7f723eb8047bc58db2f85234aea161fbc659",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/audio_processing.py"
                    }
                },
                "size": 2610
            },
            {
                "type": "code",
                "name": "config.json",
                "sha": "a28c8722cafe65cdb04a1d0771a08c3f31c09659",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/config.json"
                    }
                },
                "size": 957
            },
            {
                "type": "code",
                "name": "data_utils.py",
                "sha": "fdfd287b37672126803ac499f89f33180dec71ce",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/data_utils.py"
                    }
                },
                "size": 4469
            },
            {
                "type": "code",
                "name": "distributed.py",
                "sha": "cce74944bfec6a69ac50921ec82a74d4874d6070",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/distributed.py"
                    }
                },
                "size": 7234
            },
            {
                "type": "code",
                "name": "filelists",
                "sha": "13f202d516642cfee8e0df8c63a6ad383e7c3b7e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/tree/master/filelists"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "hparams.py",
                "sha": "8e4699fdb2ca58b76d9f4002b3fac69c0ee9ad8b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/hparams.py"
                    }
                },
                "size": 2912
            },
            {
                "type": "code",
                "name": "inference_isip_edit.ipynb",
                "sha": "c66d8418c175d48c14f7bcadcf260970eff621cf",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/inference_isip_edit.ipynb"
                    }
                },
                "size": 210176
            },
            {
                "type": "code",
                "name": "isiparams.py",
                "sha": "e774102065c8f17c097082d62c4c566ae281db41",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/isiparams.py"
                    }
                },
                "size": 2923
            },
            {
                "type": "code",
                "name": "isiparams_0.py",
                "sha": "18537af8b475ba7ea00c78fe8eb71a9586d843fc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/isiparams_0.py"
                    }
                },
                "size": 2914
            },
            {
                "type": "code",
                "name": "layers.py",
                "sha": "615a64a4394cb7482cc19bd0bd26f842d38d9b3d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/layers.py"
                    }
                },
                "size": 2984
            },
            {
                "type": "code",
                "name": "logger.py",
                "sha": "9b999adf6b05743181cafad7488ddffe0399b41d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/logger.py"
                    }
                },
                "size": 1960
            },
            {
                "type": "code",
                "name": "loss_function.py",
                "sha": "99cae952bbc38cb5d89a5686b07a8dd458725e22",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/loss_function.py"
                    }
                },
                "size": 673
            },
            {
                "type": "code",
                "name": "loss_scaler.py",
                "sha": "0662a60e2a719e4405748c68eb516f3bd3622348",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/loss_scaler.py"
                    }
                },
                "size": 4400
            },
            {
                "type": "code",
                "name": "model.py",
                "sha": "4c7d7d267287ab5aded5e3de5563248120f635d4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/model.py"
                    }
                },
                "size": 20645
            },
            {
                "type": "code",
                "name": "multiproc.py",
                "sha": "060ff937ace6c4170f12189e442c65f5093e0ecf",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/multiproc.py"
                    }
                },
                "size": 647
            },
            {
                "type": "code",
                "name": "plotting_utils.py",
                "sha": "ca7e16880ea01ca3a03b8842a5d885e0285ed489",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/plotting_utils.py"
                    }
                },
                "size": 1757
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "11eccea4442fde368e827262bff5e075fd2ce54e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/requirements.txt"
                    }
                },
                "size": 129
            },
            {
                "type": "code",
                "name": "stft.py",
                "sha": "edfc44ae8bdec2887920a1ffab012432ca09a33d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/stft.py"
                    }
                },
                "size": 5892
            },
            {
                "type": "code",
                "name": "tensorboard.png",
                "sha": "59223cf822ae6bf76c1b7ce2b1732059a68a10cc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/tensorboard.png"
                    }
                },
                "size": 174241
            },
            {
                "type": "code",
                "name": "text",
                "sha": "404501591ef974e08196a7e8204adbdc226fb766",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/tree/master/text"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "ae6b7fbd0ece7905906d2fc857b8043e2cff92fd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/train.py"
                    }
                },
                "size": 11248
            },
            {
                "type": "code",
                "name": "train_0.py",
                "sha": "e93917bbc1ac1371063150667c26a28c3a1c1cf7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/train_0.py"
                    }
                },
                "size": 11208
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "6aa710effe71c8a601f80f401cb262a289652446",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/utils.py"
                    }
                },
                "size": 781
            },
            {
                "type": "code",
                "name": "waveglow",
                "sha": "3d3d0e14269503be32ca098bc1ab4adfd2fc05a9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/tree/master/waveglow"
                    }
                },
                "num_files": 16
            }
        ]
    },
    "trained_model": {
        "binaries": [
            {
                "type": "binary",
                "name": "Dockerfile",
                "sha": "adf4658ac10deef30571ca6728d49da301b4cba1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/izzajalandoni/tts_models/blob/master/Dockerfile"
                    }
                },
                "size": 378
            }
        ]
    },
    "authors": [
        {
            "name": "izzajalandoni",
            "github_id": "izzajalandoni"
        }
    ],
    "tags": [],
    "description": "A compilation of Text-to-Speech Synthesis projects",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/izzajalandoni/tts_models",
            "stars": 5,
            "issues": true,
            "readme": "# tts_models\nA compilation of Text-to-Speech Synthesis projects\n\n## Famous Works\n### Single-Speaker TTS\n1. NVIDIA's Tacotron 2<br>\n    [Paper] https://arxiv.org/pdf/1712.05884.pdf<br>\n    [Code] https://github.com/NVIDIA/tacotron2<br>\n\n2. NVIDIA's OpenSeq2Seq <br>\n    [Paper] https://nvidia.github.io/OpenSeq2Seq/<br>\n    [Code] https://github.com/NVIDIA/OpenSeq2Seq<br>\n\n3. Deep Convolutional TTS <br>\n    [Paper] https://arxiv.org/pdf/1710.08969.pdf<br>\n    [Code] https://github.com/Kyubyong/dc_tts<br>\n*Implemented by a third-party and not by the writers themselves<br>\n4. Google's Tacotron <br>\n    [Paper] https://arxiv.org/pdf/1703.10135.pdf<br>\n    [Code] https://github.com/keithito/tacotron<br>\n    [Code] https://github.com/MycroftAI/mimic2<br>\n*Tensorflow implementation of Tacotron, not by the writers themselves<br>\n5. Mozilla Text-to-Speech<br>\n    [Code] https://github.com/mozilla/TTS<br>\n6. Stanford's GloVe<br>\n    [Documentation] https://nlp.stanford.edu/projects/glove/<br>\n    [Code] https://github.com/stanfordnlp/GloVe<br>\n    \n7. DeepMind's GAN-TTS\n    [Documentation] https://arxiv.org/pdf/1909.11646.pdf<br>\n    [Code] https://github.com/yanggeng1995/GAN-TTS<br>\n### Other Directories\n1. https://github.com/topics/text-to-speech<br>\n2. https://github.com/topics/google-text-to-speech<br>\n### Multi-Speaker TTS\n1. Multi-Speaker Tacotron in TensorFlow<br>\n    [Code] https://github.com/carpedm20/multi-speaker-tacotron-tensorflow<br>\n2. DeepVoice Series<br>\n    [DeepVoice 2] https://github.com/jdbermeol/deep_voice_2<br>\n    [DeepVoice 3] https://github.com/r9y9/deepvoice3_pytorch<br>\n** Most MS-TTS are unofficial code implementations\n\n# Tagalog Text-to-Speech Synthesis\nUses any or a combination of existing works, but applied in the Tagalog language. For this project, using NVIDIA's [tacotron2](https://github.com/NVIDIA/tacotron2) and [waveglow](https://github.com/NVIDIA/waveglow) provided the best results despite the networks being optimized for single-speaker data and our tagalog dataset being multi-speaker. This might be because, given that tacotron2 trains on per-character level, it properly learns the voice-independent features such as prosody. Hence, the network was able to capture this information but fails in modeling the voice.\n\nTraining was done similar to NVIDIA and Ryuichi Yamamoto's deepvoice3. Data was edited and organised to match the expected inputs of the networks, and config files were changed to match the tagalog dataset.\n\nTraining tacotron2: `python train.py --output_directory \\[output dir] --log_directory \\[log dir] -c \\[optional, checkpoint file]`<br>\nTraining waveglow (in waveglow folder): `python train.py -c config.json`<br>\nTraining deepvoice3 (in deepvoice3 folder): `python train.py --data-root=\\[data file] --preset=\\[preset file] --checkpoint=\\[optional, checkpoint file]`<br>\n\nCheckpoints can be found here: [checkpoints](https://drive.google.com/drive/folders/1CuV7v9up5PcHuPzFsOsvx9_KQ2q2O-ky?usp=sharing)<br>\n\n#### Voice Conversion Option\nAdding in Kobayashi's Sprocket was supposedly a test if whether implementing a voice conversion <i>after</i> the network would mitigate the grittiness of the output. As expected, results showed no improvements to poor performance, especially when tested with longer sentences.\n\nTraining was done by, first, generating the source voice using network and the target taken from the data. Both source and target must speak the same words. Moreover, all target data must come from a single speaker. This can be done manually. Or you can download some of our used data [here](https://drive.google.com/drive/folders/1CuV7v9up5PcHuPzFsOsvx9_KQ2q2O-ky?usp=sharing), and paste it inside `/sprocket/example/data/`\n\nFor training and/ generation, please follow the steps [here](https://github.com/k2kobayashi/sprocket/blob/master/docs/vc_example.md)\n",
            "readme_url": "https://github.com/izzajalandoni/tts_models",
            "frameworks": [
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention",
            "arxiv": "1710.08969",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.08969v2",
            "abstract": "This paper describes a novel text-to-speech (TTS) technique based on deep\nconvolutional neural networks (CNN), without use of any recurrent units.\nRecurrent neural networks (RNN) have become a standard technique to model\nsequential data recently, and this technique has been used in some cutting-edge\nneural TTS techniques. However, training RNN components often requires a very\npowerful computer, or a very long time, typically several days or weeks. Recent\nother studies, on the other hand, have shown that CNN-based sequence synthesis\ncan be much faster than RNN-based techniques, because of high\nparallelizability. The objective of this paper is to show that an alternative\nneural TTS based only on CNN alleviate these economic costs of training. In our\nexperiment, the proposed Deep Convolutional TTS was sufficiently trained\novernight (15 hours), using an ordinary gaming PC equipped with two GPUs, while\nthe quality of the synthesized speech was almost acceptable.",
            "authors": [
                "Hideyuki Tachibana",
                "Katsuya Uenoyama",
                "Shunsuke Aihara"
            ]
        },
        {
            "title": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions",
            "arxiv": "1712.05884",
            "year": 2017,
            "url": "http://arxiv.org/abs/1712.05884v2",
            "abstract": "This paper describes Tacotron 2, a neural network architecture for speech\nsynthesis directly from text. The system is composed of a recurrent\nsequence-to-sequence feature prediction network that maps character embeddings\nto mel-scale spectrograms, followed by a modified WaveNet model acting as a\nvocoder to synthesize timedomain waveforms from those spectrograms. Our model\nachieves a mean opinion score (MOS) of $4.53$ comparable to a MOS of $4.58$ for\nprofessionally recorded speech. To validate our design choices, we present\nablation studies of key components of our system and evaluate the impact of\nusing mel spectrograms as the input to WaveNet instead of linguistic, duration,\nand $F_0$ features. We further demonstrate that using a compact acoustic\nintermediate representation enables significant simplification of the WaveNet\narchitecture.",
            "authors": [
                "Jonathan Shen",
                "Ruoming Pang",
                "Ron J. Weiss",
                "Mike Schuster",
                "Navdeep Jaitly",
                "Zongheng Yang",
                "Zhifeng Chen",
                "Yu Zhang",
                "Yuxuan Wang",
                "RJ Skerry-Ryan",
                "Rif A. Saurous",
                "Yannis Agiomyrgiannakis",
                "Yonghui Wu"
            ]
        },
        {
            "title": "High Fidelity Speech Synthesis with Adversarial Networks",
            "arxiv": "1909.11646",
            "year": 2019,
            "url": "http://arxiv.org/abs/1909.11646v2",
            "abstract": "Generative adversarial networks have seen rapid development in recent years\nand have led to remarkable improvements in generative modelling of images.\nHowever, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in\ngenerative modelling of audio signals such as human speech. To address this\npaucity, we introduce GAN-TTS, a Generative Adversarial Network for\nText-to-Speech. Our architecture is composed of a conditional feed-forward\ngenerator producing raw speech audio, and an ensemble of discriminators which\noperate on random windows of different sizes. The discriminators analyse the\naudio both in terms of general realism, as well as how well the audio\ncorresponds to the utterance that should be pronounced. To measure the\nperformance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean\nOpinion Score), as well as novel quantitative metrics (Fr\\'echet DeepSpeech\nDistance and Kernel DeepSpeech Distance), which we find to be well correlated\nwith MOS. We show that GAN-TTS is capable of generating high-fidelity speech\nwith naturalness comparable to the state-of-the-art models, and unlike\nautoregressive models, it is highly parallelisable thanks to an efficient\nfeed-forward generator. Listen to GAN-TTS reading this abstract at\nhttps://storage.googleapis.com/deepmind-media/research/abstract.wav.",
            "authors": [
                "Miko\u0142aj Bi\u0144kowski",
                "Jeff Donahue",
                "Sander Dieleman",
                "Aidan Clark",
                "Erich Elsen",
                "Norman Casagrande",
                "Luis C. Cobo",
                "Karen Simonyan"
            ]
        },
        {
            "title": "Tacotron: Towards End-to-End Speech Synthesis",
            "arxiv": "1703.10135",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.10135v2",
            "abstract": "A text-to-speech synthesis system typically consists of multiple stages, such\nas a text analysis frontend, an acoustic model and an audio synthesis module.\nBuilding these components often requires extensive domain expertise and may\ncontain brittle design choices. In this paper, we present Tacotron, an\nend-to-end generative text-to-speech model that synthesizes speech directly\nfrom characters. Given <text, audio> pairs, the model can be trained completely\nfrom scratch with random initialization. We present several key techniques to\nmake the sequence-to-sequence framework perform well for this challenging task.\nTacotron achieves a 3.82 subjective 5-scale mean opinion score on US English,\noutperforming a production parametric system in terms of naturalness. In\naddition, since Tacotron generates speech at the frame level, it's\nsubstantially faster than sample-level autoregressive methods.",
            "authors": [
                "Yuxuan Wang",
                "RJ Skerry-Ryan",
                "Daisy Stanton",
                "Yonghui Wu",
                "Ron J. Weiss",
                "Navdeep Jaitly",
                "Zongheng Yang",
                "Ying Xiao",
                "Zhifeng Chen",
                "Samy Bengio",
                "Quoc Le",
                "Yannis Agiomyrgiannakis",
                "Rob Clark",
                "Rif A. Saurous"
            ]
        }
    ],
    "domain": {
        "domain_type": "Speech",
        "domain_prob": 0.9859645850332918
    }
}