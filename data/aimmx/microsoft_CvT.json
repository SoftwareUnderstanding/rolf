{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Introduction",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "microsoft",
                "owner_type": "Organization",
                "name": "CvT",
                "url": "https://github.com/microsoft/CvT",
                "stars": 253,
                "pushed_at": "2021-12-07 18:54:55+00:00",
                "created_at": "2021-05-25 19:26:14+00:00",
                "language": "Python",
                "description": "This is an official implementation of CvT: Introducing Convolutions to Vision Transformers.",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "dfcfd56f444f9ae40e1082c07fe254cc547136cf",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/microsoft/CvT/blob/main/.gitignore"
                    }
                },
                "size": 6002
            },
            {
                "type": "code",
                "name": "CODE_OF_CONDUCT.md",
                "sha": "f9ba8cf65f3e3104dd061c178066ec8247811f33",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/microsoft/CvT/blob/main/CODE_OF_CONDUCT.md"
                    }
                },
                "size": 444
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "9e841e7a26e4eb057b24511e7b92d42b257a80e5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/microsoft/CvT/blob/main/LICENSE"
                    }
                },
                "size": 1141
            },
            {
                "type": "code",
                "name": "SECURITY.md",
                "sha": "f7b89984f0fb5dd204028bc525e19eefc0859f4f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/microsoft/CvT/blob/main/SECURITY.md"
                    }
                },
                "size": 2780
            },
            {
                "type": "code",
                "name": "experiments",
                "sha": "5594af22a1421c389097e7855eb5594fb43145c7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/microsoft/CvT/tree/main/experiments"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "figures",
                "sha": "4f9ef06313426b53de9ab2888b456971e995d9a9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/microsoft/CvT/tree/main/figures"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "lib",
                "sha": "264647d2a24dc7cdc9b35b9c4f8281ee5bbe1111",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/microsoft/CvT/tree/main/lib"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "d1c92b51b726d39ce02e7cb456290b31d900f9cc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/microsoft/CvT/blob/main/requirements.txt"
                    }
                },
                "size": 124
            },
            {
                "type": "code",
                "name": "run.sh",
                "sha": "216b5af39245bcee09efabec1cc3d24252fafb05",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/microsoft/CvT/blob/main/run.sh"
                    }
                },
                "size": 1686
            },
            {
                "type": "code",
                "name": "tools",
                "sha": "d2c7ad317923a4aa7e7e19a4e410c1d6158f84a8",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/microsoft/CvT/tree/main/tools"
                    }
                },
                "num_files": 3
            }
        ]
    },
    "authors": [
        {
            "name": "Microsoft Open Source",
            "github_id": "microsoftopensource"
        },
        {
            "name": "Mike",
            "github_id": "lmk123568"
        },
        {
            "name": "microsoft-github-operations[bot]",
            "github_id": "microsoft-github-operations[bot]"
        }
    ],
    "tags": [
        "cvt",
        "deep-learning",
        "classification",
        "imagenet",
        "computer-vision"
    ],
    "description": "This is an official implementation of CvT: Introducing Convolutions to Vision Transformers.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/microsoft/CvT",
            "stars": 253,
            "issues": true,
            "readme": "# Introduction\nThis is an official implementation of [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808). We present a new architecture, named Convolutional vision Transformers (CvT), that improves Vision Transformers (ViT) in performance and efficienty by introducing convolutions into ViT to yield the best of both disignes. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (e.g. shift, scale, and distortion invariance) while maintaining the merits of Transformers (e.g. dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger dataset (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely removed in our model, simplifying the design for higher resolution vision tasks. \n\n![](figures/pipeline.svg)\n\n# Main results\n## Models pre-trained on ImageNet-1k\n| Model  | Resolution | Param | GFLOPs | Top-1 |\n|--------|------------|-------|--------|-------|\n| CvT-13 | 224x224    | 20M   | 4.5    | 81.6  |\n| CvT-21 | 224x224    | 32M   | 7.1    | 82.5  |\n| CvT-13 | 384x384    | 20M   | 16.3   | 83.0  |\n| CvT-21 | 384x384    | 32M   | 24.9   | 83.3  |\n\n## Models pre-trained on ImageNet-22k\n| Model   | Resolution | Param | GFLOPs | Top-1 |\n|---------|------------|-------|--------|-------|\n| CvT-13  | 384x384    | 20M   | 16.3   | 83.3  |\n| CvT-21  | 384x384    | 32M   | 24.9   | 84.9  |\n| CvT-W24 | 384x384    | 277M  | 193.2  | 87.6  |\n\nYou can download all the models from our [model zoo](https://1drv.ms/u/s!AhIXJn_J-blW9RzF3rMW7SsLHa8h?e=blQ0Al).\n\n\n# Quick start\n## Installation\nAssuming that you have installed PyTroch and TorchVision, if not, please follow the [officiall instruction](https://pytorch.org/) to install them firstly. \nIntall the dependencies using cmd:\n\n``` sh\npython -m pip install -r requirements.txt --user -q\n```\n\nThe code is developed and tested using pytorch 1.7.1. Other versions of pytorch are not fully tested.\n\n## Data preparation\nPlease prepare the data as following:\n\n``` sh\n|-DATASET\n  |-imagenet\n    |-train\n    | |-class1\n    | | |-img1.jpg\n    | | |-img2.jpg\n    | | |-...\n    | |-class2\n    | | |-img3.jpg\n    | | |-...\n    | |-class3\n    | | |-img4.jpg\n    | | |-...\n    | |-...\n    |-val\n      |-class1\n      | |-img5.jpg\n      | |-...\n      |-class2\n      | |-img6.jpg\n      | |-...\n      |-class3\n      | |-img7.jpg\n      | |-...\n      |-...\n```\n\n\n## Run\nEach experiment is defined by a yaml config file, which is saved under the directory of `experiments`. The directory of `experiments` has a tree structure like this:\n\n``` sh\nexperiments\n|-{DATASET_A}\n| |-{ARCH_A}\n| |-{ARCH_B}\n|-{DATASET_B}\n| |-{ARCH_A}\n| |-{ARCH_B}\n|-{DATASET_C}\n| |-{ARCH_A}\n| |-{ARCH_B}\n|-...\n```\n\nWe provide a `run.sh` script for running jobs in local machine.\n\n``` sh\nUsage: run.sh [run_options]\nOptions:\n  -g|--gpus <1> - number of gpus to be used\n  -t|--job-type <aml> - job type (train|test)\n  -p|--port <9000> - master port\n  -i|--install-deps - If install dependencies (default: False)\n```\n\n### Training on local machine\n\n``` sh\nbash run.sh -g 8 -t train --cfg experiments/imagenet/cvt/cvt-13-224x224.yaml\n```\n\nYou can also modify the config paramters by the command line. For example, if you want to change the lr rate to 0.1, you can run the command:\n``` sh\nbash run.sh -g 8 -t train --cfg experiments/imagenet/cvt/cvt-13-224x224.yaml TRAIN.LR 0.1\n```\n\nNotes:\n- The checkpoint, model, and log files will be saved in OUTPUT/{dataset}/{training config} by default.\n\n### Testing pre-trained models\n\n``` sh\nbash run.sh -t test --cfg experiments/imagenet/cvt/cvt-13-224x224.yaml TEST.MODEL_FILE ${PRETRAINED_MODLE_FILE}\n```\n\n# Citation\nIf you find this work or code is helpful in your research, please cite:\n\n```\n@article{wu2021cvt,\n  title={Cvt: Introducing convolutions to vision transformers},\n  author={Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},\n  journal={arXiv preprint arXiv:2103.15808},\n  year={2021}\n}\n```\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n",
            "readme_url": "https://github.com/microsoft/CvT",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "CvT: Introducing Convolutions to Vision Transformers",
            "arxiv": "2103.15808",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.15808v1",
            "abstract": "We present in this paper a new architecture, named Convolutional vision\nTransformer (CvT), that improves Vision Transformer (ViT) in performance and\nefficiency by introducing convolutions into ViT to yield the best of both\ndesigns. This is accomplished through two primary modifications: a hierarchy of\nTransformers containing a new convolutional token embedding, and a\nconvolutional Transformer block leveraging a convolutional projection. These\nchanges introduce desirable properties of convolutional neural networks (CNNs)\nto the ViT architecture (\\ie shift, scale, and distortion invariance) while\nmaintaining the merits of Transformers (\\ie dynamic attention, global context,\nand better generalization). We validate CvT by conducting extensive\nexperiments, showing that this approach achieves state-of-the-art performance\nover other Vision Transformers and ResNets on ImageNet-1k, with fewer\nparameters and lower FLOPs. In addition, performance gains are maintained when\npretrained on larger datasets (\\eg ImageNet-22k) and fine-tuned to downstream\ntasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of\n87.7\\% on the ImageNet-1k val set. Finally, our results show that the\npositional encoding, a crucial component in existing Vision Transformers, can\nbe safely removed in our model, simplifying the design for higher resolution\nvision tasks. Code will be released at \\url{https://github.com/leoxiaobin/CvT}.",
            "authors": [
                "Haiping Wu",
                "Bin Xiao",
                "Noel Codella",
                "Mengchen Liu",
                "Xiyang Dai",
                "Lu Yuan",
                "Lei Zhang"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9303295311300687,
        "task": "Image Classification",
        "task_prob": 0.9283789475094013
    }
}