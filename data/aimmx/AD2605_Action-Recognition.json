{
    "visibility": {
        "visibility": "public"
    },
    "name": "Description",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "AD2605",
                "owner_type": "User",
                "name": "Action-Recognition",
                "url": "https://github.com/AD2605/Action-Recognition",
                "stars": 1,
                "pushed_at": "2020-11-24 13:43:28+00:00",
                "created_at": "2020-04-27 09:33:56+00:00",
                "language": "Python",
                "description": "An implementation of R2plus1D and 3DMobileNets in pytorch for Action Classification ",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".idea",
                "sha": "a672c71c99987db60ed286e31ed0443ecd708c64",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/AD2605/Action-Recognition/tree/master/.idea"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "Dataloaders.py",
                "sha": "0d8693354cde4b72ac7604876b8ca5026f49de6e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/AD2605/Action-Recognition/blob/master/Dataloaders.py"
                    }
                },
                "size": 3758
            },
            {
                "type": "code",
                "name": "MobilenetV2.py",
                "sha": "77bc1df3582dfe508416ac2057fa234595ed8a0d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/AD2605/Action-Recognition/blob/master/MobilenetV2.py"
                    }
                },
                "size": 5115
            },
            {
                "type": "code",
                "name": "Model.py",
                "sha": "f9301e3673b1105c3af6089c2f2ed13bd17d2bb3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/AD2605/Action-Recognition/blob/master/Model.py"
                    }
                },
                "size": 6531
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "a96edb60bd70857d286e1a0adfcc0b687b9f6296",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/AD2605/Action-Recognition/blob/master/main.py"
                    }
                },
                "size": 320
            }
        ]
    },
    "authors": [
        {
            "name": "Atharva Dubey",
            "email": "atharvadubey26@gmail.com",
            "github_id": "AD2605"
        }
    ],
    "tags": [
        "action-recognition",
        "r2plus1d",
        "pytorch"
    ],
    "description": "An implementation of R2plus1D and 3DMobileNets in pytorch for Action Classification ",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/AD2605/Action-Recognition",
            "stars": 1,
            "issues": true,
            "readme": "# Description\nThe aim of this project is to dive into the field of action recognition and explore various techniques.\nTill now 2 models have been implemented <br/>\n1 - The model.py  is a pytorch implementation of the paper - **A Closer Look at Spatiotemporal Convolutions for Action Recognition**\nLink to the paper is - **https://arxiv.org/abs/1711.11248v3**\n\n2 - A pytorch implementation of MobileNets for less computational Models. Consult the paper - **MobileNetV2: Inverted Residuals and Linear Bottlenecks  - https://arxiv.org/abs/1801.04381v4**\n\n# Dependencies \n1 - Opencv to load and resize videos and <br/>\n2 - For mixed precision training and fp16 conversions Apex library is used. For installation and more info see https://github.com/NVIDIA/apex.\n\n\n# To Do \n1 - Implement attention and compare the performance. <br/>\n2 - Visualize where the model focuses <br/>\n",
            "readme_url": "https://github.com/AD2605/Action-Recognition",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "A Closer Look at Spatiotemporal Convolutions for Action Recognition",
            "arxiv": "1711.11248",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.11248v3",
            "abstract": "In this paper we discuss several forms of spatiotemporal convolutions for\nvideo analysis and study their effects on action recognition. Our motivation\nstems from the observation that 2D CNNs applied to individual frames of the\nvideo have remained solid performers in action recognition. In this work we\nempirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within\nthe framework of residual learning. Furthermore, we show that factorizing the\n3D convolutional filters into separate spatial and temporal components yields\nsignificantly advantages in accuracy. Our empirical study leads to the design\nof a new spatiotemporal convolutional block \"R(2+1)D\" which gives rise to CNNs\nthat achieve results comparable or superior to the state-of-the-art on\nSports-1M, Kinetics, UCF101 and HMDB51.",
            "authors": [
                "Du Tran",
                "Heng Wang",
                "Lorenzo Torresani",
                "Jamie Ray",
                "Yann LeCun",
                "Manohar Paluri"
            ]
        },
        {
            "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
            "arxiv": "1801.04381",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.04381v4",
            "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that\nimproves the state of the art performance of mobile models on multiple tasks\nand benchmarks as well as across a spectrum of different model sizes. We also\ndescribe efficient ways of applying these mobile models to object detection in\na novel framework we call SSDLite. Additionally, we demonstrate how to build\nmobile semantic segmentation models through a reduced form of DeepLabv3 which\nwe call Mobile DeepLabv3.\n  The MobileNetV2 architecture is based on an inverted residual structure where\nthe input and output of the residual block are thin bottleneck layers opposite\nto traditional residual models which use expanded representations in the input\nan MobileNetV2 uses lightweight depthwise convolutions to filter features in\nthe intermediate expansion layer. Additionally, we find that it is important to\nremove non-linearities in the narrow layers in order to maintain\nrepresentational power. We demonstrate that this improves performance and\nprovide an intuition that led to this design. Finally, our approach allows\ndecoupling of the input/output domains from the expressiveness of the\ntransformation, which provides a convenient framework for further analysis. We\nmeasure our performance on Imagenet classification, COCO object detection, VOC\nimage segmentation. We evaluate the trade-offs between accuracy, and number of\noperations measured by multiply-adds (MAdd), as well as the number of\nparameters",
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9996357177097579,
        "task": "Image Classification",
        "task_prob": 0.4977091212055447
    },
    "training": {
        "datasets": [
            {
                "name": "UCF101"
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "COCO"
            },
            {
                "name": "HMDB51"
            }
        ]
    }
}