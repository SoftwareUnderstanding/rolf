{
    "visibility": {
        "visibility": "public",
        "license": "GNU General Public License v3.0"
    },
    "name": "archibrain",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "adityagilra",
                "owner_type": "User",
                "name": "archibrain",
                "url": "https://github.com/adityagilra/archibrain",
                "stars": 10,
                "pushed_at": "2021-04-14 14:47:28+00:00",
                "created_at": "2017-02-06 10:09:38+00:00",
                "language": "Python",
                "description": "Synthesize bio-plausible neural networks for cognitive tasks, mimicking brain architecture",
                "license": "GNU General Public License v3.0",
                "frameworks": [
                    "Keras",
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "72364f99fe4bf8d5262df3b19b33102aeaa791e5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adityagilra/archibrain/blob/master/.gitignore"
                    }
                },
                "size": 1045
            },
            {
                "type": "code",
                "name": "AuGMEnT",
                "sha": "2bb1a722eadf1fac2698b295a3d23a6e0cbc32ee",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adityagilra/archibrain/tree/master/AuGMEnT"
                    }
                },
                "num_files": 23
            },
            {
                "type": "code",
                "name": "COMPARATIVE_RESULTS",
                "sha": "92adc948687ca6d7585590e00e0ba1ffefed0443",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adityagilra/archibrain/tree/master/COMPARATIVE_RESULTS"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "DNC_ANALYSIS",
                "sha": "a01726dd6ec63b9d65c601cb905f90ae9599f47c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adityagilra/archibrain/tree/master/DNC_ANALYSIS"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "HER",
                "sha": "e372bcde2f617d0c7a55dd811afd5354ac3c5f5f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adityagilra/archibrain/tree/master/HER"
                    }
                },
                "num_files": 13
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "9cecc1d4669ee8af2ca727a5d8cde10cd8b2d7cc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adityagilra/archibrain/blob/master/LICENSE"
                    }
                },
                "size": 35141
            },
            {
                "type": "code",
                "name": "LSTM",
                "sha": "04291092c80ac923f368df438bd188ba0808cb1e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adityagilra/archibrain/tree/master/LSTM"
                    }
                },
                "num_files": 12
            },
            {
                "type": "code",
                "name": "MODEL_DIAGRAMS",
                "sha": "d208a5afb74acb8d24526e5849215d06b601e2f5",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adityagilra/archibrain/tree/master/MODEL_DIAGRAMS"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "Pure Machine Learning",
                "sha": "cbdc7fed43358bad90abd371f87531e1ac1c93d2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adityagilra/archibrain/tree/master/Pure Machine Learning"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "TASKS",
                "sha": "0c3daaeee988ab3cc1867e43e66447052591c8e7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adityagilra/archibrain/tree/master/TASKS"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "hybrid-AuGMEnT",
                "sha": "4329b5cf6ca153b9855ef1531b2f18a6384b89fd",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adityagilra/archibrain/tree/master/hybrid-AuGMEnT"
                    }
                },
                "num_files": 16
            },
            {
                "type": "code",
                "name": "interface.py",
                "sha": "075df6f624dbf7da8d090134aabbba1d082f0c72",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adityagilra/archibrain/blob/master/interface.py"
                    }
                },
                "size": 6188
            }
        ]
    },
    "authors": [
        {
            "name": "Aditya Gilra",
            "github_id": "adityagilra"
        },
        {
            "name": "Marco Martinolli",
            "github_id": "martin592"
        },
        {
            "name": "Vineet Jain",
            "email": "vinjain96@gmail.com",
            "github_id": "vineetjain96"
        },
        {
            "name": "PoliMartinolli",
            "github_id": "PoliMartinolli"
        }
    ],
    "tags": [
        "machine-learning",
        "computational-neuroscience",
        "cognitive-neural-processes",
        "reinforcement-learning"
    ],
    "description": "Synthesize bio-plausible neural networks for cognitive tasks, mimicking brain architecture",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/adityagilra/archibrain",
            "stars": 10,
            "issues": true,
            "readme": "# archibrain\nWe will develop biologically plausible neural network models based on brain architecture that solve cognitive tasks performed in the laboratory.  \n  \nInspired by brain architecture, the machine learning community has recently developed various memory-augmented neural networks, that enable symbol and data manipulation tasks that are difficult with standard neural network approaches, see especially from Google Deepmind ([NTM](https://arxiv.org/abs/1410.5401), [DNC](http://www.nature.com/nature/journal/v538/n7626/abs/nature20101.html), [one-shot learner](https://arxiv.org/abs/1605.06065)).  \n  \nAt the same time, models based closely on brain architecture, that perform experimentally-studied tasks, have existed for a while in the neuroscience community, notably from the labs of Eliasmith ([SPAWN](http://www.sciencemag.org/content/338/6111/1202)), O'Reilly ([PBWM](http://dx.doi.org/10.1162/089976606775093909)-[LEABRA-Emergent](http://www.colorado.edu/faculty/oreilly/research)), Alexander+Brown ([HER](http://dx.doi.org/10.1162/NECO_a_00779)), Roelfsema ([AuGMEnT](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004060)), Hawkins ([HTM](https://en.wikipedia.org/wiki/Hierarchical_temporal_memory)), and others ([Heeger et al 2017](http://www.pnas.org/content/114/8/1773.abstract.html?etoc), ...). How these models compare to each other on standard tasks is unclear. Further, biological plausibility of these models is quite variable.\n  \nFrom the neuroscience perspective, we want to figure out how the brain performs cognitive tasks, by synthesizing current models and tasks, constrained by known architecture and learning rules. From the machine learning perspective, we will explore whether brain-inspired architecture(s) can improve artificial intelligence (cf. copying bird flight didn't help build airplanes, but copying neurons helped machine learning).  \n  \n==========================================================================  \n  \n2020 Update:  \n  \nA group of 5 Masters students, Chunyu Deng, Xingchen Xiao, Chengkun Zeng, Jie Zhang, Jiqing Feng, supervised by Aditya Gilra, at the University of Sheffield, rewrote, modularized, and enhanced the code in PyTorch, which is available at:\n[https://github.com/CgnRLAgent](https://github.com/CgnRLAgent) as  \ntwo separate [tasks (as AI Gym environments) repo](https://github.com/CgnRLAgent/cog_ml_tasks) and [agents repo](https://github.com/CgnRLAgent/cog_tasks_rl_agents).  \n  \n==========================================================================  \n  \n2017-2018 Update:  \n  \nAs part of this project, we introduced an extension of [AuGMEnT](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004060), called hybrid AuGMEnT, that incorporates multiple timescales of memory dynamics, enabling it to solve tasks like 12AX which the original AuGMEnT could not. See our article:  \nMulti-timescale memory dynamics extend task repertoire in a reinforcement learning network with attention-gated memory  \nMarco Martinolli, Wulfram Gerstner, Aditya Gilra  \nFront. Comput. Neurosci. 2018 | [doi: 10.3389/fncom.2018.00050](https://www.frontiersin.org/articles/10.3389/fncom.2018.00050)  \npreprint at:  \n[arXiv:1712.10062 \\[q-bio.NC\\]](https://arxiv.org/abs/1712.10062).  \nCode for this article is available at [https://github.com/martin592/hybrid_AuGMEnT](https://github.com/martin592/hybrid_AuGMEnT).  \n  \n============================================================================  \n  \n2016-2017:  \n  \nWe utilize a modular architecture to:\n1) Specify the model such that we can 'plug and play' different modules -- controller, differentiable memories (multiple can be used at the same time). We should be able to interface both the abstract 'neurons' (LSTM, GRU, McCullough-Pitts, ReLU, ...) but also more biological spiking neurons.\n2) Specify Reinforcement Learning or other tasks -- 1-2AX, Raven progressive matrices, BABI tasks...\n  \nCurrently, we have HER, AuGMEnT, and LSTM implementations that can be run on these tasks:  \n  '0':'task 1-2',  \n\t'1':'task AX_CPT',  \n\t'2':'task 12-AX_S',  \n\t'3':'task 12-AX',  \n\t'4':'saccade/anti-saccade task',  \n\t'5':'sequence prediction task',  \n\t'6':'copy task',  \n\t'7':'repeat copy task'  \nusing the script interface.py . We also cloned and modified the official DNC implementation (see README in DNC_analysis folder). We also replicated the One-shot NTM on the onmiglot task (also in DNC_analysis folder). \n  \nWe will also explore different memory interfacing schemes like content or list-based as in the DNC, or Plate's/Eliasmith's Holographic Reduced Representations/Semantic Pointer Architecture, address-value augmentation, etc.  \n  \nA larger goal will be to see if the synthesized 'network' can build models of the 'world' which generalize across tasks.\n  \nCurrently, we have three contributors: [Marco Martinolli](https://github.com/martin592), [Vineet Jain](https://github.com/vineetjain96) and [Aditya Gilra](https://github.com/adityagilra). We are looking for more contributors!  \n  \nAditya initiated and supervises the project, with reviews of ideas and architectures and pointers to synthesize them.  \n  \nMarco implemented the Hierarchical Error Representation (HER) model by Alexander and Brown (2015, 2016), incorporating hierarchical predictive coding and gated working memory structures, and the AuGMEnT model by Rombouts, Bohte and Roelfsema (2015), as well as the relevant tasks Saccade-AntiSaccade, 12AX, and sequence prediction tasks. See the extention of AuGMEnT,  [hybrid AuGMEnT]((https://github.com/martin592/hybrid_AuGMEnT)), developed by him as part of this project.  \n  \nVineet developed a common API for models and tasks as well as implemented some tasks. He also tested various parts of the memory architectures of DNC and NTM whose code has been incorporated from their official repositories. See his [one shot learning implementation](https://github.com/vineetjain96/one-shot-mann), an offshoot of this project.  \n  \nSee also:  \n[Overview of architectures (work in progress)](https://github.com/adityagilra/archibrain/wiki/Brain-models---ML-architectures)  \n[Brief survey of toolkits] (https://github.com/adityagilra/archibrain/wiki/Machine-Learning-library-comparisons-for-testing-brain-architectures). We have chosen [Keras](https://keras.io/) as a primary toolkit. We will follow an [agile software development](https://en.wikipedia.org/wiki/Agile_software_development) process, and frequently re-factor code (might even change the framework).\n",
            "readme_url": "https://github.com/adityagilra/archibrain",
            "frameworks": [
                "Keras",
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Neural Turing Machines",
            "arxiv": "1410.5401",
            "year": 2014,
            "url": "http://arxiv.org/abs/1410.5401v2",
            "abstract": "We extend the capabilities of neural networks by coupling them to external\nmemory resources, which they can interact with by attentional processes. The\ncombined system is analogous to a Turing Machine or Von Neumann architecture\nbut is differentiable end-to-end, allowing it to be efficiently trained with\ngradient descent. Preliminary results demonstrate that Neural Turing Machines\ncan infer simple algorithms such as copying, sorting, and associative recall\nfrom input and output examples.",
            "authors": [
                "Alex Graves",
                "Greg Wayne",
                "Ivo Danihelka"
            ]
        },
        {
            "title": "One-shot Learning with Memory-Augmented Neural Networks",
            "arxiv": "1605.06065",
            "year": 2016,
            "url": "http://arxiv.org/abs/1605.06065v1",
            "abstract": "Despite recent breakthroughs in the applications of deep neural networks, one\nsetting that presents a persistent challenge is that of \"one-shot learning.\"\nTraditional gradient-based networks require a lot of data to learn, often\nthrough extensive iterative training. When new data is encountered, the models\nmust inefficiently relearn their parameters to adequately incorporate the new\ninformation without catastrophic interference. Architectures with augmented\nmemory capacities, such as Neural Turing Machines (NTMs), offer the ability to\nquickly encode and retrieve new information, and hence can potentially obviate\nthe downsides of conventional models. Here, we demonstrate the ability of a\nmemory-augmented neural network to rapidly assimilate new data, and leverage\nthis data to make accurate predictions after only a few samples. We also\nintroduce a new method for accessing an external memory that focuses on memory\ncontent, unlike previous methods that additionally use memory location-based\nfocusing mechanisms.",
            "authors": [
                "Adam Santoro",
                "Sergey Bartunov",
                "Matthew Botvinick",
                "Daan Wierstra",
                "Timothy Lillicrap"
            ]
        },
        {
            "title": "Multi-timescale memory dynamics in a reinforcement learning network with attention-gated memory",
            "arxiv": "1712.10062",
            "year": 2017,
            "url": "http://arxiv.org/abs/1712.10062v1",
            "abstract": "Learning and memory are intertwined in our brain and their relationship is at\nthe core of several recent neural network models. In particular, the\nAttention-Gated MEmory Tagging model (AuGMEnT) is a reinforcement learning\nnetwork with an emphasis on biological plausibility of memory dynamics and\nlearning. We find that the AuGMEnT network does not solve some hierarchical\ntasks, where higher-level stimuli have to be maintained over a long time, while\nlower-level stimuli need to be remembered and forgotten over a shorter\ntimescale. To overcome this limitation, we introduce hybrid AuGMEnT, with leaky\nor short-timescale and non-leaky or long-timescale units in memory, that allow\nto exchange lower-level information while maintaining higher-level one, thus\nsolving both hierarchical and distractor tasks.",
            "authors": [
                "Marco Martinolli",
                "Wulfram Gerstner",
                "Aditya Gilra"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "bAbi"
            },
            {
                "name": "Wikipedia"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.5911145633798736,
        "task": "Machine Translation",
        "task_prob": 0.7856806097044475
    }
}