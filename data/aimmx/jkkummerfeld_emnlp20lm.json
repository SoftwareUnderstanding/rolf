{
    "visibility": {
        "visibility": "public",
        "license": "BSD 3-Clause \"New\" or \"Revised\" License"
    },
    "name": "Improving Low Compute Language Modeling with In-Domain Embedding Initialisation",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "jkkummerfeld",
                "owner_type": "User",
                "name": "emnlp20lm",
                "url": "https://github.com/jkkummerfeld/emnlp20lm",
                "stars": 2,
                "pushed_at": "2020-09-21 18:54:32+00:00",
                "created_at": "2020-09-21 18:40:48+00:00",
                "language": "Python",
                "description": "Code for the paper \"Improving Low Compute Language Modeling with In-Domain Embedding Initialisation\" at EMNLP 2020",
                "license": "BSD 3-Clause \"New\" or \"Revised\" License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "09d493bf1fc257505c1336f3f87425568ab9da3c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/LICENSE"
                    }
                },
                "size": 1500
            },
            {
                "type": "code",
                "name": "_config.yml",
                "sha": "cbfb7ec8e7de0e90e3dd4a11152eb22795ff74ac",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/_config.yml"
                    }
                },
                "size": 210
            },
            {
                "type": "code",
                "name": "data-preprocessing",
                "sha": "70a768eec20d08ca95e5d16e38a899e4657ab797",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/tree/master/data-preprocessing"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "data.py",
                "sha": "6a3aa7f02ad722c3b2e51df7132a441e7125a772",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/data.py"
                    }
                },
                "size": 1628
            },
            {
                "type": "code",
                "name": "do-check.sh",
                "sha": "c9ce308dc327f033d05c85ed7d08f2ec5b0ee676",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/do-check.sh"
                    }
                },
                "size": 609
            },
            {
                "type": "code",
                "name": "embed_regularize.py",
                "sha": "f4ad922486d89723601aba31069021a09c2fb36c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/embed_regularize.py"
                    }
                },
                "size": 1084
            },
            {
                "type": "code",
                "name": "example-run.sh",
                "sha": "ad9e59aa60bc7aad396c42303fc24acbf9aae20c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/example-run.sh"
                    }
                },
                "size": 647
            },
            {
                "type": "code",
                "name": "finetune.py",
                "sha": "f5f72fc9470dfc2bf203d338ca6121cf47dec3fb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/finetune.py"
                    }
                },
                "size": 11993
            },
            {
                "type": "code",
                "name": "generate.py",
                "sha": "22fe5bcf195e9a68ef351d3f1cbdf129c209ce33",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/generate.py"
                    }
                },
                "size": 2597
            },
            {
                "type": "code",
                "name": "getdata.sh",
                "sha": "3c7a45a4c7c8e716d1577692566330a2059890ae",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/getdata.sh"
                    }
                },
                "size": 783
            },
            {
                "type": "code",
                "name": "locked_dropout.py",
                "sha": "253f9822629b51dbb2e3ec4340b3a9d9ff8142d9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/locked_dropout.py"
                    }
                },
                "size": 454
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "057e1f86b185c20eefe2ee84314dda20c046230a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/main.py"
                    }
                },
                "size": 14438
            },
            {
                "type": "code",
                "name": "model.py",
                "sha": "757a1125e5af7ef5a862c764db6f6c45dd7d3574",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/model.py"
                    }
                },
                "size": 4061
            },
            {
                "type": "code",
                "name": "pointer.py",
                "sha": "077fa6c6d928ab468863be602c8133d2c16ab76f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/pointer.py"
                    }
                },
                "size": 5652
            },
            {
                "type": "code",
                "name": "print-decoder.py",
                "sha": "647e43f6429e485180fdadd6eec906a733a76284",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/print-decoder.py"
                    }
                },
                "size": 2603
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "82bb1574b84e3e1b91f8f20f79e91d64bc366bb5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/utils.py"
                    }
                },
                "size": 961
            },
            {
                "type": "code",
                "name": "weight_drop.py",
                "sha": "5b20bc997aae011686f8d06471035a64ef47139d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jkkummerfeld/emnlp20lm/blob/master/weight_drop.py"
                    }
                },
                "size": 1768
            }
        ]
    },
    "authors": [
        {
            "name": "Jonathan Kummerfeld",
            "email": "jkummerf@umich.edu",
            "github_id": "jkkummerfeld"
        }
    ],
    "tags": [],
    "description": "Code for the paper \"Improving Low Compute Language Modeling with In-Domain Embedding Initialisation\" at EMNLP 2020",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/jkkummerfeld/emnlp20lm",
            "stars": 2,
            "issues": true,
            "readme": "# Improving Low Compute Language Modeling with In-Domain Embedding Initialisation\n\nThis repository contains code for language modeling experiments, as described in:\n\n  - [Improving Low Compute Language Modeling with In-Domain Embedding Initialisation](),\n  Charles Welch, Rada Mihalcea \\and Jonathan K. Kummerfeld\n  EMNLP 2020\n\nIt is based on the [original version](https://github.com/salesforce/awd-lstm-lm/tree/4582a1e9ecb1de177c01d01510dccd00b9abbbde) of the AWD-LSTM language model (later versions of the code had slightly different performance, so we used the original to match the original paper).\nMost of this readme file is taken from that code.\n\n## Data preparation\n\nThis repository contains the code we used to pre-process the data. There are files for extraction:\n\n- `data-preprocessing/gigaword-extract.py`\n- `data-preprocessing/cord-extract.py`\n- `data-preprocessing/wiki-extract.py`\n- `data-preprocessing/nanc-extract.py`\n- `data-preprocessing/irc-extract.py`\n\nAnd files for tokenising with Stanza and converting numbers:\n\n- `data-preprocessing/tokenise.py`\n- `data-preprocessing/convert_nums.py`\n\nWe also include a script that reads the LDC PTB tgz file and produces our version of the PTB:\n\n- `data-preprocessing/make-non-unk-ptb.py`\n\nFor example, to prepare the data in the same way we did, run these two commands (where `treebank_3_LDC99T42.tgz` must be downloaded from the LDC).\n\n```\n./data-preprocessing/make-non-unk-ptb.py --prefix ptb.std. treebank_3_LDC99T42.tgz\n./data-preprocessing/make-non-unk-ptb.py --prefix ptb.rare. --no-unks treebank_3_LDC99T42.tgz\n```\n\n## Changes to AWD-LSTM\n\nThe model code has been modified to support the experiments described in the paper.\nSpecifically, we added:\n\n- Initialising embeddings from provided vectors.\n- Freezing embeddings.\n- Untying embeddings.\n- Specifying separate input and output embeddings properties (e.g. size).\n\nThese are controlled via command line options:\n\n```\n  --emsize EMSIZE       size of word embeddings\n  --nout NOUT           size of output embedding. Must match emsize if tying\n  --untied              Do not tie the input and output weights\n  --random-in           Use random init for the input embeddings\n  --random-out          Use random init for the output embeddings\n  --freeze-in           Freeze the input embeddings\n  --freeze-out          Freeze the output embeddings but not the bias vector\n  --freeze-out-withbias\n                        Freeze the output embeddings and the bias vector\n  --embed EMBED         File with word embeddings\n```\n\n# AWD-LSTM Language Model\n\n### Averaged Stochastic Gradient Descent with Weight Dropped LSTM\n\nThis repository contains the code used for [Salesforce Research](https://einstein.ai/)'s [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/abs/1708.02182) paper, originally forked from the [PyTorch word level language modeling example](https://github.com/pytorch/examples/tree/master/word_language_model).\nThe model comes with instructions to train a word level language model over the Penn Treebank (PTB) and [WikiText-2](https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset) (WT2) datasets, though the model is likely extensible to many other datasets.\n\n+ Install PyTorch 0.1.12_2\n+ Run `getdata.sh` to acquire the Penn Treebank and WikiText-2 datasets\n+ Train the base model using `main.py`\n+ Finetune the model using `finetune.py`\n+ Apply the [continuous cache pointer](http://xxx.lanl.gov/abs/1612.04426) to the finetuned model using `pointer.py`\n\nIf you use this code or our results in your research, please cite:\n\n```\n@article{merityRegOpt,\n  title={{Regularizing and Optimizing LSTM Language Models}},\n  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},\n  journal={arXiv preprint arXiv:1708.02182},\n  year={2017}\n}\n```\n\n## Software Requirements\n\nThis codebase requires Python 3 and PyTorch 0.1.12_2.\nIf you are using Anaconda, this can be achieved via:\n`conda install pytorch=0.1.12 -c soumith`.\n\nNote the older version of PyTorch - upgrading to later versions would require minor updates and would prevent the exact reproductions of the results below.\nPull requests which update to later PyTorch versions are welcome, especially if they have baseline numbers to report too :)\n\n## Experiments\n\nThe codebase was modified during the writing of the paper, preventing exact reproduction due to minor differences in random seeds or similar.\nThe guide below produces results largely similar to the numbers reported.\n\nFor data setup, run `./getdata.sh`.\nThis script collects the Mikolov pre-processed Penn Treebank and the WikiText-2 datasets and places them in the `data` directory.\n\n**Important:** If you're going to continue experimentation beyond reproduction, comment out the test code and use the validation metrics until reporting your final results.\nThis is proper experimental practice and is especially important when tuning hyperparameters, such as those used by the pointer.\n\n#### Penn Treebank (PTB)\n\nThe instruction below trains a PTB model that without finetuning achieves perplexities of `61.2` / `58.9` (validation / testing), with finetuning achieves perplexities of `58.8` / `56.6`, and with the continuous cache pointer augmentation achieves perplexities of `53.5` / `53.0`.\n\nFirst, train the model:\n\n`python main.py --batch_size 20 --data data/penn --dropouti 0.4 --seed 28 --epoch 300 --save PTB.pt`\n\nThe first epoch should result in a validation perplexity of `308.03`.\n\nTo then fine-tune that model:\n\n`python finetune.py --batch_size 20 --data data/penn --dropouti 0.4 --seed 28 --epoch 300 --save PTB.pt`\n\nThe validation perplexity after the first epoch should be `60.85`.\n\n**Note:** Fine-tuning modifies the original saved model in `PTB.pt` - if you wish to keep the original weights you must copy the file.\n\nFinally, to run the pointer:\n\n`python pointer.py --data data/penn --save PTB.pt --lambdasm 0.1 --theta 1.0 --window 500 --bptt 5000` \n\nNote that the model in the paper was trained for 500 epochs and the batch size was 40, in comparison to 300 and 20 for the model above.\nThe window size for this pointer is chosen to be 500 instead of 2000 as in the paper.\n\n**Note:** BPTT just changes the length of the sequence pushed onto the GPU but won't impact the final result.\n\n#### WikiText-2 (WT2)\n\nThe instruction below train a WT2 model that without finetuning achieves perplexities of `69.1` / `66.1` (validation / testing), with finetuning achieves perplexities of `68.7` / `65.8`, and with the continuous cache pointer augmentation achieves perplexities of `53.6` / `52.0` (`51.95` specifically).\n\n`python main.py --seed 20923 --epochs 750 --data data/wikitext-2 --save WT2.pt`\n\nThe first epoch should result in a validation perplexity of `629.93`.\n\n`python -u finetune.py --seed 1111 --epochs 750 --data data/wikitext-2 --save WT2.pt`\n\nThe validation perplexity after the first epoch should be `69.14`.\n\n**Note:** Fine-tuning modifies the original saved model in `PTB.pt` - if you wish to keep the original weights you must copy the file.\n\nFinally, run the pointer:\n\n`python pointer.py --save WT2.pt --lambdasm 0.1279 --theta 0.662 --window 3785 --bptt 2000 --data data/wikitext-2`\n\n**Note:** BPTT just changes the length of the sequence pushed onto the GPU but won't impact the final result.\n\n## Speed\n\nAll the augmentations to the LSTM, including our variant of [DropConnect (Wan et al. 2013)](https://cs.nyu.edu/~wanli/dropc/dropc.pdf) termed weight dropping which adds recurrent dropout, allow for the use of NVIDIA's cuDNN LSTM implementation.\nPyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed.\nThis ensures the model is fast to train even when convergence may take many hundreds of epochs.\n\nThe default speeds for the model during training on an NVIDIA Quadro GP100:\n\n+ Penn Treebank: approximately 45 seconds per epoch for batch size 40, approximately 65 seconds per epoch with batch size 20\n+ WikiText-2: approximately 105 seconds per epoch for batch size 80\n\nSpeeds are approximately three times slower on a K80. On a K80 or other memory cards with less memory you may wish to enable [the cap on the maximum sampled sequence length](https://github.com/salesforce/awd-lstm-lm/blob/ef9369d277f8326b16a9f822adae8480b6d492d0/main.py#L131) to prevent out-of-memory (OOM) errors, especially for WikiText-2.\n\nIf speed is a major issue, SGD converges more quickly than our non-monotonically triggered variant of ASGD though achieves a worse overall perplexity.\n",
            "readme_url": "https://github.com/jkkummerfeld/emnlp20lm",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Regularizing and Optimizing LSTM Language Models",
            "arxiv": "1708.02182",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.02182v1",
            "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2.",
            "authors": [
                "Stephen Merity",
                "Nitish Shirish Keskar",
                "Richard Socher"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Penn Treebank"
            },
            {
                "name": "WikiText-2"
            },
            {
                "name": "GigaWord"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999971505642351,
        "task": "Language Modelling",
        "task_prob": 0.9729717963980884
    }
}