{
    "visibility": {
        "visibility": "public"
    },
    "name": "Deep Learning with PyTorch Notes",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "maciejdomagala",
                "owner_type": "User",
                "name": "DLWP_notes",
                "url": "https://github.com/maciejdomagala/DLWP_notes",
                "stars": 0,
                "pushed_at": "2020-08-11 16:00:10+00:00",
                "created_at": "2020-08-05 20:48:45+00:00",
                "language": "Jupyter Notebook",
                "description": "Notes and code based on the Deep Learning with PyTorch book.",
                "frameworks": [
                    "PyTorch",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "DLWP_notes.md",
                "sha": "53538bc8701e59fd2838d6508cc05e905dd03e76",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/maciejdomagala/DLWP_notes/blob/master/DLWP_notes.md"
                    }
                },
                "size": 11402
            },
            {
                "type": "code",
                "name": "chap5.ipynb",
                "sha": "df4b35752864cc788fe64269395e63e2100ab0b2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/maciejdomagala/DLWP_notes/blob/master/chap5.ipynb"
                    }
                },
                "size": 242357
            },
            {
                "type": "code",
                "name": "chap6.ipynb",
                "sha": "48bdf39dc535e32eed3663d0ed080670600d070c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/maciejdomagala/DLWP_notes/blob/master/chap6.ipynb"
                    }
                },
                "size": 324487
            },
            {
                "type": "code",
                "name": "chap7.ipynb",
                "sha": "9153dfb560d6bf0f97613a1515cf192eecd4de8e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/maciejdomagala/DLWP_notes/blob/master/chap7.ipynb"
                    }
                },
                "size": 24037
            },
            {
                "type": "code",
                "name": "chap8.ipynb",
                "sha": "8db7149295e20cd1b1898e7cb3ca1a6c798cab1f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/maciejdomagala/DLWP_notes/blob/master/chap8.ipynb"
                    }
                },
                "size": 58978
            },
            {
                "type": "code",
                "name": "models",
                "sha": "cb0ce30a47af1911b87771947fbb423f8aee0a45",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/maciejdomagala/DLWP_notes/tree/master/models"
                    }
                },
                "num_files": 1
            }
        ]
    },
    "authors": [
        {
            "name": "Maciej Domaga\u0142a",
            "github_id": "maciejdomagala"
        }
    ],
    "tags": [],
    "description": "Notes and code based on the Deep Learning with PyTorch book.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/maciejdomagala/DLWP_notes",
            "stars": 0,
            "issues": true,
            "readme": "# Deep Learning with PyTorch Notes\n\nSource: [https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf)\n\nNotes made for chosen chapters when reading a book. \nAdditional informations are included in each jupyter notebook for given chapter.\n\n## Chapter 5\n\n- 110 - L1 vs L2 metric for error (MSE)\n- 111 - torch.ones(()), using size, shape, type functions, broadcasting (more on broadcasting semantics: [https://pytorch.org/docs/stable/notes/broadcasting.html](https://pytorch.org/docs/stable/notes/broadcasting.html), examples in Jupyter).\n- 113 - idea of a gradient descent, rate of change of the function (ROC) formula\n- 114 - why the analytically calcuated gradient is better then calculating ROC for discrete values\n- 118 - overtraining with too big learning rate problem\n- 119 - input normalization, problem of having parameters of different scale\n\nhaving different learning rates for different parameters is not a good option, becomes redundant as the model scales up. always normalize the inputs.\n\n- 123 - using autograd functionality\n\ncalling backward on a function accumulates the gradient. after updating parameters use **params.grad.zero_()** to set the gradient to 0 before next iteration of gradient calculating.\n\nusage of **torch.no_grad()**: it stops tracing the grad changes. used when updating the parameters, since grad would track that in-place change being made.\nanother way of doing that: creating a copy of a tensor using the **tensor.detach()** functionality (also removes the grad tracking).\nalso **torch.set_grad_enabled()** which takes boolean value could be used\n\n- 127 - adding the optimizers, torch.optim\n\noptimizers have two primary methods: **zero_grad** and **step.** when constructing optimizer, we are passing the parameters inside. the **zero_grad** will zero the grad for these parameters. **step** method is updating the value of the parameters (we don't have to change the values of parameters after backward pass in place anymore)\n\n- 130 - ADAM optimizer\n\nADAM is a lot less sensitive to the scale of the parameters than SGD. it has the adaptive learning rate property.\n\n- 131 - train/test/valid dataset separation, overfitting\n\nif the training loss and validation loss are diverge with the training going forward, it means we are overfitting. the model is fitting too well to the training data and therefore it's not generalizing well\n\nways to prevent overfitting:\n1. using more data (more samples for model to learn \u2192 statistically better generalizing)\n2. using regularization methods - penalizing big differences between the prediction and ground truths. this makes sure that the model will be as regular in-between the training data.\n3. using data augmentation - providing more training samples by manipulation of already existing data (adding noise, translations etc.)\n4. making the model simpler - having less parameters \u2192 a bit worse fit to the data \u2192 less error during prediction on new data\n5. apply dropout\nusual way of performing the neural network fitting in two steps:\n1. make it bigger until it fits the data\n2. scale it down so it prevents overfitting.\n\n- 134 - splitting a dataset\n\n**torch.randperm()** is shuffling the tensor indices\n\n## Chapter 6\n\n- 143 - artificial neuron, weight/bias\n- 144 - multilayer network\n- 145 - activation function\n\nactivation function has two main roles:\n1. it presents non-linearity to the model. normally, neuron construction only allows linear changes (mul and add in the neuron). we can later apply activation function of any type (tanh, relu etc.) to change the output.\n2. it allows us to get a certain type of output from the network when included on the last layer (for instance classes with softmax)\n\n- 147 - types of activation functions\n\nReLU - widely used, one of the state-of-the-art performances in networks.\nSoftmax - used to be best, now used mostly when the output from the network should be within the [0,1] range (e.g. probability)\n\nactivation functions properties:\n1. non-linear\n2. differantiable (for gradient calculation). point discontinuities are fine (so ReLU works)\n\n- 151 - torch.nn module\n- 152 - __call__ vs forward in nn.Module\n- 155 - batching\n\nreasons for batching:\n1. processing only one sample at the time doesn't saturate CPU/GPU performance. these are normally parallelized which means we can perform whole batches at once to use all of the computing power\n2. some models use statistics which are calculated on the whole batches\n\nto use the nn.Module we need to provide the data where first dimension is the batch quantity. for single tensors as inputs we can use **torch.unsqueeze()**\n\n- 159 - nn.Sequential\n- 160 - inspecting the parameters of the network\n\nuse **model.parameters()** (it's a generator) to inspect the parameters.\nto check them with names use **model.named_parameters().**\n\nuse OrderedDict to define sequential model with layer names. makes it easier for parameters inspection later.\n\n## Chapter 7\n\n- 165 - torchvision, downloading dataset\n- 168 - torchvision.transforms\n\n**torchvision.transforms** provides a translation of PIL and numpy to tensors. it can be used directly when loading a dataset when using **transform=transforms.ToTensor()**\n\nmatplotlib expects HxWxC shape (color channels at the end) when plotting. pytorch uses CxHxW so there is a need to **permute** it.\n\n- 170 - normalizing the data, using **transform.Compose**\n\n**torch.view** allows to make calculations on a reshaped tensor without defining a new one (same storage is used), works similar to **np.reshape** (https://numpy.org/doc/stable/reference/generated/numpy.reshape.html)\n\n- 175 - classification problem\n\ndealing with the categorical input requires using different functions. **softmax** gives us output as a probability at the end of the network\n\n- 180 - loss functions for classification, NLLLoss\n\nnegative log-likelihood (in theory):\n1. for a single sample in the batch, look at the probability value *p* that model assigned to the correct class.\n2. calculate the -log(*p*), which is high when probability is low and vice versa\n3. add that to the overall loss for a batch\n\nin pytorch, the NLLLoss is not implemented as in theory, it doesn't apply the log value to the probability. therefore, we need to already input the logarithmic value inside of the loss function. for that, we should switch **softmax** to **logsoftmax** in the model, so the output tensor from the model is already with log.\n\n- 183 - sample, mini-batch and batch training\n- 185 - DataLoader\n\nusing **LogSoftmax** with **NLLLoss** results in the same result as using **nn.CrossEntropyLoss** at one go for the loss value (without using any softmax function at the end of the network). difference  is, the results of the network won't be presentable as probabilities (or log_probabilities), since softmax gives us that.\n\n- 186 - checking the accuracy of the model\n- 189 - limitations of nn.Linear\n\nfully connected network is not translation invariant. recognizing a pattern in one place on the image doesn't contribute to recognizing the pattern in a different place. that is why linear layers are redundant - they need too many parameters to take every possibility into account.\n\n## Chapter 8\n\n- 194 - convolutions\n\nusing convolution kernels instead of fully-connected layers:\n1. local operations on the neighboourhoods,\n2. translation invariance (we slide the kernel over the whole image)\n3. fewer parameters\n\n- 198 - padding\n\npadding is used to leave the image dimensions unchanged after the convolution layer. important in big architectures.\n\n- 203 - downsampling\n\nidea of downsampling: even though convolutions are reducing the number of parameters and equip us with locality and translation invariance, the kernels have limited (and usually *small*) size. therefore we are unable to get the \"big picture\" properties of an image. to address that, we could use bigger kernels or use downsampling between convolutions.\n\ndownsampling techniques:\n1. average pixels - e.g. 4 pixels from each 2x2 square are being represented as one using the average value of all of them\n2. max pixels - from 4 pixels we leave one with the highest value (the most responsive one) (**nn.MaxPool2d**)\n3. strided convolution when we are looking with the convolutional filter at the pixels that are further from each other.\n\n- 205 - receptive field of an output neuron\n\nuse **numel()** to get the number of parameters in each of the network layer\n\nwhen working with **nn.Sequential** we can get from multichannel, 2d layer output (for instance after conv2d layer) to the Linear layer (at the end, when we want to calculate the probabilities) using the **nn.Flatten** functionality. without it, we must build the forward network pass by hand.\n\n- 208 - building **nn.Module** subclass and defining forward pass\n- 210 - using **nn.functional**\n- 211 - functional vs modular API\n\nmain difference between using **nn.Module** and **nn.functional** utilities is that with the former, we trace the parameters, the latter is purely functional input \u2192 output mapping, nothing is remember. useful e.g for activation functions, pooling etc.\n\n- 214 - saving a model\n- 215 - loading a model\n- 215 - GPU training\n\nwhen loading model that was trained on different device we want to load it into, we can use **map_location** keyword in **torch.load** function.\n\n- 218 - adding width to the network\n- 219 - regularization methods\n\nL2 regularization is called *weight decay*, it sums the squares of the weights in the model\nL1 is called *lasso regression,* it is a sum of absolute values in the model\nL1 is good for sparsity, when there are many inputs and you believe that only a few of them are meaningful. L2 is good at dealing with correlated inputs. If two inputs are perfectly correlated L2 put half of the weight (beta) on each input, while L1 would pick one randomly (so less stable). One can use a combination of L1, L2 to get a balance of both, also known as Elastic Net.\n\n- 221 - dropout\n- 222 - batch normalization\n\nreckon that when using **nn.BatchNorm2d** in the network we should set bias=False in the **nn.Conv2d** since the bias is already included in the batch normalization part. \n\n- 223 - **model.train()** and **model.eval()**\n- 224 - vanishing gradient problem\n\ndeep networks prior to 2015 had ~20 layers, the training on more of them was highly uneffective due to the vanishing gradient problem. Multiplying a lot of small numbers during backpropagation led to the parameters on the early layer to be very difficult to train properly. problem was addressed in 2015 by introducing the ResNets (https://arxiv.org/abs/1512.03385) and reaching even 100 layers in the networks.\n\n- 227 - creating deep ResNet sequentially with the usage of ResBlocks.\n- 228 - weight initialization\n- 230 - overgeneralization\n\n## Chapter 10\n\n- 257 - bash commands to explore the .csv data.\n\nuse **wl -l** command to count the number of rows, use **head** to check the first few lines\n\n- 259 - collections.namedtuple usage\n- 260 - caching data on the disk\n- 263 - MetalIO format from SimpleITK\n- 271 - custom Dataset creation\n\n Dataset subclasses in PyTorch API need two methods: \"__len__\" and \"__getitem__\"\n\n- 275 - training/validation datasets\n\nboth of validation and training sets should have a good representation of all of the data (all variations of inputs). unless the training is meant to be robust for outliers they should not include any.\n",
            "readme_url": "https://github.com/maciejdomagala/DLWP_notes",
            "frameworks": [
                "PyTorch",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9995742269647953,
        "task": "Image Classification",
        "task_prob": 0.7693957022306447
    },
    "training": {
        "datasets": [
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "COCO"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ImageNet"
            }
        ]
    }
}