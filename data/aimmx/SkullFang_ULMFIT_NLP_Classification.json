{
    "visibility": {
        "visibility": "public"
    },
    "name": "NLP transfer learning with ULMFIT",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "SkullFang",
                "owner_type": "User",
                "name": "ULMFIT_NLP_Classification",
                "url": "https://github.com/SkullFang/ULMFIT_NLP_Classification",
                "stars": 6,
                "pushed_at": "2019-03-10 02:45:22+00:00",
                "created_at": "2019-01-30 09:13:33+00:00",
                "language": "Jupyter Notebook",
                "description": "Use transfer learning by ULMFIT for nlp classification",
                "frameworks": [
                    "NLTK",
                    "scikit-learn"
                ]
            },
            {
                "type": "code",
                "name": ".gitattributes",
                "sha": "dfe0770424b2a19faf507a501ebfc23be8f54e7b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/SkullFang/ULMFIT_NLP_Classification/blob/master/.gitattributes"
                    }
                },
                "size": 66
            },
            {
                "type": "code",
                "name": ".ipynb_checkpoints",
                "sha": "740989ff40b6db83bb02db1c49b3760e8b58d937",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/SkullFang/ULMFIT_NLP_Classification/tree/master/.ipynb_checkpoints"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "NLP_transfer_learning_text_classification.ipynb",
                "sha": "e98c60a6e71e3e1fab3257ce133a9992c3215042",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/SkullFang/ULMFIT_NLP_Classification/blob/master/NLP_transfer_learning_text_classification.ipynb"
                    }
                },
                "size": 59831
            }
        ]
    },
    "authors": [
        {
            "name": "Yan Zhang",
            "github_id": "SkullFang"
        }
    ],
    "tags": [],
    "description": "Use transfer learning by ULMFIT for nlp classification",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/SkullFang/ULMFIT_NLP_Classification",
            "stars": 6,
            "issues": true,
            "readme": "# NLP transfer learning with ULMFIT\n\n# Introduction\n\nUse the fastai (https://www.fast.ai/)framework to implement transfer learning for text classification.\n\nThe language model was trained on a corpus named Wikitext-103.(https://openreview.net/pdf?id=Byj72udxe)\n\nIf you want to know some detailed information, please refer to Jeremy Howard\u2019s paper.(https://arxiv.org/abs/1801.06146v5)\n#  Environmental configuration\n\nfasiai 1.0.30\npytroch 1.0.0\n\n## download anaconda\n\nhttps://www.anaconda.com/download/#linux\n\n## install fasiai \nfasiai Github https://github.com/fastai/fastai\n\n\uff08Note: It is highly recommended to use the conda method to install.\uff09\n\nThe tutorial below the official github course folder is fastai version is 0.7. I am using the address of the corresponding tutorial for 1.0.0 should be:\n\nhttps://github.com/fastai/course-v3\n\nvideo:\n\nhttps://www.usfca.edu/data-institute/certificates/deep-learning-part-two",
            "readme_url": "https://github.com/SkullFang/ULMFIT_NLP_Classification",
            "frameworks": [
                "NLTK",
                "scikit-learn"
            ]
        }
    ],
    "references": [
        {
            "title": "Universal Language Model Fine-tuning for Text Classification",
            "arxiv": "1801.06146",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.06146v5",
            "abstract": "Inductive transfer learning has greatly impacted computer vision, but\nexisting approaches in NLP still require task-specific modifications and\ntraining from scratch. We propose Universal Language Model Fine-tuning\n(ULMFiT), an effective transfer learning method that can be applied to any task\nin NLP, and introduce techniques that are key for fine-tuning a language model.\nOur method significantly outperforms the state-of-the-art on six text\nclassification tasks, reducing the error by 18-24% on the majority of datasets.\nFurthermore, with only 100 labeled examples, it matches the performance of\ntraining from scratch on 100x more data. We open-source our pretrained models\nand code.",
            "authors": [
                "Jeremy Howard",
                "Sebastian Ruder"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "WikiText-103"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9998262454074887,
        "task": "Sentiment Analysis",
        "task_prob": 0.9608697810056794
    }
}