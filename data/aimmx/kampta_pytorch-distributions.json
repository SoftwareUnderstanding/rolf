{
    "visibility": {
        "visibility": "public"
    },
    "name": "Bayesian Deep Learning with torch distributions",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "kampta",
                "owner_type": "User",
                "name": "pytorch-distributions",
                "url": "https://github.com/kampta/pytorch-distributions",
                "stars": 14,
                "pushed_at": "2021-01-06 04:00:55+00:00",
                "created_at": "2018-12-24 23:13:33+00:00",
                "language": "Python",
                "description": "Basic VAE flow using pytorch distributions",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "binconcrete_vae.py",
                "sha": "b03c1de0c280783648b2370b5201f51c70ba76c8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kampta/pytorch-distributions/blob/master/binconcrete_vae.py"
                    }
                },
                "size": 5749
            },
            {
                "type": "code",
                "name": "concrete_vae.py",
                "sha": "1859a2484d45edb49a7feeee71003cd312aacaa7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kampta/pytorch-distributions/blob/master/concrete_vae.py"
                    }
                },
                "size": 5972
            },
            {
                "type": "code",
                "name": "gaussian_vae.py",
                "sha": "2b94aaaf45d12a1e53845f2e039a66c8478c1f7c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kampta/pytorch-distributions/blob/master/gaussian_vae.py"
                    }
                },
                "size": 5263
            },
            {
                "type": "code",
                "name": "imgs",
                "sha": "40b86a53aa5443c9dd6e91b6d031bb6ba1ca8a7e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kampta/pytorch-distributions/tree/master/imgs"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "2333803ba1916776557ad6130c7d81cf444ac3dc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kampta/pytorch-distributions/blob/master/requirements.txt"
                    }
                },
                "size": 26
            },
            {
                "type": "code",
                "name": "results",
                "sha": "dcf8240eb4df3fb759b1a0aeeb3528becc5c7050",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kampta/pytorch-distributions/tree/master/results"
                    }
                },
                "num_files": 20
            },
            {
                "type": "code",
                "name": "vae_mnist.py",
                "sha": "dd897beffe698f3b7dc71b4a1dca6d1947c831c0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kampta/pytorch-distributions/blob/master/vae_mnist.py"
                    }
                },
                "size": 5099
            }
        ]
    },
    "authors": [
        {
            "name": "Kamal Gupta",
            "github_id": "kampta"
        }
    ],
    "tags": [],
    "description": "Basic VAE flow using pytorch distributions",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/kampta/pytorch-distributions",
            "stars": 14,
            "issues": true,
            "readme": "# Bayesian Deep Learning with torch distributions\nThis repository contains basic examples of [pytorch `distributions`](https://pytorch.org/docs/stable/distributions.html) package.\nThe `distributions` package is pytorch adaptation of [tensorflow `distributions`](https://arxiv.org/abs/1711.10604) which implements\nbuilding blocks for Bayesian Deep Learning. To read more about the examples in this\nrepository, you can read my blog [here]().\n\n**Installation**\n```\npip install -r requirements.txt\n```\n\n### Basic VAE setup\nBorrowed as it is from [pytorch repo](https://github.com/pytorch/examples/tree/master/vae).\nIt is implementation of the paper [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) by Kingma and Welling.\n\n\n**Usage**\n```\npython vae.py\n...\n...\n...\n====> Epoch: 10 Average loss: 106.3110\n====> Test set loss: 105.5890\n```\n\n**Reconstructed samples**\n\nSome sample reconstructions from the basic VAE (trained for 10 epochs on MNIST)  \n\n![alt text](imgs/vae_recon_sample.png \"Reconstructions from Standard VAE\")\n\n**Generated Samples**\n\nWe can also generate some sample digits from the basic VAE by providing random numbers\ngenerated from a normal distribution as input.\n\n![alt text](imgs/vae_sample.png \"Samples from Standard VAE\")\n\n\nWe will use this example as template for rest of the code.\n\n### Gaussian example\n\nWe'll construct the exact same example using using `distributions` package now.\nWe'll need to modify very little code. Notice the changes in `forward` and `loss_function`. \n\n```\npython gaussian_vae.py\n...\n...\n...\n====> Epoch: 10 Average loss: 106.3209\n====> Test set loss: 105.6140\n```\n\n**Reconstructed samples**\n\nSome sample reconstructions from the basic VAE (trained for 10 epochs on MNIST)  \n\n![alt text](imgs/gaussian_recon_sample.png \"Reconstructions from Standard VAE\")\n\n**Generated Samples**\n\n![alt text](imgs/gaussian_vae_sample.png \"Samples from Standard VAE\")\n\n\n### Benoulli example\n\nWe can make our latent representation bernoulli by using [relaxed bernoulli](https://pytorch.org/docs/stable/_modules/torch/distributions/relaxed_bernoulli.html#RelaxedBernoulli)\ndistribution. The file `binconcrete.py` contains implementation with bottleneck layer of size 20.\n\n```\npython binconcrete.py\n...\n...\n...\n====> Epoch: 10 Average loss: 126.6666\n====> Test set loss: 125.3123\n```\n\n**Reconstructed samples**\n\nSome sample reconstructions (trained for 10 epochs on MNIST)  \n\n![alt text](imgs/binconcrete_recon_sample.png \"Reconstructions from Bernoulli VAE\")\n\n**Generated Samples**\n\n![alt text](imgs/binconcrete_sample.png \"Samples from Bernoulli VAE\")\n\n\n### Categorical Example\nSimilar to the bernoulli example, a more general usecase is when the latent dimension is\ncategorical. \n\n\n```\npython concrete.py\n...\n...\n...\n====> Epoch: 10 Average loss: 110.2161\n====> Test set loss: 109.1930\n```\n\n**Reconstructed samples**\n\nSome sample reconstructions (trained for 10 epochs on MNIST)  \n\n![alt text](imgs/concrete_recon_sample.png \"Reconstructions from Categorical VAE\")\n\n**Generated Samples**\n\n![alt text](imgs/concrete_sample.png \"Samples from Categorical VAE\")\n\nFor more details on relaxed bernoulli or relaxed categorical distributions, please refer\nto the following papers\n\n[1] \"Categorical Reparameterization with Gumbel-Softmax\" - \nEric Jang, Shixiang Gu, Ben Poole - https://arxiv.org/abs/1611.01144\n\n[2] \"The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables\" -\n Chris J. Maddison, Andriy Mnih, Yee Whye Teh - https://arxiv.org/abs/1611.00712",
            "readme_url": "https://github.com/kampta/pytorch-distributions",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables",
            "arxiv": "1611.00712",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.00712v3",
            "abstract": "The reparameterization trick enables optimizing large scale stochastic\ncomputation graphs via gradient descent. The essence of the trick is to\nrefactor each stochastic node into a differentiable function of its parameters\nand a random variable with fixed distribution. After refactoring, the gradients\nof the loss propagated by the chain rule through the graph are low variance\nunbiased estimators of the gradients of the expected loss. While many\ncontinuous random variables have such reparameterizations, discrete random\nvariables lack useful reparameterizations due to the discontinuous nature of\ndiscrete states. In this work we introduce Concrete random\nvariables---continuous relaxations of discrete random variables. The Concrete\ndistribution is a new family of distributions with closed form densities and a\nsimple reparameterization. Whenever a discrete stochastic node of a computation\ngraph can be refactored into a one-hot bit representation that is treated\ncontinuously, Concrete stochastic nodes can be used with automatic\ndifferentiation to produce low-variance biased gradients of objectives\n(including objectives that depend on the log-probability of latent stochastic\nnodes) on the corresponding discrete graph. We demonstrate the effectiveness of\nConcrete relaxations on density estimation and structured prediction tasks\nusing neural networks.",
            "authors": [
                "Chris J. Maddison",
                "Andriy Mnih",
                "Yee Whye Teh"
            ]
        },
        {
            "title": "TensorFlow Distributions",
            "arxiv": "1711.10604",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.10604v1",
            "abstract": "The TensorFlow Distributions library implements a vision of probability\ntheory adapted to the modern deep-learning paradigm of end-to-end\ndifferentiable computation. Building on two basic abstractions, it offers\nflexible building blocks for probabilistic computation. Distributions provide\nfast, numerically stable methods for generating samples and computing\nstatistics, e.g., log density. Bijectors provide composable volume-tracking\ntransformations with automatic caching. Together these enable modular\nconstruction of high dimensional distributions and transformations not possible\nwith previous libraries (e.g., pixelCNNs, autoregressive flows, and reversible\nresidual networks). They are the workhorse behind deep probabilistic\nprogramming systems like Edward and empower fast black-box inference in\nprobabilistic models built on deep-network components. TensorFlow Distributions\nhas proven an important part of the TensorFlow toolkit within Google and in the\nbroader deep learning community.",
            "authors": [
                "Joshua V. Dillon",
                "Ian Langmore",
                "Dustin Tran",
                "Eugene Brevdo",
                "Srinivas Vasudevan",
                "Dave Moore",
                "Brian Patton",
                "Alex Alemi",
                "Matt Hoffman",
                "Rif A. Saurous"
            ]
        },
        {
            "title": "Auto-Encoding Variational Bayes",
            "arxiv": "1312.6114",
            "year": 2013,
            "url": "http://arxiv.org/abs/1312.6114v10",
            "abstract": "How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions is two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results.",
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ]
        },
        {
            "title": "Categorical Reparameterization with Gumbel-Softmax",
            "arxiv": "1611.01144",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.01144v5",
            "abstract": "Categorical variables are a natural choice for representing discrete\nstructure in the world. However, stochastic neural networks rarely use\ncategorical latent variables due to the inability to backpropagate through\nsamples. In this work, we present an efficient gradient estimator that replaces\nthe non-differentiable sample from a categorical distribution with a\ndifferentiable sample from a novel Gumbel-Softmax distribution. This\ndistribution has the essential property that it can be smoothly annealed into a\ncategorical distribution. We show that our Gumbel-Softmax estimator outperforms\nstate-of-the-art gradient estimators on structured output prediction and\nunsupervised generative modeling tasks with categorical latent variables, and\nenables large speedups on semi-supervised classification.",
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "MNIST"
            }
        ]
    },
    "domain": {
        "domain_type": "Unknown"
    }
}