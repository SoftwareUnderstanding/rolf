{
    "visibility": {
        "visibility": "public",
        "license": "BSD 3-Clause \"New\" or \"Revised\" License"
    },
    "name": "WaveNet implementation in Keras2",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "imdatsolak",
                "owner_type": "User",
                "name": "wavenet",
                "url": "https://github.com/imdatsolak/wavenet",
                "stars": 2,
                "pushed_at": "2019-10-02 07:46:25+00:00",
                "created_at": "2019-02-24 18:34:31+00:00",
                "language": "Python",
                "description": "Keras2-based implementation of WaveNet.",
                "license": "BSD 3-Clause \"New\" or \"Revised\" License",
                "frameworks": [
                    "Keras",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE.txt",
                "sha": "9f0ef9a3c7b5cb91371be16e153003addb529158",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/imdatsolak/wavenet/blob/master/LICENSE.txt"
                    }
                },
                "size": 1486
            },
            {
                "type": "code",
                "name": "charlie_chaplin.ini",
                "sha": "749359b05bd1a3476a9219b4ee560abe6548cd54",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/imdatsolak/wavenet/blob/master/charlie_chaplin.ini"
                    }
                },
                "size": 1018
            },
            {
                "type": "code",
                "name": "data",
                "sha": "a715cc31f8617357d21b47dac518b18318431ae8",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/imdatsolak/wavenet/tree/master/data"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "dataset.py",
                "sha": "20b37abdd20f78b50fff10cb2008902eff1f9f48",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/imdatsolak/wavenet/blob/master/dataset.py"
                    }
                },
                "size": 8699
            },
            {
                "type": "code",
                "name": "lvb_adagio.ini",
                "sha": "dad2cdb9b0b12e62c398d11c04fb309fb5b374b9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/imdatsolak/wavenet/blob/master/lvb_adagio.ini"
                    }
                },
                "size": 1012
            },
            {
                "type": "code",
                "name": "lvb_short.ini",
                "sha": "ac96e08104f11985f4710ec691e31c7a73386e43",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/imdatsolak/wavenet/blob/master/lvb_short.ini"
                    }
                },
                "size": 1011
            },
            {
                "type": "code",
                "name": "mlwavenet.py",
                "sha": "dcdf4f3aeebc823782ff4fac26a0d2860ffba26e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/imdatsolak/wavenet/blob/master/mlwavenet.py"
                    }
                },
                "size": 25204
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "63fab1ed15e8933f12a49640b66e33652d23ea89",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/imdatsolak/wavenet/blob/master/requirements.txt"
                    }
                },
                "size": 89
            },
            {
                "type": "code",
                "name": "settings.ini",
                "sha": "e82dc634412947404e8e832cd8b569d98b1f9a53",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/imdatsolak/wavenet/blob/master/settings.ini"
                    }
                },
                "size": 978
            },
            {
                "type": "code",
                "name": "wavenet_utils.py",
                "sha": "bf419ddaa66f25d16b54b5cc1ce04e5f3a9f37e9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/imdatsolak/wavenet/blob/master/wavenet_utils.py"
                    }
                },
                "size": 2171
            }
        ]
    },
    "authors": [
        {
            "name": "Imdat Solak",
            "github_id": "imdatsolak"
        }
    ],
    "tags": [],
    "description": "Keras2-based implementation of WaveNet.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/imdatsolak/wavenet",
            "stars": 2,
            "issues": true,
            "readme": "# WaveNet implementation in Keras2\nBased on https://deepmind.com/blog/wavenet-generative-model-raw-audio/ and https://arxiv.org/pdf/1609.03499.pdf.\n\nThis is the based on [Keras WaveNet implementation](https://github.com/basveeling/wavenet/) for Keras 2 and Tensorflow.\n\nI'm currently working on making the single ```mlwavenet.py``` multi-GPU-capable using Horovod, but this has not been fully tested yet, but it seems to work, though there is currently *no support* for predicting with multiple GPUs. I may add it over time...\n\nI use the following command to train on my DUAL-GPU (NVidia GeForce 1080 Ti) using Horovod & OpenMPI:\n    \n    /usr/local/bin/mpirun -np 2 -H localhost:2 -bind-to none -map-by slot -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -mca btl_tcp_if_exclude eno1 python mlwavenet.py -c multi_gpu_settings.ini -m\n\nThe ``-mca btl_tcp_if_exclude eno1`` just means that OpenMPI should not listen on that interface as that one is not configured on my machine...\n\nThe parameter  ``-m`` is important as it tells ``mlwavenet.py`` that it will be running in the multi-GPU mode!\n\nPlease check out [Horovod for details](https://github.com/uber/horovod)\n\n[Listen to a sample \ud83c\udfb6!](http://data.m-ailabs.bayern/data/Samples/Music/)\n\n## Installation:\nActivate a new python2 virtualenv (recommended):\n\n    pip install virtualenv\n    mkdir ~/virtualenvs && cd ~/virtualenvs\n    virtualenv wavenet\n    source wavenet/bin/activate\n\nClone and install requirements.\n\n    cd ~\n    git clone https://github.com/imdatsolak/keras2-wavenet.git\n    cd wavenet\n    pip install -r requirements.txt\n\n## Dependencies:\nThis implementation does not support python3 as of now.\n\n## Sampling:\nOnce the first model checkpoint is created, you can start sampling.\n\nRun:\n\n    $ python2 mlwavenet.py -c <config-file-used-for-training> -C predict -l 5\n\nThe latest model checkpoint will be retrieved and used to sample. The sample will be streamed to `[model_dir]/samples`, you can start listening when the first sample is generated.\nYou can either define the sample_length in settings-file or provide it as parameter (-l) - in seconds\n\nAlternatively, you can specify an epoch to use for generating audio:\n    \n    $ python2 mlwavenet.py -c <config-file> -C predict -l <duration-seconds> -e <epoch-no>\n\n### Sampling options:\n- `predict_length`: float. Number of seconds to sample (length in seconds).\n- `sample_argmax`: `True` or `False`. Always take the argmax\n- `sample_temperature`: `None` or float. Controls the sampling temperature. 1.0 for the original distribution, < 1.0 for less exploitation, > 1.0 for more exploration.\n- `sample_seed`: int: Controls the seed for the sampling procedure.\n- `initial_input`: string: Path to a wav file, for which the first `fragment_length` samples are used as initial input.\n\ne.g.:\n\n    $ python2 mlwavenet.py -c <settings-file> -C predict -l 5\n\n## Training:\n\nFor training, you now need to create a ```configuration-file```. The file is the Windows(r) .ini file-format. An example is provided.\n\nThe default setting is fine. You can immediately start training with it. The settings are for 4.4KHz-training.\n\n    $ python mlwavenet.py -c settings.ini -C train\n\nIf you don't provide a '-C' command-line option at all, it is assumed to be 'train'. Normally, it will automatically resume training at the last epoch if the settings-file is the same. If you want to re-start training, you can either provide '-R' (reset) or delete the models-directory.\n\nYou can, at any time, stop it using CTRL-C.\n\n## Using your own training data:\n- Create a new data directory with a train and test folder in it. All wave files in these folders will be used as data.\n    - Caveat: Make sure your wav files are supported by scipy.io.wavefile.read(): e.g. don't use 24bit wav and remove meta info.\n- Run with: `$ python2 mlwavenet.py -c <settings-file>`\n\n## Todo:\n- [ ] Local conditioning\n- [ ] Global conditioning\n- [x] Training on CSTR VCTK Corpus\n- [x] CLI option to pick a wave file for the sample generation initial input. Done: see `predict_initial_input`.\n- [x] Fully randomized training batches\n- [x] Soft targets: by convolving a gaussian kernel over the one-hot targets, the network trains faster.\n- [ ] Decaying soft targets: the stdev of the gaussian kernel should slowly decay.\n\n\n## Note on computational cost:\nThe Wavenet model is quite expensive to train and sample from. We can however trade computation cost with accuracy and fidility by lowering the sampling rate, amount of stacks and the amount of channels per layer.\n\nConfiguration: 2x GeForce 1080 Ti (each: 11GiB and ~11TFLOPS), Intel Core i7-6950X CPU @ 3.00GHz (Overclocked: 4.2GHz), 128GiB RAM, 1TB NVME SSD\n- Training at 22KHz, about 27 minutes of audio files: 6.5 hrs / epoch\n- Prediction of 5 seconds, @ 22KHz: 11 minutes\n\nDeepmind has reported that generating one second of audio with their model takes about 90 minutes.\n\n## Disclaimer\nThis is a re-implementation of the model described in the WaveNet paper by Google Deepmind. This repository is not associated with Google Deepmind.\n# wavenet\n",
            "readme_url": "https://github.com/imdatsolak/wavenet",
            "frameworks": [
                "Keras",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "WaveNet: A Generative Model for Raw Audio",
            "arxiv": "1609.03499",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.03499v2",
            "abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio\nwaveforms. The model is fully probabilistic and autoregressive, with the\npredictive distribution for each audio sample conditioned on all previous ones;\nnonetheless we show that it can be efficiently trained on data with tens of\nthousands of samples per second of audio. When applied to text-to-speech, it\nyields state-of-the-art performance, with human listeners rating it as\nsignificantly more natural sounding than the best parametric and concatenative\nsystems for both English and Mandarin. A single WaveNet can capture the\ncharacteristics of many different speakers with equal fidelity, and can switch\nbetween them by conditioning on the speaker identity. When trained to model\nmusic, we find that it generates novel and often highly realistic musical\nfragments. We also show that it can be employed as a discriminative model,\nreturning promising results for phoneme recognition.",
            "authors": [
                "Aaron van den Oord",
                "Sander Dieleman",
                "Heiga Zen",
                "Karen Simonyan",
                "Oriol Vinyals",
                "Alex Graves",
                "Nal Kalchbrenner",
                "Andrew Senior",
                "Koray Kavukcuoglu"
            ]
        }
    ],
    "domain": {
        "domain_type": "Speech",
        "domain_prob": 0.9844546058562528
    }
}