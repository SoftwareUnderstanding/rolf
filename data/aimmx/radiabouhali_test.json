{
    "visibility": {
        "visibility": "public",
        "license": "BSD 2-Clause \"Simplified\" License"
    },
    "name": "Colorful Image Colorization-->",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "radiabouhali",
                "owner_type": "User",
                "name": "test",
                "url": "https://github.com/radiabouhali/test",
                "stars": 0,
                "pushed_at": "2020-03-27 18:01:12+00:00",
                "created_at": "2020-03-27 17:57:48+00:00",
                "language": "Jupyter Notebook",
                "license": "BSD 2-Clause \"Simplified\" License",
                "frameworks": [
                    "Caffe",
                    "scikit-learn"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "6187c76461a7725da199947701a0931af7a20b3f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/radiabouhali/test/blob/master/LICENSE"
                    }
                },
                "size": 1328
            },
            {
                "type": "code",
                "name": "colorization",
                "sha": "d6c2b84adda331a2baa7b89d9200992f7fb5c6fd",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/radiabouhali/test/tree/master/colorization"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "demo",
                "sha": "527b05884cf06a33539d2edfad191175f29dd9d1",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/radiabouhali/test/tree/master/demo"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "feature_learning_tests",
                "sha": "e9a54106ccb6fb8b217754b02d42e267d581d5db",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/radiabouhali/test/tree/master/feature_learning_tests"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "interactive-deep-colorization",
                "sha": "f2f5ab2026dde3c556a5e7651490de4813feab79",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/radiabouhali/test/tree/master/interactive-deep-colorization"
                    }
                },
                "num_files": 11
            },
            {
                "type": "code",
                "name": "models",
                "sha": "c7832888ab942bae403c59d8759e16611bf5ccc5",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/radiabouhali/test/tree/master/models"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "resources",
                "sha": "79ec9727394a9b895b99ba162ddc6fe2c07de4d3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/radiabouhali/test/tree/master/resources"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "splitbrainauto",
                "sha": "55353e131bb1e3a2d12728deac1ebb9a3b5299b7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/radiabouhali/test/tree/master/splitbrainauto"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "train",
                "sha": "b3afbaa8b4e89f6e90b127abe39686f32d70b714",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/radiabouhali/test/tree/master/train"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "train_alexnet",
                "sha": "78a97cee8d54057faba5bc42f6f94cb9f9087362",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/radiabouhali/test/tree/master/train_alexnet"
                    }
                },
                "num_files": 12
            },
            {
                "type": "code",
                "name": "website",
                "sha": "0d545e22d919a621eab26547c5f9453623c6182e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/radiabouhali/test/tree/master/website"
                    }
                },
                "num_files": 2
            }
        ]
    },
    "authors": [
        {
            "name": "Richard Zhang",
            "github_id": "richzhang"
        },
        {
            "name": "Phillip Isola",
            "email": "phillip.isola@gmail.com",
            "github_id": "phillipi"
        },
        {
            "name": "Guilherme Folego",
            "email": "gfolego@gmail.com",
            "github_id": "gfolego"
        },
        {
            "name": "Hourann Bosci",
            "github_id": "azza-bazoo"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/radiabouhali/test",
            "stars": 0,
            "issues": true,
            "readme": "<!--<h3><b>Colorful Image Colorization</b></h3>-->\n## <b>Colorful Image Colorization</b> [[Project Page]](http://richzhang.github.io/colorization/) <br>\n[Richard Zhang](https://richzhang.github.io/), [Phillip Isola](http://web.mit.edu/phillipi/), [Alexei A. Efros](http://www.eecs.berkeley.edu/~efros/). In [ECCV, 2016](http://arxiv.org/pdf/1603.08511.pdf).\n\n![Teaser Image](http://richzhang.github.io/colorization/resources/images/teaser4.jpg)\n\n### Overview ###\nThis repository contains:\n\n<b>Colorization-centric functionality</b>\n - (0) a test time script to colorize an image (python script)\n - (1) a test time demonstration (IPython Notebook)\n - (2) code for training a colorization network\n - (3) links to our results on the ImageNet test set, along with a pointer to AMT real vs fake test code\n\n<b>Representation Learning-centric functionality</b>\n - (4) pre-trained AlexNet, used for representation learning tests (Section 3.2)\n - (5) code for training AlexNet with colorization\n - (6) representation learning tests\n\n<b>Appendices</b>\n - (A) Related follow-up work\n\n### Clone this repository ###\nClone the master branch of the respository using `git clone -b master --single-branch https://github.com/richzhang/colorization.git`\n\n### Dependencies ###\nThis code requires a working installation of [Caffe](http://caffe.berkeleyvision.org/) and basic Python libraries (numpy, pyplot, skimage, scipy). For guidelines and help with installation of Caffe, consult the [installation guide](http://caffe.berkeleyvision.org/) and [Caffe users group](https://groups.google.com/forum/#!forum/caffe-users).\n\n## Colorization-centric Functionality ##\n\n### (0) Test-Time Python Script ###\nWe provide a script for colorizing a single image. Run `./models/fetch_release_models.sh` to download the model. Then, run `python ./colorize.py -img_in [[INPUT_IMG_PATH]] -img_out [[OUTPUT_IMG_PATH]]`. For example, try `python ./colorize.py -img_in ./demo/imgs/ILSVRC2012_val_00041580.JPEG -img_out ./out.png`.\n\n### (1) Test-Time Demo in IPython Notebook ###\nWe also include demo usage as an iPython notebook, under [`./demo/colorization_demo_v2.ipynb`](https://github.com/richzhang/colorization/blob/master/demo/colorization_demo_v2.ipynb). This IPython Notebook demonstrates how to use our colorization network to colorize a grayscale image. To run this, after cloning the directory, `cd` into the `demo` directory, run `ipython notebook` and open `colorization_demo_v2.ipynb` in your web browser.\n\n### (2) Training Usage ###\nThe following contains instructions for training a colorization network from scratch. After cloning the repository, from the root directory:\n\n(1) Run `./train/fetch_init_model.sh`. This will load model `./models/init_v2.caffemodel`. This model was obtained using the k-means initialization implemented in [Kraehenbuehl et al, ICLR 2016](https://github.com/philkr/magic_init).\n\n(2) Run `./train/fetch_caffe.sh`. This will load a modified Caffe into directory `./caffe-colorization`. For guidelines and help with installation of Caffe, consult the [installation guide](http://caffe.berkeleyvision.org/) and [Caffe users group](https://groups.google.com/forum/#!forum/caffe-users).\n\n(3) Add the `./resources/` directory (as an absolute path) to your system environment variable $PYTHONPATH. This directory contains custom Python layers.\n\n(4) Modify paths in data layers `./models/colorization_train_val_v2.prototxt` to locate where ImageNet LMDB files are on your machine. These should be BGR images, non-mean centered, in [0,255].\n\n(5) Run `./train/train_model.sh [GPU_ID]`, where `[GPU_ID]` is the gpu you choose to specify. Notes about training:\n\n(a) Training completes around 450k iterations. Training is done on mirrored and randomly cropped 176x176 resolution images, with mini-batch size 40.\n\n(b) Snapshots every 1000 iterations will be saved in `./train/models/colornet_iter_[ITERNUMBER].caffemodel` and `./train/models/colornet_iter_[ITERNUMBER].solverstate`.\n\n(c) If training is interupted, resume training by running `./train/train_resume.sh ./train/models/colornet_iter_[ITERNUMBER].solverstate [GPU_ID]`, where `[ITERNUMBER]` is the last snapshotted model.\n\n(d) Check validation loss by running `./val_model.sh ./train/models/colornet_iter_[ITERNUMBER].caffemodel [GPU_ID] 1000`, where [ITERNUMBER] is the model you would like to validate. This runs the first 10k imagenet validation images at full 256x256 resolution through the model. Validation loss on `colorization_release_v2.caffemodel` is 7715.\n\n(e) Check model outputs by running the IPython notebook demo. Replace the release model with your snapshotted model.\n\n(f) To download reference pre-trained model, run `./models/fetch_release_models.sh`. This will load reference model `./models/colorization_release_v2.caffemodel`. This model used to generate results in the [ECCV 2016 camera ready](arxiv.org/pdf/1603.08511.pdf).\n\nFor completeness, this will also load model `./models/colorization_release_v2_norebal.caffemodel`, which is was trained without class rebalancing. This model will provide duller but \"safer\" colorizations. This will also load model `./models/colorization_release_v1.caffemodel`, which was used to generate the results in the [arXiv v1](arxiv.org/pdf/1603.08511v1.pdf) paper.\n\n### (3) Results / Real vs Fake Test ###\n\nTo run the \"real vs fake\" Amazon Mechanical Turk test (Table 1 of the paper), see [this repository]( https://github.com/phillipi/AMT_Real_vs_Fake). See line 1 of the **Usage** section. Corresponding paths are: [Ours (full)](http://colorization.eecs.berkeley.edu/imgs/classrebal_turk_imgs_438000/), [Ours (class, no rebal)](http://colorization.eecs.berkeley.edu/imgs/classnorebal_turk_imgs_446000/), [Ours (L2)](http://colorization.eecs.berkeley.edu/imgs/regress_turk_imgs_534000/), [Ours (L2, ft from class)](http://colorization.eecs.berkeley.edu/imgs/regressft_turk_imgs_312000/), [Ground Truth](http://colorization.eecs.berkeley.edu/imgs/gt_imgs_0/).\n\n## Representation Learning-centric Functionality ##\n\n### (4) Pre-trained Representation Learning models ###\n\n(1) Run `./models/fetch_alexnet_model.sh`. This will load the following model variants into `./models/`\n<!-- (i) `alexnet_release_450000.caffemodel` - resulting model after training -->\n<!-- (ii) `alexnet_release_450000_nobn.caffemodel` - batchnorms absorbed into conv layers -->\n(i) `alexnet_release_450000_nobn_rs.caffemodel` - fully convolutional model, used for ILSVRC 2012 linear readoff, PASCAL classification, and PASCAL segmentation tests\n<!-- model with rescaling from [Kraehenbuehl et al, ICLR 2016](https://github.com/philkr/magic_init) -->\n(ii) `alexnet_release_450000_nobn_fc_rs.caffemodel` - same as above, with `fc6,fc7` as `InnerProduct` layers instead of `Convolution` layers; used for PASCAL detection tests\n\n(2) You have two choices:\n\n(i) If you do the color conversion into Lab space outside of the network, use prototxt `./models/alexnet_deploy_lab.prototxt, ./models/alexnet_deploy_lab_fc.prototxt`. The input blob will be an image in Lab color space.\n\n(ii) If you wish to do the color conversion inside of the network, use prototxt `./models/alexnet_deploy.prototxt, ./models/alexnet_deploy_fc.prototxt`. The input should be BGR images, non-mean centered, in [0,255]. You will have to follow Caffe installation (described in step (2) in Section 2).\n\n### (5) Training AlexNet with Colorization ###\n\n(0) Link training and validation lmdbs to `./data/caffe-train-lmdb/` and `./data/caffe-val-lmdb/`, respectively.\n\n(1) Run `./train_alexnet/run_init.sh`. This runs the k-means initialization implemented in [Kraehenbuehl et al, ICLR 2016](https://github.com/philkr/magic_init) and will create model `./train_alexnet/mi.caffemodel`.\n\n(2) Run `./train_alexnet/train_model.sh [GPU_ID]`. Training takes 2 sec/iter = 10.5 days/450k iters on a Titan X PASCAL.\n\n(3) Run `./postprocess_model.sh [GPU_ID] 450000` (or an intermediate iteration number). This script will...\n(a) Absorb batch norm, save a model into `./train_alexnet/colornet_iter_450000_nobn.caffemodel`\n(b) Rescaling with [Kraehenbuehl et al, ICLR 2016](https://github.com/philkr/magic_init), save a model into `./train_alexnet/colornet_iter_450000_nobn_rs.caffemodel`\n(c) Save a model with `fc6,fc7` layers into `colornet_iter_450000_nobn_rs_fc.caffemodel`\n\n### (6) Representation Learning Tests ###\n\nThe provided scripts run representation learning tests. Note that the scripts run on release models. Modify scripts accordingly if you want to test your own trained model.\n\n(A) <b>ILSVRC Linear Classification</b> Run `./feature_learning_tests/linear/run_linear_tests.sh [GPU_ID]`. This will save results in `./feature_learning_tests/linear/test_acc_log`. Training takes roughly 0.25 sec/iter = 10 hrs/140k iters on a Titan X Pascal. Note that this test was slightly changed from the ECCV paper (instead of average pooling, we do bilinear resizing here). The updated test was used in our [Split-Brain Autoencoder paper](https://richzhang.github.io/splitbrainauto/). File `./feature_learning_tests/linear/test_acc_log/loss_log` shows results for layers `conv1, pool1, conv2, pool2, conv3, conv4, conv5, pool5`.\n\n(B) <b> PASCAL Classification </b>\n(a) Clone [VOC-Classification Repo](https://github.com/jeffdonahue/voc-classification). Set up directories as instructed.\n(b) Run `./feature_learning_tests/run_classification_test_[LAYER].sh [PATH_TO train_cls.py] [GPU_ID]`, where `[LAYER]` is `{fc7,pool5,conv1}`, depending on which layers you would like to fine-tune from. Results will be printed on console. The value of interest is the 10-crops on the test set. This will also generate directories `./feature_learning_tests/classification/[LAYER]`. Each test takes ~30-60 minutes on a Titan X Pascal.\n\n(C) <b> PASCAL Segmentation </b>\nThis code borrows from the [FCN repo](https://github.com/shelhamer/fcn.berkeleyvision.org). Prepare data as instructed. Then run `./run_segmentation.sh [GPU_ID]`. Results will be printed every 1000 iterations on screen. Training takes ~1 sec/iter = 44 hours/150k iterations on a Titan X Pascal.\n\n(D) <b> PASCAL Detection </b>\nWe use [Fast R-CNN](https://github.com/rbgirshick/fast-rcnn) with multi-scale training and single scale testing. Run for 150k total iterations, dropping the LR by a factor of 10 every 50k iterations. This was to follow the Kraehenbuehl et al. ICLR 2016 procedure. Set `__C.PIXEL_MEANS = np.array([[[0,0,0]]])`.\n\n## Related follow-up work ##\n\nInteractive Colorization: <b>Real-Time User-Guided Image Colorization with Learned Deep Priors.</b> R. Zhang*, J.Y. Zhu*, P. Isola, X. Geng, A. S. Lin, T. Yu, A. A. Efros. In CVPR, 2017. [Website](https://richzhang.github.io/ideepcolor/)[GitHub](https://github.com/junyanz/ideepcolor)\n\nRepresentation Learning: <b>Split-Brain Autoencoders: Unsupervised Prediction by Cross-Channel Prediction.</b> R. Zhang, P. Isola, A. A. Efros. In SIGGRAPH, 2017. [Website](https://richzhang.github.io/splitbrainauto/)[GitHub](https://github.com/richzhang/splitbrainauto)\n\n### Citation ###\nIf you find this model useful for your resesarch, please use this [bibtex](http://richzhang.github.io/colorization/resources/bibtex_eccv2016_colorization.txt) to cite.\n\n### Misc ###\nContact Richard Zhang at rich.zhang at eecs.berkeley.edu for any questions or comments.\n",
            "readme_url": "https://github.com/radiabouhali/test",
            "frameworks": [
                "Caffe",
                "scikit-learn"
            ]
        }
    ],
    "references": [
        {
            "title": "Colorful Image Colorization",
            "arxiv": "1603.08511",
            "year": 2016,
            "url": "http://arxiv.org/abs/1603.08511v5",
            "abstract": "Given a grayscale photograph as input, this paper attacks the problem of\nhallucinating a plausible color version of the photograph. This problem is\nclearly underconstrained, so previous approaches have either relied on\nsignificant user interaction or resulted in desaturated colorizations. We\npropose a fully automatic approach that produces vibrant and realistic\ncolorizations. We embrace the underlying uncertainty of the problem by posing\nit as a classification task and use class-rebalancing at training time to\nincrease the diversity of colors in the result. The system is implemented as a\nfeed-forward pass in a CNN at test time and is trained on over a million color\nimages. We evaluate our algorithm using a \"colorization Turing test,\" asking\nhuman participants to choose between a generated and ground truth color image.\nOur method successfully fools humans on 32% of the trials, significantly higher\nthan previous methods. Moreover, we show that colorization can be a powerful\npretext task for self-supervised feature learning, acting as a cross-channel\nencoder. This approach results in state-of-the-art performance on several\nfeature learning benchmarks.",
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A. Efros"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "Amazon"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9989169220551116,
        "task": "Object Detection",
        "task_prob": 0.35001092133144823
    }
}