{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Bottleneck Transformers in JAX/Flax",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "NZ99",
                "owner_type": "User",
                "name": "bottleneck-transformer-flax",
                "url": "https://github.com/NZ99/bottleneck-transformer-flax",
                "stars": 5,
                "pushed_at": "2021-04-06 19:57:05+00:00",
                "created_at": "2021-03-24 16:34:58+00:00",
                "language": "Python",
                "description": "A JAX/Flax implementation of Bottleneck Transformers for Visual Recognition",
                "license": "MIT License",
                "frameworks": []
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "fd3d1e4e0c18e7514d8178bff4d8915deba03605",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/NZ99/bottleneck-transformer-flax/tree/master/.github"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "62c893550adb53d3a8fc29a1584ff831cb829062",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/NZ99/bottleneck-transformer-flax/blob/master/.gitignore"
                    }
                },
                "size": 6
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "0085ccacaefc2741dfb5b263ba5208923bf1f7b1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/NZ99/bottleneck-transformer-flax/blob/master/LICENSE"
                    }
                },
                "size": 1075
            },
            {
                "type": "code",
                "name": "bottleneck_transformer_flax",
                "sha": "fc2ba61971b95fde97c8262516108afc7931148a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/NZ99/bottleneck-transformer-flax/tree/master/bottleneck_transformer_flax"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "57a68505e9b9bd3a8cc2f4119217148d74d276c4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/NZ99/bottleneck-transformer-flax/blob/master/setup.py"
                    }
                },
                "size": 820
            }
        ]
    },
    "tags": [],
    "description": "A JAX/Flax implementation of Bottleneck Transformers for Visual Recognition",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/NZ99/bottleneck-transformer-flax",
            "stars": 5,
            "issues": true,
            "readme": "## Bottleneck Transformers in JAX/Flax\n\nAn implementation of <a href=\"https://arxiv.org/abs/2101.11605\">Bottleneck Transformers for Visual Recognition</a>, a powerful hybrid architecture that combines a ResNet-like architecture with global relative position self-attention.\n\nThe code in this repository is limited to the image classification models and based on the <a href=\"https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2\">authors' official code</a>.\n\n## Install\n\n```bash\n$ pip install bottleneck-transformer-flax\n```\n\n## Usage\n\n```python\nfrom jax import random\nfrom jax import numpy as jnp\nfrom bottleneck_transformer_flax import BoTNet, BoTNetConfig\n\n#example configuration for BoTNet-S1-128\nconfig = BoTNetConfig(\n    stage_sizes = [3, 4, 23, 12],\n    num_classes = 1000\n)\n\nrng = random.PRNGKey(seed=0)\nmodel = BoTNet(config=config)\nparams = model.init(rng, jnp.ones((1, 256, 256, 3), dtype=config.dtype))\nimg = random.uniform(rng, (2, 256, 256, 3))\nlogits, updated_state = model.apply(params, img, mutable=['batch_stats']) # logits.shape is (2, 1000)\n```\n\n## Example Configurations\n\nA BoTNet configuration has the following arguments:\n\n```python\nclass BoTNetConfig:\n    stage_sizes: Sequence[int]                                          # Stages sizes (as in Table 13)\n    num_classes: int = 1000                                             # Number of classes\n    stride_one: bool = True                                             # Whether the model is a BoTNet-S1\n    se_ratio: float = 0.0625                                            # How much to squeeze\n    activation_fn: ModuleDef = nn.swish                                 # Activation function\n    num_heads: int = 4                                                  # Number of heads in multi head self attention\n    head_dim: int = 128                                                 # Head dimension in multi head self attention\n    initial_filters: int = 64                                           # Resnet stem output channels\n    projection_factor: int = 4                                          # Ratio between block output and input channels\n    bn_momentum: float = 0.9                                            # Batch normalization momentum\n    bn_epsilon: float = 1e-5                                            # Batch normalization epsilon\n    dtype: jnp.dtype = jnp.float32                                      # dtype of the computation\n    precision: Any = jax.lax.Precision.DEFAULT                          # Numerical precision of the computation\n    kernel_init: Callable = initializers.he_uniform()                   # Initializer function for the weight matrix\n    bias_init: Callable = initializers.normal(stddev=1e-6)              # Initializer function for the bias\n    posemb_init: Callable = initializers.normal(stddev=head_dim**-0.5)  # Initializer function for positional embeddings\n```\n\nProvided below are example configurations for all BoTNets.\n\n### BoTNet T3\n\n```python\nconfig = BoTNetConfig(\n    stage_sizes = [3, 4, 6, 6],\n    num_classes = 1000\n)\n```\n\n### BoTNet T4\n\n```python\nconfig = BoTNetConfig(\n    stage_sizes = [3, 4, 23, 6],\n    num_classes = 1000\n)\n```\n\n### BoTNet T5\n\n```python\nconfig = BoTNetConfig(\n    stage_sizes = [3, 4, 23, 12],\n    num_classes = 1000\n)\n```\n\n### BoTNet T6\n\n```python\nconfig = BoTNetConfig(\n    stage_sizes = [3, 4, 6, 12],\n    num_classes = 1000\n)\n```\n\n### BoTNet T7\n\n```python\nconfig = BoTNetConfig(\n    stage_sizes = [3, 4, 23, 12],\n    num_classes = 1000\n)\n```\n\n## Known issues\n\nIt's worth noting that the models as made available in this repository do not perfectly match the number of parameters as presented in the paper. The majority of the difference can however be explained by what I believe is an error in the script used by the authors to count the parameters: specifically, section 4.8.4 notes that Squeeze-and-Excite layers are only employed in ResNet bottleneck blocks, while the <a href=\"https://gist.github.com/aravindsrinivas/e8a9e33425e10ed0c69c1bf726b81495\">official script</a> does not correctly take this into account. Updating the script (specifically line 111) leads to an almost perfect match.\n\nThe number of parameters for each model is reported for clarity:\n\n| Model | This implementation | Updated script | Paper\n| :---: | :---: | :---: | :---: |\nT3 | 30.4M | 30.4M | 33.5M\nT4 | 51.6M | 51.5M | 54.7M\nT5 | 69.0M | 68.8M | 75.1M\nT6 | 47.8M | 47.7M | 53.9M\nT7 | 69.0M | 68.8M | 75.1M\n\nI am currently waiting for feedback from the authors about this issue and will update the repo as soon as possible.\n\n## Citation\n\n```bibtex\n@misc{srinivas2021bottleneck,\n    title   = {Bottleneck Transformers for Visual Recognition}, \n    author  = {Aravind Srinivas and Tsung-Yi Lin and Niki Parmar and Jonathon Shlens and Pieter Abbeel and Ashish Vaswani},\n    year    = {2021},\n    eprint  = {2101.11605},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```",
            "readme_url": "https://github.com/NZ99/bottleneck-transformer-flax",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Bottleneck Transformers for Visual Recognition",
            "arxiv": "2101.11605",
            "year": 2021,
            "url": "http://arxiv.org/abs/2101.11605v2",
            "abstract": "We present BoTNet, a conceptually simple yet powerful backbone architecture\nthat incorporates self-attention for multiple computer vision tasks including\nimage classification, object detection and instance segmentation. By just\nreplacing the spatial convolutions with global self-attention in the final\nthree bottleneck blocks of a ResNet and no other changes, our approach improves\nupon the baselines significantly on instance segmentation and object detection\nwhile also reducing the parameters, with minimal overhead in latency. Through\nthe design of BoTNet, we also point out how ResNet bottleneck blocks with\nself-attention can be viewed as Transformer blocks. Without any bells and\nwhistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance\nSegmentation benchmark using the Mask R-CNN framework; surpassing the previous\nbest published single model and single scale results of ResNeSt evaluated on\nthe COCO validation set. Finally, we present a simple adaptation of the BoTNet\ndesign for image classification, resulting in models that achieve a strong\nperformance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to\n1.64x faster in compute time than the popular EfficientNet models on TPU-v3\nhardware. We hope our simple and effective approach will serve as a strong\nbaseline for future research in self-attention models for vision",
            "authors": [
                "Aravind Srinivas",
                "Tsung-Yi Lin",
                "Niki Parmar",
                "Jonathon Shlens",
                "Pieter Abbeel",
                "Ashish Vaswani"
            ]
        },
        {
            "primaryclass": "cs.CV",
            "archiveprefix": "arXiv",
            "eprint": "2101.11605",
            "year": "2021",
            "author": [
                "Srinivas, Aravind",
                "Lin, Tsung-Yi",
                "Parmar, Niki",
                "Shlens, Jonathon",
                "Abbeel, Pieter",
                "Vaswani, Ashish"
            ],
            "title": "Bottleneck Transformers for Visual Recognition",
            "ENTRYTYPE": "misc",
            "ID": "srinivas2021bottleneck",
            "authors": [
                "Srinivas, Aravind",
                "Lin, Tsung-Yi",
                "Parmar, Niki",
                "Shlens, Jonathon",
                "Abbeel, Pieter",
                "Vaswani, Ashish"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9413804782934769,
        "task": "Object Detection",
        "task_prob": 0.5528045571912973
    },
    "training": {
        "datasets": [
            {
                "name": "COCO"
            },
            {
                "name": "ImageNet"
            }
        ]
    }
}