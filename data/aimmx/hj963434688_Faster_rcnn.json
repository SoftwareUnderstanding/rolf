{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "tf-faster-rcnn is deprecated:",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "hj963434688",
                "owner_type": "User",
                "name": "Faster_rcnn",
                "url": "https://github.com/hj963434688/Faster_rcnn",
                "stars": 0,
                "pushed_at": "2019-08-28 11:29:52+00:00",
                "created_at": "2019-08-28 11:19:52+00:00",
                "language": "Python",
                "license": "MIT License",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".DS_Store",
                "sha": "9b2517223fd1a4eb6c8f3efde823fbcccb80caf2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hj963434688/Faster_rcnn/blob/master/.DS_Store"
                    }
                },
                "size": 8196
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "7f81bbeb9761f0b1e5f6ab099bf1265d747c408d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hj963434688/Faster_rcnn/blob/master/.gitignore"
                    }
                },
                "size": 111
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "f68854dff19ea1acbfab17cbf886d62f89f818b1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hj963434688/Faster_rcnn/blob/master/LICENSE"
                    }
                },
                "size": 1068
            },
            {
                "type": "code",
                "name": "data",
                "sha": "a7ba5729fcb5ea4da39d8e29dada5dcc826a66c7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hj963434688/Faster_rcnn/tree/master/data"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "docker",
                "sha": "77e16534172ef74359efc47c210a3fb301ad4630",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hj963434688/Faster_rcnn/tree/master/docker"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "experiments",
                "sha": "3000ded6c56e018195ca57b74a80f6605a3ef702",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hj963434688/Faster_rcnn/tree/master/experiments"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "lib",
                "sha": "602994bf93da145b75e577be9ced5f90fd3a23da",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hj963434688/Faster_rcnn/tree/master/lib"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "tools",
                "sha": "1346229a1c14292e5521ed22b1d175eb48e7911b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hj963434688/Faster_rcnn/tree/master/tools"
                    }
                },
                "num_files": 6
            }
        ]
    },
    "authors": [
        {
            "name": "hj963434688",
            "github_id": "hj963434688"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/hj963434688/Faster_rcnn",
            "stars": 0,
            "issues": true,
            "readme": "# tf-faster-rcnn is deprecated:\nFor a good and more up-to-date implementation for faster/mask RCNN with multi-gpu support, please see the example in TensorPack [here](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN).\n\n# tf-faster-rcnn\nA Tensorflow implementation of faster RCNN detection framework by Xinlei Chen (xinleic@cs.cmu.edu). This repository is based on the python Caffe implementation of faster RCNN available [here](https://github.com/rbgirshick/py-faster-rcnn).\n\n**Note**: Several minor modifications are made when reimplementing the framework, which give potential improvements. For details about the modifications and ablative analysis, please refer to the technical report [An Implementation of Faster RCNN with Study for Region Sampling](https://arxiv.org/pdf/1702.02138.pdf). If you are seeking to reproduce the results in the original paper, please use the [official code](https://github.com/ShaoqingRen/faster_rcnn) or maybe the [semi-official code](https://github.com/rbgirshick/py-faster-rcnn). For details about the faster RCNN architecture please refer to the paper [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](http://arxiv.org/pdf/1506.01497.pdf).\n\n### Detection Performance\nThe current code supports **VGG16**, **Resnet V1** and **Mobilenet V1** models. We mainly tested it on plain VGG16 and Resnet101 (thank you @philokey!) architecture. As the baseline, we report numbers using a single model on a single convolution layer, so no multi-scale, no multi-stage bounding box regression, no skip-connection, no extra input is used. The only data augmentation technique is left-right flipping during training following the original Faster RCNN. All models are released.\n\nWith VGG16 (``conv5_3``):\n  - Train on VOC 2007 trainval and test on VOC 2007 test, **70.8**.\n  - Train on VOC 2007+2012 trainval and test on VOC 2007 test ([R-FCN](https://github.com/daijifeng001/R-FCN) schedule), **75.7**.\n  - Train on COCO 2014 [trainval35k](https://github.com/rbgirshick/py-faster-rcnn/tree/master/models) and test on [minival](https://github.com/rbgirshick/py-faster-rcnn/tree/master/models) (*Iterations*: 900k/1190k), **30.2**.\n\nWith Resnet101 (last ``conv4``):\n  - Train on VOC 2007 trainval and test on VOC 2007 test, **75.7**.\n  - Train on VOC 2007+2012 trainval and test on VOC 2007 test (R-FCN schedule), **79.8**.\n  - Train on COCO 2014 trainval35k and test on minival (900k/1190k), **35.4**.\n\nMore Results:\n  - Train Mobilenet (1.0, 224) on COCO 2014 trainval35k and test on minival (900k/1190k), **21.8**.\n  - Train Resnet50 on COCO 2014 trainval35k and test on minival (900k/1190k), **32.4**.\n  - Train Resnet152 on COCO 2014 trainval35k and test on minival (900k/1190k), **36.1**.\n\nApproximate *baseline* [setup](https://github.com/endernewton/tf-faster-rcnn/blob/master/experiments/cfgs/res101-lg.yml) from [FPN](https://arxiv.org/abs/1612.03144) (this repository does not contain training code for FPN yet):\n  - Train Resnet50 on COCO 2014 trainval35k and test on minival (900k/1190k), **34.2**.\n  - Train Resnet101 on COCO 2014 trainval35k and test on minival (900k/1190k), **37.4**.\n  - Train Resnet152 on COCO 2014 trainval35k and test on minival (900k/1190k), **38.2**.\n\n**Note**:\n  - Due to the randomness in GPU training with Tensorflow especially for VOC, the best numbers are reported (with 2-3 attempts) here. According to my experience, for COCO you can almost always get a very close number (within ~0.2%) despite the randomness.\n  - The numbers are obtained with the **default** testing scheme which selects region proposals using non-maximal suppression (TEST.MODE nms), the alternative testing scheme (TEST.MODE top) will likely result in slightly better performance (see [report](https://arxiv.org/pdf/1702.02138.pdf), for COCO it boosts 0.X AP).\n  - Since we keep the small proposals (\\< 16 pixels width/height), our performance is especially good for small objects.\n  - We do not set a threshold (instead of 0.05) for a detection to be included in the final result, which increases recall.\n  - Weight decay is set to 1e-4.\n  - For other minor modifications, please check the [report](https://arxiv.org/pdf/1702.02138.pdf). Notable ones include using ``crop_and_resize``, and excluding ground truth boxes in RoIs during training.\n  - For COCO, we find the performance improving with more iterations, and potentially better performance can be achieved with even more iterations.\n  - For Resnets, we fix the first block (total 4) when fine-tuning the network, and only use ``crop_and_resize`` to resize the RoIs (7x7) without max-pool (which I find useless especially for COCO). The final feature maps are average-pooled for classification and regression. All batch normalization parameters are fixed. Learning rate for biases is not doubled.\n  - For Mobilenets, we fix the first five layers when fine-tuning the network. All batch normalization parameters are fixed. Weight decay for Mobilenet layers is set to 4e-5.\n  - For approximate [FPN](https://arxiv.org/abs/1612.03144) baseline setup we simply resize the image with 800 pixels, add 32^2 anchors, and take 1000 proposals during testing.\n  - Check out [here](http://ladoga.graphics.cs.cmu.edu/xinleic/tf-faster-rcnn/)/[here](http://xinlei.sp.cs.cmu.edu/xinleic/tf-faster-rcnn/)/[here](https://drive.google.com/open?id=0B1_fAEgxdnvJSmF3YUlZcHFqWTQ) for the latest models, including longer COCO VGG16 models and Resnet ones.\n  \n![](data/imgs/gt.png)      |  ![](data/imgs/pred.png)\n:-------------------------:|:-------------------------:\nDisplayed Ground Truth on Tensorboard |  Displayed Predictions on Tensorboard\n\n### Additional features\nAdditional features not mentioned in the [report](https://arxiv.org/pdf/1702.02138.pdf) are added to make research life easier:\n  - **Support for train-and-validation**. During training, the validation data will also be tested from time to time to monitor the process and check potential overfitting. Ideally training and validation should be separate, where the model is loaded every time to test on validation. However I have implemented it in a joint way to save time and GPU memory. Though in the default setup the testing data is used for validation, no special attempts is made to overfit on testing set.\n  - **Support for resuming training**. I tried to store as much information as possible when snapshoting, with the purpose to resume training from the latest snapshot properly. The meta information includes current image index, permutation of images, and random state of numpy. However, when you resume training the random seed for tensorflow will be reset (not sure how to save the random state of tensorflow now), so it will result in a difference. **Note** that, the current implementation still cannot force the model to behave deterministically even with the random seeds set. Suggestion/solution is welcome and much appreciated.\n  - **Support for visualization**. The current implementation will summarize ground truth boxes, statistics of losses, activations and variables during training, and dump it to a separate folder for tensorboard visualization. The computing graph is also saved for debugging.\n\n### Prerequisites\n  - A basic Tensorflow installation. The code follows **r1.2** format. If you are using r1.0, please check out the r1.0 branch to fix the slim Resnet block issue. If you are using an older version (r0.1-r0.12), please check out the r0.12 branch. While it is not required, for experimenting the original RoI pooling (which requires modification of the C++ code in tensorflow), you can check out my tensorflow [fork](https://github.com/endernewton/tensorflow) and look for ``tf.image.roi_pooling``.\n  - Python packages you might not have: `cython`, `opencv-python`, `easydict` (similar to [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn)). For `easydict` make sure you have the right version. I use 1.6.\n  - Docker users: Since the recent upgrade, the docker image on docker hub (https://hub.docker.com/r/mbuckler/tf-faster-rcnn-deps/) is no longer valid. However, you can still build your own image by using dockerfile located at `docker` folder (cuda 8 version, as it is required by Tensorflow r1.0.) And make sure following Tensorflow installation to install and use nvidia-docker[https://github.com/NVIDIA/nvidia-docker]. Last, after launching the container, you have to build the Cython modules within the running container. \n\n### Installation\n1. Clone the repository\n  ```Shell\n  git clone https://github.com/endernewton/tf-faster-rcnn.git\n  ```\n\n2. Update your -arch in setup script to match your GPU\n  ```Shell\n  cd tf-faster-rcnn/lib\n  # Change the GPU architecture (-arch) if necessary\n  vim setup.py\n  ```\n\n  | GPU model  | Architecture |\n  | ------------- | ------------- |\n  | TitanX (Maxwell/Pascal) | sm_52 |\n  | GTX 960M | sm_50 |\n  | GTX 1080 (Ti) | sm_61 |\n  | Grid K520 (AWS g2.2xlarge) | sm_30 |\n  | Tesla K80 (AWS p2.xlarge) | sm_37 |\n\n  **Note**: You are welcome to contribute the settings on your end if you have made the code work properly on other GPUs. Also even if you are only using CPU tensorflow, GPU based code (for NMS) will be used by default, so please set **USE_GPU_NMS False** to get the correct output.\n\n\n3. Build the Cython modules\n  ```Shell\n  make clean\n  make\n  cd ..\n  ```\n\n4. Install the [Python COCO API](https://github.com/pdollar/coco). The code requires the API to access COCO dataset.\n  ```Shell\n  cd data\n  git clone https://github.com/pdollar/coco.git\n  cd coco/PythonAPI\n  make\n  cd ../../..\n  ```\n\n### Setup data\nPlease follow the instructions of py-faster-rcnn [here](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to setup VOC and COCO datasets (Part of COCO is done). The steps involve downloading data and optionally creating soft links in the ``data`` folder. Since faster RCNN does not rely on pre-computed proposals, it is safe to ignore the steps that setup proposals.\n\nIf you find it useful, the ``data/cache`` folder created on my side is also shared [here](http://ladoga.graphics.cs.cmu.edu/xinleic/tf-faster-rcnn/cache.tgz).\n\n### Demo and Test with pre-trained models\n1. Download pre-trained model\n  ```Shell\n  # Resnet101 for voc pre-trained on 07+12 set\n  ./data/scripts/fetch_faster_rcnn_models.sh\n  ```\n  **Note**: if you cannot download the models through the link, or you want to try more models, you can check out the following solutions and optionally update the downloading script:\n  - Another server [here](http://xinlei.sp.cs.cmu.edu/xinleic/tf-faster-rcnn/).\n  - Google drive [here](https://drive.google.com/open?id=0B1_fAEgxdnvJSmF3YUlZcHFqWTQ).\n\n2. Create a folder and a soft link to use the pre-trained model\n  ```Shell\n  NET=res101\n  TRAIN_IMDB=voc_2007_trainval+voc_2012_trainval\n  mkdir -p output/${NET}/${TRAIN_IMDB}\n  cd output/${NET}/${TRAIN_IMDB}\n  ln -s ../../../data/voc_2007_trainval+voc_2012_trainval ./default\n  cd ../../..\n  ```\n\n3. Demo for testing on custom images\n  ```Shell\n  # at repository root\n  GPU_ID=0\n  CUDA_VISIBLE_DEVICES=${GPU_ID} ./tools/demo.py\n  ```\n  **Note**: Resnet101 testing probably requires several gigabytes of memory, so if you encounter memory capacity issues, please install it with CPU support only. Refer to [Issue 25](https://github.com/endernewton/tf-faster-rcnn/issues/25).\n\n4. Test with pre-trained Resnet101 models\n  ```Shell\n  GPU_ID=0\n  ./experiments/scripts/test_faster_rcnn.sh $GPU_ID pascal_voc_0712 res101\n  ```\n  **Note**: If you cannot get the reported numbers (79.8 on my side), then probably the NMS function is compiled improperly, refer to [Issue 5](https://github.com/endernewton/tf-faster-rcnn/issues/5).\n\n### Train your own model\n1. Download pre-trained models and weights. The current code support VGG16 and Resnet V1 models. Pre-trained models are provided by slim, you can get the pre-trained models [here](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models) and set them in the ``data/imagenet_weights`` folder. For example for VGG16 model, you can set up like:\n   ```Shell\n   mkdir -p data/imagenet_weights\n   cd data/imagenet_weights\n   wget -v http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\n   tar -xzvf vgg_16_2016_08_28.tar.gz\n   mv vgg_16.ckpt vgg16.ckpt\n   cd ../..\n   ```\n   For Resnet101, you can set up like:\n   ```Shell\n   mkdir -p data/imagenet_weights\n   cd data/imagenet_weights\n   wget -v http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz\n   tar -xzvf resnet_v1_101_2016_08_28.tar.gz\n   mv resnet_v1_101.ckpt res101.ckpt\n   cd ../..\n   ```\n\n2. Train (and test, evaluation)\n  ```Shell\n  ./experiments/scripts/train_faster_rcnn.sh [GPU_ID] [DATASET] [NET]\n  # GPU_ID is the GPU you want to test on\n  # NET in {vgg16, res50, res101, res152} is the network arch to use\n  # DATASET {pascal_voc, pascal_voc_0712, coco} is defined in train_faster_rcnn.sh\n  # Examples:\n  ./experiments/scripts/train_faster_rcnn.sh 0 pascal_voc vgg16\n  ./experiments/scripts/train_faster_rcnn.sh 1 coco res101\n  ```\n  **Note**: Please double check you have deleted soft link to the pre-trained models before training. If you find NaNs during training, please refer to [Issue 86](https://github.com/endernewton/tf-faster-rcnn/issues/86). Also if you want to have multi-gpu support, check out [Issue 121](https://github.com/endernewton/tf-faster-rcnn/issues/121).\n\n3. Visualization with Tensorboard\n  ```Shell\n  tensorboard --logdir=tensorboard/vgg16/voc_2007_trainval/ --port=7001 &\n  tensorboard --logdir=tensorboard/vgg16/coco_2014_train+coco_2014_valminusminival/ --port=7002 &\n  ```\n\n4. Test and evaluate\n  ```Shell\n  ./experiments/scripts/test_faster_rcnn.sh [GPU_ID] [DATASET] [NET]\n  # GPU_ID is the GPU you want to test on\n  # NET in {vgg16, res50, res101, res152} is the network arch to use\n  # DATASET {pascal_voc, pascal_voc_0712, coco} is defined in test_faster_rcnn.sh\n  # Examples:\n  ./experiments/scripts/test_faster_rcnn.sh 0 pascal_voc vgg16\n  ./experiments/scripts/test_faster_rcnn.sh 1 coco res101\n  ```\n\n5. You can use ``tools/reval.sh`` for re-evaluation\n\n\nBy default, trained networks are saved under:\n\n```\noutput/[NET]/[DATASET]/default/\n```\n\nTest outputs are saved under:\n\n```\noutput/[NET]/[DATASET]/default/[SNAPSHOT]/\n```\n\nTensorboard information for train and validation is saved under:\n\n```\ntensorboard/[NET]/[DATASET]/default/\ntensorboard/[NET]/[DATASET]/default_val/\n```\n\nThe default number of training iterations is kept the same to the original faster RCNN for VOC 2007, however I find it is beneficial to train longer (see [report](https://arxiv.org/pdf/1702.02138.pdf) for COCO), probably due to the fact that the image batch size is one. For VOC 07+12 we switch to a 80k/110k schedule following [R-FCN](https://github.com/daijifeng001/R-FCN). Also note that due to the nondeterministic nature of the current implementation, the performance can vary a bit, but in general it should be within ~1% of the reported numbers for VOC, and ~0.2% of the reported numbers for COCO. Suggestions/Contributions are welcome.\n\n### Citation\nIf you find this implementation or the analysis conducted in our report helpful, please consider citing:\n\n    @article{chen17implementation,\n        Author = {Xinlei Chen and Abhinav Gupta},\n        Title = {An Implementation of Faster RCNN with Study for Region Sampling},\n        Journal = {arXiv preprint arXiv:1702.02138},\n        Year = {2017}\n    }\n    \nOr for a formal paper, [Spatial Memory Network](https://arxiv.org/abs/1704.04224):\n\n    @article{chen2017spatial,\n      title={Spatial Memory for Context Reasoning in Object Detection},\n      author={Chen, Xinlei and Gupta, Abhinav},\n      journal={arXiv preprint arXiv:1704.04224},\n      year={2017}\n    }\n\nFor convenience, here is the faster RCNN citation:\n\n    @inproceedings{renNIPS15fasterrcnn,\n        Author = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},\n        Title = {Faster {R-CNN}: Towards Real-Time Object Detection\n                 with Region Proposal Networks},\n        Booktitle = {Advances in Neural Information Processing Systems ({NIPS})},\n        Year = {2015}\n    }\n",
            "readme_url": "https://github.com/hj963434688/Faster_rcnn",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Spatial Memory for Context Reasoning in Object Detection",
            "arxiv": "1704.04224",
            "year": 2017,
            "url": "http://arxiv.org/abs/1704.04224v1",
            "abstract": "Modeling instance-level context and object-object relationships is extremely\nchallenging. It requires reasoning about bounding boxes of different classes,\nlocations \\etc. Above all, instance-level spatial reasoning inherently requires\nmodeling conditional distributions on previous detections. Unfortunately, our\ncurrent object detection systems do not have any {\\bf memory} to remember what\nto condition on! The state-of-the-art object detectors still detect all object\nin parallel followed by non-maximal suppression (NMS). While memory has been\nused for tasks such as captioning, they mostly use image-level memory cells\nwithout capturing the spatial layout. On the other hand, modeling object-object\nrelationships requires {\\bf spatial} reasoning -- not only do we need a memory\nto store the spatial layout, but also a effective reasoning module to extract\nspatial patterns. This paper presents a conceptually simple yet powerful\nsolution -- Spatial Memory Network (SMN), to model the instance-level context\nefficiently and effectively. Our spatial memory essentially assembles object\ninstances back into a pseudo \"image\" representation that is easy to be fed into\nanother ConvNet for object-object context reasoning. This leads to a new\nsequential reasoning architecture where image and memory are processed in\nparallel to obtain detections which update the memory again. We show our SMN\ndirection is promising as it provides 2.2\\% improvement over baseline Faster\nRCNN on the COCO dataset so far.",
            "authors": [
                "Xinlei Chen",
                "Abhinav Gupta"
            ]
        },
        {
            "title": "Feature Pyramid Networks for Object Detection",
            "arxiv": "1612.03144",
            "year": 2016,
            "url": "http://arxiv.org/abs/1612.03144v2",
            "abstract": "Feature pyramids are a basic component in recognition systems for detecting\nobjects at different scales. But recent deep learning object detectors have\navoided pyramid representations, in part because they are compute and memory\nintensive. In this paper, we exploit the inherent multi-scale, pyramidal\nhierarchy of deep convolutional networks to construct feature pyramids with\nmarginal extra cost. A top-down architecture with lateral connections is\ndeveloped for building high-level semantic feature maps at all scales. This\narchitecture, called a Feature Pyramid Network (FPN), shows significant\nimprovement as a generic feature extractor in several applications. Using FPN\nin a basic Faster R-CNN system, our method achieves state-of-the-art\nsingle-model results on the COCO detection benchmark without bells and\nwhistles, surpassing all existing single-model entries including those from the\nCOCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU\nand thus is a practical and accurate solution to multi-scale object detection.\nCode will be made publicly available.",
            "authors": [
                "Tsung-Yi Lin",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Kaiming He",
                "Bharath Hariharan",
                "Serge Belongie"
            ]
        },
        {
            "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
            "arxiv": "1506.01497",
            "year": 2015,
            "url": "http://arxiv.org/abs/1506.01497v3",
            "abstract": "State-of-the-art object detection networks depend on region proposal\nalgorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN\nhave reduced the running time of these detection networks, exposing region\nproposal computation as a bottleneck. In this work, we introduce a Region\nProposal Network (RPN) that shares full-image convolutional features with the\ndetection network, thus enabling nearly cost-free region proposals. An RPN is a\nfully convolutional network that simultaneously predicts object bounds and\nobjectness scores at each position. The RPN is trained end-to-end to generate\nhigh-quality region proposals, which are used by Fast R-CNN for detection. We\nfurther merge RPN and Fast R-CNN into a single network by sharing their\nconvolutional features---using the recently popular terminology of neural\nnetworks with 'attention' mechanisms, the RPN component tells the unified\nnetwork where to look. For the very deep VGG-16 model, our detection system has\na frame rate of 5fps (including all steps) on a GPU, while achieving\nstate-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS\nCOCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015\ncompetitions, Faster R-CNN and RPN are the foundations of the 1st-place winning\nentries in several tracks. Code has been made publicly available.",
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ]
        },
        {
            "title": "An Implementation of Faster RCNN with Study for Region Sampling",
            "arxiv": "1702.02138",
            "year": 2017,
            "url": "http://arxiv.org/abs/1702.02138v2",
            "abstract": "We adapted the join-training scheme of Faster RCNN framework from Caffe to\nTensorFlow as a baseline implementation for object detection. Our code is made\npublicly available. This report documents the simplifications made to the\noriginal pipeline, with justifications from ablation analysis on both PASCAL\nVOC 2007 and COCO 2014. We further investigated the role of non-maximal\nsuppression (NMS) in selecting regions-of-interest (RoIs) for region\nclassification, and found that a biased sampling toward small regions helps\nperformance and can achieve on-par mAP to NMS-based sampling when converged\nsufficiently.",
            "authors": [
                "Xinlei Chen",
                "Abhinav Gupta"
            ]
        },
        {
            "title": "setup",
            "url": "https://github.com/endernewton/tf-faster-rcnn/blob/master/experiments/cfgs/res101-lg.yml"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "COCO"
            },
            {
                "name": "COCO 2014"
            },
            {
                "name": "PASCAL VOC 2007"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999995780084923,
        "task": "Object Detection",
        "task_prob": 0.9938547688657796
    }
}