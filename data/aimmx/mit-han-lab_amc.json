{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "AutoML for Model Compression (AMC)",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "mit-han-lab",
                "owner_type": "Organization",
                "name": "amc",
                "url": "https://github.com/mit-han-lab/amc",
                "stars": 351,
                "pushed_at": "2021-02-20 02:28:18+00:00",
                "created_at": "2019-06-15 14:50:43+00:00",
                "language": "Python",
                "description": "[ECCV 2018] AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
                "license": "MIT License",
                "frameworks": [
                    "scikit-learn",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "8d5cc049cf16d6019ec9a2fd2d7d41797b1b3f3b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mit-han-lab/amc/blob/master/LICENSE"
                    }
                },
                "size": 1068
            },
            {
                "type": "code",
                "name": "amc_fine_tune.py",
                "sha": "19348227e4edb4f54adfbb1a9f62f7ca4d963951",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mit-han-lab/amc/blob/master/amc_fine_tune.py"
                    }
                },
                "size": 8384
            },
            {
                "type": "code",
                "name": "amc_search.py",
                "sha": "c252bfe22e6817d0b77cc0f9f4e7962da8c95d7a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mit-han-lab/amc/blob/master/amc_search.py"
                    }
                },
                "size": 12055
            },
            {
                "type": "code",
                "name": "checkpoints",
                "sha": "3d3095540cc17ed9493f63e13f67baa0265b6ed9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mit-han-lab/amc/tree/master/checkpoints"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "env",
                "sha": "4041e4dc361118e8237690e2a5ea58accf7f38d7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mit-han-lab/amc/tree/master/env"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "eval_mobilenet.py",
                "sha": "325ccdc1a4f44f2724c829e92b235f8e0c1b5ed3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mit-han-lab/amc/blob/master/eval_mobilenet.py"
                    }
                },
                "size": 3776
            },
            {
                "type": "code",
                "name": "lib",
                "sha": "7180332c5b32e2a7a155b232c72d6319dc1bd1b4",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mit-han-lab/amc/tree/master/lib"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "models",
                "sha": "b584512a9e6a1e1429b54f7a33b25a266e3dafe2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mit-han-lab/amc/tree/master/models"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "scripts",
                "sha": "65c1e1df12d9d667a0b50e5954b51e406294555e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mit-han-lab/amc/tree/master/scripts"
                    }
                },
                "num_files": 3
            }
        ]
    },
    "authors": [
        {
            "name": "Hanrui Wang",
            "email": "hanrui@mit.edu",
            "github_id": "Hanrui-Wang"
        },
        {
            "name": "Ji Lin",
            "email": "jilin@mit.edu",
            "github_id": "tonylins"
        },
        {
            "name": "Zhijian Liu",
            "github_id": "zhijian-liu"
        }
    ],
    "tags": [
        "automl",
        "automl-for-compression",
        "model-compression",
        "channel-pruning",
        "efficient-model",
        "on-device-ai"
    ],
    "description": "[ECCV 2018] AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/mit-han-lab/amc",
            "stars": 351,
            "issues": true,
            "readme": "# AutoML for Model Compression (AMC)\n\nThis repo contains the PyTorch implementation for paper [**AMC: AutoML for Model Compression and Acceleration on Mobile Devices**](https://arxiv.org/abs/1802.03494). \n\n![overview](https://hanlab.mit.edu/projects/amc/images/overview.png)\n\n\n\n## Reference\n\nIf you find the repo useful, please kindly cite our paper:\n\n```\n@inproceedings{he2018amc,\n  title={AMC: AutoML for Model Compression and Acceleration on Mobile Devices},\n  author={He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},\n  booktitle={European Conference on Computer Vision (ECCV)},\n  year={2018}\n}\n```\n\nOther papers related to automated model design:\n\n- HAQ: Hardware-Aware Automated Quantization with Mixed Precision ([CVPR 2019](https://arxiv.org/abs/1811.08886))\n\n- ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware ([ICLR 2019](https://arxiv.org/abs/1812.00332))\n\n\n\n## Training AMC\n\nCurrent code base supports the automated pruning of **MobileNet** on **ImageNet**. The pruning of MobileNet consists of 3 steps: **1. strategy search; 2. export the pruned weights; 3. fine-tune from pruned weights**.\n\nTo conduct the full pruning procedure, follow the instructions below (results might vary a little from the paper due to different random seed):\n\n1. **Strategy Search**\n\n   To search the strategy on MobileNet ImageNet model, first get the pretrained MobileNet checkpoint on ImageNet by running:\n\n   ```\n   bash ./checkpoints/download.sh\n   ```\n\n   It will also download our 50% FLOPs compressed model. Then run the following script to search under 50% FLOPs constraint:\n\n   ```bash\n   bash ./scripts/search_mobilenet_0.5flops.sh\n   ```\n\n   Results may differ due to different random seed. The strategy we found and reported in the paper is:\n\n   ```\n   [3, 24, 48, 96, 80, 192, 200, 328, 352, 368, 360, 328, 400, 736, 752]\n   ```\n\n2. **Export the Pruned Weights**\n\n   After searching, we need to export the pruned weights by running:\n\n   ```\n   bash ./scripts/export_mobilenet_0.5flops.sh\n   ```\n\n   Also we need to modify MobileNet file to support the new pruned model (here it is already done in `models/mobilenet.py`)\n\n3. **Fine-tune from Pruned Weights**a\n\n   After exporting, we need to fine-tune from the pruned weights. For example, we can fine-tune using cosine learning rate for 150 epochs by running:\n\n   ```\n   bash ./scripts/finetune_mobilenet_0.5flops.sh\n   ```\n\n\n\n## AMC Compressed Model\n\nWe also provide the models and weights compressed by our AMC method. We provide compressed MobileNet-V1 and MobileNet-V2 in both PyTorch and TensorFlow format [here](https://github.com/mit-han-lab/amc-compressed-models). \n\nDetailed statistics are as follows:\n\n| Models                   | Top1 Acc (%) | Top5 Acc (%) |\n| ------------------------ | ------------ | ------------ |\n| MobileNetV1-width*0.75   | 68.4         | 88.2         |\n| **MobileNetV1-50%FLOPs** | **70.494**   | **89.306**   |\n| **MobileNetV1-50%Time**  | **70.200**   | **89.430**   |\n| MobileNetV2-width*0.75   | 69.8         | 89.6         |\n| **MobileNetV2-70%FLOPs** | **70.854**   | **89.914**   |\n\n\n\n## Dependencies\n\nCurrent code base is tested under following environment:\n\n1. Python 3.7.3\n2. PyTorch 1.1.0\n3. torchvision 0.2.1\n4. NumPy 1.14.3\n5. SciPy 1.1.0\n6. scikit-learn 0.19.1\n7. [tensorboardX](https://github.com/lanpa/tensorboardX)\n8. ImageNet dataset\n\n\n\n## Contact\n\nTo contact the authors:\n\nJi Lin, jilin@mit.edu\n\nSong Han, songhan@mit.edu\n",
            "readme_url": "https://github.com/mit-han-lab/amc",
            "frameworks": [
                "scikit-learn",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
            "arxiv": "1811.08886",
            "year": 2018,
            "url": "http://arxiv.org/abs/1811.08886v3",
            "abstract": "Model quantization is a widely used technique to compress and accelerate deep\nneural network (DNN) inference. Emergent DNN hardware accelerators begin to\nsupport mixed precision (1-8 bits) to further improve the computation\nefficiency, which raises a great challenge to find the optimal bitwidth for\neach layer: it requires domain experts to explore the vast design space trading\noff among accuracy, latency, energy, and model size, which is both\ntime-consuming and sub-optimal. Conventional quantization algorithm ignores the\ndifferent hardware architectures and quantizes all the layers in a uniform way.\nIn this paper, we introduce the Hardware-Aware Automated Quantization (HAQ)\nframework which leverages the reinforcement learning to automatically determine\nthe quantization policy, and we take the hardware accelerator's feedback in the\ndesign loop. Rather than relying on proxy signals such as FLOPs and model size,\nwe employ a hardware simulator to generate direct feedback signals (latency and\nenergy) to the RL agent. Compared with conventional methods, our framework is\nfully automated and can specialize the quantization policy for different neural\nnetwork architectures and hardware architectures. Our framework effectively\nreduced the latency by 1.4-1.95x and the energy consumption by 1.9x with\nnegligible loss of accuracy compared with the fixed bitwidth (8 bits)\nquantization. Our framework reveals that the optimal policies on different\nhardware architectures (i.e., edge and cloud architectures) under different\nresource constraints (i.e., latency, energy and model size) are drastically\ndifferent. We interpreted the implication of different quantization policies,\nwhich offer insights for both neural network architecture design and hardware\narchitecture design.",
            "authors": [
                "Kuan Wang",
                "Zhijian Liu",
                "Yujun Lin",
                "Ji Lin",
                "Song Han"
            ]
        },
        {
            "title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
            "arxiv": "1812.00332",
            "year": 2018,
            "url": "http://arxiv.org/abs/1812.00332v2",
            "abstract": "Neural architecture search (NAS) has a great impact by automatically\ndesigning effective neural network architectures. However, the prohibitive\ncomputational demand of conventional NAS algorithms (e.g. $10^4$ GPU hours)\nmakes it difficult to \\emph{directly} search the architectures on large-scale\ntasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via\na continuous representation of network architecture but suffers from the high\nGPU memory consumption issue (grow linearly w.r.t. candidate set size). As a\nresult, they need to utilize~\\emph{proxy} tasks, such as training on a smaller\ndataset, or learning with only a few blocks, or training just for a few epochs.\nThese architectures optimized on proxy tasks are not guaranteed to be optimal\non the target task. In this paper, we present \\emph{ProxylessNAS} that can\n\\emph{directly} learn the architectures for large-scale target tasks and target\nhardware platforms. We address the high memory consumption issue of\ndifferentiable NAS and reduce the computational cost (GPU hours and GPU memory)\nto the same level of regular training while still allowing a large candidate\nset. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of\ndirectness and specialization. On CIFAR-10, our model achieves 2.08\\% test\nerror with only 5.7M parameters, better than the previous state-of-the-art\narchitecture AmoebaNet-B, while using 6$\\times$ fewer parameters. On ImageNet,\nour model achieves 3.1\\% better top-1 accuracy than MobileNetV2, while being\n1.2$\\times$ faster with measured GPU latency. We also apply ProxylessNAS to\nspecialize neural architectures for hardware with direct hardware metrics (e.g.\nlatency) and provide insights for efficient CNN architecture design.",
            "authors": [
                "Han Cai",
                "Ligeng Zhu",
                "Song Han"
            ]
        },
        {
            "title": "AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
            "arxiv": "1802.03494",
            "year": 2018,
            "url": "http://arxiv.org/abs/1802.03494v4",
            "abstract": "Model compression is a critical technique to efficiently deploy neural\nnetwork models on mobile devices which have limited computation resources and\ntight power budgets. Conventional model compression techniques rely on\nhand-crafted heuristics and rule-based policies that require domain experts to\nexplore the large design space trading off among model size, speed, and\naccuracy, which is usually sub-optimal and time-consuming. In this paper, we\npropose AutoML for Model Compression (AMC) which leverage reinforcement\nlearning to provide the model compression policy. This learning-based\ncompression policy outperforms conventional rule-based compression policy by\nhaving higher compression ratio, better preserving the accuracy and freeing\nhuman labor. Under 4x FLOPs reduction, we achieved 2.7% better accuracy than\nthe handcrafted model compression policy for VGG-16 on ImageNet. We applied\nthis automated, push-the-button compression pipeline to MobileNet and achieved\n1.81x speedup of measured inference latency on an Android phone and 1.43x\nspeedup on the Titan XP GPU, with only 0.1% loss of ImageNet Top-1 accuracy.",
            "authors": [
                "Yihui He",
                "Ji Lin",
                "Zhijian Liu",
                "Hanrui Wang",
                "Li-Jia Li",
                "Song Han"
            ]
        },
        {
            "year": "2018",
            "booktitle": "European Conference on Computer Vision (ECCV)",
            "author": [
                "He, Yihui",
                "Lin, Ji",
                "Liu, Zhijian",
                "Wang, Hanrui",
                "Li, Li-Jia",
                "Han, Song"
            ],
            "title": "AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
            "ENTRYTYPE": "inproceedings",
            "ID": "he2018amc",
            "authors": [
                "He, Yihui",
                "Lin, Ji",
                "Liu, Zhijian",
                "Wang, Hanrui",
                "Li, Li-Jia",
                "Han, Song"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "CIFAR-10"
            }
        ]
    },
    "domain": {
        "domain_type": "Unknown"
    }
}