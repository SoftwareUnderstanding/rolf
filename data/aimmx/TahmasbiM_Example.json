{
    "visibility": {
        "visibility": "public"
    },
    "name": "Keras-SRGAN",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "TahmasbiM",
                "owner_type": "User",
                "name": "Example",
                "url": "https://github.com/TahmasbiM/Example",
                "stars": 0,
                "pushed_at": "2019-11-03 16:34:19+00:00",
                "created_at": "2019-11-03 16:30:51+00:00",
                "language": "Python",
                "description": "An Example",
                "frameworks": [
                    "Keras",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "894a44cc066a027465cd26d634948d56d13af9af",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/TahmasbiM/Example/blob/master/.gitignore"
                    }
                },
                "size": 1203
            },
            {
                "type": "code",
                "name": "Network.py",
                "sha": "7689c651a8cf72d9e82be16ebdf5ab75a9060ebc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/TahmasbiM/Example/blob/master/Network.py"
                    }
                },
                "size": 4614
            },
            {
                "type": "code",
                "name": "Utils.py",
                "sha": "5b875f16ce16ec86efd16691debaf719d971d8fc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/TahmasbiM/Example/blob/master/Utils.py"
                    }
                },
                "size": 7439
            },
            {
                "type": "code",
                "name": "Utils_model.py",
                "sha": "63f17fde61383f330747ac5ccb1e23a320ce930a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/TahmasbiM/Example/blob/master/Utils_model.py"
                    }
                },
                "size": 1092
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "96b81d78daa6965973aae0a44774876db09d99c3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/TahmasbiM/Example/blob/master/test.py"
                    }
                },
                "size": 2767
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "6371b97aa700db739f757d92976d0766ca3e0f83",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/TahmasbiM/Example/blob/master/train.py"
                    }
                },
                "size": 5501
            }
        ]
    },
    "authors": [
        {
            "name": "TahmasbiM",
            "github_id": "TahmasbiM"
        }
    ],
    "tags": [],
    "description": "An Example",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/TahmasbiM/Example",
            "stars": 0,
            "issues": true,
            "readme": "# Keras-SRGAN\nPhoto-Realistic Single Image Super-Resolution Using a Generative Adversarial Network implemented in Keras\n\nFor more about topic check [Single Image Super Resolution Using GANs\u200a\u2014\u200aKeras](https://medium.com/@birla.deepak26/single-image-super-resolution-using-gans-keras-aca310f33112)\n\n## Problem Statement:\n    Enhancing low resolution images by applying deep network with adversarial network (Generative Adversarial Networks) \n    to produce high resolutions images.\n    \n## Architecture:\n    \n![Basic Architecture](./Architecture_images/architecture.jpg)\n    \n## Generator and Discriminator Network:\n    \n![Network](./Architecture_images/network.jpg)\n    \n## Network Details:\n    * 16 Residual blocks used.\n    * PixelShuffler x2: This is feature map upscaling. 2 sub-pixel CNN are used in Generator.\n    * PRelu(Parameterized Relu): We are using PRelu in place of Relu or LeakyRelu. It introduces learn-able parameter \n      that makes it possible to adaptively learn the negative part coefficient.\n    * k3n64s1 this means kernel 3, channels 64 and strides 1.\n    * Loss Function: We are using Perceptual loss. It comprises of Content(Reconstruction) loss and Adversarial loss.\n    \n## How it Works:\n    * We process the HR(High Resolution) images to get down-sampled LR(Low Resolution) images. Now we have both HR \n      and LR images for training data set.\n    * We pass LR images through Generator which up-samples and gives SR(Super Resolution) images.\n    * We use a discriminator to distinguish the HR images and back-propagate the GAN loss to train the discriminator\n      and the generator.\n    * As a result of this, the generator learns to produce more and more realistic images(High Resolution images) as \n      it trains.\n    \n## Documentation:\nYou can find more about this implementation in my post : [Single Image Super Resolution Using GANs\u200a\u2014\u200aKeras](https://medium.com/@birla.deepak26/single-image-super-resolution-using-gans-keras-aca310f33112)\n    \n\n## Requirements:\n\n    You will need the following to run the above:\n    Python 3.5.4\n    tensorflow 1.11.0\n    keras 2.2.4\n    numpy 1.10.4\n    matplotlib, skimage, scipy\n    \n    For training: Good GPU, I trained my model on NVIDIA Tesla P100\n    \n## Data set:\n\n    * Used COCO data set 2017. It is around 18GB having images of different dimensions.\n    * Used 800 images for training(Very less, You can take more (approx. 350 according to original paper) thousand is you can\n      collect and have very very good GPU). Preprocessing includes cropping images so that we can have same dimension images. \n      Images with same width and height are preferred. I used images of size 384 for high resolution.\n    * After above step you have High Resolution images. Now you have to get Low Resolution images which you can get by down \n      scaling HR images. I used down scale = 4. So Low resolution image of size 96 we will get. Sample code for this.\n      \n## File Structure:\n\n    Network.py : Contains Generator and Discriminator Network\n    Utils.py   : Contains utilities to process images\n    Utils_model.py : Contains optimizer and content loss code\n    train.py   : Used for training the model\n    test.py    : To test the model\n    Simplified : This folder contains code without Agrparse etc. If you hate commandline arguements just dive in here.\n                 There are just two files. Modify according to requirement and start training.\n      \n## Usage:\n    \n    Note : Image shape and downscale factor you can set in train.py file.Set according to requirement.\n    \n     * Training:\n        Run below command to train model. Set parameters accordingly.\n        > python train.py --input_dir='./data/' --output_dir='./output/' --model_save_dir='./model/' --batch_size=64 --epochs=3000 --number_of_images=1000 --train_test_ratio=0.8\n        \n        All Parameters have default values. For mode help on parameters run:\n        > python train.py -h\n        \n     * Testing:\n        test.py file contains code to test. Testing can be done in two ways using option test_type:\n            1. Test Model- Here you can test the model by providing HR images. It will process to get resulting LR images and then will generate SR images.\n               And then will save output file comprising of all LR, SR and HR images.\n               Run following command to test model:\n               > python test.py --input_high_res='./data_hr/' --output_dir='./output/' --model_dir='./model/gen_model3000.h5' --number_of_images=25 --test_type='test_model'\n               For more help run:\n               > python test.py -h\n               \n            2. Test LR images- This option directly take LR images and give resulting HR images.\n               Run following command to get HR images from LR images:\n               > python test.py --input_low_res='./data_lr/' --output_dir='./output/' --model_dir='./model/gen_model3000.h5' --number_of_images=25 --test_type='test_lr_images'\n               For more help run:\n               > python test.py -h\n          \n     If you hate commandline arguements please reffer Simplified folder. Modify parameters in file like image_shape, input folder\n     etc. according to your need and start training.\n               \n## Things's Learned:\n\n    * GAN's sometimes are hard to train. Network can be very deep sometimes, but use of residual blocks make it easier.\n    * Once you get to learn about Perceptual loss things get easier. Same Perceptual loss can be usefull for Image Style Transfer and Photo Realistic Style Transfer.\n    * This is one of the problem where i struggled to get data. You need to be carefull while choosing data and also preprossing is little bit tough.\n    * Better to use images with same width and height.\n    * Use GPU for training else it will take months to train(even you can run out of memory).\n    \n## Output:\n\nBelow are few results-\n![Output 1](./output/gan_generated_image_epoch_1110.png)\n![Output 2](./output/gan_generated_image_epoch_2580.png)\n![Output 2](./output/gan_generated_image_epoch_770.png)\n    \nMore results are in output folder\n\n## Refrences:\n\n    Paper:\n    Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network: https://arxiv.org/pdf/1609.04802.pdf\n    Perceptual Losses for Real-Time Style Transfer and Super-Resolution: https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf\n    \n    Projects doing the same thing:\n    https://github.com/MathiasGruber/SRGAN-Keras\n    https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks\n    https://github.com/eriklindernoren/Keras-GAN/tree/master/srgan\n    https://github.com/brade31919/SRGAN-tensorflow\n    https://github.com/tensorlayer/srgan\n     \n    Help on GANS:\n    https://github.com/eriklindernoren/Keras-GAN (Various GANS implemented in Keras)\n    https://github.com/JGuillaumin/SuperResGAN-keras\n    https://oshearesearch.com/index.php/2016/07/01/mnist-generative-adversarial-model-in-keras/\n    \n    VGG loss help:\n    https://blog.sicara.com/keras-generative-adversarial-networks-image-deblurring-45e3ab6977b5\n    \n    SubpixelConv2D(Deconvolution) help:\n    Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network: https://arxiv.org/abs/1609.05158\n    https://github.com/twairball/keras-subpixel-conv\n    \n    Improved Techniques for Training GANs:\n    https://arxiv.org/abs/1606.03498\n    \n\n\n               \n",
            "readme_url": "https://github.com/TahmasbiM/Example",
            "frameworks": [
                "Keras",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Improved Techniques for Training GANs",
            "arxiv": "1606.03498",
            "year": 2016,
            "url": "http://arxiv.org/abs/1606.03498v1",
            "abstract": "We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes.",
            "authors": [
                "Tim Salimans",
                "Ian Goodfellow",
                "Wojciech Zaremba",
                "Vicki Cheung",
                "Alec Radford",
                "Xi Chen"
            ]
        },
        {
            "title": "Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network",
            "arxiv": "1609.05158",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.05158v2",
            "abstract": "Recently, several models based on deep neural networks have achieved great\nsuccess in terms of both reconstruction accuracy and computational performance\nfor single image super-resolution. In these methods, the low resolution (LR)\ninput image is upscaled to the high resolution (HR) space using a single\nfilter, commonly bicubic interpolation, before reconstruction. This means that\nthe super-resolution (SR) operation is performed in HR space. We demonstrate\nthat this is sub-optimal and adds computational complexity. In this paper, we\npresent the first convolutional neural network (CNN) capable of real-time SR of\n1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN\narchitecture where the feature maps are extracted in the LR space. In addition,\nwe introduce an efficient sub-pixel convolution layer which learns an array of\nupscaling filters to upscale the final LR feature maps into the HR output. By\ndoing so, we effectively replace the handcrafted bicubic filter in the SR\npipeline with more complex upscaling filters specifically trained for each\nfeature map, whilst also reducing the computational complexity of the overall\nSR operation. We evaluate the proposed approach using images and videos from\npublicly available datasets and show that it performs significantly better\n(+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster\nthan previous CNN-based methods.",
            "authors": [
                "Wenzhe Shi",
                "Jose Caballero",
                "Ferenc Husz\u00e1r",
                "Johannes Totz",
                "Andrew P. Aitken",
                "Rob Bishop",
                "Daniel Rueckert",
                "Zehan Wang"
            ]
        },
        {
            "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
            "arxiv": "1609.04802",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.04802v5",
            "abstract": "Despite the breakthroughs in accuracy and speed of single image\nsuper-resolution using faster and deeper convolutional neural networks, one\ncentral problem remains largely unsolved: how do we recover the finer texture\ndetails when we super-resolve at large upscaling factors? The behavior of\noptimization-based super-resolution methods is principally driven by the choice\nof the objective function. Recent work has largely focused on minimizing the\nmean squared reconstruction error. The resulting estimates have high peak\nsignal-to-noise ratios, but they are often lacking high-frequency details and\nare perceptually unsatisfying in the sense that they fail to match the fidelity\nexpected at the higher resolution. In this paper, we present SRGAN, a\ngenerative adversarial network (GAN) for image super-resolution (SR). To our\nknowledge, it is the first framework capable of inferring photo-realistic\nnatural images for 4x upscaling factors. To achieve this, we propose a\nperceptual loss function which consists of an adversarial loss and a content\nloss. The adversarial loss pushes our solution to the natural image manifold\nusing a discriminator network that is trained to differentiate between the\nsuper-resolved images and original photo-realistic images. In addition, we use\na content loss motivated by perceptual similarity instead of similarity in\npixel space. Our deep residual network is able to recover photo-realistic\ntextures from heavily downsampled images on public benchmarks. An extensive\nmean-opinion-score (MOS) test shows hugely significant gains in perceptual\nquality using SRGAN. The MOS scores obtained with SRGAN are closer to those of\nthe original high-resolution images than to those obtained with any\nstate-of-the-art method.",
            "authors": [
                "Christian Ledig",
                "Lucas Theis",
                "Ferenc Huszar",
                "Jose Caballero",
                "Andrew Cunningham",
                "Alejandro Acosta",
                "Andrew Aitken",
                "Alykhan Tejani",
                "Johannes Totz",
                "Zehan Wang",
                "Wenzhe Shi"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "MNIST"
            },
            {
                "name": "COCO"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ImageNet"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.99999987572622,
        "task": "Image Generation",
        "task_prob": 0.9821765266465177
    }
}