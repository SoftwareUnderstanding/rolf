{
    "visibility": {
        "visibility": "public"
    },
    "name": "\u57fa\u4e8ePaddlePaddle\u6846\u67b6\u7684YOLOv1\u590d\u73b0",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "sunlizhuang",
                "owner_type": "User",
                "name": "YOLOv1-PaddlePaddle",
                "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle",
                "stars": 2,
                "pushed_at": "2021-11-05 08:14:19+00:00",
                "created_at": "2021-08-04 15:10:26+00:00",
                "language": "Python",
                "description": "\u57fa\u4e8ePaddlePaddle\u6846\u67b6\u7684YOLOv1\u590d\u73b0",
                "frameworks": []
            },
            {
                "type": "code",
                "name": ".DS_Store",
                "sha": "0df8ce336adc9cd3c05401fbed330e8bcf42c9a2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/blob/main/.DS_Store"
                    }
                },
                "size": 10244
            },
            {
                "type": "code",
                "name": "1.png",
                "sha": "366446e67b6f64341f39fe17554c2220a9c92890",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/blob/main/1.png"
                    }
                },
                "size": 310701
            },
            {
                "type": "code",
                "name": "2.png",
                "sha": "83daa0564f5ae5d8174a419d5f28534e729098af",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/blob/main/2.png"
                    }
                },
                "size": 285398
            },
            {
                "type": "code",
                "name": "3.png",
                "sha": "7d6be6cedb38ce3137718c47bbb7e14e9233f59f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/blob/main/3.png"
                    }
                },
                "size": 674236
            },
            {
                "type": "code",
                "name": "4.png",
                "sha": "74b5b0a295556257a964a54e9c639f1e76a6fcee",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/blob/main/4.png"
                    }
                },
                "size": 207399
            },
            {
                "type": "code",
                "name": "5.png",
                "sha": "7f9ad43c3ebf16d088228d4d0a8a30b8439c52b6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/blob/main/5.png"
                    }
                },
                "size": 262412
            },
            {
                "type": "code",
                "name": "6.png",
                "sha": "0ec7a86f3a5e74e4e5f1be12da7cde49b21ba7b4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/blob/main/6.png"
                    }
                },
                "size": 305376
            },
            {
                "type": "code",
                "name": "7.png",
                "sha": "768f1da70460a5fe9db8b7340391441d4e088e0f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/blob/main/7.png"
                    }
                },
                "size": 252690
            },
            {
                "type": "code",
                "name": "TrainLog.txt",
                "sha": "75e65afd3492b30ee555f9ed67a777b202d575cb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/blob/main/TrainLog.txt"
                    }
                },
                "size": 992439
            },
            {
                "type": "code",
                "name": "backbone",
                "sha": "2620c0e5de52805e642b6280afca1b34ed2b7d8a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/tree/main/backbone"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "checkpoints",
                "sha": "7eb7abef4105fd464fade9b55a2960762819fd49",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/tree/main/checkpoints"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "data",
                "sha": "b85a29f3974d01e25c01770a6cc37f5245123417",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/tree/main/data"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "eval.py",
                "sha": "ce5efb10fe0667b23bd5231fb2e0c6d39c446e81",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/blob/main/eval.py"
                    }
                },
                "size": 2538
            },
            {
                "type": "code",
                "name": "models",
                "sha": "a82d710869feaaae427f063f5eb78ccdb62f2a14",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/tree/main/models"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "1a40b675900c4aa0a565b92594faf6c7fb533cc8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/blob/main/test.py"
                    }
                },
                "size": 4388
            },
            {
                "type": "code",
                "name": "test_images",
                "sha": "a2cf2027fe96c177a2890a998a8209f767d0fa85",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/tree/main/test_images"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "tools.py",
                "sha": "df7deab1aa99d4c85e7b413cf9a07403a6db4805",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/blob/main/tools.py"
                    }
                },
                "size": 5097
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "7fe84a327a0157714ae28b17aaf44419b55ff87f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/blob/main/train.py"
                    }
                },
                "size": 9600
            },
            {
                "type": "code",
                "name": "utils",
                "sha": "c03cb454cf3f04f7d88eb3b972506f45396b0dd2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/tree/main/utils"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "voc_eval",
                "sha": "ae9a3994435430857a8e0f3833ec9437f51f3295",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle/tree/main/voc_eval"
                    }
                },
                "num_files": 2
            }
        ]
    },
    "authors": [
        {
            "name": "Sonrikisou",
            "email": "sunlizhuang@stu.pku.edu.cn",
            "github_id": "sunlizhuang"
        }
    ],
    "tags": [],
    "description": "\u57fa\u4e8ePaddlePaddle\u6846\u67b6\u7684YOLOv1\u590d\u73b0",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle",
            "stars": 2,
            "issues": true,
            "readme": "# \u57fa\u4e8ePaddlePaddle\u6846\u67b6\u7684YOLOv1\u590d\u73b0\n\n\u8bba\u6587\u540d\u79f0\uff1aYou only look once unified real-time object detection\n\n\u8bba\u6587\u5730\u5740\uff1ahttps://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\n\n# \u4e00\u3001\u524d\u8a00\n\u8fd1\u51e0\u5e74\u6765\uff0c\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u53d6\u5f97\u4e86\u5f88\u5927\u7684\u7a81\u7834\u3002\u6bd4\u8f83\u6d41\u884c\u7684\u7b97\u6cd5\u53ef\u4ee5\u5206\u4e3a\u4e24\u7c7b\uff0c\u4e00\u7c7b\u662f\u57fa\u4e8eRegion Proposal\u7684R-CNN\u7cfb\u7b97\u6cd5\uff08R-CNN\uff0cFast R-CNN, Faster R-CNN\uff09\uff0c\u5b83\u4eec\u662ftwo-stage\u7684\uff0c\u9700\u8981\u5148\u4f7f\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08selective search\uff09\u6216\u8005CNN\u7f51\u7edc\uff08RPN\uff09\u4ea7\u751fRegion Proposal\uff0c\u7136\u540e\u518d\u5728Region Proposal\u4e0a\u505a\u5206\u7c7b\u4e0e\u56de\u5f52\u3002\u800c\u53e6\u4e00\u7c7b\u662fYolo\uff0cSSD\u8fd9\u7c7bone-stage\u7b97\u6cd5\uff0c\u5176\u4ec5\u4ec5\u4f7f\u7528\u4e00\u4e2aCNN\u7f51\u7edc\u76f4\u63a5\u9884\u6d4b\u4e0d\u540c\u76ee\u6807\u7684\u7c7b\u522b\u4e0e\u4f4d\u7f6e\u3002\u7b2c\u4e00\u7c7b\u65b9\u6cd5\u662f\u51c6\u786e\u5ea6\u9ad8\u4e00\u4e9b\uff0c\u4f46\u662f\u901f\u5ea6\u6162\uff0c\u4f46\u662f\u7b2c\u4e8c\u7c7b\u7b97\u6cd5\u662f\u901f\u5ea6\u5feb\uff0c\u4f46\u662f\u51c6\u786e\u6027\u8981\u4f4e\u4e00\u4e9b\u3002\u8fd9\u53ef\u4ee5\u5728\u56fe2\u4e2d\u770b\u5230\u3002\u672c\u6587\u4ecb\u7ecd\u7684\u662fYolo\u7b97\u6cd5\uff0c\u5176\u5168\u79f0\u662fYou Only Look Once: Unified, Real-Time Object Detection\uff0c\u5176\u5b9e\u4e2a\u4eba\u89c9\u5f97\u8fd9\u4e2a\u9898\u76ee\u53d6\u5f97\u975e\u5e38\u597d\uff0c\u57fa\u672c\u4e0a\u628aYolo\u7b97\u6cd5\u7684\u7279\u70b9\u6982\u62ec\u5168\u4e86\uff1aYou Only Look Once\u8bf4\u7684\u662f\u53ea\u9700\u8981\u4e00\u6b21CNN\u8fd0\u7b97\uff0cUnified\u6307\u7684\u662f\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u63d0\u4f9bend-to-end\u7684\u9884\u6d4b\uff0c\u800cReal-Time\u4f53\u73b0\u662fYolo\u7b97\u6cd5\u901f\u5ea6\u5feb\u3002\u8fd9\u91cc\u6211\u4eec\u8c08\u7684\u662fYolo-v1\u7248\u672c\u7b97\u6cd5\uff0c\u5176\u6027\u80fd\u662f\u5dee\u4e8e\u540e\u6765\u7684SSD\u7b97\u6cd5\u7684\uff0c\u4f46\u662fYolo\u540e\u6765\u4e5f\u7ee7\u7eed\u8fdb\u884c\u6539\u8fdb\uff0c\u4ea7\u751f\u4e86Yolo9000\u7b97\u6cd5\u3002\n\n# \u4e8c\u3001YOLOv1\u6a21\u578b\u7684\u8bbe\u8ba1\n\u6574\u4f53\u6765\u770b\uff0cYolo\u7b97\u6cd5\u91c7\u7528\u4e00\u4e2a\u5355\u72ec\u7684CNN\u6a21\u578b\u5b9e\u73b0end-to-end\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u6574\u4e2a\u7cfb\u7edf\u5982\u56fe\u793a\uff1a\u9996\u5148\u5c06\u8f93\u5165\u56fe\u7247resize\u5230448x448\uff0c\u7136\u540e\u9001\u5165CNN\u7f51\u7edc\uff0c\u6700\u540e\u5904\u7406\u7f51\u7edc\u9884\u6d4b\u7ed3\u679c\u5f97\u5230\u68c0\u6d4b\u7684\u76ee\u6807\u3002\u76f8\u6bd4R-CNN\u7b97\u6cd5\uff0c\u5176\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5176\u901f\u5ea6\u66f4\u5feb\uff0c\u800c\u4e14Yolo\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e5f\u662fend-to-end\u7684\u3002\n\n![](./1.png)\n\n\u5177\u4f53\u6765\u8bf4\uff0cYolo\u7684CNN\u7f51\u7edc\u5c06\u8f93\u5165\u7684\u56fe\u7247\u5206\u5272\u6210\uff0c\u5176\u4e2d\u524d4\u4e2a\u8868\u5f81\u8fb9\u754c\u6846\u7684\u5927\u5c0f\u4e0e\u4f4d\u7f6e\uff0c\u800c\u6700\u540e\u4e00\u4e2a\u503c\u662f\u7f6e\u4fe1\u5ea6\u3002\n\n![](./2.png)\n\n\u8fd8\u6709\u5206\u7c7b\u95ee\u9898\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5355\u5143\u683c\u5176\u8fd8\u8981\u7ed9\u51fa\u9884\u6d4b\u51fa\u3002\u8fb9\u754c\u6846\u7c7b\u522b\u7f6e\u4fe1\u5ea6\u8868\u5f81\u7684\u662f\u8be5\u8fb9\u754c\u6846\u4e2d\u76ee\u6807\u5c5e\u4e8e\u5404\u4e2a\u7c7b\u522b\u7684\u53ef\u80fd\u6027\u5927\u5c0f\u4ee5\u53ca\u8fb9\u754c\u6846\u5339\u914d\u76ee\u6807\u7684\u597d\u574f\u3002\u540e\u9762\u4f1a\u8bf4\uff0c\u4e00\u822c\u4f1a\u6839\u636e\u7c7b\u522b\u7f6e\u4fe1\u5ea6\u6765\u8fc7\u6ee4\u7f51\u7edc\u7684\u9884\u6d4b\u6846\u3002\n\n![](./3.png)\n\n```\n\n    def set_grid(self, input_size):\n        self.input_size = input_size\n        self.grid_cell = self.create_grid(input_size)\n        self.scale = np.array([[[input_size[1], input_size[0], input_size[1], input_size[0]]]])\n        self.scale_paddle = paddle.to_tensor(self.scale.copy()).astype('float32')\n```\n\n\n\n# \u4e09\u3001YOLOv1\u7f51\u7edc\u7ed3\u6784\n\nYolo\u91c7\u7528\u5377\u79ef\u7f51\u7edc\u6765\u63d0\u53d6\u7279\u5f81\uff0c\u7136\u540e\u4f7f\u7528\u5168\u8fde\u63a5\u5c42\u6765\u5f97\u5230\u9884\u6d4b\u503c\u3002\u7f51\u7edc\u7ed3\u6784\u53c2\u8003GooLeNet\u6a21\u578b\uff0c\u5305\u542b24\u4e2a\u5377\u79ef\u5c42\u548c2\u4e2a\u5168\u8fde\u63a5\u5c42\uff0c\u5982\u56fe8\u6240\u793a\u3002\u5bf9\u4e8e\u5377\u79ef\u5c42\uff0c\u4e3b\u8981\u4f7f\u75281x1\u5377\u79ef\u6765\u505achannle reduction\uff0c\u7136\u540e\u7d27\u8ddf3x3\u5377\u79ef\u3002\u5bf9\u4e8e\u5377\u79ef\u5c42\u548c\u5168\u8fde\u63a5\u5c42\uff0c\u91c7\u7528Leaky ReLU\u6fc0\u6d3b\u51fd\u6570\uff1a\u3002\u4f46\u662f\u6700\u540e\u4e00\u5c42\u5374\u91c7\u7528\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u3002\u9664\u4e86\u4e0a\u9762\u8fd9\u4e2a\u7ed3\u6784\uff0c\u6587\u7ae0\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7248\u672cFast Yolo\uff0c\u5176\u4ec5\u4f7f\u75289\u4e2a\u5377\u79ef\u5c42\uff0c\u5e76\u4e14\u5377\u79ef\u5c42\u4e2d\u4f7f\u7528\u66f4\u5c11\u7684\u5377\u79ef\u6838\u3002\n\n![](./4.png)\n\n## \u5177\u4f53\u5b9e\u73b0\n\n```\nclass BasicBlock(nn.Layer):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 downsample=None,\n                 groups=1,\n                 base_width=64,\n                 dilation=1,\n                 norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2D\n\n        if dilation > 1:\n            raise NotImplementedError(\n                \"Dilation > 1 not supported in BasicBlock\")\n\n        self.conv1 = nn.Conv2D(\n            inplanes, planes, 3, padding=1, stride=stride, bias_attr=False)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2D(planes, planes, 3, padding=1, bias_attr=False)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass BottleneckBlock(nn.Layer):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 downsample=None,\n                 groups=1,\n                 base_width=64,\n                 dilation=1,\n                 norm_layer=None):\n        super(BottleneckBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2D\n        width = int(planes * (base_width / 64.)) * groups\n\n        self.conv1 = nn.Conv2D(inplanes, width, 1, bias_attr=False)\n        self.bn1 = norm_layer(width)\n\n        self.conv2 = nn.Conv2D(\n            width,\n            width,\n            3,\n            padding=dilation,\n            stride=stride,\n            groups=groups,\n            dilation=dilation,\n            bias_attr=False)\n        self.bn2 = norm_layer(width)\n\n        self.conv3 = nn.Conv2D(\n            width, planes * self.expansion, 1, bias_attr=False)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU()\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Layer):\n    \"\"\"ResNet model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        Block (BasicBlock|BottleneckBlock): block module of model.\n        depth (int): layers of resnet, default: 50.\n        num_classes (int): output dim of last fc layer. If num_classes <=0, last fc layer\n                            will not be defined. Default: 1000.\n        with_pool (bool): use pool before the last fc layer or not. Default: True.\n    Examples:\n        .. code-block:: python\n            from paddle.vision.models import ResNet\n            from paddle.vision.models.resnet import BottleneckBlock, BasicBlock\n            resnet50 = ResNet(BottleneckBlock, 50)\n            resnet18 = ResNet(BasicBlock, 18)\n    \"\"\"\n\n    def __init__(self, block, depth, num_classes=1000, with_pool=True):\n        super(ResNet, self).__init__()\n        layer_cfg = {\n            18: [2, 2, 2, 2],\n            34: [3, 4, 6, 3],\n            50: [3, 4, 6, 3],\n            101: [3, 4, 23, 3],\n            152: [3, 8, 36, 3]\n        }\n        layers = layer_cfg[depth]\n        self.num_classes = num_classes\n        self.with_pool = with_pool\n        self._norm_layer = nn.BatchNorm2D\n\n        self.inplanes = 64\n        self.dilation = 1\n\n        self.conv1 = nn.Conv2D(\n            3,\n            self.inplanes,\n            kernel_size=7,\n            stride=2,\n            padding=3,\n            bias_attr=False)\n        self.bn1 = self._norm_layer(self.inplanes)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2D(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        if with_pool:\n            self.avgpool = nn.AdaptiveAvgPool2D((1, 1))\n\n        # if num_classes > 0:\n        #     self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2D(\n                    self.inplanes,\n                    planes * block.expansion,\n                    1,\n                    stride=stride,\n                    bias_attr=False),\n                norm_layer(planes * block.expansion), )\n\n        layers = []\n        layers.append(\n            block(self.inplanes, planes, stride, downsample, 1, 64,\n                  previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, norm_layer=norm_layer))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        # if self.with_pool:\n        #     x = self.avgpool(x)\n\n        # if self.num_classes > 0:\n        #     x = paddle.flatten(x, 1)\n        #     x = self.fc(x)\n\n        return x\n\n```\n\n\n\n\u53ef\u4ee5\u770b\u5230\u7f51\u7edc\u7684\u6700\u540e\u8f93\u51fa\u4e3a\u662f\u8fb9\u754c\u6846\u7684\u9884\u6d4b\u7ed3\u679c\u3002\u8fd9\u6837\uff0c\u63d0\u53d6\u6bcf\u4e2a\u90e8\u5206\u662f\u975e\u5e38\u65b9\u4fbf\u7684\uff0c\u8fd9\u4f1a\u65b9\u9762\u540e\u9762\u7684\u8bad\u7ec3\u53ca\u9884\u6d4b\u65f6\u7684\u8ba1\u7b97\u3002\n\n![](./5.png)\n\n## \u5177\u4f53\u5b9e\u73b0\n\n```\n def forward(self, x, target=None):\n          # backbone\n          C_5 = self.backbone(x)\n\n          # head\n\n          # pred\n          prediction_result = self.pred(C_5)\n\n          prediction_result=paddle.reshape(prediction_result,shape=[C_5.shape[0], 1 + self.num_classes + 4, -1])\n          prediction_result=paddle.fluid.layers.transpose(prediction_result,perm=[0,2,1])\n\n\n          # prediction_result = prediction_result.view(C_5.size(0), 1 + self.num_classes + 4, -1).permute(0, 2, 1)\n          # B, HW, C = prediction_result.size()\n\n          # Divide prediction to obj_pred, txtytwth_pred and cls_pred   \n          # [B, H*W, 1]\n          conf_pred = prediction_result[:, :, :1]\n          # [B, H*W, num_cls]\n          cls_pred = prediction_result[:, :, 1 : 1 + self.num_classes]\n          # [B, H*W, 4]\n          txtytwth_pred = prediction_result[:, :, 1 + self.num_classes:]\n\n          # test\n\n          if not self.trainable:\n              with paddle.no_grad():\n                  # batch size = 1\n\n                  all_conf = paddle.nn.functional.sigmoid(conf_pred)[0]           # 0 is because that these is only 1 batch.\n                  all_bbox = paddle.fluid.layers.clip((self.decode_boxes(txtytwth_pred) / self.scale_paddle)[0], 0., 1.)\n                  all_class = (paddle.nn.functional.softmax(cls_pred[0, :, :], 1) * all_conf)\n\n                  # separate box pred and class conf\n                  all_conf = all_conf.numpy()\n                  all_class = all_class.numpy()\n                  all_bbox = all_bbox.numpy()\n\n                  bboxes, scores, cls_inds = self.postprocess(all_bbox, all_class)\n\n                  return bboxes, scores, cls_inds\n          else:\n              conf_loss, cls_loss, txtytwth_loss, total_loss = tools.loss(pred_conf=conf_pred, pred_cls=cls_pred,\n                                                                          pred_txtytwth=txtytwth_pred,\n                                                                          label=target)\n\n              return conf_loss, cls_loss, txtytwth_loss, total_loss\n```\n\n\n# \u56db\u3001\u8bad\u7ec3\u4e0e\u635f\u5931\u51fd\u6570\n\u5728\u8bad\u7ec3\u4e4b\u524d\uff0c\u5148\u5728ImageNet\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\uff0c\u5176\u9884\u8bad\u7ec3\u7684\u5206\u7c7b\u6a21\u578b\u91c7\u7528\u56fe8\u4e2d\u524d20\u4e2a\u5377\u79ef\u5c42\uff0c\u7136\u540e\u6dfb\u52a0\u4e00\u4e2aaverage-pool\u5c42\u548c\u5168\u8fde\u63a5\u5c42\u3002\u9884\u8bad\u7ec3\u4e4b\u540e\uff0c\u5728\u9884\u8bad\u7ec3\u5f97\u5230\u768420\u5c42\u5377\u79ef\u5c42\u4e4b\u4e0a\u52a0\u4e0a\u968f\u673a\u521d\u59cb\u5316\u76844\u4e2a\u5377\u79ef\u5c42\u548c2\u4e2a\u5168\u8fde\u63a5\u5c42\u3002\u7531\u4e8e\u68c0\u6d4b\u4efb\u52a1\u4e00\u822c\u9700\u8981\u66f4\u9ad8\u6e05\u7684\u56fe\u7247\uff0c\u6240\u4ee5\u5c06\u7f51\u7edc\u7684\u8f93\u5165\u4ece224x224\u589e\u52a0\u5230\u4e86448x448\u3002\u6574\u4e2a\u7f51\u7edc\u7684\u6d41\u7a0b\u5982\u4e0b\u56fe\u6240\u793a\uff1a\n\n![](./6.png)\n\n\u4e0b\u9762\u662f\u8bad\u7ec3\u635f\u5931\u51fd\u6570\u7684\u5206\u6790\uff0cYolo\u7b97\u6cd5\u5c06\u76ee\u6807\u68c0\u6d4b\u770b\u6210\u56de\u5f52\u95ee\u9898\uff0c\u6240\u4ee5\u91c7\u7528\u7684\u662f\u5747\u65b9\u5dee\u635f\u5931\u51fd\u6570\u3002\u4f46\u662f\u5bf9\u4e0d\u540c\u7684\u90e8\u5206\u91c7\u7528\u4e86\u4e0d\u540c\u7684\u6743\u91cd\u503c\u3002\u9996\u5148\u533a\u5206\u5b9a\u4f4d\u8bef\u5dee\u548c\u5206\u7c7b\u8bef\u5dee\u3002\u5bf9\u4e8e\u5b9a\u4f4d\u8bef\u5dee\uff0c\u5373\u8fb9\u754c\u6846\u5750\u6807\u9884\u6d4b\u8bef\u5dee\uff0c\u91c7\u7528\u8f83\u5927\u7684\u6743\u91cd\u3002\n\u53e6\u5916\u4e00\u70b9\u65f6\uff0c\u7531\u4e8e\u6bcf\u4e2a\u5355\u5143\u683c\u9884\u6d4b\u591a\u4e2a\u8fb9\u754c\u6846\u3002\u4f46\u662f\u5176\u5bf9\u5e94\u7c7b\u522b\u53ea\u6709\u4e00\u4e2a\u3002\u90a3\u4e48\u5728\u8bad\u7ec3\u65f6\uff0c\u5982\u679c\u8be5\u5355\u5143\u683c\u5185\u786e\u5b9e\u5b58\u5728\u76ee\u6807\uff0c\u90a3\u4e48\u53ea\u9009\u62e9\u4e0eground truth\u7684IOU\u6700\u5927\u7684\u90a3\u4e2a\u8fb9\u754c\u6846\u6765\u8d1f\u8d23\u9884\u6d4b\u8be5\u76ee\u6807\uff0c\u800c\u5176\u5b83\u8fb9\u754c\u6846\u8ba4\u4e3a\u4e0d\u5b58\u5728\u76ee\u6807\u3002\u8fd9\u6837\u8bbe\u7f6e\u7684\u4e00\u4e2a\u7ed3\u679c\u5c06\u4f1a\u4f7f\u4e00\u4e2a\u5355\u5143\u683c\u5bf9\u5e94\u7684\u8fb9\u754c\u6846\u66f4\u52a0\u4e13\u4e1a\u5316\uff0c\u5176\u53ef\u4ee5\u5206\u522b\u9002\u7528\u4e0d\u540c\u5927\u5c0f\uff0c\u4e0d\u540c\u9ad8\u5bbd\u6bd4\u7684\u76ee\u6807\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u5927\u5bb6\u53ef\u80fd\u4f1a\u60f3\u5982\u679c\u4e00\u4e2a\u5355\u5143\u683c\u5185\u5b58\u5728\u591a\u4e2a\u76ee\u6807\u600e\u4e48\u529e\uff0c\u5176\u5b9e\u8fd9\u65f6\u5019Yolo\u7b97\u6cd5\u5c31\u53ea\u80fd\u9009\u62e9\u5176\u4e2d\u4e00\u4e2a\u6765\u8bad\u7ec3\uff0c\u8fd9\u4e5f\u662fYolo\u7b97\u6cd5\u7684\u7f3a\u70b9\u4e4b\u4e00\u3002\u8981\u6ce8\u610f\u7684\u4e00\u70b9\u65f6\uff0c\u5bf9\u4e8e\u4e0d\u5b58\u5728\u5bf9\u5e94\u76ee\u6807\u7684\u8fb9\u754c\u6846\uff0c\u5176\u8bef\u5dee\u9879\u5c31\u662f\u53ea\u6709\u7f6e\u4fe1\u5ea6\uff0c\u5de6\u6807\u9879\u8bef\u5dee\u662f\u6ca1\u6cd5\u8ba1\u7b97\u7684\u3002\u800c\u53ea\u6709\u5f53\u4e00\u4e2a\u5355\u5143\u683c\u5185\u786e\u5b9e\u5b58\u5728\u76ee\u6807\u65f6\uff0c\u624d\u8ba1\u7b97\u5206\u7c7b\u8bef\u5dee\u9879\uff0c\u5426\u5219\u8be5\u9879\u4e5f\u662f\u65e0\u6cd5\u8ba1\u7b97\u7684\u3002\n\u7efc\u4e0a\u8ba8\u8bba\uff0c\u6700\u7ec8\u7684\u635f\u5931\u51fd\u6570\u8ba1\u7b97\u5982\u4e0b\uff1a\n\n![](./7.png)\n\n\n\u5176\u4e2d\u7b2c\u4e00\u9879\u662f\u8fb9\u754c\u6846\u4e2d\u5fc3\u5750\u6807\u7684\u8bef\u5dee\u9879\uff0c\u4e2a\u5355\u5143\u683c\u5b58\u5728\u76ee\u6807\u3002\n\n```\ndef loss(pred_conf, pred_cls, pred_txtytwth, label):\n    obj = 5.0\n    noobj = 1.0\n\n    # create loss_f\n    conf_loss_function = MSELoss(reduction='mean')\n    cls_loss_function = nn.CrossEntropyLoss(reduction='none')\n    txty_loss_function = nn.BCEWithLogitsLoss(reduction='none')\n    twth_loss_function = nn.MSELoss(reduction='none')\n\n    pred_conf = paddle.nn.functional.sigmoid(pred_conf[:, :, 0])\n    pred_cls=paddle.fluid.layers.transpose(pred_cls,perm=[0,2,1])\n    # pred_cls = pred_cls.permute(0, 2, 1)\n    pred_txty = pred_txtytwth[:, :, :2]\n    pred_twth = pred_txtytwth[:, :, 2:]\n        \n    gt_obj = label[:, :, 0]\n    gt_cls = label[:, :, 1]\n    gt_txtytwth = label[:, :, 2:-1]\n    gt_box_scale_weight = label[:, :, -1]\n\n    # objectness loss\n\n    pred_conf=pred_conf.astype(\"float32\")\n    gt_obj=gt_obj.astype(\"float32\")\n    pos_loss, neg_loss = conf_loss_function(pred_conf,gt_obj)\n\n    conf_loss = obj * pos_loss + noobj * neg_loss\n\n\n    \n    # class loss\n\n    pred_cls=pred_cls.astype('float32')\n    gt_cls=gt_cls.astype('int64')\n\n    pred_cls_t=paddle.fluid.layers.transpose(pred_cls,perm=[0,2,1])\n\n    cls_loss_total=paddle.zeros(shape=[0])\n    for i in range(pred_cls_t.shape[0]):\n        for j in range(pred_cls_t.shape[1]):\n            cls_loss_total=paddle.concat(x=[cls_loss_total,cls_loss_function(pred_cls_t[i][j],gt_cls[i][j])],axis=0)\n    cls_loss_total=paddle.to_tensor(cls_loss_total,stop_gradient=False)\n    cls_loss_total=paddle.reshape(cls_loss_total,shape=[pred_cls_t.shape[0],pred_cls_t.shape[1]])\n\n    temp_result=cls_loss_total * gt_obj\n    temp_result=paddle.to_tensor(temp_result,stop_gradient=False).astype('float32')\n\n    \n    cls_loss = paddle.mean(paddle.sum(temp_result, 1))\n\n\n\n    \n    # box loss\n    pred_txty=pred_txty.astype('float32')\n    gt_txtytwth=gt_txtytwth.astype('float32')\n\n\n    txty_loss = paddle.mean(paddle.sum(paddle.sum(txty_loss_function(pred_txty, gt_txtytwth[:, :, :2]), 2) * gt_box_scale_weight * gt_obj, 1))\n    twth_loss = paddle.mean(paddle.sum(paddle.sum(twth_loss_function(pred_twth, gt_txtytwth[:, :, 2:]), 2) * gt_box_scale_weight * gt_obj, 1))\n\n    txtytwth_loss = txty_loss + twth_loss\n\n    total_loss = conf_loss + cls_loss + txtytwth_loss\n\n    return conf_loss, cls_loss, txtytwth_loss, total_loss\n```\n\n\n\n# \u4e94\u3001\u6a21\u578b\u590d\u73b0\u6307\u6807\n## 5.1 \u6a21\u578b\u53c2\u6570\n- Batchsize: 32\n- \u57fa\u7840\u5b66\u4e60\u7387: 1e-3\n- epoch: 160\n- LRstep: 60, 90\n- \u4f18\u5316\u65b9\u5f0f: \u52a8\u91cf\u7248SGD\n\n\n\n## 5.2 \u590d\u73b0\u73af\u5883\n\n- Python3.7, opencv-python, PaddlePaddle 2.1.2\n\n## 5.3 \u5b9e\u9a8c\u7ed3\u679c\n- VOC2007-test:\n![](https://ai-studio-static-online.cdn.bcebos.com/fea016840ab14735afbe809529b85fe0afc55601204f43769b31b7623e1955e6)\n\n\n\n# \u516d\u3001\u5b9e\u9a8c\u6d41\u7a0b\n\n\u767b\u5f55AI Studio\u53ef\u5728\u7ebf\u8fd0\u884c\uff1ahttps://aistudio.baidu.com/aistudio/projectdetail/2259467\n\n\n\u5c06\u5e73\u53f0\u6302\u8f7dVOC2007+2012\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u538b\u7f29\u5305VOCdevkit.zip\u79fb\u5230/work/datasets\u6587\u4ef6\u5939\u4e0b\uff0c\u89e3\u538b\u6570\u636e\u96c6 ```datasets/```\u4e0b\u7684 ```VOCdevkit.zip``\n\n\n## 6.1 \u8bad\u7ec3\n```Shell\npython train.py\n```\n\n## \u5982\u9700\u8bbe\u7f6e\u53c2\u6570\u8bf7\u6309\u7167\u4ee5\u4e0b\u683c\u5f0f\u586b\u5199\n\n```\n parser = argparse.ArgumentParser(description='YOLO Detection')\n    parser.add_argument('-v', '--version', default='yolo',\n                        help='yolo')\n    parser.add_argument('-d', '--dataset', default='voc',\n                        help='voc or coco')\n    parser.add_argument('-hr', '--high_resolution', action='store_true', default=False,\n                        help='use high resolution to pretrain.')  \n    parser.add_argument('-ms', '--multi_scale', action='store_true', default=False,\n                        help='use multi-scale trick')                  \n    parser.add_argument('--batch_size', default=32, type=int, \n                        help='Batch size for training')\n    parser.add_argument('--lr', default=1e-3, type=float, \n                        help='initial learning rate')\n    parser.add_argument('-cos', '--cos', action='store_true', default=False,\n                        help='use cos lr')\n    parser.add_argument('-no_wp', '--no_warm_up', action='store_true', default=False,\n                        help='yes or no to choose using warmup strategy to train')\n    parser.add_argument('--wp_epoch', type=int, default=2,\n                        help='The upper bound of warm-up')\n    parser.add_argument('--start_epoch', type=int, default=0,\n                        help='start epoch to train')\n    parser.add_argument('-r', '--resume', default=None, type=str,help='keep training')\n    # parser.add_argument('-r', '--resume', default=None, type=str,\n    #                     help='keep training')\n    parser.add_argument('--momentum', default=0.9, type=float, \n                        help='Momentum value for optim')\n    parser.add_argument('--weight_decay', default=5e-4, type=float, \n                        help='Weight decay for SGD')\n    parser.add_argument('--gamma', default=0.1, type=float, \n                        help='Gamma update for SGD')\n    parser.add_argument('--num_workers', default=8, type=int, \n                        help='Number of workers used in dataloading')\n    parser.add_argument('--eval_epoch', type=int,\n                            default=10, help='interval between evaluations')\n    parser.add_argument('--gpu', action='store_true', default=True,\n                        help='use gpu.')\n    parser.add_argument('--tfboard', action='store_true', default=False,\n                        help='use tensorboard')\n    parser.add_argument('--debug', action='store_true', default=False,\n                        help='debug mode where only one image is trained')\n    parser.add_argument('--save_folder', default='weights/', type=str, \n                        help='Gamma update for SGD')\n```\n\n## 6.2 \u6d4b\u8bd5\n```Shell\npython test.py\n```\n\n## \u5982\u9700\u8bbe\u7f6e\u53c2\u6570\u8bf7\u6309\u7167\u4ee5\u4e0b\u683c\u5f0f\u586b\u5199\n```\nparser = argparse.ArgumentParser(description='YOLO Detection')\nparser.add_argument('-v', '--version', default='yolo',\n                    help='yolo')\nparser.add_argument('-d', '--dataset', default='voc',\n                    help='voc, coco-val.')\nparser.add_argument('-size', '--input_size', default=416, type=int,\n                    help='input_size')\nparser.add_argument('--trained_model', default='./checkpoints/yolo-model-best.pdparams',\n                    type=str, help='Trained state_dict file path to open')\nparser.add_argument('--conf_thresh', default=0.1, type=float,\n                    help='Confidence threshold')\nparser.add_argument('--nms_thresh', default=0.50, type=float,\n                    help='NMS threshold')\nparser.add_argument('--visual_threshold', default=0.3, type=float,\n                    help='Final confidence threshold')\nparser.add_argument('--gpu', action='store_true', default=True, \n                    help='use cuda.')\n```\n\n\n## 6.3 \u8bc4\u4f30\n```Shell\npython eval.py\n```\n\n## \u5982\u9700\u8bbe\u7f6e\u53c2\u6570\u8bf7\u6309\u7167\u4ee5\u4e0b\u683c\u5f0f\u586b\u5199\n\n```\nparser = argparse.ArgumentParser(description='YOLO Detector Evaluation')\nparser.add_argument('-v', '--version', default='yolo',\n                    help='yolo.')\nparser.add_argument('-d', '--dataset', default='voc',\n                    help='voc, coco-val, coco-test.')\nparser.add_argument('--trained_model', type=str,\n                    default='./checkpoints/yolo-model-best.pdparams',\n                    help='Trained state_dict file path to open')\nparser.add_argument('-size', '--input_size', default=416, type=int,\n                    help='input_size')\nparser.add_argument('--gpu', action='store_true', default=True,\n                    help='Use gpu')\n```\n\n\n\n##  \u53c2\u8003\n\nhttps://github.com/AlexeyAB/darknet\n\nhttps://github.com/TowardsNorth/yolo_v1_tensorflow_guiyu\n\nhttps://github.com/yjh0410/new-YOLOv1_PyTorch\n\nhttps://github.com/abeardear/pytorch-YOLO-v1\n\n\n",
            "readme_url": "https://github.com/sunlizhuang/YOLOv1-PaddlePaddle",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "COCO"
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "CIFAR-10"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.5863940276309701,
        "task": "Object Detection",
        "task_prob": 0.5320216914569855
    }
}