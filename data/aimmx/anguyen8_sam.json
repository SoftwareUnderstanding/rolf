{
    "visibility": {
        "visibility": "public"
    },
    "name": "SAM: The Sensitivity of Attribution Methods to Hyperparameters",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "anguyen8",
                "owner_type": "User",
                "name": "sam",
                "url": "https://github.com/anguyen8/sam",
                "stars": 23,
                "pushed_at": "2022-03-12 00:17:42+00:00",
                "created_at": "2020-02-18 17:47:23+00:00",
                "language": "Python",
                "description": "Code for the CVPR 2020 [ORAL] paper \"SAM: The Sensitivity of Attribution Methods to Hyperparameters\"",
                "frameworks": [
                    "TensorFlow",
                    "scikit-learn",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".DS_Store",
                "sha": "7747cafb26b09eac747a6d9c20fa19fc577558c3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/.DS_Store"
                    }
                },
                "size": 6148
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "eb3834dda9d321310c76fa31ccda4890651c2726",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/.gitignore"
                    }
                },
                "size": 1849
            },
            {
                "type": "code",
                "name": "Gradient_Madry.py",
                "sha": "acb0a59cb3abbdfeca9ebd57cd235db8f990de5a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/Gradient_Madry.py"
                    }
                },
                "size": 10509
            },
            {
                "type": "code",
                "name": "IG_Madry.py",
                "sha": "0b40b1e93612b21da3c53681a20782c1b8615272",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/IG_Madry.py"
                    }
                },
                "size": 17938
            },
            {
                "type": "code",
                "name": "Images",
                "sha": "48866bf9b48ac4fe97a04246a1d1f1dabaf37cd6",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/tree/master/Images"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "Input_times_Gradient_Madry.py",
                "sha": "b2260d514b0d6f3b5affbbdd71af67339308dec2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/Input_times_Gradient_Madry.py"
                    }
                },
                "size": 10584
            },
            {
                "type": "code",
                "name": "LIME_Madry.py",
                "sha": "3ce5714f15ef4bc4295c9e8f6e5182fe5db53407",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/LIME_Madry.py"
                    }
                },
                "size": 30439
            },
            {
                "type": "code",
                "name": "MP_MADRY.py",
                "sha": "95d0663873111799b9a2054a8c7b4cbd48f4364b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/MP_MADRY.py"
                    }
                },
                "size": 38221
            },
            {
                "type": "code",
                "name": "Occlusion_Madry.py",
                "sha": "27bd1c3a1b9a3ea4577dfab397b5539817e183ba",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/Occlusion_Madry.py"
                    }
                },
                "size": 14151
            },
            {
                "type": "code",
                "name": "RISE_evaluation.py",
                "sha": "2f9a58b1dcf92cc36283901d6ac71fd05633cfc4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/RISE_evaluation.py"
                    }
                },
                "size": 8409
            },
            {
                "type": "code",
                "name": "RISE_utils.py",
                "sha": "ae18e4fb5f4198c3ab58c6cc5834b6e21f565239",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/RISE_utils.py"
                    }
                },
                "size": 2563
            },
            {
                "type": "code",
                "name": "Sensitivity_Analysis_Basic.py",
                "sha": "4a2e8ede2268e9865319046c48398876d5db3887",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/Sensitivity_Analysis_Basic.py"
                    }
                },
                "size": 15468
            },
            {
                "type": "code",
                "name": "Sensitivity_Analysis_Basic_LIME_Comp_With_Default_Settings.py",
                "sha": "ebc4a131f046509282110eb9daf9dac4cb949ab1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/Sensitivity_Analysis_Basic_LIME_Comp_With_Default_Settings.py"
                    }
                },
                "size": 17054
            },
            {
                "type": "code",
                "name": "Sensitivity_Analysis_Basic_MP_Comp_With_Default_Settings.py",
                "sha": "98a94773954e6c8d7ab252431542c3dcf42e8932",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/Sensitivity_Analysis_Basic_MP_Comp_With_Default_Settings.py"
                    }
                },
                "size": 17267
            },
            {
                "type": "code",
                "name": "Sensitivity_Analysis_Basic_Occlusion_Comp_With_Default_Settings.py",
                "sha": "1ce4cc1c1cc5d40ee6da81bf004715275723a265",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/Sensitivity_Analysis_Basic_Occlusion_Comp_With_Default_Settings.py"
                    }
                },
                "size": 17517
            },
            {
                "type": "code",
                "name": "Sensitivity_Analysis_Basic_SmoothGrad_Comp_With_Default_Settings.py",
                "sha": "960a2e536bf75e762d6484b476935683bc7e4284",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/Sensitivity_Analysis_Basic_SmoothGrad_Comp_With_Default_Settings.py"
                    }
                },
                "size": 17150
            },
            {
                "type": "code",
                "name": "Sensitivity_Analysis_IOU.py",
                "sha": "9bde62fd4c4efe8434acbca66946a2ba7db5c1ee",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/Sensitivity_Analysis_IOU.py"
                    }
                },
                "size": 23792
            },
            {
                "type": "code",
                "name": "Sensitivity_Analysis_Model_Dependent.py",
                "sha": "8280bd00e14045e6f261c73599dfcb6e40fdc03f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/Sensitivity_Analysis_Model_Dependent.py"
                    }
                },
                "size": 19149
            },
            {
                "type": "code",
                "name": "SmoothGrad_Madry.py",
                "sha": "4b7d1954a79bde39da8a91a803dff6c7fec7fe87",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/SmoothGrad_Madry.py"
                    }
                },
                "size": 14979
            },
            {
                "type": "code",
                "name": "formal_plot_gradient.py",
                "sha": "929ed0c781d3883f35ee017b681e3ccf6c6e8f47",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/formal_plot_gradient.py"
                    }
                },
                "size": 2634
            },
            {
                "type": "code",
                "name": "formal_plot_teaser.py",
                "sha": "ac332c84f25a7ab78f7850e580e96097dcc07e36",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/formal_plot_teaser.py"
                    }
                },
                "size": 2609
            },
            {
                "type": "code",
                "name": "formal_sensitivity_occlusion.jpg",
                "sha": "fad123b8202ac90ce00856d90f11177fa2875d1b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/formal_sensitivity_occlusion.jpg"
                    }
                },
                "size": 304387
            },
            {
                "type": "code",
                "name": "gradient.sh",
                "sha": "c50bfa5085dece8349f8ec75ffe142d5cb53fdc2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/gradient.sh"
                    }
                },
                "size": 1123
            },
            {
                "type": "code",
                "name": "imagenet_class_mappings",
                "sha": "75d3c8f8120ce7bd001e8672104ac506ffa0918c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/tree/master/imagenet_class_mappings"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "img_name_files",
                "sha": "bc6166e9feca492256558d7305e3c6c1c1e0c3e9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/tree/master/img_name_files"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "madry_files",
                "sha": "6649fb79d4b8bf67df5d73612eda91da146bc2bc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/tree/master/madry_files"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "naman_robustness",
                "sha": "11c1f577fb8aee319b0c92e8c6c38a16a1eab712",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/tree/master/naman_robustness"
                    }
                },
                "num_files": 14
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "d839f05930d0e181ecaae7bb025ed94edae62b9a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/requirements.txt"
                    }
                },
                "size": 1714
            },
            {
                "type": "code",
                "name": "results",
                "sha": "d6844cc4e91bb2345f774e72c14afd972729152f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/tree/master/results"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "sensitivity.sh",
                "sha": "1f8b56db82ed32a1708207d476ba08d41c81a174",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/sensitivity.sh"
                    }
                },
                "size": 6974
            },
            {
                "type": "code",
                "name": "settings.py",
                "sha": "021e1b11f737c672957ba37d85426035b3efe8ec",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/settings.py"
                    }
                },
                "size": 469
            },
            {
                "type": "code",
                "name": "synset_words.txt",
                "sha": "a9e8c7f50d144ef6034d5231709dd3545b10b69c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/synset_words.txt"
                    }
                },
                "size": 31675
            },
            {
                "type": "code",
                "name": "teaser.sh",
                "sha": "80dab16c02b61d3a1ae91f08278ebce2f554492e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/teaser.sh"
                    }
                },
                "size": 2482
            },
            {
                "type": "code",
                "name": "train.sh",
                "sha": "4e15f7e56b626a9560c095be78c2b864a8ea7870",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/train.sh"
                    }
                },
                "size": 491
            },
            {
                "type": "code",
                "name": "user_constants.py",
                "sha": "862988e2f5869cdc19ac2e2e0945d41b39e29afc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/user_constants.py"
                    }
                },
                "size": 552
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "b6edf6376027083e83339a5c263b1928cdb48453",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/anguyen8/sam/blob/master/utils.py"
                    }
                },
                "size": 29990
            }
        ]
    },
    "authors": [
        {
            "name": "Chirag Agarwal",
            "github_id": "chirag126"
        },
        {
            "name": "Naman Bansal",
            "email": "bnaman50@gmail.com",
            "github_id": "bnaman50"
        },
        {
            "name": "Anh M. Nguyen",
            "email": "anh.ng8@gmail.com",
            "github_id": "anguyen8"
        }
    ],
    "tags": [],
    "description": "Code for the CVPR 2020 [ORAL] paper \"SAM: The Sensitivity of Attribution Methods to Hyperparameters\"",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/anguyen8/sam",
            "stars": 23,
            "issues": true,
            "readme": "## SAM: The Sensitivity of Attribution Methods to Hyperparameters\n\nThis repository contains source code necessary to reproduce some of the main results in our paper:\n\nBansal*, Agarwal*, Nguyen* (2020). _SAM: The sensitivity of attribution methods to hyperparameters_. Computer Vision and Pattern Recognition (CVPR). **Oral** presentation. [paper](http://anhnguyen.me/project/sam/) | [code](https://github.com/anguyen8/sam)\n\n**If you use this software, please consider citing:**\n\n    @inproceedings{bansal2020sam,\n        title={SAM: The Sensitivity of Attribution Methods to Hyperparameters},\n        author={Naman Bansal, Chirag Agarwal, Anh Nguyen},\n        booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n        pages={},\n        year={2020}\n    }\n    \n## 1. Setup\n\n### Installing software\nThis repository is built using PyTorch. You can install the necessary libraries by pip installing the requirements text file `pip install -r ./requirements.txt`\nThe code was set up using **python=3.6.7**\n\n### Pretrained models\nAll of our experiments were conducted on two groups of classifiers: (a) [GoogLeNet](https://arxiv.org/pdf/1409.4842.pdf) and [ResNet-50](https://arxiv.org/pdf/1512.03385.pdf) pre-trained on the 1000-class 2012 ImageNet dataset; and (b) the robust versions of them, i.e. GoogLeNet-R and ResNet-R that were trained to also be invariant to small adversarial changes in the input image. The two regular models are obtained from the [PyTorch model zoo](https://pytorch.org/docs/stable/torchvision/models.html), the ResNet-R from [Engstrom et al.](https://arxiv.org/pdf/1906.00945.pdf), and we trained GoogLeNet-R by ourselves using the code released by the author. While the two robust classifiers are more invariant to pixel-wise noise they have lower ImageNet validation-set accuracy scores (50.94% and 56.25%) than those of the original GoogLeNet and ResNet (68.86% and 75.59%).\n    \n#### Adversarial training details\n[Engstrom et al.](https://arxiv.org/pdf/1906.00945.pdf) adversarially trained a ResNet-50 model using Projected Gradient Descent (PGD) attack with a normalized step size. We followed the author and trained robust GoogLeNet model, denoted as GoogLeNet-R, for our sensitivity experiments. We used adversarial perturbation in <a href=\"https://www.codecogs.com/eqnedit.php?latex=L_2\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L_2\"/></a>-norm for generating adversarial samples during training. Additionally, we used <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\epsilon\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\epsilon\"/></a>=3, a step size of 0.5 and the number of steps as 7 for PGD. The model was trained end-to-end for 90 epochs using a batch-size of 256 on 4 Tesla-V100 GPU's. We used SGD optimizer with a learning rate (lr) scheduler starting with lr=0.1 and dropping the learning rate by 10 after every 30 epochs.\n\nAll the pre-trained models are available [here](https://drive.google.com/drive/u/2/folders/1KdJ0aK0rPjmowS8Swmzxf8hX6gU5gG2U). The user has to download the weight files and store them under the **./models/** directory.\nGoogLeNet-R can be trained using the script provided in [train.sh](train.sh). The user has to install the [robustness](https://github.com/MadryLab/robustness) repo and provide the input directory for the ImageNet training images under **data_path** argument. \n\n**Note:** Before running the [train.sh](train.sh) scripts, replace the files under `robustness/imagenet_models/` (e.g. `~/anaconda3/envs/your_env/lib/python3.6/site-packages/robustness/imagenet_models/`) in the robustness library folder with the files in the folder [madry_files](./madry_files/).\n\nThese are the following modifations that we made to the **robustness** directory (now `./naman_robustness`). \n- By default, the robust model returns a tuple *(logits, input)*. We changed it to output only *logits*.\n- By default, it requires a different input normalization as mentioned in the original repo. We have modified it to allow both the normal pytorch pre-processing and the one used originally. Please refer to the function `load_madry_model()`, in `./utils.py` for more details. \n- If you are wrtiting your own explanation method or want to rewrite one of the methods in our repo for robust models, we would recommend setting `my_attacker=True` while calling `load_madry_model()`. This greatly simplifies the implementation. You should only set it to **False** if you want to adversarially perturb the image. \n\n## 2. Dataset\nWe ran our experiments on a 1735 images from the original ImageNet validation dataset as described in the paper. We have provided a text file in `./img_file_names/paper_images_list.txt`. So if you run any explanation method with input directory being a path to the validation set, our implementation will only produce heatmaps on the 1735 images mentioned in the list instead of running on all the 50K images. \n\n## 3. Usage\n- The shell script for generating Figure 1 of our paper is in [teaser.sh](teaser.sh). Given an [image](./Images/teaser/ILSVRC2012_val_00002056.JPEG), the script runs SmoothGrad, Sliding-Patch, LIME, and Meaningful Perturbation algorithm for their different hyperparameters and produces a montage image of their respective [attribution maps](./results/formal_teaser.jpg)\n\n### 3.1 Script 1\nGenerating the attribution map for the class \"matchstick\".\nRunning `source teaser.sh` produces this result:\n\n<p align=\"center\">\n    <img src=\"./results/formal_teaser.jpg\" height=300px width=300px>\n</p>\n<p align=\"center\"><i> The real image followed by the different attribution maps generated using (top-->bottom) LIME, Sliding-Patch, Meaningful Perturbation and SmoothGrad algorithms. We show the sensitivity (left-->right) of each explanation algorithm with respect to its respective hyperparameter.</i></p>\n\n- The shell script for generating Figure 2 of our paper is in [gradient.sh](gradient.sh). Given an [image](./Images/grad/ILSVRC2012_val_00020735.JPEG), the script generates the [gradient](./results/formal_gradient.jpg) of four models (GoogLeNet, GoogLeNet-R, ResNet-50, and ResNet-50-R) for a clean and noisy image respectively.\n\n### 3.2 Script 2\nGenerating the attribution map for the class \"goblet\".\nRunning `source gradient.sh` produces this result:\n\n<p align=\"center\">\n    <img src=\"./results/formal_gradient.jpg\" width=500px>\n</p>\n<p align=\"center\"><i> The clean image followed by the gradient attribution maps generated using (left-->right) GoogLeNet, GoogLeNet-R, ResNet-50, and ResNet-50-R models. We show the sensitivity of the gradients on adding a small Gaussian noise to the clean image for all the models respectively.</i></p>\n\n- The shell script for evaluating the sensitivity of different explanation methods is in [sensitivity.sh](sensitivity.sh). The sensitivity is calculated for five sample images in [this](./Images/images_sensitivity/) folder across all four models (GoogLeNet, GoogLeNet-R, ResNet-50, and ResNet-50-R).\nRunning  `source sensitivity.sh` runs the sensitivity test on Vanilla Gradient (VG), Input x Gradient (IG), Sliding-Patch (SP), Meaningful Perturbation (MP), LIME, and Smooth Gradient (SG) explanation methods on their respective hyperparameters. Given the list of images in the folder, the script calculates the average accuracy scores across all the images using the evaluation metrics described in the paper.\n\n### 3.3 Script 3\nRunning \n```\nfor patch in {52,53,54}\ndo\n  CUDA_VISIBLE_DEVICES=0 python Occlusion_Madry.py -idp ./Images/images_sensitivity/ -ops ${patch} -op ./results/Sensitivity/Occlusion/\ndone\n\npython Sensitivity_Analysis_Basic_Occlusion_Comp_With_Default_Settings.py -idp ./results/Sensitivity/Occlusion/ -mn occlusion --metric_name hog -op ./results/evaluation_result_text_files/Occlusion --exp_num a03\n```\nfrom the [sensitivity.sh](sensitivity.sh) would produce the sensitivity results of the Sliding-Patch explanation algorithm to its different patch sizes (52, 53, 54). It generates a report (shown in figure below) which lists down the mean and standard deviation of all evaluation metric scores.\n<p align=\"center\">\n    <img src=\"./formal_sensitivity_occlusion.jpg\" height=300px width=300px>\n</p>\n\n## 4. Licenses\nNote that the code in this repository is licensed under MIT License, but, the pre-trained condition models used by the code have their own licenses. Please carefully check them before use. \n\n## 5. Questions?\nIf you have questions/suggestions, please feel free to email us or create github issues.\n",
            "readme_url": "https://github.com/anguyen8/sam",
            "frameworks": [
                "TensorFlow",
                "scikit-learn",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "Going Deeper with Convolutions",
            "arxiv": "1409.4842",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.4842v1",
            "abstract": "We propose a deep convolutional neural network architecture codenamed\n\"Inception\", which was responsible for setting the new state of the art for\nclassification and detection in the ImageNet Large-Scale Visual Recognition\nChallenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the\nimproved utilization of the computing resources inside the network. This was\nachieved by a carefully crafted design that allows for increasing the depth and\nwidth of the network while keeping the computational budget constant. To\noptimize quality, the architectural decisions were based on the Hebbian\nprinciple and the intuition of multi-scale processing. One particular\nincarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22\nlayers deep network, the quality of which is assessed in the context of\nclassification and detection.",
            "authors": [
                "Christian Szegedy",
                "Wei Liu",
                "Yangqing Jia",
                "Pierre Sermanet",
                "Scott Reed",
                "Dragomir Anguelov",
                "Dumitru Erhan",
                "Vincent Vanhoucke",
                "Andrew Rabinovich"
            ]
        },
        {
            "title": "Adversarial Robustness as a Prior for Learned Representations",
            "arxiv": "1906.00945",
            "year": 2019,
            "url": "http://arxiv.org/abs/1906.00945v2",
            "abstract": "An important goal in deep learning is to learn versatile, high-level feature\nrepresentations of input data. However, standard networks' representations seem\nto possess shortcomings that, as we illustrate, prevent them from fully\nrealizing this goal. In this work, we show that robust optimization can be\nre-cast as a tool for enforcing priors on the features learned by deep neural\nnetworks. It turns out that representations learned by robust models address\nthe aforementioned shortcomings and make significant progress towards learning\na high-level encoding of inputs. In particular, these representations are\napproximately invertible, while allowing for direct visualization and\nmanipulation of salient input features. More broadly, our results indicate\nadversarial robustness as a promising avenue for improving learned\nrepresentations. Our code and models for reproducing these results is available\nat https://git.io/robust-reps .",
            "authors": [
                "Logan Engstrom",
                "Andrew Ilyas",
                "Shibani Santurkar",
                "Dimitris Tsipras",
                "Brandon Tran",
                "Aleksander Madry"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "OCCLUSION"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999989798429423,
        "task": "Object Detection",
        "task_prob": 0.9330657538959594
    }
}