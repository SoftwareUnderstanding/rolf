{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "universe-starter-agent",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "vladfi1",
                "owner_type": "User",
                "name": "universe-starter-agent",
                "url": "https://github.com/vladfi1/universe-starter-agent",
                "stars": 8,
                "pushed_at": "2017-01-31 01:32:48+00:00",
                "created_at": "2017-01-26 20:13:00+00:00",
                "language": "Python",
                "description": "A starter agent that can solve a number of universe environments.",
                "license": "MIT License",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "72364f99fe4bf8d5262df3b19b33102aeaa791e5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vladfi1/universe-starter-agent/blob/master/.gitignore"
                    }
                },
                "size": 1045
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "eb11badbc3a623e12ba7a67e89ef7cca51eedda4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vladfi1/universe-starter-agent/blob/master/LICENSE"
                    }
                },
                "size": 1063
            },
            {
                "type": "code",
                "name": "a3c.py",
                "sha": "9b37adf8ecae0ab4ba2550a18729a8dc7b205d49",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vladfi1/universe-starter-agent/blob/master/a3c.py"
                    }
                },
                "size": 11854
            },
            {
                "type": "code",
                "name": "envs.py",
                "sha": "b6c7d9ca46175eea2870a8c976f2cf684a709632",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vladfi1/universe-starter-agent/blob/master/envs.py"
                    }
                },
                "size": 11344
            },
            {
                "type": "code",
                "name": "imgs",
                "sha": "6066ac486e646ab2d6b7821aae2d4e86eb63a4ad",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vladfi1/universe-starter-agent/tree/master/imgs"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "model.py",
                "sha": "4f916412a2c488b3c5e51cfb9ce7db331ed5f48b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vladfi1/universe-starter-agent/blob/master/model.py"
                    }
                },
                "size": 4210
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "e58b2fd286bdecf7a174ab4d718eefb9ad80e492",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vladfi1/universe-starter-agent/blob/master/train.py"
                    }
                },
                "size": 4409
            },
            {
                "type": "code",
                "name": "worker.py",
                "sha": "209902f0298fa955861f1b093fdd179dd641d667",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vladfi1/universe-starter-agent/blob/master/worker.py"
                    }
                },
                "size": 6021
            }
        ]
    },
    "authors": [
        {
            "name": "Trevor Blackwell",
            "email": "tlb@tlb.org",
            "github_id": "tlbtlbtlb"
        },
        {
            "name": "Rafal Jozefowicz",
            "email": "rafjoz@gmail.com",
            "github_id": "rafaljozefowicz"
        },
        {
            "name": "ilyasu123",
            "github_id": "ilyasu123"
        },
        {
            "name": "Jonathan Gray",
            "github_id": "jonathathan"
        },
        {
            "name": "Avital Oliver",
            "email": "avital@thewe.net",
            "github_id": "avital"
        },
        {
            "name": "Jim Fan",
            "github_id": "LinxiFan"
        },
        {
            "name": "Jens Timmerman",
            "email": "github@caret.be",
            "github_id": "JensTimmerman"
        },
        {
            "name": "jietang",
            "github_id": "jietang"
        },
        {
            "name": "Thomas Wood",
            "email": "odell.wood@gmail.com",
            "github_id": "odellus"
        },
        {
            "name": "Vlad Firoiu",
            "github_id": "vladfi1"
        },
        {
            "name": "Greg Brockman",
            "email": "greg@gregbrockman.com",
            "github_id": "gdb"
        },
        {
            "name": "Gioele Ciaparrone",
            "github_id": "Juggernaut93"
        },
        {
            "name": "Moaaz Sidat",
            "github_id": "moaazsidat"
        },
        {
            "name": "openai-sys-okta-integration",
            "github_id": "openai-sys-okta-integration"
        },
        {
            "name": "Tom B Brown",
            "email": "nottombrown@gmail.com",
            "github_id": "nottombrown"
        },
        {
            "name": "Caleb Elston",
            "github_id": "calebelston"
        },
        {
            "name": "Conchylicultor",
            "email": "etiennefg.pot@gmail.com",
            "github_id": "Conchylicultor"
        },
        {
            "name": "Eddie",
            "email": "edpierce30@gmail.com",
            "github_id": "eddiepierce"
        },
        {
            "name": "jkramar",
            "github_id": "jkramar"
        },
        {
            "name": "futurely",
            "github_id": "futurely"
        },
        {
            "name": "Matvey Ezhov",
            "github_id": "ematvey"
        },
        {
            "name": "Michael Blume",
            "email": "blume.mike@gmail.com",
            "github_id": "MichaelBlume"
        },
        {
            "name": "Taco Cohen",
            "email": "taco.cohen@gmail.com",
            "github_id": "tscohen"
        },
        {
            "name": "Yaroslav Bulatov",
            "email": "yaroslavvb@gmail.com",
            "github_id": "yaroslavvb"
        },
        {
            "name": "Yuriy Dybskiy",
            "email": "yuriy@dybskiy.com",
            "github_id": "html5cat"
        },
        {
            "name": "Adam Bouhenguel",
            "github_id": "ajbouh"
        },
        {
            "name": "baoblackcoal",
            "email": "baoblackcoal@hotmail.com",
            "github_id": "baoblackcoal"
        }
    ],
    "tags": [],
    "description": "A starter agent that can solve a number of universe environments.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/vladfi1/universe-starter-agent",
            "stars": 8,
            "issues": false,
            "readme": "# universe-starter-agent\n\nThe codebase implements a starter agent that can solve a number of `universe` environments.\nIt contains a basic implementation of the [A3C algorithm](https://arxiv.org/abs/1602.01783), adapted for real-time environments.\n\n# Dependencies\n\n* Python 2.7 or 3.5\n* [six](https://pypi.python.org/pypi/six) (for py2/3 compatibility)\n* [TensorFlow](https://www.tensorflow.org/) 0.11\n* [tmux](https://tmux.github.io/) (the start script opens up a tmux session with multiple windows)\n* [htop](https://hisham.hm/htop/) (shown in one of the tmux windows)\n* [gym](https://pypi.python.org/pypi/gym)\n* gym[atari]\n* [universe](https://pypi.python.org/pypi/universe)\n* [opencv-python](https://pypi.python.org/pypi/opencv-python)\n* [numpy](https://pypi.python.org/pypi/numpy)\n* [scipy](https://pypi.python.org/pypi/scipy)\n\n# Getting Started\n\n```\nconda create --name universe-starter-agent python=3.5\nsource activate universe-starter-agent\n\nbrew install tmux htop      # On Linux use sudo apt-get install -y tmux htop\n\npip install gym[atari]\npip install universe\npip install six\npip install tensorflow\nconda install -y -c https://conda.binstar.org/menpo opencv3\nconda install -y numpy\nconda install -y scipy\n```\n\n\nAdd the following to your `.bashrc` so that you'll have the correct environment when the `train.py` script spawns new bash shells\n```source activate universe-starter-agent```\n\n## Atari Pong\n\n`python train.py --num-workers 2 --env-id PongDeterministic-v3 --log-dir /tmp/pong`\n\nThe command above will train an agent on Atari Pong using ALE simulator.\nIt will see two workers that will be learning in parallel (`--num-workers` flag) and will output intermediate results into given directory.\n\nThe code will launch the following processes:\n* worker-0 - a process that runs policy gradient\n* worker-1 - a process identical to process-1, that uses different random noise from the environment\n* ps - the parameter server, which synchronizes the parameters among the different workers\n* tb - a tensorboard process for convenient display of the statistics of learning\n\nOnce you start the training process, it will create a tmux session with a window for each of these processes. You can connect to them by typing `tmux a` in the console.\nOnce in the tmux session, you can see all your windows with `ctrl-b w`.\nTo switch to window number 0, type: `ctrl-b 0`. Look up tmux documentation for more commands.\n\nTo access TensorBoard to see various monitoring metrics of the agent, open [http://localhost:12345/](http://localhost:12345/) in a browser.\n\nUsing 16 workers, the agent should be able to solve `PongDeterministic-v3` (not VNC) within 30 minutes (often less) on an `m4.10xlarge` instance.\nUsing 32 workers, the agent is able to solve the same environment in 10 minutes on an `m4.16xlarge` instance.\nIf you run this experiment on a high-end MacBook Pro, the above job will take just under 2 hours to solve Pong.\n\n![pong](https://github.com/openai/universe-starter-agent/raw/master/imgs/tb_pong.png \"Pong\")\n\nFor best performance, it is recommended for the number of workers to not exceed available number of CPU cores.\n\nYou can stop the experiment with `tmux kill-session` command.\n\n## Playing games over remote desktop\n\nThe main difference with the previous experiment is that now we are going to play the game through VNC protocol.\nThe VNC environments are hosted on the EC2 cloud and have an interface that's different from a conventional Atari Gym\nenvironment;  luckily, with the help of several wrappers (which are used within `envs.py` file)\nthe experience should be similar to the agent as if it was played locally. The problem itself is more difficult\nbecause the observations and actions are delayed due to the latency induced by the network.\n\nMore interestingly, you can also peek at what the agent is doing with a VNCViewer.\n\nNote that the default behavior of `train.py` is to start the remotes on a local machine. Take a look at https://github.com/openai/universe/blob/master/doc/remotes.rst for documentation on managing your remotes. Pass additional `-r` flag to point to pre-existing instances.\n\n### VNC Pong\n\n`python train.py --num-workers 2 --env-id gym-core.PongDeterministic-v3 --log-dir /tmp/vncpong`\n\n_Peeking into the agent's environment with TurboVNC_\n\nYou can use your system viewer as `open vnc://localhost:5900` (or `open vnc://${docker_ip}:5900`) or connect TurboVNC to that ip/port.\nVNC password is `\"openai\"`.\n\n![pong](https://github.com/openai/universe-starter-agent/raw/master/imgs/vnc_pong.png \"Pong over VNC\")\n\n#### Important caveats\n\nOne of the novel challenges in using Universe environments is that\nthey operate in *real time*, and in addition, it takes time for the\nenvironment to transmit the observation to the agent.  This time\ncreates a lag: where the greater the lag, the harder it is to solve\nenvironment with today's RL algorithms.  Thus, to get the best\npossible results it is necessary to reduce the lag, which can be\nachieved by having both the environments and the agent live\non the same high-speed computer network.  So for example, if you have\na fast local network, you could host the environments on one set of\nmachines, and the agent on another machine that can speak to the\nenvironments with low latency.  Alternatively, you can run the\nenvironments and the agent on the same EC2/Azure region.  Other\nconfigurations tend to have greater lag.\n\nTo keep track of your lag, look for the phrase `reaction_time` in\nstderr.  If you run both the agent and the environment on nearby\nmachines on the cloud, your `reaction_time` should be as low as 40ms.\nThe `reaction_time` statistic is printed to stderr because we wrap our\nenvironment with the `Logger` wrapper, as done in\n[here](<https://github.com/openai/universe-starter-agent/blob/master/envs.py#L32>).\n\nGenerally speaking, environments that are most affected by lag are\ngames that place a lot of emphasis on reaction time.  For example,\nthis agent is able to solve VNC Pong\n(`gym-core.PongDeterministic-v3`) in under 2 hours when both the agent\nand the environment are co-located on the cloud, but this agent had\ndifficulty solving VNC Pong when the environment was on the cloud\nwhile the agent was not.  This issue affects environments that place\ngreat emphasis on reaction time.\n\n### A note on tuning\n\nThis implementation has been tuned to do well on VNC Pong, and we do not guarantee\nits performance on other tasks.  It is meant as a starting point.\n\n### Playing flash games\n\nYou may run the following command to launch the agent on the game Neon Race:\n\n`python train.py --num-workers 2 --env-id flashgames.NeonRace-v0 --log-dir /tmp/neonrace`\n\n_What agent sees when playing Neon Race_\n(you can connect to this view via [note](#vnc-pong) above)\n![neon](https://github.com/openai/universe-starter-agent/raw/master/imgs/neon_race.png \"Neon Race\")\n\nGetting 80% of the maximal score takes between 1 and 2 hours with 16 workers, and getting to 100% of the score\ntakes about 12 hours.  Also, flash games are run at 5fps by default, so it should be possible to productively\nuse 16 workers on a machine with 8 (and possibly even 4) cores.\n\n### Next steps\n\nNow that you have seen an example agent, develop agents of your own.  We hope that you will find\ndoing so to be an exciting and an enjoyable task.\n",
            "readme_url": "https://github.com/vladfi1/universe-starter-agent",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Asynchronous Methods for Deep Reinforcement Learning",
            "arxiv": "1602.01783",
            "year": 2016,
            "url": "http://arxiv.org/abs/1602.01783v2",
            "abstract": "We propose a conceptually simple and lightweight framework for deep\nreinforcement learning that uses asynchronous gradient descent for optimization\nof deep neural network controllers. We present asynchronous variants of four\nstandard reinforcement learning algorithms and show that parallel\nactor-learners have a stabilizing effect on training allowing all four methods\nto successfully train neural network controllers. The best performing method,\nan asynchronous variant of actor-critic, surpasses the current state-of-the-art\non the Atari domain while training for half the time on a single multi-core CPU\ninstead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds\non a wide variety of continuous motor control problems as well as on a new task\nof navigating random 3D mazes using a visual input.",
            "authors": [
                "Volodymyr Mnih",
                "Adri\u00e0 Puigdom\u00e8nech Badia",
                "Mehdi Mirza",
                "Alex Graves",
                "Timothy P. Lillicrap",
                "Tim Harley",
                "David Silver",
                "Koray Kavukcuoglu"
            ]
        },
        {
            "title": "six",
            "url": "https://pypi.python.org/pypi/six"
        },
        {
            "title": "TensorFlow",
            "url": "https://www.tensorflow.org/"
        },
        {
            "title": "tmux",
            "url": "https://tmux.github.io/"
        },
        {
            "title": "htop",
            "url": "https://hisham.hm/htop/"
        },
        {
            "title": "gym",
            "url": "https://pypi.python.org/pypi/gym"
        },
        {
            "title": "universe",
            "url": "https://pypi.python.org/pypi/universe"
        },
        {
            "title": "opencv-python",
            "url": "https://pypi.python.org/pypi/opencv-python"
        },
        {
            "title": "numpy",
            "url": "https://pypi.python.org/pypi/numpy"
        },
        {
            "title": "scipy",
            "url": "https://pypi.python.org/pypi/scipy"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "RACE"
            }
        ]
    },
    "domain": {
        "domain_type": "Playing Games",
        "domain_prob": 0.992760357525952
    }
}