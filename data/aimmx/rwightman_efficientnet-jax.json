{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "EfficientNet JAX - Flax Linen and Objax",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "rwightman",
                "owner_type": "User",
                "name": "efficientnet-jax",
                "url": "https://github.com/rwightman/efficientnet-jax",
                "stars": 90,
                "pushed_at": "2021-05-31 22:48:32+00:00",
                "created_at": "2020-09-12 19:06:41+00:00",
                "language": "Python",
                "description": "EfficientNet, MobileNetV3, MobileNetV2, MixNet, etc in JAX w/ Flax Linen and Objax",
                "license": "Apache License 2.0",
                "frameworks": [
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "b6e47617de110dea7ca47e087ff1347cc2646eda",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rwightman/efficientnet-jax/blob/master/.gitignore"
                    }
                },
                "size": 1799
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "96bfb826e3b300fe8c94abc99d3881dff8ba1cb7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rwightman/efficientnet-jax/blob/master/LICENSE"
                    }
                },
                "size": 11343
            },
            {
                "type": "code",
                "name": "docker",
                "sha": "ab8d7c27730fbc0b0a48ca211d896c66f2f21d14",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rwightman/efficientnet-jax/tree/master/docker"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "export_pytorch.py",
                "sha": "db3963e091d6b3df927fc26738b38c30a1be1de1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rwightman/efficientnet-jax/blob/master/export_pytorch.py"
                    }
                },
                "size": 4166
            },
            {
                "type": "code",
                "name": "jeffnet",
                "sha": "ef037942d5a41ccc398f641a78ed8ac5155115fd",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rwightman/efficientnet-jax/tree/master/jeffnet"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "pt_linen_validate.py",
                "sha": "5efae2f5d11fbbd4740bf4846cda39815c5219a7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rwightman/efficientnet-jax/blob/master/pt_linen_validate.py"
                    }
                },
                "size": 5012
            },
            {
                "type": "code",
                "name": "pt_objax_validate.py",
                "sha": "1f8acfa99982a12fbf3a141c9ea0e3e9bdc523b9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rwightman/efficientnet-jax/blob/master/pt_objax_validate.py"
                    }
                },
                "size": 4636
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "36a6ef4e8d8125a9d834458ffa628de38d36a36c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rwightman/efficientnet-jax/blob/master/requirements.txt"
                    }
                },
                "size": 59
            },
            {
                "type": "code",
                "name": "tf_linen_train.py",
                "sha": "6c70c4459da6da51d72f1500814fe2a51864067b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rwightman/efficientnet-jax/blob/master/tf_linen_train.py"
                    }
                },
                "size": 16795
            },
            {
                "type": "code",
                "name": "tf_linen_validate.py",
                "sha": "b8c6dfb0015861d3d9f274ed0311d615b4d9bcb7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rwightman/efficientnet-jax/blob/master/tf_linen_validate.py"
                    }
                },
                "size": 5530
            },
            {
                "type": "code",
                "name": "tf_objax_validate.py",
                "sha": "c73434f58d7e0ef0729ed1961253aafce4ac0731",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rwightman/efficientnet-jax/blob/master/tf_objax_validate.py"
                    }
                },
                "size": 3909
            },
            {
                "type": "code",
                "name": "train_configs",
                "sha": "e0223eeb1033109bceaf519448bb73ddbc32c552",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rwightman/efficientnet-jax/tree/master/train_configs"
                    }
                },
                "num_files": 3
            }
        ]
    },
    "authors": [
        {
            "name": "Ross Wightman",
            "github_id": "rwightman"
        }
    ],
    "tags": [
        "jax",
        "objax",
        "flax",
        "efficientnet",
        "mobilenetv3",
        "mobilenetv2",
        "mixnet",
        "tpu",
        "flax-linen"
    ],
    "description": "EfficientNet, MobileNetV3, MobileNetV2, MixNet, etc in JAX w/ Flax Linen and Objax",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/rwightman/efficientnet-jax",
            "stars": 90,
            "issues": true,
            "readme": "# EfficientNet JAX - Flax Linen and Objax\n\n## Acknowledgements\n\nVerification of training code was made possible with Cloud TPUs via Google's TPU Research Cloud (TRC) (https://www.tensorflow.org/tfrc)\n\n## Intro\nThis is very much a giant steaming work in progress. Jax, jaxlib, and the NN libraries I'm using are shifting week to week.\n\nThis code base currently supports:\n * Flax Linen (https://github.com/google/flax/tree/master/flax/linen) -- for models, validation w/ pretrained weights, and training from scratch\n * Objax (https://github.com/google/objax) -- for model and model validation with pretrained weights\n\nThis is essentially an adaptation of my PyTorch EfficienNet generator code (https://github.com/rwightman/gen-efficientnet-pytorch and also found in https://github.com/rwightman/pytorch-image-models) to JAX.\n\nI started this to\n* learn JAX by working with familiar code / models as a starting point,\n* figure out which JAX modelling interface libraries ('frameworks') I liked,\n* compare the training / inference runtime traits of non-trivial models across combinations of PyTorch, JAX, GPU and TPU in order to drive cost optimizations for scaling up of future projects\n\nWhere are we at:\n* Training works on single node, multi-GPU and TPU v3-8 for Flax Linen variants w/ Tensorflow Datasets based pipeline\n* The Objax and Flax Linen (nn.compact) variants of models are working (for inference) \n* Weights are ported from PyTorch (my timm training) and Tensorflow (original paper author releases) and are organized in zoo of sorts (borrowed PyTorch code) \n* Tensorflow and PyTorch data pipeline based validation scripts work with models and weights. For PT pipeline with PT models and TF pipeline with TF models the results are pretty much exact.\n\nTODO:\n- [x] Fix model weight inits (working for Flax Linen variants)\n- [x] Fix dropout/drop path impl and other training specifics (verified for Flax Linen variants)\n- [ ] Add more instructions / help in the README on how to get an optimal environment with JAX up and running (with GPU support)\n- [x] Add basic training code. The main point of this is to scale up training.\n- [ ] Add more advance data augmentation pipeline \n- [ ] Training on lots of GPUs\n- [ ] Training on lots of TPUs\n\nSome odd things:\n* Objax layers are reimplemented to make my initial work easier, scratch some itches, make more consistent with PyTorch (because why not?)\n* Flax Linen layers are by default fairly consistent with Tensorflow (left as is)\n* I use wrappers around Flax Linen layers for some argument consistency and reduced visual noise (no redundant tuples)\n* I made a 'LIKE' padding mode, sort of like 'SAME' but different, hence the name. It calculates symmetric padding for PyTorch models.\n* Models with Tensorflow 'SAME' padding and TF origin weights are prefixed with `tf_`. Models with PyTorch trained weights and symmetric PyTorch style padding ('LIKE' here) are prefixed with `pt_`\n* I use `pt` and `tf` to refer to PyTorch and Tensorflow for both the models and environments. These two do not need to be used together. `pt` models with 'LIKE' padding will work fine running in a Tensorflow based environment and vice versa. I did this to show the full flexibility here, that one can use JAX models with PyTorch data pipelines and datasets or with Tensorflow based data pipelines and TFDS. \n\n## Models\n\nSupported models and their paper's\n* EfficientNet NoisyStudent (B0-B7, L2) - https://arxiv.org/abs/1911.04252\n* EfficientNet AdvProp (B0-B8) - https://arxiv.org/abs/1911.09665\n* EfficientNet (B0-B7) - https://arxiv.org/abs/1905.11946\n* EfficientNet-EdgeTPU (S, M, L) - https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html\n* MixNet - https://arxiv.org/abs/1907.09595\n* MobileNet-V3 - https://arxiv.org/abs/1905.02244\n* MobileNet-V2 - https://arxiv.org/abs/1801.04381\n* MNASNet B1, A1 (Squeeze-Excite), and Small - https://arxiv.org/abs/1807.11626\n* Single-Path NAS - https://arxiv.org/abs/1904.02877\n* FBNet-C - https://arxiv.org/abs/1812.03443\n\nModels by their config name w/ valid pretrained weights that should be working here:\n```\npt_mnasnet_100\npt_semnasnet_100\npt_mobilenetv2_100\npt_mobilenetv2_110d\npt_mobilenetv2_120d\npt_mobilenetv2_140\npt_fbnetc_100\npt_spnasnet_100\npt_efficientnet_b0\npt_efficientnet_b1\npt_efficientnet_b2\npt_efficientnet_b3\ntf_efficientnet_b0\ntf_efficientnet_b1\ntf_efficientnet_b2\ntf_efficientnet_b3\ntf_efficientnet_b4\ntf_efficientnet_b5\ntf_efficientnet_b6\ntf_efficientnet_b7\ntf_efficientnet_b8\ntf_efficientnet_b0_ap\ntf_efficientnet_b1_ap\ntf_efficientnet_b2_ap\ntf_efficientnet_b3_ap\ntf_efficientnet_b4_ap\ntf_efficientnet_b5_ap\ntf_efficientnet_b6_ap\ntf_efficientnet_b7_ap\ntf_efficientnet_b8_ap\ntf_efficientnet_b0_ns\ntf_efficientnet_b1_ns\ntf_efficientnet_b2_ns\ntf_efficientnet_b3_ns\ntf_efficientnet_b4_ns\ntf_efficientnet_b5_ns\ntf_efficientnet_b6_ns\ntf_efficientnet_b7_ns\ntf_efficientnet_l2_ns_475\ntf_efficientnet_l2_ns\npt_efficientnet_es\npt_efficientnet_em\ntf_efficientnet_es\ntf_efficientnet_em\ntf_efficientnet_el\npt_efficientnet_lite0\ntf_efficientnet_lite0\ntf_efficientnet_lite1\ntf_efficientnet_lite2\ntf_efficientnet_lite3\ntf_efficientnet_lite4\npt_mixnet_s\npt_mixnet_m\npt_mixnet_l\npt_mixnet_xl\ntf_mixnet_s\ntf_mixnet_m\ntf_mixnet_l\npt_mobilenetv3_large_100\ntf_mobilenetv3_large_075\ntf_mobilenetv3_large_100\ntf_mobilenetv3_large_minimal_100\ntf_mobilenetv3_small_075\ntf_mobilenetv3_small_100\ntf_mobilenetv3_small_minimal_100\n```\n\n## Environment\n\nWorking with JAX I've found the best approach for having a working GPU compatible environment that performs well is to use Docker containers based on the latest NVIDIA NGC releases. I've found it challenging or flaky getting local conda/pip venvs or Tensorflow docker containers working well with good GPU performance, proper NCCL distributed support, etc. I use CPU JAX install in conda env for dev/debugging.\n\n### Dockerfiles\n\nThere are several container definitions in `docker/`. They use NGC containers as their parent image so you'll need to be setup to pull NGC containers: https://www.nvidia.com/en-us/gpu-cloud/containers/ . I'm currently using recent NGC containers w/ CUDA 11.1 support, the host system will need a very recent NVIDIA driver to support this but doesn't need a matching CUDA 11.1 / cuDNN 8 install.\n\nCurrent dockerfiles:\n* `pt_git.Dockerfile` - PyTorch 20.12 NGC as parent, CUDA 11.1, cuDNN 8. git (source install) of jaxlib, jax, objax, and flax.\n* `pt_pip.Dockerfile` - PyTorch 20.12 NGC as parent, CUDA 11.1, cuDNN 8. pip (latest ver) install of jaxlib, jax, objax, and flax.\n* `tf_git.Dockerfile` - Tensorflow 2 21.02 NGC as parent, CUDA 11.2, cuDNN 8. git (source install) of jaxlib, jax, objax, and flax.\n* `tf_pip.Dockerfile` - Tensorflow 2 21.02 NGC as parent, CUDA 11.2, cuDNN 8. pip (latest ver) install of jaxlib, jax, objax, and flax.\n\nThe 'git' containers take some time to build jaxlib, they pull the masters of all respective repos so are up to the bleeding edge but more likely to have possible regression or incompatibilities that go with that. The pip install containers are quite a bit quicker to get up and running, based on the latest pip versions of all repos.\n\n### Docker Usage (GPU)\n\n1. Make sure you have a recent version of docker and the NVIDIA Container Toolkit setup (https://github.com/NVIDIA/nvidia-docker) \n2. Build the container `docker build -f docker/tf_pip.Dockerfile -t jax_tf_pip .`\n3. Run the container, ideally map jeffnet and datasets (ImageNet) into the container\n    * For tf containers, `docker run --gpus all -it -v /path/to/tfds/root:/data/ -v /path/to/efficientnet-jax/:/workspace/jeffnet --rm --ipc=host jax_tf_pip`\n    * For pt containers, `docker run --gpus all -it -v /path/to/imagenet/root:/data/ -v /path/to/efficientnet-jax/:/workspace/jeffnet --rm --ipc=host jax_pt_pip`\n4. Model validation w/ pretrained weights (once inside running container):\n    * For tf, in `worskpace/jeffnet`, `python tf_linen_validate.py /data/ --model tf_efficientnet_b0_ns`\n    * For pt, in `worskpace/jeffnet`, `python pt_objax_validate.py /data/validation --model pt_efficientnet_b0`\n5. Training (within container)\n    * In `worskpace/jeffnet`, `tf_linen_train.py --config train_configs/tf_efficientnet_b0-gpu_24gb_x2.py --config.data_dir /data`\n\n### TPU\n\nI've successfully used this codebase on TPU VM environments as is. Any of the `tpu_x8` training configs should work out of the box on a v3-8 TPU. I have not tackled training with TPU Pods.\n",
            "readme_url": "https://github.com/rwightman/efficientnet-jax",
            "frameworks": [
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Searching for MobileNetV3",
            "arxiv": "1905.02244",
            "year": 2019,
            "url": "http://arxiv.org/abs/1905.02244v5",
            "abstract": "We present the next generation of MobileNets based on a combination of\ncomplementary search techniques as well as a novel architecture design.\nMobileNetV3 is tuned to mobile phone CPUs through a combination of\nhardware-aware network architecture search (NAS) complemented by the NetAdapt\nalgorithm and then subsequently improved through novel architecture advances.\nThis paper starts the exploration of how automated search algorithms and\nnetwork design can work together to harness complementary approaches improving\nthe overall state of the art. Through this process we create two new MobileNet\nmodels for release: MobileNetV3-Large and MobileNetV3-Small which are targeted\nfor high and low resource use cases. These models are then adapted and applied\nto the tasks of object detection and semantic segmentation. For the task of\nsemantic segmentation (or any dense pixel prediction), we propose a new\nefficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling\n(LR-ASPP). We achieve new state of the art results for mobile classification,\ndetection and segmentation. MobileNetV3-Large is 3.2\\% more accurate on\nImageNet classification while reducing latency by 15\\% compared to MobileNetV2.\nMobileNetV3-Small is 4.6\\% more accurate while reducing latency by 5\\% compared\nto MobileNetV2. MobileNetV3-Large detection is 25\\% faster at roughly the same\naccuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\\%\nfaster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.",
            "authors": [
                "Andrew Howard",
                "Mark Sandler",
                "Grace Chu",
                "Liang-Chieh Chen",
                "Bo Chen",
                "Mingxing Tan",
                "Weijun Wang",
                "Yukun Zhu",
                "Ruoming Pang",
                "Vijay Vasudevan",
                "Quoc V. Le",
                "Hartwig Adam"
            ]
        },
        {
            "title": "Adversarial Examples Improve Image Recognition",
            "arxiv": "1911.09665",
            "year": 2019,
            "url": "http://arxiv.org/abs/1911.09665v2",
            "abstract": "Adversarial examples are commonly viewed as a threat to ConvNets. Here we\npresent an opposite perspective: adversarial examples can be used to improve\nimage recognition models if harnessed in the right manner. We propose AdvProp,\nan enhanced adversarial training scheme which treats adversarial examples as\nadditional examples, to prevent overfitting. Key to our method is the usage of\na separate auxiliary batch norm for adversarial examples, as they have\ndifferent underlying distributions to normal examples.\n  We show that AdvProp improves a wide range of models on various image\nrecognition tasks and performs better when the models are bigger. For instance,\nby applying AdvProp to the latest EfficientNet-B7 [28] on ImageNet, we achieve\nsignificant improvements on ImageNet (+0.7%), ImageNet-C (+6.5%), ImageNet-A\n(+7.0%), Stylized-ImageNet (+4.8%). With an enhanced EfficientNet-B8, our\nmethod achieves the state-of-the-art 85.5% ImageNet top-1 accuracy without\nextra data. This result even surpasses the best model in [20] which is trained\nwith 3.5B Instagram images (~3000X more than ImageNet) and ~9.4X more\nparameters. Models are available at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.",
            "authors": [
                "Cihang Xie",
                "Mingxing Tan",
                "Boqing Gong",
                "Jiang Wang",
                "Alan Yuille",
                "Quoc V. Le"
            ]
        },
        {
            "title": "FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
            "arxiv": "1812.03443",
            "year": 2018,
            "url": "http://arxiv.org/abs/1812.03443v3",
            "abstract": "Designing accurate and efficient ConvNets for mobile devices is challenging\nbecause the design space is combinatorially large. Due to this, previous neural\narchitecture search (NAS) methods are computationally expensive. ConvNet\narchitecture optimality depends on factors such as input resolution and target\ndevices. However, existing approaches are too expensive for case-by-case\nredesigns. Also, previous work focuses primarily on reducing FLOPs, but FLOP\ncount does not always reflect actual latency. To address these, we propose a\ndifferentiable neural architecture search (DNAS) framework that uses\ngradient-based methods to optimize ConvNet architectures, avoiding enumerating\nand training individual architectures separately as in previous methods.\nFBNets, a family of models discovered by DNAS surpass state-of-the-art models\nboth designed manually and generated automatically. FBNet-B achieves 74.1%\ntop-1 accuracy on ImageNet with 295M FLOPs and 23.1 ms latency on a Samsung S8\nphone, 2.4x smaller and 1.5x faster than MobileNetV2-1.3 with similar accuracy.\nDespite higher accuracy and lower latency than MnasNet, we estimate FBNet-B's\nsearch cost is 420x smaller than MnasNet's, at only 216 GPU-hours. Searched for\ndifferent resolutions and channel sizes, FBNets achieve 1.5% to 6.4% higher\naccuracy than MobileNetV2. The smallest FBNet achieves 50.2% accuracy and 2.9\nms latency (345 frames per second) on a Samsung S8. Over a Samsung-optimized\nFBNet, the iPhone-X-optimized model achieves a 1.4x speedup on an iPhone X.",
            "authors": [
                "Bichen Wu",
                "Xiaoliang Dai",
                "Peizhao Zhang",
                "Yanghan Wang",
                "Fei Sun",
                "Yiming Wu",
                "Yuandong Tian",
                "Peter Vajda",
                "Yangqing Jia",
                "Kurt Keutzer"
            ]
        },
        {
            "title": "Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours",
            "arxiv": "1904.02877",
            "year": 2019,
            "url": "http://arxiv.org/abs/1904.02877v1",
            "abstract": "Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.",
            "authors": [
                "Dimitrios Stamoulis",
                "Ruizhou Ding",
                "Di Wang",
                "Dimitrios Lymberopoulos",
                "Bodhi Priyantha",
                "Jie Liu",
                "Diana Marculescu"
            ]
        },
        {
            "title": "MixConv: Mixed Depthwise Convolutional Kernels",
            "arxiv": "1907.09595",
            "year": 2019,
            "url": "http://arxiv.org/abs/1907.09595v3",
            "abstract": "Depthwise convolution is becoming increasingly popular in modern efficient\nConvNets, but its kernel size is often overlooked. In this paper, we\nsystematically study the impact of different kernel sizes, and observe that\ncombining the benefits of multiple kernel sizes can lead to better accuracy and\nefficiency. Based on this observation, we propose a new mixed depthwise\nconvolution (MixConv), which naturally mixes up multiple kernel sizes in a\nsingle convolution. As a simple drop-in replacement of vanilla depthwise\nconvolution, our MixConv improves the accuracy and efficiency for existing\nMobileNets on both ImageNet classification and COCO object detection. To\ndemonstrate the effectiveness of MixConv, we integrate it into AutoML search\nspace and develop a new family of models, named as MixNets, which outperform\nprevious mobile models including MobileNetV2 [20] (ImageNet top-1 accuracy\n+4.2%), ShuffleNetV2 [16] (+3.5%), MnasNet [26] (+1.3%), ProxylessNAS [2]\n(+2.2%), and FBNet [27] (+2.0%). In particular, our MixNet-L achieves a new\nstate-of-the-art 78.9% ImageNet top-1 accuracy under typical mobile settings\n(<600M FLOPS). Code is at https://github.com/\ntensorflow/tpu/tree/master/models/official/mnasnet/mixnet",
            "authors": [
                "Mingxing Tan",
                "Quoc V. Le"
            ]
        },
        {
            "title": "Self-training with Noisy Student improves ImageNet classification",
            "arxiv": "1911.04252",
            "year": 2019,
            "url": "http://arxiv.org/abs/1911.04252v4",
            "abstract": "We present Noisy Student Training, a semi-supervised learning approach that\nworks well even when labeled data is abundant. Noisy Student Training achieves\n88.4% top-1 accuracy on ImageNet, which is 2.0% better than the\nstate-of-the-art model that requires 3.5B weakly labeled Instagram images. On\nrobustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to\n83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces\nImageNet-P mean flip rate from 27.8 to 12.2.\n  Noisy Student Training extends the idea of self-training and distillation\nwith the use of equal-or-larger student models and noise added to the student\nduring learning. On ImageNet, we first train an EfficientNet model on labeled\nimages and use it as a teacher to generate pseudo labels for 300M unlabeled\nimages. We then train a larger EfficientNet as a student model on the\ncombination of labeled and pseudo labeled images. We iterate this process by\nputting back the student as the teacher. During the learning of the student, we\ninject noise such as dropout, stochastic depth, and data augmentation via\nRandAugment to the student so that the student generalizes better than the\nteacher. Models are available at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.\nCode is available at https://github.com/google-research/noisystudent.",
            "authors": [
                "Qizhe Xie",
                "Minh-Thang Luong",
                "Eduard Hovy",
                "Quoc V. Le"
            ]
        },
        {
            "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
            "arxiv": "1801.04381",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.04381v4",
            "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that\nimproves the state of the art performance of mobile models on multiple tasks\nand benchmarks as well as across a spectrum of different model sizes. We also\ndescribe efficient ways of applying these mobile models to object detection in\na novel framework we call SSDLite. Additionally, we demonstrate how to build\nmobile semantic segmentation models through a reduced form of DeepLabv3 which\nwe call Mobile DeepLabv3.\n  The MobileNetV2 architecture is based on an inverted residual structure where\nthe input and output of the residual block are thin bottleneck layers opposite\nto traditional residual models which use expanded representations in the input\nan MobileNetV2 uses lightweight depthwise convolutions to filter features in\nthe intermediate expansion layer. Additionally, we find that it is important to\nremove non-linearities in the narrow layers in order to maintain\nrepresentational power. We demonstrate that this improves performance and\nprovide an intuition that led to this design. Finally, our approach allows\ndecoupling of the input/output domains from the expressiveness of the\ntransformation, which provides a convenient framework for further analysis. We\nmeasure our performance on Imagenet classification, COCO object detection, VOC\nimage segmentation. We evaluate the trade-offs between accuracy, and number of\noperations measured by multiply-adds (MAdd), as well as the number of\nparameters",
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ]
        },
        {
            "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "arxiv": "1905.11946",
            "year": 2019,
            "url": "http://arxiv.org/abs/1905.11946v5",
            "abstract": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed\nresource budget, and then scaled up for better accuracy if more resources are\navailable. In this paper, we systematically study model scaling and identify\nthat carefully balancing network depth, width, and resolution can lead to\nbetter performance. Based on this observation, we propose a new scaling method\nthat uniformly scales all dimensions of depth/width/resolution using a simple\nyet highly effective compound coefficient. We demonstrate the effectiveness of\nthis method on scaling up MobileNets and ResNet.\n  To go even further, we use neural architecture search to design a new\nbaseline network and scale it up to obtain a family of models, called\nEfficientNets, which achieve much better accuracy and efficiency than previous\nConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3%\ntop-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on\ninference than the best existing ConvNet. Our EfficientNets also transfer well\nand achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%),\nand 3 other transfer learning datasets, with an order of magnitude fewer\nparameters. Source code is at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.",
            "authors": [
                "Mingxing Tan",
                "Quoc V. Le"
            ]
        },
        {
            "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
            "arxiv": "1807.11626",
            "year": 2018,
            "url": "http://arxiv.org/abs/1807.11626v3",
            "abstract": "Designing convolutional neural networks (CNN) for mobile devices is\nchallenging because mobile models need to be small and fast, yet still\naccurate. Although significant efforts have been dedicated to design and\nimprove mobile CNNs on all dimensions, it is very difficult to manually balance\nthese trade-offs when there are so many architectural possibilities to\nconsider. In this paper, we propose an automated mobile neural architecture\nsearch (MNAS) approach, which explicitly incorporate model latency into the\nmain objective so that the search can identify a model that achieves a good\ntrade-off between accuracy and latency. Unlike previous work, where latency is\nconsidered via another, often inaccurate proxy (e.g., FLOPS), our approach\ndirectly measures real-world inference latency by executing the model on mobile\nphones. To further strike the right balance between flexibility and search\nspace size, we propose a novel factorized hierarchical search space that\nencourages layer diversity throughout the network. Experimental results show\nthat our approach consistently outperforms state-of-the-art mobile CNN models\nacross multiple vision tasks. On the ImageNet classification task, our MnasNet\nachieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x\nfaster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than\nNASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP\nquality than MobileNets for COCO object detection. Code is at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/mnasnet",
            "authors": [
                "Mingxing Tan",
                "Bo Chen",
                "Ruoming Pang",
                "Vijay Vasudevan",
                "Mark Sandler",
                "Andrew Howard",
                "Quoc V. Le"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "Cityscapes"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9995391829948226,
        "task": "Image Classification",
        "task_prob": 0.6180718880073751
    }
}