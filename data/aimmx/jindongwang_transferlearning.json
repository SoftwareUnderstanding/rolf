{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "\u8fc1\u79fb\u5b66\u4e60 Transfer Learning",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "jindongwang",
                "owner_type": "User",
                "name": "transferlearning",
                "url": "https://github.com/jindongwang/transferlearning",
                "stars": 9011,
                "pushed_at": "2022-03-25 09:30:48+00:00",
                "created_at": "2017-04-30 11:32:21+00:00",
                "language": "Python",
                "description": "Transfer learning / domain adaptation / domain generalization / multi-task learning etc. Papers, codes, datasets, applications, tutorials.-\u8fc1\u79fb\u5b66\u4e60",
                "license": "MIT License",
                "frameworks": [
                    "scikit-learn",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitattributes",
                "sha": "b634d85f0161af823e093eaf9605f06fbf84cd36",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jindongwang/transferlearning/blob/master/.gitattributes"
                    }
                },
                "size": 42
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "c158ca4060bec58af4a74a0505e16de11f15e872",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jindongwang/transferlearning/tree/master/.github"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "d7acdb481a00b22a158c0ae91793a556b4c3c334",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jindongwang/transferlearning/blob/master/.gitignore"
                    }
                },
                "size": 74
            },
            {
                "type": "code",
                "name": "CONTRIBUTING.md",
                "sha": "32d9a2992188b02684d1a011e9b6ea261fb5e5aa",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jindongwang/transferlearning/blob/master/CONTRIBUTING.md"
                    }
                },
                "size": 714
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "0ed54881e6f800f063178176d408f7ec815d77dd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jindongwang/transferlearning/blob/master/LICENSE"
                    }
                },
                "size": 1069
            },
            {
                "type": "code",
                "name": "code",
                "sha": "b4df6093758e1ffbb06f80158bf04263ef443d71",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jindongwang/transferlearning/tree/master/code"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "data",
                "sha": "a37a16f67ebff27fe26c50d0ad42f2c7cf764b48",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jindongwang/transferlearning/tree/master/data"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "doc",
                "sha": "51c6cc60afb71428fbc117f713210cabd9be441e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jindongwang/transferlearning/tree/master/doc"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "8a33e0deeb12a3bfc0d5368a30ee0661ece2975a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jindongwang/transferlearning/tree/master/docs"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "notebooks",
                "sha": "292060761c4e125c46dbde83ff8f57c007909684",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jindongwang/transferlearning/tree/master/notebooks"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "png",
                "sha": "506f77ac7b6e1e92b98e3e2ec2fbe7e713a1708a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jindongwang/transferlearning/tree/master/png"
                    }
                },
                "num_files": 6
            }
        ]
    },
    "authors": [
        {
            "name": "Jindong Wang",
            "email": "jindongwang@outlook.com",
            "github_id": "jindongwang"
        },
        {
            "name": "HWX.",
            "email": "641686859@qq.com",
            "github_id": "houwenxin"
        },
        {
            "name": "Yuntao Du",
            "email": "duyuntao@smail.nju.edu.cn",
            "github_id": "yuntaodu"
        },
        {
            "name": "Yongchun Zhu",
            "email": "1299192934@qq.com",
            "github_id": "easezyc"
        },
        {
            "name": "lw0517",
            "github_id": "lw0517"
        },
        {
            "name": "Yuanliang Sun",
            "github_id": "sun254"
        },
        {
            "name": "Orvin Demsy",
            "github_id": "orvindemsy"
        },
        {
            "name": "PatrickZH",
            "github_id": "PatrickZH"
        },
        {
            "name": "Kaiyang",
            "github_id": "KaiyangZhou"
        },
        {
            "name": "Werner Zellinger",
            "github_id": "wzell"
        },
        {
            "name": "zengxianfang",
            "email": "zzlongjuanfeng@zju.edu.cn",
            "github_id": "Zzlongjuanfeng"
        },
        {
            "name": "Mrzhangxiaohua",
            "github_id": "Mrzhangxiaohua"
        },
        {
            "name": "Min-Hung (Steve) Chen",
            "email": "vitec6@gmail.com",
            "github_id": "cmhungsteve"
        },
        {
            "name": "Vinod K Kurmi ",
            "email": "vinodkkurmi@gmail.com",
            "github_id": "vinodkkurmi"
        },
        {
            "name": "mcj",
            "github_id": "mengchuangji"
        },
        {
            "name": "Cven Wu",
            "email": "yirufeng@foxmail.com",
            "github_id": "sivanWu0222"
        },
        {
            "name": "Lu Yan",
            "email": "jasonengineer@hotmail.com",
            "github_id": "YanLu-nyu"
        },
        {
            "name": "Chang Liu",
            "email": "changliu@microsoft.com",
            "github_id": "changliu00"
        },
        {
            "name": "Christian Clauss",
            "email": "cclauss@me.com",
            "github_id": "cclauss"
        },
        {
            "name": "Imgbot",
            "email": "help@imgbot.net",
            "github_id": "ImgBotApp"
        },
        {
            "name": "Shin-Fu Wu",
            "email": "fxp61005@gmail.com",
            "github_id": "KodeWorker"
        },
        {
            "name": "Sun Haozhe",
            "email": "sunhaozhe275940200@gmail.com",
            "github_id": "SunHaozhe"
        },
        {
            "name": "Vincent",
            "email": "V.Vercruyssen@gmail.com",
            "github_id": "Vincent-Vercruyssen"
        },
        {
            "name": "biaozhou",
            "email": "www.815152974@qq.com",
            "github_id": "Dr-Zhou"
        },
        {
            "name": "dependabot[bot]",
            "github_id": "dependabot[bot]"
        },
        {
            "name": "Kevin Ke-Yun Lin",
            "email": "kevinlin311.tw@gmail.com",
            "github_id": "kevinlin311tw"
        },
        {
            "name": "lizhong",
            "github_id": "zhonglii"
        },
        {
            "name": "Seung Min Lee",
            "github_id": "postBG"
        },
        {
            "name": "tringwald",
            "github_id": "tringwald"
        },
        {
            "name": "Zhen",
            "email": "hi@wogong.net",
            "github_id": "wogong"
        }
    ],
    "tags": [
        "transferlearning",
        "domain-adaptation",
        "transfer-learning",
        "survey",
        "deep-learning",
        "generalization",
        "few-shot",
        "tutorial-code",
        "theory",
        "papers",
        "few-shot-learning",
        "meta-learning",
        "domain-generalization",
        "representation-learning",
        "unsupervised-learning",
        "machine-learning",
        "self-supervised-learning",
        "paper",
        "style-transfer",
        "domain-adaption"
    ],
    "description": "Transfer learning / domain adaptation / domain generalization / multi-task learning etc. Papers, codes, datasets, applications, tutorials.-\u8fc1\u79fb\u5b66\u4e60",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/jindongwang/transferlearning",
            "stars": 9011,
            "issues": true,
            "readme": "# \u8fc1\u79fb\u5b66\u4e60 Transfer Learning  \n\n<h1 align=\"center\">\n  <br>\n  <img src=\"png/logo.jpg\" alt=\"Transfer Leanring\" width=\"500\">\n</h1>\n\n<h4 align=\"center\">Everything about Transfer Learning.</h4>\n\n<p align=\"center\">\n  <strong><a href=\"#0papers-\u8bba\u6587\">Papers</a></strong> \u2022\n  <strong><a href=\"#1introduction-and-tutorials-\u7b80\u4ecb\u4e0e\u6559\u7a0b\">Tutorials</a></strong> \u2022\n  <a href=\"#2transfer-learning-areas-and-papers-\u7814\u7a76\u9886\u57df\u4e0e\u76f8\u5173\u8bba\u6587\">Research areas</a> \u2022\n  <a href=\"#3theory-and-survey-\u7406\u8bba\u4e0e\u7efc\u8ff0\">Theory</a> \u2022\n  <a href=\"#3theory-and-survey-\u7406\u8bba\u4e0e\u7efc\u8ff0\">Survey</a> \u2022\n  <strong><a href=\"#4code-\u4ee3\u7801\">Code</a></strong> \u2022\n  <strong><a href=\"#7datasets-and-benchmarks-\u6570\u636e\u96c6\u4e0e\u8bc4\u6d4b\u7ed3\u679c\">Dataset & benchmark</a></strong>\n</p>\n<p align=\"center\">\n  <a href=\"#6transfer-learning-thesis-\u7855\u535a\u58eb\u8bba\u6587\">Thesis</a> \u2022\n  <a href=\"#5transfer-learning-scholars-\u8457\u540d\u5b66\u8005\">Scholars</a> \u2022\n  <a href=\"#8transfer-learning-challenges-\u8fc1\u79fb\u5b66\u4e60\u6bd4\u8d5b\">Contests</a> \u2022\n  <a href=\"#journals-and-conferences\">Journal/conference</a> \u2022\n  <a href=\"#applications-\u8fc1\u79fb\u5b66\u4e60\u5e94\u7528\">Applications</a> \u2022\n  <a href=\"#other-resources-\u5176\u4ed6\u8d44\u6e90\">Others</a> \u2022\n  <a href=\"#contributing-\u6b22\u8fce\u53c2\u4e0e\u8d21\u732e\">Contributing</a>\n</p>\n\n**Widely used by top conferences and journals:** \n- Conferences: [[NeurIPS'21](https://proceedings.neurips.cc/paper/2021/file/731b03008e834f92a03085ef47061c4a-Paper.pdf)] [[IJCAI'21](https://arxiv.org/abs/2103.03097)] [[ESEC/FSE'20](https://dl.acm.org/doi/abs/10.1145/3368089.3409696)] [[IJCNN'20](https://ieeexplore.ieee.org/abstract/document/9207556)] [[ACMMM'18](https://dl.acm.org/doi/abs/10.1145/3240508.3240512)] [[ICME'19](https://ieeexplore.ieee.org/abstract/document/8784776/)]\n- Journals: [[ACM TIST](https://dl.acm.org/doi/abs/10.1145/3360309)] [[Information sciences](https://www.sciencedirect.com/science/article/pii/S0020025520308458)] [[Neurocomputing](https://www.sciencedirect.com/science/article/pii/S0925231221007025)] [[IEEE Transactions on Cognitive and Developmental Systems](https://ieeexplore.ieee.org/abstract/document/9659817)]\n\n```\n@Misc{transferlearning.xyz,\nhowpublished = {\\url{http://transferlearning.xyz}},   \ntitle = {Everything about Transfer Learning and Domain Adapation},  \nauthor = {Wang, Jindong and others}  \n}  \n```\n\n[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) [![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT) [![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg)](https://github.com/996icu/996.ICU/blob/master/LICENSE) [![996.icu](https://img.shields.io/badge/link-996.icu-red.svg)](https://996.icu) \n\nRelated repos\uff1a[[TorchSSL: a unified library for semi-supervised learning](https://github.com/TorchSSL/TorchSSL)] | [[Activity recognition](https://github.com/jindongwang/activityrecognition)]\uff5c[[Machine learning](https://github.com/jindongwang/MachineLearning)]\n\n- - -\n\n**NOTE:** You can directly open the code in [Gihub Codespaces](https://docs.github.com/en/codespaces/getting-started/quickstart#introduction) on the web to run them without downloading! Also, try [github.dev](https://github.dev/jindongwang/transferlearning).\n\n## 0.Papers (\u8bba\u6587)\n\n[Awesome transfer learning papers (\u8fc1\u79fb\u5b66\u4e60\u6587\u7ae0\u6c47\u603b)](https://github.com/jindongwang/transferlearning/tree/master/doc/awesome_paper.md)\n\n- [Paperweekly](http://www.paperweekly.site/collections/231/papers): A website to recommend and read paper notes\n\n**Latest papers**: \n\n- By topic: [doc/awesome_papers.md](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md)\n- By date: [[2022-02](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#2022-02)] [[2022-01](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#2022-01)] [[2021-12](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#2021-12)] [[2021-11](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#2021-11)] [[2021-10](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#2021-10)] [[2021-09](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#2021-09)] [[2021-08](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#2021-08)] [[2021-07](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#2021-07)]\n\n\n*Updated at 2022-03-23:*\n\n- [Gated Domain-Invariant Feature Disentanglement for Domain Generalizable Object Detection](https://arxiv.org/abs/2203.11432)\n  - Channel masking for domain generalization object detection\n  - \u901a\u8fc7\u4e00\u4e2agate\u63a7\u5236channel masking\u8fdb\u884cobject detection DG\n\n- [A Broad Study of Pre-training for Domain Generalization and Adaptation](https://arxiv.org/abs/2203.11819)\n  - A broad study of pre-training models for DA and DG\n  - \u5927\u91cf\u7684\u5b9e\u9a8c\u8fdb\u884cDA\u548cDG\n\n- ISPASS-22 [Benchmarking Test-Time Unsupervised Deep Neural Network Adaptation on Edge Devices](https://arxiv.org/abs/2203.11295)\n  - Benchmarking test-time adaptation for edge devices\n  - \u5728\u7aef\u8bbe\u5907\u4e0a\u8bc4\u6d4btest-time adaptation\u7b97\u6cd5\n\n- [Multi-Source Domain Adaptation Based on Federated Knowledge Alignment](https://arxiv.org/abs/2203.11635)\n  - Multi-source domain adaptation\n  - \u591a\u6e90\u57df\u81ea\u9002\u5e94\n\n- [Improving Generalization in Federated Learning by Seeking Flat Minima](https://arxiv.org/abs/2203.11834)\n  - Seeking flat minima for domain generalization in federated learning\n  - \u901a\u8fc7\u5bfb\u627e\u5e73\u5766\u503c\u8fdb\u884c\u8054\u90a6\u5b66\u4e60\u9886\u57df\u6cdb\u5316\n  \n\n*Updated at 2022-03-18:*\n\n- CVPR-22 [Decoupled Knowledge Distillation](https://arxiv.org/abs/2203.08679)\n  - Decoupled knowledge distillation\n  - \u89e3\u8026\u7684\u77e5\u8bc6\u84b8\u998f\n\n- [SemiPFL: Personalized Semi-Supervised Federated Learning Framework for Edge Intelligence](https://arxiv.org/abs/2203.08176)\n  - Personalized federated learning\n  - \u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\n\n*Updated at 2022-03-14:*\n\n- ICSE-22 [ReMoS: Reducing Defect Inheritance in Transfer Learning via Relevant Model Slicing](https://link.zhihu.com/?target=https%3A//jd92.wang/assets/files/icse22-remos.pdf) | [Code](https://github.com/ziqi-zhang/ReMoS_artifact) | [Blog](https://zhuanlan.zhihu.com/p/446453487) | [Video](https://www.bilibili.com/video/BV1mi4y1C7bP)\n  - Safe transfer learning by reducing defect inheritance\n  - \u5b89\u5168\u8fc1\u79fb\u5b66\u4e60\u7684\u6700\u65b0\u5de5\u4f5c\n\n*Updated at 2022-03-08:*\n\n- ACL-22 [Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with Articulatory Features](https://arxiv.org/abs/2203.03191)\n  - Language-agnostic meta-learning for TTS\n  - \u8bed\u8a00\u65e0\u5173\u7684\u5143\u5b66\u4e60\u7528\u4e8eTTS\n\n- [Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models](https://arxiv.org/abs/2203.03131)\n  - Adapt unfamiliar inputs to frozen pretrained models\n  - \u8ba9\u56fa\u5b9a\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u914d\u4e0d\u719f\u6089\u7684\u8f93\u5165\n\n- [One Model, Multiple Tasks: Pathways for Natural Language Understanding](https://arxiv.org/abs/2203.03312)\n  - Pathways for natural language understanding\n  - \u4f7f\u7528\u4e00\u4e2amodel\u7528\u4e8e\u6240\u6709NLP\u4efb\u52a1\n\n- [Pre-trained Token-replaced Detection Model as Few-shot Learner](https://arxiv.org/abs/2203.03235)\n  - Pre-trained token-replaced detection model as few-shot learner\n  - \u9884\u8bad\u7ec3\u7684\u66ff\u6362token\u7684\u68c0\u6d4b\u6a21\u578b\n\n- [Open Set Domain Adaptation By Novel Class Discovery](https://arxiv.org/abs/2203.03329)\n  - Open set DA by novel class discovery\n  - \u57fa\u4e8e\u65b0\u7c7b\u53d1\u73b0\u7684open set da\n\n- ICML-21 workshop [Domain Adaptation with Factorizable Joint Shift](https://arxiv.org/abs/2203.02902)\n  - Domain adaptation with factorizable joint shift\n  - \u57fa\u4e8e\u53ef\u5206\u89e3\u7684\u8054\u5408\u6f02\u79fb\u7684\u9886\u57df\u81ea\u9002\u5e94\n\n- ICC-22 [Knowledge Transfer in Deep Reinforcement Learning for Slice-Aware Mobility Robustness Optimization](https://arxiv.org/abs/2203.03227)\n  - Knowledge transfer in RL\n  - \u5f3a\u5316\u8fc1\u79fb\u5b66\u4e60\n\n\n*Updated at 2022-03-02:*\n\n- ACL-22 [Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings](https://arxiv.org/abs/2203.00211)\n  - Investigate selective prediction approaches in IID, OOD, and ADV settings\n  - \u5728\u72ec\u7acb\u540c\u5206\u5e03\u3001\u5206\u5e03\u5916\u3001\u5bf9\u6297\u60c5\u5883\u4e2d\u8c03\u7814\u9009\u62e9\u6027\u9884\u6d4b\u65b9\u6cd5\n\n- PAKDD-22 [Layer Adaptive Deep Neural Networks for Out-of-distribution Detection](https://arxiv.org/abs/2203.00192)\n  - Layer adaptive network for OOD detection\n  - \u5c42\u81ea\u9002\u5e94\u7684\u7f51\u7edc\u8fdb\u884cOOD\u68c0\u6d4b\n\n- [Learning Semantic Segmentation from Multiple Datasets with Label Shifts](https://arxiv.org/abs/2202.14030)\n  - Learning semantic segmentation from many datasets with label shifts\n  - \u5728\u6709\u6807\u7b7e\u6f02\u79fb\u7684\u60c5\u51b5\u4e0b\u4ece\u591a\u4e2a\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u8bed\u4e49\u5206\u5272\n\n- [Causal Domain Adaptation with Copula Entropy based Conditional Independence Test](https://arxiv.org/abs/2202.13482)\n  - Use copula entropy based conditional independence test for csusal domain adaptation\n  - \u4f7f\u7528\u57fa\u4e8ecopula entopy\u7684\u6761\u4ef6\u72ec\u7acb\u6d4b\u8bd5\u8fdb\u884ccausal domain adaptation\n\n- [Interpretable Concept-based Prototypical Networks for Few-Shot Learning](https://arxiv.org/abs/2202.13474)\n  - Concept-based prototypical network for few-shot learning\n  - \u57fa\u4e8e\u6982\u5ff5\u7684\u539f\u578b\u7f51\u7edc\u7528\u4e8e\u5c0f\u6837\u672c\u5b66\u4e60\n\n- - -\n\n## 1.Introduction and Tutorials (\u7b80\u4ecb\u4e0e\u6559\u7a0b)\n\nWant to quickly learn transfer learning\uff1f\u60f3\u5c3d\u5feb\u5165\u95e8\u8fc1\u79fb\u5b66\u4e60\uff1f\u770b\u4e0b\u9762\u7684\u6559\u7a0b\u3002\n\n- Books \u4e66\u7c4d\n  - **\u300a\u8fc1\u79fb\u5b66\u4e60\u300b\uff08\u6768\u5f3a\uff09** [[Buy](https://item.jd.com/12930984.html)] [[English version](https://www.cambridge.org/core/books/transfer-learning/CCFFAFE3CDBC245047F1DEC71D9EF3C7)]\n  - **\u300a\u8fc1\u79fb\u5b66\u4e60\u5bfc\u8bba\u300b(\u738b\u664b\u4e1c\u3001\u9648\u76ca\u5f3a\u8457)** [[Homepage](http://jd92.wang/tlbook)] [[Buy](https://item.jd.com/13283188.html)]\n\n- Blogs \u535a\u5ba2\n  - [Zhihu blogs - \u77e5\u4e4e\u4e13\u680f\u300a\u5c0f\u738b\u7231\u8fc1\u79fb\u300b\u7cfb\u5217\u6587\u7ae0](https://zhuanlan.zhihu.com/p/130244395)\n\t\n- Video tutorials \u89c6\u9891\u6559\u7a0b \n  - [Recent advance of transfer learning - 2021\u5e74\u6700\u65b0\u8fc1\u79fb\u5b66\u4e60\u53d1\u5c55\u73b0\u72b6\u63a2\u8ba8](https://www.bilibili.com/video/BV1N5411T7Sb)\n  - [Definitions of transfer learning area - \u8fc1\u79fb\u5b66\u4e60\u9886\u57df\u540d\u8bcd\u89e3\u91ca](https://www.bilibili.com/video/BV1fu411o7BW) [[Article](https://zhuanlan.zhihu.com/p/428097044)]\n  - [Domain generalization - \u8fc1\u79fb\u5b66\u4e60\u65b0\u5174\u7814\u7a76\u65b9\u5411\u9886\u57df\u6cdb\u5316](https://www.bilibili.com/video/BV1ro4y1S7dd/)\n  - [Domain adaptation - \u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5(\u4e2d\u6587)](https://www.bilibili.com/video/BV1T7411R75a/) \n  - [Transfer learning by Hung-yi Lee @ NTU - \u53f0\u6e7e\u5927\u5b66\u674e\u5b8f\u6bc5\u7684\u89c6\u9891\u8bb2\u89e3(\u4e2d\u6587\u89c6\u9891)](https://www.youtube.com/watch?v=qD6iD4TFsdQ)\n\n- Brief introduction and slides \u7b80\u4ecb\u4e0eppt\u8d44\u6599\n  - [Recent advance of transfer learning](http://jd92.wang/assets/files/l15_jiqizhixin.pdf)\n  - [Domain generalization survey](http://jd92.wang/assets/files/DGSurvey-ppt.pdf)\n  - [Brief introduction in Chinese](https://github.com/jindongwang/transferlearning/blob/master/doc/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B.md)\n\t- [PPT (English)](http://jd92.wang/assets/files/l03_transferlearning.pdf) | [PPT (\u4e2d\u6587)](http://jd92.wang/assets/files/l08_tl_zh.pdf)\n  - \u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5 Domain adaptation: [PDF](http://jd92.wang/assets/files/l12_da.pdf) \uff5c [Video on Bilibili](https://www.bilibili.com/video/BV1T7411R75a/) | [Video on Youtube](https://www.youtube.com/watch?v=RbIsHNtluwQ&t=22s)\n  - Tutorial on transfer learning by Qiang Yang: [IJCAI'13](http://ijcai13.org/files/tutorial_slides/td2.pdf) | [2016 version](http://kddchina.org/file/IntroTL2016.pdf)\n\n- Talk is cheap, show me the code \u52a8\u624b\u6559\u7a0b\u3001\u4ee3\u7801\u3001\u6570\u636e \n  - [Pytorch tutorial on transfer learning](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n\t- [Pytorch finetune](https://github.com/jindongwang/transferlearning/tree/master/code/AlexNet_ResNet)\n\t- [DeepDA: a unified deep domain adaptation toolbox](https://github.com/jindongwang/transferlearning/tree/master/code/DeepDA)\n\t- [DeepDG: a unified deep domain generalization toolbox](https://github.com/jindongwang/transferlearning/tree/master/code/DeepDG)\n\t- [\u66f4\u591a More...](https://github.com/jindongwang/transferlearning/tree/master/code)\n\n- [Transfer Learning Scholars and Labs - \u8fc1\u79fb\u5b66\u4e60\u9886\u57df\u7684\u8457\u540d\u5b66\u8005\u3001\u4ee3\u8868\u5de5\u4f5c\u53ca\u5b9e\u9a8c\u5ba4\u4ecb\u7ecd](https://github.com/jindongwang/transferlearning/blob/master/doc/scholar_TL.md)\n- [Negative transfer - \u8d1f\u8fc1\u79fb](https://www.zhihu.com/question/66492194/answer/242870418)\n\n- - -\n\n## 2.Transfer Learning Areas and Papers (\u7814\u7a76\u9886\u57df\u4e0e\u76f8\u5173\u8bba\u6587)\n\n- [Survey](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#survey)\n- [Theory](#theory)\n- [Per-training/Finetuning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#per-trainingfinetuning)\n- [Knowledge distillation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#knowledge-distillation)\n- [Traditional domain adaptation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#traditional-domain-adaptation)\n- [Deep domain adaptation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#deep-domain-adaptation)\n- [Domain generalization](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#domain-generalization)\n- [Source-free domain adaptation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#source-free-domain-adaptation)\n- [Multi-source domain adaptation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#multi-source-domain-adaptation)\n- [Heterogeneous transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#heterogeneous-transfer-learning)\n- [Online transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#online-transfer-learning)\n- [Zero-shot / few-shot learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#zero-shot--few-shot-learning)\n- [Multi-task learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#multi-task-learning)\n- [Transfer reinforcement learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#transfer-reinforcement-learning)\n- [Transfer metric learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#transfer-metric-learning)\n- [Federated transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#federated-transfer-learning)\n- [Lifelong transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#lifelong-transfer-learning)\n- [Safe transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#safe-transfer-learning)\n- [Transfer learning applications](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#transfer-learning-applications)\n\n- - -\n\n## 3.Theory and Survey (\u7406\u8bba\u4e0e\u7efc\u8ff0)\n\nHere are some articles on transfer learning theory and survey.\n\n**Survey (\u7efc\u8ff0\u6587\u7ae0)\uff1a**\n\n- 2022 [Transfer Learning for Future Wireless Networks: A Comprehensive Survey](https://arxiv.org/abs/2102.07572)\n- 2022 [A Review of Deep Transfer Learning and Recent Advancements](https://arxiv.org/abs/2201.09679)\n- 2022 [Transferability in Deep Learning: A Survey](https://paperswithcode.com/paper/transferability-in-deep-learning-a-survey), from Mingsheng Long in THU.\n- 2021 Domain generalization: IJCAI-21 [Generalizing to Unseen Domains: A Survey on Domain Generalization](https://arxiv.org/abs/2103.03097) | [\u77e5\u4e4e\u6587\u7ae0](https://zhuanlan.zhihu.com/p/354740610) | [\u5fae\u4fe1\u516c\u4f17\u53f7](https://mp.weixin.qq.com/s/DsoVDYqLB1N7gj9X5UnYqw)\n  - First survey on domain generalization\n  - \u7b2c\u4e00\u7bc7\u5bf9Domain generalization (\u9886\u57df\u6cdb\u5316)\u7684\u7efc\u8ff0\n- 2021 Vision-based activity recognition: [A Survey of Vision-Based Transfer Learning in Human Activity Recognition](https://www.mdpi.com/2079-9292/10/19/2412)\n- 2021 ICSAI [A State-of-the-Art Survey of Transfer Learning in Structural Health Monitoring](https://ieeexplore.ieee.org/abstract/document/9664171)\n- 2020 [Transfer learning: survey and classification](https://link.springer.com/chapter/10.1007/978-981-15-5345-5_13), Advances in Intelligent Systems and Computing. \n- 2020 \u8fc1\u79fb\u5b66\u4e60\u6700\u65b0survey\uff0c\u6765\u81ea\u4e2d\u79d1\u9662\u8ba1\u7b97\u6240\u5e84\u798f\u632f\u56e2\u961f\uff0c\u53d1\u8868\u5728Proceedings of the IEEE: [A Comprehensive Survey on Transfer Learning](https://arxiv.org/abs/1911.02685)\n- 2020 \u8d1f\u8fc1\u79fb\u7684\u7efc\u8ff0\uff1a[Overcoming Negative Transfer: A Survey](https://arxiv.org/abs/2009.00909)\n- 2020 \u77e5\u8bc6\u84b8\u998f\u7684\u7efc\u8ff0: [Knowledge Distillation: A Survey](https://arxiv.org/abs/2006.05525)\n- \u7528transfer learning\u8fdb\u884csentiment classification\u7684\u7efc\u8ff0\uff1a[A Survey of Sentiment Analysis Based on Transfer Learning](https://ieeexplore.ieee.org/abstract/document/8746210) \n- 2019 \u4e00\u7bc7\u65b0survey\uff1a[Transfer Adaptation Learning: A Decade Survey](https://arxiv.org/abs/1903.04687)\n- 2018 \u4e00\u7bc7\u8fc1\u79fb\u5ea6\u91cf\u5b66\u4e60\u7684\u7efc\u8ff0: [Transfer Metric Learning: Algorithms, Applications and Outlooks](https://arxiv.org/abs/1810.03944)\n- 2018 \u4e00\u7bc7\u6700\u8fd1\u7684\u975e\u5bf9\u79f0\u60c5\u51b5\u4e0b\u7684\u5f02\u6784\u8fc1\u79fb\u5b66\u4e60\u7efc\u8ff0\uff1a[Asymmetric Heterogeneous Transfer Learning: A Survey](https://arxiv.org/abs/1804.10834)\n- 2018 Neural style transfer\u7684\u4e00\u4e2asurvey\uff1a[Neural Style Transfer: A Review](https://arxiv.org/abs/1705.04058)\n- 2018 \u6df1\u5ea6domain adaptation\u7684\u4e00\u4e2a\u7efc\u8ff0\uff1a[Deep Visual Domain Adaptation: A Survey](https://www.sciencedirect.com/science/article/pii/S0925231218306684)\n- 2017 \u591a\u4efb\u52a1\u5b66\u4e60\u7684\u7efc\u8ff0\uff0c\u6765\u81ea\u9999\u6e2f\u79d1\u6280\u5927\u5b66\u6768\u5f3a\u56e2\u961f\uff1a[A survey on multi-task learning](https://arxiv.org/abs/1707.08114)\n- 2017 \u5f02\u6784\u8fc1\u79fb\u5b66\u4e60\u7684\u7efc\u8ff0\uff1a[A survey on heterogeneous transfer learning](https://link.springer.com/article/10.1186/s40537-017-0089-0)\n- 2017 \u8de8\u9886\u57df\u6570\u636e\u8bc6\u522b\u7684\u7efc\u8ff0\uff1a[Cross-dataset recognition: a survey](https://arxiv.org/abs/1705.04396)\n- 2016 [A survey of transfer learning](https://pan.baidu.com/s/1gfgXLXT)\u3002\u5176\u4e2d\u4ea4\u4ee3\u4e86\u4e00\u4e9b\u6bd4\u8f83\u7ecf\u5178\u7684\u5982\u540c\u6784\u3001\u5f02\u6784\u7b49\u5b66\u4e60\u65b9\u6cd5\u4ee3\u8868\u6027\u6587\u7ae0\u3002\n- 2015 \u4e2d\u6587\u7efc\u8ff0\uff1a[\u8fc1\u79fb\u5b66\u4e60\u7814\u7a76\u8fdb\u5c55](https://pan.baidu.com/s/1bpautob)\n- 2010 [A survey on transfer learning](http://ieeexplore.ieee.org/abstract/document/5288526/)\n- Survey on applications - \u5e94\u7528\u5bfc\u5411\u7684\u7efc\u8ff0\uff1a\n\t- \u89c6\u89c9domain adaptation\u7efc\u8ff0\uff1a[Visual Domain Adaptation: A Survey of Recent Advances](https://pan.baidu.com/s/1o8BR7Vc)\n\t- \u8fc1\u79fb\u5b66\u4e60\u5e94\u7528\u4e8e\u884c\u4e3a\u8bc6\u522b\u7efc\u8ff0\uff1a[Transfer Learning for Activity Recognition: A Survey](https://pan.baidu.com/s/1kVABOYr)\n\t- \u8fc1\u79fb\u5b66\u4e60\u4e0e\u589e\u5f3a\u5b66\u4e60\uff1a[Transfer Learning for Reinforcement Learning Domains: A Survey](https://pan.baidu.com/s/1slfr0w1)\n\t- \u591a\u4e2a\u6e90\u57df\u8fdb\u884c\u8fc1\u79fb\u7684\u7efc\u8ff0\uff1a[A Survey of Multi-source Domain Adaptation](https://pan.baidu.com/s/1eSGREF4)\u3002\n\n**Theory \uff08\u7406\u8bba\u6587\u7ae0\uff09:**\n\n- ICML-20 [Few-shot domain adaptation by causal mechanism transfer](https://arxiv.org/pdf/2002.03497.pdf)\n\t- The first work on causal transfer learning\n\t- \u65e5\u672c\u7406\u8bba\u7ec4\u5927\u4f6cSugiyama\u7684\u5de5\u4f5c\uff0ccausal transfer learning\n- CVPR-19 [Characterizing and Avoiding Negative Transfer](https://arxiv.org/abs/1811.09751)\n\t- Characterizing and avoid negative transfer\n\t- \u5f62\u5f0f\u5316\u5e76\u63d0\u51fa\u5982\u4f55\u907f\u514d\u8d1f\u8fc1\u79fb\n- ICML-20 [On Learning Language-Invariant Representations for Universal Machine Translation](https://arxiv.org/abs/2008.04510)\n  - Theory for universal machine translation\n  - \u5bf9\u7edf\u4e00\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u8fdb\u884c\u4e86\u7406\u8bba\u8bba\u8bc1\n- NIPS-06 [Analysis of Representations for Domain Adaptation](https://dl.acm.org/citation.cfm?id=2976474)\n- ML-10 [A Theory of Learning from Different Domains](https://link.springer.com/article/10.1007/s10994-009-5152-4)\n- NIPS-08 [Learning Bounds for Domain Adaptation](http://papers.nips.cc/paper/3212-learning-bounds-for-domain-adaptation)\n- COLT-09 [Domain adaptation: Learning bounds and algorithms](https://arxiv.org/abs/0902.3430)\n- MMD paper\uff1a[A Hilbert Space Embedding for Distributions](https://link.springer.com/chapter/10.1007/978-3-540-75225-7_5) and [A Kernel Two-Sample Test](http://www.jmlr.org/papers/v13/gretton12a.html)\n- Multi-kernel MMD paper: [Optimal kernel choice for large-scale two-sample tests](http://papers.nips.cc/paper/4727-optimal-kernel-choice-for-large-scale-two-sample-tests)\n\n_ _ _\n\n## 4.Code (\u4ee3\u7801)\n\nUnified codebases for:\n- [Deep domain adaptation](https://github.com/jindongwang/transferlearning/tree/master/code/DeepDA)\n- [Deep domain generalization](https://github.com/jindongwang/transferlearning/tree/master/code/DeepDG)\n\nMore: see [HERE](https://github.com/jindongwang/transferlearning/tree/master/code) and [HERE](https://colab.research.google.com/drive/1MVuk95mMg4ecGyUAIG94vedF81HtWQAr?usp=sharing) for an instant run using Google's Colab.\n\n_ _ _\n\n## 5.Transfer Learning Scholars (\u8457\u540d\u5b66\u8005)\n\nHere are some transfer learning scholars and labs.\n\n**\u5168\u90e8\u5217\u8868\u4ee5\u53ca\u4ee3\u8868\u5de5\u4f5c\u6027\u89c1[\u8fd9\u91cc](https://github.com/jindongwang/transferlearning/blob/master/doc/scholar_TL.md)** \n\nPlease note that this list is far not complete. A full list can be seen in [here](https://github.com/jindongwang/transferlearning/blob/master/doc/scholar_TL.md). Transfer learning is an active field. *If you are aware of some scholars, please add them here.*\n\n_ _ _\n\n## 6.Transfer Learning Thesis (\u7855\u535a\u58eb\u8bba\u6587)\n\nHere are some popular thesis on transfer learning.\n\n[\u8fd9\u91cc](https://pan.baidu.com/share/init?surl=iuzZhHdumrD64-yx_VAybA), \u63d0\u53d6\u7801\uff1atxyz\u3002\n\n- - -\n\n## 7.Datasets and Benchmarks (\u6570\u636e\u96c6\u4e0e\u8bc4\u6d4b\u7ed3\u679c)\n\nPlease see [HERE](https://github.com/jindongwang/transferlearning/blob/master/data) for the popular transfer learning **datasets and benchmark** results.\n\n[\u8fd9\u91cc](https://github.com/jindongwang/transferlearning/blob/master/data)\u6574\u7406\u4e86\u5e38\u7528\u7684\u516c\u5f00\u6570\u636e\u96c6\u548c\u4e00\u4e9b\u5df2\u53d1\u8868\u7684\u6587\u7ae0\u5728\u8fd9\u4e9b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002\n\n- - -\n\n## 8.Transfer Learning Challenges (\u8fc1\u79fb\u5b66\u4e60\u6bd4\u8d5b)\n\n- [Visual Domain Adaptation Challenge (VisDA)](http://ai.bu.edu/visda-2018/)\n\n- - -\n\n## Journals and Conferences\n\nSee [here](https://github.com/jindongwang/transferlearning/blob/master/doc/venues.md) for a full list of related journals and conferences.\n\n- - -\n\n## Applications (\u8fc1\u79fb\u5b66\u4e60\u5e94\u7528)\n\n- [Computer vision](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#computer-vision)\n- [Medical and healthcare](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#medical-and-healthcare)\n- [Natural language processing](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#natural-language-processing)\n- [Time series](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#time-series)\n- [Speech](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#speech)\n- [Multimedia](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#multimedia)\n- [Recommendation](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#recommendation)\n- [Human activity recognition](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#human-activity-recognition)\n- [Autonomous driving](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#autonomous-driving)\n- [Others](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#others)\n\nSee [HERE](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md) for transfer learning applications.\n\n\u8fc1\u79fb\u5b66\u4e60\u5e94\u7528\u8bf7\u89c1[\u8fd9\u91cc](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md)\u3002\n\n- - -\n\n## Other Resources (\u5176\u4ed6\u8d44\u6e90)\n\n- Call for papers:\n  - [Advances in Transfer Learning: Theory, Algorithms, and Applications](https://www.frontiersin.org/research-topics/21133/advances-in-transfer-learning-theory-algorithms-and-applications), DDL: October 2021\n\n- Related projects:\n  - Salad: [A semi-supervised domain adaptation library](https://domainadaptation.org)\n\n- - -\n\n## Contributing (\u6b22\u8fce\u53c2\u4e0e\u8d21\u732e)\n\nIf you are interested in contributing, please refer to [HERE](https://github.com/jindongwang/transferlearning/blob/master/CONTRIBUTING.md) for instructions in contribution.\n\n- - -\n\n### Copyright notice\n\n> ***[Notes]This Github repo can be used by following the corresponding licenses. I want to emphasis that it may contain some PDFs or thesis, which were downloaded by me and can only be used for academic purposes. The copyrights of these materials are owned by corresponding publishers or organizations. All this are for better adademic research. If any of the authors or publishers have concerns, please contact me to delete or replace them.***\n",
            "readme_url": "https://github.com/jindongwang/transferlearning",
            "frameworks": [
                "scikit-learn",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Multi-Source Domain Adaptation Based on Federated Knowledge Alignment",
            "arxiv": "2203.11635",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.11635v1",
            "abstract": "Federated Learning (FL) facilitates distributed model learning to protect\nusers' privacy. In the absence of labels for a new user's data, the knowledge\ntransfer in FL allows a learned global model to adapt to the new samples\nquickly. The multi-source domain adaptation in FL aims to improve the model's\ngenerality in a target domain by learning domain-invariant features from\ndifferent clients. In this paper, we propose Federated Knowledge Alignment\n(FedKA) that aligns features from different clients and those of the target\ntask. We identify two types of negative transfer arising in multi-source domain\nadaptation of FL and demonstrate how FedKA can alleviate such negative\ntransfers with the help of a global features disentangler enhanced by embedding\nmatching. To further facilitate representation learning of the target task, we\ndevise a federated voting mechanism to provide labels for samples from the\ntarget domain via a consensus from querying local models and fine-tune the\nglobal model with these labeled samples. Extensive experiments, including an\nablation study, on an image classification task of Digit-Five and a text\nsentiment classification task of Amazon Review, show that FedKA could be\naugmented to existing FL algorithms to improve the generality of the learned\nmodel for tackling a new task.",
            "authors": [
                "Yuwei Sun",
                "Ng Chong",
                "Ochiai Hideya"
            ]
        },
        {
            "title": "Transfer Adaptation Learning: A Decade Survey",
            "arxiv": "1903.04687",
            "year": 2019,
            "url": "http://arxiv.org/abs/1903.04687v2",
            "abstract": "The world we see is ever-changing and it always changes with people, things,\nand the environment. Domain is referred to as the state of the world at a\ncertain moment. A research problem is characterized as transfer adaptation\nlearning (TAL) when it needs knowledge correspondence between different\nmoments/domains. Conventional machine learning aims to find a model with the\nminimum expected risk on test data by minimizing the regularized empirical risk\non the training data, which, however, supposes that the training and test data\nshare similar joint probability distribution. TAL aims to build models that can\nperform tasks of target domain by learning knowledge from a semantic related\nbut distribution different source domain. It is an energetic research filed of\nincreasing influence and importance, which is presenting a blowout publication\ntrend. This paper surveys the advances of TAL methodologies in the past decade,\nand the technical challenges and essential problems of TAL have been observed\nand discussed with deep insights and new perspectives. Broader solutions of\ntransfer adaptation learning being created by researchers are identified, i.e.,\ninstance re-weighting adaptation, feature adaptation, classifier adaptation,\ndeep network adaptation and adversarial adaptation, which are beyond the early\nsemi-supervised and unsupervised split. The survey helps researchers rapidly\nbut comprehensively understand and identify the research foundation, research\nstatus, theoretical limitations, future challenges and under-studied issues\n(universality, interpretability, and credibility) to be broken in the field\ntoward universal representation and safe applications in open-world scenarios.",
            "authors": [
                "Lei Zhang",
                "Xinbo Gao"
            ]
        },
        {
            "title": "Knowledge Transfer in Deep Reinforcement Learning for Slice-Aware Mobility Robustness Optimization",
            "arxiv": "2203.03227",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.03227v1",
            "abstract": "The legacy mobility robustness optimization (MRO) in self-organizing networks\naims at improving handover performance by optimizing cell-specific handover\nparameters. However, such solutions cannot satisfy the needs of next-generation\nnetwork with network slicing, because it only guarantees the received signal\nstrength but not the per-slice service quality. To provide the truly seamless\nmobility service, we propose a deep reinforcement learning-based slice-aware\nmobility robustness optimization (SAMRO) approach, which improves handover\nperformance with per-slice service assurance by optimizing slice-specific\nhandover parameters. Moreover, to allow safe and sample efficient online\ntraining, we develop a two-step transfer learning scheme: 1) regularized\noffline reinforcement learning, and 2) effective online fine-tuning with mixed\nexperience replay. System-level simulations show that compared against the\nlegacy MRO algorithms, SAMRO significantly improves slice-aware service\ncontinuation while optimizing the handover performance.",
            "authors": [
                "Qi Liao",
                "Tianlun Hu",
                "Dan Wellington"
            ]
        },
        {
            "title": "Neural Style Transfer: A Review",
            "arxiv": "1705.04058",
            "year": 2017,
            "url": "http://arxiv.org/abs/1705.04058v7",
            "abstract": "The seminal work of Gatys et al. demonstrated the power of Convolutional\nNeural Networks (CNNs) in creating artistic imagery by separating and\nrecombining image content and style. This process of using CNNs to render a\ncontent image in different styles is referred to as Neural Style Transfer\n(NST). Since then, NST has become a trending topic both in academic literature\nand industrial applications. It is receiving increasing attention and a variety\nof approaches are proposed to either improve or extend the original NST\nalgorithm. In this paper, we aim to provide a comprehensive overview of the\ncurrent progress towards NST. We first propose a taxonomy of current algorithms\nin the field of NST. Then, we present several evaluation methods and compare\ndifferent NST algorithms both qualitatively and quantitatively. The review\nconcludes with a discussion of various applications of NST and open problems\nfor future research. A list of papers discussed in this review, corresponding\ncodes, pre-trained models and more comparison results are publicly available at\nhttps://github.com/ycjing/Neural-Style-Transfer-Papers.",
            "authors": [
                "Yongcheng Jing",
                "Yezhou Yang",
                "Zunlei Feng",
                "Jingwen Ye",
                "Yizhou Yu",
                "Mingli Song"
            ]
        },
        {
            "title": "Transfer Learning for Future Wireless Networks: A Comprehensive Survey",
            "arxiv": "2102.07572",
            "year": 2021,
            "url": "http://arxiv.org/abs/2102.07572v2",
            "abstract": "With outstanding features, Machine Learning (ML) has been the backbone of\nnumerous applications in wireless networks. However, the conventional ML\napproaches have been facing many challenges in practical implementation, such\nas the lack of labeled data, the constantly changing wireless environments, the\nlong training process, and the limited capacity of wireless devices. These\nchallenges, if not addressed, will impede the effectiveness and applicability\nof ML in future wireless networks. To address these problems, Transfer Learning\n(TL) has recently emerged to be a very promising solution. The core idea of TL\nis to leverage and synthesize distilled knowledge from similar tasks as well as\nfrom valuable experiences accumulated from the past to facilitate the learning\nof new problems. Doing so, TL techniques can reduce the dependence on labeled\ndata, improve the learning speed, and enhance the ML methods' robustness to\ndifferent wireless environments. This article aims to provide a comprehensive\nsurvey on applications of TL in wireless networks. Particularly, we first\nprovide an overview of TL including formal definitions, classification, and\nvarious types of TL techniques. We then discuss diverse TL approaches proposed\nto address emerging issues in wireless networks. The issues include spectrum\nmanagement, localization, signal recognition, security, human activity\nrecognition and caching, which are all important to next-generation networks\nsuch as 5G and beyond. Finally, we highlight important challenges, open issues,\nand future research directions of TL in future wireless networks.",
            "authors": [
                "Cong T. Nguyen",
                "Nguyen Van Huynh",
                "Nam H. Chu",
                "Yuris Mulya Saputra",
                "Dinh Thai Hoang",
                "Diep N. Nguyen",
                "Quoc-Viet Pham",
                "Dusit Niyato",
                "Eryk Dutkiewicz",
                "Won-Joo Hwang"
            ]
        },
        {
            "title": "Domain Adaptation: Learning Bounds and Algorithms",
            "arxiv": "0902.3430",
            "year": 2009,
            "url": "http://arxiv.org/abs/0902.3430v2",
            "abstract": "This paper addresses the general problem of domain adaptation which arises in\na variety of applications where the distribution of the labeled sample\navailable somewhat differs from that of the test data. Building on previous\nwork by Ben-David et al. (2007), we introduce a novel distance between\ndistributions, discrepancy distance, that is tailored to adaptation problems\nwith arbitrary loss functions. We give Rademacher complexity bounds for\nestimating the discrepancy distance from finite samples for different loss\nfunctions. Using this distance, we derive novel generalization bounds for\ndomain adaptation for a wide family of loss functions. We also present a series\nof novel adaptation bounds for large classes of regularization-based\nalgorithms, including support vector machines and kernel ridge regression based\non the empirical discrepancy. This motivates our analysis of the problem of\nminimizing the empirical discrepancy for various loss functions for which we\nalso give novel algorithms. We report the results of preliminary experiments\nthat demonstrate the benefits of our discrepancy minimization algorithms for\ndomain adaptation.",
            "authors": [
                "Yishay Mansour",
                "Mehryar Mohri",
                "Afshin Rostamizadeh"
            ]
        },
        {
            "title": "Characterizing and Avoiding Negative Transfer",
            "arxiv": "1811.09751",
            "year": 2018,
            "url": "http://arxiv.org/abs/1811.09751v4",
            "abstract": "When labeled data is scarce for a specific target task, transfer learning\noften offers an effective solution by utilizing data from a related source\ntask. However, when transferring knowledge from a less related source, it may\ninversely hurt the target performance, a phenomenon known as negative transfer.\nDespite its pervasiveness, negative transfer is usually described in an\ninformal manner, lacking rigorous definition, careful analysis, or systematic\ntreatment. This paper proposes a formal definition of negative transfer and\nanalyzes three important aspects thereof. Stemming from this analysis, a novel\ntechnique is proposed to circumvent negative transfer by filtering out\nunrelated source data. Based on adversarial networks, the technique is highly\ngeneric and can be applied to a wide range of transfer learning algorithms. The\nproposed approach is evaluated on six state-of-the-art deep transfer methods\nvia experiments on four benchmark datasets with varying levels of difficulty.\nEmpirically, the proposed method consistently improves the performance of all\nbaseline methods and largely avoids negative transfer, even when the source\ndata is degenerate.",
            "authors": [
                "Zirui Wang",
                "Zihang Dai",
                "Barnab\u00e1s P\u00f3czos",
                "Jaime Carbonell"
            ]
        },
        {
            "title": "Improving Generalization in Federated Learning by Seeking Flat Minima",
            "arxiv": "2203.11834",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.11834v2",
            "abstract": "Models trained in federated settings often suffer from degraded performances\nand fail at generalizing, especially when facing heterogeneous scenarios. In\nthis work, we investigate such behavior through the lens of geometry of the\nloss and Hessian eigenspectrum, linking the model's lack of generalization\ncapacity to the sharpness of the solution. Motivated by prior studies\nconnecting the sharpness of the loss surface and the generalization gap, we\nshow that i) training clients locally with Sharpness-Aware Minimization (SAM)\nor its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on\nthe server-side can substantially improve generalization in Federated Learning\nand help bridging the gap with centralized models. By seeking parameters in\nneighborhoods having uniform low loss, the model converges towards flatter\nminima and its generalization significantly improves in both homogeneous and\nheterogeneous scenarios. Empirical results demonstrate the effectiveness of\nthose optimizers across a variety of benchmark vision datasets (e.g.\nCIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,\nsemantic segmentation, domain generalization).",
            "authors": [
                "Debora Caldarola",
                "Barbara Caputo",
                "Marco Ciccone"
            ]
        },
        {
            "title": "Layer Adaptive Deep Neural Networks for Out-of-distribution Detection",
            "arxiv": "2203.00192",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.00192v1",
            "abstract": "During the forward pass of Deep Neural Networks (DNNs), inputs gradually\ntransformed from low-level features to high-level conceptual labels. While\nfeatures at different layers could summarize the important factors of the\ninputs at varying levels, modern out-of-distribution (OOD) detection methods\nmostly focus on utilizing their ending layer features. In this paper, we\nproposed a novel layer-adaptive OOD detection framework (LA-OOD) for DNNs that\ncan fully utilize the intermediate layers' outputs. Specifically, instead of\ntraining a unified OOD detector at a fixed ending layer, we train multiple\nOne-Class SVM OOD detectors simultaneously at the intermediate layers to\nexploit the full spectrum characteristics encoded at varying depths of DNNs. We\ndevelop a simple yet effective layer-adaptive policy to identify the best layer\nfor detecting each potential OOD example. LA-OOD can be applied to any existing\nDNNs and does not require access to OOD samples during the training. Using\nthree DNNs of varying depth and architectures, our experiments demonstrate that\nLA-OOD is robust against OODs of varying complexity and can outperform\nstate-of-the-art competitors by a large margin on some real-world datasets.",
            "authors": [
                "Haoliang Wang",
                "Chen Zhao",
                "Xujiang Zhao",
                "Feng Chen"
            ]
        },
        {
            "title": "Learning Semantic Segmentation from Multiple Datasets with Label Shifts",
            "arxiv": "2202.14030",
            "year": 2022,
            "url": "http://arxiv.org/abs/2202.14030v1",
            "abstract": "With increasing applications of semantic segmentation, numerous datasets have\nbeen proposed in the past few years. Yet labeling remains expensive, thus, it\nis desirable to jointly train models across aggregations of datasets to enhance\ndata volume and diversity. However, label spaces differ across datasets and may\neven be in conflict with one another. This paper proposes UniSeg, an effective\napproach to automatically train models across multiple datasets with differing\nlabel spaces, without any manual relabeling efforts. Specifically, we propose\ntwo losses that account for conflicting and co-occurring labels to achieve\nbetter generalization performance in unseen domains. First, a gradient conflict\nin training due to mismatched label spaces is identified and a\nclass-independent binary cross-entropy loss is proposed to alleviate such label\nconflicts. Second, a loss function that considers class-relationships across\ndatasets is proposed for a better multi-dataset training scheme. Extensive\nquantitative and qualitative analyses on road-scene datasets show that UniSeg\nimproves over multi-dataset baselines, especially on unseen datasets, e.g.,\nachieving more than 8% gain in IoU on KITTI averaged over all the settings.",
            "authors": [
                "Dongwan Kim",
                "Yi-Hsuan Tsai",
                "Yumin Suh",
                "Masoud Faraki",
                "Sparsh Garg",
                "Manmohan Chandraker",
                "Bohyung Han"
            ]
        },
        {
            "title": "One Model, Multiple Tasks: Pathways for Natural Language Understanding",
            "arxiv": "2203.03312",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.03312v1",
            "abstract": "This paper presents a Pathways approach to handle many tasks at once. Our\napproach is general-purpose and sparse. Unlike prevailing single-purpose models\nthat overspecialize at individual tasks and learn from scratch when being\nextended to new tasks, our approach is general-purpose with the ability of\nstitching together existing skills to learn new tasks more effectively.\nDifferent from traditional dense models that always activate all the model\nparameters, our approach is sparsely activated: only relevant parts of the\nmodel (like pathways through the network) are activated.\n  We take natural language understanding as a case study and define a set of\nskills like \\textit{the skill of understanding the sentiment of text} and\n\\textit{the skill of understanding natural language questions}. These skills\ncan be reused and combined to support many different tasks and situations. We\ndevelop our system using Transformer as the backbone. For each skill, we\nimplement skill-specific feed-forward networks, which are activated only if the\nskill is relevant to the task. An appealing feature of our model is that it not\nonly supports sparsely activated fine-tuning, but also allows us to pretrain\nskills in the same sparse way with masked language modeling and next sentence\nprediction. We call this model \\textbf{SkillNet}.\n  We have three major findings. First, with only one model checkpoint, SkillNet\nperforms better than task-specific fine-tuning and two multi-task learning\nbaselines (i.e., dense model and Mixture-of-Experts model) on six tasks.\nSecond, sparsely activated pre-training further improves the overall\nperformance. Third, SkillNet significantly outperforms baseline systems when\nbeing extended to new tasks.",
            "authors": [
                "Duyu Tang",
                "Fan Zhang",
                "Yong Dai",
                "Cong Zhou",
                "Shuangzhi Wu",
                "Shuming Shi"
            ]
        },
        {
            "title": "Interpretable Concept-based Prototypical Networks for Few-Shot Learning",
            "arxiv": "2202.13474",
            "year": 2022,
            "url": "http://arxiv.org/abs/2202.13474v1",
            "abstract": "Few-shot learning aims at recognizing new instances from classes with limited\nsamples. This challenging task is usually alleviated by performing\nmeta-learning on similar tasks. However, the resulting models are black-boxes.\nThere has been growing concerns about deploying black-box machine learning\nmodels and FSL is not an exception in this regard. In this paper, we propose a\nmethod for FSL based on a set of human-interpretable concepts. It constructs a\nset of metric spaces associated with the concepts and classifies samples of\nnovel classes by aggregating concept-specific decisions. The proposed method\ndoes not require concept annotations for query samples. This interpretable\nmethod achieved results on a par with six previously state-of-the-art black-box\nFSL methods on the CUB fine-grained bird classification dataset.",
            "authors": [
                "Mohammad Reza Zarei",
                "Majid Komeili"
            ]
        },
        {
            "title": "A Broad Study of Pre-training for Domain Generalization and Adaptation",
            "arxiv": "2203.11819",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.11819v1",
            "abstract": "Deep models must learn robust and transferable representations in order to\nperform well on new domains. While domain transfer methods (e.g., domain\nadaptation, domain generalization) have been proposed to learn transferable\nrepresentations across domains, they are typically applied to ResNet backbones\npre-trained on ImageNet. Thus, existing works pay little attention to the\neffects of pre-training on domain transfer tasks. In this paper, we provide a\nbroad study and in-depth analysis of pre-training for domain adaptation and\ngeneralization, namely: network architectures, size, pre-training loss, and\ndatasets. We observe that simply using a state-of-the-art backbone outperforms\nexisting state-of-the-art domain adaptation baselines and set new baselines on\nOffice-Home and DomainNet improving by 10.7\\% and 5.5\\%. We hope that this work\ncan provide more insights for future domain transfer research.",
            "authors": [
                "Donghyun Kim",
                "Kaihong Wang",
                "Stan Sclaroff",
                "Kate Saenko"
            ]
        },
        {
            "title": "Generalizing to Unseen Domains: A Survey on Domain Generalization",
            "arxiv": "2103.03097",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.03097v5",
            "abstract": "Machine learning systems generally assume that the training and testing\ndistributions are the same. To this end, a key requirement is to develop models\nthat can generalize to unseen distributions. Domain generalization (DG), i.e.,\nout-of-distribution generalization, has attracted increasing interests in\nrecent years. Domain generalization deals with a challenging setting where one\nor several different but related domain(s) are given, and the goal is to learn\na model that can generalize to an unseen test domain. Great progress has been\nmade in the area of domain generalization for years. This paper presents the\nfirst review of recent advances in this area. First, we provide a formal\ndefinition of domain generalization and discuss several related fields. We then\nthoroughly review the theories related to domain generalization and carefully\nanalyze the theory behind generalization. We categorize recent algorithms into\nthree classes: data manipulation, representation learning, and learning\nstrategy, and present several popular algorithms in detail for each category.\nThird, we introduce the commonly used datasets, applications, and our\nopen-sourced codebase for fair evaluation. Finally, we summarize existing\nliterature and present some potential research topics for the future.",
            "authors": [
                "Jindong Wang",
                "Cuiling Lan",
                "Chang Liu",
                "Yidong Ouyang",
                "Tao Qin",
                "Wang Lu",
                "Yiqiang Chen",
                "Wenjun Zeng",
                "Philip S. Yu"
            ]
        },
        {
            "title": "Knowledge Distillation: A Survey",
            "arxiv": "2006.05525",
            "year": 2020,
            "url": "http://arxiv.org/abs/2006.05525v7",
            "abstract": "In recent years, deep neural networks have been successful in both industry\nand academia, especially for computer vision tasks. The great success of deep\nlearning is mainly due to its scalability to encode large-scale data and to\nmaneuver billions of model parameters. However, it is a challenge to deploy\nthese cumbersome deep models on devices with limited resources, e.g., mobile\nphones and embedded devices, not only because of the high computational\ncomplexity but also the large storage requirements. To this end, a variety of\nmodel compression and acceleration techniques have been developed. As a\nrepresentative type of model compression and acceleration, knowledge\ndistillation effectively learns a small student model from a large teacher\nmodel. It has received rapid increasing attention from the community. This\npaper provides a comprehensive survey of knowledge distillation from the\nperspectives of knowledge categories, training schemes, teacher-student\narchitecture, distillation algorithms, performance comparison and applications.\nFurthermore, challenges in knowledge distillation are briefly reviewed and\ncomments on future research are discussed and forwarded.",
            "authors": [
                "Jianping Gou",
                "Baosheng Yu",
                "Stephen John Maybank",
                "Dacheng Tao"
            ]
        },
        {
            "title": "Pre-trained Token-replaced Detection Model as Few-shot Learner",
            "arxiv": "2203.03235",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.03235v1",
            "abstract": "Pre-trained masked language models have demonstrated remarkable ability as\nfew-shot learners. In this paper, as an alternative, we propose a novel\napproach to few-shot learning with pre-trained token-replaced detection models\nlike ELECTRA. In this approach, we reformulate a classification or a regression\ntask as a token-replaced detection problem. Specifically, we first define a\ntemplate and label description words for each task and put them into the input\nto form a natural language prompt. Then, we employ the pre-trained\ntoken-replaced detection model to predict which label description word is the\nmost original (i.e., least replaced) among all label description words in the\nprompt. A systematic evaluation on 16 datasets demonstrates that our approach\noutperforms few-shot learners with pre-trained masked language models in both\none-sentence and two-sentence learning tasks.",
            "authors": [
                "Zicheng Li",
                "Shoushan Li",
                "Guodong Zhou"
            ]
        },
        {
            "title": "Gated Domain-Invariant Feature Disentanglement for Domain Generalizable Object Detection",
            "arxiv": "2203.11432",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.11432v1",
            "abstract": "For Domain Generalizable Object Detection (DGOD), Disentangled Representation\nLearning (DRL) helps a lot by explicitly disentangling Domain-Invariant\nRepresentations (DIR) from Domain-Specific Representations (DSR). Considering\nthe domain category is an attribute of input data, it should be feasible for\nnetworks to fit a specific mapping which projects DSR into feature channels\nexclusive to domain-specific information, and thus much cleaner disentanglement\nof DIR from DSR can be achieved simply on channel dimension. Inspired by this\nidea, we propose a novel DRL method for DGOD, which is termed Gated\nDomain-Invariant Feature Disentanglement (GDIFD). In GDIFD, a Channel Gate\nModule (CGM) learns to output channel gate signals close to either 0 or 1,\nwhich can mask out the channels exclusive to domain-specific information\nhelpful for domain recognition. With the proposed GDIFD, the backbone in our\nframework can fit the desired mapping easily, which enables the channel-wise\ndisentanglement. In experiments, we demonstrate that our approach is highly\neffective and achieves state-of-the-art DGOD performance.",
            "authors": [
                "Haozhuo Zhang",
                "Huimin Yu",
                "Yuming Yan",
                "Runfa Wang"
            ]
        },
        {
            "title": "Domain Adaptation with Factorizable Joint Shift",
            "arxiv": "2203.02902",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.02902v1",
            "abstract": "Existing domain adaptation (DA) usually assumes the domain shift comes from\neither the covariates or the labels. However, in real-world applications,\nsamples selected from different domains could have biases in both the\ncovariates and the labels. In this paper, we propose a new assumption,\nFactorizable Joint Shift (FJS), to handle the co-existence of sampling bias in\ncovariates and labels. Although allowing for the shift from both sides, FJS\nassumes the independence of the bias between the two factors. We provide\ntheoretical and empirical understandings about when FJS degenerates to prior\nassumptions and when it is necessary. We further propose Joint Importance\nAligning (JIA), a discriminative learning objective to obtain joint importance\nestimators for both supervised and unsupervised domain adaptation. Our method\ncan be seamlessly incorporated with existing domain adaptation algorithms for\nbetter importance estimation and weighting on the training data. Experiments on\na synthetic dataset demonstrate the advantage of our method.",
            "authors": [
                "Hao He",
                "Yuzhe Yang",
                "Hao Wang"
            ]
        },
        {
            "title": "A Unified Framework for Domain Adaptation using Metric Learning on Manifolds",
            "arxiv": "1804.10834",
            "year": 2018,
            "url": "http://arxiv.org/abs/1804.10834v1",
            "abstract": "We present a novel framework for domain adaptation, whereby both geometric\nand statistical differences between a labeled source domain and unlabeled\ntarget domain can be integrated by exploiting the curved Riemannian geometry of\nstatistical manifolds. Our approach is based on formulating transfer from\nsource to target as a problem of geometric mean metric learning on manifolds.\nSpecifically, we exploit the curved Riemannian manifold geometry of symmetric\npositive definite (SPD) covariance matrices. We exploit a simple but important\nobservation that as the space of covariance matrices is both a Riemannian space\nas well as a homogeneous space, the shortest path geodesic between two\ncovariances on the manifold can be computed analytically. Statistics on the SPD\nmatrix manifold, such as the geometric mean of two matrices can be reduced to\nsolving the well-known Riccati equation. We show how the Ricatti-based solution\ncan be constrained to not only reduce the statistical differences between the\nsource and target domains, such as aligning second order covariances and\nminimizing the maximum mean discrepancy, but also the underlying geometry of\nthe source and target domains using diffusions on the underlying source and\ntarget manifolds. A key strength of our proposed approach is that it enables\nintegrating multiple sources of variation between source and target in a\nunified way, by reducing the combined objective function to a nested set of\nRicatti equations where the solution can be represented by a cascaded series of\ngeometric mean computations. In addition to showing the theoretical optimality\nof our solution, we present detailed experiments using standard transfer\nlearning testbeds from computer vision comparing our proposed algorithms to\npast work in domain adaptation, showing improved results over a large variety\nof previous methods.",
            "authors": [
                "Sridhar Mahadevan",
                "Bamdev Mishra",
                "Shalini Ghosh"
            ]
        },
        {
            "title": "Open Set Domain Adaptation By Novel Class Discovery",
            "arxiv": "2203.03329",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.03329v1",
            "abstract": "In Open Set Domain Adaptation (OSDA), large amounts of target samples are\ndrawn from the implicit categories that never appear in the source domain. Due\nto the lack of their specific belonging, existing methods indiscriminately\nregard them as a single class unknown. We challenge this broadly-adopted\npractice that may arouse unexpected detrimental effects because the decision\nboundaries between the implicit categories have been fully ignored. Instead, we\npropose Self-supervised Class-Discovering Adapter (SCDA) that attempts to\nachieve OSDA by gradually discovering those implicit classes, then\nincorporating them to restructure the classifier and update the domain-adaptive\nfeatures iteratively. SCDA performs two alternate steps to achieve implicit\nclass discovery and self-supervised OSDA, respectively. By jointly optimizing\nfor two tasks, SCDA achieves the state-of-the-art in OSDA and shows a\ncompetitive performance to unearth the implicit target classes.",
            "authors": [
                "Jingyu Zhuang",
                "Ziliang Chen",
                "Pengxu Wei",
                "Guanbin Li",
                "Liang Lin"
            ]
        },
        {
            "title": "Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings",
            "arxiv": "2203.00211",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.00211v1",
            "abstract": "In order to equip NLP systems with selective prediction capability, several\ntask-specific approaches have been proposed. However, which approaches work\nbest across tasks or even if they consistently outperform the simplest baseline\n'MaxProb' remains to be explored. To this end, we systematically study\n'selective prediction' in a large-scale setup of 17 datasets across several NLP\ntasks. Through comprehensive experiments under in-domain (IID), out-of-domain\n(OOD), and adversarial (ADV) settings, we show that despite leveraging\nadditional resources (held-out data/computation), none of the existing\napproaches consistently and considerably outperforms MaxProb in all three\nsettings. Furthermore, their performance does not translate well across tasks.\nFor instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate\nDetection datasets but does not fare well on NLI datasets, especially in the\nOOD setting. Thus, we recommend that future selective prediction approaches\nshould be evaluated across tasks and settings for reliable estimation of their\ncapabilities.",
            "authors": [
                "Neeraj Varshney",
                "Swaroop Mishra",
                "Chitta Baral"
            ]
        },
        {
            "title": "Benchmarking Test-Time Unsupervised Deep Neural Network Adaptation on Edge Devices",
            "arxiv": "2203.11295",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.11295v1",
            "abstract": "The prediction accuracy of the deep neural networks (DNNs) after deployment\nat the edge can suffer with time due to shifts in the distribution of the new\ndata. To improve robustness of DNNs, they must be able to update themselves to\nenhance their prediction accuracy. This adaptation at the resource-constrained\nedge is challenging as: (i) new labeled data may not be present; (ii)\nadaptation needs to be on device as connections to cloud may not be available;\nand (iii) the process must not only be fast but also memory- and\nenergy-efficient. Recently, lightweight prediction-time unsupervised DNN\nadaptation techniques have been introduced that improve prediction accuracy of\nthe models for noisy data by re-tuning the batch normalization (BN) parameters.\nThis paper, for the first time, performs a comprehensive measurement study of\nsuch techniques to quantify their performance and energy on various edge\ndevices as well as find bottlenecks and propose optimization opportunities. In\nparticular, this study considers CIFAR-10-C image classification dataset with\ncorruptions, three robust DNNs (ResNeXt, Wide-ResNet, ResNet-18), two BN\nadaptation algorithms (one that updates normalization statistics and the other\nthat also optimizes transformation parameters), and three edge devices (FPGA,\nRaspberry-Pi, and Nvidia Xavier NX). We find that the approach that only\nupdates the normalization parameters with Wide-ResNet, running on Xavier GPU,\nto be overall effective in terms of balancing multiple cost metrics. However,\nthe adaptation overhead can still be significant (around 213 ms). The results\nstrongly motivate the need for algorithm-hardware co-design for efficient\non-device DNN adaptation.",
            "authors": [
                "Kshitij Bhardwaj",
                "James Diffenderfer",
                "Bhavya Kailkhura",
                "Maya Gokhale"
            ]
        },
        {
            "title": "SemiPFL: Personalized Semi-Supervised Federated Learning Framework for Edge Intelligence",
            "arxiv": "2203.08176",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.08176v1",
            "abstract": "Recent advances in wearable devices and Internet-of-Things (IoT) have led to\nmassive growth in sensor data generated in edge devices. Labeling such massive\ndata for classification tasks has proven to be challenging. In addition, data\ngenerated by different users bear various personal attributes and edge\nheterogeneity, rendering it impractical to develop a global model that adapts\nwell to all users. Concerns over data privacy and communication costs also\nprohibit centralized data accumulation and training. This paper proposes a\nnovel personalized semi-supervised federated learning (SemiPFL) framework to\nsupport edge users having no label or limited labeled datasets and a sizable\namount of unlabeled data that is insufficient to train a well-performing model.\nIn this work, edge users collaborate to train a hyper-network in the server,\ngenerating personalized autoencoders for each user. After receiving updates\nfrom edge users, the server produces a set of base models for each user, which\nthe users locally aggregate them using their own labeled dataset. We\ncomprehensively evaluate our proposed framework on various public datasets and\ndemonstrate that SemiPFL outperforms state-of-art federated learning frameworks\nunder the same assumptions. We also show that the solution performs well for\nusers without labeled datasets or having limited labeled datasets and\nincreasing performance for increased labeled data and number of users,\nsignifying the effectiveness of SemiPFL for handling edge heterogeneity and\nlimited annotation. By leveraging personalized semi-supervised learning,\nSemiPFL dramatically reduces the need for annotating data and preserving\nprivacy in a wide range of application scenarios, from wearable health to IoT.",
            "authors": [
                "Arvin Tashakori",
                "Wenwen Zhang",
                "Z. Jane Wang",
                "Peyman Servati"
            ]
        },
        {
            "title": "On Learning Language-Invariant Representations for Universal Machine Translation",
            "arxiv": "2008.04510",
            "year": 2020,
            "url": "http://arxiv.org/abs/2008.04510v1",
            "abstract": "The goal of universal machine translation is to learn to translate between\nany pair of languages, given a corpus of paired translated documents for\n\\emph{a small subset} of all pairs of languages. Despite impressive empirical\nresults and an increasing interest in massively multilingual models,\ntheoretical analysis on translation errors made by such universal machine\ntranslation models is only nascent. In this paper, we formally prove certain\nimpossibilities of this endeavour in general, as well as prove positive results\nin the presence of additional (but natural) structure of data.\n  For the former, we derive a lower bound on the translation error in the\nmany-to-many translation setting, which shows that any algorithm aiming to\nlearn shared sentence representations among multiple language pairs has to make\na large translation error on at least one of the translation tasks, if no\nassumption on the structure of the languages is made. For the latter, we show\nthat if the paired documents in the corpus follow a natural\n\\emph{encoder-decoder} generative process, we can expect a natural notion of\n``generalization'': a linear number of language pairs, rather than quadratic,\nsuffices to learn a good representation. Our theory also explains what kinds of\nconnection graphs between pairs of languages are better suited: ones with\nlonger paths result in worse sample complexity in terms of the total number of\ndocuments per language pair needed. We believe our theoretical insights and\nimplications contribute to the future algorithmic design of universal machine\ntranslation.",
            "authors": [
                "Han Zhao",
                "Junjie Hu",
                "Andrej Risteski"
            ]
        },
        {
            "title": "Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with Articulatory Features",
            "arxiv": "2203.03191",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.03191v1",
            "abstract": "While neural text-to-speech systems perform remarkably well in high-resource\nscenarios, they cannot be applied to the majority of the over 6,000 spoken\nlanguages in the world due to a lack of appropriate training data. In this\nwork, we use embeddings derived from articulatory vectors rather than\nembeddings derived from phoneme identities to learn phoneme representations\nthat hold across languages. In conjunction with language agnostic meta\nlearning, this enables us to fine-tune a high-quality text-to-speech model on\njust 30 minutes of data in a previously unseen language spoken by a previously\nunseen speaker.",
            "authors": [
                "Florian Lux",
                "Ngoc Thang Vu"
            ]
        },
        {
            "title": "Few-shot Domain Adaptation by Causal Mechanism Transfer",
            "arxiv": "2002.03497",
            "year": 2020,
            "url": "http://arxiv.org/abs/2002.03497v2",
            "abstract": "We study few-shot supervised domain adaptation (DA) for regression problems,\nwhere only a few labeled target domain data and many labeled source domain data\nare available. Many of the current DA methods base their transfer assumptions\non either parametrized distribution shift or apparent distribution\nsimilarities, e.g., identical conditionals or small distributional\ndiscrepancies. However, these assumptions may preclude the possibility of\nadaptation from intricately shifted and apparently very different\ndistributions. To overcome this problem, we propose mechanism transfer, a\nmeta-distributional scenario in which a data generating mechanism is invariant\namong domains. This transfer assumption can accommodate nonparametric shifts\nresulting in apparently different distributions while providing a solid\nstatistical basis for DA. We take the structural equations in causal modeling\nas an example and propose a novel DA method, which is shown to be useful both\ntheoretically and experimentally. Our method can be seen as the first attempt\nto fully leverage the structural causal models for DA.",
            "authors": [
                "Takeshi Teshima",
                "Issei Sato",
                "Masashi Sugiyama"
            ]
        },
        {
            "title": "Decoupled Knowledge Distillation",
            "arxiv": "2203.08679",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.08679v1",
            "abstract": "State-of-the-art distillation methods are mainly based on distilling deep\nfeatures from intermediate layers, while the significance of logit distillation\nis greatly overlooked. To provide a novel viewpoint to study logit\ndistillation, we reformulate the classical KD loss into two parts, i.e., target\nclass knowledge distillation (TCKD) and non-target class knowledge distillation\n(NCKD). We empirically investigate and prove the effects of the two parts: TCKD\ntransfers knowledge concerning the \"difficulty\" of training samples, while NCKD\nis the prominent reason why logit distillation works. More importantly, we\nreveal that the classical KD loss is a coupled formulation, which (1)\nsuppresses the effectiveness of NCKD and (2) limits the flexibility to balance\nthese two parts. To address these issues, we present Decoupled Knowledge\nDistillation (DKD), enabling TCKD and NCKD to play their roles more efficiently\nand flexibly. Compared with complex feature-based methods, our DKD achieves\ncomparable or even better results and has better training efficiency on\nCIFAR-100, ImageNet, and MS-COCO datasets for image classification and object\ndetection tasks. This paper proves the great potential of logit distillation,\nand we hope it will be helpful for future research. The code is available at\nhttps://github.com/megvii-research/mdistiller.",
            "authors": [
                "Borui Zhao",
                "Quan Cui",
                "Renjie Song",
                "Yiyu Qiu",
                "Jiajun Liang"
            ]
        },
        {
            "title": "Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models",
            "arxiv": "2203.03131",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.03131v1",
            "abstract": "Recently the prompt-tuning paradigm has attracted significant attention. By\nonly tuning continuous prompts with a frozen pre-trained language model (PLM),\nprompt-tuning takes a step towards deploying a shared frozen PLM to serve\nnumerous downstream tasks. Although prompt-tuning shows good performance on\ncertain natural language understanding (NLU) tasks, its effectiveness on\nnatural language generation (NLG) tasks is still under-explored. In this paper,\nwe argue that one of the factors hindering the development of prompt-tuning on\nNLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different\nfrom the pretraining corpus). For example, our preliminary exploration reveals\na large performance gap between prompt-tuning and fine-tuning when unfamiliar\ninputs occur frequently in NLG tasks. This motivates us to propose\ninput-tuning, which fine-tunes both the continuous prompts and the input\nrepresentations, leading to a more effective way to adapt unfamiliar inputs to\nfrozen PLMs. Our proposed input-tuning is conceptually simple and empirically\npowerful. Experimental results on seven NLG tasks demonstrate that input-tuning\nis significantly and consistently better than prompt-tuning. Furthermore, on\nthree of these tasks, input-tuning can achieve a comparable or even better\nperformance than fine-tuning.",
            "authors": [
                "Shengnan An",
                "Yifei Li",
                "Zeqi Lin",
                "Qian Liu",
                "Bei Chen",
                "Qiang Fu",
                "Weizhu Chen",
                "Nanning Zheng",
                "Jian-Guang Lou"
            ]
        },
        {
            "title": "Causal Domain Adaptation with Copula Entropy based Conditional Independence Test",
            "arxiv": "2202.13482",
            "year": 2022,
            "url": "http://arxiv.org/abs/2202.13482v1",
            "abstract": "Domain Adaptation (DA) is a typical problem in machine learning that aims to\ntransfer the model trained on source domain to target domain with different\ndistribution. Causal DA is a special case of DA that solves the problem from\nthe view of causality. It embeds the probabilistic relationships in multiple\ndomains in a larger causal structure network of a system and tries to find the\ncausal source (or intervention) on the system as the reason of distribution\ndrifts of the system states across domains. In this sense, causal DA is\ntransformed as a causal discovery problem that finds invariant representation\nacross domains through the conditional independence between the state variables\nand observable state of the system given interventions. Testing conditional\nindependence is the corner stone of causal discovery. Recently, a copula\nentropy based conditional independence test was proposed with a rigorous theory\nand a non-parametric estimation method. In this paper, we first present a\nmathemetical model for causal DA problem and then propose a method for causal\nDA that finds the invariant representation across domains with the copula\nentropy based conditional independence test. The effectiveness of the method is\nverified on two simulated data. The power of the proposed method is then\ndemonstrated on two real-world data: adult census income data and gait\ncharacteristics data.",
            "authors": [
                "Jian Ma"
            ]
        },
        {
            "title": "A Review of Deep Transfer Learning and Recent Advancements",
            "arxiv": "2201.09679",
            "year": 2022,
            "url": "http://arxiv.org/abs/2201.09679v1",
            "abstract": "A successful deep learning model is dependent on extensive training data and\nprocessing power and time (known as training costs). There exist many tasks\nwithout enough number of labeled data to train a deep learning model. Further,\nthe demand is rising for running deep learning models on edge devices with\nlimited processing capacity and training time. Deep transfer learning (DTL)\nmethods are the answer to tackle such limitations, e.g., fine-tuning a\npre-trained model on a massive semi-related dataset proved to be a simple and\neffective method for many problems. DTLs handle limited target data concerns as\nwell as drastically reduce the training costs. In this paper, the definition\nand taxonomy of deep transfer learning is reviewed. Then we focus on the\nsub-category of network-based DTLs since it is the most common types of DTLs\nthat have been applied to various applications in the last decade.",
            "authors": [
                "Mohammadreza Iman",
                "Khaled Rasheed",
                "Hamid R. Arabnia"
            ]
        },
        {
            "title": "A Comprehensive Survey on Transfer Learning",
            "arxiv": "1911.02685",
            "year": 2019,
            "url": "http://arxiv.org/abs/1911.02685v3",
            "abstract": "Transfer learning aims at improving the performance of target learners on\ntarget domains by transferring the knowledge contained in different but related\nsource domains. In this way, the dependence on a large number of target domain\ndata can be reduced for constructing target learners. Due to the wide\napplication prospects, transfer learning has become a popular and promising\narea in machine learning. Although there are already some valuable and\nimpressive surveys on transfer learning, these surveys introduce approaches in\na relatively isolated way and lack the recent advances in transfer learning.\nDue to the rapid expansion of the transfer learning area, it is both necessary\nand challenging to comprehensively review the relevant studies. This survey\nattempts to connect and systematize the existing transfer learning researches,\nas well as to summarize and interpret the mechanisms and the strategies of\ntransfer learning in a comprehensive way, which may help readers have a better\nunderstanding of the current research status and ideas. Unlike previous\nsurveys, this survey paper reviews more than forty representative transfer\nlearning approaches, especially homogeneous transfer learning approaches, from\nthe perspectives of data and model. The applications of transfer learning are\nalso briefly introduced. In order to show the performance of different transfer\nlearning models, over twenty representative transfer learning models are used\nfor experiments. The models are performed on three different datasets, i.e.,\nAmazon Reviews, Reuters-21578, and Office-31. And the experimental results\ndemonstrate the importance of selecting appropriate transfer learning models\nfor different applications in practice.",
            "authors": [
                "Fuzhen Zhuang",
                "Zhiyuan Qi",
                "Keyu Duan",
                "Dongbo Xi",
                "Yongchun Zhu",
                "Hengshu Zhu",
                "Hui Xiong",
                "Qing He"
            ]
        },
        {
            "title": "Transfer Metric Learning: Algorithms, Applications and Outlooks",
            "arxiv": "1810.03944",
            "year": 2018,
            "url": "http://arxiv.org/abs/1810.03944v3",
            "abstract": "Distance metric learning (DML) aims to find an appropriate way to reveal the\nunderlying data relationship. It is critical in many machine learning, pattern\nrecognition and data mining algorithms, and usually require large amount of\nlabel information (such as class labels or pair/triplet constraints) to achieve\nsatisfactory performance. However, the label information may be insufficient in\nreal-world applications due to the high-labeling cost, and DML may fail in this\ncase. Transfer metric learning (TML) is able to mitigate this issue for DML in\nthe domain of interest (target domain) by leveraging knowledge/information from\nother related domains (source domains). Although achieved a certain level of\ndevelopment, TML has limited success in various aspects such as selective\ntransfer, theoretical understanding, handling complex data, big data and\nextreme cases. In this survey, we present a systematic review of the TML\nliterature. In particular, we group TML into different categories according to\ndifferent settings and metric transfer strategies, such as direct metric\napproximation, subspace approximation, distance approximation, and distribution\napproximation. A summarization and insightful discussion of the various TML\napproaches and their applications will be presented. Finally, we indicate some\nchallenges and provide possible future directions.",
            "authors": [
                "Yong Luo",
                "Yonggang Wen",
                "Ling-Yu Duan",
                "Dacheng Tao"
            ]
        },
        {
            "title": "A Survey on Multi-Task Learning",
            "arxiv": "1707.08114",
            "year": 2017,
            "url": "http://arxiv.org/abs/1707.08114v3",
            "abstract": "Multi-Task Learning (MTL) is a learning paradigm in machine learning and its\naim is to leverage useful information contained in multiple related tasks to\nhelp improve the generalization performance of all the tasks. In this paper, we\ngive a survey for MTL from the perspective of algorithmic modeling,\napplications and theoretical analyses. For algorithmic modeling, we give a\ndefinition of MTL and then classify different MTL algorithms into five\ncategories, including feature learning approach, low-rank approach, task\nclustering approach, task relation learning approach and decomposition approach\nas well as discussing the characteristics of each approach. In order to improve\nthe performance of learning tasks further, MTL can be combined with other\nlearning paradigms including semi-supervised learning, active learning,\nunsupervised learning, reinforcement learning, multi-view learning and\ngraphical models. When the number of tasks is large or the data dimensionality\nis high, we review online, parallel and distributed MTL models as well as\ndimensionality reduction and feature hashing to reveal their computational and\nstorage advantages. Many real-world applications use MTL to boost their\nperformance and we review representative works in this paper. Finally, we\npresent theoretical analyses and discuss several future directions for MTL.",
            "authors": [
                "Yu Zhang",
                "Qiang Yang"
            ]
        },
        {
            "title": "Recent Advances in Transfer Learning for Cross-Dataset Visual Recognition: A Problem-Oriented Perspective",
            "arxiv": "1705.04396",
            "year": 2017,
            "url": "http://arxiv.org/abs/1705.04396v3",
            "abstract": "This paper takes a problem-oriented perspective and presents a comprehensive\nreview of transfer learning methods, both shallow and deep, for cross-dataset\nvisual recognition. Specifically, it categorises the cross-dataset recognition\ninto seventeen problems based on a set of carefully chosen data and label\nattributes. Such a problem-oriented taxonomy has allowed us to examine how\ndifferent transfer learning approaches tackle each problem and how well each\nproblem has been researched to date. The comprehensive problem-oriented review\nof the advances in transfer learning with respect to the problem has not only\nrevealed the challenges in transfer learning for visual recognition, but also\nthe problems (e.g. eight of the seventeen problems) that have been scarcely\nstudied. This survey not only presents an up-to-date technical review for\nresearchers, but also a systematic approach and a reference for a machine\nlearning practitioner to categorise a real problem and to look up for a\npossible solution accordingly.",
            "authors": [
                "Jing Zhang",
                "Wanqing Li",
                "Philip Ogunbona",
                "Dong Xu"
            ]
        },
        {
            "title": "A Survey on Negative Transfer",
            "arxiv": "2009.00909",
            "year": 2020,
            "url": "http://arxiv.org/abs/2009.00909v4",
            "abstract": "Transfer learning (TL) utilizes data or knowledge from one or more source\ndomains to facilitate the learning in a target domain. It is particularly\nuseful when the target domain has very few or no labeled data, due to\nannotation expense, privacy concerns, etc. Unfortunately, the effectiveness of\nTL is not always guaranteed. Negative transfer (NT), i.e., leveraging source\ndomain data/knowledge undesirably reduces the learning performance in the\ntarget domain, has been a long-standing and challenging problem in TL. Various\napproaches have been proposed in the literature to handle it. However, there\ndoes not exist a systematic survey on the formulation of NT, the factors\nleading to NT, and the algorithms that mitigate NT. This paper fills this gap,\nby first introducing the definition of NT and its factors, then reviewing about\nfifty representative approaches for overcoming NT, according to four\ncategories: secure transfer, domain similarity estimation, distant transfer,\nand NT mitigation. NT in related fields, e.g., multi-task learning, lifelong\nlearning, and adversarial attacks, are also discussed.",
            "authors": [
                "Wen Zhang",
                "Lingfei Deng",
                "Lei Zhang",
                "Dongrui Wu"
            ]
        },
        {
            "author": [
                "Wang, Jindong",
                "others, "
            ],
            "title": "Everything about Transfer Learning and Domain Adapation",
            "howpublished": "\\url{http://transferlearning.xyz}",
            "ENTRYTYPE": "misc",
            "ID": "transferlearning.xyz",
            "authors": [
                "Wang, Jindong",
                "others, "
            ]
        },
        {
            "title": "[Buy",
            "url": "https://item.jd.com/12930984.html"
        },
        {
            "title": "[Homepage",
            "url": "http://jd92.wang/tlbook"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Learning Semantic Segmentation from Multiple Datasets with Label Shifts",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://arxiv.org/abs/2202.14030"
                    }
                }
            },
            {
                "name": "Cross-dataset recognition: a survey",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://arxiv.org/abs/1705.04396"
                    }
                }
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "Office-Home"
            },
            {
                "name": "Reuters-21578"
            },
            {
                "name": "Amazon"
            },
            {
                "name": "Office-31"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "MS-COCO"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "CUB"
            },
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.8669914325712583,
        "task": "Sentiment Analysis",
        "task_prob": 0.6217530324206287
    }
}