{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "pix2tex - LaTeX OCR",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "lukas-blecher",
                "owner_type": "User",
                "name": "LaTeX-OCR",
                "url": "https://github.com/lukas-blecher/LaTeX-OCR",
                "stars": 894,
                "pushed_at": "2022-03-28 11:53:45+00:00",
                "created_at": "2020-12-11 16:35:13+00:00",
                "language": "Python",
                "description": "pix2tex: Using a ViT to convert images of equations into LaTeX code.",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "dd629ced2fa8eb1f359121a3eb8ba82f7145524b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/blob/main/.gitignore"
                    }
                },
                "size": 1918
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "bc91fcb23658ebd20a8803e8684a38d8a101fc0e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/blob/main/LICENSE"
                    }
                },
                "size": 1070
            },
            {
                "type": "code",
                "name": "checkpoints",
                "sha": "a206d6c2e248e3e9aeff22fe9f178e89794b3510",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/tree/main/checkpoints"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "dataset",
                "sha": "a977c2e4cf02e81779f085dd961f279ad81b0a05",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/tree/main/dataset"
                    }
                },
                "num_files": 12
            },
            {
                "type": "code",
                "name": "eval.py",
                "sha": "3372de4150d3032ca65a19fa39cae1dccd65f52b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/blob/main/eval.py"
                    }
                },
                "size": 5231
            },
            {
                "type": "code",
                "name": "gui.py",
                "sha": "fb910beb90fb1799fc015b1f3a0bc1eac7259e99",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/blob/main/gui.py"
                    }
                },
                "size": 9057
            },
            {
                "type": "code",
                "name": "models.py",
                "sha": "8b96680d2f12733e27910578c5134318785eee41",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/blob/main/models.py"
                    }
                },
                "size": 6025
            },
            {
                "type": "code",
                "name": "pix2tex.py",
                "sha": "f4609ee0bf5560a749877f22d2f4307b8cdb2cb3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/blob/main/pix2tex.py"
                    }
                },
                "size": 8817
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "41d0f6c29119faa3b7d38fc472b176a3c44f186e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/blob/main/requirements.txt"
                    }
                },
                "size": 387
            },
            {
                "type": "code",
                "name": "resources",
                "sha": "b096d113797dd8557af099aad2dc925dbf42ad10",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/tree/main/resources"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "settings",
                "sha": "f7f769456bfeaa485c3025a4c40dbe07afbfb9db",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/tree/main/settings"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "setup_desktop.py",
                "sha": "588ddcb7a2d57b2ce1a5182d1a1c976230670717",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/blob/main/setup_desktop.py"
                    }
                },
                "size": 4134
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "502b7ddba7afd1b4e6e348cca62ef062f052408f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/blob/main/train.py"
                    }
                },
                "size": 3653
            },
            {
                "type": "code",
                "name": "train_resizer.py",
                "sha": "10d10f776977747ff17282c23892ad62cf0223a2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/blob/main/train_resizer.py"
                    }
                },
                "size": 5755
            },
            {
                "type": "code",
                "name": "utils",
                "sha": "9c3b8975e672be6badb6e40fe9b11a38b837ce39",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukas-blecher/LaTeX-OCR/tree/main/utils"
                    }
                },
                "num_files": 2
            }
        ]
    },
    "authors": [
        {
            "name": "Lukas Blecher",
            "github_id": "lukas-blecher"
        },
        {
            "name": "katie-lim",
            "github_id": "katie-lim"
        },
        {
            "name": "R. H\u00e4cker",
            "email": "richardhaecker1@gmail.com",
            "github_id": "R-Haecker"
        },
        {
            "name": "JCGoran",
            "github_id": "JCGoran"
        },
        {
            "name": "Zhi-Qiang Zhou",
            "github_id": "zhouzq-thu"
        },
        {
            "name": "rainyl",
            "email": "rainyliusy3@gmail.com",
            "github_id": "rainyl"
        },
        {
            "name": "Ha YongWook",
            "email": "ywha12@gmail.com",
            "github_id": "YongWookHa"
        }
    ],
    "tags": [
        "machine-learning",
        "transformer",
        "im2latex",
        "deep-learning",
        "image2text",
        "latex",
        "dataset",
        "pytorch",
        "im2markup",
        "ocr",
        "latex-ocr",
        "vit",
        "math-ocr",
        "vision-transformer",
        "image-processing",
        "python"
    ],
    "description": "pix2tex: Using a ViT to convert images of equations into LaTeX code.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/lukas-blecher/LaTeX-OCR",
            "stars": 894,
            "issues": true,
            "readme": "# pix2tex - LaTeX OCR\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ba_qCGJl29dFQqfBjdqMik3o_EqPE4fr)\n\nThe goal of this project is to create a learning based system that takes an image of a math formula and returns corresponding LaTeX code. \n\n![header](https://user-images.githubusercontent.com/55287601/109183599-69431f00-778e-11eb-9809-d42b9451e018.png)\n\n## Requirements\n### Model\n* PyTorch (tested on v1.7.1)\n* Python 3.7+ & dependencies (`requirements.txt`)\n  ```\n  pip install -r requirements.txt\n  ```\n### Dataset\nIn order to render the math in many different fonts we use  XeLaTeX, generate a PDF and finally convert it to a PNG. For the last step we need to use some third party tools: \n* [XeLaTeX](https://www.ctan.org/pkg/xetex)\n* [ImageMagick](https://imagemagick.org/) with [Ghostscript](https://www.ghostscript.com/index.html). (for converting pdf to png)\n* [Node.js](https://nodejs.org/) to run [KaTeX](https://github.com/KaTeX/KaTeX) (for normalizing Latex code)\n* [`de-macro`](https://www.ctan.org/pkg/de-macro) >= 1.4 (only for parsing arxiv papers)\n* Python 3.7+ & dependencies (`requirements.txt`)\n\n## Using the model\n1. Download/Clone this repository\n2. For now you need to install the Python dependencies specified in `requirements.txt` (look [above](#Requirements))\n3. The latest model checkpoint will be downloaded the first time the program is executed. Alternatively you can download the `weights.pth` (and optionally `image_resizer.pth`) file from the [Releases](https://github.com/lukas-blecher/LaTeX-OCR/releases/latest)->Assets section and place it in the `checkpoints` directory\n\nThanks to [@katie-lim](https://github.com/katie-lim), you can use a nice user interface as a quick way to get the model prediction. Just call the GUI with `python gui.py`. From here you can take a screenshot and the predicted latex code is rendered using [MathJax](https://www.mathjax.org/) and copied to your clipboard.\n\n![demo](https://user-images.githubusercontent.com/55287601/117812740-77b7b780-b262-11eb-81f6-fc19766ae2ae.gif)\n\nIf the model is unsure about the what's in the image it might output a different prediction every time you click \"Retry\". With the `temperature` parameter you can control this behavior (low temperature will produce the same result).\n\nAlternatively you can use `pix2tex.py` with similar functionality as `gui.py`, only as command line tool. In this case you don't need to install PyQt5. Using this script you can also parse already existing images from the disk.\n\n**Note:** As of right now it works best with images of smaller resolution. Don't zoom in all the way before taking a picture. Double check the result carefully. You can try to redo the prediction with an other resolution if the answer was wrong.\n\n**Update:** I have trained an image classifier on randomly scaled images of the training data to predict the original size.\nThis model will automatically resize the custom image to best resemble the training data and thus increase performance of images found in the wild. To use this preprocessing step, all you have to do is download the second weights file mentioned above. You should be able to take bigger (or smaller) images of the formula and still get a satisfying result\n\n## Training the model [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1MqZSKzSgEnJB9lU7LyPma4bo4J3dnj1E)\n\n1. First we need to combine the images with their ground truth labels. I wrote a dataset class (which needs further improving) that saves the relative paths to the images with the LaTeX code they were rendered with. To generate the dataset pickle file run \n\n```\npython dataset/dataset.py --equations path_to_textfile --images path_to_images --tokenizer dataset/tokenizer.json --out dataset.pkl\n```\n\nYou can find my generated training data on the [Google Drive](https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO) as well (formulae.zip - images, math.txt - labels). Repeat the step for the validation and test data. All use the same label text file.\n\n2. Edit the `data` (and `valdata`) entry in the config file to the newly generated `.pkl` file. Change other hyperparameters if you want to. See `settings/config.yaml` for a template.\n3. Now for the actual training run \n```\npython train.py --config path_to_config_file\n```\n\nIf you want to use your own data you might be interested in creating your own tokenizer with\n```\npython dataset/dataset.py --equations path_to_textfile --vocab-size 8000 --out tokenizer.json\n```\nDon't forget to update the path to the tokenizer in the config file and set `num_tokens` to your vocabulary size.\n\n## Model\nThe model consist of a ViT [[1](#References)] encoder with a ResNet backbone and a Transformer [[2](#References)] decoder.\n\n### Performance\n| BLEU score | normed edit distance |\n| ---------- | -------------------- |\n| 0.88       | 0.10                 |\n\n## Data\nWe need paired data for the network to learn. Luckily there is a lot of LaTeX code on the internet, e.g. [wikipedia](https://www.wikipedia.org), [arXiv](https://www.arxiv.org). We also use the formulae from the [im2latex-100k](https://zenodo.org/record/56198#.V2px0jXT6eA) dataset.\nAll of it can be found [here](https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO)\n\n### Fonts\nLatin Modern Math, GFSNeohellenicMath.otf, Asana Math, XITS Math, Cambria Math\n\n\n## TODO\n- [x] add more evaluation metrics\n- [x] create a GUI\n- [ ] add beam search\n- [ ] support handwritten formulae\n- [ ] reduce model size (distillation)\n- [ ] find optimal hyperparameters\n- [ ] tweak model structure\n- [ ] fix data scraping and scrape more data\n- [ ] trace the model\n\n\n## Contribution\nContributions of any kind are welcome.\n\n## Acknowledgment\nCode taken and modified from [lucidrains](https://github.com/lucidrains), [rwightman](https://github.com/rwightman/pytorch-image-models), [im2markup](https://github.com/harvardnlp/im2markup), [arxiv_leaks](https://github.com/soskek/arxiv_leaks), [pkra: Mathjax](https://github.com/pkra/MathJax-single-file), [harupy: snipping tool](https://github.com/harupy/snipping-tool)\n\n## References\n[1] [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)\n\n[2] [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
            "readme_url": "https://github.com/lukas-blecher/LaTeX-OCR",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Attention Is All You Need",
            "arxiv": "1706.03762",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.03762v5",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ]
        },
        {
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "arxiv": "2010.11929",
            "year": 2020,
            "url": "http://arxiv.org/abs/2010.11929v2",
            "abstract": "While the Transformer architecture has become the de-facto standard for\nnatural language processing tasks, its applications to computer vision remain\nlimited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional\nnetworks while keeping their overall structure in place. We show that this\nreliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks.\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\nTransformer (ViT) attains excellent results compared to state-of-the-art\nconvolutional networks while requiring substantially fewer computational\nresources to train.",
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ]
        },
        {
            "title": "XeLaTeX",
            "url": "https://www.ctan.org/pkg/xetex"
        },
        {
            "title": "ImageMagick",
            "url": "https://imagemagick.org/"
        },
        {
            "title": "Node.js",
            "url": "https://nodejs.org/"
        },
        {
            "title": "`de-macro`",
            "url": "https://www.ctan.org/pkg/de-macro"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Wikipedia"
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "CIFAR-100"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9454030123937126,
        "task": "Machine Translation",
        "task_prob": 0.9764779074878119
    }
}