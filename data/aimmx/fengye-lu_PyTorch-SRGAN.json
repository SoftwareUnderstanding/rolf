{
    "visibility": {
        "visibility": "public",
        "license": "GNU General Public License v3.0"
    },
    "name": "PyTorch-SRGAN",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "fengye-lu",
                "owner_type": "User",
                "name": "PyTorch-SRGAN",
                "url": "https://github.com/fengye-lu/PyTorch-SRGAN",
                "stars": 0,
                "pushed_at": "2020-03-18 11:57:33+00:00",
                "created_at": "2020-03-18 11:52:20+00:00",
                "language": "Python",
                "description": "SRGAN-study",
                "license": "GNU General Public License v3.0",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "7bbc71c09205c78d790739d246bbe4f9f1881c17",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fengye-lu/PyTorch-SRGAN/blob/master/.gitignore"
                    }
                },
                "size": 1157
            },
            {
                "type": "code",
                "name": ".idea",
                "sha": "75470ee659984d18b8b404ef1c6813a203413f81",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fengye-lu/PyTorch-SRGAN/tree/master/.idea"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "9cecc1d4669ee8af2ca727a5d8cde10cd8b2d7cc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fengye-lu/PyTorch-SRGAN/blob/master/LICENSE"
                    }
                },
                "size": 35141
            },
            {
                "type": "code",
                "name": "checkpoints",
                "sha": "660cf0dce21bb29aca07ec9316ff1a5fda65b977",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fengye-lu/PyTorch-SRGAN/tree/master/checkpoints"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "data",
                "sha": "7b8b9d87b978fe6f9e17ce19cef3043375e9f03e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fengye-lu/PyTorch-SRGAN/tree/master/data"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "models.py",
                "sha": "683b945e4305635a4d6fe4b2a7307dbec4be42cd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fengye-lu/PyTorch-SRGAN/blob/master/models.py"
                    }
                },
                "size": 3950
            },
            {
                "type": "code",
                "name": "output",
                "sha": "13e0ffb62714f25ba3260573778a5ad8bd52f000",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fengye-lu/PyTorch-SRGAN/tree/master/output"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "test",
                "sha": "61cd1dc205517459a7098cce8e593496d4c478fe",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fengye-lu/PyTorch-SRGAN/blob/master/test"
                    }
                },
                "size": 6782
            },
            {
                "type": "code",
                "name": "train",
                "sha": "71cf710b528ebd54f566a97e0e02fba5b83b0901",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fengye-lu/PyTorch-SRGAN/blob/master/train"
                    }
                },
                "size": 10070
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "37555cda3aedab6fcbcb3dca7c985dc1e4b2876e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fengye-lu/PyTorch-SRGAN/blob/master/utils.py"
                    }
                },
                "size": 1739
            }
        ]
    },
    "authors": [
        {
            "name": "jackey",
            "github_id": "fengye-lu"
        }
    ],
    "tags": [],
    "description": "SRGAN-study",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/fengye-lu/PyTorch-SRGAN",
            "stars": 0,
            "issues": true,
            "readme": "# PyTorch-SRGAN\nA modern PyTorch implementation of SRGAN\n\nIt is deeply based on __Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network__ paper published by the Twitter team (https://arxiv.org/abs/1609.04802) but I replaced activations by Swish (https://arxiv.org/abs/1710.05941)\n\nYou can start training out-of-the-box with the CIFAR-10 or CIFAR-100 datasets, to emulate the paper results however, you will need to download and clean the ImageNet dataset yourself. Results and weights are provided for the ImageNet dataset. \n\nContributions are welcome!\n\n## Requirements\n\n* PyTorch\n* torchvision\n* tensorboard_logger (https://github.com/TeamHG-Memex/tensorboard_logger)\n\n## Training\n\n```\nusage: train [-h] [--dataset DATASET] [--dataroot DATAROOT]\n             [--workers WORKERS] [--batchSize BATCHSIZE]\n             [--imageSize IMAGESIZE] [--upSampling UPSAMPLING]\n             [--nEpochs NEPOCHS] [--generatorLR GENERATORLR]\n             [--discriminatorLR DISCRIMINATORLR] [--cuda] [--nGPU NGPU]\n             [--generatorWeights GENERATORWEIGHTS]\n             [--discriminatorWeights DISCRIMINATORWEIGHTS] [--out OUT]\n```\n\nExample: ```./train --cuda```\n\nThis will start a training session in the GPU. First it will pre-train the generator using MSE error for 2 epochs, then it will train the full GAN (generator + discriminator) for 100 epochs, using content (mse + vgg) and adversarial loss. Although weights are already provided in the repository, this script will also generate them in the checkpoints file.\n\n## Testing\n\n```\nusage: test [-h] [--dataset DATASET] [--dataroot DATAROOT] [--workers WORKERS]\n            [--batchSize BATCHSIZE] [--imageSize IMAGESIZE]\n            [--upSampling UPSAMPLING] [--cuda] [--nGPU NGPU]\n            [--generatorWeights GENERATORWEIGHTS]\n            [--discriminatorWeights DISCRIMINATORWEIGHTS]\n\n```\n\nExample: ```./test --cuda```\n\nThis will start a testing session in the GPU. It will display mean error values and save the generated images in the output directory, all three versions: low resolution, high resolution (original) and high resolution (generated).\n\n## Results\n\n### Training\nThe following results have been obtained with the current training setup:\n\n* Dataset: 350K randomly selected ImageNet samples\n* Input image size: 24x24\n* Output image size: 96x96 (16x)\n\nOther training parameters are the default of _train_ script\n\n![Tensorboard training graphs](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/training_results.png)\n\n### Testing\nTesting has been executed on 128 randomly selected ImageNet samples (disjoint from training set)\n\n```[7/8] Discriminator_Loss: 1.4123 Generator_Loss (Content/Advers/Total): 0.0901/0.6152/0.0908```\n\n### Examples\nSee more under the _output_ directory\n\n__High resolution / Low resolution / Recovered High Resolution__\n\n![Original doggy](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/41.png)\n<img src=\"https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/41.png\" alt=\"Low res doggy\" width=\"96\" height=\"96\">\n![Generated doggy](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/41.png)\n\n![Original woman](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/38.png)\n<img src=\"https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/38.png\" alt=\"Low res woman\" width=\"96\" height=\"96\">\n![Generated woman](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/38.png)\n\n![Original hair](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/127.png)\n<img src=\"https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/127.png\" alt=\"Low res hair\" width=\"96\" height=\"96\">\n![Generated hair](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/127.png)\n\n![Original sand](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/72.png)\n<img src=\"https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/72.png\" alt=\"Low res sand\" width=\"96\" height=\"96\">\n![Generated sand](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/72.png)\n",
            "readme_url": "https://github.com/fengye-lu/PyTorch-SRGAN",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Searching for Activation Functions",
            "arxiv": "1710.05941",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.05941v2",
            "abstract": "The choice of activation functions in deep networks has a significant effect\non the training dynamics and task performance. Currently, the most successful\nand widely-used activation function is the Rectified Linear Unit (ReLU).\nAlthough various hand-designed alternatives to ReLU have been proposed, none\nhave managed to replace it due to inconsistent gains. In this work, we propose\nto leverage automatic search techniques to discover new activation functions.\nUsing a combination of exhaustive and reinforcement learning-based search, we\ndiscover multiple novel activation functions. We verify the effectiveness of\nthe searches by conducting an empirical evaluation with the best discovered\nactivation function. Our experiments show that the best discovered activation\nfunction, $f(x) = x \\cdot \\text{sigmoid}(\\beta x)$, which we name Swish, tends\nto work better than ReLU on deeper models across a number of challenging\ndatasets. For example, simply replacing ReLUs with Swish units improves top-1\nclassification accuracy on ImageNet by 0.9\\% for Mobile NASNet-A and 0.6\\% for\nInception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it\neasy for practitioners to replace ReLUs with Swish units in any neural network.",
            "authors": [
                "Prajit Ramachandran",
                "Barret Zoph",
                "Quoc V. Le"
            ]
        },
        {
            "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
            "arxiv": "1609.04802",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.04802v5",
            "abstract": "Despite the breakthroughs in accuracy and speed of single image\nsuper-resolution using faster and deeper convolutional neural networks, one\ncentral problem remains largely unsolved: how do we recover the finer texture\ndetails when we super-resolve at large upscaling factors? The behavior of\noptimization-based super-resolution methods is principally driven by the choice\nof the objective function. Recent work has largely focused on minimizing the\nmean squared reconstruction error. The resulting estimates have high peak\nsignal-to-noise ratios, but they are often lacking high-frequency details and\nare perceptually unsatisfying in the sense that they fail to match the fidelity\nexpected at the higher resolution. In this paper, we present SRGAN, a\ngenerative adversarial network (GAN) for image super-resolution (SR). To our\nknowledge, it is the first framework capable of inferring photo-realistic\nnatural images for 4x upscaling factors. To achieve this, we propose a\nperceptual loss function which consists of an adversarial loss and a content\nloss. The adversarial loss pushes our solution to the natural image manifold\nusing a discriminator network that is trained to differentiate between the\nsuper-resolved images and original photo-realistic images. In addition, we use\na content loss motivated by perceptual similarity instead of similarity in\npixel space. Our deep residual network is able to recover photo-realistic\ntextures from heavily downsampled images on public benchmarks. An extensive\nmean-opinion-score (MOS) test shows hugely significant gains in perceptual\nquality using SRGAN. The MOS scores obtained with SRGAN are closer to those of\nthe original high-resolution images than to those obtained with any\nstate-of-the-art method.",
            "authors": [
                "Christian Ledig",
                "Lucas Theis",
                "Ferenc Huszar",
                "Jose Caballero",
                "Andrew Cunningham",
                "Alejandro Acosta",
                "Andrew Aitken",
                "Alykhan Tejani",
                "Johannes Totz",
                "Zehan Wang",
                "Wenzhe Shi"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "CIFAR-10"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999979532500848,
        "task": "Image Generation",
        "task_prob": 0.8640604672978015
    }
}