{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "For a description of our implementation and experimental results please see the pre-print of our paper which will appear as a conference paper at ICANN 2018: https://arxiv.org/abs/1807.08518",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "SourangshuGhosh",
                "owner_type": "User",
                "name": "NeuralTuringMachine",
                "url": "https://github.com/SourangshuGhosh/NeuralTuringMachine",
                "stars": 4,
                "pushed_at": "2020-07-31 03:45:01+00:00",
                "created_at": "2020-07-31 03:37:36+00:00",
                "language": "Python",
                "description": "This repository contains a stable, successful Tensorflow implementation of a Neural Turing Machine which has been tested on the Copy, Repeat Copy and Associative Recall tasks from the original paper.",
                "license": "MIT License",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "bb25aa72be2f67dacc29b90f150c8cb61fabe4dd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/SourangshuGhosh/NeuralTuringMachine/blob/master/.gitignore"
                    }
                },
                "size": 1168
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "1e470ca5e4ae115a3fe32687e7884f86cc1945ed",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/SourangshuGhosh/NeuralTuringMachine/blob/master/LICENSE"
                    }
                },
                "size": 1073
            },
            {
                "type": "code",
                "name": "exp3S.py",
                "sha": "eb2a2b17b7663cb5448b7f01b1ca0b77641c3249",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/SourangshuGhosh/NeuralTuringMachine/blob/master/exp3S.py"
                    }
                },
                "size": 2646
            },
            {
                "type": "code",
                "name": "generate_data.py",
                "sha": "bcdb99f0999ed73c0071d050c1213a3413eecbfe",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/SourangshuGhosh/NeuralTuringMachine/blob/master/generate_data.py"
                    }
                },
                "size": 16199
            },
            {
                "type": "code",
                "name": "img",
                "sha": "d564b472017ac94dc92a43f72d1fac0cc2a7500a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/SourangshuGhosh/NeuralTuringMachine/tree/master/img"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "ntm.py",
                "sha": "01a18456aba5d00e0716e92731f593078fe2c1ec",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/SourangshuGhosh/NeuralTuringMachine/blob/master/ntm.py"
                    }
                },
                "size": 9302
            },
            {
                "type": "code",
                "name": "produce_heat_maps.py",
                "sha": "382e1c5d2a38498f04447c01c5272bfa5021139d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/SourangshuGhosh/NeuralTuringMachine/blob/master/produce_heat_maps.py"
                    }
                },
                "size": 2059
            },
            {
                "type": "code",
                "name": "run_tasks.py",
                "sha": "1a6a69b5186d151049b659855d914072ba13c8c9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/SourangshuGhosh/NeuralTuringMachine/blob/master/run_tasks.py"
                    }
                },
                "size": 16348
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "d63350c2f0fb59279c54f8c0ded2e99d6b01c84e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/SourangshuGhosh/NeuralTuringMachine/blob/master/utils.py"
                    }
                },
                "size": 473
            }
        ]
    },
    "authors": [
        {
            "name": "Sourangshu Ghosh",
            "email": "sourangshug123@gmail.com",
            "github_id": "SourangshuGhosh"
        }
    ],
    "tags": [],
    "description": "This repository contains a stable, successful Tensorflow implementation of a Neural Turing Machine which has been tested on the Copy, Repeat Copy and Associative Recall tasks from the original paper.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/SourangshuGhosh/NeuralTuringMachine",
            "stars": 4,
            "issues": true,
            "readme": "**Update 2019-05-26: Google has integrated our NTM implementation into the official TensorFlow release. For more details read here: https://www.scss.tcd.ie/joeran.beel/blog/2019/05/25/google-integrates-our-neural-turing-machine-implementation-in-tensorflow/**\n\nFor a description of our implementation and experimental results please see the pre-print of our paper which will appear as a conference paper at ICANN 2018: https://arxiv.org/abs/1807.08518\n\nOur key contribution is not to implement a Neural Turing Machine in code but to make training stable and reliable. We do not observe the slow learning or gradients becoming NaN that other implementations have reported.\n\nYou can cite the paper as follows:\n\n```\n@article{collierbeel2018ntms, title={Implementing Neural Turing Machines,\nauthor={Collier, Mark and Beel, Joeran},\njournal={International Conference on Artificial Neural Networks, ICANN.}, year={2018}}\n```\n\nThis work was done with [Joeran Beel](https://www.scss.tcd.ie/joeran.beel/) Ussher Assistant Professor in Intelligent Systems\nat the [Adapt Centre, Trinity College Dublin](https://www.adaptcentre.ie/) as part of my undergraduate thesis at Trinity College Dublin.\n\n# Neural Turing Machine\n\nThis repository contains a stable, successful Tensorflow implementation of a Neural Turing Machine which has been tested on the Copy, Repeat Copy and Associative Recall tasks from the [original paper](https://arxiv.org/abs/1410.5401).\n\n## Usage\n\n```python\nfrom ntm import NTMCell\n\ncell = NTMCell(num_controller_layers, num_controller_units, num_memory_locations, memory_size,\n    num_read_heads, num_write_heads, shift_range=3, output_dim=num_bits_per_output_vector,\n    clip_value=clip_controller_output_to_value)\n\noutputs, _ = tf.nn.dynamic_rnn(\n    cell=cell,\n    inputs=inputs,\n    time_major=False)\n```\n\nThe implementation is derived from https://github.com/snowkylin/ntm, another open source NTM implementation. We make small but meaningful changes to the linked code that have a large effect on making our implementation more reliable to train and faster to converge as well as being easier to integrate with Tensorflow. Our contribution is:\n- We compare three different memory initialization schemes and find that initializing the memory contents of a Neural Turing Machine to small constant values works much better than random initilization or backpropagating through memory initialization.\n- We clip the outputs from the NTM controller into a range, which helps with optimization difficulties.\n- The NTMCell implements the [Tensorflow RNNCell interface](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell) so can be used directly with [tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn), etc.\n- We never see loss go to NaN as some other implementations report.\n- We implement 3 of the 5 tasks from the NTM paper. We run many experiments and report convergence speed and generalization performance for our implementation, compared to an LSTM, a DNC and for 3 memory contents initialization schemes.\n\n## Sample Outputs\n\nBelow are some sample outputs on the Copy and Associative Recall tasks. We replicated the hyperparameters from the [original paper](https://arxiv.org/abs/1410.5401) for the 2 tasks:\n\n- Memory Size: 128 X 20\n- Controller: LSTM - 100 units\n- Optimizer: RMSProp - learning rate = 10^-4\n\nThe Copy task network was trained on sequences of length sampled from Uniform(1,20) with 8-dimensional random bit vectors. The Associative Recall task network was trained on sequences with the number of items sampled from Uniform(2,6) each item consisted of 3 6-dimensional random bit vectors.\n\n#### Example performance of NTM on Copy task with sequence length = 20 (output is perfect):\n![Neural Turing Machine Copy Task - Seq len=20](/img/copy_ntm_20_0.png)\n\n#### Example performance of NTM on Copy task with sequence length = 40 (network only trained on sequences of length up to 20 - performance degrades on example after 36th input):\n![Neural Turing Machine Copy Task - Seq len=40](/img/copy_ntm_40_1.png)\n\n#### Example performance of NTM on Associative Recall task with 6 items (output is perfect):\n![Neural Turing Machine Associate Recall Task - Seq len=6 items](/img/associative_recall_ntm_6_0.png)\n\n#### Example performance of NTM on Associative Recall task with 12 items (despite only being trained on sequences of up to 6 items to network generalizes perfectly to 12 items):\n![Neural Turing Machine Associate Recall Task - Seq len=12 items](/img/associative_recall_ntm_12_0.png)\n\nIn order to interpret how the NTM used its external memory we trained a network with 32 memory locations on the Copy task and graphed the read and write head address locations over time.\n\nAs you can see from the below graphs, the network first writes the sequence to memory and then reads it back in the same order it wrote it to memory. This uses both the content and location based addressing capabilities of the NTM. The pattern of writes followed by reads is what we would expect of a reasonable solution to the Copy task.\n\n#### Write head locations of NTM with 32 memory locations trained on Copy task:\n![Write head locations of NTM with 32 memory locations trained on Copy task](/img/ntm_copy_write_head.png)\n\n#### Read head locations of NTM with 32 memory locations trained on Copy task:\n![Read head locations of NTM with 32 memory locations trained on Copy task](/img/ntm_copy_read_head.png)\n\nFurther results on memory initilization comparison and learning curves to come...\n",
            "readme_url": "https://github.com/SourangshuGhosh/NeuralTuringMachine",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Neural Turing Machines",
            "arxiv": "1410.5401",
            "year": 2014,
            "url": "http://arxiv.org/abs/1410.5401v2",
            "abstract": "We extend the capabilities of neural networks by coupling them to external\nmemory resources, which they can interact with by attentional processes. The\ncombined system is analogous to a Turing Machine or Von Neumann architecture\nbut is differentiable end-to-end, allowing it to be efficiently trained with\ngradient descent. Preliminary results demonstrate that Neural Turing Machines\ncan infer simple algorithms such as copying, sorting, and associative recall\nfrom input and output examples.",
            "authors": [
                "Alex Graves",
                "Greg Wayne",
                "Ivo Danihelka"
            ]
        },
        {
            "title": "Implementing Neural Turing Machines",
            "arxiv": "1807.08518",
            "year": 2018,
            "url": "http://arxiv.org/abs/1807.08518v3",
            "abstract": "Neural Turing Machines (NTMs) are an instance of Memory Augmented Neural\nNetworks, a new class of recurrent neural networks which decouple computation\nfrom memory by introducing an external memory unit. NTMs have demonstrated\nsuperior performance over Long Short-Term Memory Cells in several sequence\nlearning tasks. A number of open source implementations of NTMs exist but are\nunstable during training and/or fail to replicate the reported performance of\nNTMs. This paper presents the details of our successful implementation of a\nNTM. Our implementation learns to solve three sequential learning tasks from\nthe original NTM paper. We find that the choice of memory contents\ninitialization scheme is crucial in successfully implementing a NTM. Networks\nwith memory contents initialized to small constant values converge on average 2\ntimes faster than the next best memory contents initialization scheme.",
            "authors": [
                "Mark Collier",
                "Joeran Beel"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9622124732748222,
        "task": "Machine Translation",
        "task_prob": 0.7442727292446237
    }
}