{
    "visibility": {
        "visibility": "public"
    },
    "name": "Student ID, name of each team member.",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "BeyondCloud",
                "owner_type": "User",
                "name": "Comp04_ReverseImageCaption",
                "url": "https://github.com/BeyondCloud/Comp04_ReverseImageCaption",
                "stars": 0,
                "pushed_at": "2020-06-28 22:15:52+00:00",
                "created_at": "2019-01-09 06:06:46+00:00",
                "language": "Jupyter Notebook",
                "frameworks": [
                    "NLTK",
                    "TensorFlow",
                    "Theano"
                ]
            },
            {
                "type": "code",
                "name": "Code",
                "sha": "78bcb61dd782da2a474f59233708ea769bce9ced",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/BeyondCloud/Comp04_ReverseImageCaption/tree/master/Code"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "Report",
                "sha": "34f22909aa8bf93f661815a7f220e8af1f38aa6d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/BeyondCloud/Comp04_ReverseImageCaption/tree/master/Report"
                    }
                },
                "num_files": 11
            }
        ]
    },
    "authors": [
        {
            "name": "BeyondCloud",
            "email": "a1989796@yahoo.com.tw",
            "github_id": "BeyondCloud"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/BeyondCloud/Comp04_ReverseImageCaption",
            "stars": 0,
            "issues": true,
            "readme": "\n# Student ID, name of each team member.\n    106061514 \u8a31\u921e\u68e0  106061536 \u5ed6\u5b78\u7152\n# Demo: Pick 5 descriptions from testing data and generate 5 images with different z respectively.    \n<img src=\"./Report/25_inf.png\">\n    (first row ~ last row)<br>\n    0633: this flower has petals that are yellow with red blotches <br>\n    0194: the flower has white stringy petals with yellow and purple pollen tubes<br>\n    2014: this flower is pink in color with only one large petal<br>\n    4683: this flower is yellow in color with petals that are rounded<br>\n    3327: the flower has a several pieces of yellow colored petals that looks similar to its leaves\n# The models you tried during competition. Briefly describe the main idea of this model and why you chose this model.\n   ### GAN-CLS (Based on DCGAN )algorithm (http://arxiv.org/abs/1605.05396)\n    We had implemented three models(small-scale, large-scale and WGAN-GP). The small one required <2G GPU RAM while the large one \n    required ~8.5G (for batch size = 64)\n    \n   ### The large-scale model comprises of three parts:\n    1.Textencoder + CNNencoder\n    This section is used for generating appropriate text embedding. First, we feed correct images and mismatched images\n    into CNNencoder, which generates encoded vector x and x_w. As for the textencoder , we feed correct captions and\n    mismatched captions into encoder to generate v and v_w. Finally, the similarity of text and image can be calculated by:\n        alpha = 0.2\n        rnn_loss = tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(x, v) + cosine_similarity(x, v_w))) + \\\n                tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(x, v) + cosine_similarity(x_w, v)))\n\n    2.Generator:\n    The generator consists of 6 nets. The feature map start from 4x4, doubling its size by upsampling one \n    time when passing each of the last 4 nets. Plus, the network uses highway connection which adds two \n    features( before passing CNN and after passing CNN) together. This could be regarded as \"adding some images\n    detail to the output of previous layer.\" Since the pixel value of the output image is 0~1, we use \n    tanh(last_layer_output) or tanh(last_layer_output)*0.5+0.5 to map the output value to reasonable range.\n    \n    Note about tanh(last_layer_output) or tanh(last_layer_output)*0.5+0.5\n        In scipy image the pixel value < 0 will be converted to zero automatically. In our small-scale model\n     we used the later one (tanh(last_layer_output)*0.5+0.5 range= [0,1]). However, the large-scale model will \n     collapse if the later activation function is applied. So it's really case by case to choose which activation\n     function should be used in the last layer\n\n    \n    3.Discriminator\n    There are 4 nets inside the discriminator. The top 3 layers use CONV2D with stride = 2 to compress the\n    information of the image. At the 4th layer the model concatenates the textvector and 3rd layer \n    output. After the concatenated vector passing  through 2 CONV2D network, the model will generate an output \n    logit with shape = (batch_size,).\n    \n    Below are the equations used for calculating the generator/discriminator loss\n    \n    disc_real = Discriminator(real_image, text_encoder.out(correct_caption))\n    disc_fake = Discriminator(fake_image, text_encoder.out(correct_caption))\n    disc_wrong = Discriminator(real_image, text_lencoder.out(wrong_caption))\n    d_loss1 = reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_real.logits,labels=tf.ones_like(disc_real.logits)*0.9))\n    d_loss2 = reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_wrong.logits,labels=tf.zeros_like(disc_wrong.logits)+0.1'))\n    d_loss3 = reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake.logits,labels=tf.zeros_like(disc_fake.logits)+0.1))\n    d_loss = d_loss1 + (d_loss2 + d_loss3) * 0.5\n    \n    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake.logits,labels=tf.ones_like(disc_fake.logits)*0.9))\n   ###  The small-scale model ( input = skip_thought word vector ,no RNN)\n    The small model also use GAN-CLS but without high-way connection and RNN encoder\n   ### WGAN-GP\n    We also tried modifying the original DCGAN loss by WGAN loss with gradient penalty. However, if we just change the loss to WGAN loss, the training would be failed for converging a good result. The follows are our modifications:\n    1. The loss for mis-matching pairs was not used. \n    2. The gradient penalty is zero-centered gradient penalty (w.r.t the real image) from this paper https://arxiv.org/pdf/1801.04406.pdf.\n    Therefore, the loss became:\n    \n    self.fake_discriminator = Discriminator(generator.outputs, text_encoder.outputs, ...)\n    self.real_discriminator = Discriminator(self.real_image_n, text_encoder.outputs, ...)\n    ...\n    d_loss1 = tf.reduce_mean(self.real_discriminator.logits)\n    d_loss2 = tf.reduce_mean(self.fake_discriminator.logits)\n    self.d_loss = (d_loss2)  -  d_loss1 + self.rnn_loss +10 * self.gradient_penalty\n    self.g_loss = -d_loss2\n    , where rnn_loss is the the same as the loss in GAN-CLS.\n    \n# List the experiment you did. For example, the hyper-parameters tuning, architecture tuning, optimizer tuning and so on.\n\n    \n   ### soft label\n       Changing the discriminator true image label from 1 to 0.9 and fake image label from 0 to 0.1 might speed \n       up the training process at the beginning of training. During the early training stage, the discriminator\n       could easily tell the difference between real and fake image. Making this change could also prevent \n       the discriminator loss drop to zero since the generator always generates nonsense images to fool the discriminator.\n       \n   ### Sample noise from normal distribution\n        Using normal distribution noise is more intuitive for generating natural images. However, we found that whether \n        we use uniform distribution or normal distribution noise vector, they generate similar inception score.\n        \n   ### Using dropout in generator\n        In our experiment, we found that dropout is useful in the small-scale model. At the early stage of training. \n        the model always output similar flowers for each kind of input. This problem can be solved by adding 0.5 \n        dropout in several generator's layers. However, the large-scale model could generate wide variety of flower\n        at the initial stage so the dropout layer is unnecessary.\n        \n        Plus, we found that the inception score of small model is more fluctuating than the large model for different\n        random noise input. \n   ### Virtual batch normalization\n        Sometimes the synthesised image might have similar color for each batch of input(see gif below). \n        Whenever this happens, changing the batch_normalization layer to VBN could solve the problem.\n        See paper https://arxiv.org/pdf/1606.03498.pdf\n        \n<img src=\"./Report/test.gif\">\n   ### The n_critic parameter in WGAN\n        In the original WGAN-GP paper, the authors train the discriminator 5 times for training generator 1 time. \n        Using the same setting for training the GAN-CLS leads the speed of convergence very slow.\n        After tuning this parameter, we found we can train with n_critic = 1 and it became faster. \n        Unfortunately, such modigication also made the training slightly unstable. Sometimes the generated image \n        changes dramatically in two consecutive training epochs.\n        The potential reason is that the disciminator still had not been trained well but the generator had been update.\n        We expect that tuning the model of discriminator and making it stronger than generator can solve this problem.\n\n   ### Structure loss(fail)\n        Although the generator is able to synthesis images with right color, we found that some images have\n        distorted shape.\n        In order to deal with this problem, we add two extra losses to the discriminator, hoping the model could\n        care more about the shape of the flower:\n        Below is what we'd done\n        1. loss1 = convert the real image to grayscale, duplicate its value to 3 channels, label it as 1\n        2. loss2 = convert the fake image to grayscale, duplicate its value to 3 channels, label it as 0\n        3. add loss1 ,loss2 to discriminator loss\n        Unfortunately, the generator outputs grayscale image rather than the image with better shape\n<img src=\"./Report/bk.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n\n   ###  Multi-sampling generated image\n       We found that the image generating quality is highly depends on the noise vector input. Consequently, the \n       inception score might be fluctuated. To solve this problem, we load the discriminator while running inference.\n       The algorithm is discribed below:\n       1. Using 10 different random vectors to generate 10 images from same caption\n       2. Feeding 10 images and captions into the discriminator\n       3. Pick the image with the highest discriminator output value\n       This method could more or less increase the stability of output score.\n   \n### How did you preprocess your data (if you did it by yourself, not using the method on course notebook)\n            ## Data Augmentation\n                In an attempt to generate more training data, the input image will be flipped left-right, up-down,\n                left-rigt-up-down randomly before feeding into the network\n            ## Image/Captions caching\n                Saving the processed image (resize/rotate) and caption(encoded text vector) to npy file could\n                make the training process more efficient.\n            ## More information about captions\n                We use five captions for each image. Plus, we remove all the symbol.\n           \n        - Do you use pre-trained model? (word embedding model, RNN model, etc.) \n   ### Skip thought vector  (upper64: generated lower64: ground truth) (inception score 0.114)\n <img src=\"./Report/ST.jpg\">\n\n                We had done some experiment using the pretrain model. However, we didn't use it in kaggle private\n                leaderboard since the competition is not allowed to do so. There are two benefits about using the\n                skip-thought vector:\n  \n                1. No RNN network required. Thus, the training speed is faster and the memory usage is relatively \n                low. (<2G GPU memory for small-scale model (batch size = 64)). Also the text embedded is more \n                stable than RNN since it's always unchanged\n                \n                2. When using RNN we need to preprocess the sentence to the same length and the max length of \n                sentence/vocabulary size are also limited. Skip tought vector could generate vector from different\n                length sentence and the vocabulary dictionary is also large.\n                \n                However, the skipthought vector also comes with some drawbacks:\n                1.Generating word embedding is time consuming. It takes about 5 hours to build the skipthought vector\n                from all training and testing captions (5 captions per image)\n                2. We found that it takes more time for the model to learn the color information from skipthought\n                vector in comparison with RNN text-encoder.\n\n\n  ### SRCNN  - super resolution network (fail)\n           \n           Although the generated image looks like the flower, the image is lack of detail information. We could use \n           stack-GAN to solve this problem. However, the implementation of stack-GAN is complicated and we also don't\n           have sufficient memory. Therefore, We attempted to solve this problem by increasing the low resolution \n           generated image using the super resolution network post-processing.\n           \n           We loaded the pretrained model, converting the image from 64x64 to 256x256 and resizing it back to 64x64.\n           Nevertheless, the inception score doesn't improve since some image is distorted after resizing.\n           \n           Figure below: upper: fail case, lower: successful case\n           \n<img src=\"./Report/SRCNN.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n        - Any tricks or techniques you used in this task.\n        \n            # Sharpen-Filter post-processing\n            \n            In \"A Note on the Inception Score (2018)  Shane Barratt, Rishi Sharma\", it claims that the sharp image  \n            is more likely to get higher inception score. Consequently, we apply the sharpen filter to the output \n            test images by using python Image lib sharpness filter:\n                img = Image.fromarray(np.uint8(images*255))\n                enhancer = ImageEnhance.Sharpness(img)\n                img  = enhancer.enhance(3.0)\n            This process could increase about 0.03 inception score \n            Our model is able to achieve 0.117 inception score after applying the sharpen filter\n <img src=\"./Report/sharp.jpg\">\n   # Conclusions (interesting findings, pitfalls, takeaway lessons, etc.)\n    We found that GAN is really hard to train since the loss of generator and discriminator are unstable and they\n    don't have too much reference value (also hard to debug). Plus, this is the first time I understand the importance\n    of tf.variable_scope. The model have three networks (RNN,G,D) and three optimizers, so we need to control which\n    network's variable should be update.\n    \n    We also found that the inception score is a mediocre indicator since it's easy to generate high inception score \n    from non-sense images. For instance, we scored 0.121 by outputing the same image from one of the training images\n    and adding some noise to them (see figure below). There's also a paper talking about how to generate near perfect \n    inception score from poor quality image \n    https://arxiv.org/abs/1801.01973\n<img src=\"./Report/suck.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n\n\n```python\n\n```\n",
            "readme_url": "https://github.com/BeyondCloud/Comp04_ReverseImageCaption",
            "frameworks": [
                "NLTK",
                "TensorFlow",
                "Theano"
            ]
        }
    ],
    "references": [
        {
            "title": "Which Training Methods for GANs do actually Converge?",
            "arxiv": "1801.04406",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.04406v4",
            "abstract": "Recent work has shown local convergence of GAN training for absolutely\ncontinuous data and generator distributions. In this paper, we show that the\nrequirement of absolute continuity is necessary: we describe a simple yet\nprototypical counterexample showing that in the more realistic case of\ndistributions that are not absolutely continuous, unregularized GAN training is\nnot always convergent. Furthermore, we discuss regularization strategies that\nwere recently proposed to stabilize GAN training. Our analysis shows that GAN\ntraining with instance noise or zero-centered gradient penalties converges. On\nthe other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number\nof discriminator updates per generator update do not always converge to the\nequilibrium point. We discuss these results, leading us to a new explanation\nfor the stability problems of GAN training. Based on our analysis, we extend\nour convergence results to more general GANs and prove local convergence for\nsimplified gradient penalties even if the generator and data distribution lie\non lower dimensional manifolds. We find these penalties to work well in\npractice and use them to learn high-resolution generative image models for a\nvariety of datasets with little hyperparameter tuning.",
            "authors": [
                "Lars Mescheder",
                "Andreas Geiger",
                "Sebastian Nowozin"
            ]
        },
        {
            "title": "A Note on the Inception Score",
            "arxiv": "1801.01973",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.01973v2",
            "abstract": "Deep generative models are powerful tools that have produced impressive\nresults in recent years. These advances have been for the most part empirically\ndriven, making it essential that we use high quality evaluation metrics. In\nthis paper, we provide new insights into the Inception Score, a recently\nproposed and widely used evaluation metric for generative models, and\ndemonstrate that it fails to provide useful guidance when comparing models. We\ndiscuss both suboptimalities of the metric itself and issues with its\napplication. Finally, we call for researchers to be more systematic and careful\nwhen evaluating and comparing generative models, as the advancement of the\nfield depends upon it.",
            "authors": [
                "Shane Barratt",
                "Rishi Sharma"
            ]
        },
        {
            "title": "Generative Adversarial Text to Image Synthesis",
            "arxiv": "1605.05396",
            "year": 2016,
            "url": "http://arxiv.org/abs/1605.05396v2",
            "abstract": "Automatic synthesis of realistic images from text would be interesting and\nuseful, but current AI systems are still far from this goal. However, in recent\nyears generic and powerful recurrent neural network architectures have been\ndeveloped to learn discriminative text feature representations. Meanwhile, deep\nconvolutional generative adversarial networks (GANs) have begun to generate\nhighly compelling images of specific categories, such as faces, album covers,\nand room interiors. In this work, we develop a novel deep architecture and GAN\nformulation to effectively bridge these advances in text and image model- ing,\ntranslating visual concepts from characters to pixels. We demonstrate the\ncapability of our model to generate plausible images of birds and flowers from\ndetailed text descriptions.",
            "authors": [
                "Scott Reed",
                "Zeynep Akata",
                "Xinchen Yan",
                "Lajanugen Logeswaran",
                "Bernt Schiele",
                "Honglak Lee"
            ]
        },
        {
            "title": "Improved Techniques for Training GANs",
            "arxiv": "1606.03498",
            "year": 2016,
            "url": "http://arxiv.org/abs/1606.03498v1",
            "abstract": "We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes.",
            "authors": [
                "Tim Salimans",
                "Ian Goodfellow",
                "Wojciech Zaremba",
                "Vicki Cheung",
                "Alec Radford",
                "Xi Chen"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999994495616965,
        "task": "Image Generation",
        "task_prob": 0.9580171969912749
    },
    "training": {
        "datasets": [
            {
                "name": "SVHN"
            },
            {
                "name": "MNIST"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ImageNet"
            }
        ]
    }
}