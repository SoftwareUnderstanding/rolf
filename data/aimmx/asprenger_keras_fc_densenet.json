{
    "visibility": {
        "visibility": "public"
    },
    "name": "Keras Fully Connected DenseNet",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "asprenger",
                "owner_type": "User",
                "name": "keras_fc_densenet",
                "url": "https://github.com/asprenger/keras_fc_densenet",
                "stars": 9,
                "pushed_at": "2018-09-27 20:42:36+00:00",
                "created_at": "2018-09-11 08:26:49+00:00",
                "language": "Jupyter Notebook",
                "description": "Keras implementation of the Fully Convolutional DenseNets for Semantic Segmentation paper. ",
                "frameworks": [
                    "Keras",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "32253496a88c523b4eeb57311a90c7a61c1b0e7c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/asprenger/keras_fc_densenet/blob/master/.gitignore"
                    }
                },
                "size": 41
            },
            {
                "type": "code",
                "name": "camvid_dataset.py",
                "sha": "bd43a4ae08acee7e5410475fcb2367e977670d3c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/asprenger/keras_fc_densenet/blob/master/camvid_dataset.py"
                    }
                },
                "size": 5138
            },
            {
                "type": "code",
                "name": "camvid_utils.py",
                "sha": "89a0ba1f719c1ab454c808cccf31d819a97aa4b6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/asprenger/keras_fc_densenet/blob/master/camvid_utils.py"
                    }
                },
                "size": 2081
            },
            {
                "type": "code",
                "name": "common.py",
                "sha": "f4792e1bd5470ddc4b8d7e77c591ea865155e500",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/asprenger/keras_fc_densenet/blob/master/common.py"
                    }
                },
                "size": 4919
            },
            {
                "type": "code",
                "name": "images",
                "sha": "355e389725a9b2b2faf91ee58c874078703d944b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/asprenger/keras_fc_densenet/tree/master/images"
                    }
                },
                "num_files": 12
            },
            {
                "type": "code",
                "name": "keras_fc_densenet.py",
                "sha": "2b5e3e6531beb02b2c2baa255d8cef583f3c1e16",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/asprenger/keras_fc_densenet/blob/master/keras_fc_densenet.py"
                    }
                },
                "size": 18125
            },
            {
                "type": "code",
                "name": "metrics.py",
                "sha": "e1aa8ce645fc61e760c3ad913ec8ff33b76d79ff",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/asprenger/keras_fc_densenet/blob/master/metrics.py"
                    }
                },
                "size": 2595
            },
            {
                "type": "code",
                "name": "notebooks",
                "sha": "207fe8a41819cc9c79398ae0a530f26c7b576a71",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/asprenger/keras_fc_densenet/tree/master/notebooks"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "street_pics",
                "sha": "a573795138e5bb7bfff99bc607528c465e5f481c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/asprenger/keras_fc_densenet/tree/master/street_pics"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "05e5659abc51d902c9523d9b5b890fbac2e47d1d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/asprenger/keras_fc_densenet/blob/master/train.py"
                    }
                },
                "size": 6560
            },
            {
                "type": "code",
                "name": "write_camvid_tfrecords.py",
                "sha": "87f16563ba50df8fa04ec83fb2e4723f211d6450",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/asprenger/keras_fc_densenet/blob/master/write_camvid_tfrecords.py"
                    }
                },
                "size": 4447
            }
        ]
    },
    "authors": [
        {
            "name": "Andre Sprenger",
            "github_id": "asprenger"
        }
    ],
    "tags": [
        "keras",
        "tensorflow",
        "machine-learning",
        "densenet",
        "image-processing"
    ],
    "description": "Keras implementation of the Fully Convolutional DenseNets for Semantic Segmentation paper. ",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/asprenger/keras_fc_densenet",
            "stars": 9,
            "issues": true,
            "readme": "# Keras Fully Connected DenseNet\n\nThis is a Keras implementation of the [Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/abs/1611.09326) paper.\nThe model is trained on the [CamVid](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/) dataset. \n\n## Introduction\n\nFully Convolutional Networks (FCNs) are a natural extension of CNNs to tackle per pixel prediction problems such as semantic image\nsegmentation. FCNs add upsampling layers to standard CNNs to recover the spatial resolution of the input at the output layer. In \norder to compensate for the resolution loss induced by pooling layers, FCNs introduce skip connections between their downsampling \nand upsampling paths. Skip connections help the upsampling path recover fine-grained information from the downsampling layers.\n\nOne evolution of CNNs are [Residual Networks](https://arxiv.org/abs/1512.03385) (ResNets). ResNets are designed to ease the training of \nvery deep networks by introducing a residual block that sums the non-linear transformation of the input and its identity mapping. \nThe identity mapping is implemented by means of a shortcut connection. ResNets can be extended to work as FCNs. ResNets incorporate \nshortcut paths to FCNs and increase the number of connections within a network. This additional shortcut paths improve the segmentation \naccuracy and also help the network to converge faster.\n\nRecently another CNN architecture called [DenseNet](https://arxiv.org/abs/1608.06993) has been introduced. DenseNets are built from \n*dense blocks* and pooling operations, where each dense block is an iterative concatenation of previous feature maps. This architecture \ncan be seen as an extension of ResNets, which performs iterative summation of previous feature maps. The result of this modification \nis that DenseNets are more efficient in there parameter usage.\n\nThe [https://arxiv.org/abs/1611.09326](https://arxiv.org/abs/1611.09326) paper extends DenseNets to work as FCNs by adding an upsampling \npath to recover the full input resolution.\n \n## Train workflow\n\nClone Github repo with CamVid data\n\n    git clone https://github.com/mostafaizz/camvid.git\n\nCreate TFRecord files\n\n    python write_camvid_tfrecords.py --input-path ./camvid --output-path ./camvid-preprocessed --image-height 384 --image-width 480\n\nTrain model with cropped image size 224x224:\n\n    python -u train.py \\\n        --train-path ./camvid-preprocessed/camvid-384x480-train.tfrecords \\\n        --test-path ./camvid-preprocessed/camvid-384x480-test.tfrecords \\\n        --model-path ./models \\\n        --image-height 224 \\\n        --image-width 224 \\\n        --batch-size 5 \\\n        --crop-images \\\n        --num-crops 5\n\nRetrain model with full image size 384x480:\n\n    python -u train.py \\\n        --train-path ./camvid-preprocessed/camvid-384x480-train.tfrecords \\\n        --test-path ./camvid-preprocessed/camvid-384x480-test.tfrecords \\\n        --checkpoint-path ${CHECKPOINT_PATH}\n        --image-height 384 \\\n        --image-width 480 \\\n        --batch-size 5\n\nThe higher image size may cause OOM issues on some GPU devices. This can be solved by reducing the batch size.\n\n## Classification examples\n\nHere are the color encodings for the labels:\n\n![\"LabelsColorKey\"](images/LabelsColorKey.jpg?raw=true \"LabelsColorKey\")\n\nThe following examples show the original image, the true label map and the predicted label map:\n\n![\"camvid-segmentation-1\"](images/camvid-segmentation-1.png?raw=true \"camvid-segmentation-1\")\n\n![\"camvid-segmentation-1\"](images/camvid-segmentation-2.png?raw=true \"camvid-segmentation-2\")\n\n![\"camvid-segmentation-3\"](images/camvid-segmentation-3.png?raw=true \"camvid-segmentation-3\")\n\n![\"camvid-segmentation-4\"](images/camvid-segmentation-4.png?raw=true \"camvid-segmentation-4\")\n\n![\"camvid-segmentation-5\"](images/camvid-segmentation-5.png?raw=true \"camvid-segmentation-5\")\n\n## Metrics\n\n### Evaluation metrics for the initial training\n\nLoss:\n\n<img src=\"images/camvid_eval_loss.png\" height=\"384\" width=\"480\"/>\n\nIntersection over Union (IOU):\n\n<img src=\"images/camvid_eval_iou.png\" height=\"384\" width=\"480\"/>\n\nAccuracy:\n\n<img src=\"images/camvid_eval_accuracy.png\" height=\"384\" width=\"480\"/>\n\n\n### Evaluation metrics for the retraining\n\nLoss:\n\n<img src=\"images/camvid_eval_loss_retrain.png\" height=\"384\" width=\"480\"/>\n\nIntersection over Union (IOU):\n\n<img src=\"images/camvid_eval_iou_retrain.png\" height=\"384\" width=\"480\"/>\n\nAccuracy:\n\n<img src=\"images/camvid_eval_accuracy_retrain.png\" height=\"384\" width=\"480\"/>\n\n",
            "readme_url": "https://github.com/asprenger/keras_fc_densenet",
            "frameworks": [
                "Keras",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "Densely Connected Convolutional Networks",
            "arxiv": "1608.06993",
            "year": 2016,
            "url": "http://arxiv.org/abs/1608.06993v5",
            "abstract": "Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high performance. Code and\npre-trained models are available at https://github.com/liuzhuang13/DenseNet .",
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens van der Maaten",
                "Kilian Q. Weinberger"
            ]
        },
        {
            "title": "The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation",
            "arxiv": "1611.09326",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.09326v3",
            "abstract": "State-of-the-art approaches for semantic image segmentation are built on\nConvolutional Neural Networks (CNNs). The typical segmentation architecture is\ncomposed of (a) a downsampling path responsible for extracting coarse semantic\nfeatures, followed by (b) an upsampling path trained to recover the input image\nresolution at the output of the model and, optionally, (c) a post-processing\nmodule (e.g. Conditional Random Fields) to refine the model predictions.\n  Recently, a new CNN architecture, Densely Connected Convolutional Networks\n(DenseNets), has shown excellent results on image classification tasks. The\nidea of DenseNets is based on the observation that if each layer is directly\nconnected to every other layer in a feed-forward fashion then the network will\nbe more accurate and easier to train.\n  In this paper, we extend DenseNets to deal with the problem of semantic\nsegmentation. We achieve state-of-the-art results on urban scene benchmark\ndatasets such as CamVid and Gatech, without any further post-processing module\nnor pretraining. Moreover, due to smart construction of the model, our approach\nhas much less parameters than currently published best entries for these\ndatasets.\n  Code to reproduce the experiments is available here :\nhttps://github.com/SimJeg/FC-DenseNet/blob/master/train.py",
            "authors": [
                "Simon J\u00e9gou",
                "Michal Drozdzal",
                "David Vazquez",
                "Adriana Romero",
                "Yoshua Bengio"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "CamVid"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "COCO"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "ImageNet"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.999999700246819,
        "task": "Semantic Segmentation",
        "task_prob": 0.8229593307839722
    }
}