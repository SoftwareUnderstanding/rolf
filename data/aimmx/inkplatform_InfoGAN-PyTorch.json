{
    "visibility": {
        "visibility": "public"
    },
    "name": "InfoGAN-PyTorch",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "inkplatform",
                "owner_type": "User",
                "name": "InfoGAN-PyTorch",
                "url": "https://github.com/inkplatform/InfoGAN-PyTorch",
                "stars": 0,
                "pushed_at": "2020-06-24 11:16:41+00:00",
                "created_at": "2020-06-24 09:12:34+00:00",
                "language": "Python",
                "description": "code for InfoGAN",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".idea",
                "sha": "dab77684666ce41c5dcbcceb44b74e87defb1a16",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/inkplatform/InfoGAN-PyTorch/tree/master/.idea"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "config.py",
                "sha": "084ebd6198eed13cd9ad919ee4ba6f8d8368e945",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/inkplatform/InfoGAN-PyTorch/blob/master/config.py"
                    }
                },
                "size": 448
            },
            {
                "type": "code",
                "name": "dataloader.py",
                "sha": "c3e40d3b66e78f0e9f568fadc086f26def3d2142",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/inkplatform/InfoGAN-PyTorch/blob/master/dataloader.py"
                    }
                },
                "size": 2765
            },
            {
                "type": "code",
                "name": "mnist_generate.py",
                "sha": "989880215ddb30d3221634ceebd03f966a3c44c6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/inkplatform/InfoGAN-PyTorch/blob/master/mnist_generate.py"
                    }
                },
                "size": 2026
            },
            {
                "type": "code",
                "name": "models",
                "sha": "476bd9ba26c6efafe0a73715815caf62447624f2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/inkplatform/InfoGAN-PyTorch/tree/master/models"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "82e39df36476f56bb0b66e887099972cfa9771f5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/inkplatform/InfoGAN-PyTorch/blob/master/train.py"
                    }
                },
                "size": 11483
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "42299ac791ca51a5c9c179cd617d3026aa31faf9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/inkplatform/InfoGAN-PyTorch/blob/master/utils.py"
                    }
                },
                "size": 2306
            }
        ]
    },
    "authors": [
        {
            "name": "muuvi-bit",
            "github_id": "muuvi-bit"
        }
    ],
    "tags": [],
    "description": "code for InfoGAN",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/inkplatform/InfoGAN-PyTorch",
            "stars": 0,
            "issues": true,
            "readme": "# InfoGAN-PyTorch\n\nPyTorch implementation of [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657) with result of experiments on *MNIST*, *FashionMNIST*, *SVHN* and *CelebA* datasets.\n\n## Introduction\nInfoGAN is an information-theoretic extension to the simple Generative Adversarial Networks that is able to learn disentangled representations in a completely unsupervised manner. What this means is that InfoGAN successfully disentangle wrirting styles from digit shapes on th MNIST dataset and discover visual concepts such as hair styles and gender on the CelebA dataset. To achieve this an information-theoretic regularization is added to the loss function that enforces the maximization of mutual information between latent codes, c, and the generator distribution G(z, c).\n\n## Folder structure\nThe following shows basic folder structure.\n```\n\u251c\u2500\u2500 train.py # train script\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 mnist # mnist data (not included in this repo)\n\u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 fashion-mnist # fashion-mnist data (not included in this repo)\n\u2502\n\u251c\u2500\u2500 config.py # hyperparameters for training\n\u251c\u2500\u2500 utils.py # utils\n\u251c\u2500\u2500 dataloader.py # dataloader\n\u251c\u2500\u2500 models # infoGAN networks for different datasets\n\u2502   \u251c\u2500\u2500 mnist_model.py\n\u2502   \u251c\u2500\u2500 svhn_model.py\n\u2502   \u2514\u2500\u2500 celeba_model.py\n\u2514\u2500\u2500 results # generation results to be saved here\n```\n\n## Development Environment\n* Ubuntu 16.04 LTS\n* NVIDIA GeForce GTX 1060\n* cuda 9.0\n* Python 3.6.5\n* PyTorch 1.0.0\n* torchvision 0.2.1\n* numpy 1.14.3\n* matplotlib 2.2.2\n\n## Usage\nEdit the **`config.py`** file to select training parameters and the dataset to use. Choose *`dataset`* from **['MNIST', 'FashionMNIST', 'SVHN', 'CelebA']**\n\nTo train the model run **`train.py`**:\n```sh\npython3 train.py\n```\nAfter training the network to experiment with the latent code for the `MNIST` dataset run **`mnist_generate.py`**:\n```sh\npython3 mnist_generate.py --load_path /path/to/pth/checkpoint\n```\n\n## Results\n### MNIST\n<table align='center'>\n<tr align='center'>\n<th> Training Data </th>\n<th> Generation GIF </th>\n</tr>\n<tr>\n<td><img src = 'results/mnist_results/Training Images MNIST.png' height = '450'>\n<td><img src = 'results/mnist_results/infoGAN_MNIST.gif' height = '450'>\n</tr>\n</table>\n\n<table align='center'>\n<tr align='center'>\n<th> Epoch 1 </th>\n<th> Epoch 50 </th>\n<th> Epoch 100 </th>\n</tr>\n<tr>\n<td><img src = 'results/mnist_results/Epoch_1_MNIST.png' height = '300'>\n<td><img src = 'results/mnist_results/Epoch_50_MNIST.png' height = '300'>\n<td><img src = 'results/mnist_results/Epoch_100_MNIST.png' height = '300'>\n</tr>\n</table>\n\n#### Training Loss Curve:\n<img src = 'results/mnist_results/Loss Curve MNIST.png'>\n\n#### Manipulating Latent Code\n\n**Rotation of digits.** <br>\n*Row represents categorical variable from K = 0 to K = 9 (top to buttom) to characterize digits.\nColumn represents continuous variable varying from -2 to 2 (left to right).*<br>\n<img src = 'results/mnist_results/Rotation.png' height = '300'> <br>\n**Variation in Width**<br>\n*Row represents categorical variable from K = 0 to K = 9 (top to buttom) to characterize digits.\nColumn represents continuous variable varying from -2 to 2 (left to right).* <br>\n<img src = 'results/mnist_results/Width.png' height = '300'> \n\n\n### FashionMNIST\n<table align='center'>\n<tr align='center'>\n<th> Training Data </th>\n<th> Generation GIF </th>\n</tr>\n<tr>\n<td><img src = 'results/fashion_results/Training Images FashionMNIST.png' height = '450'>\n<td><img src = 'results/fashion_results/infoGAN_FashionMNIST.gif' height = '450'>\n</tr>\n</table>\n\n<table align='center'>\n<tr align='center'>\n<th> Epoch 1 </th>\n<th> Epoch 50 </th>\n<th> Epoch 100 </th>\n</tr>\n<tr>\n<td><img src = 'results/fashion_results/Epoch_1_FashionMNIST.png' height = '300'>\n<td><img src = 'results/fashion_results/Epoch_50_FashionMNIST.png' height = '300'>\n<td><img src = 'results/fashion_results/Epoch_100_FashionMNIST.png' height = '300'>\n</tr>\n</table>\n\n#### Training Loss Curve:\n<img src = 'results/fashion_results/Loss Curve FashionMNIST.png'>\n\n#### Manipulating Latent Code\n\n**Thickness of items.** <br>\n*Row represents categorical variable from K = 0 to K = 9 (top to buttom) to characterize items.\nColumn represents continuous variable varying from -2 to 2 (left to right).*<br>\n<img src = 'results/fashion_results/Thickness.png' height = '300'> <br>\n\n### SVHN\n<table align='center'>\n<tr align='center'>\n<th> Training Data </th>\n<th> Generation GIF </th>\n</tr>\n<tr>\n<td><img src = 'results/svhn_results/Training Images SVHN.png' height = '450'>\n<td><img src = 'results/svhn_results/infoGAN_SVHN.gif' height = '450'>\n</tr>\n</table>\n\n<table align='center'>\n<tr align='center'>\n<th> Epoch 1 </th>\n<th> Epoch 50 </th>\n<th> Epoch 100 </th>\n</tr>\n<tr>\n<td><img src = 'results/svhn_results/Epoch_1_SVHN.png' height = '300'>\n<td><img src = 'results/svhn_results/Epoch_50_SVHN.png' height = '300'>\n<td><img src = 'results/svhn_results/Epoch_100_SVHN.png' height = '300'>\n</tr>\n</table>\n\n#### Training Loss Curve:\n<img src = 'results/svhn_results/Loss Curve SVHN.png'>\n\n#### Manipulating Latent Code\n\n**Continuous Variation:** *Lighting* <br>\n<img src = 'results/svhn_results/Lighting 2.png' height = '300'> <br>\n**Discrete Variation:** *Plate Context* <br>\n<img src = 'results/svhn_results/Plate Context.png' height = '300'> <br>\n\n### CelebA\n<table align='center'>\n<tr align='center'>\n<th> Training Data </th>\n<th> Generation GIF </th>\n</tr>\n<tr>\n<td><img src = 'results/celeba_results/Training Images CelebA.png' height = '450'>\n<td><img src = 'results/celeba_results/infoGAN_CelebA.gif' height = '450'>\n</tr>\n</table>\n\n<table align='center'>\n<tr align='center'>\n<th> Epoch 1 </th>\n<th> Epoch 50 </th>\n<th> Epoch 100 </th>\n</tr>\n<tr>\n<td><img src = 'results/celeba_results/Epoch_1_CelebA.png' height = '300'>\n<td><img src = 'results/celeba_results/Epoch_50_CelebA.png' height = '300'>\n<td><img src = 'results/celeba_results/Epoch_100_CelebA.png' height = '300'>\n</tr>\n</table>\n\n#### Training Loss Curve:\n<img src = 'results/celeba_results/Loss Curve CelebA.png'>\n\n#### Manipulating Latent Code\n\n**Azimuth (pose)** <br>\n<img src = 'results/celeba_results/Azimuth.png' height = '300'> <br>\n**Gender:** *Roughly ordered from male to female (left to right)*<br>\n<img src = 'results/celeba_results/gender.png' height = '300'> <br>\n**Emotion** <br>\n<img src = 'results/celeba_results/Emotions.png' height = '300'> <br>\n**Hair Style and Color** <br>\n<img src = 'results/celeba_results/Hair Style.png' height = '300'> <br>\n**Hair Quantity:** *Roughly ordered from less hair to more hair (left to right)*<br>\n<img src = 'results/celeba_results/Hair Amount.png' height = '300'> <br>\n\n## References\n1. **Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel.** *InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.* [[arxiv](https://arxiv.org/abs/1606.03657)]\n2. **pianomania/infoGAN-pytorch** [[repo](https://github.com/pianomania/infoGAN-pytorch)]\n",
            "readme_url": "https://github.com/inkplatform/InfoGAN-PyTorch",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets",
            "arxiv": "1606.03657",
            "year": 2016,
            "url": "http://arxiv.org/abs/1606.03657v1",
            "abstract": "This paper describes InfoGAN, an information-theoretic extension to the\nGenerative Adversarial Network that is able to learn disentangled\nrepresentations in a completely unsupervised manner. InfoGAN is a generative\nadversarial network that also maximizes the mutual information between a small\nsubset of the latent variables and the observation. We derive a lower bound to\nthe mutual information objective that can be optimized efficiently, and show\nthat our training procedure can be interpreted as a variation of the Wake-Sleep\nalgorithm. Specifically, InfoGAN successfully disentangles writing styles from\ndigit shapes on the MNIST dataset, pose from lighting of 3D rendered images,\nand background digits from the central digit on the SVHN dataset. It also\ndiscovers visual concepts that include hair styles, presence/absence of\neyeglasses, and emotions on the CelebA face dataset. Experiments show that\nInfoGAN learns interpretable representations that are competitive with\nrepresentations learned by existing fully supervised methods.",
            "authors": [
                "Xi Chen",
                "Yan Duan",
                "Rein Houthooft",
                "John Schulman",
                "Ilya Sutskever",
                "Pieter Abbeel"
            ]
        },
        {
            "title": "[repo",
            "url": "https://github.com/pianomania/infoGAN-pytorch"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Fashion-MNIST"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "MNIST"
            },
            {
                "name": "CelebA"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.999936799883753,
        "task": "Image Generation",
        "task_prob": 0.9841836066035514
    }
}