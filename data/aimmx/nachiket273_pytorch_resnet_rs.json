{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "pytorch_resnet_rs",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "nachiket273",
                "owner_type": "User",
                "name": "pytorch_resnet_rs",
                "url": "https://github.com/nachiket273/pytorch_resnet_rs",
                "stars": 8,
                "pushed_at": "2021-07-09 07:15:02+00:00",
                "created_at": "2021-06-29 08:28:47+00:00",
                "language": "Python",
                "description": "Pytorch implementation of \"Revisiting ResNets: Improved Training and Scaling Strategies\"(https://arxiv.org/pdf/2103.07579.pdf)",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "b6e47617de110dea7ca47e087ff1347cc2646eda",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nachiket273/pytorch_resnet_rs/blob/main/.gitignore"
                    }
                },
                "size": 1799
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "685eb8742343173e0195f36ee697889f480f34de",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nachiket273/pytorch_resnet_rs/blob/main/LICENSE"
                    }
                },
                "size": 1065
            },
            {
                "type": "code",
                "name": "model",
                "sha": "11d6011be8cbddad0d9f61912b250c119b4ad515",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nachiket273/pytorch_resnet_rs/tree/main/model"
                    }
                },
                "num_files": 6
            }
        ]
    },
    "authors": [
        {
            "name": "Nachiket",
            "email": "nachiket.tanksale@gmail.com",
            "github_id": "nachiket273"
        }
    ],
    "tags": [
        "pytorch",
        "deep-learning",
        "resnet",
        "imagenet",
        "python3",
        "pretrained-models",
        "computer-vision"
    ],
    "description": "Pytorch implementation of \"Revisiting ResNets: Improved Training and Scaling Strategies\"(https://arxiv.org/pdf/2103.07579.pdf)",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/nachiket273/pytorch_resnet_rs",
            "stars": 8,
            "issues": true,
            "readme": "# pytorch_resnet_rs\nPytorch implementation of <a href=https://arxiv.org/pdf/2103.07579.pdf>\"Revisiting ResNets: Improved Training and Scaling Strategies\"</a>\n\n## Details\nThis repository contains pretrained weights for following models. <br>\n* resnetrs50\n* resnetrs101\n* resnetrs152\n* resnetrs200\n<br>\nPretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs are adjusted for the implementation.<br>\n<br>\nRepository also contains implementation for: <br>\n1) Exponential Moving Averages<br>\n2) RandAugment\n<br>\n<br>\nStochastic depth implementation is from <a href=\"https://github.com/rwightman/pytorch-image-models\">timm</br>\n\n## Usage\n### ResNetRS\n1) Git clone the repoository and change to directory\n```Python\ngit clone https://github.com/nachiket273/pytorch_resnet_rs.git\ncd pytorch_resnet_rs\n```\n\n2) Import\n```Python\nfrom model import ResnetRS\n```\n\n3) List Pretrained Models\n```Python\nResnetRS.list_pretrained()\n```\n\n4) Create Pretrained Model\n```Python\nResnetRS.create_pretrained(model_name, in_ch=input_channels, num_classes=num_classes,\n                           drop_rate=stochastic_depth_ratio)\n```\n5) Create Custom Model\n```Python\nfrom model.base import BasicBlock, Bottleneck\n# Specify block as either BasicBlock or Bottleneck\n# Specify list of number of ResBlocks as layers\n# e.g layers = [3, 4, 6, 3] \nResNetRS.create_model(block, layers, num_classes=1000, in_ch=3,\n                      stem_width=64, down_kernel_size=1,\n                      actn=partial(nn.ReLU, inplace=True),\n                      norm_layer=nn.BatchNorm2d, seblock=True,\n                      reduction_ratio=0.25, dropout_ratio=0.,\n                      stochastic_depth_rate=0.0,\n                      zero_init_last_bn=True)\n# If you want to load custom weights\nfrom model.util import load_checkpoint\nload_checkpoint(model, filename, strict=True)\n```\n\n### Exponential Moving Averages(EMA)\n1) Intialize\n```Python\nfrom model.ema import EMA\nema = EMA(model.parameters(), decay_rate=0.995, num_updates=0)\n```\n\n2) Usage in train loop\n```Python\nfor i, (ip, tgt) in enumerate(trainloader):\n    ...\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    ema.update(model.parameters())\n```\n\n3) Usage in validation loop\n```Python\nfor i, (ip, tgt) in enumerate(testloader):\n    ...\n    ema.store(model.parameters())\n    ema.copy(model.parameters())\n    output = model(ip)\n    loss = criterion(output, tgt)\n    ema.copy_back(model.parameters())\n```\n\n### RandAugment\n```Python\nfrom model.randaugment import RandAugment\nraug = RandAugment(n=5, m=10)\n```\n\n## Citations\n\n```bibtex\n@misc{\n    title={Revisiting ResNets: Improved Training and Scaling Strategies},\n    author={Irwan Bello, William Fedus, Xianzhi Du, Ekin D. Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens, Barret Zoph},\n    year={2021},\n    url={https://arxiv.org/pdf/2103.07579.pdf}\n}\n\n@misc{\n    title={RandAugment: Practical automated data augmentation with a reduced search space},\n    author={Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, Quoc V. Le - Google Research, Brain Team},\n    year={2019},\n    url={https://arxiv.org/pdf/1909.13719v2.pdf}\n}\n\n@misc{\n    title={Deep Networks with Stochastic Depth},\n    author={Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Q. Weinberger},\n    year={2016},\n    url={https://arxiv.org/pdf/1603.09382v3.pdf}\n}\n```\n",
            "readme_url": "https://github.com/nachiket273/pytorch_resnet_rs",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Revisiting ResNets: Improved Training and Scaling Strategies",
            "arxiv": "2103.07579",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.07579v1",
            "abstract": "Novel computer vision architectures monopolize the spotlight, but the impact\nof the model architecture is often conflated with simultaneous changes to\ntraining methodology and scaling strategies. Our work revisits the canonical\nResNet (He et al., 2015) and studies these three aspects in an effort to\ndisentangle them. Perhaps surprisingly, we find that training and scaling\nstrategies may matter more than architectural changes, and further, that the\nresulting ResNets match recent state-of-the-art models. We show that the best\nperforming scaling strategy depends on the training regime and offer two new\nscaling strategies: (1) scale model depth in regimes where overfitting can\noccur (width scaling is preferable otherwise); (2) increase image resolution\nmore slowly than previously recommended (Tan & Le, 2019). Using improved\ntraining and scaling strategies, we design a family of ResNet architectures,\nResNet-RS, which are 1.7x - 2.7x faster than EfficientNets on TPUs, while\nachieving similar accuracies on ImageNet. In a large-scale semi-supervised\nlearning setup, ResNet-RS achieves 86.2% top-1 ImageNet accuracy, while being\n4.7x faster than EfficientNet NoisyStudent. The training techniques improve\ntransfer performance on a suite of downstream tasks (rivaling state-of-the-art\nself-supervised algorithms) and extend to video classification on Kinetics-400.\nWe recommend practitioners use these simple revised ResNets as baselines for\nfuture research.",
            "authors": [
                "Irwan Bello",
                "William Fedus",
                "Xianzhi Du",
                "Ekin D. Cubuk",
                "Aravind Srinivas",
                "Tsung-Yi Lin",
                "Jonathon Shlens",
                "Barret Zoph"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.6072857612150542,
        "task": "Image Classification",
        "task_prob": 0.8845908711228153
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            }
        ]
    }
}