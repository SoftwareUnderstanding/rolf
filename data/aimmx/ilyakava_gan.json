{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "cGANs with Multi-Hinge Loss: tensorflow TPU and GPU implementation",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "ilyakava",
                "owner_type": "User",
                "name": "gan",
                "url": "https://github.com/ilyakava/gan",
                "stars": 1,
                "pushed_at": "2021-01-10 19:00:29+00:00",
                "created_at": "2020-05-19 20:17:10+00:00",
                "language": "Jupyter Notebook",
                "description": "cGANs with Multi-Hinge Loss: tensorflow TPU and GPU implementation",
                "license": "Apache License 2.0",
                "frameworks": [
                    "TensorFlow",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "985e0f145cbcc5ae2710234522419e4c65166755",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ilyakava/gan/blob/master/.gitignore"
                    }
                },
                "size": 70
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "f63d6734804dac614c84e1bcf5b5a64108cc2b74",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ilyakava/gan/blob/master/LICENSE"
                    }
                },
                "size": 11424
            },
            {
                "type": "code",
                "name": "pip_pkg.sh",
                "sha": "bf23fae0df1b92bf789901c119e4ab31c00bc73a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ilyakava/gan/blob/master/pip_pkg.sh"
                    }
                },
                "size": 1560
            },
            {
                "type": "code",
                "name": "pypi_utils.sh",
                "sha": "016534deb1926d010685a9bc63c163e86a952017",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ilyakava/gan/blob/master/pypi_utils.sh"
                    }
                },
                "size": 3843
            },
            {
                "type": "code",
                "name": "release_pypi_package.sh",
                "sha": "f8699c8fae6d961dec8a8c96f656daa2cdc4ed76",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ilyakava/gan/blob/master/release_pypi_package.sh"
                    }
                },
                "size": 2152
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "1ba2f6cd9baa6e9cff01c75a525753b862c1da3f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ilyakava/gan/blob/master/setup.py"
                    }
                },
                "size": 4465
            },
            {
                "type": "code",
                "name": "tensorflow_gan",
                "sha": "0c0776639c88aa7673bb84cef85c03a7d7410c81",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ilyakava/gan/tree/master/tensorflow_gan"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "test_releases.sh",
                "sha": "09ed3ad16e1b7621972bf216ff37268e6a923265",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ilyakava/gan/blob/master/test_releases.sh"
                    }
                },
                "size": 646
            },
            {
                "type": "code",
                "name": "venvtf2p1.yml",
                "sha": "4786b4522e0db1c31b3c85876189c0c7ab6935e3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ilyakava/gan/blob/master/venvtf2p1.yml"
                    }
                },
                "size": 3678
            }
        ]
    },
    "authors": [
        {
            "name": "Joel Shor",
            "github_id": "joel-shor"
        },
        {
            "name": "Ilya Kavalerov",
            "email": "ilyakavalerov@gmail.com",
            "github_id": "ilyakava"
        },
        {
            "name": "Artem Grunichev",
            "github_id": "Renha"
        },
        {
            "name": "Christopher Suter",
            "email": "cgs@google.com",
            "github_id": "csuter"
        },
        {
            "name": "ebrevdo",
            "github_id": "ebrevdo"
        },
        {
            "name": "pierrot0",
            "github_id": "pierrot0"
        },
        {
            "name": "Sergei Lebedev",
            "github_id": "superbobry"
        },
        {
            "name": "Uri Bram",
            "github_id": "marbiru"
        },
        {
            "name": "yongjun823",
            "email": "yongjun823@skku.edu",
            "github_id": "yongjun823"
        }
    ],
    "tags": [],
    "description": "cGANs with Multi-Hinge Loss: tensorflow TPU and GPU implementation",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/ilyakava/gan",
            "stars": 1,
            "issues": true,
            "readme": "# cGANs with Multi-Hinge Loss: tensorflow TPU and GPU implementation\n\n![MHGAN sample](./tensorflow_gan/examples/self_attention_estimator/images/both_rows.jpg)\n\nThis code is forked from TF-GAN. TF-GAN is a lightweight library for training and evaluating [Generative\nAdversarial Networks (GANs)](https://arxiv.org/abs/1406.2661).\n\nThis code implements cGANs with Multi-Hinge Loss from [this paper](https://arxiv.org/abs/1912.04216), for fully and semi supervised settings.\nIt uses the Imagenet, Cifar100, Cifar10 datasets.\n\nPlease cite:\n\n```\n@InProceedings{Kavalerov_2021_WACV,\nauthor = {Kavalerov, Ilya and Czaja, Wojciech and Chellappa, Rama},\ntitle = {A Multi-Class Hinge Loss for Conditional GANs},\nbooktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},\nmonth = {January},\nyear = {2021}\n}\n```\n\n\n## What's in this repository\n\n- Baseline SAGAN\n- Working Baseline ACGAN with many auxiliary loss choices\n- Multi-Hinge GAN\n- batched intra-fid calculation for significant speedup\n    + Imagenet 1000 class intra-fid at 2.5k images per class takes < 18h on v3-8 TPU\n- K+1 GANs\n- print out eval metrics in google cloud without tensorboard and with no more than 6GB of mem required.\n\nThese work on both TPU and GPU, but the TPU implementation has more features.\n\nThis code builds off of the tf example for [Self-Attention Generative Adversarial Networks](https://arxiv.org/abs/1805.08318).\nSee more info [here](https://github.com/tensorflow/gan/tree/master/tensorflow_gan/examples/self_attention_estimator).\nAll other examples are ignored.\nBecause of the design choices everything outside of the SAGAN example, including all tests not specified below, may no longer work.\n\n## Download pretrained models\n\n[Downloads root is here](https://drive.google.com/drive/folders/1SZeCaPrEqRYXUyhWi_BLEEpiJFdtpX1B?usp=sharing)\n\nNote: in some cases multiple checkpoints of weights are given. To control which weights are loaded edit the number in txt 'checkpoint' file in each model directory.\n\nNote: to run eval you need the original dataset.\n\n- Imagenet 128x128 SAGAN\n    - [download pretrained baseline SAGAN model here](https://drive.google.com/drive/folders/1oLBISsbAbs5G-emQ6Roix2-LOQiZEJy1?usp=sharing)\n    - step 1130000 has FID 21.9846 and IS 52.8251\n    - used in `/gan/tensorflow_gan/examples/gpu/example_genimages_imagenet128_baseline.sh`\n    - used in `/gan/tensorflow_gan/examples/gpu/example_eval_imagenet128_baseline.sh`\n- Imagenet 128x128 MHGAN\n    - [download pretrained MHGAN model here](https://drive.google.com/drive/folders/1acQL6rhSIuIzpPvymclbw6cNaee7N1Fy?usp=sharing)\n    - has FID 19.1119 and IS 60.9724\n    - used in `/gan/tensorflow_gan/examples/gpu/example_genimages_imagenet128_expt.sh`\n    - used in `/gan/tensorflow_gan/examples/gpu/example_eval_imagenet128_expt.sh`\n- Imagenet 64x64 high-fidelity-low-diversity contrast\n    - contrast the normal SAGAN model with the high-fidelity-low-diversity one at the later checkpoint\n    - [download pretrained models here](https://drive.google.com/drive/folders/1GfuPWCks08v1Ftgdd7jHL-_TRs7QxgZ-?usp=sharing)\n    - step 999999 has FID 15.482412 and IS 19.486443\n    - step 1014999 has FID 10.249778 and IS 29.656748 but very low diversity\n    - used in `/gan/tensorflow_gan/examples/gpu/example_lowdiversity_eval_imagenet64.sh`\n    - used in `/gan/tensorflow_gan/examples/gpu/example_lowdiversity_genimages_imagenet64.sh`\n- Imagenet 128x128 high-fidelity-low-diversity contrast\n    - contrast the normal SAGAN model with the high-fidelity-low-diversity one at the later checkpoint\n    - [download pretrained models here](https://drive.google.com/drive/folders/1CGjsCqhRinxB3qRf0fmpAA8fYxauOiO6?usp=sharing)\n    - step 580000 has IS 47.79 and FID 17.10\n    - step 585000 has IS 169.68 and FID 8.87 but very low diversity    \n    - used in `/gan/tensorflow_gan/examples/gpu/example_lowdiversity_genimages_imagenet128.sh`\n    - 64000 images (class is random) sampled from each of these models are [available here as tarball](https://drive.google.com/drive/folders/1OKMHYQSZCNHQBsmNCmU3xLFx-Y42NfYC?usp=sharing)\n    - [Browser viewable images here](https://drive.google.com/drive/folders/1iN7io65N7QzkiXYx079SRWr4naOCHbHU?usp=sharing) of 36 images per class from each of these models.\n\n## Performance on Imagenet128\n\n- Baseline SAGAN runs the same as seen [here](https://github.com/tensorflow/gan/tree/master/tensorflow_gan/examples/self_attention_estimator), (best ever) IS 52.79 and FID 16.39 after 1M iters. Explodes soon after.\n- MHGAN (best ever) IS 61.98 and FID 13.27 within 1M iter. Explodes around the same time.\n- ACGAN with cross entropy does (best ever) IS 48.94 and FID 24.72.\n\nBatch size of 1024, 1 D step per G step, 64 chan, and more as seen in `gan/tensorflow_gan/examples/tpu/imagenet128_baseline.sh`.\n1M steps takes about 10 days on a v3-8 TPU.\n\n## How to run\n\nSee scripts in:\n- `gan/tensorflow_gan/examples/gpu/*.sh`\n- `gan/tensorflow_gan/examples/tpu/*.sh`\n\nTo replicate the baseline:\n\n- `gan/tensorflow_gan/examples/tpu/imagenet128_baseline.sh`\n\nTo replicate MHingeGAN:\n\n- `gan/tensorflow_gan/examples/tpu/imagenet128.sh`\n\nTo eval:\n\n- `gan/tensorflow_gan/examples/tpu/eval_ifid_imagenet128.sh`\n- `gan/tensorflow_gan/examples/gpu/eval_imagenet128.sh`\n- `gan/tensorflow_gan/examples/tpu/genimages_imagenet128.sh`\n\nTo run a small experiment, see for example:\n\n- `/gan/tensorflow_gan/examples/gpu/cifar_ramawks69.sh`\n- `/gan/tensorflow_gan/examples/tpu/cifar100.sh`\n\nSuch an experiment takes only an hour to get to 200k on a v3-8 TPU.\n\n## Installation GPU\n\nUse miniconda3, python 3.7.7, tensorflow 2.1, cuda/10.1.243, cudnn/v7.6.5. Install `venvtf2p1.yml`.\n\nIn general:\n\n\n```\npip install tensorflow_datasets\npip install tensorflow_gan\npip uninstall -y tensorflow_probability\npip install tensorflow_probability==0.7\npip install pillow\n```\n\n## Setup GPU\n\n```\nexport TFGAN_REPO=`pwd`\nexport PYTHONPATH=${TFGAN_REPO}/tensorflow_gan/examples:${PYTHONPATH}\nexport PYTHONPATH=${TFGAN_REPO}:${PYTHONPATH}\ncd tensorflow_gan/examples\n```\n\nThen run a script in `gpu/`.\n\n## Requesitioning TPU\n\nSee [Quickstart](https://cloud.google.com/tpu/docs/quickstart) but in general:\n\n```\nexport PROJECT_NAME=xxx-xxx-xxx\nexport PROJECT_ID=${PROJECT_NAME}\ngcloud config set project ${PROJECT_NAME}\n\nexport ZONE=\"europe-west4-a\"\nexport REGION=\"europe-west4\"\nctpu up --zone=${ZONE} --tpu-size=\"v3-8\" --tf-version=2.1 --name=${TPU_NAME} --machine-type=n1-standard-1\n```\n\nFor lower costs the following custom size is sufficient for a v3-8 TPU.\nThe following command uses an image that can be created after launching in the general way above.\n\n```\ngcloud beta compute instances create tpu-eu-1 --zone=${ZONE} --source-machine-image tf2p1 --custom-cpu 1 --custom-memory 6 --custom-vm-type n1\n```\n\n## (Re)Connect to TPU\n\nIn general:\n\n```\ngcloud compute ssh ${TPU_NAME} --zone=${ZONE}\n```\n\nOR if using a custom instance:\n\n```\ngcloud beta compute ssh --zone=${ZONE} ${TPU_NAME} --project=${PROJECT_NAME}\n```\n\nOnce inside the TPU:\n\n```\nexport ZONE=\"europe-west4-a\"\nexport REGION=\"europe-west4\"\nexport BUCKET_NAME=\"mybucket\"\n```\n\n```\nexport PROJECT_NAME=xxx-xxx-xxx\nexport PROJECT_ID=${PROJECT_NAME}\nexport STORAGE_BUCKET=gs://${BUCKET_NAME}\nexport TPU_ZONE=${ZONE}\n```\n\n## Installation TPU\n\nUse python3.7, tensorflow 2.1.\n\nAfter launching run:\n\n```\npip3.7 install --upgrade tensorflow_datasets --user\ngit clone --single-branch --branch dev https://github.com/ilyakavagan.git\npip3.7 install tensorflow_gan --user\n```\n\n## Setup TPU\n\n```\nexport TFGAN_REPO=/home/ilyak/gan\nexport PYTHONPATH=${TFGAN_REPO}/tensorflow_gan/examples:${PYTHONPATH}\nexport PYTHONPATH=${TFGAN_REPO}:${PYTHONPATH}\n\ncd gan/tensorflow_gan/examples\n\ngit pull origin dev\nsource tpu/retry.sh\n```\n\n## TPU monitoring\n\n### Print eval metrics in the cloud\n\nThis requires a separate cpu instance, 6GB is enough memory.\n\nUse `gan/tensorflow_gan/examples/print_tf_log.py`\n\n### Monitoring in the cloud\n\nCloud tpu tensorboard crashes very often, but it does work during the 1st hour of training while there are few logfiles.\nRun it with:\n\n```\nexport TPU_IP=XX.XXX.X.X\ntensorboard --logdir=${STORAGE_BUCKET}/experiments/${EXPERIMENT_NAME} --master_tpu_unsecure_channel=${TPU_IP}\n```\n\nMost useful is the [TPU profiling](https://cloud.google.com/tpu/docs/tensorboard-setup#static-trace-viewer):\n\n```\ncapture_tpu_profile --port=8080 --tpu=${TPU_NAME} --tpu_zone=${ZONE} --logdir=gs://${BUCKET_NAME}/experiments/${EXPERIMENT_NAME}/logdir\n```\n\n### Downloading log files and monitoring locally (Not recommended)\n\n[Install gsutil](https://cloud.google.com/storage/docs/gsutil_install)\n\n`gsutil cp -R ${STORAGE_BUCKET}/experiments/${EXPERIMENT_NAME}/eval_eval ./${EXPERIMENT_NAME}/`\n\nand launch tensorboard locally. Will incure high egress costs.\n\n## TPU/Cloud Teardown\n\nCheck status with:\n\n`ctpu status --zone=$ZONE --name=${TPU_NAME}`\n\nUse the google cloud console. Note that TPUs and their CPU hosts have to be killed separately.\n- https://console.cloud.google.com/compute/tpus\n- https://console.cloud.google.com/compute/instances\n\nRemove buckets with:\n\n`gsutil rm -r gs://${BUCKET_NAME}/experiments/${EXPERIMENT_NAME}`\n\n## Setup: Prepare Data\n\nOpen a python REPL and:\n\n```\n# if you don't do this, it will crash on Imagenet\nimport resource\nlow, high = resource.getrlimit(resource.RLIMIT_NOFILE)\nresource.setrlimit(resource.RLIMIT_NOFILE, (high, high))\n\nimport tensorflow_datasets as tfds\n```\n\nThen depending on your dataset:\n\n- `ds = tfds.load('imagenet_resized/64x64', split=\"train\", data_dir=\"/mydir/data/tfdf\")`\n- `ds = tfds.load('cifar100', split=\"train\", data_dir=\"/mydir/data/tfdf\")`\n- `ds = tfds.load(\"cifar10\", split=\"train\", data_dir=\"/mydir/data/tfdf\")`\n\nFor Imagenet at 128x128, you need to manually download the data and place the compressed files in the tfdf data_dir downloads folder before running \n\n`ds = tfds.load(\"imagenet2012\", split=\"train\", data_dir=\"gs://mybucket/data\")`\n\nBe forewarned Imagenet128 takes several hours to download and over 24 hours to setup initially on a n1-standard-2 instance.\n\n## Run Tests GPU\n\nThe only tests maintained are related to intra-fid calculation and some new losses:\n\n```\nCUDA_VISIBLE_DEVICES=0 python tensorflow_gan/python/eval/eval_utils_test.py\nCUDA_VISIBLE_DEVICES=0 python tensorflow_gan/python/eval/classifier_metrics_test.py\nCUDA_VISIBLE_DEVICES=0 python tensorflow_gan/python/losses/other_losses_impl_test.py\n```\n\n## Contributions\n\nPRs are welcome! See `/gan/DEVREADME.md` for more info.\n",
            "readme_url": "https://github.com/ilyakava/gan",
            "frameworks": [
                "TensorFlow",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Self-Attention Generative Adversarial Networks",
            "arxiv": "1805.08318",
            "year": 2018,
            "url": "http://arxiv.org/abs/1805.08318v2",
            "abstract": "In this paper, we propose the Self-Attention Generative Adversarial Network\n(SAGAN) which allows attention-driven, long-range dependency modeling for image\ngeneration tasks. Traditional convolutional GANs generate high-resolution\ndetails as a function of only spatially local points in lower-resolution\nfeature maps. In SAGAN, details can be generated using cues from all feature\nlocations. Moreover, the discriminator can check that highly detailed features\nin distant portions of the image are consistent with each other. Furthermore,\nrecent work has shown that generator conditioning affects GAN performance.\nLeveraging this insight, we apply spectral normalization to the GAN generator\nand find that this improves training dynamics. The proposed SAGAN achieves the\nstate-of-the-art results, boosting the best published Inception score from 36.8\nto 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the\nchallenging ImageNet dataset. Visualization of the attention layers shows that\nthe generator leverages neighborhoods that correspond to object shapes rather\nthan local regions of fixed shape.",
            "authors": [
                "Han Zhang",
                "Ian Goodfellow",
                "Dimitris Metaxas",
                "Augustus Odena"
            ]
        },
        {
            "title": "cGANs with Multi-Hinge Loss",
            "arxiv": "1912.04216",
            "year": 2019,
            "url": "http://arxiv.org/abs/1912.04216v2",
            "abstract": "We propose a new algorithm to incorporate class conditional information into\nthe critic of GANs via a multi-class generalization of the commonly used Hinge\nloss that is compatible with both supervised and semi-supervised settings. We\nstudy the compromise between training a state of the art generator and an\naccurate classifier simultaneously, and propose a way to use our algorithm to\nmeasure the degree to which a generator and critic are class conditional. We\nshow the trade-off between a generator-critic pair respecting class\nconditioning inputs and generating the highest quality images. With our\nmulti-hinge loss modification we are able to improve Inception Scores and\nFrechet Inception Distance on the Imagenet dataset. We make our tensorflow code\navailable at https://github.com/ilyakava/gan.",
            "authors": [
                "Ilya Kavalerov",
                "Wojciech Czaja",
                "Rama Chellappa"
            ]
        },
        {
            "title": "Generative Adversarial Networks",
            "arxiv": "1406.2661",
            "year": 2014,
            "url": "http://arxiv.org/abs/1406.2661v1",
            "abstract": "We propose a new framework for estimating generative models via an\nadversarial process, in which we simultaneously train two models: a generative\nmodel G that captures the data distribution, and a discriminative model D that\nestimates the probability that a sample came from the training data rather than\nG. The training procedure for G is to maximize the probability of D making a\nmistake. This framework corresponds to a minimax two-player game. In the space\nof arbitrary functions G and D, a unique solution exists, with G recovering the\ntraining data distribution and D equal to 1/2 everywhere. In the case where G\nand D are defined by multilayer perceptrons, the entire system can be trained\nwith backpropagation. There is no need for any Markov chains or unrolled\napproximate inference networks during either training or generation of samples.\nExperiments demonstrate the potential of the framework through qualitative and\nquantitative evaluation of the generated samples.",
            "authors": [
                "Ian J. Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ]
        },
        {
            "year": "2021",
            "month": "January",
            "booktitle": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
            "title": "A Multi-Class Hinge Loss for Conditional GANs",
            "author": [
                "Kavalerov, Ilya",
                "Czaja, Wojciech",
                "Chellappa, Rama"
            ],
            "ENTRYTYPE": "inproceedings",
            "ID": "Kavalerov_2021_WACV",
            "authors": [
                "Kavalerov, Ilya",
                "Czaja, Wojciech",
                "Chellappa, Rama"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet 128x128"
            },
            {
                "name": "ImageNet"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9998981558322554,
        "task": "Image Generation",
        "task_prob": 0.9772951574859338
    }
}