{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "CrossViT : Cross-Attention Multi-Scale Vision Transformer for Image Classification",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "rishikksh20",
                "owner_type": "User",
                "name": "CrossViT-pytorch",
                "url": "https://github.com/rishikksh20/CrossViT-pytorch",
                "stars": 73,
                "pushed_at": "2021-04-07 08:42:02+00:00",
                "created_at": "2021-03-30 02:12:18+00:00",
                "language": "Python",
                "description": "Implementation of CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "b6e47617de110dea7ca47e087ff1347cc2646eda",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rishikksh20/CrossViT-pytorch/blob/master/.gitignore"
                    }
                },
                "size": 1799
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "eb916f2926421087ebfe54c0eaa97da03428852f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rishikksh20/CrossViT-pytorch/blob/master/LICENSE"
                    }
                },
                "size": 1087
            },
            {
                "type": "code",
                "name": "assets",
                "sha": "c7a0506f5cbadd19e95b241d1f71909071d766b4",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rishikksh20/CrossViT-pytorch/tree/master/assets"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "crossvit.py",
                "sha": "6cd7f4d1e76257a351167f53025bf15ee0374255",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rishikksh20/CrossViT-pytorch/blob/master/crossvit.py"
                    }
                },
                "size": 7731
            },
            {
                "type": "code",
                "name": "module.py",
                "sha": "62269b4b5be59a512b6a6a7571e92dc139a5d721",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rishikksh20/CrossViT-pytorch/blob/master/module.py"
                    }
                },
                "size": 3211
            }
        ]
    },
    "authors": [
        {
            "name": "Rishikesh (\u090b\u0937\u093f\u0915\u0947\u0936)",
            "email": "rishikksh20@gmail.com",
            "github_id": "rishikksh20"
        }
    ],
    "tags": [
        "pytorch",
        "transformers",
        "image-classification",
        "computer-vision",
        "classifier",
        "vision-transformers"
    ],
    "description": "Implementation of CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/rishikksh20/CrossViT-pytorch",
            "stars": 73,
            "issues": true,
            "readme": "# CrossViT : Cross-Attention Multi-Scale Vision Transformer for Image Classification\nThis is an unofficial PyTorch implementation of [CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification](https://arxiv.org/abs/2103.14899) .\n![](assets/model.PNG)\n\n\n## Usage :\n```python\nimport torch\nfrom crossvit import CrossViT\n\nimg = torch.ones([1, 3, 224, 224])\n    \nmodel = CrossViT(image_size = 224, channels = 3, num_classes = 100)\nout = model(img)\n\nprint(\"Shape of out :\", out.shape)      # [B, num_classes]\n\n\n```\n\n## Citation\n```\n@misc{chen2021crossvit,\n      title={CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification}, \n      author={Chun-Fu Chen and Quanfu Fan and Rameswar Panda},\n      year={2021},\n      eprint={2103.14899},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n## Acknowledgement\n* Base ViT code is borrowed from [@lucidrains](https://github.com/lucidrains) repo : https://github.com/lucidrains/vit-pytorch\n",
            "readme_url": "https://github.com/rishikksh20/CrossViT-pytorch",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
            "arxiv": "2103.14899",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.14899v2",
            "abstract": "The recently developed vision transformer (ViT) has achieved promising\nresults on image classification compared to convolutional neural networks.\nInspired by this, in this paper, we study how to learn multi-scale feature\nrepresentations in transformer models for image classification. To this end, we\npropose a dual-branch transformer to combine image patches (i.e., tokens in a\ntransformer) of different sizes to produce stronger image features. Our\napproach processes small-patch and large-patch tokens with two separate\nbranches of different computational complexity and these tokens are then fused\npurely by attention multiple times to complement each other. Furthermore, to\nreduce computation, we develop a simple yet effective token fusion module based\non cross attention, which uses a single token for each branch as a query to\nexchange information with other branches. Our proposed cross-attention only\nrequires linear time for both computational and memory complexity instead of\nquadratic time otherwise. Extensive experiments demonstrate that our approach\nperforms better than or on par with several concurrent works on vision\ntransformer, in addition to efficient CNN models. For example, on the\nImageNet1K dataset, with some architectural changes, our approach outperforms\nthe recent DeiT by a large margin of 2\\% with a small to moderate increase in\nFLOPs and model parameters. Our source codes and models are available at\n\\url{https://github.com/IBM/CrossViT}.",
            "authors": [
                "Chun-Fu Chen",
                "Quanfu Fan",
                "Rameswar Panda"
            ]
        },
        {
            "primaryclass": "cs.CV",
            "archiveprefix": "arXiv",
            "eprint": "2103.14899",
            "year": "2021",
            "author": [
                "Chen, Chun-Fu",
                "Fan, Quanfu",
                "Panda, Rameswar"
            ],
            "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
            "ENTRYTYPE": "misc",
            "ID": "chen2021crossvit",
            "authors": [
                "Chen, Chun-Fu",
                "Fan, Quanfu",
                "Panda, Rameswar"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.7894543085363017,
        "task": "Machine Translation",
        "task_prob": 0.945119274795395
    }
}