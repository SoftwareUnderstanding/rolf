{
    "visibility": {
        "visibility": "public"
    },
    "name": "LLVIP: A Visible-infrared Paired Dataset for Low-light Vision !visitors",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "bupt-ai-cz",
                "owner_type": "User",
                "name": "LLVIP",
                "url": "https://github.com/bupt-ai-cz/LLVIP",
                "stars": 220,
                "pushed_at": "2022-03-28 07:04:12+00:00",
                "created_at": "2021-06-15 08:10:42+00:00",
                "language": "Jupyter Notebook",
                "description": "LLVIP: A Visible-infrared Paired Dataset for Low-light Vision",
                "frameworks": [
                    "Caffe",
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "FusionGAN",
                "sha": "f87956a76868522751585188d5a3058d4bddf42d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bupt-ai-cz/LLVIP/tree/main/FusionGAN"
                    }
                },
                "num_files": 11
            },
            {
                "type": "code",
                "name": "Term of Use and License.md",
                "sha": "a3d29ad27f52d3c5050af57b8f36afb84c14736d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bupt-ai-cz/LLVIP/blob/main/Term of Use and License.md"
                    }
                },
                "size": 1921
            },
            {
                "type": "code",
                "name": "imagefusion_densefuse",
                "sha": "e6664f465e54ba6bac79f8bea46b80ce389bf65f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bupt-ai-cz/LLVIP/tree/main/imagefusion_densefuse"
                    }
                },
                "num_files": 18
            },
            {
                "type": "code",
                "name": "imgs",
                "sha": "082aee778c778f5ec8216ef7de2e9201994327fb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bupt-ai-cz/LLVIP/tree/main/imgs"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "pix2pixGAN",
                "sha": "248aa447cb032e585c4834712db394011dfe163e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bupt-ai-cz/LLVIP/tree/main/pix2pixGAN"
                    }
                },
                "num_files": 16
            },
            {
                "type": "code",
                "name": "yolov3",
                "sha": "dd4796cc503f46ee77a63137cb0f4b9e97a4e168",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bupt-ai-cz/LLVIP/tree/main/yolov3"
                    }
                },
                "num_files": 17
            },
            {
                "type": "code",
                "name": "yolov5",
                "sha": "8352a14a70e4dca23efb18c7204f1565311c04b3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bupt-ai-cz/LLVIP/tree/main/yolov5"
                    }
                },
                "num_files": 18
            }
        ]
    },
    "authors": [
        {
            "name": "CVSM Group -  email: czhu@bupt.edu.cn",
            "github_id": "bupt-ai-cz"
        },
        {
            "name": "shengjieLiu",
            "github_id": "SantJay"
        },
        {
            "name": "Wenqi Tang",
            "github_id": "super233"
        },
        {
            "name": "Xinchang Shen",
            "github_id": "ShenXinchang"
        }
    ],
    "tags": [
        "computer-vision",
        "visible-infrared",
        "low-light-image",
        "image-fusion",
        "object-detection",
        "cnn",
        "gan",
        "deep-learning",
        "low-light-vision",
        "image-to-image-translation",
        "iccv2021"
    ],
    "description": "LLVIP: A Visible-infrared Paired Dataset for Low-light Vision",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/bupt-ai-cz/LLVIP",
            "stars": 220,
            "issues": true,
            "readme": "# LLVIP: A Visible-infrared Paired Dataset for Low-light Vision ![visitors](https://visitor-badge.glitch.me/badge?page_id=bupt-ai-cz.LLVIP)\n[Project](https://bupt-ai-cz.github.io/LLVIP/) | [Arxiv](https://arxiv.org/abs/2108.10831) | [Kaggle](https://www.kaggle.com/c/find-person-in-the-dark) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/llvip-a-visible-infrared-paired-dataset-for/pedestrian-detection-on-llvip)](https://paperswithcode.com/sota/pedestrian-detection-on-llvip?p=llvip-a-visible-infrared-paired-dataset-for) | [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Codes%20and%20Data%20for%20Our%20Paper:%20\"LLVIP:%20A%20Visible-infrared%20Paired%20Dataset%20for%20Low-light%20Vision\"%20&url=https://github.com/bupt-ai-cz/LLVIP)  \n\n## News\n- \u26a1(2022-3-27): We released some raw data (unregistered image pairs and videos) for further research including image registration. Please visit [homepage](https://bupt-ai-cz.github.io/LLVIP/) to get the update. (2022-3-28 We have updated the link of Baidu Yun of LLVIP raw data, the data downloaded from the new link supports decompression under `windows` and `macos`. The original link only support `windows`.)\n- \u26a1(2021-12-25): We released a Kaggle Community Competition \"[Find Person in the Dark!](https://www.kaggle.com/c/find-person-in-the-dark)\" based on part of LLVIP dataset. Welcome playing and having fun! Attention: only the visible-image data we uploaded in Kaggle platform is allowed to use (the infrared images in LLVIP or other external data are forbidden).\n- \u26a1(2021-11-24): Pedestrian detection models were released. \n- \u26a1(2021-09-01): We have released the dataset, please visit [homepage](https://bupt-ai-cz.github.io/LLVIP/) to get the dataset. (Note that we removed some low-quality images from the original dataset, and for this version there are 30976 images.)\n\n---\n\n![figure1-LR](imgs/figure1-LR.png)\n\n---\n\n## Citation\nIf you use this data for your research, please cite our paper [LLVIP: A Visible-infrared Paired Dataset for Low-light Vision](https://arxiv.org/abs/2108.10831):\n\n```\n@inproceedings{jia2021llvip,\n  title={LLVIP: A Visible-infrared Paired Dataset for Low-light Vision},\n  author={Jia, Xinyu and Zhu, Chuang and Li, Minzhen and Tang, Wenqi and Zhou, Wenli},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={3496--3504},\n  year={2021}\n}\n```\n\n<h2> <p align=\"center\"> Image Fusion </p> </h2>  \n\nBaselines\n   - [GTF](https://github.com/jiayi-ma/GTF)\n   - [FusionGAN](https://github.com/jiayi-ma/FusionGAN)\n   - [Densefuse](https://github.com/hli1221/imagefusion_densefuse)\n   - [IFCNN](https://github.com/uzeful/IFCNN)\n\n## FusionGAN\n### Preparation\n- Install requirements\n  ```bash\n  git clone https://github.com/bupt-ai-cz/LLVIP.git\n  cd LLVIP/FusionGAN\n  # Create your virtual environment using anaconda\n  conda create -n FusionGAN python=3.7\n  conda activate FusionGAN\n  \n  conda install matplotlib scipy==1.2.1 tensorflow-gpu==1.14.0 \n  pip install opencv-python\n  sudo apt install libgl1-mesa-glx\n  ```\n- File structure\n  ```\n  FusionGAN\n  \u251c\u2500\u2500 ...\n  \u251c\u2500\u2500 Test_LLVIP_ir\n  |   \u251c\u2500\u2500 190001.jpg\n  |   \u251c\u2500\u2500 190002.jpg\n  |   \u2514\u2500\u2500 ...\n  \u251c\u2500\u2500 Test_LLVIP_vi\n  |   \u251c\u2500\u2500 190001.jpg\n  |   \u251c\u2500\u2500 190002.jpg\n  |   \u2514\u2500\u2500 ...\n  \u251c\u2500\u2500 Train_LLVIP_ir\n  |   \u251c\u2500\u2500 010001.jpg\n  |   \u251c\u2500\u2500 010002.jpg\n  |   \u2514\u2500\u2500 ...\n  \u2514\u2500\u2500 Train_LLVIP_vi\n      \u251c\u2500\u2500 010001.jpg\n      \u251c\u2500\u2500 010002.jpg\n      \u2514\u2500\u2500 ...\n  ```\n### Train\n  ```bash\n  python main.py --epoch 10 --batch_size 32\n  ```\nSee more training options in `main.py`.\n### Test\n  ```bash\n  python test_one_image.py\n  ```\nRemember to put pretrained model in your `checkpoint` folder and change corresponding model name in `test_one_image.py`.\nTo acquire complete LLVIP dataset, please visit https://bupt-ai-cz.github.io/LLVIP/.\n\n## Densefuse\n### Preparation\n- Install requirements\n  ```bash\n  git clone https://github.com/bupt-ai-cz/LLVIP.git\n  cd LLVIP/imagefusion_densefuse\n  \n  # Create your virtual environment using anaconda\n  conda create -n Densefuse python=3.7\n  conda activate Densefuse\n  \n  conda install scikit-image scipy==1.2.1 tensorflow-gpu==1.14.0\n  ```\n- File structure\n  ```\n  imagefusion_densefuse\n  \u251c\u2500\u2500 ...\n  \u251c\u2500\u2500datasets\n  |  \u251c\u2500\u2500010001_ir.jpg\n  |  \u251c\u2500\u2500010001_vi.jpg\n  |  \u2514\u2500\u2500 ...\n  \u251c\u2500\u2500test\n  |  \u251c\u2500\u2500190001_ir.jpg\n  |  \u251c\u2500\u2500190001_vi.jpg\n  |  \u2514\u2500\u2500 ...\n  \u2514\u2500\u2500LLVIP\n     \u251c\u2500\u2500 infrared\n     |   \u251c\u2500\u2500train\n     |   |  \u251c\u2500\u2500 010001.jpg\n     |   |  \u251c\u2500\u2500 010002.jpg\n     |   |  \u2514\u2500\u2500 ...\n     |   \u2514\u2500\u2500test\n     |      \u251c\u2500\u2500 190001.jpg\n     |      \u251c\u2500\u2500 190002.jpg\n     |      \u2514\u2500\u2500 ...\n     \u2514\u2500\u2500 visible\n         \u251c\u2500\u2500train\n         |   \u251c\u2500\u2500 010001.jpg\n         |   \u251c\u2500\u2500 010002.jpg\n         |   \u2514\u2500\u2500 ...\n         \u2514\u2500\u2500 test\n             \u251c\u2500\u2500 190001.jpg\n             \u251c\u2500\u2500 190002.jpg\n             \u2514\u2500\u2500 ...\n  ```\n  \n### Train & Test\n  ```bash\n  python main.py \n  ```\nCheck and modify training/testing options in `main.py`. Before training/testing, you need to rename the images in LLVIP dataset and put them in the designated folder. We have provided a script named `rename.py` to rename the images and save them in the `datasets` or `test` folder. Checkpoints are saved in `./models/densefuse_gray/`. To acquire complete LLVIP dataset, please visit https://bupt-ai-cz.github.io/LLVIP/.\n\n## IFCNN\nPlease visit https://github.com/uzeful/IFCNN.\n\n<h2> <p align=\"center\"> Pedestrian Detection </p> </h2> \n\nBaselines\n   - [Yolov5](https://github.com/ultralytics/yolov5)\n   - [Yolov3](https://github.com/ultralytics/yolov3)\n## Yolov5\n### Preparation\n#### Linux and Python>=3.6.0\n- Install requirements\n  ```bash\n  git clone https://github.com/bupt-ai-cz/LLVIP.git\n  cd LLVIP/yolov5\n  pip install -r requirements.txt\n  ```\n- File structure\n\n  The training set of LLVIP is used for training the yolov5 model and the testing set of LLVIP is used for the validation of the yolov5 model.\n  ```\n  yolov5\n  \u251c\u2500\u2500 ...\n  \u2514\u2500\u2500LLVIP\n     \u251c\u2500\u2500 labels\n     |   \u251c\u2500\u2500train\n     |   |  \u251c\u2500\u2500 010001.txt\n     |   |  \u251c\u2500\u2500 010002.txt\n     |   |  \u2514\u2500\u2500 ...\n     |   \u2514\u2500\u2500val\n     |      \u251c\u2500\u2500 190001.txt\n     |      \u251c\u2500\u2500 190002.txt\n     |      \u2514\u2500\u2500 ...\n     \u2514\u2500\u2500 images\n         \u251c\u2500\u2500train\n         |   \u251c\u2500\u2500 010001.jpg\n         |   \u251c\u2500\u2500 010002.jpg\n         |   \u2514\u2500\u2500 ...\n         \u2514\u2500\u2500 val\n             \u251c\u2500\u2500 190001.jpg\n             \u251c\u2500\u2500 190002.jpg\n             \u2514\u2500\u2500 ...\n  ```\n  We provide a script named `xml2txt_yolov5.py` to convert xml files to txt files, remember to modify the file path before using.\n### Train\n  ```bash\n  python train.py --img 1280 --batch 8 --epochs 200 --data LLVIP.yaml --weights yolov5l.pt --name LLVIP_export\n  ```\nSee more training options in `train.py`. The pretrained model `yolov5l.pt` can be downloaded from [here](https://github.com/ultralytics/yolov5/releases). The trained model will be saved in `./runs/train/LLVIP_export/weights` folder.\n### Test\n  ```bash\n  python val.py --data --img 1280 --weights last.pt --data LLVIP.yaml\n  ```\n  Remember to put the trained model in the same folder as `val.py`.\n  \n  Our trained model can be downloaded from here: [Google-Drive-Yolov5-model](https://drive.google.com/file/d/1SPbr0PDiItape602-g-bstkX0P7NZo0q/view?usp=sharing) or [BaiduYun-Yolov5-model](https://pan.baidu.com/s/1q3mGhQzT_D3uiqdfAHVqCA) (code: qepr)\n- Click [Here](yolov3/README.md) for the tutorial of Yolov3 \uff08Our trained Yolov3 model can be downloaded from here: [Google-Drive-Yolov3-model](https://drive.google.com/file/d/1-BYauAZGXhw7PjKp8M4CHlnMNYqm8n7J/view?usp=sharing) or [BaiduYun-Yolov3-model](https://pan.baidu.com/s/1eZcyugmpo_3VZjd3wpwcow) (code: ine5)\uff09.\n### Results\nWe retrained and tested Yolov5l and Yolov3 on the updated dataset (30976 images).\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/33684330/138012320-3340bf17-481a-4d69-a8a9-fc7427055cf4.jpg\" height=\"130\" width=\"700\">\n</div>\n\nWhere AP means the average of AP at IoU threshold of 0.5 to 0.95, with an interval of 0.05.\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/33684330/138388453-9953c403-4a5f-4b45-a488-e44fcec7b955.png\" height=\"510\" width=\"700\">\n</div>\nThe figure above shows the change of AP under different IoU thresholds. When the IoU threshold is higher than 0.7, the AP value drops rapidly. Besides, the infrared image highlights pedestrains and achieves a better effect than the visible image in the detection task, which not only proves the necessity of infrared images but also indicates that the performance of visible-image pedestrian detection algorithm is not good enough under low-light conditions.\n\nWe also calculated log average miss rate based on the test results and drew the miss rate-FPPI curve.\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/33684330/138281822-ea7dd310-bb9d-4197-8dfe-f17bb4534986.jpeg\" height=\"110\" width=\"700\">\n</div>\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/33684330/138312218-274810d5-0191-4fbc-abfe-01736dc285bf.png\" height=\"510\" width=\"700\">\n</div>\n\n<h2> <p align=\"center\"> Image-to-Image Translation </p> </h2> \n\nBaseline\n   - [pix2pixGAN](https://github.com/phillipi/pix2pix)\n## pix2pixGAN\n### Preparation\n- Install requirements\n  ```bash\n  cd pix2pixGAN\n  pip install -r requirements.txt\n  ```\n- [Prepare dataset](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md)\n- File structure\n  ```\n  pix2pixGAN\n  \u251c\u2500\u2500 ...\n  \u2514\u2500\u2500datasets\n     \u251c\u2500\u2500 ...\n     \u2514\u2500\u2500LLVIP\n        \u251c\u2500\u2500 train\n        |   \u251c\u2500\u2500 010001.jpg\n        |   \u251c\u2500\u2500 010002.jpg\n        |   \u251c\u2500\u2500 010003.jpg\n        |   \u2514\u2500\u2500 ...\n        \u2514\u2500\u2500 test\n            \u251c\u2500\u2500 190001.jpg\n            \u251c\u2500\u2500 190002.jpg\n            \u251c\u2500\u2500 190003.jpg\n            \u2514\u2500\u2500 ...\n  ```\n\n### Train\n  ```bash\n  python train.py --dataroot ./datasets/LLVIP --name LLVIP --model pix2pix --direction AtoB --batch_size 8 --preprocess scale_width_and_crop --load_size 320 --crop_size 256 --gpu_ids 0 --n_epochs 100 --n_epochs_decay 100\n  ```\n### Test\n  ```bash\n  python test.py --dataroot ./datasets/LLVIP --name LLVIP --model pix2pix --direction AtoB --gpu_ids 0 --preprocess scale_width_and_crop --load_size 320 --crop_size 256\n  ```\n  See `./pix2pixGAN/options` for more train and test options.\n\n\n<img src='imgs/LLVIP.gif' align=\"right\"  height=192 width=448>\n<br>\n\n### Results\nWe retrained and tested pix2pixGAN  on the updated dataset(30976 images). The structure of generator is unet256, and the structure of discriminator is the basic PatchGAN as default. \n\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/33684330/138233570-1440a5bf-7a05-4e96-b8ab-fc32a9c59748.jpeg\" height=\"100\" width=\"700\">\n</div>\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/33684330/135420925-72b9722a-3838-437b-b1a7-5f9e81c91d85.png\" height=\"610\" width=\"700\">\n</div>\n\n## License\nThis LLVIP Dataset is made freely available to academic and non-academic entities for non-commercial purposes such as academic research, teaching, scientific publications, or personal experimentation. Permission is granted to use the data given that you agree to our [license terms](Term%20of%20Use%20and%20License.md).\n\n## Call For Contributions\n\nWelcome to point out errors in data annotation. Also welcome to contribute more data annotations, such as segmentation. Please contact us.\n\n## Contact\n\nemail: shengjie.Liu@bupt.edu.cn, czhu@bupt.edu.cn, jiaxinyujxy@qq.com, tangwenqi@bupt.edu.cn\n",
            "readme_url": "https://github.com/bupt-ai-cz/LLVIP",
            "frameworks": [
                "Caffe",
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "LLVIP: A Visible-infrared Paired Dataset for Low-light Vision",
            "arxiv": "2108.10831",
            "year": 2021,
            "url": "http://arxiv.org/abs/2108.10831v2",
            "abstract": "It is very challenging for various visual tasks such as image fusion,\npedestrian detection and image-to-image translation in low light conditions due\nto the loss of effective target areas. In this case, infrared and visible\nimages can be used together to provide both rich detail information and\neffective target areas. In this paper, we present LLVIP, a visible-infrared\npaired dataset for low-light vision. This dataset contains 30976 images, or\n15488 pairs, most of which were taken at very dark scenes, and all of the\nimages are strictly aligned in time and space. Pedestrians in the dataset are\nlabeled. We compare the dataset with other visible-infrared datasets and\nevaluate the performance of some popular visual algorithms including image\nfusion, pedestrian detection and image-to-image translation on the dataset. The\nexperimental results demonstrate the complementary effect of fusion on image\ninformation, and find the deficiency of existing algorithms of the three visual\ntasks in very low-light conditions. We believe the LLVIP dataset will\ncontribute to the community of computer vision by promoting image fusion,\npedestrian detection and image-to-image translation in very low-light\napplications. The dataset is being released in\nhttps://bupt-ai-cz.github.io/LLVIP.",
            "authors": [
                "Xinyu Jia",
                "Chuang Zhu",
                "Minzhen Li",
                "Wenqi Tang",
                "Wenli Zhou"
            ]
        },
        {
            "year": "2021",
            "pages": "3496--3504",
            "booktitle": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "author": [
                "Jia, Xinyu",
                "Zhu, Chuang",
                "Li, Minzhen",
                "Tang, Wenqi",
                "Zhou, Wenli"
            ],
            "title": "LLVIP: A Visible-infrared Paired Dataset for Low-light Vision",
            "ENTRYTYPE": "inproceedings",
            "ID": "jia2021llvip",
            "authors": [
                "Jia, Xinyu",
                "Zhu, Chuang",
                "Li, Minzhen",
                "Tang, Wenqi",
                "Zhou, Wenli"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "LLVIP: A Visible-infrared Paired Dataset for Low-light Vision",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://arxiv.org/abs/2108.10831"
                    }
                }
            },
            {
                "name": "Prepare dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md"
                    }
                }
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.999999788612193,
        "task": "Image-to-Image Translation",
        "task_prob": 0.8406456588256102
    }
}