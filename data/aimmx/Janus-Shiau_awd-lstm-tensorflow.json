{
    "visibility": {
        "visibility": "public"
    },
    "name": "AWD-LSTM (Weight Drop LSTM) with training-award quantization in Tensorflow",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "Janus-Shiau",
                "owner_type": "User",
                "name": "awd-lstm-tensorflow",
                "url": "https://github.com/Janus-Shiau/awd-lstm-tensorflow",
                "stars": 12,
                "pushed_at": "2019-09-18 15:26:36+00:00",
                "created_at": "2019-08-24 14:04:48+00:00",
                "language": "Python",
                "description": "AWD-LSTM from \"Regularizing and Optimizing LSTM Language Models\" with training-award quantization support for tensorflow.",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "8a31d354fba1785c51deb119087e7bed5642bb2d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Janus-Shiau/awd-lstm-tensorflow/blob/master/.gitignore"
                    }
                },
                "size": 42
            },
            {
                "type": "code",
                "name": "doc",
                "sha": "75ddba2a3e92a43bfa4712e05a8d324c22ad4ca7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Janus-Shiau/awd-lstm-tensorflow/tree/master/doc"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "example_awd_lstm.py",
                "sha": "2b0f6e9072e317aa83c77ce21de797a54cf89991",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Janus-Shiau/awd-lstm-tensorflow/blob/master/example_awd_lstm.py"
                    }
                },
                "size": 5236
            },
            {
                "type": "code",
                "name": "example_vd.py",
                "sha": "4dcc9f764ac9d33c9380f0ce14ec79633b3990aa",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Janus-Shiau/awd-lstm-tensorflow/blob/master/example_vd.py"
                    }
                },
                "size": 2930
            },
            {
                "type": "code",
                "name": "quantize_ops.py",
                "sha": "a93b27ca1c8912425cfb5686bb63445e2ed2f765",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Janus-Shiau/awd-lstm-tensorflow/blob/master/quantize_ops.py"
                    }
                },
                "size": 11948
            },
            {
                "type": "code",
                "name": "variational_dropout.py",
                "sha": "db12417cfc76b9d15d7b31b71869d2cee0482c2a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Janus-Shiau/awd-lstm-tensorflow/blob/master/variational_dropout.py"
                    }
                },
                "size": 2257
            },
            {
                "type": "code",
                "name": "weight_drop_lstm.py",
                "sha": "9b466618ab833b37eadc164d09ee156b9dd7e556",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Janus-Shiau/awd-lstm-tensorflow/blob/master/weight_drop_lstm.py"
                    }
                },
                "size": 12659
            }
        ]
    },
    "authors": [
        {
            "name": "Jia-Yau Shiau",
            "email": "jiayau.shiau@gmail.com",
            "github_id": "Janus-Shiau"
        }
    ],
    "tags": [
        "deep-learning",
        "language-model",
        "recurrent-neural-networks",
        "tensorflow"
    ],
    "description": "AWD-LSTM from \"Regularizing and Optimizing LSTM Language Models\" with training-award quantization support for tensorflow.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/Janus-Shiau/awd-lstm-tensorflow",
            "stars": 12,
            "issues": true,
            "readme": "# AWD-LSTM (Weight Drop LSTM) with training-award quantization in Tensorflow\nAWD-LSTM from ([\"Regularizing and Optimizing LSTM Language Models\"](https://arxiv.org/abs/1708.02182)) for tensorflow.\n\nTraining-award quantization for integer-arithmetic-only inference ([\"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\"](https://arxiv.org/abs/1712.05877)) is also provided.\n\n## AWD-LSTM (Weight Drop LSTM)\n\n### Environment \nThis code is implemmented and tested with [tensorflow](https://www.tensorflow.org/) 1.11.0. and 1.13.0.\n\n### Usage\n1. Simply initial AWD-LSTM, it's a standard [`LayerRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerRNNCell).\n```\nfrom weight_drop_lstm import WeightDropLSTMCell\n\nlstm_cell = WeightDropLSTMCell(\n    num_units=CELL_NUM, weight_drop_kr=WEIGHT_DP_KR, \n    use_vd=True, input_size=INPUT_SIZE)\n```\nArguments are define as follows:\n> `num_units`: the number of cell in LSTM layer. [ints]\\\n> `weight_drop_kr`: the number of steps that fast weights go forward. [int]\\\n> `use_vd`: If true, using variational dropout on weight drop-connect, standard dropout otherwise. [bool]\\\n> `input_size`: If `use_vd=True`, input_size (dimension of last channel) should be provided. [int]\n\nThe remaining keyword arguments is exactly the same as [`tf.nn.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell). \n\nNoted that, if the weight_drop_kr is not provided or provided with 1.0, `WeightDropLSTMCell` is reducted as `LSTMCell`.\n\n2. Insert update operation of dropout kernel to the place you want.\n\n```\n# By simply sess.run in each training step\nsess.run(lstm_cell.get_vd_update_op())\n\n# Or use control_dependencies\nvd_update_ops = lstm_cell.get_vd_update_op() \nwith tf.control_dependencies(vd_update_ops):\n    tf.train.AdamOptimizer(learning_rate).minimize(loss)\n```\n\nYou can also add `get_vd_update_op()` to [`GraphKeys.UPDATE_OPS`](https://www.tensorflow.org/api_docs/python/tf/GraphKeys) when calling `WeightDropLSTMCell`.\n\nNoted that, if you use [`control_dependencies`](https://www.tensorflow.org/api_docs/python/tf/control_dependencies), please be careful for the order of execution.\\\nThe variational dropout kernel should not be update before the optimizer step.\n\n\n### Implementation Details\n\nThe main idea of AWD-LSTM is the drop-connect weights and concatinated inputs.\n<img src=\"doc/vd2.png\" \nalt=\"The drop-connect of weight and concatinated inputs\" border=\"10\" width=\"500\" /></a>\n\nIf `is_vd=True`, variables will be used to saved the dropout kernel.\n<img src=\"doc/vd1.png\" \nalt=\"The update operation for variational dropout\" border=\"10\" width=\"500\" /></a>\n\n\n#### Experimental results\nI have conduct experiments on a many-to-many recursive task this implementation and carry out a better results than simple `LSTMCell`.\n\n## Training-Award Quantization\n\n### In a nutshell\n```\nlstm_cell = WeightDropLSTMCell(\n    num_units=CELL_NUM, weight_drop_kr=WEIGHT_DP_KR, \n    is_quant=True, is_train=True)\n    \ntf.contrib.quantize.create_training_graph(sess.graph, quant_delay=0)\n```\n\n#### Detail explanation will be updated soon.\n\n#### Noted that: some issue of quantization will occure in `tf.while` with version higher than 1.12.0\n\n## Addiction: Variational Dropout\nI also provided a tensorflow implementation of variational dropout, which is more flexible than [`DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/DropoutWrapper) in tensorflow.\n\nThe usage is similar to using `WeightDropLSTMCell`:\n```\nfrom variational_dropout import VariationalDropout\n\nvd = VariationalDropout(input_shape=[5], keep_prob=0.5)\n\n# Directly sess.run() to update\nsess.run(vd.get_update_mask_op())\n\n# Or use control_dependencies\nwith tf.control_dependencies(vd.get_update_mask_op()):\n    step, results_array = tf.while_loop(\n        cond=lambda step, _: step < 5,\n        body=main_loop,\n        loop_vars=(step, results_array))\n\"\"\"\n    This is just a simple example. \n    Usually, control_dependencies will be placed where optimizer stepping.\n\"\"\"\n```\n\nYou can also add `get_update_mask_op()` to `GraphKeys.UPDATE_OPS` when calling `VariationalDropout`.\n\nOnce again, if you use `control_dependencies`, please be careful for the order of execution.\n\n### TO-DO\n1. Provide the regulization utilities mentioned in the paper.\n2. Maybe there is some more elegant way to implement variational dropout.\n3. Pull out quantization delay.\n4. Provide interface for non-quantized model and quantized mode.\n5. Documentation for quantization training.\n\nIf you have any suggestion, please let me know. I'll be pretty grateful!\n\n### Contact & Copy Right\nCode work by Jia-Yau Shiau <jiayau.shiau@gmail.com>.\\\nQuantization code work is advised and forked from Peter Huang <peter124574@gmail.com>\n",
            "readme_url": "https://github.com/Janus-Shiau/awd-lstm-tensorflow",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference",
            "arxiv": "1712.05877",
            "year": 2017,
            "url": "http://arxiv.org/abs/1712.05877v1",
            "abstract": "The rising popularity of intelligent mobile devices and the daunting\ncomputational cost of deep learning-based models call for efficient and\naccurate on-device inference schemes. We propose a quantization scheme that\nallows inference to be carried out using integer-only arithmetic, which can be\nimplemented more efficiently than floating point inference on commonly\navailable integer-only hardware. We also co-design a training procedure to\npreserve end-to-end model accuracy post quantization. As a result, the proposed\nquantization scheme improves the tradeoff between accuracy and on-device\nlatency. The improvements are significant even on MobileNets, a model family\nknown for run-time efficiency, and are demonstrated in ImageNet classification\nand COCO detection on popular CPUs.",
            "authors": [
                "Benoit Jacob",
                "Skirmantas Kligys",
                "Bo Chen",
                "Menglong Zhu",
                "Matthew Tang",
                "Andrew Howard",
                "Hartwig Adam",
                "Dmitry Kalenichenko"
            ]
        },
        {
            "title": "Regularizing and Optimizing LSTM Language Models",
            "arxiv": "1708.02182",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.02182v1",
            "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2.",
            "authors": [
                "Stephen Merity",
                "Nitish Shirish Keskar",
                "Richard Socher"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9901482739027969,
        "task": "Language Modelling",
        "task_prob": 0.7875960622894024
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "Penn Treebank"
            },
            {
                "name": "WikiText-2"
            },
            {
                "name": "COCO"
            }
        ]
    }
}