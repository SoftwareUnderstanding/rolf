{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "<!---",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "huggingface",
                "owner_type": "Organization",
                "name": "transformers",
                "url": "https://github.com/huggingface/transformers",
                "stars": 59710,
                "pushed_at": "2022-03-21 20:46:31+00:00",
                "created_at": "2018-10-29 13:56:00+00:00",
                "language": "Python",
                "description": "\ud83e\udd17 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.",
                "license": "Apache License 2.0",
                "frameworks": [
                    "TensorFlow",
                    "MXNet",
                    "NLTK",
                    "scikit-learn",
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".circleci",
                "sha": "55e8db31be66759ca103fc26a6839483f4846229",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/tree/master/.circleci"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": ".coveragerc",
                "sha": "9a1103b8af3d012e8894408308f4b12dbcebf58e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/.coveragerc"
                    }
                },
                "size": 207
            },
            {
                "type": "code",
                "name": ".gitattributes",
                "sha": "7a6ba382df2d9dba5655dc1c3b8a79ea15418392",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/.gitattributes"
                    }
                },
                "size": 36
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "668839b1e62df5531e89206e0d6de658b73d30a1",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/tree/master/.github"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "cf81834636134e74ed7e042a0c733ada01e35f86",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/.gitignore"
                    }
                },
                "size": 1780
            },
            {
                "type": "code",
                "name": "CITATION.cff",
                "sha": "b4d5156786f94df7ca0aebe5322b17dbb1521cb3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/CITATION.cff"
                    }
                },
                "size": 2331
            },
            {
                "type": "code",
                "name": "CODE_OF_CONDUCT.md",
                "sha": "c8ad966288a9faeeb71b2fad3ba12f6048e1a03f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/CODE_OF_CONDUCT.md"
                    }
                },
                "size": 5226
            },
            {
                "type": "code",
                "name": "CONTRIBUTING.md",
                "sha": "06497929e1496f144ad8e37af477387d1b9ab10e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md"
                    }
                },
                "size": 16271
            },
            {
                "type": "code",
                "name": "ISSUES.md",
                "sha": "fa0c896100254bc3a5db96c5758ae304c309593c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/ISSUES.md"
                    }
                },
                "size": 18849
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "68b7d66c97d66c58de883ed0c451af2b3183e6f3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/LICENSE"
                    }
                },
                "size": 11418
            },
            {
                "type": "code",
                "name": "MANIFEST.in",
                "sha": "1aba38f67a2211cf5b09466d7b411206cb7223bf",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/MANIFEST.in"
                    }
                },
                "size": 16
            },
            {
                "type": "code",
                "name": "Makefile",
                "sha": "91d51c8f72bc9d950b362ace2903b7ed01580c34",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/Makefile"
                    }
                },
                "size": 3108
            },
            {
                "type": "code",
                "name": "README_ko.md",
                "sha": "36bf28697c016501fa77d75fcd3c4e3bc85ce437",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/README_ko.md"
                    }
                },
                "size": 53661
            },
            {
                "type": "code",
                "name": "README_zh-hans.md",
                "sha": "2de4ca361bcd899d8ab7e3e0a85eab64dedc6182",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/README_zh-hans.md"
                    }
                },
                "size": 52205
            },
            {
                "type": "code",
                "name": "README_zh-hant.md",
                "sha": "dcf4a1b21cc4dadd2d6b39520976b92cfece1732",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/README_zh-hant.md"
                    }
                },
                "size": 52654
            },
            {
                "type": "code",
                "name": "conftest.py",
                "sha": "e71ada998a6df99ccd4851c210abdcd1dac8c5d6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/conftest.py"
                    }
                },
                "size": 2846
            },
            {
                "type": "code",
                "name": "docker",
                "sha": "85ccc79322dc5370d1fc27ec4f3e693c760356b7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/tree/master/docker"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "be074db7c1c7531342ea717c8ee25e66369ebf66",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/tree/master/docs"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "examples",
                "sha": "23f9134c59fb878301e2a9f5cbae6ab697b4abd4",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/tree/master/examples"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "hubconf.py",
                "sha": "6c60cd4213d5c49a8696d166c33a0176929518d5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/hubconf.py"
                    }
                },
                "size": 8496
            },
            {
                "type": "code",
                "name": "model_cards",
                "sha": "dd343fb24dd10e2d49cddea890c7834da2437260",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/tree/master/model_cards"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "notebooks",
                "sha": "fc92ae905ab2ca5c51367dc9113ae45f3ac17c1f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/tree/master/notebooks"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "pyproject.toml",
                "sha": "291558c9a3deaa48eb4f7cc16e3379b1dda8c782",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/pyproject.toml"
                    }
                },
                "size": 57
            },
            {
                "type": "code",
                "name": "scripts",
                "sha": "fb2ee7224a1e25591a480c0e562aec3d53f7bd00",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/tree/master/scripts"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "setup.cfg",
                "sha": "2d605ccceca788875ad5888c4950cf8ec3ebf8f2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/setup.cfg"
                    }
                },
                "size": 874
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "d398d59618eac057eea342f4f3b37cc3944cf237",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/setup.py"
                    }
                },
                "size": 14261
            },
            {
                "type": "code",
                "name": "src",
                "sha": "354b70ee0a4a8ce5f08f5b5d574da5943cce1ced",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/tree/master/src"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "templates",
                "sha": "1374f34ef9afa9aa279fcad65ebe8b1ede28bd84",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/tree/master/templates"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "46d829d944c78c51c58aaff37e7c1e4570e12515",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/tree/master/tests"
                    }
                },
                "num_files": 131
            },
            {
                "type": "code",
                "name": "utils",
                "sha": "a8b902257ad4cf16d1a14baec10441a6d233bf31",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/tree/master/utils"
                    }
                },
                "num_files": 21
            },
            {
                "type": "code",
                "name": "valohai.yaml",
                "sha": "14441e27d02d4e052c76fa1934c43a688efb0fbb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huggingface/transformers/blob/master/valohai.yaml"
                    }
                },
                "size": 3237
            }
        ]
    },
    "authors": [
        {
            "name": "Thomas Wolf",
            "github_id": "thomwolf"
        },
        {
            "name": "Lysandre Debut",
            "email": "lysandre.debut@reseau.eseo.fr",
            "github_id": "LysandreJik"
        },
        {
            "name": "Sylvain Gugger",
            "github_id": "sgugger"
        },
        {
            "name": "Patrick von Platen",
            "email": "patrick.v.platen@gmail.com",
            "github_id": "patrickvonplaten"
        },
        {
            "name": "Stas Bekman",
            "github_id": "stas00"
        },
        {
            "name": "Julien Chaumond",
            "email": "julien@huggingface.co",
            "github_id": "julien-c"
        },
        {
            "name": "Sam Shleifer",
            "email": "sshleifer@gmail.com",
            "github_id": "sshleifer"
        },
        {
            "name": "Victor SANH",
            "github_id": "VictorSanh"
        },
        {
            "name": "Suraj Patil",
            "email": "surajp815@gmail.com",
            "github_id": "patil-suraj"
        },
        {
            "name": "Manuel Romero",
            "email": "mrm8488@gmail.com",
            "github_id": "mrm8488"
        },
        {
            "name": "Funtowicz Morgan",
            "github_id": "mfuntowicz"
        },
        {
            "name": "Nicolas Patry",
            "github_id": "Narsil"
        },
        {
            "name": "Julien Plu",
            "github_id": "jplu"
        },
        {
            "name": "Yih-Dar",
            "github_id": "ydshieh"
        },
        {
            "name": "Aymeric Augustin",
            "github_id": "aaugustin"
        },
        {
            "name": "NielsRogge",
            "github_id": "NielsRogge"
        },
        {
            "name": "Matt",
            "github_id": "Rocketknight1"
        },
        {
            "name": "R\u00e9mi Louf",
            "email": "remilouf@gmail.com",
            "github_id": "rlouf"
        },
        {
            "name": "Stefan Schweter",
            "github_id": "stefan-it"
        },
        {
            "name": "Denis",
            "github_id": "lukovnikov"
        },
        {
            "name": "erenup",
            "github_id": "erenup"
        },
        {
            "name": "Anton Lozhkov",
            "github_id": "anton-l"
        },
        {
            "name": "Joe Davison",
            "email": "josephddavison@gmail.com",
            "github_id": "joeddav"
        },
        {
            "name": "Kamal Raj",
            "email": "kamalraj97@gmail.com",
            "github_id": "kamalkraj"
        },
        {
            "name": "AK391",
            "github_id": "AK391"
        },
        {
            "name": "Kevin Canwen Xu",
            "email": "canwenxu@outlook.com",
            "github_id": "JetRunner"
        },
        {
            "name": "Daniel Stancl",
            "github_id": "stancld"
        },
        {
            "name": "Philipp Schmid",
            "github_id": "philschmid"
        },
        {
            "name": "Bram Vanroy",
            "email": "Bram.Vanroy@UGent.be",
            "github_id": "BramVanroy"
        },
        {
            "name": "Teven",
            "github_id": "TevenLeScao"
        },
        {
            "name": "Li-Huai (Allan) Lin",
            "email": "qqaatw@gmail.com",
            "github_id": "qqaatw"
        },
        {
            "name": "Anthony MOI",
            "email": "m.anthony.moi@gmail.com",
            "github_id": "n1t0"
        },
        {
            "name": "Piero Molino",
            "email": "w4nderlust@gmail.com",
            "github_id": "w4nderlust"
        },
        {
            "name": "Gr\u00e9gory Ch\u00e2tel",
            "github_id": "rodgzilla"
        },
        {
            "name": "lewtun",
            "email": "lewis.c.tunstall@gmail.com",
            "github_id": "lewtun"
        },
        {
            "name": "Philip May",
            "email": "eniak.info@gmail.com",
            "github_id": "PhilipMay"
        },
        {
            "name": "Clement",
            "email": "clementdelangue@gmail.Com",
            "github_id": "clmnt"
        },
        {
            "name": "Bhadresh Savani",
            "email": "bhadreshpsavani@gmail.com",
            "github_id": "bhadreshpsavani"
        },
        {
            "name": "Gunjan Chhablani",
            "email": "chhablani.gunjan@gmail.com",
            "github_id": "gchhablani"
        },
        {
            "name": "Francesco Saverio Zuppichini",
            "email": "francesco.zuppichini@gmail.com",
            "github_id": "FrancescoSaverioZuppichini"
        },
        {
            "name": "Joao Gante",
            "email": "joaofranciscocardosogante@gmail.com",
            "github_id": "gante"
        },
        {
            "name": "SaulLu",
            "github_id": "SaulLu"
        },
        {
            "name": "Steven Liu",
            "github_id": "stevhliu"
        },
        {
            "name": "Gunnlaugur Thor Briem",
            "email": "gunnlaugur@gmail.com",
            "github_id": "gthb"
        },
        {
            "name": "Sanchit Gandhi",
            "github_id": "sanchit-gandhi"
        },
        {
            "name": "Boris Dayma",
            "email": "boris.dayma@gmail.com",
            "github_id": "borisdayma"
        },
        {
            "name": "Tim Rault",
            "email": "tim@huggingface.co",
            "github_id": "trault14"
        },
        {
            "name": "Michael Benayoun",
            "github_id": "michaelbenayoun"
        },
        {
            "name": "cronoik",
            "github_id": "cronoik"
        },
        {
            "name": "guillaume-be",
            "github_id": "guillaume-be"
        },
        {
            "name": "Mishig Davaadorj",
            "email": "dmishig@gmail.com",
            "github_id": "mishig25"
        },
        {
            "name": "Santiago Castro",
            "email": "bryant1410@gmail.com",
            "github_id": "bryant1410"
        },
        {
            "name": "Malte Pietsch",
            "email": "malte.pietsch@deepset.ai",
            "github_id": "tholor"
        },
        {
            "name": "Bilal Khan",
            "email": "bilal.khan1@uwaterloo.ca",
            "github_id": "bilal2vec"
        },
        {
            "name": "Catalin Voss",
            "email": "catalin@cs.stanford.edu",
            "github_id": "CatalinVoss"
        },
        {
            "name": "Lorenzo Ampil",
            "email": "lorenzojulioampil@gmail.com",
            "github_id": "enzoampil"
        },
        {
            "name": "Quentin Lhoest",
            "github_id": "lhoestq"
        },
        {
            "name": "sandip",
            "email": "sandip.aero@gmail.com",
            "github_id": "spatil6"
        },
        {
            "name": "Ali Safaya",
            "email": "alisafaya@gmail.com",
            "github_id": "alisafaya"
        },
        {
            "name": "Davide Fiocco",
            "github_id": "davidefiocco"
        },
        {
            "name": "Jannes",
            "github_id": "jannesgg"
        },
        {
            "name": "Yusuke Mori",
            "email": "ymori2112@gmail.com",
            "github_id": "forest1988"
        },
        {
            "name": "flozi00",
            "email": "flozi00.fz@gmail.com",
            "github_id": "flozi00"
        },
        {
            "name": "HUSEIN ZOLKEPLI",
            "email": "husein.zol05@gmail.com",
            "github_id": "huseinzol05"
        },
        {
            "name": "Louis Martin",
            "github_id": "louismartin"
        },
        {
            "name": "Vasudev Gupta",
            "email": "7vasudevgupta@gmail.com",
            "github_id": "vasudevgupta7"
        },
        {
            "name": "Fei Wang",
            "email": "fwang1412@gmail.com",
            "github_id": "FeiWang96"
        },
        {
            "name": "Binny Mathew",
            "email": "binny.iitkgp@gmail.com",
            "github_id": "binny-mathew"
        },
        {
            "name": "Genta Indra Winata",
            "email": "gentaindrawinata@gmail.com",
            "github_id": "gentaiscool"
        },
        {
            "name": "Jonathan Chang",
            "github_id": "cccntu"
        },
        {
            "name": "Leandro von Werra",
            "github_id": "lvwerra"
        },
        {
            "name": "Mehrdad Farahani",
            "github_id": "m3hrdadfi"
        },
        {
            "name": "Sava\u015f Y\u0131ld\u0131r\u0131m",
            "email": "savasy@gmail.com",
            "github_id": "savasy"
        },
        {
            "name": "abhishek thakur",
            "github_id": "abhishekkrthakur"
        },
        {
            "name": "dependabot[bot]",
            "github_id": "dependabot[bot]"
        },
        {
            "name": "Jared T Nielsen",
            "github_id": "jarednielsen"
        },
        {
            "name": "Alex Hedges",
            "github_id": "aphedges"
        },
        {
            "name": "Martin Malmsten",
            "email": "martin@martinmalmsten.net",
            "github_id": "marma"
        },
        {
            "name": "Will Frey",
            "github_id": "willfrey"
        },
        {
            "name": "ahotrod",
            "github_id": "ahotrod"
        },
        {
            "name": "Astariul",
            "github_id": "astariul"
        },
        {
            "name": "Abed khooli",
            "github_id": "abedkhooli"
        },
        {
            "name": "Ahmed Elnaggar",
            "email": "ahmed.elnaggar@tum.de",
            "github_id": "agemagician"
        },
        {
            "name": "Benjamin Muller",
            "email": "benjamin.muller@inria.fr",
            "github_id": "benjamin-mlr"
        },
        {
            "name": "David del R\u00edo Medina",
            "email": "ddrm86@gmail.com",
            "github_id": "ddrm86"
        },
        {
            "name": "Masatoshi Suzuki",
            "github_id": "singletongue"
        },
        {
            "name": "Nathan Raw",
            "email": "nxr9266@g.rit.edu",
            "github_id": "nateraw"
        },
        {
            "name": "Oren Amsalem",
            "github_id": "orena1"
        },
        {
            "name": "Zulfat Miftahutdinov",
            "github_id": "dartrevan"
        },
        {
            "name": "Kai Fricke",
            "github_id": "krfricke"
        },
        {
            "name": "Omar Sanseviero",
            "email": "osanseviero@gmail.com",
            "github_id": "osanseviero"
        },
        {
            "name": "Timo Moeller",
            "github_id": "Timoeller"
        },
        {
            "name": "Zhu Baohe",
            "github_id": "ZhuBaohe"
        },
        {
            "name": "Ananya Harsh Jha",
            "email": "ananyahjha93",
            "github_id": "ananyahjha93"
        },
        {
            "name": "Branden Chan",
            "github_id": "brandenchan"
        },
        {
            "name": "Ethan Perez",
            "email": "perez@nyu.edu",
            "github_id": "ethanjperez"
        },
        {
            "name": "Guillem Garc\u00eda Subies",
            "github_id": "GuillemGSubies"
        },
        {
            "name": "Sagor Sarker",
            "email": "sagorhem3532@gmail.com",
            "github_id": "sagorbrur"
        },
        {
            "name": "Jinoo",
            "github_id": "jinoobaek-qz"
        },
        {
            "name": "novice",
            "github_id": "novice03"
        },
        {
            "name": "wlhgtc",
            "github_id": "wlhgtc"
        },
        {
            "name": "Dhanajit Brahma",
            "github_id": "dhanajitb"
        },
        {
            "name": "Nils Reimers",
            "email": "info@nils-reimers.de",
            "github_id": "nreimers"
        },
        {
            "name": "Adriano Diniz",
            "email": "adordi@gmail.com",
            "github_id": "aodiniz"
        },
        {
            "name": "Jake Tae",
            "email": "jaesungtae@gmail.com",
            "github_id": "jaketae"
        },
        {
            "name": "Juha Kiili",
            "email": "juha@kiili.org",
            "github_id": "JuhaKiili"
        },
        {
            "name": "Junyi_Li",
            "email": "ljyduke@gmail.com",
            "github_id": "DukeEnglish"
        },
        {
            "name": "Marc van Zee",
            "github_id": "marcvanzee"
        },
        {
            "name": "Nikolai Korolev",
            "github_id": "CrafterKolyan"
        },
        {
            "name": "RafaelWO",
            "github_id": "RafaelWO"
        },
        {
            "name": "Ratthachat (Jung)",
            "github_id": "ratthachat"
        },
        {
            "name": "Reza Gharibi",
            "github_id": "h4iku"
        },
        {
            "name": "Shijie Wu",
            "email": "sjwu@hey.com",
            "github_id": "shijie-wu"
        },
        {
            "name": "Suzana Ili\u0107",
            "github_id": "suzana-ilic"
        },
        {
            "name": "Thomas Chaigneau",
            "github_id": "ChainYo"
        },
        {
            "name": "Will Rice",
            "email": "will@spokestack.io",
            "github_id": "will-rice"
        },
        {
            "name": "Yohei Tamura",
            "email": "tamuhey@gmail.com",
            "github_id": "tamuhey"
        },
        {
            "name": "Yongbo Wang",
            "email": "yongbowin@outlook.com",
            "github_id": "yongbowin"
        },
        {
            "name": "yzy5630",
            "github_id": "yzy5630"
        },
        {
            "name": "Kevin Ko",
            "email": "kevin.ko@tunib.ai",
            "github_id": "hyunwoongko"
        },
        {
            "name": "Prajjwal",
            "github_id": "prajjwal1"
        },
        {
            "name": "Jangwon Park",
            "email": "adieujw@gmail.com",
            "github_id": "monologg"
        },
        {
            "name": "Ngo Quang Huy",
            "email": "ngoquanghuy1999lp@gmail.com",
            "github_id": "ngoquanghuy99"
        },
        {
            "name": "Simone ",
            "email": "sadak_med@protonmail.com",
            "github_id": "sadakmed"
        },
        {
            "name": "Chi-Liang, Liu",
            "email": "liangtaiwan1230@gmail.com",
            "github_id": "Liangtaiwan"
        },
        {
            "name": "Avital Oliver",
            "email": "avital@thewe.net",
            "github_id": "avital"
        },
        {
            "name": "Bashar Talafha",
            "github_id": "bashartalafha"
        },
        {
            "name": "Bayartsogt Yadamsuren",
            "github_id": "bayartsogt-ya"
        },
        {
            "name": "Bharat Raghunathan",
            "email": "bharatraghunthan9767@gmail.com",
            "github_id": "Bharat123rox"
        },
        {
            "name": "Chan Woo Kim",
            "email": "chanwkim01@gmail.com",
            "github_id": "cwkeam"
        },
        {
            "name": "Dan Tegzes",
            "github_id": "Tegzes"
        },
        {
            "name": "Douglas Blank",
            "email": "doug.blank@gmail.com",
            "github_id": "dsblank"
        },
        {
            "name": "Eliott C.",
            "github_id": "coyotte508"
        },
        {
            "name": "Fran\u00e7ois REMY",
            "email": "francois.remy.dev@outlook.com",
            "github_id": "FremyCompany"
        },
        {
            "name": "Gabriele Sarti",
            "email": "gabriele.sarti996@gmail.com",
            "github_id": "gsarti"
        },
        {
            "name": "Hamid Shojanazeri",
            "email": "hamid.nazeri2010@gmail.com",
            "github_id": "HamidShojanazeri"
        },
        {
            "name": "Ibraheem Moosa",
            "email": "ibraheemmoosa1347@gmail.com",
            "github_id": "ibraheem-moosa"
        },
        {
            "name": "Iz Beltagy",
            "email": "iz@beltagy.net",
            "github_id": "ibeltagy"
        },
        {
            "name": "Jade Abbott",
            "email": "jabbott@retrorabbit.co.za",
            "github_id": "jaderabbit"
        },
        {
            "name": "Joel Grus",
            "github_id": "joelgrus"
        },
        {
            "name": "Karthik Uppuluri",
            "email": "karthik.uppuluri@gmail.com",
            "github_id": "kuppulur"
        },
        {
            "name": "Kiyoung Kim",
            "email": "kky416@gmail.com",
            "github_id": "kiyoungkim1"
        },
        {
            "name": "LSinev",
            "github_id": "LSinev"
        },
        {
            "name": "Martin M\u00fcller",
            "github_id": "mar-muel"
        },
        {
            "name": "Matt Maybeno",
            "email": "mmaybeno@gmail.com",
            "github_id": "mmaybeno"
        },
        {
            "name": "Mohamed El-Geish",
            "github_id": "elgeish"
        },
        {
            "name": "Navjot",
            "github_id": "navjotts"
        },
        {
            "name": "Nick Doiron",
            "email": "ndoiron@mapmeld.com",
            "github_id": "mapmeld"
        },
        {
            "name": "Pavel Belevich",
            "github_id": "pbelevich"
        },
        {
            "name": "Pierric Cistac",
            "github_id": "Pierrci"
        },
        {
            "name": "Pradhy729",
            "github_id": "Pradhy729"
        },
        {
            "name": "Qbiwan",
            "github_id": "Qbiwan"
        },
        {
            "name": "Qingqing Cao",
            "github_id": "csarron"
        },
        {
            "name": "Rohan Rajpal",
            "email": "rohan46000@gmail.com",
            "github_id": "rohanrajpal"
        },
        {
            "name": "Shamane Siri",
            "email": "shamane@ahlab.org",
            "github_id": "shamanez"
        },
        {
            "name": "Simon Brandeis",
            "github_id": "SBrandeis"
        },
        {
            "name": "Simon Layton",
            "github_id": "slayton58"
        },
        {
            "name": "Suraj Parmar",
            "email": "parmarsuraj99@gmail.com",
            "github_id": "parmarsuraj99"
        },
        {
            "name": "Tanmay Thakur",
            "email": "t.thakur@goalist.co.jp",
            "github_id": "lordtt13"
        },
        {
            "name": "Tommy Chiang",
            "github_id": "oToToT"
        },
        {
            "name": "WybeKoper",
            "github_id": "WybeKoper"
        },
        {
            "name": "Yifan Peng",
            "email": "pengyifan.mail@gmail.com",
            "github_id": "yfpeng"
        },
        {
            "name": "Zhiqi Huang",
            "email": "mazicwong@gmail.com",
            "github_id": "mazicwong"
        },
        {
            "name": "Chris Yuhao Liu",
            "github_id": "chrisliu298"
        },
        {
            "name": "ktrapeznikov",
            "github_id": "ktrapeznikov"
        },
        {
            "name": "kumapo",
            "github_id": "kumapo"
        },
        {
            "name": "rmroczkowski",
            "github_id": "rmroczkowski"
        },
        {
            "name": "yujun",
            "github_id": "JunnYu"
        },
        {
            "name": "Alan deLevie",
            "email": "adelevie@gmail.com",
            "github_id": "adelevie"
        },
        {
            "name": "Zeyao Du",
            "email": "ned1991@gmail.com",
            "github_id": "Morizeyao"
        },
        {
            "name": "MottoX",
            "github_id": "MottoX"
        },
        {
            "name": "Timothy Liu",
            "github_id": "tlkh"
        },
        {
            "name": "Wissam Antoun",
            "email": "wissam.antoun@gmail.com",
            "github_id": "WissamAntoun"
        },
        {
            "name": "Sasha Rush",
            "email": "srush.research@gmail.com",
            "github_id": "srush"
        },
        {
            "name": "Alexander Measure",
            "email": "ameasure@gmail.com",
            "github_id": "ameasure"
        },
        {
            "name": "Amil Khare",
            "email": "amilaxelius@gmail.com",
            "github_id": "aretius"
        },
        {
            "name": "Amog Kamsetty",
            "github_id": "amogkam"
        },
        {
            "name": "AndreaSottana",
            "github_id": "AndreaSottana"
        },
        {
            "name": "Antonio Carlos Falc\u00e3o Petri",
            "github_id": "falcaopetri"
        },
        {
            "name": "Antti Virtanen",
            "github_id": "haamis"
        },
        {
            "name": "Apoorv Garg",
            "github_id": "Apoorvgarg-creator"
        },
        {
            "name": "Baizhou Huang",
            "github_id": "skpig"
        },
        {
            "name": "Bhavika Tekwani",
            "github_id": "bhavika"
        },
        {
            "name": "Bhavitvya Malik",
            "email": "bhavitvya.malik@gmail.com",
            "github_id": "bhavitvyamalik"
        },
        {
            "name": "Chengxi Guo",
            "email": "mymusise1@gmail.com",
            "github_id": "mymusise"
        },
        {
            "name": "Christopher Goh",
            "email": "chrisgzf@gmail.com",
            "github_id": "chrisgzf"
        },
        {
            "name": "Darek K\u0142eczek",
            "email": "darek.kleczek@gmail.com",
            "github_id": "kldarek"
        },
        {
            "name": "Dat Quoc Nguyen",
            "github_id": "datquocnguyen"
        },
        {
            "name": "Eduardo Gonzalez Ponferrada",
            "github_id": "edugp"
        },
        {
            "name": "Eldar Kurtic",
            "github_id": "eldarkurtic"
        },
        {
            "name": "Evpok",
            "email": "evpok.padding@gmail.com",
            "github_id": "Evpok"
        },
        {
            "name": "vidiemme-brainy",
            "github_id": "vidiemme-brainy"
        },
        {
            "name": "Fran\u00e7ois Lagunas",
            "github_id": "madlag"
        },
        {
            "name": "Girishkumar",
            "github_id": "girishponkiya"
        },
        {
            "name": "Harutaka Kawamura",
            "github_id": "harupy"
        },
        {
            "name": "Hemil Desai",
            "email": "hemil.desai10@gmail.com",
            "github_id": "hemildesai"
        },
        {
            "name": "Henrik Holm",
            "github_id": "henholm"
        },
        {
            "name": "Jabin Huang",
            "email": "huangjipengnju@gmail.com",
            "github_id": "hjptriplebee"
        },
        {
            "name": "Jay Zhang",
            "github_id": "fatcat-z"
        },
        {
            "name": "Johannes Kolbe",
            "github_id": "johko"
        },
        {
            "name": "Jo\u00e3o Gustavo A. Amorim",
            "email": "joaogustavoamorim@gmail.com",
            "github_id": "johnnv1"
        },
        {
            "name": "Kilian Kluge",
            "github_id": "ionicsolutions"
        },
        {
            "name": "Matej Svejda",
            "github_id": "matej-svejda"
        },
        {
            "name": "Matthew Goldey",
            "email": "matthew.goldey@gmail.com",
            "github_id": "mgoldey"
        },
        {
            "name": "Mehrad Moradshahi",
            "email": "mehrad@stanford.edu",
            "github_id": "Mehrad0711"
        },
        {
            "name": "Moseli Motsoehli",
            "email": "moselim@hawaii.edu",
            "github_id": "DeepsMoseli"
        },
        {
            "name": "Nishant Prabhu",
            "github_id": "nishprabhu"
        },
        {
            "name": "Ola Piktus",
            "github_id": "ola13"
        },
        {
            "name": "Pasquale Minervini",
            "email": "p.minervini@gmail.com",
            "github_id": "pminervini"
        },
        {
            "name": "Patrick Sodr\u00e9",
            "github_id": "sodre"
        },
        {
            "name": "Pavel Soriano",
            "github_id": "psorianom"
        },
        {
            "name": "\u27e0 Rodolfo De Nadai",
            "email": "rdenadai@gmail.com",
            "github_id": "rdenadai"
        },
        {
            "name": "Romain Rigaux",
            "email": "hello@getromain.com",
            "github_id": "romainr"
        },
        {
            "name": "Russell Klopfer",
            "github_id": "riklopfer"
        },
        {
            "name": "Setu Shah",
            "email": "setu4993@yahoo.co.in",
            "github_id": "setu4993"
        },
        {
            "name": "Sidd Karamcheti",
            "email": "skaramcheti@cs.stanford.edu",
            "github_id": "siddk"
        },
        {
            "name": "Siddharth Jain",
            "github_id": "SidJain1412"
        },
        {
            "name": "Takuya Makino",
            "email": "takuyamakino15@gmail.com",
            "github_id": "tma15"
        },
        {
            "name": "Tanmay Garg",
            "github_id": "tanmay17061"
        },
        {
            "name": "Txus",
            "github_id": "txus"
        },
        {
            "name": "Utku Saglam",
            "github_id": "utkusaglm"
        },
        {
            "name": "Viktor Alm",
            "github_id": "ViktorAlm"
        },
        {
            "name": "Wietse de Vries",
            "email": "wietse.de.vries@rug.nl",
            "github_id": "wietsedv"
        },
        {
            "name": "Yacine Jernite",
            "github_id": "yjernite"
        },
        {
            "name": "\u5468\u4ee5\u6674",
            "github_id": "Yiqing-Zhou"
        },
        {
            "name": "Yizhe",
            "github_id": "dreasysnail"
        },
        {
            "name": "Yoshitomo Matsubara",
            "github_id": "yoshitomo-matsubara"
        },
        {
            "name": "Yuta Hayashibe",
            "github_id": "shirayu"
        },
        {
            "name": "Zane Lim",
            "github_id": "zyuanlim"
        },
        {
            "name": "Burc Turkoglu",
            "email": "burcturkoglu@gmail.com",
            "github_id": "burcturkoglu"
        },
        {
            "name": "cedspam",
            "github_id": "cedspam"
        },
        {
            "name": "chris larson",
            "github_id": "chrislarson1"
        },
        {
            "name": "Danai Antoniou",
            "github_id": "danai-antoniou"
        },
        {
            "name": "elk-cloner",
            "email": "rezakakhki.rk@gmail.com",
            "github_id": "elk-cloner"
        },
        {
            "name": "kkadowa",
            "github_id": "kkadowa"
        },
        {
            "name": "smanjil",
            "email": "shresthamanjil21@gmail.com",
            "github_id": "smanjil"
        },
        {
            "name": "tuner007",
            "github_id": "tuner007"
        },
        {
            "name": "Vladimir Blagojevic",
            "github_id": "vblagoje"
        },
        {
            "name": "zcain117",
            "github_id": "zcain117"
        },
        {
            "name": "Alexandr",
            "github_id": "av-maslov"
        },
        {
            "name": "Ben Mann",
            "github_id": "8enmann"
        },
        {
            "name": "Eliza Szczechla",
            "github_id": "elsanns"
        },
        {
            "name": "MAKSYM DEL",
            "email": "max.del.edu@gmail.com",
            "github_id": "maksym-del"
        },
        {
            "name": "Wang Ran (\u6c6a\u7136)",
            "github_id": "WrRan"
        },
        {
            "name": "Aleksey Tikhonov",
            "email": "altsoph@gmail.com",
            "github_id": "altsoph"
        },
        {
            "name": "samuelbroscheit",
            "github_id": "samuelbroscheit"
        },
        {
            "name": "ziliwang",
            "email": "wzlnot@gmail.com",
            "github_id": "ziliwang"
        },
        {
            "name": "TuringOnly",
            "email": "779222056@qq.com",
            "github_id": "guotong1988"
        },
        {
            "name": "Abhi Sharma",
            "github_id": "SudoSharma"
        },
        {
            "name": "Abhilash Majumder",
            "github_id": "abhilash1910"
        },
        {
            "name": "Adam Montgomerie",
            "email": "adam.montgomerie971@gmail.com",
            "github_id": "AMontgomerie"
        },
        {
            "name": "Adam Pocock",
            "github_id": "Craigacp"
        },
        {
            "name": "Aditya Soni",
            "email": "soniaditya922@gmail.com",
            "github_id": "AdityaSoni19031997"
        },
        {
            "name": "Akmal",
            "github_id": "Wikidepia"
        },
        {
            "name": "Aleksander Smywi\u0144ski-Pohl",
            "github_id": "apohllo"
        },
        {
            "name": "Alex Combessie",
            "email": "alex@giskard.ai",
            "github_id": "alexcombessie"
        },
        {
            "name": "Ali Hamdi Ali Fadel",
            "github_id": "AliOsm"
        },
        {
            "name": "Ali Modarressi",
            "github_id": "amodaresi"
        },
        {
            "name": "Andrea Cappelli",
            "github_id": "ak314"
        },
        {
            "name": "Andr\u00e9s Felipe Cruz",
            "github_id": "afcruzs"
        },
        {
            "name": "Antonio V Mendoza",
            "email": "antonio36764@gmail.com",
            "github_id": "eltoto1219"
        },
        {
            "name": "Arman Cohan",
            "email": "armanc@allenai.org",
            "github_id": "armancohan"
        },
        {
            "name": "Arnav Sharma",
            "email": "arnavsharma93@gmail.com",
            "github_id": "arnavsharma93"
        },
        {
            "name": "SchizoidBat",
            "github_id": "SchizoidBat"
        },
        {
            "name": "Benjamin Minixhofer",
            "email": "bminixhofer@gmail.com",
            "github_id": "bminixhofer"
        },
        {
            "name": "Binoy Dalal",
            "github_id": "bdalal"
        },
        {
            "name": "Bobby Donchev",
            "github_id": "donchev7"
        },
        {
            "name": "Brian",
            "github_id": "brian41005"
        },
        {
            "name": "CeShine Lee",
            "github_id": "ceshine"
        },
        {
            "name": "Ceyda Cinarel",
            "github_id": "cceyda"
        },
        {
            "name": "Chang-Uk Shin",
            "email": "changuk.shin.1991@gmail.com",
            "github_id": "changukshin"
        },
        {
            "name": "Christopher Akiki",
            "email": "christopher.akiki@uni-leipzig.de",
            "github_id": "cakiki"
        },
        {
            "name": "fullyz",
            "github_id": "fullyz"
        },
        {
            "name": "Clara Meister",
            "email": "meistecl@inf.ethz.ch",
            "github_id": "cimeister"
        },
        {
            "name": "Daniel Hug",
            "github_id": "danielpatrickhug"
        },
        {
            "name": "Darigov Research",
            "github_id": "darigovresearch"
        },
        {
            "name": "David Mark Nemeskey",
            "github_id": "DavidNemeskey"
        },
        {
            "name": "Derrick Blakely",
            "email": "blakelyderrick@gmail.com",
            "github_id": "dblakely"
        },
        {
            "name": "Deyu Fu",
            "email": "Deyu.Foo@gmail.com",
            "github_id": "FDecaYed"
        },
        {
            "name": "Dhaval Taunk",
            "github_id": "DhavalTaunk08"
        },
        {
            "name": "Dima",
            "github_id": "dimagalat"
        },
        {
            "name": "Elad Segal",
            "github_id": "eladsegal"
        },
        {
            "name": "Ella Charlaix",
            "github_id": "echarlaix"
        },
        {
            "name": "Emily Alsentzer",
            "github_id": "EmilyAlsentzer"
        },
        {
            "name": "Ethan",
            "email": "ethanyt@qq.com",
            "github_id": "Ethan-yt"
        },
        {
            "name": "Felipe Curti",
            "email": "fmcurti@gmail.com",
            "github_id": "fmcurti"
        },
        {
            "name": "fmikaelian",
            "github_id": "fmikaelian"
        },
        {
            "name": "Forrest Iandola",
            "github_id": "forresti"
        },
        {
            "name": "Frederik Bode",
            "email": "frederik@paperbox.ai",
            "github_id": "fredo838"
        },
        {
            "name": "GmailB ",
            "github_id": "gianfrancobarone"
        },
        {
            "name": "Gong Linyuan",
            "email": "gonglinyuan@hotmail.com",
            "github_id": "gonglinyuan"
        },
        {
            "name": "Guillaume Filion",
            "email": "guillaume.filion@gmail.com",
            "github_id": "gui11aume"
        },
        {
            "name": "Hamel Husain",
            "github_id": "hamelsmu"
        },
        {
            "name": "formiel",
            "github_id": "formiel"
        },
        {
            "name": "Henryk Borzymowski",
            "github_id": "borhenryk"
        },
        {
            "name": "mojave",
            "github_id": "mojave-pku"
        },
        {
            "name": "Hwijeen Ahn",
            "email": "hwijeen@gmail.com",
            "github_id": "hwijeen"
        },
        {
            "name": "Igli Manaj",
            "email": "manajigli@gmail.com",
            "github_id": "iglimanaj"
        },
        {
            "name": "Ikuya Yamada",
            "email": "ikuya@ikuya.net",
            "github_id": "ikuyamada"
        },
        {
            "name": "JME-P",
            "github_id": "JME-P"
        },
        {
            "name": "Jack McDonald",
            "github_id": "jcmc00"
        },
        {
            "name": "James Betker",
            "github_id": "neonbjb"
        },
        {
            "name": "ShnitzelKiller",
            "email": "jamesn8@uw.edu",
            "github_id": "ShnitzelKiller"
        },
        {
            "name": "Jason Phang",
            "github_id": "zphang"
        },
        {
            "name": "Javier de la Rosa",
            "github_id": "versae"
        },
        {
            "name": "Jay",
            "github_id": "schmidek"
        },
        {
            "name": "Jayendra",
            "email": "jayendra0parmar@gmail.com",
            "github_id": "jayendra13"
        },
        {
            "name": "Jeff Rasley",
            "email": "jeffra45@gmail.com",
            "github_id": "jeffra"
        },
        {
            "name": "Jethro Kuan",
            "email": "jethrokuan95@gmail.com",
            "github_id": "jethrokuan"
        },
        {
            "name": "Jim O\u2019Regan",
            "email": "jaoregan@tcd.ie",
            "github_id": "jimregan"
        },
        {
            "name": "John Hewitt",
            "email": "johnhew@stanford.edu",
            "github_id": "john-hewitt"
        },
        {
            "name": "Josh Tanner",
            "email": "mindful.jt@gmail.com",
            "github_id": "Mindful"
        },
        {
            "name": "Joydeep Bhattacharjee",
            "email": "joydeepubuntu@gmail.com",
            "github_id": "infinite-Joy"
        },
        {
            "name": "Koichi Yasuoka",
            "email": "yasuoka@kanji.zinbun.kyoto-u.ac.jp",
            "github_id": "KoichiYasuoka"
        },
        {
            "name": "Kyeongpil Kang",
            "email": "rudvlf0413@korea.ac.kr",
            "github_id": "Kyeongpil"
        },
        {
            "name": "Samy Lahfa",
            "email": "samy@lahfa.xyz",
            "github_id": "AkechiShiro"
        },
        {
            "name": "Lalit Pagaria",
            "github_id": "lalitpagaria"
        },
        {
            "name": "Levent Serinol",
            "github_id": "lserinol"
        },
        {
            "name": "Li Dong",
            "email": "donglixp@gmail.com",
            "github_id": "donglixp"
        },
        {
            "name": "llidev",
            "email": "li.li.signal@gmail.com",
            "github_id": "llidev"
        },
        {
            "name": "Lorenzo De Mattei",
            "email": "lorenzo.demattei@gmail.com",
            "github_id": "LoreDema"
        },
        {
            "name": "Lukas Weiner",
            "github_id": "killazz67"
        },
        {
            "name": "Masatoshi TSUCHIYA",
            "github_id": "tsuchm"
        },
        {
            "name": "Maxwell Forbes",
            "github_id": "mbforbes"
        },
        {
            "name": "Michael Watkins",
            "github_id": "watkinsm"
        },
        {
            "name": "Michal Szutenberg",
            "github_id": "szutenberg"
        },
        {
            "name": "Michal Malyska",
            "email": "malyskamichal@gmail.com",
            "github_id": "MichalMalyska"
        },
        {
            "name": "Minh Chien Vu",
            "github_id": "vumichien"
        },
        {
            "name": "Nguyen Van Nha",
            "github_id": "bino282"
        },
        {
            "name": "Nicholas Broad",
            "github_id": "nbroad1881"
        },
        {
            "name": "Nicholas Vadivelu",
            "email": "nicholas.vadivelu@gmail.com",
            "github_id": "n2cholas"
        },
        {
            "name": "Nicola De Cao",
            "email": "nicola.decao@gmail.com",
            "github_id": "nicola-decao"
        },
        {
            "name": "Oleksandr Bushkovskyi",
            "github_id": "obsh"
        },
        {
            "name": "Oliver Guhr",
            "github_id": "oliverguhr"
        },
        {
            "name": "Ori Ram",
            "email": "ori.ram@cs.tau.ac.il",
            "github_id": "oriram"
        },
        {
            "name": "Panggi Libersa Jasri Akadol",
            "github_id": "panggi"
        },
        {
            "name": "Paul O'Leary McCann",
            "email": "polm@dampfkraft.com",
            "github_id": "polm"
        },
        {
            "name": "PaulLerner",
            "github_id": "PaulLerner"
        },
        {
            "name": "Peiqin Lin",
            "email": "lpq29743@gmail.com",
            "github_id": "lpq29743"
        },
        {
            "name": "Peng Qi",
            "github_id": "qipeng"
        },
        {
            "name": "Pengcheng He",
            "github_id": "BigBird01"
        },
        {
            "name": "Pierre Guillou",
            "github_id": "piegu"
        },
        {
            "name": "Rens",
            "github_id": "RensDimmendaal"
        },
        {
            "name": "Richard Liaw",
            "email": "rliaw@berkeley.edu",
            "github_id": "richardliaw"
        },
        {
            "name": "Rohit Kumar Singh",
            "email": "rohitku.singh8@gmail.com",
            "github_id": "SKRohit"
        },
        {
            "name": "Rostislav Nedelchev",
            "github_id": "roskoN"
        },
        {
            "name": "Ryokan RI",
            "github_id": "Ryou0634"
        },
        {
            "name": "Sai Saketh Aluru",
            "email": "alurusaisaketh@gmail.com",
            "github_id": "SaiSakethAluru"
        },
        {
            "name": "Samuel",
            "github_id": "albanie"
        },
        {
            "name": "Sang-Kil Park",
            "email": "likejazz@gmail.com",
            "github_id": "likejazz"
        },
        {
            "name": "Shai Erera",
            "github_id": "shaie"
        },
        {
            "name": "Shashank Gupta",
            "github_id": "shngt"
        },
        {
            "name": "Simon Boehm",
            "github_id": "siboehm"
        },
        {
            "name": "Souvic Chakraborty",
            "email": "chakra.souvic@gmail.com",
            "github_id": "Souvic"
        },
        {
            "name": "Stella Biderman",
            "email": "stellabiderman@gmail.com",
            "github_id": "StellaAthena"
        },
        {
            "name": "Sukuya",
            "github_id": "sukuya"
        },
        {
            "name": "Surya Kasturi",
            "email": "suryak@udel.edu",
            "github_id": "ksurya"
        },
        {
            "name": "Tanay Mehta",
            "email": "heyytanay@gmail.com",
            "github_id": "heytanay"
        },
        {
            "name": "Tavin Turner",
            "github_id": "itsTurner"
        },
        {
            "name": "Terencio Agozzino",
            "github_id": "rememberYou"
        },
        {
            "name": "Thibault FEVRY",
            "email": "ThibaultFevry@gmail.com",
            "github_id": "Iwontbecreative"
        },
        {
            "name": "Thomas Wang",
            "github_id": "thomasw21"
        },
        {
            "name": "Th\u00e9o Matussi\u00e8re",
            "github_id": "theo-m"
        },
        {
            "name": "Tom Grek",
            "email": "tom.grek@gmail.com",
            "github_id": "tomgrek"
        },
        {
            "name": "Tom Hosking",
            "email": "code@tomhosking.co.uk",
            "github_id": "tomhosking"
        },
        {
            "name": "Tomy Hsieh",
            "email": "tomy0000000@gmail.com",
            "github_id": "tomy0000000"
        },
        {
            "name": "Travis M.",
            "github_id": "traviemcg"
        },
        {
            "name": "Valentin",
            "github_id": "valentindey"
        },
        {
            "name": "Vladimir Maryasin",
            "github_id": "vmaryasin"
        },
        {
            "name": "William Falcon",
            "email": "waf2107@columbia.edu",
            "github_id": "williamFalcon"
        },
        {
            "name": "Xiaohan Zou",
            "email": "renovamenzxh@gmail.com",
            "github_id": "Renovamen"
        },
        {
            "name": "Xing Han Lu",
            "github_id": "xhlulu"
        },
        {
            "name": "Hongshen Xu",
            "email": "xuhongshen@sjtu.edu.cn",
            "github_id": "importpandas"
        },
        {
            "name": "Xu Song",
            "email": "xusong.vip@gmail.com",
            "github_id": "xu-song"
        },
        {
            "name": "Yanming Wang",
            "email": "yanmingwang01@gmail.com",
            "github_id": "ymwangg"
        },
        {
            "name": "Yaser Martinez Palenzuela",
            "github_id": "elyase"
        },
        {
            "name": "Zhangyx",
            "email": "stdcoutzyx@163.com",
            "github_id": "stdcoutzyx"
        },
        {
            "name": "Alberto Blanco Garc\u00e9s",
            "email": "4lb3rduris@gmail.com",
            "github_id": "alberduris"
        },
        {
            "name": "calpt",
            "email": "calpt@mail.de",
            "github_id": "calpt"
        },
        {
            "name": "cchen-dialpad",
            "github_id": "cchen-dialpad"
        },
        {
            "name": "chutaklee",
            "email": "emctopgear@gmail.com",
            "github_id": "chutaklee"
        },
        {
            "name": "davidleonfdez",
            "github_id": "davidleonfdez"
        },
        {
            "name": "Ryan",
            "github_id": "yet-another-account"
        },
        {
            "name": "fatih",
            "github_id": "fcakyon"
        },
        {
            "name": "fgaim",
            "github_id": "fgaim"
        },
        {
            "name": "guhur",
            "github_id": "guhur"
        },
        {
            "name": "hassoudi",
            "github_id": "hassoudi"
        },
        {
            "name": "HUANG Fei",
            "email": "hzhwcmhf@gmail.com",
            "github_id": "hzhwcmhf"
        },
        {
            "name": "Eunkwang Jeon",
            "email": "jeonsworld@gmail.com",
            "github_id": "jeonsworld"
        },
        {
            "name": "jjacampos",
            "github_id": "jjacampos"
        },
        {
            "name": "karthikrangasai",
            "github_id": "karthikrangasai"
        },
        {
            "name": "kding1",
            "email": "ke.ding@intel.com",
            "github_id": "kding1"
        },
        {
            "name": "kolk",
            "github_id": "kolk"
        },
        {
            "name": "lexhuismans",
            "github_id": "lexhuismans"
        },
        {
            "name": "martindh",
            "email": "martin@dhoffschmidt.com",
            "github_id": "mdhoffschmidt"
        },
        {
            "name": "mataney",
            "email": "mataneyal1@gmail.com",
            "github_id": "mataney"
        },
        {
            "name": "moniquebm",
            "github_id": "moniquebm"
        },
        {
            "name": "nbertagnolli",
            "github_id": "nbertagnolli"
        },
        {
            "name": "noise-field",
            "github_id": "noise-field"
        },
        {
            "name": "onepointconsulting",
            "github_id": "onepointconsulting"
        },
        {
            "name": "shabie",
            "github_id": "shabie"
        },
        {
            "name": "Shoya Wada",
            "github_id": "sy-wada"
        },
        {
            "name": "tucan9389",
            "github_id": "tucan9389"
        },
        {
            "name": "Vitalii Radchenko",
            "github_id": "vitaliyradchenko"
        },
        {
            "name": "weiyumou",
            "github_id": "weiyumou"
        },
        {
            "name": "xujiaze13",
            "github_id": "xujiaze13"
        },
        {
            "name": "zijunsun",
            "github_id": "zijunsun"
        },
        {
            "name": "zolekode",
            "github_id": "zolekode"
        },
        {
            "name": "ibrahim ethem demirci",
            "github_id": "iedmrc"
        },
        {
            "name": "lee1jun",
            "email": "lee1jun@postech.ac.kr",
            "github_id": "21jun"
        },
        {
            "name": "(Bill) Yuchen Lin",
            "email": "yuchen.lin@usc.edu",
            "github_id": "yuchenlin"
        },
        {
            "name": "Aakash Tripathi",
            "email": "saaakaaa95@gmail.com",
            "github_id": "tripathiaakash"
        },
        {
            "name": "Aarni Koskela",
            "email": "akx@iki.fi",
            "github_id": "akx"
        },
        {
            "name": "Aaron Mangum",
            "github_id": "potatochip"
        },
        {
            "name": "Abdelrhman-Hosny",
            "github_id": "Abdelrhman-Hosny"
        },
        {
            "name": "Abel",
            "email": "abel@kta.io",
            "github_id": "abelriboulot"
        },
        {
            "name": "Abhishek Kumar Mishra",
            "email": "abhimishra.91@gmail.com",
            "github_id": "abhimishra91"
        },
        {
            "name": "Abi See",
            "github_id": "abisee"
        },
        {
            "name": "Adalberto",
            "email": "multiolx@gmail.com",
            "github_id": "thedarkzeno"
        },
        {
            "name": "Adam Kaczmarek",
            "email": "adamjankaczmarek@gmail.com",
            "github_id": "adamjankaczmarek"
        }
    ],
    "tags": [
        "nlp",
        "natural-language-processing",
        "pytorch",
        "language-model",
        "tensorflow",
        "bert",
        "language-models",
        "pytorch-transformers",
        "nlp-library",
        "transformer",
        "model-hub",
        "pretrained-models",
        "jax",
        "flax",
        "seq2seq",
        "speech-recognition",
        "hacktoberfest",
        "python",
        "machine-learning",
        "deep-learning"
    ],
    "description": "\ud83e\udd17 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/huggingface/transformers",
            "stars": 59710,
            "issues": true,
            "readme": "<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_logo_name.png\" width=\"400\"/>\n    <br>\n<p>\n<p align=\"center\">\n    <a href=\"https://circleci.com/gh/huggingface/transformers\">\n        <img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/master\">\n    </a>\n    <a href=\"https://github.com/huggingface/transformers/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\">\n    </a>\n    <a href=\"https://huggingface.co/docs/transformers/index\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\">\n    </a>\n    <a href=\"https://github.com/huggingface/transformers/releases\">\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\">\n    </a>\n    <a href=\"https://github.com/huggingface/transformers/blob/master/CODE_OF_CONDUCT.md\">\n        <img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\">\n    </a>\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface/transformers/blob/master/README_zh-hans.md\">\u7b80\u4f53\u4e2d\u6587</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/master/README_zh-hant.md\">\u7e41\u9ad4\u4e2d\u6587</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/master/README_ko.md\">\ud55c\uad6d\uc5b4</a>\n    <p>\n</h4>\n\n<h3 align=\"center\">\n    <p>State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow</p>\n</h3>\n\n<h3 align=\"center\">\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\n</h3>\n\n\ud83e\udd17 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. \n\nThese models can be applied on:\n\n* \ud83d\udcdd Text, for tasks like text classification, information extraction, question answering, summarization, translation, text generation, in over 100 languages. \n* \ud83d\uddbc\ufe0f Images, for tasks like image classification, object detection, and segmentation. \n* \ud83d\udde3\ufe0f Audio, for tasks like speech recognition and audio classification. \n\nTransformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n\n\ud83e\udd17 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.\n\n\ud83e\udd17 Transformers is backed by the three most popular deep learning libraries \u2014 [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) \u2014 with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.\n\n## Online demos\n\nYou can test most of our models directly on their pages from the [model hub](https://huggingface.co/models). We also offer [private model hosting, versioning, & an inference API](https://huggingface.co/pricing) for public and private models.\n\nHere are a few examples:\n\n In Natural Language Processing:\n- [Masked word completion with BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\n- [Name Entity Recognition with Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\n- [Text generation with GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)\n- [Natural Language Inference with RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)\n- [Summarization with BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)\n- [Question answering with DistilBERT](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)\n- [Translation with T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)\n\nIn Computer Vision:\n- [Image classification with ViT](https://huggingface.co/google/vit-base-patch16-224)\n- [Object Detection with DETR](https://huggingface.co/facebook/detr-resnet-50)\n- [Image Segmentation with DETR](https://huggingface.co/facebook/detr-resnet-50-panoptic)\n\nIn Audio:\n- [Automatic Speech Recognition with Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)\n- [Keyword Spotting with Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\n\n**[Write With Transformer](https://transformer.huggingface.co)**, built by the Hugging Face team, is the official demo of this repo\u2019s text generation capabilities.\n\n## If you are looking for custom support from the Hugging Face team\n\n<a target=\"_blank\" href=\"https://huggingface.co/support\">\n    <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://huggingface.co/front/thumbnails/support.png\" style=\"max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\n</a><br>\n\n## Quick tour\n\nTo immediately use a model on a given input (text, image, audio, ...), we provide the `pipeline` API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts:\n\n```python\n>>> from transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('We are very happy to introduce pipeline to the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```\n\nThe second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here the answer is \"positive\" with a confidence of 99.97%.\n\nMany NLP tasks have a pre-trained `pipeline` ready to go. For example, we can easily extract question answers given context:\n\n``` python\n>>> from transformers import pipeline\n\n# Allocate a pipeline for question-answering\n>>> question_answerer = pipeline('question-answering')\n>>> question_answerer({\n...     'question': 'What is the name of the repository ?',\n...     'context': 'Pipeline has been included in the huggingface/transformers repository'\n... })\n{'score': 0.30970096588134766, 'start': 34, 'end': 58, 'answer': 'huggingface/transformers'}\n\n```\n\nIn addition to the answer, the pretrained model used here returned its confidence score, along with the start position and end position of the answer in the tokenized sentence. You can learn more about the tasks supported by the `pipeline` API in [this tutorial](https://huggingface.co/docs/transformers/task_summary).\n\nTo download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version:\n```python\n>>> from transformers import AutoTokenizer, AutoModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> model = AutoModel.from_pretrained(\"bert-base-uncased\")\n\n>>> inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n```\nAnd here is the equivalent code for TensorFlow:\n```python\n>>> from transformers import AutoTokenizer, TFAutoModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n\n>>> inputs = tokenizer(\"Hello world!\", return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n```\n\nThe tokenizer is responsible for all the preprocessing the pretrained model expects, and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator.\n\nThe model itself is a regular [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) or a [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) (depending on your backend) which you can use normally. [This tutorial](https://huggingface.co/docs/transformers/training) explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our `Trainer` API to quickly fine-tune on a new dataset.\n\n## Why should I use transformers?\n\n1. Easy-to-use state-of-the-art models:\n    - High performance on natural language understanding & generation, computer vision, and audio tasks.\n    - Low barrier to entry for educators and practitioners.\n    - Few user-facing abstractions with just three classes to learn.\n    - A unified API for using all our pretrained models.\n\n1. Lower compute costs, smaller carbon footprint:\n    - Researchers can share trained models instead of always retraining.\n    - Practitioners can reduce compute time and production costs.\n    - Dozens of architectures with over 20,000 pretrained models, some in more than 100 languages.\n\n1. Choose the right framework for every part of a model's lifetime:\n    - Train state-of-the-art models in 3 lines of code.\n    - Move a single model between TF2.0/PyTorch/JAX frameworks at will.\n    - Seamlessly pick the right framework for training, evaluation and production.\n\n1. Easily customize a model or an example to your needs:\n    - We provide examples for each architecture to reproduce the results published by its original authors.\n    - Model internals are exposed as consistently as possible.\n    - Model files can be used independently of the library for quick experiments.\n\n## Why shouldn't I use transformers?\n\n- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n- The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops, you should use another library.\n- While we strive to present as many use cases as possible, the scripts in our [examples folder](https://github.com/huggingface/transformers/tree/master/examples) are just that: examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.\n\n## Installation\n\n### With pip\n\nThis repository is tested on Python 3.6+, Flax 0.3.2+, PyTorch 1.3.1+ and TensorFlow 2.3+.\n\nYou should install \ud83e\udd17 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n\nFirst, create a virtual environment with the version of Python you're going to use and activate it.\n\nThen, you will need to install at least one of Flax, PyTorch or TensorFlow.\nPlease refer to [TensorFlow installation page](https://www.tensorflow.org/install/), [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation) installation pages regarding the specific install command for your platform.\n\nWhen one of those backends has been installed, \ud83e\udd17 Transformers can be installed using pip as follows:\n\n```bash\npip install transformers\n```\n\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must [install the library from source](https://huggingface.co/docs/transformers/installation#installing-from-source).\n\n### With conda\n\nSince Transformers version v4.0.0, we now have a conda channel: `huggingface`.\n\n\ud83e\udd17 Transformers can be installed using conda as follows:\n\n```shell script\nconda install -c huggingface transformers\n```\n\nFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.\n\n## Model architectures\n\n**[All the model checkpoints](https://huggingface.co/models)** provided by \ud83e\udd17 Transformers are seamlessly integrated from the huggingface.co [model hub](https://huggingface.co) where they are uploaded directly by [users](https://huggingface.co/users) and [organizations](https://huggingface.co/organizations).\n\nCurrent number of checkpoints: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)\n\n\ud83e\udd17 Transformers currently provides the following architectures (see [here](https://huggingface.co/docs/transformers/model_summary) for a high-level summary of each them):\n\n1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\n1. **[BART](https://huggingface.co/docs/transformers/model_doc/bart)** (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.\n1. **[BARThez](https://huggingface.co/docs/transformers/model_doc/barthez)** (from \u00c9cole polytechnique) released with the paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.\n1. **[BARTpho](https://huggingface.co/docs/transformers/model_doc/bartpho)** (from VinAI Research) released with the paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.\n1. **[BEiT](https://huggingface.co/docs/transformers/model_doc/beit)** (from Microsoft) released with the paper [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by Hangbo Bao, Li Dong, Furu Wei.\n1. **[BERT](https://huggingface.co/docs/transformers/model_doc/bert)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\n1. **[BERTweet](https://huggingface.co/docs/transformers/model_doc/bertweet)** (from VinAI Research) released with the paper [BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/) by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.\n1. **[BERT For Sequence Generation](https://huggingface.co/docs/transformers/model_doc/bert-generation)** (from Google) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n1. **[BigBird-RoBERTa](https://huggingface.co/docs/transformers/model_doc/big_bird)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n1. **[BigBird-Pegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n1. **[Blenderbot](https://huggingface.co/docs/transformers/model_doc/blenderbot)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n1. **[BlenderbotSmall](https://huggingface.co/docs/transformers/model_doc/blenderbot-small)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n1. **[BORT](https://huggingface.co/docs/transformers/model_doc/bort)** (from Alexa) released with the paper [Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499) by Adrian de Wynter and Daniel J. Perry.\n1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n1. **[ConvNeXT](https://huggingface.co/docs/transformers/master/model_doc/convnext)** (from Facebook AI) released with the paper [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\n1. **[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)** (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.\n1. **[ConvBERT](https://huggingface.co/docs/transformers/model_doc/convbert)** (from YituTech) released with the paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\n1. **[CPM](https://huggingface.co/docs/transformers/model_doc/cpm)** (from Tsinghua University) released with the paper [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\n1. **[CTRL](https://huggingface.co/docs/transformers/model_doc/ctrl)** (from Salesforce) released with the paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.\n1. **[Data2Vec](https://huggingface.co/docs/transformers/master/model_doc/data2vec)** (from Facebook) released with the paper [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.\n1. **[DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n1. **[DeBERTa-v2](https://huggingface.co/docs/transformers/model_doc/deberta-v2)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n1. **[DiT](https://huggingface.co/docs/transformers/master/model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n1. **[DeiT](https://huggingface.co/docs/transformers/model_doc/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou.\n1. **[DETR](https://huggingface.co/docs/transformers/model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n1. **[DialoGPT](https://huggingface.co/docs/transformers/model_doc/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n1. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/master/examples/research_projects/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/master/examples/research_projects/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/master/examples/research_projects/distillation) and a German version of DistilBERT.\n1. **[DPR](https://huggingface.co/docs/transformers/model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval\nfor Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin, Barlas O\u011fuz, Sewon\nMin, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.\n1. **[EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoder-decoder)** (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n1. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n1. **[FlauBERT](https://huggingface.co/docs/transformers/model_doc/flaubert)** (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, Lo\u00efc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Beno\u00eet Crabb\u00e9, Laurent Besacier, Didier Schwab.\n1. **[FNet](https://huggingface.co/docs/transformers/model_doc/fnet)** (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.\n1. **[Funnel Transformer](https://huggingface.co/docs/transformers/model_doc/funnel)** (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.\n1. **[GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)** (from OpenAI) released with the paper [Improving Language Understanding by Generative Pre-Training](https://blog.openai.com/language-unsupervised/) by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.\n1. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)** (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/) by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.\n1. **[GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)** (from EleutherAI) released in the repository [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) by Ben Wang and Aran Komatsuzaki.\n1. **[GPT Neo](https://huggingface.co/docs/transformers/model_doc/gpt_neo)** (from EleutherAI) released in the repository [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.\n1. **[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)** (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\n1. **[I-BERT](https://huggingface.co/docs/transformers/model_doc/ibert)** (from Berkeley) released with the paper [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.\n1. **[ImageGPT](https://huggingface.co/docs/transformers/master/model_doc/imagegpt)** (from OpenAI) released with the paper [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/) by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.\n1. **[LayoutLM](https://huggingface.co/docs/transformers/model_doc/layoutlm)** (from Microsoft Research Asia) released with the paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.\n1. **[LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)** (from Microsoft Research Asia) released with the paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.\n1. **[LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)** (from Microsoft Research Asia) released with the paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.\n1. **[LED](https://huggingface.co/docs/transformers/model_doc/led)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n1. **[Longformer](https://huggingface.co/docs/transformers/model_doc/longformer)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n1. **[LUKE](https://huggingface.co/docs/transformers/model_doc/luke)** (from Studio Ousia) released with the paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.\n1. **[mLUKE](https://huggingface.co/docs/transformers/model_doc/mluke)** (from Studio Ousia) released with the paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.\n1. **[LXMERT](https://huggingface.co/docs/transformers/model_doc/lxmert)** (from UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://arxiv.org/abs/1908.07490) by Hao Tan and Mohit Bansal.\n1. **[M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)** (from Facebook) released with the paper [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n1. **[MarianMT](https://huggingface.co/docs/transformers/model_doc/marian)** Machine translation models trained using [OPUS](http://opus.nlpl.eu/) data by J\u00f6rg Tiedemann. The [Marian Framework](https://marian-nmt.github.io/) is being developed by the Microsoft Translator Team.\n1. **[MaskFormer](https://huggingface.co/docs/transformers/master/model_doc/maskformer)** (from Meta and UIUC) released with the paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.\n1. **[MBart](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.\n1. **[MBart-50](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\n1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\n1. **[Megatron-GPT2](https://huggingface.co/docs/transformers/model_doc/megatron_gpt2)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\n1. **[MPNet](https://huggingface.co/docs/transformers/model_doc/mpnet)** (from Microsoft Research) released with the paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n1. **[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)** (from Google AI) released with the paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.\n1. **[Nystr\u00f6mformer](https://huggingface.co/docs/transformers/master/model_doc/nystromformer)** (from the University of Wisconsin - Madison) released with the paper [Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\n1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.\n1. **[Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver)** (from Deepmind) released with the paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier H\u00e9naff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Jo\u00e3o Carreira.\n1. **[PhoBERT](https://huggingface.co/docs/transformers/model_doc/phobert)** (from VinAI Research) released with the paper [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/) by Dat Quoc Nguyen and Anh Tuan Nguyen.\n1. **[PLBart](https://huggingface.co/docs/transformers/master/model_doc/plbart)** (from UCLA NLP) released with the paper [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\n1. **[PoolFormer](https://huggingface.co/docs/transformers/master/model_doc/poolformer)** (from Sea AI Labs) released with the paper [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418) by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.\n1. **[ProphetNet](https://huggingface.co/docs/transformers/model_doc/prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\n1. **[QDQBert](https://huggingface.co/docs/transformers/model_doc/qdqbert)** (from NVIDIA) released with the paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.\n1. **[REALM](https://huggingface.co/transformers/model_doc/realm.html)** (from Google Research) released with the paper [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.\n1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya.\n1. **[RemBERT](https://huggingface.co/docs/transformers/model_doc/rembert)** (from Google Research) released with the paper [Rethinking embedding coupling in pre-trained language models](https://arxiv.org/abs/2010.12821) by Hyung Won Chung, Thibault F\u00e9vry, Henry Tsai, M. Johnson, Sebastian Ruder.\n1. **[ResNet](https://huggingface.co/docs/transformers/master/model_doc/resnet)** (from Microsoft Research) released with the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n1. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** (from Facebook), released together with the paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n1. **[RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer)** (from ZhuiyiTechnology), released together with the paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\n1. **[SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\n1. **[SEW](https://huggingface.co/docs/transformers/model_doc/sew)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n1. **[SEW-D](https://huggingface.co/docs/transformers/model_doc/sew_d)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n1. **[SpeechToTextTransformer](https://huggingface.co/docs/transformers/model_doc/speech_to_text)** (from Facebook), released together with the paper [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.\n1. **[SpeechToTextTransformer2](https://huggingface.co/docs/transformers/model_doc/speech_to_text_2)** (from Facebook), released together with the paper [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\n1. **[Splinter](https://huggingface.co/docs/transformers/model_doc/splinter)** (from Tel Aviv University), released together with the paper [Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.\n1. **[SqueezeBert](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\n1. **[Swin Transformer](https://huggingface.co/docs/transformers/master/model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\n1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\n1. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)** (from Google AI) released in the repository [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\n1. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)** (from Google AI) released with the paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349) by Jonathan Herzig, Pawe\u0142 Krzysztof Nowak, Thomas M\u00fcller, Francesco Piccinno and Julian Martin Eisenschlos.\n1. **[Transformer-XL](https://huggingface.co/docs/transformers/model_doc/transfo-xl)** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n1. **[TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)** (from Microsoft), released together with the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\n1. **[UniSpeech](https://huggingface.co/docs/transformers/model_doc/unispeech)** (from Microsoft Research) released with the paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.\n1. **[UniSpeechSat](https://huggingface.co/docs/transformers/model_doc/unispeech-sat)** (from Microsoft Research) released with the paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER\nAWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.\n1. **[VAN](https://huggingface.co/docs/transformers/master/model_doc/van)** (from Tsinghua University and Nankai University) released with the paper [Visual Attention Network](https://arxiv.org/abs/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n1. **[ViLT](https://huggingface.co/docs/transformers/master/model_doc/vilt)** (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.\n1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[ViTMAE](https://huggingface.co/docs/transformers/master/model_doc/vit_mae)** (from Meta AI) released with the paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick.\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n1. **[WavLM](https://huggingface.co/docs/transformers/master/model_doc/wavlm)** (from Microsoft Research) released with the paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.\n1. **[Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)** (from Facebook AI) released with the paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n1. **[Wav2Vec2Phoneme](https://huggingface.co/docs/master/transformers/model_doc/wav2vec2_phoneme)** (from Facebook AI) released with the paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://arxiv.org/abs/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.\n1. **[XGLM](https://huggingface.co/docs/master/transformers/model_doc/xglm)** (From Facebook AI) released with the paper [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668) by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\n1. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau.\n1. **[XLM-ProphetNet](https://huggingface.co/docs/transformers/model_doc/xlm-prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\n1. **[XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta)** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.\n1. **[XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/master/model_doc/xlm-roberta-xl)** (from Facebook AI), released together with the paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\n1. **[XLNet](https://huggingface.co/docs/transformers/model_doc/xlnet)** (from Google/CMU) released with the paper [\u200bXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (from Facebook AI) released with the paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.\n1. **[XLS-R](https://huggingface.co/docs/master/transformers/model_doc/xls_r)** (from Facebook AI) released with the paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\n1. **[YOSO](https://huggingface.co/docs/transformers/master/model_doc/yoso)** (from the University of Wisconsin - Madison) released with the paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://arxiv.org/abs/2111.09714) by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.\n1. Want to contribute a new model? We have added a **detailed guide and templates** to guide you in the process of adding a new model. You can find them in the [`templates`](./templates) folder of the repository. Be sure to check the [contributing guidelines](./CONTRIBUTING.md) and contact the maintainers or open an issue to collect feedbacks before starting your PR.\n\nTo check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the \ud83e\udd17 Tokenizers library, refer to [this table](https://huggingface.co/docs/transformers/index#supported-frameworks).\n\nThese implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the [documentation](https://huggingface.co/docs/transformers/examples).\n\n\n## Learn more\n\n| Section | Description |\n|-|-|\n| [Documentation](https://huggingface.co/docs/transformers/) | Full API documentation and tutorials |\n| [Task summary](https://huggingface.co/docs/transformers/task_summary) | Tasks supported by \ud83e\udd17 Transformers |\n| [Preprocessing tutorial](https://huggingface.co/docs/transformers/preprocessing) | Using the `Tokenizer` class to prepare data for the models |\n| [Training and fine-tuning](https://huggingface.co/docs/transformers/training) | Using the models provided by \ud83e\udd17 Transformers in a PyTorch/TensorFlow training loop and the `Trainer` API |\n| [Quick tour: Fine-tuning/usage scripts](https://github.com/huggingface/transformers/tree/master/examples) | Example scripts for fine-tuning models on a wide range of tasks |\n| [Model sharing and uploading](https://huggingface.co/docs/transformers/model_sharing) | Upload and share your fine-tuned models with the community |\n| [Migration](https://huggingface.co/docs/transformers/migration) | Migrate to \ud83e\udd17 Transformers from `pytorch-transformers` or `pytorch-pretrained-bert` |\n\n## Citation\n\nWe now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cite for the \ud83e\udd17 Transformers library:\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\u00e9mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = oct,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\n    pages = \"38--45\"\n}\n```\n",
            "readme_url": "https://github.com/huggingface/transformers",
            "frameworks": [
                "TensorFlow",
                "MXNet",
                "NLTK",
                "scikit-learn",
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "arxiv": "2010.11929",
            "year": 2020,
            "url": "http://arxiv.org/abs/2010.11929v2",
            "abstract": "While the Transformer architecture has become the de-facto standard for\nnatural language processing tasks, its applications to computer vision remain\nlimited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional\nnetworks while keeping their overall structure in place. We show that this\nreliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks.\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\nTransformer (ViT) attains excellent results compared to state-of-the-art\nconvolutional networks while requiring substantially fewer computational\nresources to train.",
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ]
        },
        {
            "title": "Few-Shot Question Answering by Pretraining Span Selection",
            "arxiv": "2101.00438",
            "year": 2021,
            "url": "http://arxiv.org/abs/2101.00438v2",
            "abstract": "In several question answering benchmarks, pretrained models have reached\nhuman parity through fine-tuning on an order of 100,000 annotated questions and\nanswers. We explore the more realistic few-shot setting, where only a few\nhundred training examples are available, and observe that standard models\nperform poorly, highlighting the discrepancy between current pretraining\nobjectives and question answering. We propose a new pretraining scheme tailored\nfor question answering: recurring span selection. Given a passage with multiple\nsets of recurring spans, we mask in each set all recurring spans but one, and\nask the model to select the correct span in the passage for each masked span.\nMasked spans are replaced with a special token, viewed as a question\nrepresentation, that is later used during fine-tuning to select the answer\nspan. The resulting model obtains surprisingly good results on multiple\nbenchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while\nmaintaining competitive performance in the high-resource setting.",
            "authors": [
                "Ori Ram",
                "Yuval Kirstain",
                "Jonathan Berant",
                "Amir Globerson",
                "Omer Levy"
            ]
        },
        {
            "title": "BEiT: BERT Pre-Training of Image Transformers",
            "arxiv": "2106.08254",
            "year": 2021,
            "url": "http://arxiv.org/abs/2106.08254v1",
            "abstract": "We introduce a self-supervised vision representation model BEiT, which stands\nfor Bidirectional Encoder representation from Image Transformers. Following\nBERT developed in the natural language processing area, we propose a masked\nimage modeling task to pretrain vision Transformers. Specifically, each image\nhas two views in our pre-training, i.e, image patches (such as 16x16 pixels),\nand visual tokens (i.e., discrete tokens). We first \"tokenize\" the original\nimage into visual tokens. Then we randomly mask some image patches and fed them\ninto the backbone Transformer. The pre-training objective is to recover the\noriginal visual tokens based on the corrupted image patches. After pre-training\nBEiT, we directly fine-tune the model parameters on downstream tasks by\nappending task layers upon the pretrained encoder. Experimental results on\nimage classification and semantic segmentation show that our model achieves\ncompetitive results with previous pre-training methods. For example, base-size\nBEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming\nfrom-scratch DeiT training (81.8%) with the same setup. Moreover, large-size\nBEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with\nsupervised pre-training on ImageNet-22K (85.2%). The code and pretrained models\nare available at https://aka.ms/beit.",
            "authors": [
                "Hangbo Bao",
                "Li Dong",
                "Furu Wei"
            ]
        },
        {
            "title": "ConvBERT: Improving BERT with Span-based Dynamic Convolution",
            "arxiv": "2008.02496",
            "year": 2020,
            "url": "http://arxiv.org/abs/2008.02496v3",
            "abstract": "Pre-trained language models like BERT and its variants have recently achieved\nimpressive performance in various natural language understanding tasks.\nHowever, BERT heavily relies on the global self-attention block and thus\nsuffers large memory footprint and computation cost. Although all its attention\nheads query on the whole input sequence for generating the attention map from a\nglobal perspective, we observe some heads only need to learn local\ndependencies, which means the existence of computation redundancy. We therefore\npropose a novel span-based dynamic convolution to replace these self-attention\nheads to directly model local dependencies. The novel convolution heads,\ntogether with the rest self-attention heads, form a new mixed attention block\nthat is more efficient at both global and local context learning. We equip BERT\nwith this mixed attention design and build a ConvBERT model. Experiments have\nshown that ConvBERT significantly outperforms BERT and its variants in various\ndownstream tasks, with lower training cost and fewer model parameters.\nRemarkably, ConvBERTbase model achieves 86.4 GLUE score, 0.7 higher than\nELECTRAbase, while using less than 1/4 training cost. Code and pre-trained\nmodels will be released.",
            "authors": [
                "Zihang Jiang",
                "Weihao Yu",
                "Daquan Zhou",
                "Yunpeng Chen",
                "Jiashi Feng",
                "Shuicheng Yan"
            ]
        },
        {
            "title": "I-BERT: Integer-only BERT Quantization",
            "arxiv": "2101.01321",
            "year": 2021,
            "url": "http://arxiv.org/abs/2101.01321v3",
            "abstract": "Transformer based models, like BERT and RoBERTa, have achieved\nstate-of-the-art results in many Natural Language Processing tasks. However,\ntheir memory footprint, inference latency, and power consumption are\nprohibitive efficient inference at the edge, and even at the data center. While\nquantization can be a viable solution for this, previous work on quantizing\nTransformer based models use floating-point arithmetic during inference, which\ncannot efficiently utilize integer-only logical units such as the recent Turing\nTensor Cores, or traditional integer-only ARM processors. In this work, we\npropose I-BERT, a novel quantization scheme for Transformer based models that\nquantizes the entire inference with integer-only arithmetic. Based on\nlightweight integer-only approximation methods for nonlinear operations, e.g.,\nGELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end\ninteger-only BERT inference without any floating point calculation. We evaluate\nour approach on GLUE downstream tasks using RoBERTa-Base/Large. We show that\nfor both cases, I-BERT achieves similar (and slightly higher) accuracy as\ncompared to the full-precision baseline. Furthermore, our preliminary\nimplementation of I-BERT shows a speedup of 2.4-4.0x for INT8 inference on a T4\nGPU system as compared to FP32 inference. The framework has been developed in\nPyTorch and has been open-sourced.",
            "authors": [
                "Sehoon Kim",
                "Amir Gholami",
                "Zhewei Yao",
                "Michael W. Mahoney",
                "Kurt Keutzer"
            ]
        },
        {
            "title": "Unsupervised Cross-lingual Representation Learning for Speech Recognition",
            "arxiv": "2006.13979",
            "year": 2020,
            "url": "http://arxiv.org/abs/2006.13979v2",
            "abstract": "This paper presents XLSR which learns cross-lingual speech representations by\npretraining a single model from the raw waveform of speech in multiple\nlanguages. We build on wav2vec 2.0 which is trained by solving a contrastive\ntask over masked latent speech representations and jointly learns a\nquantization of the latents shared across languages. The resulting model is\nfine-tuned on labeled data and experiments show that cross-lingual pretraining\nsignificantly outperforms monolingual pretraining. On the CommonVoice\nbenchmark, XLSR shows a relative phoneme error rate reduction of 72% compared\nto the best known results. On BABEL, our approach improves word error rate by\n16% relative compared to a comparable system. Our approach enables a single\nmultilingual speech recognition model which is competitive to strong individual\nmodels. Analysis shows that the latent discrete speech representations are\nshared across languages with increased sharing for related languages. We hope\nto catalyze research in low-resource speech understanding by releasing XLSR-53,\na large model pretrained in 53 languages.",
            "authors": [
                "Alexis Conneau",
                "Alexei Baevski",
                "Ronan Collobert",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ]
        },
        {
            "title": "Rethinking embedding coupling in pre-trained language models",
            "arxiv": "2010.12821",
            "year": 2020,
            "url": "http://arxiv.org/abs/2010.12821v1",
            "abstract": "We re-evaluate the standard practice of sharing weights between input and\noutput embeddings in state-of-the-art pre-trained language models. We show that\ndecoupled embeddings provide increased modeling flexibility, allowing us to\nsignificantly improve the efficiency of parameter allocation in the input\nembedding of multilingual models. By reallocating the input embedding\nparameters in the Transformer layers, we achieve dramatically better\nperformance on standard natural language understanding tasks with the same\nnumber of parameters during fine-tuning. We also show that allocating\nadditional capacity to the output embedding provides benefits to the model that\npersist through the fine-tuning stage even though the output embedding is\ndiscarded after pre-training. Our analysis shows that larger output embeddings\nprevent the model's last layers from overspecializing to the pre-training task\nand encourage Transformer representations to be more general and more\ntransferable to other tasks and languages. Harnessing these findings, we are\nable to train models that achieve strong performance on the XTREME benchmark\nwithout increasing the number of parameters at the fine-tuning stage.",
            "authors": [
                "Hyung Won Chung",
                "Thibault F\u00e9vry",
                "Henry Tsai",
                "Melvin Johnson",
                "Sebastian Ruder"
            ]
        },
        {
            "title": "mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models",
            "arxiv": "2110.08151",
            "year": 2021,
            "url": "http://arxiv.org/abs/2110.08151v1",
            "abstract": "Recent studies have shown that multilingual pretrained language models can be\neffectively improved with cross-lingual alignment information from Wikipedia\nentities. However, existing methods only exploit entity information in\npretraining and do not explicitly use entities in downstream tasks. In this\nstudy, we explore the effectiveness of leveraging entity representations for\ndownstream cross-lingual tasks. We train a multilingual language model with 24\nlanguages with entity representations and show the model consistently\noutperforms word-based pretrained models in various cross-lingual transfer\ntasks. We also analyze the model and the key insight is that incorporating\nentity representations into the input allows us to extract more\nlanguage-agnostic features. We also evaluate the model with a multilingual\ncloze prompt task with the mLAMA dataset. We show that entity-based prompt\nelicits correct factual knowledge more likely than using only word\nrepresentations.",
            "authors": [
                "Ryokan Ri",
                "Ikuya Yamada",
                "Yoshimasa Tsuruoka"
            ]
        },
        {
            "title": "A ConvNet for the 2020s",
            "arxiv": "2201.03545",
            "year": 2022,
            "url": "http://arxiv.org/abs/2201.03545v2",
            "abstract": "The \"Roaring 20s\" of visual recognition began with the introduction of Vision\nTransformers (ViTs), which quickly superseded ConvNets as the state-of-the-art\nimage classification model. A vanilla ViT, on the other hand, faces\ndifficulties when applied to general computer vision tasks such as object\ndetection and semantic segmentation. It is the hierarchical Transformers (e.g.,\nSwin Transformers) that reintroduced several ConvNet priors, making\nTransformers practically viable as a generic vision backbone and demonstrating\nremarkable performance on a wide variety of vision tasks. However, the\neffectiveness of such hybrid approaches is still largely credited to the\nintrinsic superiority of Transformers, rather than the inherent inductive\nbiases of convolutions. In this work, we reexamine the design spaces and test\nthe limits of what a pure ConvNet can achieve. We gradually \"modernize\" a\nstandard ResNet toward the design of a vision Transformer, and discover several\nkey components that contribute to the performance difference along the way. The\noutcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt.\nConstructed entirely from standard ConvNet modules, ConvNeXts compete favorably\nwith Transformers in terms of accuracy and scalability, achieving 87.8%\nImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection\nand ADE20K segmentation, while maintaining the simplicity and efficiency of\nstandard ConvNets.",
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie"
            ]
        },
        {
            "title": "FNet: Mixing Tokens with Fourier Transforms",
            "arxiv": "2105.03824",
            "year": 2021,
            "url": "http://arxiv.org/abs/2105.03824v3",
            "abstract": "We show that Transformer encoder architectures can be sped up, with limited\naccuracy costs, by replacing the self-attention sublayers with simple linear\ntransformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling\nsemantic relationships in several text classification tasks. Most surprisingly,\nwe find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the\naccuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on\nGPUs and 70% faster on TPUs at standard 512 input lengths. At longer input\nlengths, our FNet model is significantly faster: when compared to the\n\"efficient\" Transformers on the Long Range Arena benchmark, FNet matches the\naccuracy of the most accurate models, while outpacing the fastest models across\nall sequence lengths on GPUs (and across relatively shorter lengths on TPUs).\nFinally, FNet has a light memory footprint and is particularly efficient at\nsmaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.",
            "authors": [
                "James Lee-Thorp",
                "Joshua Ainslie",
                "Ilya Eckstein",
                "Santiago Ontanon"
            ]
        },
        {
            "title": "Reformer: The Efficient Transformer",
            "arxiv": "2001.04451",
            "year": 2020,
            "url": "http://arxiv.org/abs/2001.04451v2",
            "abstract": "Large Transformer models routinely achieve state-of-the-art results on a\nnumber of tasks but training these models can be prohibitively costly,\nespecially on long sequences. We introduce two techniques to improve the\nefficiency of Transformers. For one, we replace dot-product attention by one\nthat uses locality-sensitive hashing, changing its complexity from O($L^2$) to\nO($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use\nreversible residual layers instead of the standard residuals, which allows\nstoring activations only once in the training process instead of $N$ times,\nwhere $N$ is the number of layers. The resulting model, the Reformer, performs\non par with Transformer models while being much more memory-efficient and much\nfaster on long sequences.",
            "authors": [
                "Nikita Kitaev",
                "\u0141ukasz Kaiser",
                "Anselm Levskaya"
            ]
        },
        {
            "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
            "arxiv": "2003.10555",
            "year": 2020,
            "url": "http://arxiv.org/abs/2003.10555v1",
            "abstract": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the\ninput by replacing some tokens with [MASK] and then train a model to\nreconstruct the original tokens. While they produce good results when\ntransferred to downstream NLP tasks, they generally require large amounts of\ncompute to be effective. As an alternative, we propose a more sample-efficient\npre-training task called replaced token detection. Instead of masking the\ninput, our approach corrupts it by replacing some tokens with plausible\nalternatives sampled from a small generator network. Then, instead of training\na model that predicts the original identities of the corrupted tokens, we train\na discriminative model that predicts whether each token in the corrupted input\nwas replaced by a generator sample or not. Thorough experiments demonstrate\nthis new pre-training task is more efficient than MLM because the task is\ndefined over all input tokens rather than just the small subset that was masked\nout. As a result, the contextual representations learned by our approach\nsubstantially outperform the ones learned by BERT given the same model size,\ndata, and compute. The gains are particularly strong for small models; for\nexample, we train a model on one GPU for 4 days that outperforms GPT (trained\nusing 30x more compute) on the GLUE natural language understanding benchmark.\nOur approach also works well at scale, where it performs comparably to RoBERTa\nand XLNet while using less than 1/4 of their compute and outperforms them when\nusing the same amount of compute.",
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V. Le",
                "Christopher D. Manning"
            ]
        },
        {
            "title": "You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling",
            "arxiv": "2111.09714",
            "year": 2021,
            "url": "http://arxiv.org/abs/2111.09714v1",
            "abstract": "Transformer-based models are widely used in natural language processing\n(NLP). Central to the transformer model is the self-attention mechanism, which\ncaptures the interactions of token pairs in the input sequences and depends\nquadratically on the sequence length. Training such models on longer sequences\nis expensive. In this paper, we show that a Bernoulli sampling attention\nmechanism based on Locality Sensitive Hashing (LSH), decreases the quadratic\ncomplexity of such models to linear. We bypass the quadratic cost by\nconsidering self-attention as a sum of individual tokens associated with\nBernoulli random variables that can, in principle, be sampled at once by a\nsingle hash (although in practice, this number may be a small constant). This\nleads to an efficient sampling scheme to estimate self-attention which relies\non specific modifications of LSH (to enable deployment on GPU architectures).\nWe evaluate our algorithm on the GLUE benchmark with standard 512 sequence\nlength where we see favorable performance relative to a standard pretrained\nTransformer. On the Long Range Arena (LRA) benchmark, for evaluating\nperformance on long sequences, our method achieves results consistent with\nsoftmax self-attention but with sizable speed-ups and memory savings and often\noutperforms other efficient self-attention methods. Our code is available at\nhttps://github.com/mlpen/YOSO",
            "authors": [
                "Zhanpeng Zeng",
                "Yunyang Xiong",
                "Sathya N. Ravi",
                "Shailesh Acharya",
                "Glenn Fung",
                "Vikas Singh"
            ]
        },
        {
            "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
            "arxiv": "2104.09864",
            "year": 2021,
            "url": "http://arxiv.org/abs/2104.09864v2",
            "abstract": "Position encoding in transformer architecture provides supervision for\ndependency modeling between elements at different positions in the sequence. We\ninvestigate various methods to encode positional information in\ntransformer-based language models and propose a novel implementation named\nRotary Position Embedding(RoPE). The proposed RoPE encodes absolute positional\ninformation with rotation matrix and naturally incorporates explicit relative\nposition dependency in self-attention formulation. Notably, RoPE comes with\nvaluable properties such as flexibility of being expand to any sequence\nlengths, decaying inter-token dependency with increasing relative distances,\nand capability of equipping the linear self-attention with relative position\nencoding. As a result, the enhanced transformer with rotary position embedding,\nor RoFormer, achieves superior performance in tasks with long texts. We release\nthe theoretical analysis along with some preliminary experiment results on\nChinese data. The undergoing experiment for English benchmark will soon be\nupdated.",
            "authors": [
                "Jianlin Su",
                "Yu Lu",
                "Shengfeng Pan",
                "Bo Wen",
                "Yunfeng Liu"
            ]
        },
        {
            "title": "MetaFormer is Actually What You Need for Vision",
            "arxiv": "2111.11418",
            "year": 2021,
            "url": "http://arxiv.org/abs/2111.11418v2",
            "abstract": "Transformers have shown great potential in computer vision tasks. A common\nbelief is their attention-based token mixer module contributes most to their\ncompetence. However, recent works show the attention-based module in\ntransformers can be replaced by spatial MLPs and the resulted models still\nperform quite well. Based on this observation, we hypothesize that the general\narchitecture of the transformers, instead of the specific token mixer module,\nis more essential to the model's performance. To verify this, we deliberately\nreplace the attention module in transformers with an embarrassingly simple\nspatial pooling operator to conduct only the most basic token mixing.\nSurprisingly, we observe that the derived model, termed as PoolFormer, achieves\ncompetitive performance on multiple computer vision tasks. For example, on\nImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned\nvision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy\nwith 35%/52% fewer parameters and 48%/60% fewer MACs. The effectiveness of\nPoolFormer verifies our hypothesis and urges us to initiate the concept of\n\"MetaFormer\", a general architecture abstracted from transformers without\nspecifying the token mixer. Based on the extensive experiments, we argue that\nMetaFormer is the key player in achieving superior results for recent\ntransformer and MLP-like models on vision tasks. This work calls for more\nfuture research dedicated to improving MetaFormer instead of focusing on the\ntoken mixer modules. Additionally, our proposed PoolFormer could serve as a\nstarting baseline for future MetaFormer architecture design. Code is available\nat https://github.com/sail-sg/poolformer",
            "authors": [
                "Weihao Yu",
                "Mi Luo",
                "Pan Zhou",
                "Chenyang Si",
                "Yichen Zhou",
                "Xinchao Wang",
                "Jiashi Feng",
                "Shuicheng Yan"
            ]
        },
        {
            "title": "Masked Autoencoders Are Scalable Vision Learners",
            "arxiv": "2111.06377",
            "year": 2021,
            "url": "http://arxiv.org/abs/2111.06377v3",
            "abstract": "This paper shows that masked autoencoders (MAE) are scalable self-supervised\nlearners for computer vision. Our MAE approach is simple: we mask random\npatches of the input image and reconstruct the missing pixels. It is based on\ntwo core designs. First, we develop an asymmetric encoder-decoder architecture,\nwith an encoder that operates only on the visible subset of patches (without\nmask tokens), along with a lightweight decoder that reconstructs the original\nimage from the latent representation and mask tokens. Second, we find that\nmasking a high proportion of the input image, e.g., 75%, yields a nontrivial\nand meaningful self-supervisory task. Coupling these two designs enables us to\ntrain large models efficiently and effectively: we accelerate training (by 3x\nor more) and improve accuracy. Our scalable approach allows for learning\nhigh-capacity models that generalize well: e.g., a vanilla ViT-Huge model\nachieves the best accuracy (87.8%) among methods that use only ImageNet-1K\ndata. Transfer performance in downstream tasks outperforms supervised\npre-training and shows promising scaling behavior.",
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ]
        },
        {
            "title": "Big Bird: Transformers for Longer Sequences",
            "arxiv": "2007.14062",
            "year": 2020,
            "url": "http://arxiv.org/abs/2007.14062v2",
            "abstract": "Transformers-based models, such as BERT, have been one of the most successful\ndeep learning models for NLP. Unfortunately, one of their core limitations is\nthe quadratic dependency (mainly in terms of memory) on the sequence length due\nto their full attention mechanism. To remedy this, we propose, BigBird, a\nsparse attention mechanism that reduces this quadratic dependency to linear. We\nshow that BigBird is a universal approximator of sequence functions and is\nTuring complete, thereby preserving these properties of the quadratic, full\nattention model. Along the way, our theoretical analysis reveals some of the\nbenefits of having $O(1)$ global tokens (such as CLS), that attend to the\nentire sequence as part of the sparse attention mechanism. The proposed sparse\nattention can handle sequences of length up to 8x of what was previously\npossible using similar hardware. As a consequence of the capability to handle\nlonger context, BigBird drastically improves performance on various NLP tasks\nsuch as question answering and summarization. We also propose novel\napplications to genomics data.",
            "authors": [
                "Manzil Zaheer",
                "Guru Guruganesh",
                "Avinava Dubey",
                "Joshua Ainslie",
                "Chris Alberti",
                "Santiago Ontanon",
                "Philip Pham",
                "Anirudh Ravula",
                "Qifan Wang",
                "Li Yang",
                "Amr Ahmed"
            ]
        },
        {
            "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
            "arxiv": "1909.08053",
            "year": 2019,
            "url": "http://arxiv.org/abs/1909.08053v4",
            "abstract": "Recent work in language modeling demonstrates that training large transformer\nmodels advances the state of the art in Natural Language Processing\napplications. However, very large models can be quite difficult to train due to\nmemory constraints. In this work, we present our techniques for training very\nlarge transformer models and implement a simple, efficient intra-layer model\nparallel approach that enables training transformer models with billions of\nparameters. Our approach does not require a new compiler or library changes, is\northogonal and complimentary to pipeline model parallelism, and can be fully\nimplemented with the insertion of a few communication operations in native\nPyTorch. We illustrate this approach by converging transformer based models up\nto 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the\nentire application with 76% scaling efficiency when compared to a strong single\nGPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To\ndemonstrate that large language models can further advance the state of the art\n(SOTA), we train an 8.3 billion parameter transformer language model similar to\nGPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful\nattention to the placement of layer normalization in BERT-like models is\ncritical to achieving increased performance as the model size grows. Using the\nGPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA\nperplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%)\ndatasets. Our BERT model achieves SOTA results on the RACE dataset (90.9%\ncompared to SOTA accuracy of 89.4%).",
            "authors": [
                "Mohammad Shoeybi",
                "Mostofa Patwary",
                "Raul Puri",
                "Patrick LeGresley",
                "Jared Casper",
                "Bryan Catanzaro"
            ]
        },
        {
            "title": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention",
            "arxiv": "2010.01057",
            "year": 2020,
            "url": "http://arxiv.org/abs/2010.01057v1",
            "abstract": "Entity representations are useful in natural language tasks involving\nentities. In this paper, we propose new pretrained contextualized\nrepresentations of words and entities based on the bidirectional transformer.\nThe proposed model treats words and entities in a given text as independent\ntokens, and outputs contextualized representations of them. Our model is\ntrained using a new pretraining task based on the masked language model of\nBERT. The task involves predicting randomly masked words and entities in a\nlarge entity-annotated corpus retrieved from Wikipedia. We also propose an\nentity-aware self-attention mechanism that is an extension of the\nself-attention mechanism of the transformer, and considers the types of tokens\n(words or entities) when computing attention scores. The proposed model\nachieves impressive empirical performance on a wide range of entity-related\ntasks. In particular, it obtains state-of-the-art results on five well-known\ndatasets: Open Entity (entity typing), TACRED (relation classification),\nCoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering),\nand SQuAD 1.1 (extractive question answering). Our source code and pretrained\nrepresentations are available at https://github.com/studio-ousia/luke.",
            "authors": [
                "Ikuya Yamada",
                "Akari Asai",
                "Hiroyuki Shindo",
                "Hideaki Takeda",
                "Yuji Matsumoto"
            ]
        },
        {
            "title": "Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation",
            "arxiv": "2004.09602",
            "year": 2020,
            "url": "http://arxiv.org/abs/2004.09602v1",
            "abstract": "Quantization techniques can reduce the size of Deep Neural Networks and\nimprove inference latency and throughput by taking advantage of high throughput\ninteger instructions. In this paper we review the mathematical aspects of\nquantization parameters and evaluate their choices on a wide range of neural\nnetwork models for different application domains, including vision, speech, and\nlanguage. We focus on quantization techniques that are amenable to acceleration\nby processors with high-throughput integer math pipelines. We also present a\nworkflow for 8-bit quantization that is able to maintain accuracy within 1% of\nthe floating-point baseline on all networks studied, including models that are\nmore difficult to quantize, such as MobileNets and BERT-large.",
            "authors": [
                "Hao Wu",
                "Patrick Judd",
                "Xiaojie Zhang",
                "Mikhail Isaev",
                "Paulius Micikevicius"
            ]
        },
        {
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "arxiv": "2010.11934",
            "year": 2020,
            "url": "http://arxiv.org/abs/2010.11934v3",
            "abstract": "The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified\ntext-to-text format and scale to attain state-of-the-art results on a wide\nvariety of English-language NLP tasks. In this paper, we introduce mT5, a\nmultilingual variant of T5 that was pre-trained on a new Common Crawl-based\ndataset covering 101 languages. We detail the design and modified training of\nmT5 and demonstrate its state-of-the-art performance on many multilingual\nbenchmarks. We also describe a simple technique to prevent \"accidental\ntranslation\" in the zero-shot setting, where a generative model chooses to\n(partially) translate its prediction into the wrong language. All of the code\nand model checkpoints used in this work are publicly available.",
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel"
            ]
        },
        {
            "title": "Large-Scale Self- and Semi-Supervised Learning for Speech Translation",
            "arxiv": "2104.06678",
            "year": 2021,
            "url": "http://arxiv.org/abs/2104.06678v1",
            "abstract": "In this paper, we improve speech translation (ST) through effectively\nleveraging large quantities of unlabeled speech and text data in different and\ncomplementary ways. We explore both pretraining and self-training by using the\nlarge Libri-Light speech audio corpus and language modeling with CommonCrawl.\nOur experiments improve over the previous state of the art by 2.6 BLEU on\naverage on all four considered CoVoST 2 language pairs via a simple recipe of\ncombining wav2vec 2.0 pretraining, a single iteration of self-training and\ndecoding with a language model. Different to existing work, our approach does\nnot leverage any other supervision than ST data. Code and models will be\npublicly released.",
            "authors": [
                "Changhan Wang",
                "Anne Wu",
                "Juan Pino",
                "Alexei Baevski",
                "Michael Auli",
                "Alexis Conneau"
            ]
        },
        {
            "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
            "arxiv": "1906.08237",
            "year": 2019,
            "url": "http://arxiv.org/abs/1906.08237v2",
            "abstract": "With the capability of modeling bidirectional contexts, denoising\nautoencoding based pretraining like BERT achieves better performance than\npretraining approaches based on autoregressive language modeling. However,\nrelying on corrupting the input with masks, BERT neglects dependency between\nthe masked positions and suffers from a pretrain-finetune discrepancy. In light\nof these pros and cons, we propose XLNet, a generalized autoregressive\npretraining method that (1) enables learning bidirectional contexts by\nmaximizing the expected likelihood over all permutations of the factorization\norder and (2) overcomes the limitations of BERT thanks to its autoregressive\nformulation. Furthermore, XLNet integrates ideas from Transformer-XL, the\nstate-of-the-art autoregressive model, into pretraining. Empirically, under\ncomparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a\nlarge margin, including question answering, natural language inference,\nsentiment analysis, and document ranking.",
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Ruslan Salakhutdinov",
                "Quoc V. Le"
            ]
        },
        {
            "title": "BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese",
            "arxiv": "2109.09701",
            "year": 2021,
            "url": "http://arxiv.org/abs/2109.09701v2",
            "abstract": "In this paper, we present BARTpho with two versions BARTpho-syllable and\nBARTpho-word, which are the first public large-scale monolingual\nsequence-to-sequence models pre-trained for Vietnamese. BARTpho uses the\n\"large\" architecture and the pre-training scheme of the sequence-to-sequence\ndenoising autoencoder BART, thus especially suitable for generative NLP tasks.\nWe conduct experiments to compare our BARTpho with its competitor mBART on a\ndownstream task of Vietnamese text summarization and show that: in both\nautomatic and human evaluations, BARTpho outperforms the strong baseline mBART\nand improves the state-of-the-art. We release BARTpho to facilitate future\nresearch and applications of generative Vietnamese NLP tasks. Our BARTpho\nmodels are publicly available at: https://github.com/VinAIResearch/BARTpho",
            "authors": [
                "Nguyen Luong Tran",
                "Duong Minh Le",
                "Dat Quoc Nguyen"
            ]
        },
        {
            "title": "Learning Transferable Visual Models From Natural Language Supervision",
            "arxiv": "2103.00020",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.00020v1",
            "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set\nof predetermined object categories. This restricted form of supervision limits\ntheir generality and usability since additional labeled data is needed to\nspecify any other visual concept. Learning directly from raw text about images\nis a promising alternative which leverages a much broader source of\nsupervision. We demonstrate that the simple pre-training task of predicting\nwhich caption goes with which image is an efficient and scalable way to learn\nSOTA image representations from scratch on a dataset of 400 million (image,\ntext) pairs collected from the internet. After pre-training, natural language\nis used to reference learned visual concepts (or describe new ones) enabling\nzero-shot transfer of the model to downstream tasks. We study the performance\nof this approach by benchmarking on over 30 different existing computer vision\ndatasets, spanning tasks such as OCR, action recognition in videos,\ngeo-localization, and many types of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is often competitive with a\nfully supervised baseline without the need for any dataset specific training.\nFor instance, we match the accuracy of the original ResNet-50 on ImageNet\nzero-shot without needing to use any of the 1.28 million training examples it\nwas trained on. We release our code and pre-trained model weights at\nhttps://github.com/OpenAI/CLIP.",
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ]
        },
        {
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
            "arxiv": "1909.11942",
            "year": 2019,
            "url": "http://arxiv.org/abs/1909.11942v6",
            "abstract": "Increasing model size when pretraining natural language representations often\nresults in improved performance on downstream tasks. However, at some point\nfurther model increases become harder due to GPU/TPU memory limitations and\nlonger training times. To address these problems, we present two\nparameter-reduction techniques to lower memory consumption and increase the\ntraining speed of BERT. Comprehensive empirical evidence shows that our\nproposed methods lead to models that scale much better compared to the original\nBERT. We also use a self-supervised loss that focuses on modeling\ninter-sentence coherence, and show it consistently helps downstream tasks with\nmulti-sentence inputs. As a result, our best model establishes new\nstate-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having\nfewer parameters compared to BERT-large. The code and the pretrained models are\navailable at https://github.com/google-research/ALBERT.",
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut"
            ]
        },
        {
            "title": "UniSpeech-SAT: Universal Speech Representation Learning with Speaker Aware Pre-Training",
            "arxiv": "2110.05752",
            "year": 2021,
            "url": "http://arxiv.org/abs/2110.05752v1",
            "abstract": "Self-supervised learning (SSL) is a long-standing goal for speech processing,\nsince it utilizes large-scale unlabeled data and avoids extensive human\nlabeling. Recent years witness great successes in applying self-supervised\nlearning in speech recognition, while limited exploration was attempted in\napplying SSL for modeling speaker characteristics. In this paper, we aim to\nimprove the existing SSL framework for speaker representation learning. Two\nmethods are introduced for enhancing the unsupervised speaker information\nextraction. First, we apply the multi-task learning to the current SSL\nframework, where we integrate the utterance-wise contrastive loss with the SSL\nobjective function. Second, for better speaker discrimination, we propose an\nutterance mixing strategy for data augmentation, where additional overlapped\nutterances are created unsupervisely and incorporate during training. We\nintegrate the proposed methods into the HuBERT framework. Experiment results on\nSUPERB benchmark show that the proposed system achieves state-of-the-art\nperformance in universal representation learning, especially for speaker\nidentification oriented tasks. An ablation study is performed verifying the\nefficacy of each proposed method. Finally, we scale up training dataset to 94\nthousand hours public audio data and achieve further performance improvement in\nall SUPERB tasks.",
            "authors": [
                "Sanyuan Chen",
                "Yu Wu",
                "Chengyi Wang",
                "Zhengyang Chen",
                "Zhuo Chen",
                "Shujie Liu",
                "Jian Wu",
                "Yao Qian",
                "Furu Wei",
                "Jinyu Li",
                "Xiangzhan Yu"
            ]
        },
        {
            "title": "UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data",
            "arxiv": "2101.07597",
            "year": 2021,
            "url": "http://arxiv.org/abs/2101.07597v2",
            "abstract": "In this paper, we propose a unified pre-training approach called UniSpeech to\nlearn speech representations with both unlabeled and labeled data, in which\nsupervised phonetic CTC learning and phonetically-aware contrastive\nself-supervised learning are conducted in a multi-task learning manner. The\nresultant representations can capture information more correlated with phonetic\nstructures and improve the generalization across languages and domains. We\nevaluate the effectiveness of UniSpeech for cross-lingual representation\nlearning on public CommonVoice corpus. The results show that UniSpeech\noutperforms self-supervised pretraining and supervised transfer learning for\nspeech recognition by a maximum of 13.4% and 17.8% relative phone error rate\nreductions respectively (averaged over all testing languages). The\ntransferability of UniSpeech is also demonstrated on a domain-shift speech\nrecognition task, i.e., a relative word error rate reduction of 6% against the\nprevious approach.",
            "authors": [
                "Chengyi Wang",
                "Yu Wu",
                "Yao Qian",
                "Kenichi Kumatani",
                "Shujie Liu",
                "Furu Wei",
                "Michael Zeng",
                "Xuedong Huang"
            ]
        },
        {
            "title": "Few-shot Learning with Multilingual Language Models",
            "arxiv": "2112.10668",
            "year": 2021,
            "url": "http://arxiv.org/abs/2112.10668v1",
            "abstract": "Large-scale autoregressive language models such as GPT-3 are few-shot\nlearners that can perform a wide range of language tasks without fine-tuning.\nWhile these models are known to be able to jointly represent many different\nlanguages, their training data is dominated by English, potentially limiting\ntheir cross-lingual generalization. In this work, we train multilingual\nautoregressive language models on a balanced corpus covering a diverse set of\nlanguages, and study their few- and zero-shot learning capabilities in a wide\nrange of tasks. Our largest model with 7.5 billion parameters sets new state of\nthe art in few-shot learning in more than 20 representative languages,\noutperforming GPT-3 of comparable size in multilingual commonsense reasoning\n(with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in\n4-shot settings) and natural language inference (+5.4% in each of 0-shot and\n4-shot settings). On the FLORES-101 machine translation benchmark, our model\noutperforms GPT-3 on 171 out of 182 translation directions with 32 training\nexamples, while surpassing the official supervised baseline in 45 directions.\nWe present a detailed analysis of where the model succeeds and fails, showing\nin particular that it enables cross-lingual in-context learning on some tasks,\nwhile there is still room for improvement on surface form robustness and\nadaptation to tasks that do not have a natural cloze form. Finally, we evaluate\nour models in social value tasks such as hate speech detection in five\nlanguages and find it has limitations similar to comparable sized GPT-3 models.",
            "authors": [
                "Xi Victoria Lin",
                "Todor Mihaylov",
                "Mikel Artetxe",
                "Tianlu Wang",
                "Shuohui Chen",
                "Daniel Simig",
                "Myle Ott",
                "Naman Goyal",
                "Shruti Bhosale",
                "Jingfei Du",
                "Ramakanth Pasunuru",
                "Sam Shleifer",
                "Punit Singh Koura",
                "Vishrav Chaudhary",
                "Brian O'Horo",
                "Jeff Wang",
                "Luke Zettlemoyer",
                "Zornitsa Kozareva",
                "Mona Diab",
                "Veselin Stoyanov",
                "Xian Li"
            ]
        },
        {
            "title": "Unified Pre-training for Program Understanding and Generation",
            "arxiv": "2103.06333",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.06333v2",
            "abstract": "Code summarization and generation empower conversion between programming\nlanguage (PL) and natural language (NL), while code translation avails the\nmigration of legacy code from one PL to another. This paper introduces PLBART,\na sequence-to-sequence model capable of performing a broad spectrum of program\nand language understanding and generation tasks. PLBART is pre-trained on an\nextensive collection of Java and Python functions and associated NL text via\ndenoising autoencoding. Experiments on code summarization in the English\nlanguage, code generation, and code translation in seven programming languages\nshow that PLBART outperforms or rivals state-of-the-art models. Moreover,\nexperiments on discriminative tasks, e.g., program repair, clone detection, and\nvulnerable code detection, demonstrate PLBART's effectiveness in program\nunderstanding. Furthermore, analysis reveals that PLBART learns program syntax,\nstyle (e.g., identifier naming convention), logical flow (e.g., if block inside\nan else block is equivalent to else if block) that are crucial to program\nsemantics and thus excels even with limited annotations.",
            "authors": [
                "Wasi Uddin Ahmad",
                "Saikat Chakraborty",
                "Baishakhi Ray",
                "Kai-Wei Chang"
            ]
        },
        {
            "title": "Multilingual Denoising Pre-training for Neural Machine Translation",
            "arxiv": "2001.08210",
            "year": 2020,
            "url": "http://arxiv.org/abs/2001.08210v2",
            "abstract": "This paper demonstrates that multilingual denoising pre-training produces\nsignificant performance gains across a wide variety of machine translation (MT)\ntasks. We present mBART -- a sequence-to-sequence denoising auto-encoder\npre-trained on large-scale monolingual corpora in many languages using the BART\nobjective. mBART is one of the first methods for pre-training a complete\nsequence-to-sequence model by denoising full texts in multiple languages, while\nprevious approaches have focused only on the encoder, decoder, or\nreconstructing parts of the text. Pre-training a complete model allows it to be\ndirectly fine tuned for supervised (both sentence-level and document-level) and\nunsupervised machine translation, with no task-specific modifications. We\ndemonstrate that adding mBART initialization produces performance gains in all\nbut the highest-resource settings, including up to 12 BLEU points for low\nresource MT and over 5 BLEU points for many document-level and unsupervised\nmodels. We also show it also enables new types of transfer to language pairs\nwith no bi-text or that were not in the pre-training corpus, and present\nextensive analysis of which factors contribute the most to effective\npre-training.",
            "authors": [
                "Yinhan Liu",
                "Jiatao Gu",
                "Naman Goyal",
                "Xian Li",
                "Sergey Edunov",
                "Marjan Ghazvininejad",
                "Mike Lewis",
                "Luke Zettlemoyer"
            ]
        },
        {
            "title": "Longformer: The Long-Document Transformer",
            "arxiv": "2004.05150",
            "year": 2020,
            "url": "http://arxiv.org/abs/2004.05150v2",
            "abstract": "Transformer-based models are unable to process long sequences due to their\nself-attention operation, which scales quadratically with the sequence length.\nTo address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process\ndocuments of thousands of tokens or longer. Longformer's attention mechanism is\na drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work\non long-sequence transformers, we evaluate Longformer on character-level\nlanguage modeling and achieve state-of-the-art results on text8 and enwik8. In\ncontrast to most prior work, we also pretrain Longformer and finetune it on a\nvariety of downstream tasks. Our pretrained Longformer consistently outperforms\nRoBERTa on long document tasks and sets new state-of-the-art results on WikiHop\nand TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a\nLongformer variant for supporting long document generative sequence-to-sequence\ntasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan"
            ]
        },
        {
            "title": "Leveraging Pre-trained Checkpoints for Sequence Generation Tasks",
            "arxiv": "1907.12461",
            "year": 2019,
            "url": "http://arxiv.org/abs/1907.12461v2",
            "abstract": "Unsupervised pre-training of large neural models has recently revolutionized\nNatural Language Processing. By warm-starting from the publicly released\ncheckpoints, NLP practitioners have pushed the state-of-the-art on multiple\nbenchmarks while saving significant amounts of compute time. So far the focus\nhas been mainly on the Natural Language Understanding tasks. In this paper, we\ndemonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We\ndeveloped a Transformer-based sequence-to-sequence model that is compatible\nwith publicly available pre-trained BERT, GPT-2 and RoBERTa checkpoints and\nconducted an extensive empirical study on the utility of initializing our\nmodel, both encoder and decoder, with these checkpoints. Our models result in\nnew state-of-the-art results on Machine Translation, Text Summarization,\nSentence Splitting, and Sentence Fusion.",
            "authors": [
                "Sascha Rothe",
                "Shashi Narayan",
                "Aliaksei Severyn"
            ]
        },
        {
            "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
            "arxiv": "2106.07447",
            "year": 2021,
            "url": "http://arxiv.org/abs/2106.07447v1",
            "abstract": "Self-supervised approaches for speech representation learning are challenged\nby three unique problems: (1) there are multiple sound units in each input\nutterance, (2) there is no lexicon of input sound units during the pre-training\nphase, and (3) sound units have variable lengths with no explicit segmentation.\nTo deal with these three problems, we propose the Hidden-Unit BERT (HuBERT)\napproach for self-supervised speech representation learning, which utilizes an\noffline clustering step to provide aligned target labels for a BERT-like\nprediction loss. A key ingredient of our approach is applying the prediction\nloss over the masked regions only, which forces the model to learn a combined\nacoustic and language model over the continuous inputs. HuBERT relies primarily\non the consistency of the unsupervised clustering step rather than the\nintrinsic quality of the assigned cluster labels. Starting with a simple\nk-means teacher of 100 clusters, and using two iterations of clustering, the\nHuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0\nperformance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with\n10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model,\nHuBERT shows up to 19% and 13% relative WER reduction on the more challenging\ndev-other and test-other evaluation subsets.",
            "authors": [
                "Wei-Ning Hsu",
                "Benjamin Bolte",
                "Yao-Hung Hubert Tsai",
                "Kushal Lakhotia",
                "Ruslan Salakhutdinov",
                "Abdelrahman Mohamed"
            ]
        },
        {
            "title": "CPM: A Large-scale Generative Chinese Pre-trained Language Model",
            "arxiv": "2012.00413",
            "year": 2020,
            "url": "http://arxiv.org/abs/2012.00413v1",
            "abstract": "Pre-trained Language Models (PLMs) have proven to be beneficial for various\ndownstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB\ntraining data, drew a lot of attention due to the capacity of few-shot (even\nzero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is\nstill challenging, as the training corpus of GPT-3 is primarily English, and\nthe parameters are not publicly available. In this technical report, we release\nthe Chinese Pre-trained Language Model (CPM) with generative pre-training on\nlarge-scale Chinese training data. To the best of our knowledge, CPM, with 2.6\nbillion parameters and 100GB Chinese training data, is the largest Chinese\npre-trained language model, which could facilitate several downstream Chinese\nNLP tasks, such as conversation, essay generation, cloze test, and language\nunderstanding. Extensive experiments demonstrate that CPM achieves strong\nperformance on many NLP tasks in the settings of few-shot (even zero-shot)\nlearning. The code and parameters are available at\nhttps://github.com/TsinghuaAI/CPM-Generate.",
            "authors": [
                "Zhengyan Zhang",
                "Xu Han",
                "Hao Zhou",
                "Pei Ke",
                "Yuxian Gu",
                "Deming Ye",
                "Yujia Qin",
                "Yusheng Su",
                "Haozhe Ji",
                "Jian Guan",
                "Fanchao Qi",
                "Xiaozhi Wang",
                "Yanan Zheng",
                "Guoyang Zeng",
                "Huanqi Cao",
                "Shengqi Chen",
                "Daixuan Li",
                "Zhenbo Sun",
                "Zhiyuan Liu",
                "Minlie Huang",
                "Wentao Han",
                "Jie Tang",
                "Juanzi Li",
                "Xiaoyan Zhu",
                "Maosong Sun"
            ]
        },
        {
            "title": "Per-Pixel Classification is Not All You Need for Semantic Segmentation",
            "arxiv": "2107.06278",
            "year": 2021,
            "url": "http://arxiv.org/abs/2107.06278v2",
            "abstract": "Modern approaches typically formulate semantic segmentation as a per-pixel\nclassification task, while instance-level segmentation is handled with an\nalternative mask classification. Our key insight: mask classification is\nsufficiently general to solve both semantic- and instance-level segmentation\ntasks in a unified manner using the exact same model, loss, and training\nprocedure. Following this observation, we propose MaskFormer, a simple mask\nclassification model which predicts a set of binary masks, each associated with\na single global class label prediction. Overall, the proposed mask\nclassification-based method simplifies the landscape of effective approaches to\nsemantic and panoptic segmentation tasks and shows excellent empirical results.\nIn particular, we observe that MaskFormer outperforms per-pixel classification\nbaselines when the number of classes is large. Our mask classification-based\nmethod outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K)\nand panoptic segmentation (52.7 PQ on COCO) models.",
            "authors": [
                "Bowen Cheng",
                "Alexander G. Schwing",
                "Alexander Kirillov"
            ]
        },
        {
            "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
            "arxiv": "2111.09296",
            "year": 2021,
            "url": "http://arxiv.org/abs/2111.09296v3",
            "abstract": "This paper presents XLS-R, a large-scale model for cross-lingual speech\nrepresentation learning based on wav2vec 2.0. We train models with up to 2B\nparameters on nearly half a million hours of publicly available speech audio in\n128 languages, an order of magnitude more public data than the largest known\nprior work. Our evaluation covers a wide range of tasks, domains, data regimes\nand languages, both high and low-resource. On the CoVoST-2 speech translation\nbenchmark, we improve the previous state of the art by an average of 7.4 BLEU\nover 21 translation directions into English. For speech recognition, XLS-R\nimproves over the best known prior work on BABEL, MLS, CommonVoice as well as\nVoxPopuli, lowering error rates by 14-34% relative on average. XLS-R also sets\na new state of the art on VoxLingua107 language identification. Moreover, we\nshow that with sufficient model size, cross-lingual pretraining can outperform\nEnglish-only pretraining when translating English speech into other languages,\na setting which favors monolingual pretraining. We hope XLS-R can help to\nimprove speech processing tasks for many more languages of the world.",
            "authors": [
                "Arun Babu",
                "Changhan Wang",
                "Andros Tjandra",
                "Kushal Lakhotia",
                "Qiantong Xu",
                "Naman Goyal",
                "Kritika Singh",
                "Patrick von Platen",
                "Yatharth Saraf",
                "Juan Pino",
                "Alexei Baevski",
                "Alexis Conneau",
                "Michael Auli"
            ]
        },
        {
            "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
            "arxiv": "1908.07490",
            "year": 2019,
            "url": "http://arxiv.org/abs/1908.07490v3",
            "abstract": "Vision-and-language reasoning requires an understanding of visual concepts,\nlanguage semantics, and, most importantly, the alignment and relationships\nbetween these two modalities. We thus propose the LXMERT (Learning\nCross-Modality Encoder Representations from Transformers) framework to learn\nthese vision-and-language connections. In LXMERT, we build a large-scale\nTransformer model that consists of three encoders: an object relationship\nencoder, a language encoder, and a cross-modality encoder. Next, to endow our\nmodel with the capability of connecting vision and language semantics, we\npre-train the model with large amounts of image-and-sentence pairs, via five\ndiverse representative pre-training tasks: masked language modeling, masked\nobject prediction (feature regression and label classification), cross-modality\nmatching, and image question answering. These tasks help in learning both\nintra-modality and cross-modality relationships. After fine-tuning from our\npre-trained parameters, our model achieves the state-of-the-art results on two\nvisual question answering datasets (i.e., VQA and GQA). We also show the\ngeneralizability of our pre-trained cross-modality model by adapting it to a\nchallenging visual-reasoning task, NLVR2, and improve the previous best result\nby 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies\nto prove that both our novel model components and pre-training strategies\nsignificantly contribute to our strong results; and also present several\nattention visualizations for the different encoders. Code and pre-trained\nmodels publicly available at: https://github.com/airsplay/lxmert",
            "authors": [
                "Hao Tan",
                "Mohit Bansal"
            ]
        },
        {
            "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention",
            "arxiv": "2102.03902",
            "year": 2021,
            "url": "http://arxiv.org/abs/2102.03902v3",
            "abstract": "Transformers have emerged as a powerful tool for a broad range of natural\nlanguage processing tasks. A key component that drives the impressive\nperformance of Transformers is the self-attention mechanism that encodes the\ninfluence or dependence of other tokens on each specific token. While\nbeneficial, the quadratic complexity of self-attention on the input sequence\nlength has limited its application to longer sequences -- a topic being\nactively studied in the community. To address this limitation, we propose\nNystr\\\"{o}mformer -- a model that exhibits favorable scalability as a function\nof sequence length. Our idea is based on adapting the Nystr\\\"{o}m method to\napproximate standard self-attention with $O(n)$ complexity. The scalability of\nNystr\\\"{o}mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE\nbenchmark and IMDB reviews with standard sequence length, and find that our\nNystr\\\"{o}mformer performs comparably, or in a few cases, even slightly better,\nthan standard self-attention. On longer sequence tasks in the Long Range Arena\n(LRA) benchmark, Nystr\\\"{o}mformer performs favorably relative to other\nefficient self-attention methods. Our code is available at\nhttps://github.com/mlpen/Nystromformer.",
            "authors": [
                "Yunyang Xiong",
                "Zhanpeng Zeng",
                "Rudrasis Chakraborty",
                "Mingxing Tan",
                "Glenn Fung",
                "Yin Li",
                "Vikas Singh"
            ]
        },
        {
            "title": "Beyond English-Centric Multilingual Machine Translation",
            "arxiv": "2010.11125",
            "year": 2020,
            "url": "http://arxiv.org/abs/2010.11125v1",
            "abstract": "Existing work in translation demonstrated the potential of massively\nmultilingual machine translation by training a single model able to translate\nbetween any pair of languages. However, much of this work is English-Centric by\ntraining only on data which was translated from or to English. While this is\nsupported by large sources of training data, it does not reflect translation\nneeds worldwide. In this work, we create a true Many-to-Many multilingual\ntranslation model that can translate directly between any pair of 100\nlanguages. We build and open source a training dataset that covers thousands of\nlanguage directions with supervised data, created through large-scale mining.\nThen, we explore how to effectively increase model capacity through a\ncombination of dense scaling and language-specific sparse parameters to create\nhigh quality models. Our focus on non-English-Centric models brings gains of\nmore than 10 BLEU when directly translating between non-English directions\nwhile performing competitively to the best single systems of WMT. We\nopen-source our scripts so that others may reproduce the data, evaluation, and\nfinal M2M-100 model.",
            "authors": [
                "Angela Fan",
                "Shruti Bhosale",
                "Holger Schwenk",
                "Zhiyi Ma",
                "Ahmed El-Kishky",
                "Siddharth Goyal",
                "Mandeep Baines",
                "Onur Celebi",
                "Guillaume Wenzek",
                "Vishrav Chaudhary",
                "Naman Goyal",
                "Tom Birch",
                "Vitaliy Liptchinsky",
                "Sergey Edunov",
                "Edouard Grave",
                "Michael Auli",
                "Armand Joulin"
            ]
        },
        {
            "title": "Multilingual Translation with Extensible Multilingual Pretraining and Finetuning",
            "arxiv": "2008.00401",
            "year": 2020,
            "url": "http://arxiv.org/abs/2008.00401v1",
            "abstract": "Recent work demonstrates the potential of multilingual pretraining of\ncreating one model that can be used for various tasks in different languages.\nPrevious work in multilingual pretraining has demonstrated that machine\ntranslation systems can be created by finetuning on bitext. In this work, we\nshow that multilingual translation models can be created through multilingual\nfinetuning. Instead of finetuning on one direction, a pretrained model is\nfinetuned on many directions at the same time. Compared to multilingual models\ntrained from scratch, starting from pretrained models incorporates the benefits\nof large quantities of unlabeled monolingual data, which is particularly\nimportant for low resource languages where bitext is not available. We\ndemonstrate that pretrained models can be extended to incorporate additional\nlanguages without loss of performance. We double the number of languages in\nmBART to support multilingual machine translation models of 50 languages.\nFinally, we create the ML50 benchmark, covering low, mid, and high resource\nlanguages, to facilitate reproducible research by standardizing training and\nevaluation data. On ML50, we demonstrate that multilingual finetuning improves\non average 1 BLEU over the strongest baselines (being either multilingual from\nscratch or bilingual finetuning) while improving 9.3 BLEU on average over\nbilingual baselines from scratch.",
            "authors": [
                "Yuqing Tang",
                "Chau Tran",
                "Xian Li",
                "Peng-Jen Chen",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Jiatao Gu",
                "Angela Fan"
            ]
        },
        {
            "title": "Visual Attention Network",
            "arxiv": "2202.09741",
            "year": 2022,
            "url": "http://arxiv.org/abs/2202.09741v3",
            "abstract": "While originally designed for natural language processing tasks, the\nself-attention mechanism has recently taken various computer vision areas by\nstorm. However, the 2D nature of images brings three challenges for applying\nself-attention in computer vision. (1) Treating images as 1D sequences neglects\ntheir 2D structures. (2) The quadratic complexity is too expensive for\nhigh-resolution images. (3) It only captures spatial adaptability but ignores\nchannel adaptability. In this paper, we propose a novel large kernel attention\n(LKA) module to enable self-adaptive and long-range correlations in\nself-attention while avoiding the above issues. We further introduce a novel\nneural network based on LKA, namely Visual Attention Network (VAN). While\nextremely simple, VAN outperforms the state-of-the-art vision transformers and\nconvolutional neural networks with a large margin in extensive experiments,\nincluding image classification, object detection, semantic segmentation,\ninstance segmentation, etc. Code is available at\nhttps://github.com/Visual-Attention-Network.",
            "authors": [
                "Meng-Hao Guo",
                "Cheng-Ze Lu",
                "Zheng-Ning Liu",
                "Ming-Ming Cheng",
                "Shi-Min Hu"
            ]
        },
        {
            "title": "LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding",
            "arxiv": "2104.08836",
            "year": 2021,
            "url": "http://arxiv.org/abs/2104.08836v3",
            "abstract": "Multimodal pre-training with text, layout, and image has achieved SOTA\nperformance for visually-rich document understanding tasks recently, which\ndemonstrates the great potential for joint learning across different\nmodalities. In this paper, we present LayoutXLM, a multimodal pre-trained model\nfor multilingual document understanding, which aims to bridge the language\nbarriers for visually-rich document understanding. To accurately evaluate\nLayoutXLM, we also introduce a multilingual form understanding benchmark\ndataset named XFUND, which includes form understanding samples in 7 languages\n(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and\nkey-value pairs are manually labeled for each language. Experiment results show\nthat the LayoutXLM model has significantly outperformed the existing SOTA\ncross-lingual pre-trained models on the XFUND dataset. The pre-trained\nLayoutXLM model and the XFUND dataset are publicly available at\nhttps://aka.ms/layoutxlm.",
            "authors": [
                "Yiheng Xu",
                "Tengchao Lv",
                "Lei Cui",
                "Guoxin Wang",
                "Yijuan Lu",
                "Dinei Florencio",
                "Cha Zhang",
                "Furu Wei"
            ]
        },
        {
            "title": "Dense Passage Retrieval for Open-Domain Question Answering",
            "arxiv": "2004.04906",
            "year": 2020,
            "url": "http://arxiv.org/abs/2004.04906v3",
            "abstract": "Open-domain question answering relies on efficient passage retrieval to\nselect candidate contexts, where traditional sparse vector space models, such\nas TF-IDF or BM25, are the de facto method. In this work, we show that\nretrieval can be practically implemented using dense representations alone,\nwhere embeddings are learned from a small number of questions and passages by a\nsimple dual-encoder framework. When evaluated on a wide range of open-domain QA\ndatasets, our dense retriever outperforms a strong Lucene-BM25 system largely\nby 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our\nend-to-end QA system establish new state-of-the-art on multiple open-domain QA\nbenchmarks.",
            "authors": [
                "Vladimir Karpukhin",
                "Barlas O\u011fuz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih"
            ]
        },
        {
            "title": "Cross-lingual Language Model Pretraining",
            "arxiv": "1901.07291",
            "year": 2019,
            "url": "http://arxiv.org/abs/1901.07291v1",
            "abstract": "Recent studies have demonstrated the efficiency of generative pretraining for\nEnglish natural language understanding. In this work, we extend this approach\nto multiple languages and show the effectiveness of cross-lingual pretraining.\nWe propose two methods to learn cross-lingual language models (XLMs): one\nunsupervised that only relies on monolingual data, and one supervised that\nleverages parallel data with a new cross-lingual language model objective. We\nobtain state-of-the-art results on cross-lingual classification, unsupervised\nand supervised machine translation. On XNLI, our approach pushes the state of\nthe art by an absolute gain of 4.9% accuracy. On unsupervised machine\ntranslation, we obtain 34.3 BLEU on WMT'16 German-English, improving the\nprevious state of the art by more than 9 BLEU. On supervised machine\ntranslation, we obtain a new state of the art of 38.5 BLEU on WMT'16\nRomanian-English, outperforming the previous best approach by more than 4 BLEU.\nOur code and pretrained models will be made publicly available.",
            "authors": [
                "Guillaume Lample",
                "Alexis Conneau"
            ]
        },
        {
            "title": "DiT: Self-supervised Pre-training for Document Image Transformer",
            "arxiv": "2203.02378",
            "year": 2022,
            "url": "http://arxiv.org/abs/2203.02378v1",
            "abstract": "Image Transformer has recently achieved significant progress for natural\nimage understanding, either using supervised (ViT, DeiT, etc.) or\nself-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we\npropose DiT, a self-supervised pre-trained Document Image Transformer model\nusing large-scale unlabeled text images for Document AI tasks, which is\nessential since no supervised counterparts ever exist due to the lack of human\nlabeled document images. We leverage DiT as the backbone network in a variety\nof vision-based Document AI tasks, including document image classification,\ndocument layout analysis, as well as table detection. Experiment results have\nillustrated that the self-supervised pre-trained DiT model achieves new\nstate-of-the-art results on these downstream tasks, e.g. document image\nclassification (91.11 $\\rightarrow$ 92.69), document layout analysis (91.0\n$\\rightarrow$ 94.9) and table detection (94.23 $\\rightarrow$ 96.55). The code\nand pre-trained models are publicly available at \\url{https://aka.ms/msdit}.",
            "authors": [
                "Junlong Li",
                "Yiheng Xu",
                "Tengchao Lv",
                "Lei Cui",
                "Cha Zhang",
                "Furu Wei"
            ]
        },
        {
            "title": "Training data-efficient image transformers & distillation through attention",
            "arxiv": "2012.12877",
            "year": 2020,
            "url": "http://arxiv.org/abs/2012.12877v2",
            "abstract": "Recently, neural networks purely based on attention were shown to address\nimage understanding tasks such as image classification. However, these visual\ntransformers are pre-trained with hundreds of millions of images using an\nexpensive infrastructure, thereby limiting their adoption.\n  In this work, we produce a competitive convolution-free transformer by\ntraining on Imagenet only. We train them on a single computer in less than 3\ndays. Our reference vision transformer (86M parameters) achieves top-1 accuracy\nof 83.1% (single-crop evaluation) on ImageNet with no external data.\n  More importantly, we introduce a teacher-student strategy specific to\ntransformers. It relies on a distillation token ensuring that the student\nlearns from the teacher through attention. We show the interest of this\ntoken-based distillation, especially when using a convnet as a teacher. This\nleads us to report results competitive with convnets for both Imagenet (where\nwe obtain up to 85.2% accuracy) and when transferring to other tasks. We share\nour code and models.",
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herv\u00e9 J\u00e9gou"
            ]
        },
        {
            "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
            "arxiv": "2006.03654",
            "year": 2020,
            "url": "http://arxiv.org/abs/2006.03654v6",
            "abstract": "Recent progress in pre-trained neural language models has significantly\nimproved the performance of many natural language processing (NLP) tasks. In\nthis paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT\nwith disentangled attention) that improves the BERT and RoBERTa models using\ntwo novel techniques. The first is the disentangled attention mechanism, where\neach word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed\nusing disentangled matrices on their contents and relative positions,\nrespectively. Second, an enhanced mask decoder is used to incorporate absolute\npositions in the decoding layer to predict the masked tokens in model\npre-training. In addition, a new virtual adversarial training method is used\nfor fine-tuning to improve models' generalization. We show that these\ntechniques significantly improve the efficiency of model pre-training and the\nperformance of both natural language understanding (NLU) and natural langauge\ngeneration (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model\ntrained on half of the training data performs consistently better on a wide\nrange of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),\non SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).\nNotably, we scale up DeBERTa by training a larger version that consists of 48\nTransform layers with 1.5 billion parameters. The significant performance boost\nmakes the single DeBERTa model surpass the human performance on the SuperGLUE\nbenchmark (Wang et al., 2019a) for the first time in terms of macro-average\nscore (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the\nSuperGLUE leaderboard as of January 6, 2021, out performing the human baseline\nby a decent margin (90.3 versus 89.8).",
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen"
            ]
        },
        {
            "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing",
            "arxiv": "2006.03236",
            "year": 2020,
            "url": "http://arxiv.org/abs/2006.03236v1",
            "abstract": "With the success of language pretraining, it is highly desirable to develop\nmore efficient architectures of good scalability that can exploit the abundant\nunlabeled data at a lower cost. To improve the efficiency, we examine the\nmuch-overlooked redundancy in maintaining a full-length token-level\npresentation, especially for tasks that only require a single-vector\npresentation of the sequence. With this intuition, we propose\nFunnel-Transformer which gradually compresses the sequence of hidden states to\na shorter one and hence reduces the computation cost. More importantly, by\nre-investing the saved FLOPs from length reduction in constructing a deeper or\nwider model, we further improve the model capacity. In addition, to perform\ntoken-level predictions as required by common pretraining objectives,\nFunnel-Transformer is able to recover a deep representation for each token from\nthe reduced hidden sequence via a decoder. Empirically, with comparable or\nfewer FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide\nvariety of sequence-level prediction tasks, including text classification,\nlanguage understanding, and reading comprehension. The code and pretrained\ncheckpoints are available at https://github.com/laiguokun/Funnel-Transformer.",
            "authors": [
                "Zihang Dai",
                "Guokun Lai",
                "Yiming Yang",
                "Quoc V. Le"
            ]
        },
        {
            "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
            "arxiv": "2103.14030",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.14030v2",
            "abstract": "This paper presents a new vision Transformer, called Swin Transformer, that\ncapably serves as a general-purpose backbone for computer vision. Challenges in\nadapting Transformer from language to vision arise from differences between the\ntwo domains, such as large variations in the scale of visual entities and the\nhigh resolution of pixels in images compared to words in text. To address these\ndifferences, we propose a hierarchical Transformer whose representation is\ncomputed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme\nbrings greater efficiency by limiting self-attention computation to\nnon-overlapping local windows while also allowing for cross-window connection.\nThis hierarchical architecture has the flexibility to model at various scales\nand has linear computational complexity with respect to image size. These\nqualities of Swin Transformer make it compatible with a broad range of vision\ntasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and\ndense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP\non COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its\nperformance surpasses the previous state-of-the-art by a large margin of +2.7\nbox AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the\npotential of Transformer-based models as vision backbones. The hierarchical\ndesign and the shifted window approach also prove beneficial for all-MLP\narchitectures. The code and models are publicly available\nat~\\url{https://github.com/microsoft/Swin-Transformer}.",
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ]
        },
        {
            "title": "MPNet: Masked and Permuted Pre-training for Language Understanding",
            "arxiv": "2004.09297",
            "year": 2020,
            "url": "http://arxiv.org/abs/2004.09297v2",
            "abstract": "BERT adopts masked language modeling (MLM) for pre-training and is one of the\nmost successful pre-training models. Since BERT neglects dependency among\npredicted tokens, XLNet introduces permuted language modeling (PLM) for\npre-training to address this problem. However, XLNet does not leverage the full\nposition information of a sentence and thus suffers from position discrepancy\nbetween pre-training and fine-tuning. In this paper, we propose MPNet, a novel\npre-training method that inherits the advantages of BERT and XLNet and avoids\ntheir limitations. MPNet leverages the dependency among predicted tokens\nthrough permuted language modeling (vs. MLM in BERT), and takes auxiliary\nposition information as input to make the model see a full sentence and thus\nreducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a\nlarge-scale dataset (over 160GB text corpora) and fine-tune on a variety of\ndown-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet\noutperforms MLM and PLM by a large margin, and achieves better results on these\ntasks compared with previous state-of-the-art pre-trained methods (e.g., BERT,\nXLNet, RoBERTa) under the same model setting. The code and the pre-trained\nmodels are available at: https://github.com/microsoft/MPNet.",
            "authors": [
                "Kaitao Song",
                "Xu Tan",
                "Tao Qin",
                "Jianfeng Lu",
                "Tie-Yan Liu"
            ]
        },
        {
            "title": "TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models",
            "arxiv": "2109.10282",
            "year": 2021,
            "url": "http://arxiv.org/abs/2109.10282v3",
            "abstract": "Text recognition is a long-standing research problem for document\ndigitalization. Existing approaches for text recognition are usually built\nbased on CNN for image understanding and RNN for char-level text generation. In\naddition, another language model is usually needed to improve the overall\naccuracy as a post-processing step. In this paper, we propose an end-to-end\ntext recognition approach with pre-trained image Transformer and text\nTransformer models, namely TrOCR, which leverages the Transformer architecture\nfor both image understanding and wordpiece-level text generation. The TrOCR\nmodel is simple but effective, and can be pre-trained with large-scale\nsynthetic data and fine-tuned with human-labeled datasets. Experiments show\nthat the TrOCR model outperforms the current state-of-the-art models on both\nprinted and handwritten text recognition tasks. The code and models will be\npublicly available at https://aka.ms/TrOCR.",
            "authors": [
                "Minghao Li",
                "Tengchao Lv",
                "Lei Cui",
                "Yijuan Lu",
                "Dinei Florencio",
                "Cha Zhang",
                "Zhoujun Li",
                "Furu Wei"
            ]
        },
        {
            "title": "LayoutLM: Pre-training of Text and Layout for Document Image Understanding",
            "arxiv": "1912.13318",
            "year": 2019,
            "url": "http://arxiv.org/abs/1912.13318v5",
            "abstract": "Pre-training techniques have been verified successfully in a variety of NLP\ntasks in recent years. Despite the widespread use of pre-training models for\nNLP applications, they almost exclusively focus on text-level manipulation,\nwhile neglecting layout and style information that is vital for document image\nunderstanding. In this paper, we propose the \\textbf{LayoutLM} to jointly model\ninteractions between text and layout information across scanned document\nimages, which is beneficial for a great number of real-world document image\nunderstanding tasks such as information extraction from scanned documents.\nFurthermore, we also leverage image features to incorporate words' visual\ninformation into LayoutLM. To the best of our knowledge, this is the first time\nthat text and layout are jointly learned in a single framework for\ndocument-level pre-training. It achieves new state-of-the-art results in\nseveral downstream tasks, including form understanding (from 70.72 to 79.27),\nreceipt understanding (from 94.02 to 95.24) and document image classification\n(from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly\navailable at \\url{https://aka.ms/layoutlm}.",
            "authors": [
                "Yiheng Xu",
                "Minghao Li",
                "Lei Cui",
                "Shaohan Huang",
                "Furu Wei",
                "Ming Zhou"
            ]
        },
        {
            "title": "ByT5: Towards a token-free future with pre-trained byte-to-byte models",
            "arxiv": "2105.13626",
            "year": 2021,
            "url": "http://arxiv.org/abs/2105.13626v3",
            "abstract": "Most widely-used pre-trained language models operate on sequences of tokens\ncorresponding to word or subword units. By comparison, token-free models that\noperate directly on raw text (bytes or characters) have many benefits: they can\nprocess text in any language out of the box, they are more robust to noise, and\nthey minimize technical debt by removing complex and error-prone text\npreprocessing pipelines. Since byte or character sequences are longer than\ntoken sequences, past work on token-free models has often introduced new model\narchitectures designed to amortize the cost of operating directly on raw text.\nIn this paper, we show that a standard Transformer architecture can be used\nwith minimal modifications to process byte sequences. We characterize the\ntrade-offs in terms of parameter count, training FLOPs, and inference speed,\nand show that byte-level models are competitive with their token-level\ncounterparts. We also demonstrate that byte-level models are significantly more\nrobust to noise and perform better on tasks that are sensitive to spelling and\npronunciation. As part of our contribution, we release a new set of pre-trained\nbyte-level Transformer models based on the T5 architecture, as well as all code\nand data used in our experiments.",
            "authors": [
                "Linting Xue",
                "Aditya Barua",
                "Noah Constant",
                "Rami Al-Rfou",
                "Sharan Narang",
                "Mihir Kale",
                "Adam Roberts",
                "Colin Raffel"
            ]
        },
        {
            "title": "Recipes for building an open-domain chatbot",
            "arxiv": "2004.13637",
            "year": 2020,
            "url": "http://arxiv.org/abs/2004.13637v2",
            "abstract": "Building open-domain chatbots is a challenging area for machine learning\nresearch. While prior work has shown that scaling neural models in the number\nof parameters and the size of the data they are trained on gives improved\nresults, we show that other ingredients are important for a high-performing\nchatbot. Good conversation requires a number of skills that an expert\nconversationalist blends in a seamless way: providing engaging talking points\nand listening to their partners, and displaying knowledge, empathy and\npersonality appropriately, while maintaining a consistent persona. We show that\nlarge scale models can learn these skills when given appropriate training data\nand choice of generation strategy. We build variants of these recipes with 90M,\n2.7B and 9.4B parameter models, and make our models and code publicly\navailable. Human evaluations show our best models are superior to existing\napproaches in multi-turn dialogue in terms of engagingness and humanness\nmeasurements. We then discuss the limitations of this work by analyzing failure\ncases of our models.",
            "authors": [
                "Stephen Roller",
                "Emily Dinan",
                "Naman Goyal",
                "Da Ju",
                "Mary Williamson",
                "Yinhan Liu",
                "Jing Xu",
                "Myle Ott",
                "Kurt Shuster",
                "Eric M. Smith",
                "Y-Lan Boureau",
                "Jason Weston"
            ]
        },
        {
            "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
            "arxiv": "2002.08909",
            "year": 2020,
            "url": "http://arxiv.org/abs/2002.08909v1",
            "abstract": "Language model pre-training has been shown to capture a surprising amount of\nworld knowledge, crucial for NLP tasks such as question answering. However,\nthis knowledge is stored implicitly in the parameters of a neural network,\nrequiring ever-larger networks to cover more facts.\n  To capture knowledge in a more modular and interpretable way, we augment\nlanguage model pre-training with a latent knowledge retriever, which allows the\nmodel to retrieve and attend over documents from a large corpus such as\nWikipedia, used during pre-training, fine-tuning and inference. For the first\ntime, we show how to pre-train such a knowledge retriever in an unsupervised\nmanner, using masked language modeling as the learning signal and\nbackpropagating through a retrieval step that considers millions of documents.\n  We demonstrate the effectiveness of Retrieval-Augmented Language Model\npre-training (REALM) by fine-tuning on the challenging task of Open-domain\nQuestion Answering (Open-QA). We compare against state-of-the-art models for\nboth explicit and implicit knowledge storage on three popular Open-QA\nbenchmarks, and find that we outperform all previous methods by a significant\nmargin (4-16% absolute accuracy), while also providing qualitative benefits\nsuch as interpretability and modularity.",
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Ming-Wei Chang"
            ]
        },
        {
            "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs",
            "arxiv": "2107.14795",
            "year": 2021,
            "url": "http://arxiv.org/abs/2107.14795v3",
            "abstract": "A central goal of machine learning is the development of systems that can\nsolve many problems in as many data domains as possible. Current architectures,\nhowever, cannot be applied beyond a small set of stereotyped settings, as they\nbake in domain & task assumptions or scale poorly to large inputs or outputs.\nIn this work, we propose Perceiver IO, a general-purpose architecture that\nhandles data from arbitrary settings while scaling linearly with the size of\ninputs and outputs. Our model augments the Perceiver with a flexible querying\nmechanism that enables outputs of various sizes and semantics, doing away with\nthe need for task-specific architecture engineering. The same architecture\nachieves strong results on tasks spanning natural language and visual\nunderstanding, multi-task and multi-modal reasoning, and StarCraft II. As\nhighlights, Perceiver IO outperforms a Transformer-based BERT baseline on the\nGLUE language benchmark despite removing input tokenization and achieves\nstate-of-the-art performance on Sintel optical flow estimation with no explicit\nmechanisms for multiscale correspondence.",
            "authors": [
                "Andrew Jaegle",
                "Sebastian Borgeaud",
                "Jean-Baptiste Alayrac",
                "Carl Doersch",
                "Catalin Ionescu",
                "David Ding",
                "Skanda Koppula",
                "Daniel Zoran",
                "Andrew Brock",
                "Evan Shelhamer",
                "Olivier H\u00e9naff",
                "Matthew M. Botvinick",
                "Andrew Zisserman",
                "Oriol Vinyals",
                "Jo\u0101o Carreira"
            ]
        },
        {
            "title": "SqueezeBERT: What can computer vision teach NLP about efficient neural networks?",
            "arxiv": "2006.11316",
            "year": 2020,
            "url": "http://arxiv.org/abs/2006.11316v1",
            "abstract": "Humans read and write hundreds of billions of messages every day. Further,\ndue to the availability of large datasets, large computing systems, and better\nneural network models, natural language processing (NLP) technology has made\nsignificant strides in understanding, proofreading, and organizing these\nmessages. Thus, there is a significant opportunity to deploy NLP in myriad\napplications to help web users, social networks, and businesses. In particular,\nwe consider smartphones and other mobile devices as crucial platforms for\ndeploying NLP models at scale. However, today's highly-accurate NLP neural\nnetwork models such as BERT and RoBERTa are extremely computationally\nexpensive, with BERT-base taking 1.7 seconds to classify a text snippet on a\nPixel 3 smartphone. In this work, we observe that methods such as grouped\nconvolutions have yielded significant speedups for computer vision networks,\nbut many of these techniques have not been adopted by NLP neural network\ndesigners. We demonstrate how to replace several operations in self-attention\nlayers with grouped convolutions, and we use this technique in a novel network\narchitecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the\nPixel 3 while achieving competitive accuracy on the GLUE test set. The\nSqueezeBERT code will be released.",
            "authors": [
                "Forrest N. Iandola",
                "Albert E. Shaw",
                "Ravi Krishna",
                "Kurt W. Keutzer"
            ]
        },
        {
            "title": "Unsupervised Cross-lingual Representation Learning at Scale",
            "arxiv": "1911.02116",
            "year": 2019,
            "url": "http://arxiv.org/abs/1911.02116v2",
            "abstract": "This paper shows that pretraining multilingual language models at scale leads\nto significant performance gains for a wide range of cross-lingual transfer\ntasks. We train a Transformer-based masked language model on one hundred\nlanguages, using more than two terabytes of filtered CommonCrawl data. Our\nmodel, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a\nvariety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI,\n+13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs\nparticularly well on low-resource languages, improving 15.7% in XNLI accuracy\nfor Swahili and 11.4% for Urdu over previous XLM models. We also present a\ndetailed empirical analysis of the key factors that are required to achieve\nthese gains, including the trade-offs between (1) positive transfer and\ncapacity dilution and (2) the performance of high and low resource languages at\nscale. Finally, we show, for the first time, the possibility of multilingual\nmodeling without sacrificing per-language performance; XLM-R is very\ncompetitive with strong monolingual models on the GLUE and XNLI benchmarks. We\nwill make our code, data and models publicly available.",
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ]
        },
        {
            "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
            "arxiv": "2102.03334",
            "year": 2021,
            "url": "http://arxiv.org/abs/2102.03334v2",
            "abstract": "Vision-and-Language Pre-training (VLP) has improved performance on various\njoint vision-and-language downstream tasks. Current approaches to VLP heavily\nrely on image feature extraction processes, most of which involve region\nsupervision (e.g., object detection) and the convolutional architecture (e.g.,\nResNet). Although disregarded in the literature, we find it problematic in\nterms of both (1) efficiency/speed, that simply extracting input features\nrequires much more computation than the multimodal interaction steps; and (2)\nexpressive power, as it is upper bounded to the expressive power of the visual\nembedder and its predefined visual vocabulary. In this paper, we present a\nminimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the\nsense that the processing of visual inputs is drastically simplified to just\nthe same convolution-free manner that we process textual inputs. We show that\nViLT is up to tens of times faster than previous VLP models, yet with\ncompetitive or better downstream task performance. Our code and pre-trained\nweights are available at https://github.com/dandelin/vilt.",
            "authors": [
                "Wonjae Kim",
                "Bokyung Son",
                "Ildoo Kim"
            ]
        },
        {
            "title": "Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition",
            "arxiv": "2109.06870",
            "year": 2021,
            "url": "http://arxiv.org/abs/2109.06870v1",
            "abstract": "This paper is a study of performance-efficiency trade-offs in pre-trained\nmodels for automatic speech recognition (ASR). We focus on wav2vec 2.0, and\nformalize several architecture designs that influence both the model\nperformance and its efficiency. Putting together all our observations, we\nintroduce SEW (Squeezed and Efficient Wav2vec), a pre-trained model\narchitecture with significant improvements along both performance and\nefficiency dimensions across a variety of training setups. For example, under\nthe 100h-960h semi-supervised setup on LibriSpeech, SEW achieves a 1.9x\ninference speedup compared to wav2vec 2.0, with a 13.5% relative reduction in\nword error rate. With a similar inference time, SEW reduces word error rate by\n25-50% across different model sizes.",
            "authors": [
                "Felix Wu",
                "Kwangyoun Kim",
                "Jing Pan",
                "Kyu Han",
                "Kilian Q. Weinberger",
                "Yoav Artzi"
            ]
        },
        {
            "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
            "arxiv": "2110.13900",
            "year": 2021,
            "url": "http://arxiv.org/abs/2110.13900v4",
            "abstract": "Self-supervised learning (SSL) achieves great success in speech recognition,\nwhile limited exploration has been attempted for other speech processing tasks.\nAs speech signal contains multi-faceted information including speaker identity,\nparalinguistics, spoken content, etc., learning universal representations for\nall speech tasks is challenging. To tackle the problem, we propose a new\npre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM\njointly learns masked speech prediction and denoising in pre-training. By this\nmeans, WavLM does not only keep the speech content modeling capability by the\nmasked speech prediction, but also improves the potential to non-ASR tasks by\nthe speech denoising. In addition, WavLM employs gated relative position bias\nfor the Transformer structure to better capture sequence ordering of input\nspeech, and scale up the training dataset from 60k hours to 94k hours. WavLM\nLarge achieves state-of-the-art performance on the SUPERB benchmark, and brings\nsignificant improvements for various speech processing tasks on their\nrepresentative benchmarks. The code and pre-trained models are available at\nhttps://aka.ms/wavlm.",
            "authors": [
                "Sanyuan Chen",
                "Chengyi Wang",
                "Zhengyang Chen",
                "Yu Wu",
                "Shujie Liu",
                "Zhuo Chen",
                "Jinyu Li",
                "Naoyuki Kanda",
                "Takuya Yoshioka",
                "Xiong Xiao",
                "Jian Wu",
                "Long Zhou",
                "Shuo Ren",
                "Yanmin Qian",
                "Yao Qian",
                "Jian Wu",
                "Michael Zeng",
                "Xiangzhan Yu",
                "Furu Wei"
            ]
        },
        {
            "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
            "arxiv": "2202.03555",
            "year": 2022,
            "url": "http://arxiv.org/abs/2202.03555v1",
            "abstract": "While the general idea of self-supervised learning is identical across\nmodalities, the actual algorithms and objectives differ widely because they\nwere developed with a single modality in mind. To get us closer to general\nself-supervised learning, we present data2vec, a framework that uses the same\nlearning method for either speech, NLP or computer vision. The core idea is to\npredict latent representations of the full input data based on a masked view of\nthe input in a self-distillation setup using a standard Transformer\narchitecture. Instead of predicting modality-specific targets such as words,\nvisual tokens or units of human speech which are local in nature, data2vec\npredicts contextualized latent representations that contain information from\nthe entire input. Experiments on the major benchmarks of speech recognition,\nimage classification, and natural language understanding demonstrate a new\nstate of the art or competitive performance to predominant approaches.",
            "authors": [
                "Alexei Baevski",
                "Wei-Ning Hsu",
                "Qiantong Xu",
                "Arun Babu",
                "Jiatao Gu",
                "Michael Auli"
            ]
        },
        {
            "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
            "arxiv": "1912.08777",
            "year": 2019,
            "url": "http://arxiv.org/abs/1912.08777v3",
            "abstract": "Recent work pre-training Transformers with self-supervised objectives on\nlarge text corpora has shown great success when fine-tuned on downstream NLP\ntasks including text summarization. However, pre-training objectives tailored\nfor abstractive text summarization have not been explored. Furthermore there is\na lack of systematic evaluation across diverse domains. In this work, we\npropose pre-training large Transformer-based encoder-decoder models on massive\ntext corpora with a new self-supervised objective. In PEGASUS, important\nsentences are removed/masked from an input document and are generated together\nas one output sequence from the remaining sentences, similar to an extractive\nsummary. We evaluated our best PEGASUS model on 12 downstream summarization\ntasks spanning news, science, stories, instructions, emails, patents, and\nlegislative bills. Experiments demonstrate it achieves state-of-the-art\nperformance on all 12 downstream datasets measured by ROUGE scores. Our model\nalso shows surprising performance on low-resource summarization, surpassing\nprevious state-of-the-art results on 6 datasets with only 1000 examples.\nFinally we validated our results using human evaluation and show that our model\nsummaries achieve human performance on multiple datasets.",
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter J. Liu"
            ]
        },
        {
            "title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training",
            "arxiv": "2001.04063",
            "year": 2020,
            "url": "http://arxiv.org/abs/2001.04063v3",
            "abstract": "This paper presents a new sequence-to-sequence pre-training model called\nProphetNet, which introduces a novel self-supervised objective named future\nn-gram prediction and the proposed n-stream self-attention mechanism. Instead\nof optimizing one-step-ahead prediction in the traditional sequence-to-sequence\nmodel, the ProphetNet is optimized by n-step ahead prediction that predicts the\nnext n tokens simultaneously based on previous context tokens at each time\nstep. The future n-gram prediction explicitly encourages the model to plan for\nthe future tokens and prevent overfitting on strong local correlations. We\npre-train ProphetNet using a base scale dataset (16GB) and a large-scale\ndataset (160GB), respectively. Then we conduct experiments on CNN/DailyMail,\nGigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question\ngeneration tasks. Experimental results show that ProphetNet achieves new\nstate-of-the-art results on all these datasets compared to the models using the\nsame scale pre-training corpus.",
            "authors": [
                "Weizhen Qi",
                "Yu Yan",
                "Yeyun Gong",
                "Dayiheng Liu",
                "Nan Duan",
                "Jiusheng Chen",
                "Ruofei Zhang",
                "Ming Zhou"
            ]
        },
        {
            "title": "TAPAS: Weakly Supervised Table Parsing via Pre-training",
            "arxiv": "2004.02349",
            "year": 2020,
            "url": "http://arxiv.org/abs/2004.02349v2",
            "abstract": "Answering natural language questions over tables is usually seen as a\nsemantic parsing task. To alleviate the collection cost of full logical forms,\none popular approach focuses on weak supervision consisting of denotations\ninstead of logical forms. However, training semantic parsers from weak\nsupervision poses difficulties, and in addition, the generated logical forms\nare only used as an intermediate step prior to retrieving the denotation. In\nthis paper, we present TAPAS, an approach to question answering over tables\nwithout generating logical forms. TAPAS trains from weak supervision, and\npredicts the denotation by selecting table cells and optionally applying a\ncorresponding aggregation operator to such selection. TAPAS extends BERT's\narchitecture to encode tables as input, initializes from an effective joint\npre-training of text segments and tables crawled from Wikipedia, and is trained\nend-to-end. We experiment with three different semantic parsing datasets, and\nfind that TAPAS outperforms or rivals semantic parsing models by improving\nstate-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with\nthe state-of-the-art on WIKISQL and WIKITQ, but with a simpler model\narchitecture. We additionally find that transfer learning, which is trivial in\nour setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the\nstate-of-the-art.",
            "authors": [
                "Jonathan Herzig",
                "Pawe\u0142 Krzysztof Nowak",
                "Thomas M\u00fcller",
                "Francesco Piccinno",
                "Julian Martin Eisenschlos"
            ]
        },
        {
            "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
            "arxiv": "2006.11477",
            "year": 2020,
            "url": "http://arxiv.org/abs/2006.11477v3",
            "abstract": "We show for the first time that learning powerful representations from speech\naudio alone followed by fine-tuning on transcribed speech can outperform the\nbest semi-supervised methods while being conceptually simpler. wav2vec 2.0\nmasks the speech input in the latent space and solves a contrastive task\ndefined over a quantization of the latent representations which are jointly\nlearned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER\non the clean/other test sets. When lowering the amount of labeled data to one\nhour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour\nsubset while using 100 times less labeled data. Using just ten minutes of\nlabeled data and pre-training on 53k hours of unlabeled data still achieves\n4.8/8.2 WER. This demonstrates the feasibility of speech recognition with\nlimited amounts of labeled data.",
            "authors": [
                "Alexei Baevski",
                "Henry Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ]
        },
        {
            "title": "Optimal Subarchitecture Extraction For BERT",
            "arxiv": "2010.10499",
            "year": 2020,
            "url": "http://arxiv.org/abs/2010.10499v2",
            "abstract": "We extract an optimal subset of architectural parameters for the BERT\narchitecture from Devlin et al. (2018) by applying recent breakthroughs in\nalgorithms for neural architecture search. This optimal subset, which we refer\nto as \"Bort\", is demonstrably smaller, having an effective (that is, not\ncounting the embedding layer) size of $5.5\\%$ the original BERT-large\narchitecture, and $16\\%$ of the net size. Bort is also able to be pretrained in\n$288$ GPU hours, which is $1.2\\%$ of the time required to pretrain the\nhighest-performing BERT parametric architectural variant, RoBERTa-large (Liu et\nal., 2019), and about $33\\%$ of that of the world-record, in GPU hours,\nrequired to train BERT-large on the same hardware. It is also $7.9$x faster on\na CPU, as well as being better performing than other compressed variants of the\narchitecture, and some of the non-compressed variants: it obtains performance\nimprovements of between $0.3\\%$ and $31\\%$, absolute, with respect to\nBERT-large, on multiple public natural language understanding (NLU) benchmarks.",
            "authors": [
                "Adrian de Wynter",
                "Daniel J. Perry"
            ]
        },
        {
            "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
            "arxiv": "2105.15203",
            "year": 2021,
            "url": "http://arxiv.org/abs/2105.15203v3",
            "abstract": "We present SegFormer, a simple, efficient yet powerful semantic segmentation\nframework which unifies Transformers with lightweight multilayer perception\n(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a\nnovel hierarchically structured Transformer encoder which outputs multiscale\nfeatures. It does not need positional encoding, thereby avoiding the\ninterpolation of positional codes which leads to decreased performance when the\ntesting resolution differs from training. 2) SegFormer avoids complex decoders.\nThe proposed MLP decoder aggregates information from different layers, and thus\ncombining both local attention and global attention to render powerful\nrepresentations. We show that this simple and lightweight design is the key to\nefficient segmentation on Transformers. We scale our approach up to obtain a\nseries of models from SegFormer-B0 to SegFormer-B5, reaching significantly\nbetter performance and efficiency than previous counterparts. For example,\nSegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x\nsmaller and 2.2% better than the previous best method. Our best model,\nSegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows\nexcellent zero-shot robustness on Cityscapes-C. Code will be released at:\ngithub.com/NVlabs/SegFormer.",
            "authors": [
                "Enze Xie",
                "Wenhai Wang",
                "Zhiding Yu",
                "Anima Anandkumar",
                "Jose M. Alvarez",
                "Ping Luo"
            ]
        },
        {
            "title": "FlauBERT: Unsupervised Language Model Pre-training for French",
            "arxiv": "1912.05372",
            "year": 2019,
            "url": "http://arxiv.org/abs/1912.05372v4",
            "abstract": "Language models have become a key step to achieve state-of-the art results in\nmany different Natural Language Processing (NLP) tasks. Leveraging the huge\namount of unlabeled texts nowadays available, they provide an efficient way to\npre-train continuous word representations that can be fine-tuned for a\ndownstream task, along with their contextualization at the sentence level. This\nhas been widely demonstrated for English using contextualized representations\n(Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al.,\n2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and\nshare FlauBERT, a model learned on a very large and heterogeneous French\ncorpus. Models of different sizes are trained using the new CNRS (French\nNational Centre for Scientific Research) Jean Zay supercomputer. We apply our\nFrench language models to diverse NLP tasks (text classification, paraphrasing,\nnatural language inference, parsing, word sense disambiguation) and show that\nmost of the time they outperform other pre-training approaches. Different\nversions of FlauBERT as well as a unified evaluation protocol for the\ndownstream tasks, called FLUE (French Language Understanding Evaluation), are\nshared to the research community for further reproducible experiments in French\nNLP.",
            "authors": [
                "Hang Le",
                "Lo\u00efc Vial",
                "Jibril Frej",
                "Vincent Segonne",
                "Maximin Coavoux",
                "Benjamin Lecouteux",
                "Alexandre Allauzen",
                "Beno\u00eet Crabb\u00e9",
                "Laurent Besacier",
                "Didier Schwab"
            ]
        },
        {
            "title": "fairseq S2T: Fast Speech-to-Text Modeling with fairseq",
            "arxiv": "2010.05171",
            "year": 2020,
            "url": "http://arxiv.org/abs/2010.05171v1",
            "abstract": "We introduce fairseq S2T, a fairseq extension for speech-to-text (S2T)\nmodeling tasks such as end-to-end speech recognition and speech-to-text\ntranslation. It follows fairseq's careful design for scalability and\nextensibility. We provide end-to-end workflows from data pre-processing, model\ntraining to offline (online) inference. We implement state-of-the-art RNN-based\nas well as Transformer-based models and open-source detailed training recipes.\nFairseq's machine translation models and language models can be seamlessly\nintegrated into S2T workflows for multi-task learning or transfer learning.\nFairseq S2T documentation and examples are available at\nhttps://github.com/pytorch/fairseq/tree/master/examples/speech_to_text.",
            "authors": [
                "Changhan Wang",
                "Yun Tang",
                "Xutai Ma",
                "Anne Wu",
                "Dmytro Okhonko",
                "Juan Pino"
            ]
        },
        {
            "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
            "arxiv": "2105.00572",
            "year": 2021,
            "url": "http://arxiv.org/abs/2105.00572v1",
            "abstract": "Recent work has demonstrated the effectiveness of cross-lingual language\nmodel pretraining for cross-lingual understanding. In this study, we present\nthe results of two larger multilingual masked language models, with 3.5B and\n10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform\nXLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the\nRoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on\naverage while handling 99 more languages. This suggests pretrained models with\nlarger capacity may obtain both strong performance on high-resource languages\nwhile greatly improving low-resource languages. We make our code and models\npublicly available.",
            "authors": [
                "Naman Goyal",
                "Jingfei Du",
                "Myle Ott",
                "Giri Anantharaman",
                "Alexis Conneau"
            ]
        },
        {
            "title": "BARThez: a Skilled Pretrained French Sequence-to-Sequence Model",
            "arxiv": "2010.12321",
            "year": 2020,
            "url": "http://arxiv.org/abs/2010.12321v2",
            "abstract": "Inductive transfer learning has taken the entire NLP field by storm, with\nmodels such as BERT and BART setting new state of the art on countless NLU\ntasks. However, most of the available models and research have been conducted\nfor English. In this work, we introduce BARThez, the first large-scale\npretrained seq2seq model for French. Being based on BART, BARThez is\nparticularly well-suited for generative tasks. We evaluate BARThez on five\ndiscriminative tasks from the FLUE benchmark and two generative tasks from a\nnovel summarization dataset, OrangeSum, that we created for this research. We\nshow BARThez to be very competitive with state-of-the-art BERT-based French\nlanguage models such as CamemBERT and FlauBERT. We also continue the\npretraining of a multilingual BART on BARThez' corpus, and show our resulting\nmodel, mBARThez, to significantly boost BARThez' generative performance. Code,\ndata and models are publicly available.",
            "authors": [
                "Moussa Kamal Eddine",
                "Antoine J. -P. Tixier",
                "Michalis Vazirgiannis"
            ]
        },
        {
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "arxiv": "1910.01108",
            "year": 2019,
            "url": "http://arxiv.org/abs/1910.01108v4",
            "abstract": "As Transfer Learning from large-scale pre-trained models becomes more\nprevalent in Natural Language Processing (NLP), operating these large models in\non-the-edge and/or under constrained computational training or inference\nbudgets remains challenging. In this work, we propose a method to pre-train a\nsmaller general-purpose language representation model, called DistilBERT, which\ncan then be fine-tuned with good performances on a wide range of tasks like its\nlarger counterparts. While most prior work investigated the use of distillation\nfor building task-specific models, we leverage knowledge distillation during\nthe pre-training phase and show that it is possible to reduce the size of a\nBERT model by 40%, while retaining 97% of its language understanding\ncapabilities and being 60% faster. To leverage the inductive biases learned by\nlarger models during pre-training, we introduce a triple loss combining\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\nfor on-device computations in a proof-of-concept experiment and a comparative\non-device study.",
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf"
            ]
        },
        {
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "arxiv": "1907.11692",
            "year": 2019,
            "url": "http://arxiv.org/abs/1907.11692v1",
            "abstract": "Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.",
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ]
        },
        {
            "title": "CamemBERT: a Tasty French Language Model",
            "arxiv": "1911.03894",
            "year": 2019,
            "url": "http://arxiv.org/abs/1911.03894v3",
            "abstract": "Pretrained language models are now ubiquitous in Natural Language Processing.\nDespite their success, most available models have either been trained on\nEnglish data or on the concatenation of data in multiple languages. This makes\npractical use of such models --in all languages except English-- very limited.\nIn this paper, we investigate the feasibility of training monolingual\nTransformer-based language models for other languages, taking French as an\nexample and evaluating our language models on part-of-speech tagging,\ndependency parsing, named entity recognition and natural language inference\ntasks. We show that the use of web crawled data is preferable to the use of\nWikipedia data. More surprisingly, we show that a relatively small web crawled\ndataset (4GB) leads to results that are as good as those obtained using larger\ndatasets (130+GB). Our best performing model CamemBERT reaches or improves the\nstate of the art in all four downstream tasks.",
            "authors": [
                "Louis Martin",
                "Benjamin Muller",
                "Pedro Javier Ortiz Su\u00e1rez",
                "Yoann Dupont",
                "Laurent Romary",
                "\u00c9ric Villemonte de la Clergerie",
                "Djam\u00e9 Seddah",
                "Beno\u00eet Sagot"
            ]
        },
        {
            "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation",
            "arxiv": "1909.05858",
            "year": 2019,
            "url": "http://arxiv.org/abs/1909.05858v2",
            "abstract": "Large-scale language models show promising text generation capabilities, but\nusers cannot easily control particular aspects of the generated text. We\nrelease CTRL, a 1.63 billion-parameter conditional transformer language model,\ntrained to condition on control codes that govern style, content, and\ntask-specific behavior. Control codes were derived from structure that\nnaturally co-occurs with raw text, preserving the advantages of unsupervised\nlearning while providing more explicit control over text generation. These\ncodes also allow CTRL to predict which parts of the training data are most\nlikely given a sequence. This provides a potential method for analyzing large\namounts of data via model-based source attribution. We have released multiple\nfull-sized, pretrained versions of CTRL at https://github.com/salesforce/ctrl.",
            "authors": [
                "Nitish Shirish Keskar",
                "Bryan McCann",
                "Lav R. Varshney",
                "Caiming Xiong",
                "Richard Socher"
            ]
        },
        {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "arxiv": "1910.10683",
            "year": 2019,
            "url": "http://arxiv.org/abs/1910.10683v3",
            "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.",
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ]
        },
        {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "arxiv": "1810.04805",
            "year": 2018,
            "url": "http://arxiv.org/abs/1810.04805v2",
            "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).",
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ]
        },
        {
            "title": "DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation",
            "arxiv": "1911.00536",
            "year": 2019,
            "url": "http://arxiv.org/abs/1911.00536v3",
            "abstract": "We present a large, tunable neural conversational response generation model,\nDialoGPT (dialogue generative pre-trained transformer). Trained on 147M\nconversation-like exchanges extracted from Reddit comment chains over a period\nspanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch\ntransformer to attain a performance close to human both in terms of automatic\nand human evaluation in single-turn dialogue settings. We show that\nconversational systems that leverage DialoGPT generate more relevant,\ncontentful and context-consistent responses than strong baseline systems. The\npre-trained model and training pipeline are publicly released to facilitate\nresearch into neural response generation and the development of more\nintelligent open-domain dialogue systems.",
            "authors": [
                "Yizhe Zhang",
                "Siqi Sun",
                "Michel Galley",
                "Yen-Chun Chen",
                "Chris Brockett",
                "Xiang Gao",
                "Jianfeng Gao",
                "Jingjing Liu",
                "Bill Dolan"
            ]
        },
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "End-to-End Object Detection with Transformers",
            "arxiv": "2005.12872",
            "year": 2020,
            "url": "http://arxiv.org/abs/2005.12872v3",
            "abstract": "We present a new method that views object detection as a direct set\nprediction problem. Our approach streamlines the detection pipeline,\neffectively removing the need for many hand-designed components like a\nnon-maximum suppression procedure or anchor generation that explicitly encode\nour prior knowledge about the task. The main ingredients of the new framework,\ncalled DEtection TRansformer or DETR, are a set-based global loss that forces\nunique predictions via bipartite matching, and a transformer encoder-decoder\narchitecture. Given a fixed small set of learned object queries, DETR reasons\nabout the relations of the objects and the global image context to directly\noutput the final set of predictions in parallel. The new model is conceptually\nsimple and does not require a specialized library, unlike many other modern\ndetectors. DETR demonstrates accuracy and run-time performance on par with the\nwell-established and highly-optimized Faster RCNN baseline on the challenging\nCOCO object detection dataset. Moreover, DETR can be easily generalized to\nproduce panoptic segmentation in a unified manner. We show that it\nsignificantly outperforms competitive baselines. Training code and pretrained\nmodels are available at https://github.com/facebookresearch/detr.",
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ]
        },
        {
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
            "arxiv": "1910.13461",
            "year": 2019,
            "url": "http://arxiv.org/abs/1910.13461v1",
            "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence\nmodels. BART is trained by (1) corrupting text with an arbitrary noising\nfunction, and (2) learning a model to reconstruct the original text. It uses a\nstandard Tranformer-based neural machine translation architecture which,\ndespite its simplicity, can be seen as generalizing BERT (due to the\nbidirectional encoder), GPT (with the left-to-right decoder), and many other\nmore recent pretraining schemes. We evaluate a number of noising approaches,\nfinding the best performance by both randomly shuffling the order of the\noriginal sentences and using a novel in-filling scheme, where spans of text are\nreplaced with a single mask token. BART is particularly effective when fine\ntuned for text generation but also works well for comprehension tasks. It\nmatches the performance of RoBERTa with comparable training resources on GLUE\nand SQuAD, achieves new state-of-the-art results on a range of abstractive\ndialogue, question answering, and summarization tasks, with gains of up to 6\nROUGE. BART also provides a 1.1 BLEU increase over a back-translation system\nfor machine translation, with only target language pretraining. We also report\nablation experiments that replicate other pretraining schemes within the BART\nframework, to better measure which factors most influence end-task performance.",
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ]
        },
        {
            "title": "LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding",
            "arxiv": "2012.14740",
            "year": 2020,
            "url": "http://arxiv.org/abs/2012.14740v4",
            "abstract": "Pre-training of text and layout has proved effective in a variety of\nvisually-rich document understanding tasks due to its effective model\narchitecture and the advantage of large-scale unlabeled scanned/digital-born\ndocuments. We propose LayoutLMv2 architecture with new pre-training tasks to\nmodel the interaction among text, layout, and image in a single multi-modal\nframework. Specifically, with a two-stream multi-modal Transformer encoder,\nLayoutLMv2 uses not only the existing masked visual-language modeling task but\nalso the new text-image alignment and text-image matching tasks, which make it\nbetter capture the cross-modality interaction in the pre-training stage.\nMeanwhile, it also integrates a spatial-aware self-attention mechanism into the\nTransformer architecture so that the model can fully understand the relative\npositional relationship among different text blocks. Experiment results show\nthat LayoutLMv2 outperforms LayoutLM by a large margin and achieves new\nstate-of-the-art results on a wide variety of downstream visually-rich document\nunderstanding tasks, including FUNSD (0.7895 $\\to$ 0.8420), CORD (0.9493 $\\to$\n0.9601), SROIE (0.9524 $\\to$ 0.9781), Kleister-NDA (0.8340 $\\to$ 0.8520),\nRVL-CDIP (0.9443 $\\to$ 0.9564), and DocVQA (0.7295 $\\to$ 0.8672). We made our\nmodel and code publicly available at \\url{https://aka.ms/layoutlmv2}.",
            "authors": [
                "Yang Xu",
                "Yiheng Xu",
                "Tengchao Lv",
                "Lei Cui",
                "Furu Wei",
                "Guoxin Wang",
                "Yijuan Lu",
                "Dinei Florencio",
                "Cha Zhang",
                "Wanxiang Che",
                "Min Zhang",
                "Lidong Zhou"
            ]
        },
        {
            "title": "CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation",
            "arxiv": "2103.06874",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.06874v3",
            "abstract": "Pipelined NLP systems have largely been superseded by end-to-end neural\nmodeling, yet nearly all commonly-used models still require an explicit\ntokenization step. While recent tokenization approaches based on data-derived\nsubword lexicons are less brittle than manually engineered tokenizers, these\ntechniques are not equally suited to all languages, and the use of any fixed\nvocabulary may limit a model's ability to adapt. In this paper, we present\nCANINE, a neural encoder that operates directly on character sequences, without\nexplicit tokenization or vocabulary, and a pre-training strategy that operates\neither directly on characters or optionally uses subwords as a soft inductive\nbias. To use its finer-grained input effectively and efficiently, CANINE\ncombines downsampling, which reduces the input sequence length, with a deep\ntransformer stack, which encodes context. CANINE outperforms a comparable mBERT\nmodel by 2.8 F1 on TyDi QA, a challenging multilingual benchmark, despite\nhaving 28% fewer model parameters.",
            "authors": [
                "Jonathan H. Clark",
                "Dan Garrette",
                "Iulia Turc",
                "John Wieting"
            ]
        },
        {
            "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
            "arxiv": "1901.02860",
            "year": 2019,
            "url": "http://arxiv.org/abs/1901.02860v3",
            "abstract": "Transformers have a potential of learning longer-term dependency, but are\nlimited by a fixed-length context in the setting of language modeling. We\npropose a novel neural architecture Transformer-XL that enables learning\ndependency beyond a fixed length without disrupting temporal coherence. It\nconsists of a segment-level recurrence mechanism and a novel positional\nencoding scheme. Our method not only enables capturing longer-term dependency,\nbut also resolves the context fragmentation problem. As a result,\nTransformer-XL learns dependency that is 80% longer than RNNs and 450% longer\nthan vanilla Transformers, achieves better performance on both short and long\nsequences, and is up to 1,800+ times faster than vanilla Transformers during\nevaluation. Notably, we improve the state-of-the-art results of bpc/perplexity\nto 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion\nWord, and 54.5 on Penn Treebank (without finetuning). When trained only on\nWikiText-103, Transformer-XL manages to generate reasonably coherent, novel\ntext articles with thousands of tokens. Our code, pretrained models, and\nhyperparameters are available in both Tensorflow and PyTorch.",
            "authors": [
                "Zihang Dai",
                "Zhilin Yang",
                "Yiming Yang",
                "Jaime Carbonell",
                "Quoc V. Le",
                "Ruslan Salakhutdinov"
            ]
        },
        {
            "title": "Simple and Effective Zero-shot Cross-lingual Phoneme Recognition",
            "arxiv": "2109.11680",
            "year": 2021,
            "url": "http://arxiv.org/abs/2109.11680v1",
            "abstract": "Recent progress in self-training, self-supervised pretraining and\nunsupervised learning enabled well performing speech recognition systems\nwithout any labeled data. However, in many cases there is labeled data\navailable for related languages which is not utilized by these methods. This\npaper extends previous work on zero-shot cross-lingual transfer learning by\nfine-tuning a multilingually pretrained wav2vec 2.0 model to transcribe unseen\nlanguages. This is done by mapping phonemes of the training languages to the\ntarget language using articulatory features. Experiments show that this simple\nmethod significantly outperforms prior work which introduced task-specific\narchitectures and used only part of a monolingually pretrained model.",
            "authors": [
                "Qiantong Xu",
                "Alexei Baevski",
                "Michael Auli"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "SQuAD"
            },
            {
                "name": "Amazon"
            },
            {
                "name": "Text8"
            },
            {
                "name": "RACE"
            },
            {
                "name": "enwiki8"
            },
            {
                "name": "GigaWord"
            },
            {
                "name": "ADE20K"
            },
            {
                "name": "RVL-CDIP"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "MultiNLI"
            },
            {
                "name": "Cityscapes"
            },
            {
                "name": "TACRED"
            },
            {
                "name": "Wikipedia"
            },
            {
                "name": "WikiText-103"
            },
            {
                "name": "COCO"
            },
            {
                "name": "GLUE"
            },
            {
                "name": "Penn Treebank"
            },
            {
                "name": "WikiHop"
            },
            {
                "name": "Librispeech"
            },
            {
                "name": "TriviaQA"
            },
            {
                "name": "IMDb"
            },
            {
                "name": "One Billion Word"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "WikiSQL"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "ImageNet"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999999926967413,
        "task": "Machine Translation",
        "task_prob": 0.93786429899214
    }
}