{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Team-Bulldawgs",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "ankit-vaghela30",
                "owner_type": "User",
                "name": "Google-landmark-prediction",
                "url": "https://github.com/ankit-vaghela30/Google-landmark-prediction",
                "stars": 0,
                "pushed_at": "2019-03-12 19:14:37+00:00",
                "created_at": "2019-03-12 19:14:00+00:00",
                "language": "Jupyter Notebook",
                "description": "A Kaggle challenge which aims at helping people organize their photo collection, Result: 46th percentile",
                "license": "MIT License",
                "frameworks": [
                    "Keras",
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "7bbc71c09205c78d790739d246bbe4f9f1881c17",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ankit-vaghela30/Google-landmark-prediction/blob/master/.gitignore"
                    }
                },
                "size": 1157
            },
            {
                "type": "code",
                "name": "CONTRIBUTORS.md",
                "sha": "ce6cf36d62b410e0b1769e28f03fdc69e466cd8b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ankit-vaghela30/Google-landmark-prediction/blob/master/CONTRIBUTORS.md"
                    }
                },
                "size": 965
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "01f91480b206e909b17ab269c50214ad62b296a0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ankit-vaghela30/Google-landmark-prediction/blob/master/LICENSE"
                    }
                },
                "size": 1079
            },
            {
                "type": "code",
                "name": "bulldawg",
                "sha": "8372be76496723abb947309e0591faf72d79380b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ankit-vaghela30/Google-landmark-prediction/tree/master/bulldawg"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "0889a375f00a96e14c9a73e067a2c9369bb9858c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ankit-vaghela30/Google-landmark-prediction/tree/master/docs"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "media",
                "sha": "183ea8a0541e0adde17cb9325509139760bf807f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ankit-vaghela30/Google-landmark-prediction/tree/master/media"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "notebooks",
                "sha": "b9ff46ca800e0cad8139e9e64a06fe02a4192c97",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ankit-vaghela30/Google-landmark-prediction/tree/master/notebooks"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "c0cbfe932fa729fa72d06eab87cbd86a8e31bf27",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ankit-vaghela30/Google-landmark-prediction/blob/master/setup.py"
                    }
                },
                "size": 350
            }
        ]
    },
    "authors": [
        {
            "name": "Ankit Vaghela",
            "email": "ankit.vaghela30@gmail.com",
            "github_id": "ankit-vaghela30"
        },
        {
            "name": "Hiten Mahesh Nirmal",
            "email": "hitennirmal94@gmail.com",
            "github_id": "hitennirmal"
        }
    ],
    "tags": [],
    "description": "A Kaggle challenge which aims at helping people organize their photo collection, Result: 46th percentile",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/ankit-vaghela30/Google-landmark-prediction",
            "stars": 0,
            "issues": true,
            "readme": "# Team-Bulldawgs\n\n## Team-Bulldawgs implementation of Final Project 5 (Google Landmark Prediction) of Data Science Practicum Spring 2018\n\n### Project Description\nWith the vast amount of landmark images on the Internet, the time has come to think about landmarks globally, namely to build a landmark prediction engine, on the scale of the entire earth.\n\nDevelop a technology that can predict landmark labels directly from image pixels, to help people better understand and organize the photo collections.The project challenges to build models that recognize the correct landmark in a dataset of challenging test images. The Kaggle challenge provides access to annotated data which consists of various links to google images along with their respective labeled classes.\n\nLink to the Kaggle competition: [https://www.kaggle.com/c/landmark-recognition-challenge](https://www.kaggle.com/c/landmark-recognition-challenge)\n\n### Data-Set\n\nThe dataset for this Kaggle competition is available on the following website: \nhttps://www.kaggle.com/c/landmark-recognition-challenge/data\n\nFile descriptions:\n\n`train.csv` - the training image set\n\n`test.csv` - the test set containing the test images for which we may predict landmarks\n\n`sample submission.csv` - a sample submission file format\n\nThere are overall approximately 1.2 million train images with 15,000 unique classes, whereas 0.1 million testing images for labeling and classification.\n\n### Requirements\n\nThe project requires the following technologies to be installed.\n* Instructions to download and install Python can be found [here](https://www.python.org/).\n* Instructions to download and install Keras can be found [here](https://keras.io/).\n* Instructions to download and install Anaconda can be found [here](https://www.continuum.io/downloads).\n* Instructions to download and install Tensor Flow can be found [here](https://www.tensorflow.org/install/install_mac).\n* Instructions to download and install OpenCV Library can be found [here](https://opencv.org/).\n\n### Execution Step\n```\npython3 -m bulldawg.__main__ <args>\n```\nThe following arguments are supported by our model:\n- **model** : Specify the deep learning model to be used\n                  Ex: --model=\"resnet\", --model=\"cnn\"\n- **process** : Specify if you want to train or test the dataset and keep empty if train and test both required\n                  Ex: --process= \"train\", --mode= \"test\"\n- **operation** : Specify if you want to download and prepare the dataset or keep empty if you want to use the model\n                  Ex: --operation=\"d_data\"\n- **path** : Specify the path where dataset and model will be saved and loaded from\n                  Ex: --path=\"/home/ubuntu/img.npy\"\n - **num_top_classes** : Specify number of most frequency image labels you want to use. Empty if you want to use entire dataset\n                  Ex: --num_top_classes=\"400\"\n### Approach\n\nTwo Models were implemented:\n\n* Simple Convolutional Network- More details can be found [here](https://github.com/dsp-uga/Team-Bulldawgs/wiki/Network-Models#simple-convolutional-network)\n\n* Residual Learning for Image Recognition- Resnet50- More details can be found [here](https://github.com/dsp-uga/Team-Bulldawgs/wiki/Network-Models#residual-learning-for-image-recognition)\n\n### Final Output\n\nWe predict the landmark and their respective classes for the test data-set and submitted it to kaggle competiton\n\nThe Sample Submission format is\n\n`id,landmarks`\n\n`000088da12d664db,8815 0.03`\n\n`0001623c6d808702,5523 0.85`\n\n`0001bbb682d45002,5328 0.5`\n\n**Our Final Kaggle Rank is 134 out of 309 participants [as on 27th April 16.00] with a score of 0.003**\n\nBelow is the screenshot attached for the same.\nWe got this score by training on just 50 percent of the dataset, so we hope to get a better score by training on entire dataset.\n\n\n<p align=\"center\" >\n  <img src=\"https://github.com/dsp-uga/Team-Bulldawgs/blob/master/media/kaggle_score.png\", height=\"350\" width=\"500\">\n</p>\n\n### Contributors\n\nSee [contributor](https://github.com/dsp-uga/Team-Bulldawgs/blob/master/CONTRIBUTORS.md) file for more details.\n\n### License\n\nThis project is licensed under the MIT License - see the [License file](https://github.com/dsp-uga/Team-Bulldawgs/blob/master/LICENSE) for details\n\n### Acknowledgments\n\n* This project was completed as a part of the Data Science Practicum 2018 course at the University of Georgia\n*This work would not have been possible without the support of Dr. Shannon Quinn, [Assistant\nProfessor, University of Georgia Departments of Computer Science and Cellular Biology] who\nworked actively to provide us with academic time and advice to pursue those goals.\n* Other resources used have been cited in their corresponding wiki page.\n\n### References\n\nReferences for this Project:\n\n* https://arxiv.org/pdf/1512.03385.pdf\n* https://arxiv.org/abs/1512.03385\n* https://github.com/raghakot/keras-resnet\n* https://github.com/tensorflow/models/tree/master/research/object_detection\n* https://www.kaggle.com/c/landmark-recognition-challenge\n* http://pages.cs.wisc.edu/~dyer/cs534-spring10/papers/google_landmark_recognition.pdf\n* https://www.youtube.com/watch?v=yDVap0lpYKg\n\n",
            "readme_url": "https://github.com/ankit-vaghela30/Google-landmark-prediction",
            "frameworks": [
                "Keras",
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999944659138403,
        "task": "Object Detection",
        "task_prob": 0.9894708686061561
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "COCO"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ILSVRC 2015"
            }
        ]
    }
}