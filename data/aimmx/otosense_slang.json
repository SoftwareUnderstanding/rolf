{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "Table of contents generated with markdown-toc",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "otosense",
                "owner_type": "Organization",
                "name": "slang",
                "url": "https://github.com/otosense/slang",
                "stars": 4,
                "pushed_at": "2022-03-01 22:14:00+00:00",
                "created_at": "2020-02-26 19:17:25+00:00",
                "language": "Python",
                "description": "A light weight version of Slang: Tools to build a language of sound.",
                "license": "Apache License 2.0",
                "frameworks": [
                    "scikit-learn"
                ]
            },
            {
                "type": "code",
                "name": ".gitattributes",
                "sha": "2f77e919cdc5d76d29bfc56dbc96e814aee0dc9a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/otosense/slang/blob/master/.gitattributes"
                    }
                },
                "size": 31
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "bf829736aeb1e74a7f4cf20414cbe48d53c801e4",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/otosense/slang/tree/master/.github"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "b6e47617de110dea7ca47e087ff1347cc2646eda",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/otosense/slang/blob/master/.gitignore"
                    }
                },
                "size": 1799
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "261eeb9e9f8b2b4b0d119366dda99c6fd7d35c64",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/otosense/slang/blob/master/LICENSE"
                    }
                },
                "size": 11357
            },
            {
                "type": "code",
                "name": "conftest.py",
                "sha": "1cb08c861315d36ee176f23791ef1df7add182d6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/otosense/slang/blob/master/conftest.py"
                    }
                },
                "size": 389
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "0a0d2a87c9f03638d5852f5819fe4e5c7b0838ad",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/otosense/slang/tree/master/docs"
                    }
                },
                "num_files": 13
            },
            {
                "type": "code",
                "name": "docsrc",
                "sha": "8fa20167d3c9897c98e65d6b4331a1990d5ffd32",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/otosense/slang/tree/master/docsrc"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "img",
                "sha": "cef43dcda20cc6881c93081eafea16081ccf1307",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/otosense/slang/tree/master/img"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "setup.cfg",
                "sha": "1df08b5758d33fa2305517845639dbb590e76618",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/otosense/slang/blob/master/setup.cfg"
                    }
                },
                "size": 644
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "201cd4c29338ced795461cb4c0ab37bf9eaa3359",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/otosense/slang/blob/master/setup.py"
                    }
                },
                "size": 91
            },
            {
                "type": "code",
                "name": "slang",
                "sha": "c2d15f6dc0bf2ce0356a5b86fe79f1435306f7f8",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/otosense/slang/tree/master/slang"
                    }
                },
                "num_files": 12
            }
        ]
    },
    "authors": [
        {
            "name": "Thor Whalen",
            "github_id": "thorwhalen"
        }
    ],
    "tags": [],
    "description": "A light weight version of Slang: Tools to build a language of sound.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/otosense/slang",
            "stars": 4,
            "issues": true,
            "readme": "- [Slang: Light weight tools to build signal languages](#slang--light-weight-tools-to-build-signal-languages)\n  * [A story to paint the horizon](#a-story-to-paint-the-horizon)\n  * [Okay, but what does a pipeline look like in slang](#okay--but-what-does-a-pipeline-look-like-in-slang)\n- [Sound Language](#sound-language)\n- [Structural and Syntactical Pattern Recognition](#structural-and-syntactical-pattern-recognition)\n- [Semantic Structure](#semantic-structure)\n- [Acoustics Structure](#acoustics-structure)\n  * [Alphabetization](#alphabetization)\n  * [Snips network](#snips-network)\n- [Snips Annotations](#snips-annotations)\n  * [Relationship between Annotations and the Syntactic Approach](#relationship-between-annotations-and-the-syntactic-approach)\n- [Modeling](#modeling)\n- [References](#references)\n\n<small><i><a href='http://ecotrust-canada.github.io/markdown-toc/'>Table of contents generated with markdown-toc</a></i></small>\n\n\n# Slang: Light weight tools to build signal languages\n\nSlang is a structural approach to sound/signal machine learning. \nHere, signals are structured into inter-related annotated parts. \nThe signal's stream is transformed into a stream of symbols with associated \nqualifications, quantifications and/or relations that can be used to analyze, interpret, and \ncommunicate the signal's informational content: \nA language.\n\nWe humans have developed many systems of symbols to represent or transmit various forms of information. \nFor instance,\n- Natural spoken language, from phonemes to morphemes, to words and meta-word structures (simply put, grammar).\n- Written scripts to symbolize either the sounds of the spoken words, or the ideas they mean to symbolize. \n- Similarly, various musical notation evolved in different times and parts of the world. \nThese codified what was considered to be the essentials of musical expression, in\nsuch a way that it could be communicated in written form.\n\nSymbols, though not fully faithful representatives of what they symbolize, \ncan go a long way in communicating what's essential -- whether it's meaning, feeling, or how to make pizza.\nWhat is more; what the symbols (say words) themselves lack in accuracy, \ntheir combination and context make up for. \n\nSlang's objective is to provide that ability for signal. \nNote we will focus on sound mainly, since sound recognition is the birthplace of Slang, \nand it makes communicating ideas simpler and possibly more intuitive. \nBut we keep generalization in mind.\n\n## A story to paint the horizon\n\nImagine a device that could be dropped into a remote inhabited region with no prior knowledge of the local language. \nAfter hours/days of listening, it would figure out what phonemes the locals use, \nhow these phonemes co-occur (learning words), and eventually patterns guiding the structure of word sequences (grammar). \nIt has learned the syntax of the local language, unsupervised. \n\nNow show it examples of concrete things people are talking about (this is called \"grounding a language\"), \nand it will now be able to develop semantics.\n\nThe common thread through this learning evolutiion is the ability to detect and annotate patterns \nand relate these patterns to each other from lower to higher levels of abstraction. \n\n## Okay, but what does a pipeline look like in slang\n\nHere are the ingredients of a typical _running_ (as opposed to _learning_) pipeline.\n\n![](img/slang_flow.png)\n\n```\nsource -> [chunker] --> chk -> [featurizer] -> fv -> [quantizer] -> snip -> [ledger] -> stats -> [aggregator] -> aggr -> [trigger]\n```\n\n- `source`: A streaming signal source\n- `chunker`: Is fed the signal stream and creates a stream of signal chunks of fixed size. Parametrized by chunk size and other things, particular to the kind of chunker.\n- `featurizer`: Takes a chunk and returns a feature vector 'fv'.\n- `quantizer`: Compute a symbol (call it \"snip\" -- think _letter_, _phone_ or _atom_) from an `fv` -- the `snip` (say an integer) is from a finite set of snips.\n- `ledger`: Lookup information about the snip in a ledger and output the associated `stats`.\n- `aggregator`: Over one or several observed windows, update incrementally some aggregates of the streaming `stats`.\n- `trigger`: Given a condition on the aggregates, trigger some action.\n\nThe source stream is fed to the `chunker`, creating a stream of `chk`s, which are transformed into an stream of `stats`s by doing:\n```\nstats = lookup(quantizer(mat_mult(chk_to_spectr(chk))))\n```\nfor every `chk` created by `source+chunker`.\n\nOver one or several observed windows, update incrementally some aggregates of the streaming `stats`, and given a condition on every new aggregate, trigger some action.\n\n# Sound Language\n\nNot surprisingly, speech recognition is the sub-domain of sound recognition \nthat is closest to the syntactic method we propose. \nSpeech recognition must featurize sound at a granular level to capture micro occurrences such as phones, \nsubsequently combined to form phonemes, morphemes, and recognisable phrases. \nAdvanced speech recognition uses natural language processing to improve accuracy in that \nit exploits language contextual information in order to more accurately map sound to words.\n\nA language of sound would aspire to link sound to meaning in a similar combinatorial way, but offers a few differences \n\u2014 some simplifying and other complexifying the task. \nIn speech recognition the language, its constructs (phones, phonemes, words) \nand its combinatorial rules (grammar) are fixed and known. \nIn sound recognition, the language needs to be inferred (generated) from the context, \nits constructs defined, and its combinatorial rules learned. \nHowever, there is a fortuitous consideration: though natural language\u2019s expressive power is expansive, \nin sound recognition we need only to describe events relevant to sound.\n\nEssentially, SLANG represents both acoustic and semantic facets of the sound recognition pipeline as networks of \ninterconnected elements. An attractive implication is the possibility to apply the extensive research in \nnatural language processing (NLP) to carry out general recognition tasks. \nThis representation puts emphasis on structural aspects, \nyet the most significant quantitative characteristics of sound are kept in the properties of the elements, \nconnections, and accompanying codebooks.\n\n# Structural and Syntactical Pattern Recognition\n\nIn contrast with the standard paradigms of machine learning, \nthe less common \u200bstructured learning approach attempts to use structural information of both the classified objects and \nthe classification domain. An even lesser known research area takes this idea a step further by articulating \nthe structural aspect as a formal grammar that defines rules that derive signal constructs to classification constructs.\nWe propose an approach where both sound and its semantics are expressed in a manner that enables the detection system \nto take advantage of the structural aspects of both.\nThe importance of a structural framework is further supported as we attempt to go beyond detecting isolated \n\u200bsound occurrence\u200b toward interpreting sequences of these occurrences and discovering \u200bsound generating activities\u200b. \nThese sound generating activities can be expressed through ontologies \nand sequential rules composed from other semantical elements.\n \n\nThis approach has been coined as \u201csyntactical pattern recognition\u201d or \u201cgrammar induction\u201d. These techniques have been \nused in Chinese character recognition [\u200b4\u200b], analysis of textures [\u200b10\u200b], medical diagnosis (heart disease detection) [\u200b16\u200b],\n visual scene recognition [\u200b22\u200b], movement recognition in video [\u200b19\u200b], activity monitoring in video [\u200b19\u200b], \n and closer to sound (since uni-dimensional time-series), seismic signal analysis (eg. in oil detection) [\u200b8\u200b] \n and ECG analysis [\u200b18\u200b, \u200b16\u200b].\nAs far as we know, no research has been carried out to apply syntactical pattern recognition techniques \nto general sound recognition, and we intend to take inspiration in this literature in an effort to derive semantics \nfrom sound.\n\n# Semantic Structure\n\nThe elements of the semantic structure will be taken from plain English. \nThese will be words and phrases that are connected to sound events. These elements could describe, for example,\n- particular types of sound (\u200bbark,\u200b\u200b rustle\u200b,\u200b cling, \u200b\u200bbang)\u200b\n- sound sources (\u200bdog, wind\u200b,\u200b thunder,\u200b r\u200bunning\u200b \u200bwater\u200b)\n- sound generating activities that may have a very wide temporal range \u2014 such as \u200bstorm\u200b or cooking.\u200b\n\nThe \u200badvantage\u200b of structured learning \u200blies in its exploitation of the structure of the output space. \nClip-clop, clippety-clop, clop, clopping, clunking\u200b and \u200bclumping\u200b can be considered to be synonyms of each other \nin the context of sound. A \u200bclop\u200b (and its twins) is closer to \u200bknock\u200b and \u200bplunk than it is to \u200bhiss\u200b and \u200bbuzz.\u200b \nYet b\u200buzz\u200b and \u200bknock\u200b, though not similar acoustically, are strongly related to each other through their relation to \u200bdoor\u200b \nand activities surrounding it. If we consider all these sound labels as separate, we would be depriving ourselves \nfrom the valuable information encoded in their interrelationships. \nSemantic structure allows models to avoid problems of synonymy and polysemy, \nbut also allow the emergence of a fuller picture of the sound\u2019s contents \n\u2014 through (formal grammar) derivations such as `rustle + blowing \u2014> wind` and `wind + thunder \u2014> storm`.\n\nAs a first step, these relationships can be mined from NLP tools and APIs such as WordNet and WordsAPI. \nOnce connected to sound however, these relationships should be enhanced according to the acoustic similarities of \nthe sounds the semantic constructs are related to. \nMoreover, in practice semantics are usually grounded in action, \nso the semantic structure should be able to be edited and augmented for the application\u2019s needs.\n\n# Acoustics Structure\n\nOn the acoustic side of SLANG, a similar structured approach should be taken, identifying, symbolizing, \nand interconnecting acoustical units into ever higher combinations, eventually connecting them to the semantic \nidentifiers.\n\nThis process is based on the following steps:\n- Chunk audio streams into very short (and possibly overlapping) frames and compute feature\nvectors of these frames. We will call these frame features.\n- Quantize the frame features, creating a codebook of frame features\n- Enhance the codebook with frame similarity information\n- Use both supervised and unsupervised techniques to carry out pattern detection and annotate code subsequences with these\n- Carry out classification and structured learning techniques to link these patterns to semantic identifiers and structures\n\nThese steps will be detailed in the following sections.\n\n## Alphabetization\n\nAn audio stream is chunked into short (and possibly overlapping) frames over which we compute suitable feature vectors. \nThese features (e.g. spectrogram, chromagram, mel-spectrum, MFCC slices [17]) encode \u201cinstantaneous\u201d \ncharacteristics of sound \u2014 such as intensity and spectral features \u2014 but do not encompass wide-range characteristics \nsuch as autocorrelation and intensity monotonicity. \nThese wide-range characteristics will be captured through combinatorial analysis later on.\nThe frame features are then quantized to a discrete set of symbols [6]. \nVector quantization will map the frame features to a finite number of symbols that will represent all frame features \nwithin a bounded region. These symbols, which we will call \u201csnips\u201d (short for \u201csound nips\u201d) will play the role of our \nsound language alphabet. We record statistical information about the feature space covered by each snip in order to \nqualify and quantify feature-based relationships.\n\n## Snips network\n\nQuantization maps multi-dimensional numerical features to unidimensional nominal ones, thereby seemingly losing all \nsimilarity relationships that the numerical features contain. \nAn approximation of these similarities can be recovered through the numerical feature statistics \nwe associated with each snip, but it would be computationally intensive to have to generate these using \nthe original feature vectors every time we need this information.\n\nInstead, we will use the statistical relationships recorded about the feature space covered by the snips to build a \nnetwork documenting these similarities. \nIn this network we can store information about snips themselves (the nodes) as well as pairs of snips (the links). \nThis serves to keep and be able to readily key into, useful information about the snips and their relationships.\n\nFor example, since sequential patterns of intensity are important in sound recognition, \nwe will store statistics (such as mean and standard deviation) of the intensity of the feature subspace or train data \nframes associated to the each snip. Further, we label the pairs of snips (links of the network) with information about \nthe frames associated to them \u2014 such as various similarity metrics. \nOne notable property to retain is the \u201csnip confusion\u201d metric, \nwhich is the probability that a snip could have been another, \ngiven the arbitrary offset of the initial segmentation into frames.\n\nThe properties stored in the snips network enable us to generate a \u201chalo\u201d around snips where pattern search can operate. \nFurther, enhancing the snip codebook with such information that links back to the original raw data, \nopens the possibility to merge codebooks or translate (at least approximately) one snipping system to another.\n\n\n# Snips Annotations\n\nIn our framework, annotations replace both chunk features and semantic labels. An annotation specifies a segment of \nsound and a property associated to it. Since we now represent sound by a sequence of snips, the segment can be specified \nby a {sound source id, offset snip index, end snip index} triple, and annotations can be grouped or merged to optimize \nindexing, storage and retrieval needs. The property of an annotation can be any data that provides information about \nthe segment.\n\nAnnotations serve a purpose on both sides of the machine learning process: \n- Marking sound segments with acoustical information that may allow models to link sound to meaning. \n- Acquiring precise \u201csupervised data\u201d. Precise because (a) unlike a chunked approach, we can delineate exactly what \npart of the sound we are labeling and (b) we are not limited by single labels, but can express any multi-faceted and \neven structured details about sound segments.\n\nAnnotations may include:\n- **Frequent snip sub-sequences**\u200b: If they are frequent enough, they are important enough to note whether for negative \nor positive inference. The discovery of frequent patterns in sequential data is crucial in Bioinformatics [20]. \nThis can be compared to the use of n-grams and skip-grams in text processing. \nExamples of n-grams applied to sound can be found in [12] and [14].\n- **Frequent snip patterns**: The ability to pinpoint frequent patterns in snip sequences or sets supplies further \npattern mining processes with more \u201csynonymous sets of words\u201d to work with. Snip networks will serve to expand or \nreduce the input snips to find patterns. Compare to NLP information retrieval, \nwhere words of a query are reduced (e.g. stemming [9] and stopword removal) or expanded \n(e.g. related words expansion and edit distance radius [9]).\n- **Pattern-homogeneous sub-sequences**: The distribution of snips of a segment could be considered to belong to a \nsame \u201ctopic\u201d or latent semantic state. See for example the \u201cbag of frames\u201d techniques ([13], [15]), \nwhich cast soundscapes and music information retrieval to a classical NLP term-document approach.\n- **Aggregate features**: Since snips use only short-range features, we seem to have lost the ability to use acoustic \nfeatures that assume significance only over some period of time, but these can be approximated from the snip codebook \nlink to the original frame features statistics and only those with highest significance and utility need to be recorded \n(for example only high autocorrelation).\n- **Semantic annotations**: On the other end of the sound-to-semantics spectrum we can annotate low level semantic \nidentifiers (such as `cling`, `chop` and `splash`) , wide-range segments with words describing a sound-generating \nactivity (such as `cooking`), and context, which is crucial to the proper interpretation of sound events. \nThese annotations are typically generated by a semi-supervised process \u2014 though inferred semantic annotations \ncan be useful to quickly access and validate possible categories of interest.\nWell indexed, this annotation system provides the ability to retrieve audio, \nfeatures or semantic labels from queries expressed as audio, features or semantic labels. \nThis is not only useful as a sound search engine, but gives us all we need to extract acoustic and semantic constructs \nand build models relating these.\n\n\n## Relationship between Annotations and the Syntactic Approach\n\nAnnotated segments provide relationships between acoustical facets of sound semantics through the co-occurrence of \noverlapping annotations in a same segment. Consider all annotations that entirely contain a particular segment. \nAll properties (acoustic and semantic) contained within each annotation are related since they describe \nthe same segment.\n\nAlong with the aforementioned semantic and snips structures, this set of co-occurring properties can be used to \ngenerate a (stochastic formal) grammar over the alphabet of snips and annotations. \nThis grammar provides statistical rules that can be used to derive snips to targeted semantic annotations, \ntherefore linking sound constructs to semantic constructs.\n\nIn order to adequately use annotation overlaps to extract co-occurrence data, \nthese properties should contain information that describe how and when this can be done. \nFor example, a \u201chigh-autocorrelation\u201d property loses its significance if we\u2019re considering only a small portion \nof the annotated segment. Similarly, a different semantic annotation might be more or less stable according to how \nlittle the considered subsequence is \u2014 a 4 second `purr` might still be a purr if we consider 0.5 seconds of it, \nbut a laugh might not be recognisable at that level. \nThis indicates a need for a \u201cannotation calculus\u201d that will specify how we \ncan derive co-occurrence data from annotation overlaps.\n\n# Modeling\nAt this point we have various implicit models that connect acoustic and semantic constructs between themselves. \nAcoustic and semantic constructs become connected to each other through the acoustic-semantic co-occurrence assertions \nprovided by semantic annotations. Models must then supply a computable path between streaming sound and probabilities of \ntargeted semantic constructs.\n\nThe annotation queryable system we propose can at the very least provide a modeler with the means to effectively extract \nwell tuned training and testing data, as well as provide hints as to what acoustical facets are most correlated to the \ntargeted semantic categories, from which any type of model can be applied.\n\nHowever the syntactical approach can be applied here too. We may view the stream of snips as initial symbols \nthat should be combined in such a manner as to derive the targeted semantical symbols. \nIn practice, it is often useful to have a \u201clight\u201d model that efficiently detects only specific categories. \nTo achieve this, we can borrow a page from the speech recognition community and use automatons. \nIndeed, automatons are a suitable choice considering we are given a sequence of symbols, \nmust follow several combinatorial pathways, updated for every new incoming symbol, and when a \u201cterminal\u201d symbol is \nreached this means a detection has been made.\n\n# References\n\nReferences \n\n[1] Aucouturier, J.-J., Defreville, B. and Pachet F.: \u201cThe bag-of-frames approach to audio pattern recognition: \nA sufficient model for urban soundscapes but not for polyphonic music.\u201d \nIn: Journal of the Acoustical Society of America 122.2, pp 881\u2013891 (2007) \t\n[2] Ehsan Amid, Annamaria Mesaros, Kalle Palomaki, Jorma Laaksonen, Mikko Kurimo, \n\u201cUnsupervised feature extraction for multimedia event detection and ranking using audio content\u201d \n- 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) - (2014)\n\n[3] V. Carletti, P. Foggia, G. Percannella, A. Saggese, N. Strisciuglio, and M. Vento, \n\u201cAudio surveillance using a bag of aural words classifier,\u201d Proc. of AVSS, pp. 81\u201386, 2013. \n\n[4] K. S. Fu, \u201cSyntactic Pattern Recognition and Applications.\u201d Prentice Hall, 1982.\n\n[5] R. I. Godoy, \u201cChunking sound for musical analysis\u201d. CMMR 2008, Springer.\n\n[6] R.M. Gray, \u201cVector Quantization,\u201dIEEE ASSP Magazine, Vol. 1, 1984 .\n\n[7] T.Heittola, A.Mesaros, A.Eronen, and T.Virtanen, \u201cContext- dependent sound event detection,\u201d \nEURASIP Journal on Audio, Speech, and Music Processing, 2013.\n\n[8] K.Y. Huang, \u201cSyntactic Pattern Recognition for Seismic Oil Exploration\u201d, \nSeries in Machine Percep. Artificial Intelligence, v. 46.\n\n[9] D. Jurafsky, \u201cSpeech and language processing\u201d, 2nd edition, Prentice Hall, 2008.\n\n[10] B. Julesz, \"Textons, the elements of texture perceptions, and their interactions\", \nNature, vol 290., pp. 91-97, 1981.\n\n[11] WaveNet: \u201cA Generative Model for Raw Audio\u201d, Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, \nOriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, 2016, https://arxiv.org/abs/1609.03499\n\n[12] S. Kim, S. Sundaram, P. Georgiou, and S. Narayanan, \u201cAn N -gram model for unstructured audio signals toward \ninformation retrieval,\u201d in Multimedia Signal Processing, 2010 IEEE International Workshop on, 2010. \n\n[13]  Stephanie Pancoast and Murat Akbacak, \u201cBag-of- Audio-Words Approach for Multimedia Event Classification\u201d.\n\n[14] S. Pancoast and M. Akbacak, \u201cN-gram extension for bag-of-audio-words,\u201d in Proc. of the 38th IEEE International \nConference on Acoustics,Speech and Signal Processing(ICASSP). Vancouver, Canada: IEEE, 2013, pp. 778\u2013782. \n\n[15]  H.Phan, A.Mertins, \u201cExploring superframe co-occurrence for acoustic event recognition,\u201d in Proc. EUSIPCO, 2014, \npp. 631\u2013 635. \n\n[16] Meyer-Baese, Schmid., \u201cPattern Recognition and Signal analysis in Medical Imaging\u201d.\n\n[17] L. Su, C. Yeh, J. Liu, J. Wang, and Y. Yang. \u201cA Systematic Evaluation of the Bag-of-Frames Representation for \nMusic Information Retrieval\u201d, IEEE Transaction on Multimedia, Vol 16, N. 5, 2014.\n\n[18] P. Trahanias, E. Skordalakis, Syntactic Pattern Recognition of the ECG, IEEE transactions on pattern analysis \nand machine intelligence, v. 12, No. 7, 1990.\n\n[19] N.N Vo, A. Bobick, \u201cFrom stochastic grammar to Bayes network: probabilistic parsing of complex activity\u201d, \nCVPR 2014.\n\n[20] J. T. L Wang, M. Zaki and others, \u201cData mining in Bioinformatics\u201d, Springer, 2005.\n\n[21] C. Yu, D. H. Ballard, \u201cOn the integration of grounding language and learning objects\u201d, AAAI, 2004. \n\n[22] S-C. Zhu, D. Mumford, \u201cA stochastic Grammar of Images\u201d, \nFoundations and trends in Computer Vision and Graphics, 2006\n",
            "readme_url": "https://github.com/otosense/slang",
            "frameworks": [
                "scikit-learn"
            ]
        }
    ],
    "references": [
        {
            "title": "WaveNet: A Generative Model for Raw Audio",
            "arxiv": "1609.03499",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.03499v2",
            "abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio\nwaveforms. The model is fully probabilistic and autoregressive, with the\npredictive distribution for each audio sample conditioned on all previous ones;\nnonetheless we show that it can be efficiently trained on data with tens of\nthousands of samples per second of audio. When applied to text-to-speech, it\nyields state-of-the-art performance, with human listeners rating it as\nsignificantly more natural sounding than the best parametric and concatenative\nsystems for both English and Mandarin. A single WaveNet can capture the\ncharacteristics of many different speakers with equal fidelity, and can switch\nbetween them by conditioning on the speaker identity. When trained to model\nmusic, we find that it generates novel and often highly realistic musical\nfragments. We also show that it can be employed as a discriminative model,\nreturning promising results for phoneme recognition.",
            "authors": [
                "Aaron van den Oord",
                "Sander Dieleman",
                "Heiga Zen",
                "Karen Simonyan",
                "Oriol Vinyals",
                "Alex Graves",
                "Nal Kalchbrenner",
                "Andrew Senior",
                "Koray Kavukcuoglu"
            ]
        }
    ],
    "domain": {
        "domain_type": "Unknown"
    }
}