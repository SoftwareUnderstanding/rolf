{
    "visibility": {
        "visibility": "public"
    },
    "name": "EIP Session-5 Assignment",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "vinay-EIP",
                "owner_type": "User",
                "name": "EIP-session-5-assignment",
                "url": "https://github.com/vinay-EIP/EIP-session-5-assignment",
                "stars": 0,
                "pushed_at": "2019-07-10 05:24:49+00:00",
                "created_at": "2019-07-10 00:48:24+00:00",
                "language": "Jupyter Notebook",
                "description": "EIP session 5 assignment",
                "frameworks": [
                    "Keras"
                ]
            },
            {
                "type": "code",
                "name": "EIP_session_5_Assignment.ipynb",
                "sha": "0db88a3b8482dd4f28646da9dbf3697793227dc3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vinay-EIP/EIP-session-5-assignment/blob/master/EIP_session_5_Assignment.ipynb"
                    }
                },
                "size": 1547034
            }
        ]
    },
    "authors": [
        {
            "name": "vinay-EIP",
            "github_id": "vinay-EIP"
        }
    ],
    "tags": [],
    "description": "EIP session 5 assignment",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/vinay-EIP/EIP-session-5-assignment",
            "stars": 0,
            "issues": true,
            "readme": "# EIP Session-5 Assignment\nAssignment:\n\n    1. Find 50 misclassified images from your Session 4 submission model\n    2. Run GradCam (http://www.hackevolve.com/where-cnn-is-looking-grad-cam/) on these images\n    3. Create a gallery of your GradCam results\n    4. Upload your Colab file to a public github repo, and\n    5. Upload your GitHub Link here: https://tinyurl.com/yxt6x2qq (https://tinyurl.com/yxt6x2qq)\n    6. You need to attempt this quiz before the next session starts: https://tinyurl.com/y2t2ux8z (https://tinyurl.com/y2t2ux8z)\n\n## Since session 4 submission was not a proper standard or in some cases where few people have deleted it, ResNet was set as the standard architecture with CIFAR 10 as the stardard dataset.\n\n# About dataset:\n\nThe CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class. CIFAR-10 is a labeled subset of the 80 million tiny images dataset.\n\nSource: https://en.wikipedia.org/wiki/CIFAR-10\n\n# Requirements:\n\n    Keras\n    Classification_models : Github: https://github.com/qubvel/classification_models.git\n    Numpy\n    Matplotlib\n    OpenCV\n    \n# Network architecture: ResNet\n\n### arXiv: https://arxiv.org/abs/1512.03385\n  \n  It is observed that as the networks goes deeper and deeper, during the convergence, the degradation of weights is an inevitable problem. The weights get too small which leads to saturated accuracy.\n  To avoid this problem, skip connections are introduced into the architecture so that instead of just stacking up of layers, the prior reidual mapping is also concatenated with the current mapping so that the architecture is explicitly let to fit a residual mapping.\n  Below is a Residual block used in the ResNet architecture. Here the identity mapping of input X is also added to the output of the convolution block. On doing this in all the convolution blocks, the degradation problem is tackled.\n  \n![image](https://user-images.githubusercontent.com/52725044/60933099-83409d00-a2de-11e9-8fe6-7957f2425ff9.png)\n\nIt is trained on Imagenet and the input shape is configured to 32 x 32 x 3, which is the size of our CIFAR10 dataset\n\n# How Gradcam works:\n\n### Reference: Where CNN is looking? \u2013 Grad CAM (http://www.hackevolve.com/where-cnn-is-looking-grad-cam/)\n\n  Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for \u2018dog\u2019 or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept.\nProcess:\n\n    1. Compute the gradient of the class output value with respect to the feature map\n    2. Pool the gradients over all the axes leaving out the channel dimension\n    3. Weigh the output feature map with the computed gradient values\n    4. Average the weighed feature map along the channel dimension resulting in a heat map of size same as the input image\n    5. Finally normalize the heat map to make the values in between 0 and 1\n\n3 funtions are written which returns the activation map from thier respective layers as below:\n\n    stage1_unit1_relu2 : Initial stage of the network\n    stage1_unit2_relu2 : Layer approximately in the middle of the architecture\n    stage4_unit1_relu1: Deeper stage of the network\n\n",
            "readme_url": "https://github.com/vinay-EIP/EIP-session-5-assignment",
            "frameworks": [
                "Keras"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "Wikipedia"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999997229318481,
        "task": "Object Detection",
        "task_prob": 0.5456956227466914
    }
}