{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "External-Attention-pytorch",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "xmu-xiaoma666",
                "owner_type": "User",
                "name": "External-Attention-pytorch",
                "url": "https://github.com/xmu-xiaoma666/External-Attention-pytorch",
                "stars": 4597,
                "pushed_at": "2022-03-16 02:41:45+00:00",
                "created_at": "2021-05-08 13:11:46+00:00",
                "language": "Python",
                "description": "\ud83c\udf40 Pytorch implementation of various Attention Mechanisms, MLP, Re-parameter, Convolution, which is helpful to further understand papers.\u2b50\u2b50\u2b50",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "FightingCVimg",
                "sha": "0f1455a7640f9bd100a665167cd6718a40658cc9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xmu-xiaoma666/External-Attention-pytorch/tree/master/FightingCVimg"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "9e09fe449aa0866c7e65785bafef88f7977c9505",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xmu-xiaoma666/External-Attention-pytorch/blob/master/LICENSE"
                    }
                },
                "size": 1070
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "127bcb2e03d86b333dc69c6401427c01d697860f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xmu-xiaoma666/External-Attention-pytorch/blob/master/main.py"
                    }
                },
                "size": 306
            },
            {
                "type": "code",
                "name": "model",
                "sha": "78f03aefe6e126bc336f758dd999dcc1e2085a15",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xmu-xiaoma666/External-Attention-pytorch/tree/master/model"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "52fcaf32e256118c0a5e0f4aa4d31707cfb8cdf2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/xmu-xiaoma666/External-Attention-pytorch/blob/master/setup.py"
                    }
                },
                "size": 1771
            }
        ]
    },
    "authors": [
        {
            "name": "xmu-xiaoma666",
            "github_id": "xmu-xiaoma666"
        },
        {
            "name": "Rushirajsinh Parmar",
            "github_id": "rushi-the-neural-arch"
        },
        {
            "name": "epsilon-deltta",
            "email": "kokomong1316@gmail.com",
            "github_id": "epsilon-deltta"
        }
    ],
    "tags": [
        "attention",
        "pytorch",
        "paper",
        "cbam",
        "squeeze",
        "excitation-networks",
        "linear-layers",
        "visual-tasks"
    ],
    "description": "\ud83c\udf40 Pytorch implementation of various Attention Mechanisms, MLP, Re-parameter, Convolution, which is helpful to further understand papers.\u2b50\u2b50\u2b50",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/xmu-xiaoma666/External-Attention-pytorch",
            "stars": 4597,
            "issues": true,
            "readme": "\n<img src=\"./FightingCVimg/LOGO.gif\" height=\"200\" width=\"400\"/>\n\n\n# FightingCV Codebase For [***Attention***](#attention-series),[***Backbone***](#backbone-series), [***MLP***](#mlp-series), [***Re-parameter***](#re-parameter-series), [**Convolution**](#convolution-series)\n\n![](https://img.shields.io/badge/fightingcv-v0.0.1-brightgreen)\n![](https://img.shields.io/badge/python->=v3.0-blue)\n![](https://img.shields.io/badge/pytorch->=v1.4-red)\n\n-------\n*If this project is helpful to you, welcome to give a ***star***.* \n\n*Don't forget to ***follow*** me to learn about project updates.*\n\n-------\n\n- **\ud83d\ude04 \ud83d\ude04 \ud83d\ude04 I am looking for jobs for 2023, my interest is multi-modal pretraining, video-text retrieval, computer vision or other feilds about multi modality!!! Welcome to chat with me by wechat(id:mayiwei1998)**\n\nHello\uff0c\u5927\u5bb6\u597d\uff0c\u6211\u662f\u5c0f\u9a6c\ud83d\ude80\ud83d\ude80\ud83d\ude80\n\n***For \u5c0f\u767d\uff08Like Me\uff09\uff1a***\n\u6700\u8fd1\u5728\u8bfb\u8bba\u6587\u7684\u65f6\u5019\u4f1a\u53d1\u73b0\u4e00\u4e2a\u95ee\u9898\uff0c\u6709\u65f6\u5019\u8bba\u6587\u6838\u5fc3\u601d\u60f3\u975e\u5e38\u7b80\u5355\uff0c\u6838\u5fc3\u4ee3\u7801\u53ef\u80fd\u4e5f\u5c31\u5341\u51e0\u884c\u3002\u4f46\u662f\u6253\u5f00\u4f5c\u8005release\u7684\u6e90\u7801\u65f6\uff0c\u5374\u53d1\u73b0\u63d0\u51fa\u7684\u6a21\u5757\u5d4c\u5165\u5230\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u5206\u5272\u7b49\u4efb\u52a1\u6846\u67b6\u4e2d\uff0c\u5bfc\u81f4\u4ee3\u7801\u6bd4\u8f83\u5197\u4f59\uff0c\u5bf9\u4e8e\u7279\u5b9a\u4efb\u52a1\u6846\u67b6\u4e0d\u719f\u6089\u7684\u6211\uff0c**\u5f88\u96be\u627e\u5230\u6838\u5fc3\u4ee3\u7801**\uff0c\u5bfc\u81f4\u5728\u8bba\u6587\u548c\u7f51\u7edc\u601d\u60f3\u7684\u7406\u89e3\u4e0a\u4f1a\u6709\u4e00\u5b9a\u56f0\u96be\u3002\n\n***For \u8fdb\u9636\u8005\uff08Like You\uff09\uff1a***\n\u5982\u679c\u628aConv\u3001FC\u3001RNN\u8fd9\u4e9b\u57fa\u672c\u5355\u5143\u770b\u505a\u5c0f\u7684Lego\u79ef\u6728\uff0c\u628aTransformer\u3001ResNet\u8fd9\u4e9b\u7ed3\u6784\u770b\u6210\u5df2\u7ecf\u642d\u597d\u7684Lego\u57ce\u5821\u3002\u90a3\u4e48\u672c\u9879\u76ee\u63d0\u4f9b\u7684\u6a21\u5757\u5c31\u662f\u4e00\u4e2a\u4e2a\u5177\u6709\u5b8c\u6574\u8bed\u4e49\u4fe1\u606f\u7684Lego\u7ec4\u4ef6\u3002**\u8ba9\u79d1\u7814\u5de5\u4f5c\u8005\u4eec\u907f\u514d\u53cd\u590d\u9020\u8f6e\u5b50**\uff0c\u53ea\u9700\u601d\u8003\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u201cLego\u7ec4\u4ef6\u201d\uff0c\u642d\u5efa\u51fa\u66f4\u591a\u7eda\u70c2\u591a\u5f69\u7684\u4f5c\u54c1\u3002\n\n***For \u5927\u795e\uff08May Be Like You\uff09\uff1a***\n\u80fd\u529b\u6709\u9650\uff0c**\u4e0d\u559c\u8f7b\u55b7**\uff01\uff01\uff01\n\n***For All\uff1a***\n\u672c\u9879\u76ee\u5c31\u662f\u8981\u5b9e\u73b0\u4e00\u4e2a\u65e2\u80fd**\u8ba9\u6df1\u5ea6\u5b66\u4e60\u5c0f\u767d\u4e5f\u80fd\u641e\u61c2**\uff0c\u53c8\u80fd**\u670d\u52a1\u79d1\u7814\u548c\u5de5\u4e1a\u793e\u533a**\u7684\u4ee3\u7801\u5e93\u3002\u4f5c\u4e3a[\u3010\u8bba\u6587\u89e3\u6790\u9879\u76ee\u3011](https://github.com/xmu-xiaoma666/FightingCV-Paper-Reading)\u7684\u8865\u5145\uff0c\u672c\u9879\u76ee\u7684\u5b97\u65e8\u662f\u4ece\u4ee3\u7801\u89d2\u5ea6\uff0c\u5b9e\u73b0\ud83d\ude80**\u8ba9\u4e16\u754c\u4e0a\u6ca1\u6709\u96be\u8bfb\u7684\u8bba\u6587**\ud83d\ude80\u3002\n\n\uff08\u540c\u65f6\u4e5f\u975e\u5e38\u6b22\u8fce\u5404\u4f4d\u79d1\u7814\u5de5\u4f5c\u8005\u5c06\u81ea\u5df1\u7684\u5de5\u4f5c\u7684\u6838\u5fc3\u4ee3\u7801\u6574\u7406\u5230\u672c\u9879\u76ee\u4e2d\uff0c\u63a8\u52a8\u79d1\u7814\u793e\u533a\u7684\u53d1\u5c55\uff0c\u4f1a\u5728readme\u4e2d\u6ce8\u660e\u4ee3\u7801\u7684\u4f5c\u8005~\uff09\n\n\n\n## \u516c\u4f17\u53f7 & \u5fae\u4fe1\u4ea4\u6d41\u7fa4\n\n\u6b22\u8fce\u5927\u5bb6\u5173\u6ce8\u516c\u4f17\u53f7\uff1a**FightingCV**\n\n\u516c\u4f17\u53f7**\u6bcf\u5929**\u90fd\u4f1a\u8fdb\u884c**\u8bba\u6587\u3001\u7b97\u6cd5\u548c\u4ee3\u7801\u7684\u5e72\u8d27\u5206\u4eab**\u54e6~\n\n\n\u5df2\u5efa\u7acb**\u673a\u5668\u5b66\u4e60/\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5/\u8ba1\u7b97\u673a\u89c6\u89c9/\u591a\u6a21\u6001\u4ea4\u6d41\u7fa4**\u5fae\u4fe1\u4ea4\u6d41\u7fa4\uff01\n\n\uff08\u52a0\u4e0d\u8fdb\u53bb\u53ef\u4ee5\u52a0\u5fae\u4fe1\uff1a**775629340**\uff0c\u8bb0\u5f97\u5907\u6ce8\u3010**\u516c\u53f8/\u5b66\u6821+\u65b9\u5411+ID**\u3011\uff09\n\n**\u6bcf\u5929\u5728\u7fa4\u91cc\u5206\u4eab\u4e00\u4e9b\u8fd1\u671f\u7684\u8bba\u6587\u548c\u89e3\u6790**\uff0c\u6b22\u8fce\u5927\u5bb6\u4e00\u8d77**\u5b66\u4e60\u4ea4\u6d41**\u54c8~~~\n\n\n![](./FightingCVimg/wechat.jpg)\n\n\u5f3a\u70c8\u63a8\u8350\u5927\u5bb6\u5173\u6ce8[**\u77e5\u4e4e**](https://www.zhihu.com/people/jason-14-58-38/posts)\u8d26\u53f7\u548c[**FightingCV\u516c\u4f17\u53f7**](https://mp.weixin.qq.com/s/sgNw6XFBPcD20Ef3ddfE1w)\uff0c\u53ef\u4ee5\u5feb\u901f\u4e86\u89e3\u5230\u6700\u65b0\u4f18\u8d28\u7684\u5e72\u8d27\u8d44\u6e90\u3002\n\n\n\n\n***\n\n# Contents\n\n- [Attention Series](#attention-series)\n    - [1. External Attention Usage](#1-external-attention-usage)\n\n    - [2. Self Attention Usage](#2-self-attention-usage)\n\n    - [3. Simplified Self Attention Usage](#3-simplified-self-attention-usage)\n\n    - [4. Squeeze-and-Excitation Attention Usage](#4-squeeze-and-excitation-attention-usage)\n\n    - [5. SK Attention Usage](#5-sk-attention-usage)\n\n    - [6. CBAM Attention Usage](#6-cbam-attention-usage)\n\n    - [7. BAM Attention Usage](#7-bam-attention-usage)\n    \n    - [8. ECA Attention Usage](#8-eca-attention-usage)\n\n    - [9. DANet Attention Usage](#9-danet-attention-usage)\n\n    - [10. Pyramid Split Attention (PSA) Usage](#10-Pyramid-Split-Attention-Usage)\n\n    - [11. Efficient Multi-Head Self-Attention(EMSA) Usage](#11-Efficient-Multi-Head-Self-Attention-Usage)\n\n    - [12. Shuffle Attention Usage](#12-Shuffle-Attention-Usage)\n    \n    - [13. MUSE Attention Usage](#13-MUSE-Attention-Usage)\n  \n    - [14. SGE Attention Usage](#14-SGE-Attention-Usage)\n\n    - [15. A2 Attention Usage](#15-A2-Attention-Usage)\n\n    - [16. AFT Attention Usage](#16-AFT-Attention-Usage)\n\n    - [17. Outlook Attention Usage](#17-Outlook-Attention-Usage)\n\n    - [18. ViP Attention Usage](#18-ViP-Attention-Usage)\n\n    - [19. CoAtNet Attention Usage](#19-CoAtNet-Attention-Usage)\n\n    - [20. HaloNet Attention Usage](#20-HaloNet-Attention-Usage)\n\n    - [21. Polarized Self-Attention Usage](#21-Polarized-Self-Attention-Usage)\n\n    - [22. CoTAttention Usage](#22-CoTAttention-Usage)\n\n    - [23. Residual Attention Usage](#23-Residual-Attention-Usage)\n  \n    - [24. S2 Attention Usage](#24-S2-Attention-Usage)\n\n    - [25. GFNet Attention Usage](#25-GFNet-Attention-Usage)\n\n    - [26. Triplet Attention Usage](#26-TripletAttention-Usage)\n\n    - [27. Coordinate Attention Usage](#27-Coordinate-Attention-Usage)\n\n    - [28. MobileViT Attention Usage](#28-MobileViT-Attention-Usage)\n\n    - [29. ParNet Attention Usage](#29-ParNet-Attention-Usage)\n\n    - [30. UFO Attention Usage](#30-UFO-Attention-Usage)\n  \n\n- [Backbone Series](#Backbone-series)\n\n    - [1. ResNet Usage](#1-ResNet-Usage)\n\n    - [2. ResNeXt Usage](#2-ResNeXt-Usage)\n\n    - [3. MobileViT Usage](#3-MobileViT-Usage)\n\n    - [4. ConvMixer Usage](#4-ConvMixer-Usage)\n\n\n- [MLP Series](#mlp-series)\n\n    - [1. RepMLP Usage](#1-RepMLP-Usage)\n\n    - [2. MLP-Mixer Usage](#2-MLP-Mixer-Usage)\n\n    - [3. ResMLP Usage](#3-ResMLP-Usage)\n\n    - [4. gMLP Usage](#4-gMLP-Usage)\n\n    - [5. sMLP Usage](#5-sMLP-Usage)\n\n- [Re-Parameter(ReP) Series](#Re-Parameter-series)\n\n    - [1. RepVGG Usage](#1-RepVGG-Usage)\n\n    - [2. ACNet Usage](#2-ACNet-Usage)\n\n    - [3. Diverse Branch Block(DDB) Usage](#3-Diverse-Branch-Block-Usage)\n\n- [Convolution Series](#Convolution-series)\n\n    - [1. Depthwise Separable Convolution Usage](#1-Depthwise-Separable-Convolution-Usage)\n\n    - [2. MBConv Usage](#2-MBConv-Usage)\n\n    - [3. Involution Usage](#3-Involution-Usage)\n\n    - [4. DynamicConv Usage](#4-DynamicConv-Usage)\n\n    - [5. CondConv Usage](#5-CondConv-Usage)\n\n***\n\n\n# Attention Series\n\n- Pytorch implementation of [\"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks---arXiv 2021.05.05\"](https://arxiv.org/abs/2105.02358)\n\n- Pytorch implementation of [\"Attention Is All You Need---NIPS2017\"](https://arxiv.org/pdf/1706.03762.pdf)\n\n- Pytorch implementation of [\"Squeeze-and-Excitation Networks---CVPR2018\"](https://arxiv.org/abs/1709.01507)\n\n- Pytorch implementation of [\"Selective Kernel Networks---CVPR2019\"](https://arxiv.org/pdf/1903.06586.pdf)\n\n- Pytorch implementation of [\"CBAM: Convolutional Block Attention Module---ECCV2018\"](https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf)\n\n- Pytorch implementation of [\"BAM: Bottleneck Attention Module---BMCV2018\"](https://arxiv.org/pdf/1807.06514.pdf)\n\n- Pytorch implementation of [\"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks---CVPR2020\"](https://arxiv.org/pdf/1910.03151.pdf)\n\n- Pytorch implementation of [\"Dual Attention Network for Scene Segmentation---CVPR2019\"](https://arxiv.org/pdf/1809.02983.pdf)\n\n- Pytorch implementation of [\"EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network---arXiv 2021.05.30\"](https://arxiv.org/pdf/2105.14447.pdf)\n\n- Pytorch implementation of [\"ResT: An Efficient Transformer for Visual Recognition---arXiv 2021.05.28\"](https://arxiv.org/abs/2105.13677)\n\n- Pytorch implementation of [\"SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS---ICASSP 2021\"](https://arxiv.org/pdf/2102.00240.pdf)\n\n- Pytorch implementation of [\"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning---arXiv 2019.11.17\"](https://arxiv.org/abs/1911.09483)\n\n- Pytorch implementation of [\"Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks---arXiv 2019.05.23\"](https://arxiv.org/pdf/1905.09646.pdf)\n\n- Pytorch implementation of [\"A2-Nets: Double Attention Networks---NIPS2018\"](https://arxiv.org/pdf/1810.11579.pdf)\n\n\n- Pytorch implementation of [\"An Attention Free Transformer---ICLR2021 (Apple New Work)\"](https://arxiv.org/pdf/2105.14103v1.pdf)\n\n\n- Pytorch implementation of [VOLO: Vision Outlooker for Visual Recognition---arXiv 2021.06.24\"](https://arxiv.org/abs/2106.13112) \n  [\u3010\u8bba\u6587\u89e3\u6790\u3011](https://zhuanlan.zhihu.com/p/385561050)\n\n\n- Pytorch implementation of [Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition---arXiv 2021.06.23](https://arxiv.org/abs/2106.12368) \n  [\u3010\u8bba\u6587\u89e3\u6790\u3011](https://mp.weixin.qq.com/s/5gonUQgBho_m2O54jyXF_Q)\n\n\n- Pytorch implementation of [CoAtNet: Marrying Convolution and Attention for All Data Sizes---arXiv 2021.06.09](https://arxiv.org/abs/2106.04803) \n  [\u3010\u8bba\u6587\u89e3\u6790\u3011](https://zhuanlan.zhihu.com/p/385578588)\n\n\n- Pytorch implementation of [Scaling Local Self-Attention for Parameter Efficient Visual Backbones---CVPR2021 Oral](https://arxiv.org/pdf/2103.12731.pdf)  [\u3010\u8bba\u6587\u89e3\u6790\u3011](https://zhuanlan.zhihu.com/p/388598744)\n\n\n\n- Pytorch implementation of [Polarized Self-Attention: Towards High-quality Pixel-wise Regression---arXiv 2021.07.02](https://arxiv.org/abs/2107.00782)  [\u3010\u8bba\u6587\u89e3\u6790\u3011](https://zhuanlan.zhihu.com/p/389770482) \n\n\n- Pytorch implementation of [Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26](https://arxiv.org/abs/2107.12292)  [\u3010\u8bba\u6587\u89e3\u6790\u3011](https://zhuanlan.zhihu.com/p/394795481) \n\n\n- Pytorch implementation of [Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021](https://arxiv.org/abs/2108.02456) \n\n\n- Pytorch implementation of [S\u00b2-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02](https://arxiv.org/abs/2108.01072) [\u3010\u8bba\u6587\u89e3\u6790\u3011](https://zhuanlan.zhihu.com/p/397003638) \n\n- Pytorch implementation of [Global Filter Networks for Image Classification---arXiv 2021.07.01](https://arxiv.org/abs/2107.00645) \n\n- Pytorch implementation of [Rotate to Attend: Convolutional Triplet Attention Module---WACV 2021](https://arxiv.org/abs/2010.03045) \n\n- Pytorch implementation of [Coordinate Attention for Efficient Mobile Network Design ---CVPR 2021](https://arxiv.org/abs/2103.02907)\n\n- Pytorch implementation of [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05](https://arxiv.org/abs/2103.02907)\n\n- Pytorch implementation of [Non-deep Networks---ArXiv 2021.10.20](https://arxiv.org/abs/2110.07641)\n\n- Pytorch implementation of [UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29](https://arxiv.org/abs/2109.14382)\n\n***\n\n### 1. External Attention Usage\n#### 1.1. Paper\n[\"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks\"](https://arxiv.org/abs/2105.02358)\n\n#### 1.2. Overview\n![](./model/img/External_Attention.png)\n\n#### 1.3. Usage Code\n```python\nfrom model.attention.ExternalAttention import ExternalAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nea = ExternalAttention(d_model=512,S=8)\noutput=ea(input)\nprint(output.shape)\n```\n\n***\n\n\n### 2. Self Attention Usage\n#### 2.1. Paper\n[\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf)\n\n#### 1.2. Overview\n![](./model/img/SA.png)\n\n#### 1.3. Usage Code\n```python\nfrom model.attention.SelfAttention import ScaledDotProductAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nsa = ScaledDotProductAttention(d_model=512, d_k=512, d_v=512, h=8)\noutput=sa(input,input,input)\nprint(output.shape)\n```\n\n***\n\n### 3. Simplified Self Attention Usage\n#### 3.1. Paper\n[None]()\n\n#### 3.2. Overview\n![](./model/img/SSA.png)\n\n#### 3.3. Usage Code\n```python\nfrom model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)\noutput=ssa(input,input,input)\nprint(output.shape)\n\n```\n\n***\n\n### 4. Squeeze-and-Excitation Attention Usage\n#### 4.1. Paper\n[\"Squeeze-and-Excitation Networks\"](https://arxiv.org/abs/1709.01507)\n\n#### 4.2. Overview\n![](./model/img/SE.png)\n\n#### 4.3. Usage Code\n```python\nfrom model.attention.SEAttention import SEAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\nse = SEAttention(channel=512,reduction=8)\noutput=se(input)\nprint(output.shape)\n\n```\n\n***\n\n### 5. SK Attention Usage\n#### 5.1. Paper\n[\"Selective Kernel Networks\"](https://arxiv.org/pdf/1903.06586.pdf)\n\n#### 5.2. Overview\n![](./model/img/SK.png)\n\n#### 5.3. Usage Code\n```python\nfrom model.attention.SKAttention import SKAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\nse = SKAttention(channel=512,reduction=8)\noutput=se(input)\nprint(output.shape)\n\n```\n***\n\n### 6. CBAM Attention Usage\n#### 6.1. Paper\n[\"CBAM: Convolutional Block Attention Module\"](https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf)\n\n#### 6.2. Overview\n![](./model/img/CBAM1.png)\n\n![](./model/img/CBAM2.png)\n\n#### 6.3. Usage Code\n```python\nfrom model.attention.CBAM import CBAMBlock\nimport torch\n\ninput=torch.randn(50,512,7,7)\nkernel_size=input.shape[2]\ncbam = CBAMBlock(channel=512,reduction=16,kernel_size=kernel_size)\noutput=cbam(input)\nprint(output.shape)\n\n```\n\n***\n\n### 7. BAM Attention Usage\n#### 7.1. Paper\n[\"BAM: Bottleneck Attention Module\"](https://arxiv.org/pdf/1807.06514.pdf)\n\n#### 7.2. Overview\n![](./model/img/BAM.png)\n\n#### 7.3. Usage Code\n```python\nfrom model.attention.BAM import BAMBlock\nimport torch\n\ninput=torch.randn(50,512,7,7)\nbam = BAMBlock(channel=512,reduction=16,dia_val=2)\noutput=bam(input)\nprint(output.shape)\n\n```\n\n***\n\n### 8. ECA Attention Usage\n#### 8.1. Paper\n[\"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks\"](https://arxiv.org/pdf/1910.03151.pdf)\n\n#### 8.2. Overview\n![](./model/img/ECA.png)\n\n#### 8.3. Usage Code\n```python\nfrom model.attention.ECAAttention import ECAAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\neca = ECAAttention(kernel_size=3)\noutput=eca(input)\nprint(output.shape)\n\n```\n\n***\n\n### 9. DANet Attention Usage\n#### 9.1. Paper\n[\"Dual Attention Network for Scene Segmentation\"](https://arxiv.org/pdf/1809.02983.pdf)\n\n#### 9.2. Overview\n![](./model/img/danet.png)\n\n#### 9.3. Usage Code\n```python\nfrom model.attention.DANet import DAModule\nimport torch\n\ninput=torch.randn(50,512,7,7)\ndanet=DAModule(d_model=512,kernel_size=3,H=7,W=7)\nprint(danet(input).shape)\n\n```\n\n***\n\n### 10. Pyramid Split Attention Usage\n\n#### 10.1. Paper\n[\"EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network\"](https://arxiv.org/pdf/2105.14447.pdf)\n\n#### 10.2. Overview\n![](./model/img/psa.png)\n\n#### 10.3. Usage Code\n```python\nfrom model.attention.PSA import PSA\nimport torch\n\ninput=torch.randn(50,512,7,7)\npsa = PSA(channel=512,reduction=8)\noutput=psa(input)\nprint(output.shape)\n\n```\n\n***\n\n\n### 11. Efficient Multi-Head Self-Attention Usage\n\n#### 11.1. Paper\n[\"ResT: An Efficient Transformer for Visual Recognition\"](https://arxiv.org/abs/2105.13677)\n\n#### 11.2. Overview\n![](./model/img/EMSA.png)\n\n#### 11.3. Usage Code\n```python\n\nfrom model.attention.EMSA import EMSA\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,64,512)\nemsa = EMSA(d_model=512, d_k=512, d_v=512, h=8,H=8,W=8,ratio=2,apply_transform=True)\noutput=emsa(input,input,input)\nprint(output.shape)\n    \n```\n\n***\n\n\n### 12. Shuffle Attention Usage\n\n#### 12.1. Paper\n[\"SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS\"](https://arxiv.org/pdf/2102.00240.pdf)\n\n#### 12.2. Overview\n![](./model/img/ShuffleAttention.jpg)\n\n#### 12.3. Usage Code\n```python\n\nfrom model.attention.ShuffleAttention import ShuffleAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ninput=torch.randn(50,512,7,7)\nse = ShuffleAttention(channel=512,G=8)\noutput=se(input)\nprint(output.shape)\n\n    \n```\n\n\n***\n\n\n### 13. MUSE Attention Usage\n\n#### 13.1. Paper\n[\"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning\"](https://arxiv.org/abs/1911.09483)\n\n#### 13.2. Overview\n![](./model/img/MUSE.png)\n\n#### 13.3. Usage Code\n```python\nfrom model.attention.MUSEAttention import MUSEAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ninput=torch.randn(50,49,512)\nsa = MUSEAttention(d_model=512, d_k=512, d_v=512, h=8)\noutput=sa(input,input,input)\nprint(output.shape)\n\n```\n\n***\n\n\n### 14. SGE Attention Usage\n\n#### 14.1. Paper\n[Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks](https://arxiv.org/pdf/1905.09646.pdf)\n\n#### 14.2. Overview\n![](./model/img/SGE.png)\n\n#### 14.3. Usage Code\n```python\nfrom model.attention.SGE import SpatialGroupEnhance\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\nsge = SpatialGroupEnhance(groups=8)\noutput=sge(input)\nprint(output.shape)\n\n```\n\n***\n\n\n### 15. A2 Attention Usage\n\n#### 15.1. Paper\n[A2-Nets: Double Attention Networks](https://arxiv.org/pdf/1810.11579.pdf)\n\n#### 15.2. Overview\n![](./model/img/A2.png)\n\n#### 15.3. Usage Code\n```python\nfrom model.attention.A2Atttention import DoubleAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\na2 = DoubleAttention(512,128,128,True)\noutput=a2(input)\nprint(output.shape)\n\n```\n\n\n\n### 16. AFT Attention Usage\n\n#### 16.1. Paper\n[An Attention Free Transformer](https://arxiv.org/pdf/2105.14103v1.pdf)\n\n#### 16.2. Overview\n![](./model/img/AFT.jpg)\n\n#### 16.3. Usage Code\n```python\nfrom model.attention.AFT import AFT_FULL\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,49,512)\naft_full = AFT_FULL(d_model=512, n=49)\noutput=aft_full(input)\nprint(output.shape)\n\n```\n\n\n\n\n\n\n### 17. Outlook Attention Usage\n\n#### 17.1. Paper\n\n\n[VOLO: Vision Outlooker for Visual Recognition\"](https://arxiv.org/abs/2106.13112)\n\n\n#### 17.2. Overview\n![](./model/img/OutlookAttention.png)\n\n#### 17.3. Usage Code\n```python\nfrom model.attention.OutlookAttention import OutlookAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,28,28,512)\noutlook = OutlookAttention(dim=512)\noutput=outlook(input)\nprint(output.shape)\n\n```\n\n\n***\n\n\n\n\n\n\n### 18. ViP Attention Usage\n\n#### 18.1. Paper\n\n\n[Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition\"](https://arxiv.org/abs/2106.12368)\n\n\n#### 18.2. Overview\n![](./model/img/ViP.png)\n\n#### 18.3. Usage Code\n```python\n\nfrom model.attention.ViP import WeightedPermuteMLP\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(64,8,8,512)\nseg_dim=8\nvip=WeightedPermuteMLP(512,seg_dim)\nout=vip(input)\nprint(out.shape)\n\n```\n\n\n***\n\n\n\n\n\n### 19. CoAtNet Attention Usage\n\n#### 19.1. Paper\n\n\n[CoAtNet: Marrying Convolution and Attention for All Data Sizes\"](https://arxiv.org/abs/2106.04803) \n\n\n#### 19.2. Overview\nNone\n\n\n#### 19.3. Usage Code\n```python\n\nfrom model.attention.CoAtNet import CoAtNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\nmbconv=CoAtNet(in_ch=3,image_size=224)\nout=mbconv(input)\nprint(out.shape)\n\n```\n\n\n***\n\n\n\n\n\n\n### 20. HaloNet Attention Usage\n\n#### 20.1. Paper\n\n\n[Scaling Local Self-Attention for Parameter Efficient Visual Backbones\"](https://arxiv.org/pdf/2103.12731.pdf) \n\n\n#### 20.2. Overview\n\n![](./model/img/HaloNet.png)\n\n#### 20.3. Usage Code\n```python\n\nfrom model.attention.HaloAttention import HaloAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,512,8,8)\nhalo = HaloAttention(dim=512,\n    block_size=2,\n    halo_size=1,)\noutput=halo(input)\nprint(output.shape)\n\n```\n\n\n***\n\n### 21. Polarized Self-Attention Usage\n\n#### 21.1. Paper\n\n[Polarized Self-Attention: Towards High-quality Pixel-wise Regression\"](https://arxiv.org/abs/2107.00782)  \n\n\n#### 21.2. Overview\n\n![](./model/img/PoSA.png)\n\n#### 21.3. Usage Code\n```python\n\nfrom model.attention.PolarizedSelfAttention import ParallelPolarizedSelfAttention,SequentialPolarizedSelfAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,512,7,7)\npsa = SequentialPolarizedSelfAttention(channel=512)\noutput=psa(input)\nprint(output.shape)\n\n\n```\n\n\n***\n\n\n### 22. CoTAttention Usage\n\n#### 22.1. Paper\n\n[Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26](https://arxiv.org/abs/2107.12292) \n\n\n#### 22.2. Overview\n\n![](./model/img/CoT.png)\n\n#### 22.3. Usage Code\n```python\n\nfrom model.attention.CoTAttention import CoTAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\ncot = CoTAttention(dim=512,kernel_size=3)\noutput=cot(input)\nprint(output.shape)\n\n\n\n```\n\n***\n\n\n### 23. Residual Attention Usage\n\n#### 23.1. Paper\n\n[Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021](https://arxiv.org/abs/2108.02456) \n\n\n#### 23.2. Overview\n\n![](./model/img/ResAtt.png)\n\n#### 23.3. Usage Code\n```python\n\nfrom model.attention.ResidualAttention import ResidualAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\nresatt = ResidualAttention(channel=512,num_class=1000,la=0.2)\noutput=resatt(input)\nprint(output.shape)\n\n\n\n```\n\n***\n\n\n\n### 24. S2 Attention Usage\n\n#### 24.1. Paper\n\n[S\u00b2-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02](https://arxiv.org/abs/2108.01072) \n\n\n#### 24.2. Overview\n\n![](./model/img/S2Attention.png)\n\n#### 24.3. Usage Code\n```python\nfrom model.attention.S2Attention import S2Attention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\ns2att = S2Attention(channels=512)\noutput=s2att(input)\nprint(output.shape)\n\n```\n\n***\n\n\n\n### 25. GFNet Attention Usage\n\n#### 25.1. Paper\n\n[Global Filter Networks for Image Classification---arXiv 2021.07.01](https://arxiv.org/abs/2107.00645) \n\n\n#### 25.2. Overview\n\n![](./model/img/GFNet.jpg)\n\n#### 25.3. Usage Code - Implemented by [Wenliang Zhao (Author)](https://scholar.google.com/citations?user=lyPWvuEAAAAJ&hl=en)\n\n```python\nfrom model.attention.gfnet import GFNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nx = torch.randn(1, 3, 224, 224)\ngfnet = GFNet(embed_dim=384, img_size=224, patch_size=16, num_classes=1000)\nout = gfnet(x)\nprint(out.shape)\n\n```\n\n***\n\n\n### 26. TripletAttention Usage\n\n#### 26.1. Paper\n\n[Rotate to Attend: Convolutional Triplet Attention Module---CVPR 2021](https://arxiv.org/abs/2010.03045) \n\n#### 26.2. Overview\n\n![](./model/img/triplet.png)\n\n#### 26.3. Usage Code - Implemented by [digantamisra98](https://github.com/digantamisra98)\n\n```python\nfrom model.attention.TripletAttention import TripletAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\ninput=torch.randn(50,512,7,7)\ntriplet = TripletAttention()\noutput=triplet(input)\nprint(output.shape)\n```\n\n\n***\n\n\n### 27. Coordinate Attention Usage\n\n#### 27.1. Paper\n\n[Coordinate Attention for Efficient Mobile Network Design---CVPR 2021](https://arxiv.org/abs/2103.02907)\n\n\n#### 27.2. Overview\n\n![](./model/img/CoordAttention.png)\n\n#### 27.3. Usage Code - Implemented by [Andrew-Qibin](https://github.com/Andrew-Qibin)\n\n```python\nfrom model.attention.CoordAttention import CoordAtt\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninp=torch.rand([2, 96, 56, 56])\ninp_dim, oup_dim = 96, 96\nreduction=32\n\ncoord_attention = CoordAtt(inp_dim, oup_dim, reduction=reduction)\noutput=coord_attention(inp)\nprint(output.shape)\n```\n\n***\n\n\n### 28. MobileViT Attention Usage\n\n#### 28.1. Paper\n\n[MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05](https://arxiv.org/abs/2103.02907)\n\n\n#### 28.2. Overview\n\n![](./model/img/MobileViTAttention.png)\n\n#### 28.3. Usage Code\n\n```python\nfrom model.attention.MobileViTAttention import MobileViTAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    m=MobileViTAttention()\n    input=torch.randn(1,3,49,49)\n    output=m(input)\n    print(output.shape)  #output:(1,3,49,49)\n    \n```\n\n***\n\n\n### 29. ParNet Attention Usage\n\n#### 29.1. Paper\n\n[Non-deep Networks---ArXiv 2021.10.20](https://arxiv.org/abs/2110.07641)\n\n\n#### 29.2. Overview\n\n![](./model/img/ParNet.png)\n\n#### 29.3. Usage Code\n\n```python\nfrom model.attention.ParNetAttention import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,512,7,7)\n    pna = ParNetAttention(channel=512)\n    output=pna(input)\n    print(output.shape) #50,512,7,7\n    \n```\n\n***\n\n\n### 30. UFO Attention Usage\n\n#### 30.1. Paper\n\n[UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29](https://arxiv.org/abs/2110.07641)\n\n\n#### 30.2. Overview\n\n![](./model/img/UFO.png)\n\n#### 30.3. Usage Code\n\n```python\nfrom model.attention.UFOAttention import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    ufo = UFOAttention(d_model=512, d_k=512, d_v=512, h=8)\n    output=ufo(input,input,input)\n    print(output.shape) #[50, 49, 512]\n    \n```\n\n***\n\n\n# Backbone Series\n\n- Pytorch implementation of [\"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper\"](https://arxiv.org/pdf/1512.03385.pdf)\n\n- Pytorch implementation of [\"Aggregated Residual Transformations for Deep Neural Networks---CVPR2017\"](https://arxiv.org/abs/1611.05431v2)\n\n- Pytorch implementation of [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05](https://arxiv.org/abs/2103.02907)\n\n- Pytorch implementation of [Patches Are All You Need?---ICLR2022 (Under Review)](https://openreview.net/forum?id=TVHS5Y4dNvM)\n\n\n### 1. ResNet Usage\n#### 1.1. Paper\n[\"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper\"](https://arxiv.org/pdf/1512.03385.pdf)\n\n#### 1.2. Overview\n![](./model/img/resnet.png)\n![](./model/img/resnet2.jpg)\n\n#### 1.3. Usage Code\n```python\n\nfrom model.backbone.resnet import ResNet50,ResNet101,ResNet152\nimport torch\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    resnet50=ResNet50(1000)\n    # resnet101=ResNet101(1000)\n    # resnet152=ResNet152(1000)\n    out=resnet50(input)\n    print(out.shape)\n\n```\n\n\n### 2. ResNeXt Usage\n#### 2.1. Paper\n\n[\"Aggregated Residual Transformations for Deep Neural Networks---CVPR2017\"](https://arxiv.org/abs/1611.05431v2)\n\n#### 2.2. Overview\n![](./model/img/resnext.png)\n\n#### 2.3. Usage Code\n```python\n\nfrom model.backbone.resnext import ResNeXt50,ResNeXt101,ResNeXt152\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    resnext50=ResNeXt50(1000)\n    # resnext101=ResNeXt101(1000)\n    # resnext152=ResNeXt152(1000)\n    out=resnext50(input)\n    print(out.shape)\n\n\n```\n\n\n\n### 3. MobileViT Usage\n#### 3.1. Paper\n\n[MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05](https://arxiv.org/abs/2103.02907)\n\n#### 3.2. Overview\n![](./model/img/mobileViT.jpg)\n\n#### 3.3. Usage Code\n```python\n\nfrom model.backbone.MobileViT import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n\n    ### mobilevit_xxs\n    mvit_xxs=mobilevit_xxs()\n    out=mvit_xxs(input)\n    print(out.shape)\n\n    ### mobilevit_xs\n    mvit_xs=mobilevit_xs()\n    out=mvit_xs(input)\n    print(out.shape)\n\n\n    ### mobilevit_s\n    mvit_s=mobilevit_s()\n    out=mvit_s(input)\n    print(out.shape)\n\n```\n\n\n\n\n\n### 4. ConvMixer Usage\n#### 4.1. Paper\n[Patches Are All You Need?---ICLR2022 (Under Review)](https://openreview.net/forum?id=TVHS5Y4dNvM)\n#### 4.2. Overview\n![](./model/img/ConvMixer.png)\n\n#### 4.3. Usage Code\n```python\n\nfrom model.backbone.ConvMixer import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    x=torch.randn(1,3,224,224)\n    convmixer=ConvMixer(dim=512,depth=12)\n    out=convmixer(x)\n    print(out.shape)  #[1, 1000]\n\n\n```\n\n\n\n\n\n\n\n# MLP Series\n\n- Pytorch implementation of [\"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition---arXiv 2021.05.05\"](https://arxiv.org/pdf/2105.01883v1.pdf)\n\n- Pytorch implementation of [\"MLP-Mixer: An all-MLP Architecture for Vision---arXiv 2021.05.17\"](https://arxiv.org/pdf/2105.01601.pdf)\n\n- Pytorch implementation of [\"ResMLP: Feedforward networks for image classification with data-efficient training---arXiv 2021.05.07\"](https://arxiv.org/pdf/2105.03404.pdf)\n\n- Pytorch implementation of [\"Pay Attention to MLPs---arXiv 2021.05.17\"](https://arxiv.org/abs/2105.08050)\n\n\n- Pytorch implementation of [\"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?---arXiv 2021.09.12\"](https://arxiv.org/abs/2109.05422)\n\n### 1. RepMLP Usage\n#### 1.1. Paper\n[\"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition\"](https://arxiv.org/pdf/2105.01883v1.pdf)\n\n#### 1.2. Overview\n![](./model/img/repmlp.png)\n\n#### 1.3. Usage Code\n```python\nfrom model.mlp.repmlp import RepMLP\nimport torch\nfrom torch import nn\n\nN=4 #batch size\nC=512 #input dim\nO=1024 #output dim\nH=14 #image height\nW=14 #image width\nh=7 #patch height\nw=7 #patch width\nfc1_fc2_reduction=1 #reduction ratio\nfc3_groups=8 # groups\nrepconv_kernels=[1,3,5,7] #kernel list\nrepmlp=RepMLP(C,O,H,W,h,w,fc1_fc2_reduction,fc3_groups,repconv_kernels=repconv_kernels)\nx=torch.randn(N,C,H,W)\nrepmlp.eval()\nfor module in repmlp.modules():\n    if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):\n        nn.init.uniform_(module.running_mean, 0, 0.1)\n        nn.init.uniform_(module.running_var, 0, 0.1)\n        nn.init.uniform_(module.weight, 0, 0.1)\n        nn.init.uniform_(module.bias, 0, 0.1)\n\n#training result\nout=repmlp(x)\n#inference result\nrepmlp.switch_to_deploy()\ndeployout = repmlp(x)\n\nprint(((deployout-out)**2).sum())\n```\n\n### 2. MLP-Mixer Usage\n#### 2.1. Paper\n[\"MLP-Mixer: An all-MLP Architecture for Vision\"](https://arxiv.org/pdf/2105.01601.pdf)\n\n#### 2.2. Overview\n![](./model/img/mlpmixer.png)\n\n#### 2.3. Usage Code\n```python\nfrom model.mlp.mlp_mixer import MlpMixer\nimport torch\nmlp_mixer=MlpMixer(num_classes=1000,num_blocks=10,patch_size=10,tokens_hidden_dim=32,channels_hidden_dim=1024,tokens_mlp_dim=16,channels_mlp_dim=1024)\ninput=torch.randn(50,3,40,40)\noutput=mlp_mixer(input)\nprint(output.shape)\n```\n\n***\n\n### 3. ResMLP Usage\n#### 3.1. Paper\n[\"ResMLP: Feedforward networks for image classification with data-efficient training\"](https://arxiv.org/pdf/2105.03404.pdf)\n\n#### 3.2. Overview\n![](./model/img/resmlp.png)\n\n#### 3.3. Usage Code\n```python\nfrom model.mlp.resmlp import ResMLP\nimport torch\n\ninput=torch.randn(50,3,14,14)\nresmlp=ResMLP(dim=128,image_size=14,patch_size=7,class_num=1000)\nout=resmlp(input)\nprint(out.shape) #the last dimention is class_num\n```\n\n***\n\n### 4. gMLP Usage\n#### 4.1. Paper\n[\"Pay Attention to MLPs\"](https://arxiv.org/abs/2105.08050)\n\n#### 4.2. Overview\n![](./model/img/gMLP.jpg)\n\n#### 4.3. Usage Code\n```python\nfrom model.mlp.g_mlp import gMLP\nimport torch\n\nnum_tokens=10000\nbs=50\nlen_sen=49\nnum_layers=6\ninput=torch.randint(num_tokens,(bs,len_sen)) #bs,len_sen\ngmlp = gMLP(num_tokens=num_tokens,len_sen=len_sen,dim=512,d_ff=1024)\noutput=gmlp(input)\nprint(output.shape)\n```\n\n***\n\n### 5. sMLP Usage\n#### 5.1. Paper\n[\"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?\"](https://arxiv.org/abs/2109.05422)\n\n#### 5.2. Overview\n![](./model/img/sMLP.jpg)\n\n#### 5.3. Usage Code\n```python\nfrom model.mlp.sMLP_block import sMLPBlock\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    smlp=sMLPBlock(h=224,w=224)\n    out=smlp(input)\n    print(out.shape)\n```\n\n\n# Re-Parameter Series\n\n- Pytorch implementation of [\"RepVGG: Making VGG-style ConvNets Great Again---CVPR2021\"](https://arxiv.org/abs/2101.03697)\n\n- Pytorch implementation of [\"ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks---ICCV2019\"](https://arxiv.org/abs/1908.03930)\n\n- Pytorch implementation of [\"Diverse Branch Block: Building a Convolution as an Inception-like Unit---CVPR2021\"](https://arxiv.org/abs/2103.13425)\n\n\n***\n\n### 1. RepVGG Usage\n#### 1.1. Paper\n[\"RepVGG: Making VGG-style ConvNets Great Again\"](https://arxiv.org/abs/2101.03697)\n\n#### 1.2. Overview\n![](./model/img/repvgg.png)\n\n#### 1.3. Usage Code\n```python\n\nfrom model.rep.repvgg import RepBlock\nimport torch\n\n\ninput=torch.randn(50,512,49,49)\nrepblock=RepBlock(512,512)\nrepblock.eval()\nout=repblock(input)\nrepblock._switch_to_deploy()\nout2=repblock(input)\nprint('difference between vgg and repvgg')\nprint(((out2-out)**2).sum())\n```\n\n\n\n***\n\n### 2. ACNet Usage\n#### 2.1. Paper\n[\"ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks\"](https://arxiv.org/abs/1908.03930)\n\n#### 2.2. Overview\n![](./model/img/acnet.png)\n\n#### 2.3. Usage Code\n```python\nfrom model.rep.acnet import ACNet\nimport torch\nfrom torch import nn\n\ninput=torch.randn(50,512,49,49)\nacnet=ACNet(512,512)\nacnet.eval()\nout=acnet(input)\nacnet._switch_to_deploy()\nout2=acnet(input)\nprint('difference:')\nprint(((out2-out)**2).sum())\n\n```\n\n\n\n***\n\n### 2. Diverse Branch Block Usage\n#### 2.1. Paper\n[\"Diverse Branch Block: Building a Convolution as an Inception-like Unit\"](https://arxiv.org/abs/2103.13425)\n\n#### 2.2. Overview\n![](./model/img/ddb.png)\n\n#### 2.3. Usage Code\n##### 2.3.1 Transform I\n```python\nfrom model.rep.ddb import transI_conv_bn\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n#conv+bn\nconv1=nn.Conv2d(64,64,3,padding=1)\nbn1=nn.BatchNorm2d(64)\nbn1.eval()\nout1=bn1(conv1(input))\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transI_conv_bn(conv1,bn1)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.2 Transform II\n```python\nfrom model.rep.ddb import transII_conv_branch\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1=nn.Conv2d(64,64,3,padding=1)\nconv2=nn.Conv2d(64,64,3,padding=1)\nout1=conv1(input)+conv2(input)\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transII_conv_branch(conv1,conv2)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.3 Transform III\n```python\nfrom model.rep.ddb import transIII_conv_sequential\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1=nn.Conv2d(64,64,1,padding=0,bias=False)\nconv2=nn.Conv2d(64,64,3,padding=1,bias=False)\nout1=conv2(conv1(input))\n\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1,bias=False)\nconv_fuse.weight.data=transIII_conv_sequential(conv1,conv2)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.4 Transform IV\n```python\nfrom model.rep.ddb import transIV_conv_concat\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1=nn.Conv2d(64,32,3,padding=1)\nconv2=nn.Conv2d(64,32,3,padding=1)\nout1=torch.cat([conv1(input),conv2(input)],dim=1)\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transIV_conv_concat(conv1,conv2)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n##### 2.3.5 Transform V\n```python\nfrom model.rep.ddb import transV_avg\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\navg=nn.AvgPool2d(kernel_size=3,stride=1)\nout1=avg(input)\n\nconv=transV_avg(64,3)\nout2=conv(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n\n##### 2.3.6 Transform VI\n```python\nfrom model.rep.ddb import transVI_conv_scale\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,64,7,7)\n\n#conv+conv\nconv1x1=nn.Conv2d(64,64,1)\nconv1x3=nn.Conv2d(64,64,(1,3),padding=(0,1))\nconv3x1=nn.Conv2d(64,64,(3,1),padding=(1,0))\nout1=conv1x1(input)+conv1x3(input)+conv3x1(input)\n\n#conv_fuse\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\nconv_fuse.weight.data,conv_fuse.bias.data=transVI_conv_scale(conv1x1,conv1x3,conv3x1)\nout2=conv_fuse(input)\n\nprint(\"difference:\",((out2-out1)**2).sum().item())\n```\n\n\n\n\n\n# Convolution Series\n\n- Pytorch implementation of [\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications---CVPR2017\"](https://arxiv.org/abs/1704.04861)\n\n- Pytorch implementation of [\"Efficientnet: Rethinking model scaling for convolutional neural networks---PMLR2019\"](http://proceedings.mlr.press/v97/tan19a.html)\n\n- Pytorch implementation of [\"Involution: Inverting the Inherence of Convolution for Visual Recognition---CVPR2021\"](https://arxiv.org/abs/2103.06255)\n\n- Pytorch implementation of [\"Dynamic Convolution: Attention over Convolution Kernels---CVPR2020 Oral\"](https://arxiv.org/abs/1912.03458)\n\n- Pytorch implementation of [\"CondConv: Conditionally Parameterized Convolutions for Efficient Inference---NeurIPS2019\"](https://arxiv.org/abs/1904.04971)\n\n***\n\n### 1. Depthwise Separable Convolution Usage\n#### 1.1. Paper\n[\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\"](https://arxiv.org/abs/1704.04861)\n\n#### 1.2. Overview\n![](./model/img/DepthwiseSeparableConv.png)\n\n#### 1.3. Usage Code\n```python\nfrom model.conv.DepthwiseSeparableConvolution import DepthwiseSeparableConvolution\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\ndsconv=DepthwiseSeparableConvolution(3,64)\nout=dsconv(input)\nprint(out.shape)\n```\n\n***\n\n\n### 2. MBConv Usage\n#### 2.1. Paper\n[\"Efficientnet: Rethinking model scaling for convolutional neural networks\"](http://proceedings.mlr.press/v97/tan19a.html)\n\n#### 2.2. Overview\n![](./model/img/MBConv.jpg)\n\n#### 2.3. Usage Code\n```python\nfrom model.conv.MBConv import MBConvBlock\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\nmbconv=MBConvBlock(ksize=3,input_filters=3,output_filters=512,image_size=224)\nout=mbconv(input)\nprint(out.shape)\n\n\n```\n\n***\n\n\n### 3. Involution Usage\n#### 3.1. Paper\n[\"Involution: Inverting the Inherence of Convolution for Visual Recognition\"](https://arxiv.org/abs/2103.06255)\n\n#### 3.2. Overview\n![](./model/img/Involution.png)\n\n#### 3.3. Usage Code\n```python\nfrom model.conv.Involution import Involution\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,4,64,64)\ninvolution=Involution(kernel_size=3,in_channel=4,stride=2)\nout=involution(input)\nprint(out.shape)\n```\n\n***\n\n\n### 4. DynamicConv Usage\n#### 4.1. Paper\n[\"Dynamic Convolution: Attention over Convolution Kernels\"](https://arxiv.org/abs/1912.03458)\n\n#### 4.2. Overview\n![](./model/img/DynamicConv.png)\n\n#### 4.3. Usage Code\n```python\nfrom model.conv.DynamicConv import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(2,32,64,64)\n    m=DynamicConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)\n    out=m(input)\n    print(out.shape) # 2,32,64,64\n\n```\n\n***\n\n\n### 5. CondConv Usage\n#### 5.1. Paper\n[\"CondConv: Conditionally Parameterized Convolutions for Efficient Inference\"](https://arxiv.org/abs/1904.04971)\n\n#### 5.2. Overview\n![](./model/img/CondConv.png)\n\n#### 5.3. Usage Code\n```python\nfrom model.conv.CondConv import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(2,32,64,64)\n    m=CondConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)\n    out=m(input)\n    print(out.shape)\n\n```\n\n***\n",
            "readme_url": "https://github.com/xmu-xiaoma666/External-Attention-pytorch",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Global Filter Networks for Image Classification",
            "arxiv": "2107.00645",
            "year": 2021,
            "url": "http://arxiv.org/abs/2107.00645v2",
            "abstract": "Recent advances in self-attention and pure multi-layer perceptrons (MLP)\nmodels for vision have shown great potential in achieving promising performance\nwith fewer inductive biases. These models are generally based on learning\ninteraction among spatial locations from raw data. The complexity of\nself-attention and MLP grows quadratically as the image size increases, which\nmakes these models hard to scale up when high-resolution features are required.\nIn this paper, we present the Global Filter Network (GFNet), a conceptually\nsimple yet computationally efficient architecture, that learns long-term\nspatial dependencies in the frequency domain with log-linear complexity. Our\narchitecture replaces the self-attention layer in vision transformers with\nthree key operations: a 2D discrete Fourier transform, an element-wise\nmultiplication between frequency-domain features and learnable global filters,\nand a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity\ntrade-offs of our models on both ImageNet and downstream tasks. Our results\ndemonstrate that GFNet can be a very competitive alternative to\ntransformer-style models and CNNs in efficiency, generalization ability and\nrobustness. Code is available at https://github.com/raoyongming/GFNet",
            "authors": [
                "Yongming Rao",
                "Wenliang Zhao",
                "Zheng Zhu",
                "Jiwen Lu",
                "Jie Zhou"
            ]
        },
        {
            "title": "S$^2$-MLPv2: Improved Spatial-Shift MLP Architecture for Vision",
            "arxiv": "2108.01072",
            "year": 2021,
            "url": "http://arxiv.org/abs/2108.01072v1",
            "abstract": "Recently, MLP-based vision backbones emerge. MLP-based vision architectures\nwith less inductive bias achieve competitive performance in image recognition\ncompared with CNNs and vision Transformers. Among them, spatial-shift MLP\n(S$^2$-MLP), adopting the straightforward spatial-shift operation, achieves\nbetter performance than the pioneering works including MLP-mixer and ResMLP.\nMore recently, using smaller patches with a pyramid structure, Vision\nPermutator (ViP) and Global Filter Network (GFNet) achieve better performance\nthan S$^2$-MLP.\n  In this paper, we improve the S$^2$-MLP vision backbone. We expand the\nfeature map along the channel dimension and split the expanded feature map into\nseveral parts. We conduct different spatial-shift operations on split parts.\n  Meanwhile, we exploit the split-attention operation to fuse these split\nparts. Moreover, like the counterparts, we adopt smaller-scale patches and use\na pyramid structure for boosting the image recognition accuracy. We term the\nimproved spatial-shift MLP vision backbone as S$^2$-MLPv2. Using 55M\nparameters, our medium-scale model, S$^2$-MLPv2-Medium achieves an $83.6\\%$\ntop-1 accuracy on the ImageNet-1K benchmark using $224\\times 224$ images\nwithout self-attention and external training data.",
            "authors": [
                "Tan Yu",
                "Xu Li",
                "Yunfeng Cai",
                "Mingming Sun",
                "Ping Li"
            ]
        },
        {
            "title": "UFO-ViT: High Performance Linear Vision Transformer without Softmax",
            "arxiv": "2109.14382",
            "year": 2021,
            "url": "http://arxiv.org/abs/2109.14382v2",
            "abstract": "Vision transformers have become one of the most important models for computer\nvision tasks. Although they outperform prior works, they require heavy\ncomputational resources on a scale that is quadratic to $N$. This is a major\ndrawback of the traditional self-attention (SA) algorithm. Here, we propose the\nUnit Force Operated Vision Transformer (UFO-ViT), a novel SA mechanism that has\nlinear complexity. The main approach of this work is to eliminate nonlinearity\nfrom the original SA. We factorize the matrix multiplication of the SA\nmechanism without complicated linear approximation. By modifying only a few\nlines of code from the original SA, the proposed models outperform most\ntransformer-based models on image classification and dense prediction tasks on\nmost capacity regimes.",
            "authors": [
                "Jeong-geun Song"
            ]
        },
        {
            "title": "Dual Attention Network for Scene Segmentation",
            "arxiv": "1809.02983",
            "year": 2018,
            "url": "http://arxiv.org/abs/1809.02983v4",
            "abstract": "In this paper, we address the scene segmentation task by capturing rich\ncontextual dependencies based on the selfattention mechanism. Unlike previous\nworks that capture contexts by multi-scale features fusion, we propose a Dual\nAttention Networks (DANet) to adaptively integrate local features with their\nglobal dependencies. Specifically, we append two types of attention modules on\ntop of traditional dilated FCN, which model the semantic interdependencies in\nspatial and channel dimensions respectively. The position attention module\nselectively aggregates the features at each position by a weighted sum of the\nfeatures at all positions. Similar features would be related to each other\nregardless of their distances. Meanwhile, the channel attention module\nselectively emphasizes interdependent channel maps by integrating associated\nfeatures among all channel maps. We sum the outputs of the two attention\nmodules to further improve feature representation which contributes to more\nprecise segmentation results. We achieve new state-of-the-art segmentation\nperformance on three challenging scene segmentation datasets, i.e., Cityscapes,\nPASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5%\non Cityscapes test set is achieved without using coarse data. We make the code\nand trained model publicly available at https://github.com/junfu1115/DANet",
            "authors": [
                "Jun Fu",
                "Jing Liu",
                "Haijie Tian",
                "Yong Li",
                "Yongjun Bao",
                "Zhiwei Fang",
                "Hanqing Lu"
            ]
        },
        {
            "title": "MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning",
            "arxiv": "1911.09483",
            "year": 2019,
            "url": "http://arxiv.org/abs/1911.09483v1",
            "abstract": "In sequence to sequence learning, the self-attention mechanism proves to be\nhighly effective, and achieves significant improvements in many tasks. However,\nthe self-attention mechanism is not without its own flaws. Although\nself-attention can model extremely long dependencies, the attention in deep\nlayers tends to overconcentrate on a single token, leading to insufficient use\nof local information and difficultly in representing long sequences. In this\nwork, we explore parallel multi-scale representation learning on sequence data,\nstriving to capture both long-range and short-range language structures. To\nthis end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple.\nMUSE-simple contains the basic idea of parallel multi-scale sequence\nrepresentation learning, and it encodes the sequence in parallel, in terms of\ndifferent scales with the help from self-attention, and pointwise\ntransformation. MUSE builds on MUSE-simple and explores combining convolution\nand self-attention for learning sequence representations from more different\nscales. We focus on machine translation and the proposed approach achieves\nsubstantial performance improvements over Transformer, especially on long\nsequences. More importantly, we find that although conceptually simple, its\nsuccess in practice requires intricate considerations, and the multi-scale\nattention must build on unified semantic space. Under common setting, the\nproposed model achieves substantial performance and outperforms all previous\nmodels on three main machine translation tasks. In addition, MUSE has potential\nfor accelerating inference due to its parallelism. Code will be available at\nhttps://github.com/lancopku/MUSE",
            "authors": [
                "Guangxiang Zhao",
                "Xu Sun",
                "Jingjing Xu",
                "Zhiyuan Zhang",
                "Liangchen Luo"
            ]
        },
        {
            "title": "Polarized Self-Attention: Towards High-quality Pixel-wise Regression",
            "arxiv": "2107.00782",
            "year": 2021,
            "url": "http://arxiv.org/abs/2107.00782v2",
            "abstract": "Pixel-wise regression is probably the most common problem in fine-grained\ncomputer vision tasks, such as estimating keypoint heatmaps and segmentation\nmasks. These regression problems are very challenging particularly because they\nrequire, at low computation overheads, modeling long-range dependencies on\nhigh-resolution inputs/outputs to estimate the highly nonlinear pixel-wise\nsemantics. While attention mechanisms in Deep Convolutional Neural\nNetworks(DCNNs) has become popular for boosting long-range dependencies,\nelement-specific attention, such as Nonlocal blocks, is highly complex and\nnoise-sensitive to learn, and most of simplified attention hybrids try to reach\nthe best compromise among multiple types of tasks. In this paper, we present\nthe Polarized Self-Attention(PSA) block that incorporates two critical designs\ntowards high-quality pixel-wise regression: (1) Polarized filtering: keeping\nhigh internal resolution in both channel and spatial attention computation\nwhile completely collapsing input tensors along their counterpart dimensions.\n(2) Enhancement: composing non-linearity that directly fits the output\ndistribution of typical fine-grained regression, such as the 2D Gaussian\ndistribution (keypoint heatmaps), or the 2D Binormial distribution (binary\nsegmentation masks). PSA appears to have exhausted the representation capacity\nwithin its channel-only and spatial-only branches, such that there is only\nmarginal metric differences between its sequential and parallel layouts.\nExperimental results show that PSA boosts standard baselines by $2-4$ points,\nand boosts state-of-the-arts by $1-2$ points on 2D pose estimation and semantic\nsegmentation benchmarks.",
            "authors": [
                "Huajun Liu",
                "Fuqiang Liu",
                "Xinyi Fan",
                "Dong Huang"
            ]
        },
        {
            "title": "Diverse Branch Block: Building a Convolution as an Inception-like Unit",
            "arxiv": "2103.13425",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.13425v2",
            "abstract": "We propose a universal building block of Convolutional Neural Network\n(ConvNet) to improve the performance without any inference-time costs. The\nblock is named Diverse Branch Block (DBB), which enhances the representational\ncapacity of a single convolution by combining diverse branches of different\nscales and complexities to enrich the feature space, including sequences of\nconvolutions, multi-scale convolutions, and average pooling. After training, a\nDBB can be equivalently converted into a single conv layer for deployment.\nUnlike the advancements of novel ConvNet architectures, DBB complicates the\ntraining-time microstructure while maintaining the macro architecture, so that\nit can be used as a drop-in replacement for regular conv layers of any\narchitecture. In this way, the model can be trained to reach a higher level of\nperformance and then transformed into the original inference-time structure for\ninference. DBB improves ConvNets on image classification (up to 1.9% higher\ntop-1 accuracy on ImageNet), object detection and semantic segmentation. The\nPyTorch code and models are released at\nhttps://github.com/DingXiaoH/DiverseBranchBlock.",
            "authors": [
                "Xiaohan Ding",
                "Xiangyu Zhang",
                "Jungong Han",
                "Guiguang Ding"
            ]
        },
        {
            "title": "Non-deep Networks",
            "arxiv": "2110.07641",
            "year": 2021,
            "url": "http://arxiv.org/abs/2110.07641v1",
            "abstract": "Depth is the hallmark of deep neural networks. But more depth means more\nsequential computation and higher latency. This begs the question -- is it\npossible to build high-performing \"non-deep\" neural networks? We show that it\nis. To do so, we use parallel subnetworks instead of stacking one layer after\nanother. This helps effectively reduce depth while maintaining high\nperformance. By utilizing parallel substructures, we show, for the first time,\nthat a network with a depth of just 12 can achieve top-1 accuracy over 80% on\nImageNet, 96% on CIFAR10, and 81% on CIFAR100. We also show that a network with\na low-depth (12) backbone can achieve an AP of 48% on MS-COCO. We analyze the\nscaling rules for our design and show how to increase performance without\nchanging the network's depth. Finally, we provide a proof of concept for how\nnon-deep networks could be used to build low-latency recognition systems. Code\nis available at https://github.com/imankgoyal/NonDeepNetworks.",
            "authors": [
                "Ankit Goyal",
                "Alexey Bochkovskiy",
                "Jia Deng",
                "Vladlen Koltun"
            ]
        },
        {
            "title": "MLP-Mixer: An all-MLP Architecture for Vision",
            "arxiv": "2105.01601",
            "year": 2021,
            "url": "http://arxiv.org/abs/2105.01601v4",
            "abstract": "Convolutional Neural Networks (CNNs) are the go-to model for computer vision.\nRecently, attention-based networks, such as the Vision Transformer, have also\nbecome popular. In this paper we show that while convolutions and attention are\nboth sufficient for good performance, neither of them are necessary. We present\nMLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs).\nMLP-Mixer contains two types of layers: one with MLPs applied independently to\nimage patches (i.e. \"mixing\" the per-location features), and one with MLPs\napplied across patches (i.e. \"mixing\" spatial information). When trained on\nlarge datasets, or with modern regularization schemes, MLP-Mixer attains\ncompetitive scores on image classification benchmarks, with pre-training and\ninference cost comparable to state-of-the-art models. We hope that these\nresults spark further research beyond the realms of well established CNNs and\nTransformers.",
            "authors": [
                "Ilya Tolstikhin",
                "Neil Houlsby",
                "Alexander Kolesnikov",
                "Lucas Beyer",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Jessica Yung",
                "Andreas Steiner",
                "Daniel Keysers",
                "Jakob Uszkoreit",
                "Mario Lucic",
                "Alexey Dosovitskiy"
            ]
        },
        {
            "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
            "arxiv": "1704.04861",
            "year": 2017,
            "url": "http://arxiv.org/abs/1704.04861v1",
            "abstract": "We present a class of efficient models called MobileNets for mobile and\nembedded vision applications. MobileNets are based on a streamlined\narchitecture that uses depth-wise separable convolutions to build light weight\ndeep neural networks. We introduce two simple global hyper-parameters that\nefficiently trade off between latency and accuracy. These hyper-parameters\nallow the model builder to choose the right sized model for their application\nbased on the constraints of the problem. We present extensive experiments on\nresource and accuracy tradeoffs and show strong performance compared to other\npopular models on ImageNet classification. We then demonstrate the\neffectiveness of MobileNets across a wide range of applications and use cases\nincluding object detection, finegrain classification, face attributes and large\nscale geo-localization.",
            "authors": [
                "Andrew G. Howard",
                "Menglong Zhu",
                "Bo Chen",
                "Dmitry Kalenichenko",
                "Weijun Wang",
                "Tobias Weyand",
                "Marco Andreetto",
                "Hartwig Adam"
            ]
        },
        {
            "title": "CondConv: Conditionally Parameterized Convolutions for Efficient Inference",
            "arxiv": "1904.04971",
            "year": 2019,
            "url": "http://arxiv.org/abs/1904.04971v3",
            "abstract": "Convolutional layers are one of the basic building blocks of modern deep\nneural networks. One fundamental assumption is that convolutional kernels\nshould be shared for all examples in a dataset. We propose conditionally\nparameterized convolutions (CondConv), which learn specialized convolutional\nkernels for each example. Replacing normal convolutions with CondConv enables\nus to increase the size and capacity of a network, while maintaining efficient\ninference. We demonstrate that scaling networks with CondConv improves the\nperformance and inference cost trade-off of several existing convolutional\nneural network architectures on both classification and detection tasks. On\nImageNet classification, our CondConv approach applied to EfficientNet-B0\nachieves state-of-the-art performance of 78.3% accuracy with only 413M\nmultiply-adds. Code and checkpoints for the CondConv Tensorflow layer and\nCondConv-EfficientNet models are available at:\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv.",
            "authors": [
                "Brandon Yang",
                "Gabriel Bender",
                "Quoc V. Le",
                "Jiquan Ngiam"
            ]
        },
        {
            "title": "Aggregated Residual Transformations for Deep Neural Networks",
            "arxiv": "1611.05431",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.05431v2",
            "abstract": "We present a simple, highly modularized network architecture for image\nclassification. Our network is constructed by repeating a building block that\naggregates a set of transformations with the same topology. Our simple design\nresults in a homogeneous, multi-branch architecture that has only a few\nhyper-parameters to set. This strategy exposes a new dimension, which we call\n\"cardinality\" (the size of the set of transformations), as an essential factor\nin addition to the dimensions of depth and width. On the ImageNet-1K dataset,\nwe empirically show that even under the restricted condition of maintaining\ncomplexity, increasing cardinality is able to improve classification accuracy.\nMoreover, increasing cardinality is more effective than going deeper or wider\nwhen we increase the capacity. Our models, named ResNeXt, are the foundations\nof our entry to the ILSVRC 2016 classification task in which we secured 2nd\nplace. We further investigate ResNeXt on an ImageNet-5K set and the COCO\ndetection set, also showing better results than its ResNet counterpart. The\ncode and models are publicly available online.",
            "authors": [
                "Saining Xie",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Zhuowen Tu",
                "Kaiming He"
            ]
        },
        {
            "title": "Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?",
            "arxiv": "2109.05422",
            "year": 2021,
            "url": "http://arxiv.org/abs/2109.05422v1",
            "abstract": "Transformers have sprung up in the field of computer vision. In this work, we\nexplore whether the core self-attention module in Transformer is the key to\nachieving excellent performance in image recognition. To this end, we build an\nattention-free network called sMLPNet based on the existing MLP-based vision\nmodels. Specifically, we replace the MLP module in the token-mixing step with a\nnovel sparse MLP (sMLP) module. For 2D image tokens, sMLP applies 1D MLP along\nthe axial directions and the parameters are shared among rows or columns. By\nsparse connection and weight sharing, sMLP module significantly reduces the\nnumber of model parameters and computational complexity, avoiding the common\nover-fitting problem that plagues the performance of MLP-like models. When only\ntrained on the ImageNet-1K dataset, the proposed sMLPNet achieves 81.9% top-1\naccuracy with only 24M parameters, which is much better than most CNNs and\nvision Transformers under the same model size constraint. When scaling up to\n66M parameters, sMLPNet achieves 83.4% top-1 accuracy, which is on par with the\nstate-of-the-art Swin Transformer. The success of sMLPNet suggests that the\nself-attention mechanism is not necessarily a silver bullet in computer vision.\nCode will be made publicly available.",
            "authors": [
                "Chuanxin Tang",
                "Yucheng Zhao",
                "Guangting Wang",
                "Chong Luo",
                "Wenxuan Xie",
                "Wenjun Zeng"
            ]
        },
        {
            "title": "Rotate to Attend: Convolutional Triplet Attention Module",
            "arxiv": "2010.03045",
            "year": 2020,
            "url": "http://arxiv.org/abs/2010.03045v2",
            "abstract": "Benefiting from the capability of building inter-dependencies among channels\nor spatial locations, attention mechanisms have been extensively studied and\nbroadly used in a variety of computer vision tasks recently. In this paper, we\ninvestigate light-weight but effective attention mechanisms and present triplet\nattention, a novel method for computing attention weights by capturing\ncross-dimension interaction using a three-branch structure. For an input\ntensor, triplet attention builds inter-dimensional dependencies by the rotation\noperation followed by residual transformations and encodes inter-channel and\nspatial information with negligible computational overhead. Our method is\nsimple as well as efficient and can be easily plugged into classic backbone\nnetworks as an add-on module. We demonstrate the effectiveness of our method on\nvarious challenging tasks including image classification on ImageNet-1k and\nobject detection on MSCOCO and PASCAL VOC datasets. Furthermore, we provide\nextensive in-sight into the performance of triplet attention by visually\ninspecting the GradCAM and GradCAM++ results. The empirical evaluation of our\nmethod supports our intuition on the importance of capturing dependencies\nacross dimensions when computing attention weights. Code for this paper can be\npublicly accessed at https://github.com/LandskapeAI/triplet-attention",
            "authors": [
                "Diganta Misra",
                "Trikay Nalamada",
                "Ajay Uppili Arasanipalai",
                "Qibin Hou"
            ]
        },
        {
            "title": "Involution: Inverting the Inherence of Convolution for Visual Recognition",
            "arxiv": "2103.06255",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.06255v2",
            "abstract": "Convolution has been the core ingredient of modern neural networks,\ntriggering the surge of deep learning in vision. In this work, we rethink the\ninherent principles of standard convolution for vision tasks, specifically\nspatial-agnostic and channel-specific. Instead, we present a novel atomic\noperation for deep neural networks by inverting the aforementioned design\nprinciples of convolution, coined as involution. We additionally demystify the\nrecent popular self-attention operator and subsume it into our involution\nfamily as an over-complicated instantiation. The proposed involution operator\ncould be leveraged as fundamental bricks to build the new generation of neural\nnetworks for visual recognition, powering different deep learning models on\nseveral prevalent benchmarks, including ImageNet classification, COCO detection\nand segmentation, together with Cityscapes segmentation. Our involution-based\nmodels improve the performance of convolutional baselines using ResNet-50 by up\nto 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU\nabsolutely while compressing the computational cost to 66%, 65%, 72%, and 57%\non the above benchmarks, respectively. Code and pre-trained models for all the\ntasks are available at https://github.com/d-li14/involution.",
            "authors": [
                "Duo Li",
                "Jie Hu",
                "Changhu Wang",
                "Xiangtai Li",
                "Qi She",
                "Lei Zhu",
                "Tong Zhang",
                "Qifeng Chen"
            ]
        },
        {
            "title": "ResMLP: Feedforward networks for image classification with data-efficient training",
            "arxiv": "2105.03404",
            "year": 2021,
            "url": "http://arxiv.org/abs/2105.03404v2",
            "abstract": "We present ResMLP, an architecture built entirely upon multi-layer\nperceptrons for image classification. It is a simple residual network that\nalternates (i) a linear layer in which image patches interact, independently\nand identically across channels, and (ii) a two-layer feed-forward network in\nwhich channels interact independently per patch. When trained with a modern\ntraining strategy using heavy data-augmentation and optionally distillation, it\nattains surprisingly good accuracy/complexity trade-offs on ImageNet. We also\ntrain ResMLP models in a self-supervised setup, to further remove priors from\nemploying a labelled dataset. Finally, by adapting our model to machine\ntranslation we achieve surprisingly good results.\n  We share pre-trained models and our code based on the Timm library.",
            "authors": [
                "Hugo Touvron",
                "Piotr Bojanowski",
                "Mathilde Caron",
                "Matthieu Cord",
                "Alaaeldin El-Nouby",
                "Edouard Grave",
                "Gautier Izacard",
                "Armand Joulin",
                "Gabriel Synnaeve",
                "Jakob Verbeek",
                "Herv\u00e9 J\u00e9gou"
            ]
        },
        {
            "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes",
            "arxiv": "2106.04803",
            "year": 2021,
            "url": "http://arxiv.org/abs/2106.04803v2",
            "abstract": "Transformers have attracted increasing interests in computer vision, but they\nstill fall behind state-of-the-art convolutional networks. In this work, we\nshow that while Transformers tend to have larger model capacity, their\ngeneralization can be worse than convolutional networks due to the lack of the\nright inductive bias. To effectively combine the strengths from both\narchitectures, we present CoAtNets(pronounced \"coat\" nets), a family of hybrid\nmodels built from two key insights: (1) depthwise Convolution and\nself-Attention can be naturally unified via simple relative attention; (2)\nvertically stacking convolution layers and attention layers in a principled way\nis surprisingly effective in improving generalization, capacity and efficiency.\nExperiments show that our CoAtNets achieve state-of-the-art performance under\ndifferent resource constraints across various datasets: Without extra data,\nCoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M\nimages from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching\nViT-huge pre-trained with 300M images from JFT-300M while using 23x less data;\nNotably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1\naccuracy on ImageNet, establishing a new state-of-the-art result.",
            "authors": [
                "Zihang Dai",
                "Hanxiao Liu",
                "Quoc V. Le",
                "Mingxing Tan"
            ]
        },
        {
            "title": "Coordinate Attention for Efficient Mobile Network Design",
            "arxiv": "2103.02907",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.02907v1",
            "abstract": "Recent studies on mobile network design have demonstrated the remarkable\neffectiveness of channel attention (e.g., the Squeeze-and-Excitation attention)\nfor lifting model performance, but they generally neglect the positional\ninformation, which is important for generating spatially selective attention\nmaps. In this paper, we propose a novel attention mechanism for mobile networks\nby embedding positional information into channel attention, which we call\n\"coordinate attention\". Unlike channel attention that transforms a feature\ntensor to a single feature vector via 2D global pooling, the coordinate\nattention factorizes channel attention into two 1D feature encoding processes\nthat aggregate features along the two spatial directions, respectively. In this\nway, long-range dependencies can be captured along one spatial direction and\nmeanwhile precise positional information can be preserved along the other\nspatial direction. The resulting feature maps are then encoded separately into\na pair of direction-aware and position-sensitive attention maps that can be\ncomplementarily applied to the input feature map to augment the representations\nof the objects of interest. Our coordinate attention is simple and can be\nflexibly plugged into classic mobile networks, such as MobileNetV2, MobileNeXt,\nand EfficientNet with nearly no computational overhead. Extensive experiments\ndemonstrate that our coordinate attention is not only beneficial to ImageNet\nclassification but more interestingly, behaves better in down-stream tasks,\nsuch as object detection and semantic segmentation. Code is available at\nhttps://github.com/Andrew-Qibin/CoordAttention.",
            "authors": [
                "Qibin Hou",
                "Daquan Zhou",
                "Jiashi Feng"
            ]
        },
        {
            "title": "Squeeze-and-Excitation Networks",
            "arxiv": "1709.01507",
            "year": 2017,
            "url": "http://arxiv.org/abs/1709.01507v4",
            "abstract": "The central building block of convolutional neural networks (CNNs) is the\nconvolution operator, which enables networks to construct informative features\nby fusing both spatial and channel-wise information within local receptive\nfields at each layer. A broad range of prior research has investigated the\nspatial component of this relationship, seeking to strengthen the\nrepresentational power of a CNN by enhancing the quality of spatial encodings\nthroughout its feature hierarchy. In this work, we focus instead on the channel\nrelationship and propose a novel architectural unit, which we term the\n\"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise\nfeature responses by explicitly modelling interdependencies between channels.\nWe show that these blocks can be stacked together to form SENet architectures\nthat generalise extremely effectively across different datasets. We further\ndemonstrate that SE blocks bring significant improvements in performance for\nexisting state-of-the-art CNNs at slight additional computational cost.\nSqueeze-and-Excitation Networks formed the foundation of our ILSVRC 2017\nclassification submission which won first place and reduced the top-5 error to\n2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%.\nModels and code are available at https://github.com/hujie-frank/SENet.",
            "authors": [
                "Jie Hu",
                "Li Shen",
                "Samuel Albanie",
                "Gang Sun",
                "Enhua Wu"
            ]
        },
        {
            "title": "BAM: Bottleneck Attention Module",
            "arxiv": "1807.06514",
            "year": 2018,
            "url": "http://arxiv.org/abs/1807.06514v2",
            "abstract": "Recent advances in deep neural networks have been developed via architecture\nsearch for stronger representational power. In this work, we focus on the\neffect of attention in general deep neural networks. We propose a simple and\neffective attention module, named Bottleneck Attention Module (BAM), that can\nbe integrated with any feed-forward convolutional neural networks. Our module\ninfers an attention map along two separate pathways, channel and spatial. We\nplace our module at each bottleneck of models where the downsampling of feature\nmaps occurs. Our module constructs a hierarchical attention at bottlenecks with\na number of parameters and it is trainable in an end-to-end manner jointly with\nany feed-forward models. We validate our BAM through extensive experiments on\nCIFAR-100, ImageNet-1K, VOC 2007 and MS COCO benchmarks. Our experiments show\nconsistent improvement in classification and detection performances with\nvarious models, demonstrating the wide applicability of BAM. The code and\nmodels will be publicly available.",
            "authors": [
                "Jongchan Park",
                "Sanghyun Woo",
                "Joon-Young Lee",
                "In So Kweon"
            ]
        },
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "Attention Is All You Need",
            "arxiv": "1706.03762",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.03762v5",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ]
        },
        {
            "title": "RepVGG: Making VGG-style ConvNets Great Again",
            "arxiv": "2101.03697",
            "year": 2021,
            "url": "http://arxiv.org/abs/2101.03697v3",
            "abstract": "We present a simple but powerful architecture of convolutional neural\nnetwork, which has a VGG-like inference-time body composed of nothing but a\nstack of 3x3 convolution and ReLU, while the training-time model has a\nmulti-branch topology. Such decoupling of the training-time and inference-time\narchitecture is realized by a structural re-parameterization technique so that\nthe model is named RepVGG. On ImageNet, RepVGG reaches over 80% top-1 accuracy,\nwhich is the first time for a plain model, to the best of our knowledge. On\nNVIDIA 1080Ti GPU, RepVGG models run 83% faster than ResNet-50 or 101% faster\nthan ResNet-101 with higher accuracy and show favorable accuracy-speed\ntrade-off compared to the state-of-the-art models like EfficientNet and RegNet.\nThe code and trained models are available at\nhttps://github.com/megvii-model/RepVGG.",
            "authors": [
                "Xiaohan Ding",
                "Xiangyu Zhang",
                "Ningning Ma",
                "Jungong Han",
                "Guiguang Ding",
                "Jian Sun"
            ]
        },
        {
            "title": "Selective Kernel Networks",
            "arxiv": "1903.06586",
            "year": 2019,
            "url": "http://arxiv.org/abs/1903.06586v2",
            "abstract": "In standard Convolutional Neural Networks (CNNs), the receptive fields of\nartificial neurons in each layer are designed to share the same size. It is\nwell-known in the neuroscience community that the receptive field size of\nvisual cortical neurons are modulated by the stimulus, which has been rarely\nconsidered in constructing CNNs. We propose a dynamic selection mechanism in\nCNNs that allows each neuron to adaptively adjust its receptive field size\nbased on multiple scales of input information. A building block called\nSelective Kernel (SK) unit is designed, in which multiple branches with\ndifferent kernel sizes are fused using softmax attention that is guided by the\ninformation in these branches. Different attentions on these branches yield\ndifferent sizes of the effective receptive fields of neurons in the fusion\nlayer. Multiple SK units are stacked to a deep network termed Selective Kernel\nNetworks (SKNets). On the ImageNet and CIFAR benchmarks, we empirically show\nthat SKNet outperforms the existing state-of-the-art architectures with lower\nmodel complexity. Detailed analyses show that the neurons in SKNet can capture\ntarget objects with different scales, which verifies the capability of neurons\nfor adaptively adjusting their receptive field sizes according to the input.\nThe code and models are available at https://github.com/implus/SKNet.",
            "authors": [
                "Xiang Li",
                "Wenhai Wang",
                "Xiaolin Hu",
                "Jian Yang"
            ]
        },
        {
            "title": "Pay Attention to MLPs",
            "arxiv": "2105.08050",
            "year": 2021,
            "url": "http://arxiv.org/abs/2105.08050v2",
            "abstract": "Transformers have become one of the most important architectural innovations\nin deep learning and have enabled many breakthroughs over the past few years.\nHere we propose a simple network architecture, gMLP, based on MLPs with gating,\nand show that it can perform as well as Transformers in key language and vision\napplications. Our comparisons show that self-attention is not critical for\nVision Transformers, as gMLP can achieve the same accuracy. For BERT, our model\nachieves parity with Transformers on pretraining perplexity and is better on\nsome downstream NLP tasks. On finetuning tasks where gMLP performs worse,\nmaking the gMLP model substantially larger can close the gap with Transformers.\nIn general, our experiments show that gMLP can scale as well as Transformers\nover increased data and compute.",
            "authors": [
                "Hanxiao Liu",
                "Zihang Dai",
                "David R. So",
                "Quoc V. Le"
            ]
        },
        {
            "title": "Residual Attention: A Simple but Effective Method for Multi-Label Recognition",
            "arxiv": "2108.02456",
            "year": 2021,
            "url": "http://arxiv.org/abs/2108.02456v2",
            "abstract": "Multi-label image recognition is a challenging computer vision task of\npractical use. Progresses in this area, however, are often characterized by\ncomplicated methods, heavy computations, and lack of intuitive explanations. To\neffectively capture different spatial regions occupied by objects from\ndifferent categories, we propose an embarrassingly simple module, named\nclass-specific residual attention (CSRA). CSRA generates class-specific\nfeatures for every category by proposing a simple spatial attention score, and\nthen combines it with the class-agnostic average pooling feature. CSRA achieves\nstate-of-the-art results on multilabel recognition, and at the same time is\nmuch simpler than them. Furthermore, with only 4 lines of code, CSRA also leads\nto consistent improvement across many diverse pretrained models and datasets\nwithout any extra training. CSRA is both easy to implement and light in\ncomputations, which also enjoys intuitive explanations and visualizations.",
            "authors": [
                "Ke Zhu",
                "Jianxin Wu"
            ]
        },
        {
            "title": "Dynamic Convolution: Attention over Convolution Kernels",
            "arxiv": "1912.03458",
            "year": 2019,
            "url": "http://arxiv.org/abs/1912.03458v2",
            "abstract": "Light-weight convolutional neural networks (CNNs) suffer performance\ndegradation as their low computational budgets constrain both the depth (number\nof convolution layers) and the width (number of channels) of CNNs, resulting in\nlimited representation capability. To address this issue, we present Dynamic\nConvolution, a new design that increases model complexity without increasing\nthe network depth or width. Instead of using a single convolution kernel per\nlayer, dynamic convolution aggregates multiple parallel convolution kernels\ndynamically based upon their attentions, which are input dependent. Assembling\nmultiple kernels is not only computationally efficient due to the small kernel\nsize, but also has more representation power since these kernels are aggregated\nin a non-linear way via attention. By simply using dynamic convolution for the\nstate-of-the-art architecture MobileNetV3-Small, the top-1 accuracy of ImageNet\nclassification is boosted by 2.9% with only 4% additional FLOPs and 2.9 AP gain\nis achieved on COCO keypoint detection.",
            "authors": [
                "Yinpeng Chen",
                "Xiyang Dai",
                "Mengchen Liu",
                "Dongdong Chen",
                "Lu Yuan",
                "Zicheng Liu"
            ]
        },
        {
            "title": "SA-Net: Shuffle Attention for Deep Convolutional Neural Networks",
            "arxiv": "2102.00240",
            "year": 2021,
            "url": "http://arxiv.org/abs/2102.00240v1",
            "abstract": "Attention mechanisms, which enable a neural network to accurately focus on\nall the relevant elements of the input, have become an essential component to\nimprove the performance of deep neural networks. There are mainly two attention\nmechanisms widely used in computer vision studies, \\textit{spatial attention}\nand \\textit{channel attention}, which aim to capture the pixel-level pairwise\nrelationship and channel dependency, respectively. Although fusing them\ntogether may achieve better performance than their individual implementations,\nit will inevitably increase the computational overhead. In this paper, we\npropose an efficient Shuffle Attention (SA) module to address this issue, which\nadopts Shuffle Units to combine two types of attention mechanisms effectively.\nSpecifically, SA first groups channel dimensions into multiple sub-features\nbefore processing them in parallel. Then, for each sub-feature, SA utilizes a\nShuffle Unit to depict feature dependencies in both spatial and channel\ndimensions. After that, all sub-features are aggregated and a \"channel shuffle\"\noperator is adopted to enable information communication between different\nsub-features. The proposed SA module is efficient yet effective, e.g., the\nparameters and computations of SA against the backbone ResNet50 are 300 vs.\n25.56M and 2.76e-3 GFLOPs vs. 4.12 GFLOPs, respectively, and the performance\nboost is more than 1.34% in terms of Top-1 accuracy. Extensive experimental\nresults on common-used benchmarks, including ImageNet-1k for classification, MS\nCOCO for object detection, and instance segmentation, demonstrate that the\nproposed SA outperforms the current SOTA methods significantly by achieving\nhigher accuracy while having lower model complexity. The code and models are\navailable at https://github.com/wofmanaf/SA-Net.",
            "authors": [
                "Qing-Long Zhang Yu-Bin Yang"
            ]
        },
        {
            "title": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones",
            "arxiv": "2103.12731",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.12731v3",
            "abstract": "Self-attention has the promise of improving computer vision systems due to\nparameter-independent scaling of receptive fields and content-dependent\ninteractions, in contrast to parameter-dependent scaling and\ncontent-independent interactions of convolutions. Self-attention models have\nrecently been shown to have encouraging improvements on accuracy-parameter\ntrade-offs compared to baseline convolutional models such as ResNet-50. In this\nwork, we aim to develop self-attention models that can outperform not just the\ncanonical baseline models, but even the high-performing convolutional models.\nWe propose two extensions to self-attention that, in conjunction with a more\nefficient implementation of self-attention, improve the speed, memory usage,\nand accuracy of these models. We leverage these improvements to develop a new\nself-attention model family, HaloNets, which reach state-of-the-art accuracies\non the parameter-limited setting of the ImageNet classification benchmark. In\npreliminary transfer learning experiments, we find that HaloNet models\noutperform much larger models and have better inference performance. On harder\ntasks such as object detection and instance segmentation, our simple local\nself-attention and convolutional hybrids show improvements over very strong\nbaselines. These results mark another step in demonstrating the efficacy of\nself-attention models on settings traditionally dominated by convolutional\nmodels.",
            "authors": [
                "Ashish Vaswani",
                "Prajit Ramachandran",
                "Aravind Srinivas",
                "Niki Parmar",
                "Blake Hechtman",
                "Jonathon Shlens"
            ]
        },
        {
            "title": "ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks",
            "arxiv": "1908.03930",
            "year": 2019,
            "url": "http://arxiv.org/abs/1908.03930v3",
            "abstract": "As designing appropriate Convolutional Neural Network (CNN) architecture in\nthe context of a given application usually involves heavy human works or\nnumerous GPU hours, the research community is soliciting the\narchitecture-neutral CNN structures, which can be easily plugged into multiple\nmature architectures to improve the performance on our real-world applications.\nWe propose Asymmetric Convolution Block (ACB), an architecture-neutral\nstructure as a CNN building block, which uses 1D asymmetric convolutions to\nstrengthen the square convolution kernels. For an off-the-shelf architecture,\nwe replace the standard square-kernel convolutional layers with ACBs to\nconstruct an Asymmetric Convolutional Network (ACNet), which can be trained to\nreach a higher level of accuracy. After training, we equivalently convert the\nACNet into the same original architecture, thus requiring no extra computations\nanymore. We have observed that ACNet can improve the performance of various\nmodels on CIFAR and ImageNet by a clear margin. Through further experiments, we\nattribute the effectiveness of ACB to its capability of enhancing the model's\nrobustness to rotational distortions and strengthening the central skeleton\nparts of square convolution kernels.",
            "authors": [
                "Xiaohan Ding",
                "Yuchen Guo",
                "Guiguang Ding",
                "Jungong Han"
            ]
        },
        {
            "title": "Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition",
            "arxiv": "2106.12368",
            "year": 2021,
            "url": "http://arxiv.org/abs/2106.12368v1",
            "abstract": "In this paper, we present Vision Permutator, a conceptually simple and data\nefficient MLP-like architecture for visual recognition. By realizing the\nimportance of the positional information carried by 2D feature representations,\nunlike recent MLP-like models that encode the spatial information along the\nflattened spatial dimensions, Vision Permutator separately encodes the feature\nrepresentations along the height and width dimensions with linear projections.\nThis allows Vision Permutator to capture long-range dependencies along one\nspatial direction and meanwhile preserve precise positional information along\nthe other direction. The resulting position-sensitive outputs are then\naggregated in a mutually complementing manner to form expressive\nrepresentations of the objects of interest. We show that our Vision Permutators\nare formidable competitors to convolutional neural networks (CNNs) and vision\ntransformers. Without the dependence on spatial convolutions or attention\nmechanisms, Vision Permutator achieves 81.5% top-1 accuracy on ImageNet without\nextra large-scale training data (e.g., ImageNet-22k) using only 25M learnable\nparameters, which is much better than most CNNs and vision transformers under\nthe same model size constraint. When scaling up to 88M, it attains 83.2% top-1\naccuracy. We hope this work could encourage research on rethinking the way of\nencoding spatial information and facilitate the development of MLP-like models.\nCode is available at https://github.com/Andrew-Qibin/VisionPermutator.",
            "authors": [
                "Qibin Hou",
                "Zihang Jiang",
                "Li Yuan",
                "Ming-Ming Cheng",
                "Shuicheng Yan",
                "Jiashi Feng"
            ]
        },
        {
            "title": "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks",
            "arxiv": "1910.03151",
            "year": 2019,
            "url": "http://arxiv.org/abs/1910.03151v4",
            "abstract": "Recently, channel attention mechanism has demonstrated to offer great\npotential in improving the performance of deep convolutional neural networks\n(CNNs). However, most existing methods dedicate to developing more\nsophisticated attention modules for achieving better performance, which\ninevitably increase model complexity. To overcome the paradox of performance\nand complexity trade-off, this paper proposes an Efficient Channel Attention\n(ECA) module, which only involves a handful of parameters while bringing clear\nperformance gain. By dissecting the channel attention module in SENet, we\nempirically show avoiding dimensionality reduction is important for learning\nchannel attention, and appropriate cross-channel interaction can preserve\nperformance while significantly decreasing model complexity. Therefore, we\npropose a local cross-channel interaction strategy without dimensionality\nreduction, which can be efficiently implemented via $1D$ convolution.\nFurthermore, we develop a method to adaptively select kernel size of $1D$\nconvolution, determining coverage of local cross-channel interaction. The\nproposed ECA module is efficient yet effective, e.g., the parameters and\ncomputations of our modules against backbone of ResNet50 are 80 vs. 24.37M and\n4.7e-4 GFLOPs vs. 3.86 GFLOPs, respectively, and the performance boost is more\nthan 2% in terms of Top-1 accuracy. We extensively evaluate our ECA module on\nimage classification, object detection and instance segmentation with backbones\nof ResNets and MobileNetV2. The experimental results show our module is more\nefficient while performing favorably against its counterparts.",
            "authors": [
                "Qilong Wang",
                "Banggu Wu",
                "Pengfei Zhu",
                "Peihua Li",
                "Wangmeng Zuo",
                "Qinghua Hu"
            ]
        },
        {
            "title": "Contextual Transformer Networks for Visual Recognition",
            "arxiv": "2107.12292",
            "year": 2021,
            "url": "http://arxiv.org/abs/2107.12292v1",
            "abstract": "Transformer with self-attention has led to the revolutionizing of natural\nlanguage processing field, and recently inspires the emergence of\nTransformer-style architecture design with competitive results in numerous\ncomputer vision tasks. Nevertheless, most of existing designs directly employ\nself-attention over a 2D feature map to obtain the attention matrix based on\npairs of isolated queries and keys at each spatial location, but leave the rich\ncontexts among neighbor keys under-exploited. In this work, we design a novel\nTransformer-style module, i.e., Contextual Transformer (CoT) block, for visual\nrecognition. Such design fully capitalizes on the contextual information among\ninput keys to guide the learning of dynamic attention matrix and thus\nstrengthens the capacity of visual representation. Technically, CoT block first\ncontextually encodes input keys via a $3\\times3$ convolution, leading to a\nstatic contextual representation of inputs. We further concatenate the encoded\nkeys with input queries to learn the dynamic multi-head attention matrix\nthrough two consecutive $1\\times1$ convolutions. The learnt attention matrix is\nmultiplied by input values to achieve the dynamic contextual representation of\ninputs. The fusion of the static and dynamic contextual representations are\nfinally taken as outputs. Our CoT block is appealing in the view that it can\nreadily replace each $3\\times3$ convolution in ResNet architectures, yielding a\nTransformer-style backbone named as Contextual Transformer Networks (CoTNet).\nThrough extensive experiments over a wide range of applications (e.g., image\nrecognition, object detection and instance segmentation), we validate the\nsuperiority of CoTNet as a stronger backbone. Source code is available at\n\\url{https://github.com/JDAI-CV/CoTNet}.",
            "authors": [
                "Yehao Li",
                "Ting Yao",
                "Yingwei Pan",
                "Tao Mei"
            ]
        },
        {
            "title": "$A^2$-Nets: Double Attention Networks",
            "arxiv": "1810.11579",
            "year": 2018,
            "url": "http://arxiv.org/abs/1810.11579v1",
            "abstract": "Learning to capture long-range relations is fundamental to image/video\nrecognition. Existing CNN models generally rely on increasing depth to model\nsuch relations which is highly inefficient. In this work, we propose the\n\"double attention block\", a novel component that aggregates and propagates\ninformative global features from the entire spatio-temporal space of input\nimages/videos, enabling subsequent convolution layers to access features from\nthe entire space efficiently. The component is designed with a double attention\nmechanism in two steps, where the first step gathers features from the entire\nspace into a compact set through second-order attention pooling and the second\nstep adaptively selects and distributes features to each location via another\nattention. The proposed double attention block is easy to adopt and can be\nplugged into existing deep neural networks conveniently. We conduct extensive\nablation studies and experiments on both image and video recognition tasks for\nevaluating its performance. On the image recognition task, a ResNet-50 equipped\nwith our double attention blocks outperforms a much larger ResNet-152\narchitecture on ImageNet-1k dataset with over 40% less the number of parameters\nand less FLOPs. On the action recognition task, our proposed model achieves the\nstate-of-the-art results on the Kinetics and UCF-101 datasets with\nsignificantly higher efficiency than recent works.",
            "authors": [
                "Yunpeng Chen",
                "Yannis Kalantidis",
                "Jianshu Li",
                "Shuicheng Yan",
                "Jiashi Feng"
            ]
        },
        {
            "title": "ResT: An Efficient Transformer for Visual Recognition",
            "arxiv": "2105.13677",
            "year": 2021,
            "url": "http://arxiv.org/abs/2105.13677v5",
            "abstract": "This paper presents an efficient multi-scale vision Transformer, called ResT,\nthat capably served as a general-purpose backbone for image recognition. Unlike\nexisting Transformer methods, which employ standard Transformer blocks to\ntackle raw images with a fixed resolution, our ResT have several advantages:\n(1) A memory-efficient multi-head self-attention is built, which compresses the\nmemory by a simple depth-wise convolution, and projects the interaction across\nthe attention-heads dimension while keeping the diversity ability of\nmulti-heads; (2) Position encoding is constructed as spatial attention, which\nis more flexible and can tackle with input images of arbitrary size without\ninterpolation or fine-tune; (3) Instead of the straightforward tokenization at\nthe beginning of each stage, we design the patch embedding as a stack of\noverlapping convolution operation with stride on the 2D-reshaped token map. We\ncomprehensively validate ResT on image classification and downstream tasks.\nExperimental results show that the proposed ResT can outperform the recently\nstate-of-the-art backbones by a large margin, demonstrating the potential of\nResT as strong backbones. The code and models will be made publicly available\nat https://github.com/wofmanaf/ResT.",
            "authors": [
                "Qinglong Zhang",
                "Yubin Yang"
            ]
        },
        {
            "title": "Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks",
            "arxiv": "1905.09646",
            "year": 2019,
            "url": "http://arxiv.org/abs/1905.09646v2",
            "abstract": "The Convolutional Neural Networks (CNNs) generate the feature representation\nof complex objects by collecting hierarchical and different parts of semantic\nsub-features. These sub-features can usually be distributed in grouped form in\nthe feature vector of each layer, representing various semantic entities.\nHowever, the activation of these sub-features is often spatially affected by\nsimilar patterns and noisy backgrounds, resulting in erroneous localization and\nidentification. We propose a Spatial Group-wise Enhance (SGE) module that can\nadjust the importance of each sub-feature by generating an attention factor for\neach spatial location in each semantic group, so that every individual group\ncan autonomously enhance its learnt expression and suppress possible noise. The\nattention factors are only guided by the similarities between the global and\nlocal feature descriptors inside each group, thus the design of SGE module is\nextremely lightweight with \\emph{almost no extra parameters and calculations}.\nDespite being trained with only category supervisions, the SGE component is\nextremely effective in highlighting multiple active areas with various\nhigh-order semantics (such as the dog's eyes, nose, etc.). When integrated with\npopular CNN backbones, SGE can significantly boost the performance of image\nrecognition tasks. Specifically, based on ResNet50 backbones, SGE achieves\n1.2\\% Top-1 accuracy improvement on the ImageNet benchmark and 1.0$\\sim$2.0\\%\nAP gain on the COCO benchmark across a wide range of detectors\n(Faster/Mask/Cascade RCNN and RetinaNet). Codes and pretrained models are\navailable at https://github.com/implus/PytorchInsight.",
            "authors": [
                "Xiang Li",
                "Xiaolin Hu",
                "Jian Yang"
            ]
        },
        {
            "title": "Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks",
            "arxiv": "2105.02358",
            "year": 2021,
            "url": "http://arxiv.org/abs/2105.02358v2",
            "abstract": "Attention mechanisms, especially self-attention, have played an increasingly\nimportant role in deep feature representation for visual tasks. Self-attention\nupdates the feature at each position by computing a weighted sum of features\nusing pair-wise affinities across all positions to capture the long-range\ndependency within a single sample. However, self-attention has quadratic\ncomplexity and ignores potential correlation between different samples. This\npaper proposes a novel attention mechanism which we call external attention,\nbased on two external, small, learnable, shared memories, which can be\nimplemented easily by simply using two cascaded linear layers and two\nnormalization layers; it conveniently replaces self-attention in existing\npopular architectures. External attention has linear complexity and implicitly\nconsiders the correlations between all data samples. We further incorporate the\nmulti-head mechanism into external attention to provide an all-MLP\narchitecture, external attention MLP (EAMLP), for image classification.\nExtensive experiments on image classification, object detection, semantic\nsegmentation, instance segmentation, image generation, and point cloud analysis\nreveal that our method provides results comparable or superior to the\nself-attention mechanism and some of its variants, with much lower\ncomputational and memory costs.",
            "authors": [
                "Meng-Hao Guo",
                "Zheng-Ning Liu",
                "Tai-Jiang Mu",
                "Shi-Min Hu"
            ]
        },
        {
            "title": "EPSANet: An Efficient Pyramid Squeeze Attention Block on Convolutional Neural Network",
            "arxiv": "2105.14447",
            "year": 2021,
            "url": "http://arxiv.org/abs/2105.14447v2",
            "abstract": "Recently, it has been demonstrated that the performance of a deep\nconvolutional neural network can be effectively improved by embedding an\nattention module into it. In this work, a novel lightweight and effective\nattention method named Pyramid Squeeze Attention (PSA) module is proposed. By\nreplacing the 3x3 convolution with the PSA module in the bottleneck blocks of\nthe ResNet, a novel representational block named Efficient Pyramid Squeeze\nAttention (EPSA) is obtained. The EPSA block can be easily added as a\nplug-and-play component into a well-established backbone network, and\nsignificant improvements on model performance can be achieved. Hence, a simple\nand efficient backbone architecture named EPSANet is developed in this work by\nstacking these ResNet-style EPSA blocks. Correspondingly, a stronger\nmulti-scale representation ability can be offered by the proposed EPSANet for\nvarious computer vision tasks including but not limited to, image\nclassification, object detection, instance segmentation, etc. Without bells and\nwhistles, the performance of the proposed EPSANet outperforms most of the\nstate-of-the-art channel attention methods. As compared to the SENet-50, the\nTop-1 accuracy is improved by 1.93% on ImageNet dataset, a larger margin of\n+2.7 box AP for object detection and an improvement of +1.7 mask AP for\ninstance segmentation by using the Mask-RCNN on MS-COCO dataset are obtained.\nOur source code is available at:https://github.com/murufeng/EPSANet.",
            "authors": [
                "Hu Zhang",
                "Keke Zu",
                "Jian Lu",
                "Yuru Zou",
                "Deyu Meng"
            ]
        },
        {
            "title": "VOLO: Vision Outlooker for Visual Recognition",
            "arxiv": "2106.13112",
            "year": 2021,
            "url": "http://arxiv.org/abs/2106.13112v2",
            "abstract": "Visual recognition has been dominated by convolutional neural networks (CNNs)\nfor years. Though recently the prevailing vision transformers (ViTs) have shown\ngreat potential of self-attention based models in ImageNet classification,\ntheir performance is still inferior to that of the latest SOTA CNNs if no extra\ndata are provided. In this work, we try to close the performance gap and\ndemonstrate that attention-based models are indeed able to outperform CNNs. We\nfind a major factor limiting the performance of ViTs for ImageNet\nclassification is their low efficacy in encoding fine-level features into the\ntoken representations. To resolve this, we introduce a novel outlook attention\nand present a simple and general architecture, termed Vision Outlooker (VOLO).\nUnlike self-attention that focuses on global dependency modeling at a coarse\nlevel, the outlook attention efficiently encodes finer-level features and\ncontexts into tokens, which is shown to be critically beneficial to recognition\nperformance but largely ignored by the self-attention. Experiments show that\nour VOLO achieves 87.1% top-1 accuracy on ImageNet-1K classification, which is\nthe first model exceeding 87% accuracy on this competitive benchmark, without\nusing any extra training data In addition, the pre-trained VOLO transfers well\nto downstream tasks, such as semantic segmentation. We achieve 84.3% mIoU score\non the cityscapes validation set and 54.3% on the ADE20K validation set. Code\nis available at \\url{https://github.com/sail-sg/volo}.",
            "authors": [
                "Li Yuan",
                "Qibin Hou",
                "Zihang Jiang",
                "Jiashi Feng",
                "Shuicheng Yan"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9928925035629023,
        "task": "Image Classification",
        "task_prob": 0.9742725310353842
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "MSCOCO"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "Cityscapes"
            },
            {
                "name": "ILSVRC 2016"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "MS-COCO"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "PASCAL Context"
            },
            {
                "name": "ADE20K"
            },
            {
                "name": "COCO"
            }
        ]
    }
}