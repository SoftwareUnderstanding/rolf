{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "paper-dots",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "harshit158",
                "owner_type": "User",
                "name": "paper-dots",
                "url": "https://github.com/harshit158/paper-dots",
                "stars": 1,
                "pushed_at": "2021-12-10 23:23:01+00:00",
                "created_at": "2021-10-08 22:20:55+00:00",
                "language": "Jupyter Notebook",
                "license": "MIT License",
                "frameworks": [
                    "NLTK",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "9d6967ceeff0687a8131e8dcd2f1062e3961a0cd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/harshit158/paper-dots/blob/main/.gitignore"
                    }
                },
                "size": 1843
            },
            {
                "type": "code",
                "name": ".travis.yml",
                "sha": "e135355e810c5208bc2f2ab0c785d3979b954b4b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/harshit158/paper-dots/blob/main/.travis.yml"
                    }
                },
                "size": 155
            },
            {
                "type": "code",
                "name": ".vscode",
                "sha": "2108d54ca601388d2405c6a8fd23e6e3f80a0d71",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/harshit158/paper-dots/tree/main/.vscode"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "40e27959e61a866d27e67d302fcb6acb1d7e6433",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/harshit158/paper-dots/blob/main/LICENSE"
                    }
                },
                "size": 1071
            },
            {
                "type": "code",
                "name": "__init__.py",
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/harshit158/paper-dots/blob/main/__init__.py"
                    }
                },
                "size": 0
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "1b7766c4411a6e8a1923550233ef7533290f082b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/harshit158/paper-dots/tree/main/docs"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "input",
                "sha": "d23d7b9ad61a711d1677d2c51d61d39f3975171e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/harshit158/paper-dots/tree/main/input"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "notebooks",
                "sha": "6b475dd4b94226bccdd8c768df38cb76cecfb16f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/harshit158/paper-dots/tree/main/notebooks"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "output",
                "sha": "7badde09ed8cc1452462a9a935b77b08ee3184a5",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/harshit158/paper-dots/tree/main/output"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "9eb6253969df1145ae0fd380a0ba16dee4effcf3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/harshit158/paper-dots/blob/main/requirements.txt"
                    }
                },
                "size": 1359
            },
            {
                "type": "code",
                "name": "src",
                "sha": "001ab25125611a7e4e9ea796d14b703190f15dd1",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/harshit158/paper-dots/tree/main/src"
                    }
                },
                "num_files": 23
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "df94b4bc0ee695de867cc2f06cb1c82ffce67df7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/harshit158/paper-dots/tree/main/tests"
                    }
                },
                "num_files": 2
            }
        ]
    },
    "authors": [
        {
            "name": "Harshit Sharma",
            "email": "harshit158@gmail.com",
            "github_id": "harshit158"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/harshit158/paper-dots",
            "stars": 1,
            "issues": true,
            "readme": "<p align=\"center\">\n  <br>\n  <img  src=\"docs/logo3.png\" width=500>\n  </br>\n</p>\n\n<p align=\"center\">\n  <br>\n    <a href=\"https://travis-ci.com/harshit158/paper-dots\">\n        <img alt=\"Build\" src=\"https://travis-ci.com/harshit158/paper-dots.svg?branch=main\">\n    </a>\n    <a href=\"https://img.shields.io/github/issues/harshit158/paper-dots\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/issues/harshit158/paper-dots\">\n    </a>\n    <a href=\"https://img.shields.io/github/license/harshit158/paper-dots\">\n        <img alt=\"License\" src=\"https://img.shields.io/github/license/harshit158/paper-dots\">\n    </a>\n  </br>\n</p>\n\n## What is Paper Dots ?\nPaper Dots is an automatic insights extraction tool from research papers, which \n* Automatically annotates a research paper PDF with important keyphrases, ensuring faster skim-reading of papers\n* Builds cumulative Knowledge Graph on top of papers read so far, helping in tracking important concepts\n* Delivers relevant papers continuously through mail, promoting consistent and directed learning\n\nThe end-to-end pipeline is shown below:\n\n<p align=\"center\">\n  <img  src=\"docs/pipeline.png\">\n</p>\n\n## Approach\nThere are 3 main components to the project:\n1) **Keyphrase Extraction**  \nImplemented using Constituency Parsing (using AllenNLP pretrained model) followed by a rule based engine to refine the extracted keyphrases  \n\nComing Soon:\n* Keyphrase extraction from entire paper and not just the abstract\n* Further division of identified keyphrases into domain specific entities like Datasets, References, Algorithms, Metrics etc\n\n<p align=\"center\">\n  <img  src=\"docs/annotated.png\" width=600>\n</p>\n\n2) **Knowledge Graph construction**  \nImplemented using Open Information Extraction (OPENIE pretrained model from AllenNLP). Extracted SVO triplets followed by refining, to generate the final nodes and edges for the knowledge graph.\n\n<p align=\"center\">\n  <img  src=\"docs/knowledge_graph_demo.gif\">\n</p>\n\n3) **Paper sampling**\n\nThe papers are sampled from [Arxiv corpus](https://www.kaggle.com/Cornell-University/arxiv) (hosted on Kaggle). To enable semantic search over the papers, we had to first obtain the embeddings for each of the papers in the corpus, for which we used [Sentence-Transformers](https://github.com/UKPLab/sentence-transformers).  \nThe corpus embeddings are available and can be downloaded from [here](https://drive.google.com/file/d/1EDdcti5J0y4L1jvuiEdpKAHDkGfJf7LT/view?usp=sharing) for research purposes.  \nOnce the corpus embeddings are in place, a new paper can be sampled from the corpus using the seed paper as follows:\n\n\n<p align=\"center\">\n  <img  src=\"docs/paper-sampling.png\">\n</p>\n\n\n## Code Structure\n> \n\n    Paper-Dots\n    \n    \u251c\u2500\u2500 docs\n    \u251c\u2500\u2500 tests\n    \u251c\u2500\u2500 output\n    \u251c\u2500\u2500 LICENSE\n    \u251c\u2500\u2500 README\n    \u251c\u2500\u2500 src\n    |   \u251c\u2500\u2500 config.py\n    |   \u251c\u2500\u2500 information_extraction.py                     # Driver of Information Extraction pipeline\n    |   \u251c\u2500\u2500 extractor.py\n    |   \u251c\u2500\u2500 constituency_parser.py\n    |   \u251c\u2500\u2500 mail_sender.py\n    |   \u251c\u2500\u2500 model_loader.py\n    |   \u251c\u2500\u2500 mongo_utils.py\n    |   \u251c\u2500\u2500 paper_walk.py\n    |   \u251c\u2500\u2500 task_keyphrase_extraction.py                  # Task 1\n    |   \u251c\u2500\u2500 task_knowledge_graph.py                       # Task 2\n    |   \u251c\u2500\u2500 utils.py\n    \u2502   \u251c\u2500\u2500 paper_sampler                                 \n    |   |   \u251c\u2500\u2500 app.py                                    # Flask App\n    |   |   \u251c\u2500\u2500 Dockerfile\n    |   |   \u251c\u2500\u2500 paper_sampler.py\n    |   |   \u251c\u2500\u2500 utils.py\n    |   |   \u251c\u2500\u2500 requirements.txt\n    |   |   \u251c\u2500\u2500 data\n    |   |   |   \u251c\u2500\u2500 corpus_embeddings.hdf5                # Embeddings of Arxiv dataset (5.5 GB)\n    |   |   |   \u251c\u2500\u2500 corpus_ids.pkl                        # Corresponding IDs of the paper\n    \n    \n    \n\n## How to use ?\nCurrently, the end-to-end pipeline is only configured for personal use, but we are working on it to make it available for public.\nHowever, you can send a mail to **paperdotsai@gmail.com** with the link of your seed paper, and we will onboard you in the next iteration.\n\nThe individual tasks of the Information Extraction sub-pipeline, however, can be used as follows:\n\n**Keyphrase Extraction**:  \n```\npython task_keyphrase_extraction.py -fp https://arxiv.org/abs/1706.03762\n```\nAll the options are as follows:\n```\n-fp [--filepath]:       This is the path to the research paper. Can be URL (both abs and pdf links are supported) or local path\n-ca [--clip_abstract]:  If true, clips the annotated abstract as an image file and doesnt do the annotation of entire PDF\n-sa [--save_abstract]:  If true, saves the annotated image at ANNOTATE_FILEPATH in config\n```\n\n**Knowledge Graph**:  \n```\npython task_knowledge_graph.py -fp https://arxiv.org/abs/1706.03762\n```\nAll the options are as follows:\n```\n-fp [--filepath]:       This is the path to the research paper. Can be URL (both abs and pdf links are supported) or local path\n```\n\n## How to contribute ?\nFeel free to raise requests for new features :)\n\n## Contact\n**paperdotsai@gmail.com**",
            "readme_url": "https://github.com/harshit158/paper-dots",
            "frameworks": [
                "NLTK",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Attention Is All You Need",
            "arxiv": "1706.03762",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.03762v5",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Arxiv corpus",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://www.kaggle.com/Cornell-University/arxiv"
                    }
                }
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9985352042612324,
        "task": "Machine Translation",
        "task_prob": 0.8718360304878024
    }
}