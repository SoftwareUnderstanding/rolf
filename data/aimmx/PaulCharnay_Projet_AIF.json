{
    "visibility": {
        "visibility": "public"
    },
    "name": "Reinforcement Learning: solving MsPacman with Actor-Critic algorithms",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "PaulCharnay",
                "owner_type": "User",
                "name": "Projet_AIF",
                "url": "https://github.com/PaulCharnay/Projet_AIF",
                "stars": 0,
                "pushed_at": "2020-01-22 15:19:47+00:00",
                "created_at": "2020-01-22 14:53:07+00:00",
                "language": "Jupyter Notebook",
                "frameworks": []
            },
            {
                "type": "code",
                "name": "show_videos.ipynb",
                "sha": "0e26eb7ea7d11f98e2a5817ed938adb5614a2daf",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/PaulCharnay/Projet_AIF/blob/master/show_videos.ipynb"
                    }
                },
                "size": 12077121
            },
            {
                "type": "code",
                "name": "train_a2c.py",
                "sha": "b7c961dc23b7381f87ab0f290a741a0b58d21cd8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/PaulCharnay/Projet_AIF/blob/master/train_a2c.py"
                    }
                },
                "size": 1439
            },
            {
                "type": "code",
                "name": "train_acer.py",
                "sha": "a6eec01a0c972355ff3a699a26483cc51f6b3c8e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/PaulCharnay/Projet_AIF/blob/master/train_acer.py"
                    }
                },
                "size": 1647
            },
            {
                "type": "code",
                "name": "video_a2c.py",
                "sha": "e5c6d5973cebdaed5f2d091121e8debfc1e69663",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/PaulCharnay/Projet_AIF/blob/master/video_a2c.py"
                    }
                },
                "size": 1302
            },
            {
                "type": "code",
                "name": "video_acer.py",
                "sha": "b9a6ffe2ebe5bf227c533048de74473f7dea53b2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/PaulCharnay/Projet_AIF/blob/master/video_acer.py"
                    }
                },
                "size": 1376
            },
            {
                "type": "code",
                "name": "videos",
                "sha": "9f8cdf0973bd31614ec3cdda7f7c61f1c5e54415",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/PaulCharnay/Projet_AIF/tree/master/videos"
                    }
                },
                "num_files": 2
            }
        ]
    },
    "authors": [
        {
            "name": "Paul Charnay",
            "github_id": "PaulCharnay"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/PaulCharnay/Projet_AIF",
            "stars": 0,
            "issues": true,
            "readme": "# Reinforcement Learning: solving MsPacman with Actor-Critic algorithms\r\nThis repository contains Python files to solve MsPacman using Actor-Critic algorithms (A2C and ACER).\r\n\r\nIt was created by the following students from INSA Toulouse:\r\n- Camille Bichet\r\n- Paul Charnay\r\n- Oumaima Dahan\r\n- Louis Delvaux\r\n- Emmeline Mon\u00e9di\u00e8res\r\n\r\n## Useful links\r\n- Actor-Critic papers: \r\n    - A2C: https://arxiv.org/abs/1602.01783\r\n    - ACER: https://arxiv.org/abs/1611.01224\r\n\r\n- Baselines blog on Actor-Critic: https://blog.openai.com/baselines-acktr-a2c/\r\n\r\n- MsPacman on OpenAI Gym: https://gym.openai.com/envs/MsPacman-v0/\r\n\r\n- Stable-baselines repository: https://github.com/hill-a/stable-baselines\r\n\r\n## Files\r\n- `train_a2c.py`: Train an A2C model for MsPacman.\r\n- `train_acer.py`: Train an A2C model for MsPacman.\r\n- `video_a2c.py`: Record a video of the trained A2C agent playing MsPacman.\r\n- `video_acer.py`: Record a video of the trained ACER agent playing MsPacman.\r\n- `show_videos.ipynb`: notebook showing the recorded videos.\r\n\r\n## Requirements\r\nThe following python libraries are required:\r\n- gym\r\n- stable-baselines",
            "readme_url": "https://github.com/PaulCharnay/Projet_AIF",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Asynchronous Methods for Deep Reinforcement Learning",
            "arxiv": "1602.01783",
            "year": 2016,
            "url": "http://arxiv.org/abs/1602.01783v2",
            "abstract": "We propose a conceptually simple and lightweight framework for deep\nreinforcement learning that uses asynchronous gradient descent for optimization\nof deep neural network controllers. We present asynchronous variants of four\nstandard reinforcement learning algorithms and show that parallel\nactor-learners have a stabilizing effect on training allowing all four methods\nto successfully train neural network controllers. The best performing method,\nan asynchronous variant of actor-critic, surpasses the current state-of-the-art\non the Atari domain while training for half the time on a single multi-core CPU\ninstead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds\non a wide variety of continuous motor control problems as well as on a new task\nof navigating random 3D mazes using a visual input.",
            "authors": [
                "Volodymyr Mnih",
                "Adri\u00e0 Puigdom\u00e8nech Badia",
                "Mehdi Mirza",
                "Alex Graves",
                "Timothy P. Lillicrap",
                "Tim Harley",
                "David Silver",
                "Koray Kavukcuoglu"
            ]
        },
        {
            "title": "Sample Efficient Actor-Critic with Experience Replay",
            "arxiv": "1611.01224",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.01224v2",
            "abstract": "This paper presents an actor-critic deep reinforcement learning agent with\nexperience replay that is stable, sample efficient, and performs remarkably\nwell on challenging environments, including the discrete 57-game Atari domain\nand several continuous control problems. To achieve this, the paper introduces\nseveral innovations, including truncated importance sampling with bias\ncorrection, stochastic dueling network architectures, and a new trust region\npolicy optimization method.",
            "authors": [
                "Ziyu Wang",
                "Victor Bapst",
                "Nicolas Heess",
                "Volodymyr Mnih",
                "Remi Munos",
                "Koray Kavukcuoglu",
                "Nando de Freitas"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "OpenAI Gym"
            }
        ]
    },
    "domain": {
        "domain_type": "Playing Games",
        "domain_prob": 0.9893562781083987
    }
}