{
    "visibility": {
        "visibility": "public"
    },
    "name": "focal-loss",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "unsky",
                "owner_type": "User",
                "name": "focal-loss",
                "url": "https://github.com/unsky/focal-loss",
                "stars": 474,
                "pushed_at": "2017-12-20 04:30:15+00:00",
                "created_at": "2017-08-13 09:30:47+00:00",
                "language": "Python",
                "description": "Focal loss for Dense Object Detection",
                "frameworks": [
                    "MXNet"
                ]
            },
            {
                "type": "code",
                "name": ".gitmodules",
                "sha": "1a3dc2f535ca82fb762ac769cc746c4180a159ed",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/unsky/focal-loss/blob/master/.gitmodules"
                    }
                },
                "size": 103
            },
            {
                "type": "code",
                "name": "faster_rcnn_mxnet",
                "sha": "69688a47fe984565d4e12e9ecd44184e1ed75ccc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/unsky/focal-loss/tree/master/faster_rcnn_mxnet"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "focal_loss.py",
                "sha": "26d876f72a7f5c8c2c1605e659d91125961f2849",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/unsky/focal-loss/blob/master/focal_loss.py"
                    }
                },
                "size": 2865
            }
        ]
    },
    "authors": [
        {
            "name": "unsky",
            "github_id": "unsky"
        }
    ],
    "tags": [],
    "description": "Focal loss for Dense Object Detection",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/unsky/focal-loss",
            "stars": 474,
            "issues": true,
            "readme": "# focal-loss\n\nThe code is unofficial version for `focal loss for Dense Object Detection`.\n https://arxiv.org/abs/1708.02002\n\nthis is implementtd using mxnet python layer.\n\nThe retina-net is in https://github.com/unsky/RetinaNet\n\n\n\n# usage\nAssue that you have put the focal_loss.py in your operator path\n\nyou can use:\n\n```\nfrom your_operators.focal_loss import *\n\ncls_prob = mx.sym.Custom(op_type='FocalLoss', name = 'cls_prob', data = cls_score, labels = label, alpha =0.25, gamma= 2)\n\n```\n\n# focal loss with softmax on kitti(10 cls)\nthis is my experiments on kitti 10 cls, the performance on hard cls is great!!\n\n| method@0.7                           | car           | van   | Truck |cyclist |pedestrian|person_sitting|tram  |misc  |dontcare|\n| -------------                        |:-------------:| -----:| -----:| ------:|---------:|-------------:|-----:|-----:|-------:|\n| base line(faster rcnn + ohem(1:2))   |      0.7892   |0.7462 |0.8465 |0.623   |0.4254    |0.1374        |0.5035|0.5007|0.1329  |\n| faster rcnn + focal loss with softmax|      0.797    |0.874  | 0.8959|0.7914  |0.5700    |0.2806        |0.7884|0.7052|0.1433  |\n\n![image](https://github.com/unsky/focal-loss/blob/master/readme/res.png)\n\n#### about parameters in this expriment\nhttps://github.com/unsky/focal-loss/issues/5\n\n\n# note!!\n\n## very important!!!\n\n~~in my experiment, i have to use the strategy in  `paper section 3.3`.~~\n\n~~LIKE:~~\n\n![image](https://github.com/unsky/focal-loss/blob/master/readme/loss1.png)\n\n~~Uder such an initialization, in the presence of class imbalance, the loss due to the frequent class can dominate total loss and cause instability in early training.~~\n \n\n\n\n~~##AND YOU CAN TRY MY INSTEAD STRATEGY:~~\n\n~~train the model using the classical softmax for several times (for examples 3 in kitti dataset)~~\n\n~~choose a litti learning rate:~~\n\n~~and the traing loss will work well:~~\n\n![image](https://github.com/unsky/focal-loss/blob/master/readme/loss2.png)\n## about alpha\n\nhttps://github.com/unsky/focal-loss/issues/4\n\n## now focal loss with softmax work well\n\n\nfocal loss value is not used in focal_loss.py, becayse we should forward the cls_pro in this layer,\nthe major task of focal_loss.py is to backward the focal loss gradient.\n\nthe focal loss vale should be calculated in metric.py and  use normalization in it.\n\nand this layer is not support `use_ignore`\n\nfor example :\n\n```python\nclass RCNNLogLossMetric(mx.metric.EvalMetric):\n    def __init__(self, cfg):\n        super(RCNNLogLossMetric, self).__init__('RCNNLogLoss')\n        self.e2e = cfg.TRAIN.END2END\n        self.ohem = cfg.TRAIN.ENABLE_OHEM\n        self.pred, self.label = get_rcnn_names(cfg)\n\n    def update(self, labels, preds):\n        pred = preds[self.pred.index('rcnn_cls_prob')]\n        if self.ohem or self.e2e:\n            label = preds[self.pred.index('rcnn_label')]\n        else:\n            label = labels[self.label.index('rcnn_label')]\n\n        last_dim = pred.shape[-1]\n        pred = pred.asnumpy().reshape(-1, last_dim)\n        label = label.asnumpy().reshape(-1,).astype('int32')\n\n        # filter with keep_inds\n        keep_inds = np.where(label != -1)[0]\n        label = label[keep_inds]\n        cls = pred[keep_inds, label]\n\n        cls += 1e-14\n        gamma = 2\n        alpha = 0.25\n\n        cls_loss = alpha*(-1.0 * np.power(1 - cls, gamma) * np.log(cls))\n\n        cls_loss = np.sum(cls_loss)/len(label)\n        #print cls_loss\n        self.sum_metric += cls_loss\n        self.num_inst += label.shape[0]\n\n```\n# the value must like\n## forward value\n![image](https://github.com/unsky/focal-loss/blob/master/readme/forward.png)\n## backward gradient value\n![image](https://github.com/unsky/focal-loss/blob/master/readme/back_cure.png)\n\nyou can check the gradient value in your debug(if need).\nBy the way\n\nthis is my derivation about backward, if it has mistake, please note to me.\n\n# softmax activation:\n\n![image](https://github.com/unsky/focal-loss/blob/master/readme/2.jpg)\n\n# cross entropy with softmax\n\n![image](https://github.com/unsky/focal-loss/blob/master/readme/3.jpg)\n\n# Focal loss with softmax\n\n![image](https://github.com/unsky/focal-loss/blob/master/readme/1.jpg)\n\n\n",
            "readme_url": "https://github.com/unsky/focal-loss",
            "frameworks": [
                "MXNet"
            ]
        }
    ],
    "references": [
        {
            "title": "Focal Loss for Dense Object Detection",
            "arxiv": "1708.02002",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.02002v2",
            "abstract": "The highest accuracy object detectors to date are based on a two-stage\napproach popularized by R-CNN, where a classifier is applied to a sparse set of\ncandidate object locations. In contrast, one-stage detectors that are applied\nover a regular, dense sampling of possible object locations have the potential\nto be faster and simpler, but have trailed the accuracy of two-stage detectors\nthus far. In this paper, we investigate why this is the case. We discover that\nthe extreme foreground-background class imbalance encountered during training\nof dense detectors is the central cause. We propose to address this class\nimbalance by reshaping the standard cross entropy loss such that it\ndown-weights the loss assigned to well-classified examples. Our novel Focal\nLoss focuses training on a sparse set of hard examples and prevents the vast\nnumber of easy negatives from overwhelming the detector during training. To\nevaluate the effectiveness of our loss, we design and train a simple dense\ndetector we call RetinaNet. Our results show that when trained with the focal\nloss, RetinaNet is able to match the speed of previous one-stage detectors\nwhile surpassing the accuracy of all existing state-of-the-art two-stage\ndetectors. Code is at: https://github.com/facebookresearch/Detectron.",
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9513118656524835,
        "task": "Object Detection",
        "task_prob": 0.7321635016150116
    }
}