{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Race Events Recognition",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "sulasen",
                "owner_type": "User",
                "name": "race-events-recognition-1",
                "url": "https://github.com/sulasen/race-events-recognition-1",
                "stars": 0,
                "pushed_at": "2019-01-10 11:35:50+00:00",
                "created_at": "2019-01-14 14:35:47+00:00",
                "language": "Jupyter Notebook",
                "description": "Predicting meaningful events in the car racing footage",
                "license": "MIT License",
                "frameworks": [
                    "Keras",
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "5e82b2ca2fb133360548f38b9a6dc980153645c6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sulasen/race-events-recognition-1/blob/master/.gitignore"
                    }
                },
                "size": 218
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "f2ae3684208507a5fa8830bff979c6c39aa20be1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sulasen/race-events-recognition-1/blob/master/LICENSE"
                    }
                },
                "size": 1135
            },
            {
                "type": "code",
                "name": "combined-training.ipynb",
                "sha": "c6f64c2957636df7829114e1f155f437ecbb1c2c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sulasen/race-events-recognition-1/blob/master/combined-training.ipynb"
                    }
                },
                "size": 86078
            },
            {
                "type": "code",
                "name": "config.pickle",
                "sha": "d71a67f39c855490739fd208ad6729f3b355dee9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sulasen/race-events-recognition-1/blob/master/config.pickle"
                    }
                },
                "size": 810
            },
            {
                "type": "code",
                "name": "content",
                "sha": "afd329773d95129d7e3e0ee31af70eef97018df7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sulasen/race-events-recognition-1/tree/master/content"
                    }
                },
                "num_files": 12
            },
            {
                "type": "code",
                "name": "notebooks",
                "sha": "e867c4f47b9d22ae0080c1a4467a92e32140c573",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sulasen/race-events-recognition-1/tree/master/notebooks"
                    }
                },
                "num_files": 13
            },
            {
                "type": "code",
                "name": "python",
                "sha": "178305ea43a3effb4bbac42755adc0ea35001c28",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sulasen/race-events-recognition-1/tree/master/python"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "677cae29aed64a02f5df0d780731f9a58644dbba",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sulasen/race-events-recognition-1/blob/master/requirements.txt"
                    }
                },
                "size": 218
            },
            {
                "type": "code",
                "name": "research",
                "sha": "8132cf4f65351a6896283fe9f5cbfc0c255477a9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sulasen/race-events-recognition-1/tree/master/research"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "utils",
                "sha": "6e7b4a410991921dd4703ce349e503648bd6fb10",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sulasen/race-events-recognition-1/tree/master/utils"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "video-pipeline.ipynb",
                "sha": "96af34a8d8e283f5a1827a52e86426de2862ba16",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sulasen/race-events-recognition-1/blob/master/video-pipeline.ipynb"
                    }
                },
                "size": 8855
            }
        ]
    },
    "authors": [
        {
            "name": "Yana Valieva",
            "email": "yyvalieva@gmail.com",
            "github_id": "vJenny"
        }
    ],
    "tags": [],
    "description": "Predicting meaningful events in the car racing footage",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/sulasen/race-events-recognition-1",
            "stars": 0,
            "issues": false,
            "readme": "# Race Events Recognition\nPredicting meaningful events in the car racing footage using three-path approach. \n\n## Table of Contents\n\n* [Project](#race-events-recognition)  \n* [Table of Contents](#table-of-contents)  \n* [Overview](#overview)  \n  * [Description](#description)  \n  * [Inspiration](#inspiration)\n  * [Solution Overview](#solution-overview)  \n* [Prerequisites](#prerequisites)\n  * [Tools](#tools)\n  * [Libraries](#libraries)\n* [Experiments](#experiments)\n  * [Dense Optical Flow](#dense-optical-flow)\n  * [Sparse Optical Flow](#sparse-optical-flow)\n  * [Focused Optical Flow](#focused-optical-flow)\n  * [Embeddings](#embeddings)\n  * [Self-Similarity Matrix](#self-similarity-matrix)\n  * [Camera Views Classification](#camera-views-classification)\n* [Usage](#usage)\n* [Credits](#credits)\n\n## Overview   \n\n### Description\nThis project is dedicated to the investigation of methods for predicting\nmeaningful events in footage of car racing. This repository is focused on the\nexploration of **collision detection** but contains a tool for the classification of  as well. During the work on this project we've also developed a\n**monadic pipeline** library [mPyPl](https://github.com/shwars/mPyPl) to\nsimplify tasks of data processing and creating complex data pipelines.   \nDue to the small amount of data in this problem; we could not rely on neural\nnetworks to learn representations as part of the training process. Instead; we\nneeded to design bespoke features, crafted with domain knowledge. After series\nof experiments, we've created a model based on features obtained using three\ndifferent approaches: \n\n* Dense Optical Flow\n* VGG16 embeddings \n* A special kind of Optical Flow - [Focused Optical Flow](#focused-optical-flow).  \n\n\n\u2139\ufe0f *After the release of mPyPl as a independent [pip-installable framework](https://pypi.org/project/mPyPl/), some experimental notebooks in the ```notebooks``` folder have not been updated, but may contain interesting things to explore.*\n\n### Inspiration\n\n* [Two-stream ConvNets for Action Recognition in Videos\u200b](http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf) by Keren Simonyan, Andrew Zisserman\n* [Review of Action Recognition and Detection Methods\u200b](https://arxiv.org/abs/1610.06906.pdf) by Soo Min Kang, Richard P. Wildes\n* [Focal Loss for Dense Object Detection\u200b](https://arxiv.org/abs/1708.02002) by Tsung-Yi Lin, et. al.\n* [Using Optical Flow for Stabilizing Image Sequences](http://www.cs.utoronto.ca/~donovan/stabilization/stabilization.pdf) by Peter O\u2019Donovan\n\n### Solution overview\nOur solution consists of the three main paths (see illustration below). A video\nis fed into the three algorithms in parallel mode. Each of them will be\ndescribed in details in the [Experiments](#experiments) section. Each output of\nthe three paths is processed by a separate neural network and then the results\nare combined for the final prediction.  \n<p align=\"center\">\n<img src=\"content/ml-workflow.png\" height=\"450\">\n</p>\n\nThis project was preceded by another one dedicated to the detection of racing cars using RetinaNet. RetinaNet was trained on [Azure Batch AI](https://azure.microsoft.com/en-us/services/batch-ai/) using [Horovod](https://github.com/uber/horovod) for distributed training. In the near future, we plan to implement the model inference using [Azure Batch](https://docs.microsoft.com/en-us/azure/batch/) to reduce the time spent on video processing.\n<p align=\"center\">\n  <img src=\"content/ml-cloud.png\" height=\"250\">\n</p>\n\n## Prerequisites\n\n### Tools\n\n* [Azure DSVM](https://azure.microsoft.com/en-gb/services/virtual-machines/data-science-virtual-machines/)\n* [Jupyter Notebooks](http://jupyter.org/)\n\n### Libraries\nIn addition to the standard set of data science packages, we've used the following:\n* [Keras on Tensorflow](https://keras.io/)\n* [opencv-python](https://github.com/skvark/opencv-python)\n* [mPyPl](https://github.com/shwars/mPyPl)\n* [keras-retinanet](https://github.com/fizyr/keras-retinanet)\n\nTo successfully run the **collision recognition** examples, you need to install all the requirements using \n```bash\npip install -r requirements.txt\n``` \n and clone all the content of [keras-retinanet](https://github.com/fizyr/keras-retinanet) repository to ```research/retina``` folder.  \n ```bash\ngit clone https://github.com/fizyr/keras-retinanet.git\nmv keras-retinanet/keras_retinanet/ race-events-recognition/research/retina/ \n```  \nTo run the **scene detection** example, you need to have installed:\n* [Visual Studio 2017 Version 15.7.4 or Newer](https://developer.microsoft.com/en-us/windows/downloads)\n* [Windows 10 - Build 17738 or higher](https://www.microsoft.com/en-us/software-download/windowsinsiderpreviewiso)\n* [Windows SDK - Build 17738 or higher](https://www.microsoft.com/en-us/software-download/windowsinsiderpreviewSDK)\n* [Win2D](https://github.com/Microsoft/Win2D)\n\n## Experiments\n\n### Dense Optical Flow\n\nOur Dense Optical Flow approach originated from [this](https://docs.opencv.org/3.4/d7/d8b/tutorial_py_lucas_kanade.html) tutorial. It is a complete vector field showing movement of every pixel between frames\u200b. Such features can show not only changes in the movement of the car, but also the style of the camera operator, which may be different during a normal race and an accident. \n<p align=\"center\">\n  <img src=\"content/dense1.jpg\" height=\"82\">\n</p> \n<p align=\"center\">\n  <img src=\"content/dense2.jpg\" height=\"82\">\n</p>  \n\nWe've also applied a technique of video stabilization described by Peter O\u2019Donovan in his [article](http://www.cs.utoronto.ca/~donovan/stabilization/stabilization.pdf). \n<p align=\"center\">\n  <img src=\"content/dense3.jpg\" height=\"82\">\n</p>  \n\nIn general, the process consists of the following steps: \n<p align=\"center\">\n  <img src=\"content/denseflow.png\" height=\"110\">\n</p>\n\n### Sparse Optical Flow\n\nThe strategy of Sparse Optical Flow is based on [the same](https://docs.opencv.org/3.4/d7/d8b/tutorial_py_lucas_kanade.html) example. The main idea was to use the changes in the trajectory of a car as input features for the model. Our experiments showed that the flow for normal situation is different from the flow when an accident occurs. \n<p align=\"center\">\n  <img src=\"content/optical1.jpg\" height=\"100\">\n</p>\n<p align=\"center\">\n  <img src=\"content/optical2.jpg\" height=\"100\">\n</p>\nOk, looks good, right?    \nOn the other hand, sometimes the standard Optical Flow approach tracks anything but the cars. The Optical Flow algorithm starts with detection of good features to track (regardless of the semantic of the frame) - just edge detection. Therefore, sometimes we do not get the flow of a car, but something extraneous. On the picture below, the algorithm is trying to track the scoreboard.  \n  \n<p align=\"center\">\n  <img src=\"content/optical3.jpg\" height=\"102\">\n</p>\nThe improved Focused Optical Flow algorithm helps us to eliminate this drawback.\n\n### Focused Optical Flow\nThe main idea of the Focused Optical Flow is based on providing the standard Optical Flow algorithm with the correct areas of interest. For this purpose, we use a trained RetinaNet object detector. Focusing on the detected areas with cars, the algorithm can select appropriate points for tracking. \n<p align=\"center\">\n  <img src=\"content/optical4.jpg\" height=\"102\">\n</p>\nSo, the whole pipeline for the Focused Optical Flow looks is shown on the figure below and in general, the post-flow steps are similar to the Dense Flow approach:\n<p align=\"center\">\n  <img src=\"content/focusedflow.png\" height=\"110\">\n</p>\n\n### Self-Similarity Matrix\n\nInitially we used a [Self-Similarity\nMatrix](https://en.wikipedia.org/wiki/Self-similarity_matrix) of cosines of\nnormalized VGG16 embeddings frames as a 2d feature for a CNN encoder.\nTheoretically; it's telling us about the regionality and structure of the video\nas a function of co-activation in the convolved embedding space. We intend to\nuse this an additional input to the overall model as it will likely capture some\nuseful temporal information about scene structure. \n\n![alt text](content/self-sim.jpg \"Self similarity matrix\")\n\n\n### Embeddings\nIn this case we use pretrained [VGG16](https://keras.io/applications/#vgg16) model to extract features for each frame of the video. After extraction, the features are stacked into two-dimensional vectors and fed into the CNN.  \n\n### Camera Views Classification\nThe application gives you the ability to score images and video files frame by frame based on onnx model. The model was trained using [Custom Vision](https://www.customvision.ai/) service and exported to be used in UWP APP written on C#. The app is based on [example](https://github.com/Microsoft/Windows-Machine-Learning) of Windows ML SDK from and extended to use with video files.  \nThere are 6 different classes the model works with: \n1. Pitstops \n2. 1st party view \n3. 3d party view \n4. Command center \n5. People\n6. 1st party back view\n\n## Usage\n* The **training** process is described in the ```combined-training.ipynb``` notebook. \n* To run the **inference** process use the ```video-pipeline.ipynb``` notebook.\n* All the working notebooks with our **experiments** are in ```notebooks``` folder (even though some notebooks are outdated, they contain interesting ideas).\n* The ```research``` folder contains our main **python modules**.  \n* The ```utils``` folder contains different **useful things** (e.g. visualization tools). \n\n\n\n## Credits\nProject team:\n* [Dmitry Soshnikov](https://github.com/shwars)\n* [Yana Valieva](https://github.com/vJenny)\n* [Tim Scarfe](https://github.com/ecsplendid)\n* [Evgeny Grigorenko](https://github.com/evgri243)\n* [Victor Kiselev](https://github.com/Gaploid)",
            "readme_url": "https://github.com/sulasen/race-events-recognition-1",
            "frameworks": [
                "Keras",
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Review of Action Recognition and Detection Methods",
            "arxiv": "1610.06906",
            "year": 2016,
            "url": "http://arxiv.org/abs/1610.06906v2",
            "abstract": "In computer vision, action recognition refers to the act of classifying an\naction that is present in a given video and action detection involves locating\nactions of interest in space and/or time. Videos, which contain photometric\ninformation (e.g. RGB, intensity values) in a lattice structure, contain\ninformation that can assist in identifying the action that has been imaged. The\nprocess of action recognition and detection often begins with extracting useful\nfeatures and encoding them to ensure that the features are specific to serve\nthe task of action recognition and detection. Encoded features are then\nprocessed through a classifier to identify the action class and their spatial\nand/or temporal locations. In this report, a thorough review of various action\nrecognition and detection algorithms in computer vision is provided by\nanalyzing the two-step process of a typical action recognition and detection\nalgorithm: (i) extraction and encoding of features, and (ii) classifying\nfeatures into action classes. In efforts to ensure that computer vision-based\nalgorithms reach the capabilities that humans have of identifying actions\nirrespective of various nuisance variables that may be present within the field\nof view, the state-of-the-art methods are reviewed and some remaining problems\nare addressed in the final chapter.",
            "authors": [
                "Soo Min Kang",
                "Richard P. Wildes"
            ]
        },
        {
            "title": "Focal Loss for Dense Object Detection",
            "arxiv": "1708.02002",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.02002v2",
            "abstract": "The highest accuracy object detectors to date are based on a two-stage\napproach popularized by R-CNN, where a classifier is applied to a sparse set of\ncandidate object locations. In contrast, one-stage detectors that are applied\nover a regular, dense sampling of possible object locations have the potential\nto be faster and simpler, but have trailed the accuracy of two-stage detectors\nthus far. In this paper, we investigate why this is the case. We discover that\nthe extreme foreground-background class imbalance encountered during training\nof dense detectors is the central cause. We propose to address this class\nimbalance by reshaping the standard cross entropy loss such that it\ndown-weights the loss assigned to well-classified examples. Our novel Focal\nLoss focuses training on a sparse set of hard examples and prevents the vast\nnumber of easy negatives from overwhelming the detector during training. To\nevaluate the effectiveness of our loss, we design and train a simple dense\ndetector we call RetinaNet. Our results show that when trained with the focal\nloss, RetinaNet is able to match the speed of previous one-stage detectors\nwhile surpassing the accuracy of all existing state-of-the-art two-stage\ndetectors. Code is at: https://github.com/facebookresearch/Detectron.",
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ]
        },
        {
            "title": "Two-stream ConvNets for Action Recognition in Videos\u200b",
            "url": "http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf"
        },
        {
            "title": "Using Optical Flow for Stabilizing Image Sequences",
            "url": "http://www.cs.utoronto.ca/~donovan/stabilization/stabilization.pdf"
        },
        {
            "title": "Azure DSVM",
            "url": "https://azure.microsoft.com/en-gb/services/virtual-machines/data-science-virtual-machines/"
        },
        {
            "title": "Jupyter Notebooks",
            "url": "http://jupyter.org/"
        },
        {
            "title": "Keras on Tensorflow",
            "url": "https://keras.io/"
        },
        {
            "title": "opencv-python",
            "url": "https://github.com/skvark/opencv-python"
        },
        {
            "title": "mPyPl",
            "url": "https://github.com/shwars/mPyPl"
        },
        {
            "title": "keras-retinanet",
            "url": "https://github.com/fizyr/keras-retinanet"
        },
        {
            "title": "Visual Studio 2017 Version 15.7.4 or Newer",
            "url": "https://developer.microsoft.com/en-us/windows/downloads"
        },
        {
            "title": "Windows 10 - Build 17738 or higher",
            "url": "https://www.microsoft.com/en-us/software-download/windowsinsiderpreviewiso"
        },
        {
            "title": "Windows SDK - Build 17738 or higher",
            "url": "https://www.microsoft.com/en-us/software-download/windowsinsiderpreviewSDK"
        },
        {
            "title": "Win2D",
            "url": "https://github.com/Microsoft/Win2D"
        },
        {
            "title": "Dmitry Soshnikov",
            "url": "https://github.com/shwars"
        },
        {
            "title": "Yana Valieva",
            "url": "https://github.com/vJenny"
        },
        {
            "title": "Tim Scarfe",
            "url": "https://github.com/ecsplendid"
        },
        {
            "title": "Evgeny Grigorenko",
            "url": "https://github.com/evgri243"
        },
        {
            "title": "Victor Kiselev",
            "url": "https://github.com/Gaploid"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "RACE"
            },
            {
                "name": "Wikipedia"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999574601678568,
        "task": "Vision Other",
        "task_prob": 0.9075212259859434
    }
}