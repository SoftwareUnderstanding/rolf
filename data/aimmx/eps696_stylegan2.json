{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "StyleGAN2 for practice",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "eps696",
                "owner_type": "User",
                "name": "stylegan2",
                "url": "https://github.com/eps696/stylegan2",
                "stars": 168,
                "pushed_at": "2022-03-16 14:10:19+00:00",
                "created_at": "2020-07-14 17:58:51+00:00",
                "language": "Python",
                "description": "StyleGAN2 for practice",
                "license": "Other",
                "frameworks": [
                    "TensorFlow",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitattributes",
                "sha": "9033af8ee77d22e44df8888334ae0f7f3f94388e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/.gitattributes"
                    }
                },
                "size": 44
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "5c13d63c6e40bbb3ccedbfe881cb2ee84a4f9b30",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/.gitignore"
                    }
                },
                "size": 124
            },
            {
                "type": "code",
                "name": "LICENSE.txt",
                "sha": "0dc413de86a96d3d7974bf4262c8e0450856b281",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/LICENSE.txt"
                    }
                },
                "size": 6182
            },
            {
                "type": "code",
                "name": "StyleGAN2.ipynb",
                "sha": "0b0cc5b2c3d73e04c0fee5755b453143f3bfede4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/StyleGAN2.ipynb"
                    }
                },
                "size": 18375
            },
            {
                "type": "code",
                "name": "StyleGAN2_colab.ipynb",
                "sha": "9414dbe134998735bfe91a8653821e5b876456b3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/StyleGAN2_colab.ipynb"
                    }
                },
                "size": 32824
            },
            {
                "type": "code",
                "name": "_in",
                "sha": "ef358443fb9b2d667a19e06cf6b092fdeaa1ea39",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/tree/master/_in"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "_out",
                "sha": "e3434fce7930f6b5605ce626715db2ad1b2b4419",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/tree/master/_out"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "data",
                "sha": "0a9d43d91974a670ecb9ecc437b5b284feee2168",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/tree/master/data"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "gen.bat",
                "sha": "2f446e4591b70aa1bebeaf39508bdb7b2b0d7929",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/gen.bat"
                    }
                },
                "size": 526
            },
            {
                "type": "code",
                "name": "model_convert.bat",
                "sha": "061f2ddb2eeb0629e2b33ad1b561b35ecd3131ab",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/model_convert.bat"
                    }
                },
                "size": 76
            },
            {
                "type": "code",
                "name": "models",
                "sha": "9f3b5fce9cc4ec9b6d90b50ee0722816e326f492",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/tree/master/models"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "models_blend.bat",
                "sha": "d095a40052e2782457780453a7031fe81313558a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/models_blend.bat"
                    }
                },
                "size": 106
            },
            {
                "type": "code",
                "name": "models_mix.bat",
                "sha": "25ca69b65e290cdcb9c18fa8a7a3d7020909767c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/models_mix.bat"
                    }
                },
                "size": 50
            },
            {
                "type": "code",
                "name": "play_dlatents.bat",
                "sha": "1e27e8c3edf36f81405c3b3242a405035040498e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/play_dlatents.bat"
                    }
                },
                "size": 410
            },
            {
                "type": "code",
                "name": "play_vectors.bat",
                "sha": "ce9c442dbdb613de5148ea3230d1adabcaac25fc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/play_vectors.bat"
                    }
                },
                "size": 389
            },
            {
                "type": "code",
                "name": "prepare_dataset.bat",
                "sha": "7f56c0e3d2aa43d2a131aa50605fd433b0b23264",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/prepare_dataset.bat"
                    }
                },
                "size": 169
            },
            {
                "type": "code",
                "name": "project.bat",
                "sha": "b0184bf4a5840c9546207d18cafaeb6e6aeb002b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/project.bat"
                    }
                },
                "size": 121
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "6a1f175e5ce244b7770d86972c52222706c85601",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/requirements.txt"
                    }
                },
                "size": 26
            },
            {
                "type": "code",
                "name": "src",
                "sha": "1aafaf1325d01842752013ae344725e4b09c57f2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/tree/master/src"
                    }
                },
                "num_files": 13
            },
            {
                "type": "code",
                "name": "train.bat",
                "sha": "c208efcbd6fb9c561f78d5b3257d02b1a49c8ec0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/train.bat"
                    }
                },
                "size": 74
            },
            {
                "type": "code",
                "name": "train",
                "sha": "d564d0bc3dd917926892c55e3706cc116d5b165e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/tree/master/train"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "train_resume.bat",
                "sha": "7f515c41c9e005fd5f5c6e2cfdd369cc5285d0b2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/eps696/stylegan2/blob/master/train_resume.bat"
                    }
                },
                "size": 89
            }
        ]
    },
    "authors": [
        {
            "name": "vadim epstein",
            "github_id": "eps696"
        }
    ],
    "tags": [],
    "description": "StyleGAN2 for practice",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/eps696/stylegan2",
            "stars": 168,
            "issues": true,
            "readme": "# StyleGAN2 for practice\n\n<p align='center'><img src='_out/palekh-512-1536x512-3x1.jpg' /></p>\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eps696/stylegan2/blob/master/StyleGAN2_colab.ipynb)\n\nThis version of famous [StyleGAN2] is intended mostly for fellow artists, who rarely look at scientific metrics, but rather need a working creative tool. At least, this is what I use daily myself. \nTested on Tensorflow 1.14, requires `pyturbojpeg` for JPG support. Sequence-to-video conversions require [FFMPEG]. For more explicit details refer to the original implementations. \n\nNotes about [StyleGAN2-ada]: \n1) ADA version on Tensorflow has shown smoother and faster convergence on the rich enough datasets, but sometimes resulted in lower output variety (comparing to Diff Augmentation approach). It has also failed in my tests on few-shot datasets (50~100 images), while Diff Aug succeeded there. So meanwhile i personally prefer this repo with Diff Augmentation training.\n2) The latest [PyTorch-based StyleGAN2-ada] flavour is claimed to be up to 30% faster, works with flat folder datasets, way easier to tweak/debug than TF-based one. It's also the only option for modern GPUs (GeForce 30xx, etc.). On my tests/datasets it was regularly failing to learn the variety of macro features though, so I have rarely used it in production. Anyway, here is **[such repo]**, adapted to the features below (custom generation, non-square RGBA data, etc.). \n\n## Features\n* inference (image generation) in arbitrary resolution (finally with proper padding on both TF and Torch)\n* **multi-latent inference** with split-frame or masked blending\n* non-square aspect ratio support (auto-picked from dataset; resolution must be divisible by 2**n, such as 512x256, 1280x768, etc.)\n* various conversion options (changing resolution/aspect, adding alpha channel, etc.) for pretrained models (for further finetuning)\n* transparency (alpha channel) support (auto-picked from dataset)\n* models mixing (SWA) and layers-blending (from [Justin Pinkney])\n* freezing lower D layers for better finetuning on similar data (from [Freeze the Discriminator])\n\nFew operation formats ::\n* Windows batch-files, described below (if you're on Windows with powerful GPU)\n* local Jupyter notebook (for non-Windows platforms)\n* [Colab notebook] (max ease of use, requires Google drive)\n\nalso, from [Data-Efficient GANs] ::\n* differential augmentation for fast training on small datasets (~100 images)\n* support of custom CUDA-compiled TF ops and (slower) Python-based reference ops\n\nalso, from [Aydao] ::\n* funky \"digression\" technique (added inside the network for ~6x speed-up)\n* cropping square models to non-square aspect ratio (experimental)\n\nalso, from [Peter Baylies] and [skyflynil] ::\n* non-progressive configs (E,F) with single-scale datasets\n* raw JPG support in TFRecords dataset (auto-picked from source images)\n* conditional support (labels)\n* vertical mirroring augmentation\n\n## Presumed file structure\n\n| stylegan2 | root\n| :--- | :----------\n| &boxvr;&nbsp; **_in** | input data for generation (check examples there)\n| &boxvr;&nbsp; **_out** | generation output (sequences, videos, projected latents)\n| &boxvr;&nbsp; **data** | datasets for training\n| &boxv;&nbsp; &boxvr;&nbsp; source | [example] folder with raw images\n| &boxv;&nbsp; &boxvr;&nbsp; mydata | [example] folder with prepared images\n| &boxv;&nbsp; &boxvr;&nbsp;  mydata-512x512.tfr | [example] prepared dataset\n| &boxv;&nbsp; &boxur;&nbsp;  &#x22ef; | \n| &boxvr;&nbsp; **models** | trained networks for inference/generation\n| &boxv;&nbsp; &boxur;&nbsp;  ffhq-1024.pkl | [example] trained network file (may contain Gs only)\n| &boxvr;&nbsp; **src** | source code\n| &boxur;&nbsp; **train** | training folders\n| &ensp;&ensp; &boxvr;&nbsp;  ffhq-512.pkl | [example] pre-trained model file (full G/D/Gs)\n| &ensp;&ensp; &boxvr;&nbsp;  000-mydata-512-.. | [example] auto-created training folder\n| &ensp;&ensp; &boxur;&nbsp;&#x22ef;  | \n\n## Training\n\n* Put your images in `data` as subfolder. Ensure they all have the same color channels (monochrome, RGB or RGBA).  \nIf needed, crop square fragments from `source` video or directory with images (feasible method, if you work with patterns or shapes, rather than compostions):\n```\n multicrop.bat source 512 256 \n```\nThis will cut every source image (or video frame) into 512x512px fragments, overlapped with 256px shift by X and Y. Result will be in directory `source-sub`, rename it as you wish. Non-square dataset should be prepared separately.\n\n* Make TFRecords dataset from directory `data/mydata`:\n```\n prepare_dataset.bat mydata\n```\nThis will create file `mydata-512x512.tfr` in `data` directory (if your dataset resolution is 512x512). Images without alpha channel will be stored directly as JPG (dramatically reducing file size). For conditional model split the data by subfolders (`mydata/1`, `mydata/2`, ..) and add `--labels` option.\n\n* Train StyleGAN2 on prepared dataset:\n```\n train.bat mydata --kimg 5000\n```\nThis will run training process, according to the settings in `src/train.py` (check and explore those!!). If there's no TFRecords file from the previous step, it will be created at this point. Results (models and samples) are saved under `train` directory, similar to original Nvidia approach.  \nBatch size is auto-calculated for GPU with 16gb RAM; you may set it lower explicitly with `--batch_size X`, if you face OOM. Another downgrading option in such case is to set less capable network with `--config E`.  \n\nPlease note: we save both compact models (containing only Gs network for inference) as `<dataset>-...pkl` (e.g. `mydata-512-0360.pkl`), and full models (containing G/D/Gs networks for further training) as `snapshot-...pkl`. The naming is for convenience only, it does not affect the operations anymore (as the arguments are stored inside the models).\n\nFor small datasets (100x images instead of 10000x) one should add `--d_aug` option to use [Differential Augmentation] for more effective training. \nTraining duration is defined by `--kimg X` argument (amount of thousands of samples processed). Reasonable value for training from scratch is 5000, while for finetuning in `--d_aug` mode 1000 may be sufficient.  \nAdd `--cond` if you want to train conditional model on the dataset with labels.\n\n* Resume training on `mydata` dataset from the last saved model at `train/000-mydata-512-f` directory:\n```\n train_resume.bat mydata 000-mydata-512-f --kimg 5000\n```\n\n* Uptrain (finetune) trained model `ffhq-512.pkl` on new data:\n```\n train_resume.bat newdata ffhq-512.pkl --kimg 1000\n```\n`--d_aug` would greatly enhance training here. There's also `--freezeD` option, supposedly enhancing finetuning on similar data.\n\n## Generation\n\nResults (frame sequences and videos) are saved by default under `_out` directory.\n\n* Test the model in its native resolution:\n```\n gen.bat ffhq-1024.pkl\n```\n\n* Generate custom animation between random latent points (in `z` space):\n```\n gen.bat ffhq-1024 1920-1080 100-20\n```\nThis will load `ffhq-1024.pkl` from `models` directory and make a 1920x1080 px looped video of 100 frames, with interpolation step of 20 frames between keypoints. Please note: omitting `.pkl` extension would load custom network, effectively enabling arbitrary resolution, multi-latent blending, etc. Using filename with extension will load original network from PKL (useful to test foreign downloaded models). There are `--cubic` and `--gauss` options for animation smoothing, and few `--scale_type` choices. Add `--save_lat` option to save all traversed dlatent `w` points as Numpy array in `*.npy` file (useful for further curating). Set `--seed X` value to produce repeatable results.\n\n* Generate more various imagery:\n```\n gen.bat ffhq-1024 3072-1024 100-20 -n 3-1\n```\nThis will produce animated composition of 3 independent frames, blended together horizontally (like the image in the repo header). Argument `--splitfine X` controls boundary fineness (0 = smoothest). \nInstead of simple frame splitting, one can load external mask(s) from b/w image file (or folder with file sequence):\n```\n gen.bat ffhq-1024 1024-1024 100-20 --latmask _in/mask.jpg\n```\nArguments `--digress X` would add some animated funky displacements with X strength (by tweaking initial const layer params). Arguments `--trunc X` controls truncation psi parameter, as usual. \n\n**NB**: Windows batch-files support only 9 command arguments; if you need more options, you have to edit batch-file itself.\n\n* Project external images onto StyleGAN2 model dlatent points (in `w` space):\n```\n project.bat ffhq-1024.pkl photo\n```\nThe result (found dlatent points as Numpy arrays in `*.npy` files, and video/still previews) will be saved to `_out/proj` directory. \n\n* Generate smooth animation between saved dlatent points (in `w` space):\n```\n play_dlatents.bat ffhq-1024 dlats 25 1920-1080\n```\nThis will load saved dlatent points from `_in/dlats` and produce a smooth looped animation between them (with resolution 1920x1080 and interpolation step of 25 frames). `dlats` may be a file or a directory with `*.npy` or `*.npz` files. To select only few frames from a sequence `somename.npy`, create text file with comma-delimited frame numbers and save it as `somename.txt` in the same directory (check examples for FFHQ model). You can also \"style\" the result: setting `--style_dlat blonde458.npy` will load dlatent from `blonde458.npy` and apply it to higher layers, producing some visual similarity. `--cubic` smoothing and `--digress X` displacements are also applicable here. \n\n* Generate animation from saved point and feature directions (say, aging/smiling/etc for faces model) in dlatent `w` space:\n```\n play_vectors.bat ffhq-1024.pkl blonde458.npy vectors_ffhq\n```\nThis will load base dlatent point from `_in/blonde458.npy` and move it along direction vectors from `_in/vectors_ffhq`, one by one. Result is saved as looped video. \n\n## Tweaking models\n\n* Strip G/D networks from a full model, leaving only Gs for inference:\n```\n model_convert.bat snapshot-1024.pkl \n```\nResulting file is saved with `-Gs` suffix. It's recommended to add `-r` option to reconstruct the network, saving necessary arguments with it. Useful for foreign downloaded models.\n\n* Add or remove layers (from a trained model) to adjust its resolution for further finetuning:\n```\n model_convert.bat snapshot-256.pkl --res 512\n```\nThis will produce new model with 512px resolution, populating weights on the layers up to 256px from the source snapshot (the rest will be initialized randomly). It also can decrease resolution (say, make 512 from 1024). Note that this effectively changes number of layers in the model. \n\nThis option works with complete (G/D/Gs) models only, since it's purposed for transfer-learning (resulting model will contain either partially random weights, or wrong `ToRGB` params). \n\n* Crop or pad layers of a trained model to adjust its aspect ratio:\n```\n model_convert.bat snapshot-1024.pkl --res 1280-768\n```\nThis produces working non-square model. In case of basic aspect conversion (like 4x4 => 5x3), complete models (G/D/Gs) will be trainable for further finetuning.  \nThese functions are experimental, with some voluntary logic, so use with care.\n\n* Add alpha channel to a trained model for further finetuning:\n```\n model_convert.bat snapshot-1024.pkl --alpha\n```\nAll above (adding/cropping/padding layers + alpha channel) can be done in one shot:\n```\n model_convert.bat snapshot-256.pkl --res 1280-768 --alpha\n```\n\n* Combine lower layers from one model with higher layers from another:\n```\n models_blend.bat model1.pkl model2.pkl <res> <level>\n```\n`<res>` is resolution, at which the models are switched (usually 16/32/64); `<level>` is 0 or 1.  \nFor inference (generation) this method works properly only for models from one \"family\", i.e. uptrained (finetuned) from the same original model. \nFor further training may be useful in other cases too (not tested yet!).\n\n* Mix few models by stochastic averaging all weights:\n```\n models_mix.bat models_dir\n```\nThis would work properly only for models from one \"family\", i.e. uptrained (finetuned) from the same original model. \n\n\n* Convert `sg2-1024.pt` model, created in [Rosinality](https://github.com/rosinality/stylegan2-pytorch/) or [StyleGAN2-NADA](https://github.com/rinongal/StyleGAN-nada) repo to Nvidia PKL format, by copying the weights on top of the base `sg2-1024.pkl` model of the same resolution:\n```\n python src/model_pt2pkl.py --model_pt sg2-1024.pt --model_pkl sg2-1024.pkl\n```\n\n## Credits\n\nStyleGAN2: \nCopyright \u00a9 2019, NVIDIA Corporation. All rights reserved.\nMade available under the [Nvidia Source Code License-NC]\nOriginal paper: http://arxiv.org/abs/1912.04958\n\nDifferentiable Augmentation for Data-Efficient GAN Training: https://arxiv.org/abs/2006.10738\n\nOther contributions:\nfollow the links in the descriptions.\n\n[Nvidia Source Code License-NC]: <https://nvlabs.github.io/stylegan2/license.html>\n[StyleGAN2]: <https://github.com/NVlabs/stylegan2>\n[StyleGAN2-ada]: <https://github.com/NVlabs/stylegan2-ada>\n[PyTorch-based StyleGAN2-ada]: <https://github.com/NVlabs/stylegan2-ada-pytorch>\n[such repo]: <https://github.com/eps696/stylegan2ada>\n[Peter Baylies]: <https://github.com/pbaylies/stylegan2>\n[Aydao]: <https://github.com/aydao/stylegan2-surgery>\n[Justin Pinkney]: <https://github.com/justinpinkney/stylegan2/blob/master/blend_models.py>\n[skyflynil]: <https://github.com/skyflynil/stylegan2>\n[Data-Efficient GANs]: <https://github.com/mit-han-lab/data-efficient-gans>\n[Differential Augmentation]: <https://github.com/mit-han-lab/data-efficient-gans>\n[Freeze the Discriminator]: <https://arxiv.org/abs/2002.10964>\n[FFMPEG]: <https://ffmpeg.org/download.html>\n[Colab notebook]: <https://colab.research.google.com/github/eps696/stylegan2/blob/master/StyleGAN2_colab.ipynb>",
            "readme_url": "https://github.com/eps696/stylegan2",
            "frameworks": [
                "TensorFlow",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Differentiable Augmentation for Data-Efficient GAN Training",
            "arxiv": "2006.10738",
            "year": 2020,
            "url": "http://arxiv.org/abs/2006.10738v4",
            "abstract": "The performance of generative adversarial networks (GANs) heavily\ndeteriorates given a limited amount of training data. This is mainly because\nthe discriminator is memorizing the exact training set. To combat it, we\npropose Differentiable Augmentation (DiffAugment), a simple method that\nimproves the data efficiency of GANs by imposing various types of\ndifferentiable augmentations on both real and fake samples. Previous attempts\nto directly augment the training data manipulate the distribution of real\nimages, yielding little benefit; DiffAugment enables us to adopt the\ndifferentiable augmentation for the generated samples, effectively stabilizes\ntraining, and leads to better convergence. Experiments demonstrate consistent\ngains of our method over a variety of GAN architectures and loss functions for\nboth unconditional and class-conditional generation. With DiffAugment, we\nachieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128x128\nand 2-4x reductions of FID given 1,000 images on FFHQ and LSUN. Furthermore,\nwith only 20% training data, we can match the top performance on CIFAR-10 and\nCIFAR-100. Finally, our method can generate high-fidelity images using only 100\nimages without pre-training, while being on par with existing transfer learning\nalgorithms. Code is available at\nhttps://github.com/mit-han-lab/data-efficient-gans.",
            "authors": [
                "Shengyu Zhao",
                "Zhijian Liu",
                "Ji Lin",
                "Jun-Yan Zhu",
                "Song Han"
            ]
        },
        {
            "title": "Freeze the Discriminator: a Simple Baseline for Fine-Tuning GANs",
            "arxiv": "2002.10964",
            "year": 2020,
            "url": "http://arxiv.org/abs/2002.10964v2",
            "abstract": "Generative adversarial networks (GANs) have shown outstanding performance on\na wide range of problems in computer vision, graphics, and machine learning,\nbut often require numerous training data and heavy computational resources. To\ntackle this issue, several methods introduce a transfer learning technique in\nGAN training. They, however, are either prone to overfitting or limited to\nlearning small distribution shifts. In this paper, we show that simple\nfine-tuning of GANs with frozen lower layers of the discriminator performs\nsurprisingly well. This simple baseline, FreezeD, significantly outperforms\nprevious techniques used in both unconditional and conditional GANs. We\ndemonstrate the consistent effect using StyleGAN and SNGAN-projection\narchitectures on several datasets of Animal Face, Anime Face, Oxford Flower,\nCUB-200-2011, and Caltech-256 datasets. The code and results are available at\nhttps://github.com/sangwoomo/FreezeD.",
            "authors": [
                "Sangwoo Mo",
                "Minsu Cho",
                "Jinwoo Shin"
            ]
        },
        {
            "title": "Analyzing and Improving the Image Quality of StyleGAN",
            "arxiv": "1912.04958",
            "year": 2019,
            "url": "http://arxiv.org/abs/1912.04958v2",
            "abstract": "The style-based GAN architecture (StyleGAN) yields state-of-the-art results\nin data-driven unconditional generative image modeling. We expose and analyze\nseveral of its characteristic artifacts, and propose changes in both model\narchitecture and training methods to address them. In particular, we redesign\nthe generator normalization, revisit progressive growing, and regularize the\ngenerator to encourage good conditioning in the mapping from latent codes to\nimages. In addition to improving image quality, this path length regularizer\nyields the additional benefit that the generator becomes significantly easier\nto invert. This makes it possible to reliably attribute a generated image to a\nparticular network. We furthermore visualize how well the generator utilizes\nits output resolution, and identify a capacity problem, motivating us to train\nlarger models for additional quality improvements. Overall, our improved model\nredefines the state of the art in unconditional image modeling, both in terms\nof existing distribution quality metrics as well as perceived image quality.",
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Miika Aittala",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "FFHQ"
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "ImageNet 128x128"
            },
            {
                "name": "Caltech"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "CUB-200-2011"
            },
            {
                "name": "CUB"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999980513405319,
        "task": "Image Generation",
        "task_prob": 0.9817871347948314
    }
}