{
    "visibility": {
        "visibility": "public"
    },
    "name": "Efficient Channel Attention:Reproducibility Challenge 2020",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "digantamisra98",
                "owner_type": "User",
                "name": "Reproducibilty-Challenge-ECANET",
                "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET",
                "stars": 27,
                "pushed_at": "2021-09-20 21:19:32+00:00",
                "created_at": "2020-12-27 00:20:49+00:00",
                "language": "Python",
                "description": "Unofficial Implementation of ECANets (CVPR 2020) for the Reproducibility Challenge 2020.",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".DS_Store",
                "sha": "c72f4d0530796b387636d00a55e650e628b0e188",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/blob/main/.DS_Store"
                    }
                },
                "size": 6148
            },
            {
                "type": "code",
                "name": "ECA_Comp.ipynb",
                "sha": "4e71859a745fe6d9f4a48579a3898b86dd49e336",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/blob/main/ECA_Comp.ipynb"
                    }
                },
                "size": 1989
            },
            {
                "type": "code",
                "name": "Weight_correction.ipynb",
                "sha": "852f45119894c7b4345a21f621cd1d452415dec0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/blob/main/Weight_correction.ipynb"
                    }
                },
                "size": 1406
            },
            {
                "type": "code",
                "name": "figures",
                "sha": "6bab61ac7447ae5fd084b943f554be569a4a9ec0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/tree/main/figures"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "inference_demo.ipynb",
                "sha": "35c0831de216155f7151c4393ff021217b1fe8de",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/blob/main/inference_demo.ipynb"
                    }
                },
                "size": 3544
            },
            {
                "type": "code",
                "name": "light_main.py",
                "sha": "c630083d54c4afea34bfbac1a68440aea1d68050",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/blob/main/light_main.py"
                    }
                },
                "size": 15054
            },
            {
                "type": "code",
                "name": "logs",
                "sha": "5f8fb0c149afe13fca491fa54b34f17d836e59d7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/tree/main/logs"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "8d84fe872fa06c08801b51c10b57d65a2ce65d06",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/blob/main/main.py"
                    }
                },
                "size": 15154
            },
            {
                "type": "code",
                "name": "models",
                "sha": "fdd34bb7fdbcaa87e60f457a990821c1525aa6d9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/tree/main/models"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "paras_flops.py",
                "sha": "0d4f4846cea57f4482a0334541124b163d1c2b52",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/blob/main/paras_flops.py"
                    }
                },
                "size": 1688
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "04eb8b8c6c0f20d7ac92839766ec1fc0e9486d87",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/blob/main/requirements.txt"
                    }
                },
                "size": 35
            },
            {
                "type": "code",
                "name": "sweep",
                "sha": "7fc8d25a31328723ed961ea47fa5b2545500f139",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/tree/main/sweep"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "train_cifar.py",
                "sha": "2bcf17505b0b97574eeeab085353c80c46d9297c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/blob/main/train_cifar.py"
                    }
                },
                "size": 8557
            }
        ]
    },
    "authors": [
        {
            "name": "Xa9aX \u30c4",
            "github_id": "digantamisra98"
        }
    ],
    "tags": [
        "research",
        "reproducible-research",
        "reproducibility",
        "reproducible-paper",
        "wandb",
        "pytorch",
        "mmdetection",
        "image-classification",
        "image-recognition",
        "image-segmentation",
        "object-detection",
        "computer-vision",
        "cvpr2020",
        "deep-learning",
        "deep-neural-networks",
        "convolutional-neural-networks"
    ],
    "description": "Unofficial Implementation of ECANets (CVPR 2020) for the Reproducibility Challenge 2020.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET",
            "stars": 27,
            "issues": true,
            "readme": "<h1 align=\"center\">Efficient Channel Attention:<br>Reproducibility Challenge 2020</h1>\n<p align=\"center\">CVPR 2020 <a href=\"https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_ECA-Net_Efficient_Channel_Attention_for_Deep_Convolutional_Neural_Networks_CVPR_2020_paper.html\" target=\"_blank\">(Official Paper)</a></p>\n\n<p align=\"center\">\n    <a href=\"https://arxiv.org/abs/1910.03151\" alt=\"ArXiv\">\n        <img src=\"https://img.shields.io/badge/Paper-arXiv-blue.svg\" /></a>\n    <a href=\"https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_ECA-Net_Efficient_Channel_Attention_for_Deep_Convolutional_Neural_Networks_CVPR_2020_paper.html\"                     alt=\"CVF\">\n          <img src=\"https://img.shields.io/badge/CVF-Page-purple.svg\" /></a>\n    <a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_ECA-Net_Efficient_Channel_Attention_for_Deep_Convolutional_Neural_Networks_CVPR_2020_paper.pdf\"                        alt=\"PDF\">\n          <img src=\"https://img.shields.io/badge/CVPR-PDF-neon.svg\" /></a>\n    <a href=\"https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Wang_ECA-Net_Efficient_Channel_CVPR_2020_supplemental.pdf\" alt=\"Supp\">\n          <img src=\"https://img.shields.io/badge/CVPR-Supp-pink.svg\" /></a>\n    <a href=\"https://www.youtube.com/watch?v=ipZ2AS1b0rI\" alt=\"Video\">\n          <img src=\"https://img.shields.io/badge/CVPR-Video-maroon.svg\" /></a>\n    <a href=\"https://mybinder.org/v2/gh/digantamisra98/Reproducibilty-Challenge-ECANET/HEAD\" alt=\"ArXiv\">\n        <img src=\"https://mybinder.org/badge_logo.svg\" /></a>\n    <a href=\"https://twitter.com/DigantaMisra1\" alt=\"Twitter\">\n          <img src=\"https://img.shields.io/twitter/url/https/twitter.com/DigantaMisra1.svg?style=social&label=Follow%20%40DigantaMisra1\" /></a>\n    <br>\n    <a href=\"https://wandb.ai/diganta/ECANet-sweep?workspace=user-diganta\" alt=\"Dashboard\">\n        <img src=\"https://img.shields.io/badge/WandB-Dashboard-gold.svg\" /></a>\n    <a href=\"https://wandb.ai/diganta/ECANet-sweep/reports/ECA-Net-Efficient-Channel-Attention-for-Deep-Convolutional-Neural-Networks-NeurIPS-Reproducibility-Challenge-2020--VmlldzozODU0NTM\" alt=\"RC2020\">\n        <img src=\"https://img.shields.io/badge/WandB-Report1-yellow.svg\" /></a>\n    <a href=\"https://wandb.ai/diganta/ECANet-sweep/reports/Efficient-Channel-Attention--VmlldzozNzgwOTE\" alt=\"Report\">\n        <img src=\"https://img.shields.io/badge/WandB-Report2-yellow.svg\" /></a>\n    <a href=\"https://github.com/BangguWu/ECANet\" alt=\"Report\">\n        <img src=\"https://img.shields.io/badge/Official-Repository-black.svg\" /></a>\n    <a href=\"https://blog.paperspace.com/attention-mechanisms-in-computer-vision-ecanet/\" alt=\"Report\">\n        <img src=\"https://img.shields.io/badge/Paperspace-Blog-white.svg\" /></a>\n</p>\n\n<p align=\"center\">\n    <img width=\"1000\" src=\"figures/seg.png\">\n    </br>\n    <em>Bounding Box and Segmentation Maps of ECANet-50-Mask-RCNN using samples from the test set of MS-COCO 2017 dataset.</em>\n</p>\n\n# Introduction\n\n<p float=\"center\">\n    <img src=\"figures/eca_module.jpg\" width=\"1000\" alt=\"Struct.\">\n    <br>\n    <em>Structural comparison of SE and ECA attention mechanism.</em>\n</p>\n\nEfficient Channel Attention (ECA) is a simple efficient extension of the popular Squeeze-and-Excitation Attention Mechanism, which is based on the foundation concept of Local Cross Channel Interaction (CCI). Instead of using fully-connected layers with reduction ratio bottleneck as in the case of SENets, ECANet uses an adaptive shared (across channels) 1D convolution kernel on the downsampled GAP *C* x 1 x 1 tensor. ECA is an equivalently plug and play module similar to SE attention mechanism and can be added anywhere in the blocks of a deep convolutional neural networks. Because of the shared 1D kernel, the parameter overhead and FLOPs cost added by ECA is significantly lower than that of SENets while achieving similar or superior performance owing to it's capabilities of constructing adaptive kernels. This work was accepted at the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. \n\n## How to run:\n\n#### Install Dependencies:\n\n```\npip install -r requirements.txt\n```\n\nThis reproduction is build on PyTorch and MMDetection. Ensure you have CUDA Toolkit > 10.1 installed. For more details regarding installation of MMDetection, please visit this [resources page](https://mmdetection.readthedocs.io/en/latest/get_started.html#installation).\n\nIf ```pip install mmcv-full``` takes a lot of time or fails, use the following line (customize the torch and cuda versions as per your requirements):\n```\npip install mmcv-full==latest+torch1.7.0+cu101 -f https://download.openmmlab.com/mmcv/dist/index.html\n```\n\nAlthough [Echo](https://github.com/digantamisra98/Echo) can be installed via pip, the features we currently use in this project aren't available in the latest pip version. So it's advisable to rather install from source by the following commands and then clone this repository within the directory where Echo source is present and installed in your environment/local/instance:\n\n```\nimport os\ngit clone https://github.com/digantamisra98/Echo.git\nos.chdir(\"/path_to_Echo\")\ngit clone https://github.com/digantamisra98/ECANet.git\npip install -e \"/path_to_Echo/\"\n```\n\n### CIFAR-10:\n\n<p>\n        <a href=\"https://colab.research.google.com/drive/1ssmRtF8U4-6NtZoXLZ7uNSu0g1skHPTT?usp=sharing\" alt=\"Colab\">\n        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>\n</p>\n\n<p float=\"center\">\n    <img src=\"figures/acc.png\" width=\"415\" alt=\"Accuracy.\">\n    <img src=\"figures/loss.png\" width=\"415\" alt=\"Loss.\">\n    <br>\n    <em>Mean training curves of different attention mechanisms using ResNet-18 for CIFAR-10 training over 5 runs.</em>\n</p>\n\nUsing the above linked colab notebook, you can run comparative runs for different attention mechanisms on CIFAR-10 using ResNets. You can add your own attention mechanisms by adding them in the source of Echo package.\n\n### Sweeps:\n\n<p>\n        <a href=\"https://colab.research.google.com/drive/1LfZWJOrxpovPAbQOKDi-6cgXTLdmRYTG?usp=sharing\" alt=\"Colab\">\n        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>\n        <a href=\"https://wandb.ai/diganta/ECANet-sweep/sweeps/z61h01i4?workspace=user-diganta\" alt=\"Dashboard\">\n        <img src=\"https://img.shields.io/badge/Sweeps-Dashboard-gold.svg\" /></a>\n</p>\n\n<p align=\"left\">\n    <img width=\"1000\" src=\"figures/sweeps_run.png\">\n    </br>\n    <em>Hyper-parameter sweep run on Weights & Biases using a ResNet-18 on CIFAR-10.</em>\n</p>\n\nTo run hyperparamter sweeps on [WandB](https://wandb.ai/site), simply run the above linked colab notebook. To add more hyperparameters, simply edit the `sweep.yaml` file present in `sweep` folder.\n\n### ImageNet:\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/eca-net-efficient-channel-attention-for-deep/image-classification-on-imagenet)](https://paperswithcode.com/sota/image-classification-on-imagenet?p=eca-net-efficient-channel-attention-for-deep)\n\nECA layer is implemented in [eca_module.py](https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/blob/main/models/eca_module.py). Since ECA is a dimentionality-preserving module, it can be inserted between convolutional layers in most stages of most networks. We recommend using the model definition provided here with our [imagenet training repo](https://github.com/LandskapeAI/imagenet) to use the fastest and most up-to-date training scripts along with detailed instructions on how to download and prepare dataset.\n\n#### Train with ResNet\n\nYou can run the `main.py` to train or evaluate as follow:\n\n```\nCUDA_VISIBLE_DEVICES={device_ids} python main -a {model_name} --project {WandB Project Name} {the path of you datasets}\n```\nFor example:\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 python main -a eca_resnet50 --project ECANet_RC2020 ./datasets/ILSVRC2012/images\n```\n\n#### Train with MobileNet_v2\nIt is same with above ResNet replace `main.py` by `light_main.py`.\n\n#### Compute the parameters and FLOPs\nIf you have install [thop](https://github.com/Lyken17/pytorch-OpCounter), you can `paras_flosp.py` to compute the parameters and FLOPs of our models. The usage is below:\n```\npython paras_flops.py -a {model_name}\n```\n\n##### Official Results: \n\n|Model|Param.|FLOPs|Top-1(%)|Top-5(%)|\n|:---:|:----:|:---:|:------:|:------:|\n|ECA-Net18|11.15M|1.70G|70.92|89.93|\n|ECA-Net34|20.79M|3.43G|74.21|91.83|\n|ECA-Net50|24.37M|3.86G|77.42|93.62|\n|ECA-Net101|42.49M|7.35G|78.65|94.34|\n|ECA-Net152|57.41M|10.83G|78.92|94.55|\n|ECA-MobileNet_v2|3.34M|319.9M|72.56|90.81||\n\n### MS-COCO:\n\n<p align=\"left\">\n    <img width=\"500\" src=\"figures/seg_ep.gif\">\n    <br>\n    <em>Training progress of ECANet-50-Mask-RCNN for 12 epochs.</em>\n</p>\n\n##### Reproduced Results:\n\n|Backbone|Detectors|BBox_AP|BBox_AP<sub>50</sub>|BBox_AP<sub>75</sub>|BBox_AP<sub>S</sub>|BBox_AP<sub>M</sub>|BBox_AP<sub>L</sub>|Segm_AP|Segm_AP<sub>50</sub>|Segm_AP<sub>75</sub>|Segm_AP<sub>S</sub>|Segm_AP<sub>M</sub>|Segm_AP<sub>L</sub>|Weights|\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n|ECANet-50|Mask RCNN|**34.1**|**53.4**|**37.0**|**21.1**|**37.2**|**42.9**|**31.4**|**50.6**|**33.2**|**18.1**|**34.3**|**41.1**|[Google Drive](https://drive.google.com/file/d/1IrxmSDDOzHKBJPkXYvCHNe-Koqm3Idtq/view?usp=sharing)|\n\n#### Download MS-COCO 2017:\n\nSimply execute [this script](https://gist.githubusercontent.com/mkocabas/a6177fc00315403d31572e17700d7fd9/raw/a6ad5e9d7567187b65f222115dffcb4b8667e047/coco.sh) in your terminal to download and process the MS-COCO 2017 dataset. You can use the following command to do the same:\n```\ncurl https://gist.githubusercontent.com/mkocabas/a6177fc00315403d31572e17700d7fd9/raw/a6ad5e9d7567187b65f222115dffcb4b8667e047/coco.sh | sh\n```\n#### Download Pretrained ImageNet Weights:\n\nDownload the pretrained weights from the [original repository](https://github.com/BangguWu/ECANet). You can download them using `gdown` if you're on Colab or GCloud. For example to download the ECANet-50 weights for training a Mask RCNN, use the following command:\n\n```\npip install gdown\ngdown https://drive.google.com/u/0/uc?id=1670rce333c_lyMWFzBlNZoVUvtxbCF_U&export=download\n```\n\nTo make the weights compatible for MS-COCO training, run [this notebook](https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/blob/main/Weight_correction.ipynb) and then move the processed weight file `eca_net.pth.tar` to a new folder named `weights` in mmdetection directory. Once done, edit the `model` dict variable in `mmdetection/configs/_base_/models/mask_rcnn_r50_fpn.py` by updating the `pretrained` parameter to ```pretrained='weights/eca_net.pth.tar'```. This will load the ECANet-50 backbone weights correctly. \n\n#### Training:\n\nThis project uses [MMDetection](https://github.com/open-mmlab/mmdetection) for training the Mask RCNN model. One would require to make the following changes in the following file in the cloned source of MMDetection codebase to train the detector model.\n\n- `mmdetection/mmdet/models/backbones/resnet.py`:\n    All that requires to be done now is to modify the source backbone code to convert it into ECA based backbone. For this case, the backbone is ECANet-50 and the detector is Mask-RCNN. Simply go to this file and add the original class definition of ECA Module which is:\n    \n    ```\n    class eca_layer(nn.Module):\n    \"\"\"Constructs a ECA module.\n    Args:\n        channel: Number of channels of the input feature map\n        k_size: Adaptive selection of kernel size\n    \"\"\"\n    def __init__(self, k_size=3):\n        super(eca_layer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False) \n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # feature descriptor on the global spatial information\n        y = self.avg_pool(x)\n\n        # Two different branches of ECA module\n        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n\n        # Multi-scale information fusion\n        y = self.sigmoid(y)\n\n        return x * y.expand_as(x)\n    ```\n    \n    Once done, in the `__init__` function of class `Bottleneck`, add the following code lines:\n    \n    ```\n    if self.planes == 64:\n            self.eca = eca_layer(k_size = 3)\n    elif self.planes == 128:\n        self.eca = eca_layer(k_size = 5)\n    elif self.planes == 256:\n        self.eca = eca_layer(k_size = 5)\n    elif self.planes == 512:\n        self.eca = eca_layer(k_size = 7)\n    ```\n    \n    *Note: This is done to ensure the backbone weights get loaded properly as ECANet-50 uses the input number of channels of the block <b>C</b> to predefine the kernel size for the 1D convolution filter in the ECA Module.*\n    \n    Lastly, just add the following line to the `forward` pass/ function of the same class right after the final conv + normalization layer:\n    ```\n    out = self.eca(out)\n    ```\n    \n- `mmdetection/configs/_base_/schedules/schedule_1x.py`\n    If you're training on 1 GPU, you would require to lower down the LR for the scheduler since MMDetection default LR strategy is set for 8 GPU based training. Simply go to this file and edit the optimizer definition with the lr value now being `0.0025`.\n\nAfter making the following changes to run the training, use the following command:\n```\npython tools/train.py configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py\n```\n\nTo resume training from any checkpoint, use the following command (for example - Epoch 5 in this case):\n```\npython tools/train.py configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py --resume-from work_dirs/mask_rcnn_r50_fpn_1x_coco/epoch_5.pth\n```\n\n#### Inference:\n\n*Note: MMDetection has significantly changed since and hence this notebook would be incompatible with the latest version.*\n\nTo run inference, simply run [this notebook](https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/blob/main/inference_demo.ipynb).\n*Although the authors provide the trained detector weights in their repository, they contain a lot of bugs which are described in this [open issue](https://github.com/BangguWu/ECANet/issues/47).*\n\n##### Logs:\n\nThe logs are provided in the [Logs folder](https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET/tree/main/logs). It contains two files:\n1. ```20210102_160817.log```: Contains logs from epoch 1 to epoch 6\n2. ```20210106_012255.log```: Contains logs from epoch 6 to epoch 12\nI restarted training from epoch 6 again since the lr was on 8 GPU setting while I was training on 1 GPU which caused nan loss at epoch 6, hence the two log files.\n\n## WandB logs:\n\nThe dashboard for this project can be accessed [here](https://wandb.ai/diganta/ECANet-sweep?workspace=user-diganta).\n\n##### Machine Specifications and Software versions:\n\n- torch: 1.7.1+cu110\n- GPU: 1 NVIDA V100, 16GB Memory on GCP\n\n## Cite:\n\n```\n@InProceedings{Wang_2020_CVPR,\nauthor = {Wang, Qilong and Wu, Banggu and Zhu, Pengfei and Li, Peihua and Zuo, Wangmeng and Hu, Qinghua},\ntitle = {ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {June},\nyear = {2020}\n}\n```\n\n<p align=\"center\">\n    Made with \u2764\ufe0f and \u26a1\n</p>\n",
            "readme_url": "https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks",
            "arxiv": "1910.03151",
            "year": 2019,
            "url": "http://arxiv.org/abs/1910.03151v4",
            "abstract": "Recently, channel attention mechanism has demonstrated to offer great\npotential in improving the performance of deep convolutional neural networks\n(CNNs). However, most existing methods dedicate to developing more\nsophisticated attention modules for achieving better performance, which\ninevitably increase model complexity. To overcome the paradox of performance\nand complexity trade-off, this paper proposes an Efficient Channel Attention\n(ECA) module, which only involves a handful of parameters while bringing clear\nperformance gain. By dissecting the channel attention module in SENet, we\nempirically show avoiding dimensionality reduction is important for learning\nchannel attention, and appropriate cross-channel interaction can preserve\nperformance while significantly decreasing model complexity. Therefore, we\npropose a local cross-channel interaction strategy without dimensionality\nreduction, which can be efficiently implemented via $1D$ convolution.\nFurthermore, we develop a method to adaptively select kernel size of $1D$\nconvolution, determining coverage of local cross-channel interaction. The\nproposed ECA module is efficient yet effective, e.g., the parameters and\ncomputations of our modules against backbone of ResNet50 are 80 vs. 24.37M and\n4.7e-4 GFLOPs vs. 3.86 GFLOPs, respectively, and the performance boost is more\nthan 2% in terms of Top-1 accuracy. We extensively evaluate our ECA module on\nimage classification, object detection and instance segmentation with backbones\nof ResNets and MobileNetV2. The experimental results show our module is more\nefficient while performing favorably against its counterparts.",
            "authors": [
                "Qilong Wang",
                "Banggu Wu",
                "Pengfei Zhu",
                "Peihua Li",
                "Wangmeng Zuo",
                "Qinghua Hu"
            ]
        },
        {
            "year": "2020",
            "month": "June",
            "booktitle": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "title": "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks",
            "author": [
                "Wang, Qilong",
                "Wu, Banggu",
                "Zhu, Pengfei",
                "Li, Peihua",
                "Zuo, Wangmeng",
                "Hu, Qinghua"
            ],
            "ENTRYTYPE": "inproceedings",
            "ID": "Wang_2020_CVPR",
            "authors": [
                "Wang, Qilong",
                "Wu, Banggu",
                "Zhu, Pengfei",
                "Li, Peihua",
                "Zuo, Wangmeng",
                "Hu, Qinghua"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "COCO 2017"
            },
            {
                "name": "MS-COCO"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9998058895208567,
        "task": "Image Classification",
        "task_prob": 0.5680915651493632
    }
}