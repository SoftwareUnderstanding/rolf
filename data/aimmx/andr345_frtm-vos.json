{
    "visibility": {
        "visibility": "public",
        "license": "GNU General Public License v3.0"
    },
    "name": "FRTM-VOS",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "andr345",
                "owner_type": "User",
                "name": "frtm-vos",
                "url": "https://github.com/andr345/frtm-vos",
                "stars": 117,
                "pushed_at": "2021-01-24 17:39:36+00:00",
                "created_at": "2020-02-27 17:32:53+00:00",
                "language": "Python",
                "description": "Code accompanying the paper Learning Fast and Robust Target Models for Video Object Segmentation",
                "license": "GNU General Public License v3.0",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE.txt",
                "sha": "f288702d2fa16d3cdf0035b15a9fcbc552cd88e7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/andr345/frtm-vos/blob/master/LICENSE.txt"
                    }
                },
                "size": 35149
            },
            {
                "type": "code",
                "name": "__init__.py",
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/andr345/frtm-vos/blob/master/__init__.py"
                    }
                },
                "size": 0
            },
            {
                "type": "code",
                "name": "evaluate.py",
                "sha": "9263a4cd017ef37d458893f73aad69fe2d5dc0aa",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/andr345/frtm-vos/blob/master/evaluate.py"
                    }
                },
                "size": 6202
            },
            {
                "type": "code",
                "name": "evaluate_ytvos_valid_all_frames.py",
                "sha": "c2fab1f19e1c7e9e072c9d087fad0215353bacda",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/andr345/frtm-vos/blob/master/evaluate_ytvos_valid_all_frames.py"
                    }
                },
                "size": 4311
            },
            {
                "type": "code",
                "name": "lib",
                "sha": "34df84f38d7689b8cf7e68280f434f489b7158e0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/andr345/frtm-vos/tree/master/lib"
                    }
                },
                "num_files": 12
            },
            {
                "type": "code",
                "name": "model",
                "sha": "ad17d03b37eebf5d45f905b110b634d2462dac7a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/andr345/frtm-vos/tree/master/model"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "bd86f547d2d029c516285dc24c797acb1ea02183",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/andr345/frtm-vos/blob/master/train.py"
                    }
                },
                "size": 6211
            },
            {
                "type": "code",
                "name": "weights",
                "sha": "5f053fe20dcb9210a4873736a55366d193c3724c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/andr345/frtm-vos/tree/master/weights"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "ytvos_validation",
                "sha": "d54f369e4f1a9bcd6f178c31e38ab3fc002d9566",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/andr345/frtm-vos/tree/master/ytvos_validation"
                    }
                },
                "num_files": 12
            }
        ]
    },
    "authors": [
        {
            "name": "andr345",
            "github_id": "andr345"
        }
    ],
    "tags": [],
    "description": "Code accompanying the paper Learning Fast and Robust Target Models for Video Object Segmentation",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/andr345/frtm-vos",
            "stars": 117,
            "issues": true,
            "readme": "# FRTM-VOS\n\nThis repository contains an implementation of the video object segmentation\nmethod FRTM. A detailed description of the method is\nfound in the CVPR 2020 paper <em>\"Learning Fast and Robust Target Models for Video Object Segmentation\"</em>\n \nCVF: [[paper]](http://openaccess.thecvf.com/content_CVPR_2020/papers/Robinson_Learning_Fast_and_Robust_Target_Models_for_Video_Object_Segmentation_CVPR_2020_paper.pdf)\n[[supplement]](http://openaccess.thecvf.com/content_CVPR_2020/supplemental/Robinson_Learning_Fast_and_CVPR_2020_supplemental.pdf) \\\nArxiv: [[paper]](https://arxiv.org/pdf/2003.00908.pdf)\n\nIf you find the code useful, please cite using:\n\n    @InProceedings{Robinson_2020_CVPR,\n        author = {Robinson, Andreas and Lawin, Felix Jaremo and Danelljan, Martin and Khan, Fahad Shahbaz and Felsberg, Michael},\n        title = {Learning Fast and Robust Target Models for Video Object Segmentation},\n        booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month = {June},\n        year = {2020}\n} \n\n## Installation\nClone the repository: `git clone https://github.com/andr345/frtm-vos.git`\n\nCreate a conda environment and install the following dependencies:\n```shell script\nsudo apt install ninja-build  # For Debian/Ubuntu\nconda install -y cython pip scipy scikit-image tqdm\nconda install -y pytorch torchvision cudatoolkit=10.1 -c pytorch\npip install opencv-python easydict\n```\n\nPyTorch 1.0.1 is slightly faster. If you wish to try to this version, replace the `conda install pytorch` above\nwith the following:\n```shell script\nconda install pytorch==1.0.1 torchvision==0.2.2 -c pytorch\npip install \"pillow<7\"\n```\n\n## Datasets\n\n### DAVIS\n\nTo test the DAVIS validation split, download and unzip the 2017 480p trainval images and annotations here:\n<https://davischallenge.org/davis2017/code.html>.\n\nOr, more precisely, [this file](https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-480p.zip).\n\n### YouTubeVOS\n\nTo test our validation split and the YouTubeVOS challenge 'valid' split, download [YouTubeVOS 2018](https://youtube-vos.org/dataset/)\nand place it in this directory structure:\n\n```\n/path/to/ytvos2018\n|-- train/\n|-- train_all_frames/\n|-- valid/\n`-- valid_all_frames/\n```\nYou only actually need 300 sequences of `train/` and `train_all_frames/` and these are listed\nin `lib/ytvos_jjvalid.txt`. Thanks to Joakim Johnander for providing this split.\n\n## Models\n\nThese pretrained models are available for download: \n\n| Name            | Backbone  | Training set       | Weights  |\n|-----------------|:---------:|:------------------:|:--------:|\n| rn18_ytvos.pth  | ResNet18  | YouTubeVOS         | [download](https://drive.google.com/open?id=1anOEzUMxXR4ff2qaUJNojAABWuAmaGvw) |\n| rn18_all.pth    | ResNet18  | YouTubeVOS + DAVIS | [download](https://drive.google.com/open?id=1t21DG1ts-2NQXDVvuQjW9LY9VVkYuXU5)\n| rn101_ytvos.pth | ResNet101 | YouTubeVOS         | [download](https://drive.google.com/open?id=1KFg7ZjdJyhLE58WzEBlznOrDpKmQqviC) |\n| rn101_all.pth   | ResNet101 | YouTubeVOS + DAVIS | [download](https://drive.google.com/open?id=1GqaB80sznVkonprCdYhURwGwqiPRhP-v) |\n| rn101_dv.pth    | ResNet101 | DAVIS              | [download](https://drive.google.com/open?id=1gRFn2NojH47BjURSws2XIyuTjzFkmuSV) |\n\n\nThe script `weights/download_weights.sh` will download all models and put them in the folder `weights/`.\n\n## Running evaluations\n\nOpen `evaluate.py` and adjust the `paths` dict to your dataset locations and where you want the output.\nThe dictionary is found near line 110, and looks approximately like this:\n\n```python\n    paths = dict(\n        models=Path(__file__).parent / \"weights\",  # The .pth files should be here\n        davis=\"/path/to/DAVIS\",  # DAVIS dataset root\n        yt2018=\"/path/to/ytvos2018\",  # YouTubeVOS 2018 root\n        output=\"/path/to/results\",  # Output path\n    )\n```\n\nThen try one of the evaluations below. The first run will pause for a few seconds while compiling a\nPyTorch C++ extension.\n\nScripts for generating the results in the paper:\n```shell script\npython evaluate.py --model rn101_ytvos.pth --dset yt2018val       # Ours YouTubeVos 2018\npython evaluate.py --model rn101_all.pth --dset dv2016val         # Ours DAVIS 2016\npython evaluate.py --model rn101_all.pth --dset dv2017val         # Ours DAVIS 2017\n\npython evaluate.py --model rn18_ytvos.pth --fast --dset yt2018val # Ours fast YouTubeVos 2018\npython evaluate.py --model rn18_all.pth --fast --dset dv2016val   # Ours fast DAVIS 2016\npython evaluate.py --model rn18_all.pth --fast --dset dv2017val   # Ours fast DAVIS 2017\n```\n\n`--model` is the name of the checkpoint to use in the `weights` directory.\n\n`--fast` reduces the number of optimizer iterations to match \"Ours fast\" in the paper.\n\n`--dset` is one of\n\n  | Name        | Description                                                |\n  |-------------|------------------------------------------------------------|\n  | dv2016val   | DAVIS 2016 validation set                                  |\n  | dv2017val   | DAVIS 2017 validation set                                  |\n  | yt2018jjval | Our validation split of YouTubeVOS 2018 \"train_all_frames\" |\n  | yt2018val   | YouTubeVOS 2018 official \"valid_all_frames\" set            |\n\n## Training\n\n### Running the trainer\n\nTraining is set up similarly to evaluation.\n\nOpen `train.py` and adjust the `paths` dict to your dataset locations, checkpoint and tensorboard\noutput directories and the place to cache target model weights.\n\nTo train a network, run\n\n```shell script\npython train.py <session-name> --ftext resnet101 --dset all --dev cuda:0\n```\n`--ftext` is the name of the feature extractor, either resnet18 or resnet101.\n\n`--dset` is one of dv2017, ytvos2018 or all (\"all\" really means \"both\").\n\n`--dev` is the name of the device to train on.\n\nReplace \"session-name\" with whatever you like. Subdirectories with this name\nwill be created under your checkpoint and tensorboard paths.\n\n### Target model cache\n\nTraining target models from scratch and filling the cache take approximately 5 days of training.\nOnce the cache is mostly full, the next training session should take less than 24 hours.\nThe cache requires 17 GB disk space for training with ResNet-101 features and 32 intermediate\nchannels (as in the paper) and 5 GB for ResNet-18 and the same number of channels.\n\nOur own cache (20 GB) is available [here](https://liuonline-my.sharepoint.com/:f:/g/personal/andro44_liu_se/Er6vfgqY0p5HqqrZqeTM-4ABHPR2tJR-3_XMqxxXBt0hJg?e=a17Jfz).\nThe link is not permanent and will change eventually, so make sure to check this readme in the GitHub\nrepository if you find that it has expired.\n \n## Contact\nAndreas Robinson\n\nemail: andreas.robinson@liu.se\n\nFelix J\u00e4remo Lawin\n\nemail: felix.jaremo-lawin@liu.se\n",
            "readme_url": "https://github.com/andr345/frtm-vos",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Learning Fast and Robust Target Models for Video Object Segmentation",
            "arxiv": "2003.00908",
            "year": 2020,
            "url": "http://arxiv.org/abs/2003.00908v2",
            "abstract": "Video object segmentation (VOS) is a highly challenging problem since the\ninitial mask, defining the target object, is only given at test-time. The main\ndifficulty is to effectively handle appearance changes and similar background\nobjects, while maintaining accurate segmentation. Most previous approaches\nfine-tune segmentation networks on the first frame, resulting in impractical\nframe-rates and risk of overfitting. More recent methods integrate generative\ntarget appearance models, but either achieve limited robustness or require\nlarge amounts of training data.\n  We propose a novel VOS architecture consisting of two network components. The\ntarget appearance model consists of a light-weight module, which is learned\nduring the inference stage using fast optimization techniques to predict a\ncoarse but robust target segmentation. The segmentation model is exclusively\ntrained offline, designed to process the coarse scores into high quality\nsegmentation masks. Our method is fast, easily trainable and remains highly\neffective in cases of limited training data. We perform extensive experiments\non the challenging YouTube-VOS and DAVIS datasets. Our network achieves\nfavorable performance, while operating at higher frame-rates compared to\nstate-of-the-art. Code and trained models are available at\nhttps://github.com/andr345/frtm-vos.",
            "authors": [
                "Andreas Robinson",
                "Felix J\u00e4remo Lawin",
                "Martin Danelljan",
                "Fahad Shahbaz Khan",
                "Michael Felsberg"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9562669564087178,
        "task": "Object Detection",
        "task_prob": 0.6239936199756327
    }
}