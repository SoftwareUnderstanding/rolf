{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Steam StyleGAN2-ADA",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "woctezuma",
                "owner_type": "User",
                "name": "steam-stylegan2-ada",
                "url": "https://github.com/woctezuma/steam-stylegan2-ada",
                "stars": 31,
                "pushed_at": "2021-02-01 12:03:14+00:00",
                "created_at": "2020-10-10 12:00:13+00:00",
                "language": "Jupyter Notebook",
                "description": "Train a StyleGAN2-ADA model on Colaboratory to generate Steam banners. ",
                "license": "MIT License",
                "frameworks": []
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "62c9e72bf3560b8c351554b743f960a983df51e2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/woctezuma/steam-stylegan2-ada/blob/main/LICENSE"
                    }
                },
                "size": 1060
            },
            {
                "type": "code",
                "name": "StyleGAN2_ADA_image_sampling.ipynb",
                "sha": "a3422a1cf04228ceba86af3895c7590cf225b0ba",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/woctezuma/steam-stylegan2-ada/blob/main/StyleGAN2_ADA_image_sampling.ipynb"
                    }
                },
                "size": 251783
            },
            {
                "type": "code",
                "name": "StyleGAN2_ADA_training.ipynb",
                "sha": "b3fbe852292efa057bbeba0c6a0c34e0264adead",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/woctezuma/steam-stylegan2-ada/blob/main/StyleGAN2_ADA_training.ipynb"
                    }
                },
                "size": 43962
            },
            {
                "type": "code",
                "name": "remove_duplicates.ipynb",
                "sha": "b1e7dafba71d958b2abcc2f1ed3cd7ab0e71a273",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/woctezuma/steam-stylegan2-ada/blob/main/remove_duplicates.ipynb"
                    }
                },
                "size": 19876355
            }
        ]
    },
    "authors": [
        {
            "name": "Wok",
            "github_id": "woctezuma"
        }
    ],
    "tags": [
        "google-colab",
        "google-colaboratory",
        "google-colab-notebook",
        "colab",
        "colaboratory",
        "colab-notebook",
        "steam",
        "steam-api",
        "steam-games",
        "steam-game",
        "steam-pics",
        "steam-store",
        "steam-data",
        "gan",
        "generative-adversarial-network",
        "stylegan",
        "stylegan-model",
        "steam-gan",
        "stylegan2",
        "stylegan2-ada"
    ],
    "description": "Train a StyleGAN2-ADA model on Colaboratory to generate Steam banners. ",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/woctezuma/steam-stylegan2-ada",
            "stars": 31,
            "issues": true,
            "readme": "# Steam StyleGAN2-ADA\n\nThe goal of this [Colab][colab-website] notebook is to capture the distribution of Steam banners with a StyleGAN2-ADA model.\n\n## Usage\n\n-   Acquire the data, e.g. as a snapshot called `256x256.zip` in [my data repository][data-repository],\n-   Run [`StyleGAN2_ADA_training.ipynb`][colab-notebook-training] to train a StyleGAN2-ADA model from scratch.\n[![Open In Colab][colab-badge]][colab-notebook-training]\n-   Run [`StyleGAN2_ADA_image_sampling.ipynb`][colab-notebook-sampling] to generate images with a trained StyleGAN2-ADA model,\n[![Open In Colab][colab-badge]][colab-notebook-sampling]\n-   To automatically resume training from the latest checkpoint, you will have to use [my fork][stylegan2-ada-fork] of StyleGAN2-ADA.\n\n## Data\n\nThe dataset consists of 14k Steam banners with RGB channels and resized from 300x450 to 256x256 resolution.\n\nImages were downloaded with [`download_steam_banners.ipynb`][download_steam_banners].\n[![Open In Colab][colab-badge]][download_steam_banners]\n\nImages were then filtered (duplicates, outliers, etc.) with [`remove_duplicates.ipynb`][filter_steam_banners].\n[![Open In Colab][colab-badge]][filter_steam_banners]\n\n## References\n\n-   DCGAN:\n    -   [Radford, Alec, et al. *Unsupervised Representation learning with Deep Convolutional GAN*. ICLR 2016.][dcgan-paper]\n    -   [Official implementation][dcgan-official-repository]\n    -   [Application to Steam banners][dcgan-applied-to-steam-banners]\n-   StyleGAN:\n    -   [Karras, Tero, et al. *A Style-Based Generator Architecture for Generative Adversarial Networks*. CVPR 2019.][stylegan1-paper]\n    -   [Official implementation][stylegan1-official-repository]\n    -   [Application to Steam banners][stylegan1-applied-to-steam-banners]\n-   StyleGAN2:\n    - [Karras, Tero, et al. *Analyzing and Improving the Image Quality of StyleGAN*. CVPR 2020.][stylegan2-paper]\n    -   [Official implementation][stylegan2-official-repository]\n    -   [Application to Steam banners][stylegan2-applied-to-steam-banners]\n-   StyleGAN2-ADA:\n    -   [Karras, Tero, et al. *Training generative adversarial networks with limited data*. NeurIPS 2020.][stylegan2-ada-paper]\n    -   Official implementations: [TensorFlow][stylegan2-ada-official-repository] and [PyTorch][stylegan2-ada-pytorch-repository]\n    -   [Application to Steam banners][stylegan2-ada-applied-to-steam-banners]\n\n<!-- Definitions -->\n\n[download_steam_banners]: <https://colab.research.google.com/github/woctezuma/google-colab/blob/master/download_steam_banners.ipynb>\n[filter_steam_banners]: <https://colab.research.google.com/github/woctezuma/steam-stylegan2-ada/blob/main/remove_duplicates.ipynb>\n\n[colab-website]: <https://colab.research.google.com>\n[colab-notebook-training]: <https://colab.research.google.com/github/woctezuma/steam-stylegan2-ada/blob/main/StyleGAN2_ADA_training.ipynb>\n[colab-notebook-sampling]: <https://colab.research.google.com/github/woctezuma/steam-stylegan2-ada/blob/main/StyleGAN2_ADA_image_sampling.ipynb>\n[colab-badge]: <https://colab.research.google.com/assets/colab-badge.svg>\n\n[data-repository]: <https://github.com/woctezuma/download-steam-banners-data>\n[stylegan2-ada-fork]: <https://github.com/woctezuma/stylegan2-ada/tree/google-colab>\n\n[dcgan-paper]: <https://arxiv.org/abs/1511.06434>\n[stylegan1-paper]: <https://arxiv.org/abs/1812.04948>\n[stylegan2-paper]: <https://arxiv.org/abs/1912.04958>\n[stylegan2-ada-paper]: <https://arxiv.org/abs/2006.06676>\n\n[dcgan-official-repository]: <https://github.com/Newmu/dcgan_code>\n[stylegan1-official-repository]: <https://github.com/NVlabs/stylegan>\n[stylegan2-official-repository]: <https://github.com/NVlabs/stylegan2>\n[stylegan2-ada-official-repository]: <https://github.com/NVlabs/stylegan2-ada>\n[stylegan2-ada-pytorch-repository]: <https://github.com/NVlabs/stylegan2-ada-pytorch>\n\n[dcgan-applied-to-steam-banners]: <https://github.com/woctezuma/google-colab>\n[stylegan1-applied-to-steam-banners]: <https://github.com/woctezuma/steam-stylegan>\n[stylegan2-applied-to-steam-banners]: <https://github.com/woctezuma/steam-stylegan2>\n[stylegan2-ada-applied-to-steam-banners]: <https://github.com/woctezuma/steam-stylegan2-ada>\n",
            "readme_url": "https://github.com/woctezuma/steam-stylegan2-ada",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Training Generative Adversarial Networks with Limited Data",
            "arxiv": "2006.06676",
            "year": 2020,
            "url": "http://arxiv.org/abs/2006.06676v2",
            "abstract": "Training generative adversarial networks (GAN) using too little data\ntypically leads to discriminator overfitting, causing training to diverge. We\npropose an adaptive discriminator augmentation mechanism that significantly\nstabilizes training in limited data regimes. The approach does not require\nchanges to loss functions or network architectures, and is applicable both when\ntraining from scratch and when fine-tuning an existing GAN on another dataset.\nWe demonstrate, on several datasets, that good results are now possible using\nonly a few thousand training images, often matching StyleGAN2 results with an\norder of magnitude fewer images. We expect this to open up new application\ndomains for GANs. We also find that the widely used CIFAR-10 is, in fact, a\nlimited data benchmark, and improve the record FID from 5.59 to 2.42.",
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Janne Hellsten",
                "Samuli Laine",
                "Jaakko Lehtinen",
                "Timo Aila"
            ]
        },
        {
            "title": "Analyzing and Improving the Image Quality of StyleGAN",
            "arxiv": "1912.04958",
            "year": 2019,
            "url": "http://arxiv.org/abs/1912.04958v2",
            "abstract": "The style-based GAN architecture (StyleGAN) yields state-of-the-art results\nin data-driven unconditional generative image modeling. We expose and analyze\nseveral of its characteristic artifacts, and propose changes in both model\narchitecture and training methods to address them. In particular, we redesign\nthe generator normalization, revisit progressive growing, and regularize the\ngenerator to encourage good conditioning in the mapping from latent codes to\nimages. In addition to improving image quality, this path length regularizer\nyields the additional benefit that the generator becomes significantly easier\nto invert. This makes it possible to reliably attribute a generated image to a\nparticular network. We furthermore visualize how well the generator utilizes\nits output resolution, and identify a capacity problem, motivating us to train\nlarger models for additional quality improvements. Overall, our improved model\nredefines the state of the art in unconditional image modeling, both in terms\nof existing distribution quality metrics as well as perceived image quality.",
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Miika Aittala",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ]
        },
        {
            "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
            "arxiv": "1812.04948",
            "year": 2018,
            "url": "http://arxiv.org/abs/1812.04948v3",
            "abstract": "We propose an alternative generator architecture for generative adversarial\nnetworks, borrowing from style transfer literature. The new architecture leads\nto an automatically learned, unsupervised separation of high-level attributes\n(e.g., pose and identity when trained on human faces) and stochastic variation\nin the generated images (e.g., freckles, hair), and it enables intuitive,\nscale-specific control of the synthesis. The new generator improves the\nstate-of-the-art in terms of traditional distribution quality metrics, leads to\ndemonstrably better interpolation properties, and also better disentangles the\nlatent factors of variation. To quantify interpolation quality and\ndisentanglement, we propose two new, automated methods that are applicable to\nany generator architecture. Finally, we introduce a new, highly varied and\nhigh-quality dataset of human faces.",
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ]
        },
        {
            "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
            "arxiv": "1511.06434",
            "year": 2015,
            "url": "http://arxiv.org/abs/1511.06434v2",
            "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and\nunsupervised learning. We introduce a class of CNNs called deep convolutional\ngenerative adversarial networks (DCGANs), that have certain architectural\nconstraints, and demonstrate that they are a strong candidate for unsupervised\nlearning. Training on various image datasets, we show convincing evidence that\nour deep convolutional adversarial pair learns a hierarchy of representations\nfrom object parts to scenes in both the generator and discriminator.\nAdditionally, we use the learned features for novel tasks - demonstrating their\napplicability as general image representations.",
            "authors": [
                "Alec Radford",
                "Luke Metz",
                "Soumith Chintala"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.999965413543032,
        "task": "Image Generation",
        "task_prob": 0.9856387587468814
    },
    "training": {
        "datasets": [
            {
                "name": "CIFAR-10"
            }
        ]
    }
}