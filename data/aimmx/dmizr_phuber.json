{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "[Re] Can gradient clipping mitigate label noise?",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "dmizr",
                "owner_type": "User",
                "name": "phuber",
                "url": "https://github.com/dmizr/phuber",
                "stars": 11,
                "pushed_at": "2021-08-21 20:27:26+00:00",
                "created_at": "2020-11-12 14:22:10+00:00",
                "language": "Python",
                "description": "[Re] Can gradient clipping mitigate label noise? (ML Reproducibility Challenge 2020)",
                "license": "MIT License",
                "frameworks": [
                    "Keras",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".flake8",
                "sha": "51cfbedd1ae1841bfeb5c5163f42416e05c8bb2f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/blob/master/.flake8"
                    }
                },
                "size": 56
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "db225978c529e9d63c493bd479004ef3fc703ea0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/tree/master/.github"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "cc5e2d343d560eb0867258309044c44896c64086",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/blob/master/.gitignore"
                    }
                },
                "size": 1963
            },
            {
                "type": "code",
                "name": ".isort.cfg",
                "sha": "dc74bfba402c9521a77fb8fd8e22a8c448465bed",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/blob/master/.isort.cfg"
                    }
                },
                "size": 161
            },
            {
                "type": "code",
                "name": "CITATION.bib",
                "sha": "6f38e5a55ba0a64befd40ac9cf4732dfdf50c7cb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/blob/master/CITATION.bib"
                    }
                },
                "size": 277
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "76cea8d95e00c970fa13c228b09aed69214f259b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/blob/master/LICENSE"
                    }
                },
                "size": 1112
            },
            {
                "type": "code",
                "name": "conf",
                "sha": "2c3e6fdb54aabfc806db6855d7b5ca68df880fcb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/tree/master/conf"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "556bd63b7bc9754d2451c53e94b445ae80c4a22d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/tree/master/docs"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "eval.py",
                "sha": "e9f91e31b7d04351a6dc9549ee8812cc68b27245",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/blob/master/eval.py"
                    }
                },
                "size": 283
            },
            {
                "type": "code",
                "name": "phuber",
                "sha": "7c4f69f4ae7e6304214159f53934d81469fb56f6",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/tree/master/phuber"
                    }
                },
                "num_files": 11
            },
            {
                "type": "code",
                "name": "requirements-dev.txt",
                "sha": "ff64911e0070f1672474fc592c8fb67d29b840d5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/blob/master/requirements-dev.txt"
                    }
                },
                "size": 27
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "031aa71bb7a36e4b2cbcdef67aeec16492a419d0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/blob/master/requirements.txt"
                    }
                },
                "size": 89
            },
            {
                "type": "code",
                "name": "synthetic",
                "sha": "32c8bae243493f1c8f4918489cae846fd3262289",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/tree/master/synthetic"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "synthetic_1.py",
                "sha": "cdb69b95137c4886b447c11377403c2f635624cc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/blob/master/synthetic_1.py"
                    }
                },
                "size": 302
            },
            {
                "type": "code",
                "name": "synthetic_2.py",
                "sha": "748d28686e9815217908fdd1c21331667a6870e5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/blob/master/synthetic_2.py"
                    }
                },
                "size": 287
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "d8dcec29ddcc22ad8fd5728349880a0f986abfbe",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dmizr/phuber/blob/master/train.py"
                    }
                },
                "size": 283
            }
        ]
    },
    "authors": [
        {
            "name": "David Mizrahi",
            "github_id": "dmizr"
        },
        {
            "name": "O\u011fuz Kaan Y\u00fcksel",
            "email": "okyksl@gmail.com",
            "github_id": "okyksl"
        },
        {
            "name": "Aiday Marlen Kyzy",
            "email": "aidaymarlenkyzy@gmail.com",
            "github_id": "aiday-mar"
        }
    ],
    "tags": [
        "label-noise",
        "robust-learning",
        "gradient-clipping",
        "pytorch"
    ],
    "description": "[Re] Can gradient clipping mitigate label noise? (ML Reproducibility Challenge 2020)",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/dmizr/phuber",
            "stars": 11,
            "issues": true,
            "readme": "# [Re] Can gradient clipping mitigate label noise?\n\n[![Python 3.8](https://img.shields.io/badge/python-3.8-blue.svg)](https://www.python.org/downloads/release/python-380//)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n\n### [ReScience](http://rescience.github.io/bibliography/Mizrahi_2021.html) | [OpenReview](https://openreview.net/forum?id=TM_SgwWJA23)\n\n\nThis is a non-official [PyTorch](https://pytorch.org/) implementation of the ICLR 2020 paper \"[Can gradient clipping mitigate label noise?](https://openreview.net/pdf?id=rklB76EKPr)\" by Menon et al. This paper studies the robustness of gradient clipping to symmetric label noise, and proposes partially Huberised (PHuber) versions of standard losses, which perform well in the presence of label noise.\n\nFor the experiments, the following losses are also implemented:\n- [Unhinged loss](https://arxiv.org/abs/1505.07634v1) (van Rooyen et al., NeurIPS 2015)\n- [Generalized Cross Entropy loss](https://arxiv.org/abs/1805.07836v4) (Zhang & Sabuncu, NeurIPS 2018)\n\nThis repository reproduces all the experiments of the original paper, as part of our participation in the [ML Reproducibility Challenge 2020](https://paperswithcode.com/rc2020). Our report can be found on [OpenReview](https://openreview.net/forum?id=TM_SgwWJA23) and in the [ReScience C journal](http://rescience.github.io/bibliography/Mizrahi_2021.html).\n\n\n## Table of Contents\n- [Dependencies](#dependencies)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Results](#results)\n- [Pretrained models](#pretrained-models)\n- [Synthetic experiments](#synthetic-experiments)\n- [Project structure](#project-structure)\n- [References](#references)\n- [Citation](#citation)\n\n\n## Dependencies\nThis project requires Python >= 3.8. Dependencies can be installed with:\n```\npip install -r requirements.txt\n```\n\n\n## Training\n\nThis project uses [Hydra](https://hydra.cc/) to configure experiments. Configurations can be overridden through config files (in `conf/`) and the command line. For more information, check out the [Hydra documentation](https://hydra.cc/docs/intro/).\n\nWith Hydra, configurations can be fully customized directly though the command line. To find out more about the configuration options, run:\n```\npython3 train.py --help\n```\n\nTo run the experiments from the paper based on real-world datasets  (72 different configurations), only 5 arguments need to be provided:\n- the dataset: `mnist, cifar10, cifar100` (e.g. `dataset=cifar100`)\n- the model: `lenet, resnet50` (e.g. `model=resnet50`)\n- the loss: `ce, gce, linear, phuber_ce, phuber_gce` (e.g. `loss=phuber_ce`)\n- the label corruption probability \u03c1 of the training set (e.g. `dataset.train.corrupt_prob=0.2`)\n- the gradient clipping max norm (gradient clipping is not used by default) (e.g. `hparams.grad_clip_max_norm=0.1`)\n\n**Note:** When choosing a dataset and model, the hyper-parameters (e.g. number of epochs, batch size, optimizer, learning rate scheduler, ...) are automatically changed to those used by the authors in their experiments. If needed, these hyper-parameters can also be overridden through command line arguments.\n\n### Examples\n\nTraining LeNet on MNIST using cross-entropy loss and no label corruption:\n```\npython3 train.py dataset=mnist model=lenet loss=ce dataset.train.corrupt_prob=0.0\n```\n\nTraining a ResNet-50 on CIFAR-10 using the partially Huberised cross-entropy loss (PHuber-CE) with \u03c4=2, and label corruption probability \u03c1 of 0.2:\n\n```\npython3 train.py dataset=cifar10 model=resnet50 loss=phuber_ce loss.tau=2 dataset.train.corrupt_prob=0.2\n```\n\nTraining a ResNet-50 on CIFAR-100 using the Generalized Cross Entropy loss (GCE) and label corruption probability \u03c1 of 0.6, with [mixed precision](https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/):\n\n```\npython3 train.py dataset=cifar100 model=resnet50 loss=gce dataset.train.corrupt_prob=0.6 mixed_precision=true\n```\n\n Training LeNet on MNIST using cross-entropy loss, and varying label corruption probability \u03c1 (0.0, 0.2, 0.4 and 0.6). This uses [Hydra's multi-run flag](https://hydra.cc/docs/tutorials/basic/running_your_app/multi-run) for parameter sweeps:\n\n```\npython3 train.py --multirun dataset=mnist model=lenet loss=ce dataset.train.corrupt_prob=0.0,0.2,0.4,0.6\n```\n\n### Run metrics and saved models\nBy default, run metrics are logged to [TensorBoard](https://www.tensorflow.org/tensorboard). In addition, the saved models, training parameters and training log can be found in the run's directory, in `outputs/`.\n\n\n## Evaluation\n\nTo evaluate a trained model using `eval.py`, you need to provide:\n- the dataset: `mnist, cifar10, cifar100` (e.g. `dataset=cifar100`)\n- the model: `lenet, resnet50` (e.g. `model=resnet50`)\n- path to the trained model weights (e.g. `checkpoint=path/to/model.pt`)\n\nFor example, to evaluate a LeNet model trained on MNIST saved as `models/lenet.pt`, run:\n```\npython3 eval.py dataset=mnist model=lenet checkpoint=models/lenet.pt\n```\n\nBy default, trained models are only evaluated on the test set. This can be modified by overriding the `dataset.train.use`, `dataset.val.use` and `dataset.test.use` arguments.\n\nTo find out more about the configuration options for evaluation, use the `--help` flag.\n\n## Results\n\n#### MNIST with LeNet-5\n\n| Loss function   | \u03c1 = 0.0   | \u03c1 = 0.2   | \u03c1 = 0.4   | \u03c1 = 0.6   |\n|:----------------|:----------|:----------|:----------|:----------|\n| CE              | **99.1\u00b10.1** | **98.8\u00b10.0** | **98.6\u00b10.0** | 98.0\u00b10.1  |\n| CE + clipping   | 97.0\u00b10.0  | 96.5\u00b10.0  | 95.7\u00b10.1  | 94.7\u00b10.1  |\n| Linear          | 95.0\u00b13.5  | 98.5\u00b10.1  | 98.2\u00b10.0  | 97.6\u00b10.0  |\n| GCE             | 98.8\u00b10.0  | 98.7\u00b10.0  | 98.5\u00b10.0  | **98.1\u00b10.0** |\n| PHuber-CE \u03c4=10  | 99.0\u00b10.0  | 98.8\u00b10.1  | 98.5\u00b10.1  | 97.6\u00b10.0  |\n| PHuber-GCE \u03c4=10 | 98.9\u00b10.0  | 98.7\u00b10.0  | 98.4\u00b10.0  | 98.0\u00b10.0  |\n\n![mnist_results](docs/mnist.png)\n\n#### CIFAR-10 with ResNet-50\n| Loss function   | \u03c1 = 0.0   | \u03c1 = 0.2   | \u03c1 = 0.4   | \u03c1 = 0.6   |\n|:----------------|:----------|:----------|:----------|:----------|\n| CE              | **95.8\u00b10.1**  | 84.0\u00b10.3  | 67.8\u00b10.3  | 44.0\u00b10.2  |\n| CE + clipping   | 89.3\u00b10.0  | 82.6\u00b11.6  | 78.7\u00b10.2  | 67.6\u00b10.1  |\n| Linear          | 94.1\u00b10.1  | 91.4\u00b10.5  | 86.0\u00b12.4  | 58.6\u00b15.2  |\n| GCE             | 95.3\u00b10.0  | 92.5\u00b10.1  | 82.4\u00b10.1  | 53.3\u00b10.3  |\n| PHuber-CE \u03c4=2   | 94.8\u00b10.0  | **92.8\u00b10.2**  | **87.8\u00b10.2**  | **73.2\u00b10.2**  |\n| PHuber-GCE \u03c4=10 | 95.4\u00b10.1  | 92.2\u00b10.2  | 81.5\u00b10.2  | 54.3\u00b10.5  |\n\n![cifar10_results](docs/cifar10.png)\n\n#### CIFAR-100 with ResNet-50\n\n| Loss function   | \u03c1 = 0.0   | \u03c1 = 0.2   | \u03c1 = 0.4   | \u03c1 = 0.6   |\n|:----------------|:----------|:----------|:----------|:----------|\n| CE              | 75.4\u00b10.3  | 62.2\u00b10.4  | 45.8\u00b10.9  | 26.7\u00b10.1  |\n| CE + clipping   | 23.5\u00b10.2  | 20.4\u00b10.4  | 16.2\u00b10.5  | 12.9\u00b10.1  |\n| Linear          | 13.7\u00b10.7  | 8.2\u00b10.3   | 5.9\u00b10.7   | 3.9\u00b10.3   |\n| GCE             | 73.3\u00b10.2  | **68.5\u00b10.3**  | 59.5\u00b10.5  | 40.3\u00b10.4  |\n| PHuber-CE \u03c4=10  | 60.6\u00b11.1  | 54.8\u00b11.2  | 43.1\u00b11.1  | 24.3\u00b10.8  |\n| PHuber-GCE \u03c4=10 | 72.7\u00b10.1  | 68.4\u00b10.1  | **60.2\u00b10.2**  | **42.2\u00b10.4**  |\n| PHuber-CE \u03c4=50  | **75.4\u00b10.2**  | 65.9\u00b10.2  | 49.1\u00b10.2  | 26.9\u00b10.0  |\n\n![cifar100_results](docs/cifar100.png)\n\n## Pretrained models\n\nFor each configuration, the models obtained during the first trial are available on Google Drive:\n- [Pretrained LeNet on MNIST](https://drive.google.com/drive/folders/1_sVDLPUqmIyRPMYD0tNTOR3V1PJATIJD?usp=sharing)\n- [Pretrained ResNet-50 on CIFAR-10](https://drive.google.com/drive/folders/1Lo06OJX-QDV01ePrecaXREgn8AWgaXgY?usp=sharing)\n- [Pretrained ResNet-50 on CIFAR-100](https://drive.google.com/drive/folders/1Aas0q2LuaYr1ljHJXKTT4tIiwP0OzD3i?usp=sharing)\n\n\n## Synthetic experiments\n\nThis repo also reproduces the experiments from the paper based on synthetic datasets. These experiments use simple linear models, which are implemented using NumPy and SciPy.  \nTo reproduce the first synthetic experiment (fig. 2a from the paper), run:\n```\npython3 synthetic_1.py\n```\nTo reproduce the second synthetic experiment (fig. 2b from the paper), run:\n```\npython3 synthetic_2.py\n```\n\n## Project structure\n\nThe codebase is separated into 3 parts:\n\n#### `phuber/`\nThis directory contains all the code related to the deep learning experiments on MNIST, CIFAR-10 and CIFAR-100, using PyTorch.   \nThis includes:\n- Noisy MNIST, CIFAR-10 and CIFAR-100 dataset classes (with symmetric label noise), in `phuber/dataset.py`\n- CE, Unhinged, GCE, PHuber-CE and PHuber-GCE losses, in `phuber/loss.py`\n- LeNet-5 and ResNets in `phuber/network.py`\n\n\n#### `synthetic/`\nThis directory contains all the code related to experiments on synthetic data with linear models, using NumPy and SciPy.\n\n\n#### `conf/`\nThis directory contains all the Hydra config files for both types of experiments:\n- Config files for Hydra settings (e.g. output folder and logger) are contained in `conf/hydra`.  \n- Config files for the synthetic experiments are exclusively contained in `conf/synthetic`.  \n- All the other files in this directory are for the deep learning experiments.\n\n\n## References\n- Menon et al., [\"Can gradient clipping mitigate label noise?\"](https://openreview.net/pdf?id=rklB76EKPr), ICLR 2020\n- van Rooyen et al., [\"Learning with Symmetric Label Noise: The Importance of Being Unhinged\"](https://arxiv.org/abs/1505.07634v1), NeurIPS 2015\n- Zhang & Sabuncu, [\"Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels\"](https://arxiv.org/abs/1805.07836), NeurIPS 2018\n- LeCun et al., [\"Gradient-based learning applied to document recognition\"](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf), IEEE 1998\n- He et al., [\"Deep Residual Learning for Image Recognition\"](https://arxiv.org/abs/1512.03385), CVPR 2016\n\n## Citation\nIf you find any piece of our code or report useful, please cite:\n```BibTeX\n@inproceedings{mizrahi2021re,\ntitle={[Re] Can gradient clipping mitigate label noise?},\nauthor={David Mizrahi and O{\\u{g}}uz Kaan Y{\\\"u}ksel and Aiday Marlen Kyzy},\nbooktitle={ML Reproducibility Challenge 2020},\nyear={2021},\nurl={https://openreview.net/forum?id=TM_SgwWJA23}\n}\n```\n",
            "readme_url": "https://github.com/dmizr/phuber",
            "frameworks": [
                "Keras",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels",
            "arxiv": "1805.07836",
            "year": 2018,
            "url": "http://arxiv.org/abs/1805.07836v4",
            "abstract": "Deep neural networks (DNNs) have achieved tremendous success in a variety of\napplications across many disciplines. Yet, their superior performance comes\nwith the expensive cost of requiring correctly annotated large-scale datasets.\nMoreover, due to DNNs' rich capacity, errors in training labels can hamper\nperformance. To combat this problem, mean absolute error (MAE) has recently\nbeen proposed as a noise-robust alternative to the commonly-used categorical\ncross entropy (CCE) loss. However, as we show in this paper, MAE can perform\npoorly with DNNs and challenging datasets. Here, we present a theoretically\ngrounded set of noise-robust loss functions that can be seen as a\ngeneralization of MAE and CCE. Proposed loss functions can be readily applied\nwith any existing DNN architecture and algorithm, while yielding good\nperformance in a wide range of noisy label scenarios. We report results from\nexperiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and\nsynthetically generated noisy labels.",
            "authors": [
                "Zhilu Zhang",
                "Mert R. Sabuncu"
            ]
        },
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "Learning with Symmetric Label Noise: The Importance of Being Unhinged",
            "arxiv": "1505.07634",
            "year": 2015,
            "url": "http://arxiv.org/abs/1505.07634v1",
            "abstract": "Convex potential minimisation is the de facto approach to binary\nclassification. However, Long and Servedio [2010] proved that under symmetric\nlabel noise (SLN), minimisation of any convex potential over a linear function\nclass can result in classification performance equivalent to random guessing.\nThis ostensibly shows that convex losses are not SLN-robust. In this paper, we\npropose a convex, classification-calibrated loss and prove that it is\nSLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of\nbeing negatively unbounded. The loss is a modification of the hinge loss, where\none does not clamp at zero; hence, we call it the unhinged loss. We show that\nthe optimal unhinged solution is equivalent to that of a strongly regularised\nSVM, and is the limiting solution for any convex potential; this implies that\nstrong l2 regularisation makes most standard learners SLN-robust. Experiments\nconfirm the SLN-robustness of the unhinged loss.",
            "authors": [
                "Brendan van Rooyen",
                "Aditya Krishna Menon",
                "Robert C. Williamson"
            ]
        },
        {
            "url": "https://openreview.net/forum?id=TM_SgwWJA23",
            "year": "2021",
            "booktitle": "ML Reproducibility Challenge 2020",
            "author": [
                "Mizrahi, David",
                "Y{\\\"u}ksel, O{\\u{g}}uz Kaan",
                "Kyzy, Aiday Marlen"
            ],
            "title": "[Re] Can gradient clipping mitigate label noise?",
            "ENTRYTYPE": "inproceedings",
            "ID": "mizrahi2021re",
            "authors": [
                "Mizrahi, David",
                "Y{\\\"u}ksel, O{\\u{g}}uz Kaan",
                "Kyzy, Aiday Marlen"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "CIFAR-100"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "MNIST"
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "COCO"
            },
            {
                "name": "Fashion-MNIST"
            },
            {
                "name": "ILSVRC 2015"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9998138631938422,
        "task": "Image Classification",
        "task_prob": 0.866033973532014
    }
}