{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "MedSeg: Medical Segmentation",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "lab-midas",
                "owner_type": "Organization",
                "name": "med_segmentation",
                "url": "https://github.com/lab-midas/med_segmentation",
                "stars": 12,
                "pushed_at": "2021-09-21 16:10:23+00:00",
                "created_at": "2020-05-04 09:24:59+00:00",
                "language": "Python",
                "description": "semantic segmentation for magnetic resonance imaging",
                "license": "Apache License 2.0",
                "frameworks": [
                    "Keras",
                    "scikit-learn",
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".all-contributorsrc",
                "sha": "660513ef85482c53ea8347ee1b3acdbe4097e31e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/blob/master/.all-contributorsrc"
                    }
                },
                "size": 3122
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "b97131ebb081445991e0372daed96006e71bd8a9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/blob/master/.gitignore"
                    }
                },
                "size": 1807
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "63f6b114e07b52dc22d053d56969a148def516ce",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/blob/master/LICENSE"
                    }
                },
                "size": 11345
            },
            {
                "type": "code",
                "name": "Patient",
                "sha": "a19cf24406ae51cbb53a5e0fb73a9bdfea666a28",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/tree/master/Patient"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "config",
                "sha": "8bad8318ea180d13f6f5170accfbbfedf84b06c7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/tree/master/config"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "evaluate.py",
                "sha": "d2cd37a901e0a521f3e3d09ca20ee9cf16903460",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/blob/master/evaluate.py"
                    }
                },
                "size": 2923
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "38aa57385b41020e34edd48f4e48402339ada863",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/blob/master/main.py"
                    }
                },
                "size": 6384
            },
            {
                "type": "code",
                "name": "med_io",
                "sha": "d2c88a48ed898fa9fc4dc03ab5cde48ddc360b16",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/tree/master/med_io"
                    }
                },
                "num_files": 16
            },
            {
                "type": "code",
                "name": "models",
                "sha": "5b6c4dd043f15af6202f0f6dfbf5a2960b8972f9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/tree/master/models"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "plot",
                "sha": "49b1afe8acaf0e2684cd959e63216308e1c67cc0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/tree/master/plot"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "predict.py",
                "sha": "c55b66fc0c1dd2ad9de8894eb79173392c095fc8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/blob/master/predict.py"
                    }
                },
                "size": 27373
            },
            {
                "type": "code",
                "name": "predict_data_processing",
                "sha": "21868c96704ce5cd2cde52620c05a0547c8d2f2c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/tree/master/predict_data_processing"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "59612c1ef197b29bc2f01c04f19e82db80aaf8c1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/blob/master/requirements.txt"
                    }
                },
                "size": 286
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "201ab549eab5df2d8ebac24aa9b6724d00421017",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/blob/master/setup.py"
                    }
                },
                "size": 620
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "b240612639f82aea10658b46c192a9558cc48c6c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/blob/master/train.py"
                    }
                },
                "size": 17949
            },
            {
                "type": "code",
                "name": "util.py",
                "sha": "2e534850a8dc48e119badd1d800cee61de554ece",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/blob/master/util.py"
                    }
                },
                "size": 9924
            },
            {
                "type": "code",
                "name": "utils",
                "sha": "db94af4db331cad15e03cc220046582aeea24fbd",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lab-midas/med_segmentation/tree/master/utils"
                    }
                },
                "num_files": 1
            }
        ]
    },
    "authors": [
        {
            "name": "Thomas Kuestner",
            "email": "thomas.kuestner@uni-tuebingen.de",
            "github_id": "thomaskuestner"
        },
        {
            "name": "KaijieMo1",
            "github_id": "KaijieMo1"
        },
        {
            "name": "fl3on",
            "github_id": "fl3on"
        },
        {
            "name": "allcontributors[bot]",
            "github_id": "allcontributors[bot]"
        },
        {
            "name": "dependabot[bot]",
            "github_id": "dependabot[bot]"
        }
    ],
    "tags": [
        "magnetic-resonance-imaging",
        "segmentation",
        "deep-neural-networks",
        "semantic-segmentation",
        "adipose-tissue",
        "organs"
    ],
    "description": "semantic segmentation for magnetic resonance imaging",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/lab-midas/med_segmentation",
            "stars": 12,
            "issues": true,
            "readme": "# MedSeg: Medical Segmentation\n<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->\n[![All Contributors](https://img.shields.io/badge/all_contributors-10-orange.svg?style=flat-square)](#contributors-)\n<!-- ALL-CONTRIBUTORS-BADGE:END -->\n\n## Introduction\nThis repository contains code to train and evaluate 3D Convolutional Neural Networks for semantic segmentation on medical images.\nThe architectures developed in this framework are a combination of auto-encoder [UNet](https://arxiv.org/abs/1505.04597) with shortcut connections as in [ResNet](https://arxiv.org/abs/1512.03385), densely connections for deep supervision as in [DensetNet](https://arxiv.org/abs/1608.06993) and Merge-And-Run mapping for attention focusing as in [MRGE](https://arxiv.org/abs/1611.07718).\n\n## Credits\nMany thanks to all [contributors](##Contributors) of this repository. If you like it, please click on Star!<br/>\n\nIf you use this package for your research, please cite the paper:<br/>\n\nK\u00fcstner T, Hepp T, Fischer M, Schwartz M, Fritsche A, H\u00e4ring HU, Nikolaou K, Bamberg F, Yang B, Schick F, Gatidis S, Machann J <br/>\n\"Fully automated and standardized segmentation of adipose tissue compartments via deep learning in 3D whole-body MRI of epidemiological cohort studies\" Radiology Artificial Intelligence 2020.<br/>\n[[BibTeX](readme/kuestner_ryai2020.bib)]&nbsp;&nbsp;&nbsp;[[Endnote](readme/kuestner_ryai2020.ris)] \n\n```bibtex\n@article{Kuestner2020,\n    title={Fully automated and standardized segmentation of adipose tissue compartments via deep learning in 3D whole-body MRI of epidemiological cohort studies},\n    author={Thomas K\\\"ustner and Tobias Hepp and Marc Fischer and Martin Schwartz and Andreas Fritsche and Hans-Ulrich H\u00e4ring and Konstantin Nikolaou and Fabian Bamberg and Bin Yang and Fritz Schick and Sergios Gatidis and J\\\"urgen Machann},\n    journal={Radiology Artificial Intelligence},\n    year={2020},\n}\n```\n\n## Documentation\n\n### Installation\nClone the repository and install the requirements\n```shell\n$ git clone https://gitlab.com/iss_mia/cnn_segmentation/ desired_directory\n$ python3 -m pip install -r requirements.txt\n```\n\n### Usage\nSet all parameters in the [configuration file](./config/config_default.yaml). Check call arguments:\n```shell\n$ python3 main.py -h \n```\n\n#### Preprocessing\nConversion of input data (DICOM, NiFTY, Matlab/HDF5) to TFRecords\n```shell\n$ python3 main.py --preprocess -c config/config_default.yml\n```\n\n#### Training\nNetwork training on specified databases\n```shell\n$ python3 main.py --train -c config/config_default.yml -e experiment_name\n```\n\n#### Evaluation\nEvaluate metrics of trained network\n```shell\n$ python3 main.py --evaluate -c config/config_default.yml -e experiment_name\n```\n\n#### Prediction\nPredict segmentation mask for test dataset with trained network\n```shell\n$ python3 main.py --predict -c config/config_default.yml -e experiment_name\n```\n\n#### Database information\nGet database information stored in `Patient` class\n```shell\n$ python3 readme/Patient_example.py -c config/config_default.yml\n```\n\n### Networks/Architectures \nArchitectures are implemented in [/models/ModelSet.py](./models/ModelSet.py).\n\n#### DCNet\ndensely connected with merge-and-run mapping network\n\n<img src=readme/MRGE.png width=\"70%\"></img>\n\n#### Dilated dense convolution\nDensely connected networks (including modifications: dilated convs, ...)\n\n<img src=readme/dilatedDense.png width=\"70%\"></img>\n\n#### UNet\nvanilla UNet and extension for inclusion of positional input\n\n#### Dilated DenseNet\nDensely connected network with dilations\n\n### Loss functions\nCustom loss functions are defined in [/models/loss_function.py](./models/loss_function.py).\n\n### Metrics\nCustom evaluation metrics are defined in [/models/metrics.py](./models/metrics.py).\n\n### Applications\n- Whole-body semantic organ segmentation\n- Whole-body adipose tissue segmentation\n\n## License\nThis project is licensed under the Apache License - see the [LICENSE](LICENSE) file for details.\n\n## Contributors \nThanks to Marc Fischer for providing the med_io pipeline around which this framework was structured.  \n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/marcfi\"><img src=\"https://avatars2.githubusercontent.com/u/48595245?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>marcfi</b></sub></a><br /><a href=\"https://github.com/lab-midas/med_segmentation/commits?author=marcfi\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#ideas-marcfi\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a> <a href=\"#maintenance-marcfi\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"#tool-marcfi\" title=\"Tools\">\ud83d\udd27</a></td>\n    <td align=\"center\"><a href=\"https://sites.google.com/site/kspaceastronauts\"><img src=\"https://avatars1.githubusercontent.com/u/15344655?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Thomas Kuestner</b></sub></a><br /><a href=\"https://github.com/lab-midas/med_segmentation/commits?author=thomaskuestner\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#ideas-thomaskuestner\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a> <a href=\"#maintenance-thomaskuestner\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"#projectManagement-thomaskuestner\" title=\"Project Management\">\ud83d\udcc6</a> <a href=\"https://github.com/lab-midas/med_segmentation/commits?author=thomaskuestner\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/lab-midas/med_segmentation/pulls?q=is%3Apr+reviewed-by%3Athomaskuestner\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a> <a href=\"https://github.com/lab-midas/med_segmentation/commits?author=thomaskuestner\" title=\"Tests\">\u26a0\ufe0f</a></td>\n    <td align=\"center\"><a href=\"https://www.is.mpg.de/de/people/thepp\"><img src=\"https://avatars1.githubusercontent.com/u/30172495?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>tobiashepp</b></sub></a><br /><a href=\"https://github.com/lab-midas/med_segmentation/commits?author=tobiashepp\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#ideas-tobiashepp\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a> <a href=\"#maintenance-tobiashepp\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"#tool-tobiashepp\" title=\"Tools\">\ud83d\udd27</a> <a href=\"#security-tobiashepp\" title=\"Security\">\ud83d\udee1\ufe0f</a></td>\n    <td align=\"center\"><a href=\"https://github.com/a-doering\"><img src=\"https://avatars1.githubusercontent.com/u/35858164?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>a-doering</b></sub></a><br /><a href=\"https://github.com/lab-midas/med_segmentation/commits?author=a-doering\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/KaijieMo1\"><img src=\"https://avatars3.githubusercontent.com/u/69183027?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>KaijieMo1</b></sub></a><br /><a href=\"https://github.com/lab-midas/med_segmentation/commits?author=KaijieMo1\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/lab-midas/med_segmentation/commits?author=KaijieMo1\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    <td align=\"center\"><a href=\"https://github.com/sergiosgatidis\"><img src=\"https://avatars1.githubusercontent.com/u/52936169?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>sergiosgatidis</b></sub></a><br /><a href=\"#ideas-sergiosgatidis\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a> <a href=\"#projectManagement-sergiosgatidis\" title=\"Project Management\">\ud83d\udcc6</a> <a href=\"#question-sergiosgatidis\" title=\"Answering Questions\">\ud83d\udcac</a></td>\n    <td align=\"center\"><a href=\"https://github.com/SarahMue\"><img src=\"https://avatars3.githubusercontent.com/u/30621951?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>SarahMue</b></sub></a><br /><a href=\"https://github.com/lab-midas/med_segmentation/commits?author=SarahMue\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#ideas-SarahMue\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a> <a href=\"#plugin-SarahMue\" title=\"Plugin/utility libraries\">\ud83d\udd0c</a> <a href=\"#maintenance-SarahMue\" title=\"Maintenance\">\ud83d\udea7</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/CDStark\"><img src=\"https://avatars.githubusercontent.com/u/64153881?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>CDStark</b></sub></a><br /><a href=\"https://github.com/lab-midas/med_segmentation/commits?author=CDStark\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#maintenance-CDStark\" title=\"Maintenance\">\ud83d\udea7</a></td>\n    <td align=\"center\"><a href=\"https://github.com/fl3on\"><img src=\"https://avatars.githubusercontent.com/u/57361099?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>fl3on</b></sub></a><br /><a href=\"https://github.com/lab-midas/med_segmentation/commits?author=fl3on\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/lab-midas/med_segmentation/commits?author=fl3on\" title=\"Tests\">\u26a0\ufe0f</a></td>\n    <td align=\"center\"><a href=\"https://github.com/hksonngan\"><img src=\"https://avatars.githubusercontent.com/u/780912?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>haison</b></sub></a><br /><a href=\"https://github.com/lab-midas/med_segmentation/issues?q=author%3Ahksonngan\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n  </tr>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n",
            "readme_url": "https://github.com/lab-midas/med_segmentation",
            "frameworks": [
                "Keras",
                "scikit-learn",
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
            "arxiv": "1505.04597",
            "year": 2015,
            "url": "http://arxiv.org/abs/1505.04597v1",
            "abstract": "There is large consent that successful training of deep networks requires\nmany thousand annotated training samples. In this paper, we present a network\nand training strategy that relies on the strong use of data augmentation to use\nthe available annotated samples more efficiently. The architecture consists of\na contracting path to capture context and a symmetric expanding path that\nenables precise localization. We show that such a network can be trained\nend-to-end from very few images and outperforms the prior best method (a\nsliding-window convolutional network) on the ISBI challenge for segmentation of\nneuronal structures in electron microscopic stacks. Using the same network\ntrained on transmitted light microscopy images (phase contrast and DIC) we won\nthe ISBI cell tracking challenge 2015 in these categories by a large margin.\nMoreover, the network is fast. Segmentation of a 512x512 image takes less than\na second on a recent GPU. The full implementation (based on Caffe) and the\ntrained networks are available at\nhttp://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ]
        },
        {
            "title": "Deep Convolutional Neural Networks with Merge-and-Run Mappings",
            "arxiv": "1611.07718",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.07718v2",
            "abstract": "A deep residual network, built by stacking a sequence of residual blocks, is\neasy to train, because identity mappings skip residual branches and thus\nimprove information flow. To further reduce the training difficulty, we present\na simple network architecture, deep merge-and-run neural networks. The novelty\nlies in a modularized building block, merge-and-run block, which assembles\nresidual branches in parallel through a merge-and-run mapping: Average the\ninputs of these residual branches (Merge), and add the average to the output of\neach residual branch as the input of the subsequent residual branch (Run),\nrespectively. We show that the merge-and-run mapping is a linear idempotent\nfunction in which the transformation matrix is idempotent, and thus improves\ninformation flow, making training easy. In comparison to residual networks, our\nnetworks enjoy compelling advantages: they contain much shorter paths, and the\nwidth, i.e., the number of channels, is increased. We evaluate the performance\non the standard recognition tasks. Our approach demonstrates consistent\nimprovements over ResNets with the comparable setup, and achieves competitive\nresults (e.g., $3.57\\%$ testing error on CIFAR-$10$, $19.00\\%$ on CIFAR-$100$,\n$1.51\\%$ on SVHN).",
            "authors": [
                "Liming Zhao",
                "Jingdong Wang",
                "Xi Li",
                "Zhuowen Tu",
                "Wenjun Zeng"
            ]
        },
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "Densely Connected Convolutional Networks",
            "arxiv": "1608.06993",
            "year": 2016,
            "url": "http://arxiv.org/abs/1608.06993v5",
            "abstract": "Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high performance. Code and\npre-trained models are available at https://github.com/liuzhuang13/DenseNet .",
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens van der Maaten",
                "Kilian Q. Weinberger"
            ]
        },
        {
            "year": "2020",
            "journal": "Radiology Artificial Intelligence",
            "author": [
                "K\\\"ustner, Thomas",
                "Hepp, Tobias",
                "Fischer, Marc",
                "Schwartz, Martin",
                "Fritsche, Andreas",
                "H\u00e4ring, Hans-Ulrich",
                "Nikolaou, Konstantin",
                "Bamberg, Fabian",
                "Yang, Bin",
                "Schick, Fritz",
                "Gatidis, Sergios",
                "Machann, J\\\"urgen"
            ],
            "title": "Fully automated and standardized segmentation of adipose tissue compartments via deep learning in 3D whole-body MRI of epidemiological cohort studies",
            "ENTRYTYPE": "article",
            "ID": "Kuestner2020",
            "authors": [
                "K\\\"ustner, Thomas",
                "Hepp, Tobias",
                "Fischer, Marc",
                "Schwartz, Martin",
                "Fritsche, Andreas",
                "H\u00e4ring, Hans-Ulrich",
                "Nikolaou, Konstantin",
                "Bamberg, Fabian",
                "Yang, Bin",
                "Schick, Fritz",
                "Gatidis, Sergios",
                "Machann, J\\\"urgen"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9998255794544232,
        "task": "Object Detection",
        "task_prob": 0.5493752087726889
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "COCO"
            }
        ]
    }
}