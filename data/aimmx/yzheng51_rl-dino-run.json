{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "rl-dino-run",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "yzheng51",
                "owner_type": "User",
                "name": "rl-dino-run",
                "url": "https://github.com/yzheng51/rl-dino-run",
                "stars": 3,
                "pushed_at": "2022-03-11 23:58:06+00:00",
                "created_at": "2019-09-01 21:00:19+00:00",
                "language": "Jupyter Notebook",
                "description": "Playing T-rex Runner with Deep Reinforcement Learning",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "437440028eb332e6919b01aaff5d9e589ff72fa7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/blob/master/.gitignore"
                    }
                },
                "size": 2422
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "216a4febf1814762edcc5de3abbd29a496674a2a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/blob/master/LICENSE"
                    }
                },
                "size": 1065
            },
            {
                "type": "code",
                "name": "agent.py",
                "sha": "9dcf18e9ce6ba7cd110adc64ac2d6a2bd004b570",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/blob/master/agent.py"
                    }
                },
                "size": 9421
            },
            {
                "type": "code",
                "name": "common.py",
                "sha": "0e679cc7327e0aa8a323cfd60d6c111f3a86c32e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/blob/master/common.py"
                    }
                },
                "size": 831
            },
            {
                "type": "code",
                "name": "defaults.py",
                "sha": "76d7105052be68680f38ffc441024392b137b836",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/blob/master/defaults.py"
                    }
                },
                "size": 252
            },
            {
                "type": "code",
                "name": "experiments",
                "sha": "f54d656715d260bee8efadb10f29d0aca3424ad8",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/tree/master/experiments"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "gym_chrome_dino",
                "sha": "84e257d75a9ef5de04f2dc7c30c6aa1638984e5e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/tree/master/gym_chrome_dino"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "images",
                "sha": "e6896defac5292987db5c840b43cc0b3ed7aec1f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/tree/master/images"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "441f43095c5e2f63acdc5fb69bc2c7217fc128fb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/blob/master/main.py"
                    }
                },
                "size": 1476
            },
            {
                "type": "code",
                "name": "memory.py",
                "sha": "cc02a8527173e03bf4e058d8371c5a6d67adb2d0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/blob/master/memory.py"
                    }
                },
                "size": 2837
            },
            {
                "type": "code",
                "name": "model.py",
                "sha": "f36a121da18b95bbd426124ee5bc8f58d90ed879",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/blob/master/model.py"
                    }
                },
                "size": 1900
            },
            {
                "type": "code",
                "name": "notebooks",
                "sha": "cbe8fc7da197904c26fad7c6cdeb50fdab87d036",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/tree/master/notebooks"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "9fded3372ed2c520ad7b6c89e1053d51c3a81b2c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/blob/master/requirements.txt"
                    }
                },
                "size": 124
            },
            {
                "type": "code",
                "name": "sum_tree.py",
                "sha": "d8bca3630a58788cf1914703a5e5e189fa430761",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/blob/master/sum_tree.py"
                    }
                },
                "size": 5022
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "5ced76cc7004c8c458dc7baf783c985a657e628d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/tree/master/tests"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "trained",
                "sha": "23076d301decd42432a3a9a9ab104f3665619131",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yzheng51/rl-dino-run/tree/master/trained"
                    }
                },
                "num_files": 3
            }
        ]
    },
    "authors": [
        {
            "name": "yzheng51",
            "github_id": "yzheng51"
        }
    ],
    "tags": [],
    "description": "Playing T-rex Runner with Deep Reinforcement Learning",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/yzheng51/rl-dino-run",
            "stars": 3,
            "issues": true,
            "readme": "# rl-dino-run\n\nThis project aims to\n\n- Creates an agent to play T-rex Runner\n- Compares the performance of different algorithms\n- Investigate the effect of batch normalization\n\n## T-rex Runner\n\nThis game environment is based on [this repo](https://github.com/elvisyjlin/gym-chrome-dino) and modify the reward function and preprocessing steps. To simplify the decision, there are only two actions in action space\n\n- Jump\n- Do nothing\n\n![alt text](./images/dino-run.png \"T-rex Runner\")\n\n## Hyperparameter Tuning\n\nThere are many hyperparameters in Reinforcement Learning. In this project, we assume each hyperparameter is independent from others so that we can tune them one by one. Below table shows all parameters after tune.\n\n| Hyperparameter  | Value     |\n| --------------- | --------- |\n| Memory Size     | 3 \u00d7 10^5  |\n| Batch Size      | 128       |\n| Gamma           | 0.99      |\n| Initial epsilon | 1 \u00d7 10^\u22121 |\n| Final epsilon   | 1 \u00d7 10^\u22124 |\n| Explore steps   | 1 \u00d7 10^5  |\n| Learning Rate   | 2 \u00d7 10^\u22125 |\n\n## Training Results\n\n### Comparison of different DQN algorithms\n\nUsing tuned hyperparameters run 200 epochs. Prioritized Experience Replay shows pretty bad effect because of weight update which is very time consuming and the game will keep runing when updating weight\n\n![alt text](./images/exp-train.png \"T-rex Runner\")\n\n### Comparison between DQN and DQN with Batch Normalization\n\n![alt text](./images/exp-train-bn.png \"T-rex Runner\")\n\n### Statistical Results in Training\n\n| Algorithm         | Mean    | Std     | Max   | 25%    | 50%   | 75%     | Time (h) |\n| ----------------- | ------- | ------- | ----- | ------ | ----- | ------- | -------- |\n| DQN               | 537.50  | 393.61  | 1915  | 195.75 | 481   | 820     | 25.87    |\n| Double DQN        | 443.31  | 394.01  | 2366  | 97.75  | 337   | 662.25  | 21.36    |\n| Dueling DQN       | 839.04  | 1521.40 | 25706 | 155    | 457   | 956.5   | 35.78    |\n| DQN with PER      | 43.50   | 2.791   | 71    | 43     | 43    | 43      | 3.31     |\n| DQN (BN)          | 777.54  | 917.26  | 8978  | 97.75  | 462.5 | 1139.25 | 32.59    |\n| Double DQN (BN)   | 696.43  | 758.81  | 5521  | 79     | 430.5 | 1104.25 | 29.40    |\n| Dueling DQN (BN)  | 1050.26 | 1477.00 | 14154 | 84     | 541.5 | 1520    | 40.12    |\n| DQN with PER (BN) | 46.14   | 7.54    | 98    | 43     | 43    | 43      | 3.44     |\n\n## Testing Results\n\nIn testing stage, each algorithm uses the latest model and run 30 times\n\n### Boxplot of all cases\n\n![alt text](./images/exp-test.png \"T-rex Runner\")\n\n### Statistical Results in Testing\n\n| Algorithm         | Mean       | Std        | Min     | Max      | 25%     | 50%       | 75%        |\n| ----------------- | ---------- | ---------- | ------- | -------- | ------- | --------- | ---------- |\n| **Human**         | **1121.9** | **499.91** | **268** | **2384** | **758** | **992.5** | **1508.5** |\n| DQN               | 1161.30    | 814.36     | 45      | 3142     | 321.5   | 1277      | 1729.5     |\n| Double DQN        | 340.93     | 251.40     | 43      | 942      | 178.75  | 259.5     | 400.75     |\n| Dueling DQN       | 2383.03    | 2703.64    | 44      | 8943     | 534.75  | 1499.5    | 2961       |\n| DQN with PER      | 43.30      | 1.64       | 43      | 52       | 43      | 43        | 43         |\n| DQN (BN)          | 2119.47    | 1595.49    | 44      | 5823     | 1218.75 | 1909.5    | 2979.75    |\n| Double DQN (BN)   | 382.17     | 188.74     | 43      | 738      | 283.75  | 356       | 525.5      |\n| Dueling DQN (BN)  | 2083.37    | 1441.50    | 213     | 5389     | 1142.5  | 1912.5    | 2659.75    |\n| DQN with PER (BN) | 45.43      | 7.384      | 43      | 78       | 43      | 43        | 43         |\n\n## Usage\n\nDownload the chrome driver from [this link](https://chromedriver.chromium.org) corresponding to your Chrome version and put the executable in the root path\n\nInstall all required modules by\n\n```sh\npip install -r requirements.txt\n```\n\nRun the sample code\n\n```sh\npython main.py\n```\n\n## References\n\n- DQN: [[paper]](https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf)[[code]](./agent.py#L19-L136)\n- Double DQN: [[paper]](https://arxiv.org/pdf/1509.06461.pdf)[[code]](./agent.py#L150-L168)\n- Dueling DQN: [[paper]](https://arxiv.org/pdf/1511.06581/pdf)[[code]](./agent.py#L139-L147)\n- DQN with Prioritized Experience Replay: [[paper]](https://arxiv.org/pdf/1511.05952.pdf)[[code]](./agent.py#L171-L209)\n- Batch Normalization: [[paper]](https://arxiv.org/pdf/1502.03167.pdf)\n",
            "readme_url": "https://github.com/yzheng51/rl-dino-run",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
            "arxiv": "1502.03167",
            "year": 2015,
            "url": "http://arxiv.org/abs/1502.03167v3",
            "abstract": "Training Deep Neural Networks is complicated by the fact that the\ndistribution of each layer's inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful parameter initialization, and makes it notoriously\nhard to train models with saturating nonlinearities. We refer to this\nphenomenon as internal covariate shift, and address the problem by normalizing\nlayer inputs. Our method draws its strength from making normalization a part of\nthe model architecture and performing the normalization for each training\nmini-batch. Batch Normalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regularizer, in some\ncases eliminating the need for Dropout. Applied to a state-of-the-art image\nclassification model, Batch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model by a significant\nmargin. Using an ensemble of batch-normalized networks, we improve upon the\nbest published result on ImageNet classification: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the accuracy of human raters.",
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ]
        },
        {
            "title": "Prioritized Experience Replay",
            "arxiv": "1511.05952",
            "year": 2015,
            "url": "http://arxiv.org/abs/1511.05952v4",
            "abstract": "Experience replay lets online reinforcement learning agents remember and\nreuse experiences from the past. In prior work, experience transitions were\nuniformly sampled from a replay memory. However, this approach simply replays\ntransitions at the same frequency that they were originally experienced,\nregardless of their significance. In this paper we develop a framework for\nprioritizing experience, so as to replay important transitions more frequently,\nand therefore learn more efficiently. We use prioritized experience replay in\nDeep Q-Networks (DQN), a reinforcement learning algorithm that achieved\nhuman-level performance across many Atari games. DQN with prioritized\nexperience replay achieves a new state-of-the-art, outperforming DQN with\nuniform replay on 41 out of 49 games.",
            "authors": [
                "Tom Schaul",
                "John Quan",
                "Ioannis Antonoglou",
                "David Silver"
            ]
        },
        {
            "title": "Deep Reinforcement Learning with Double Q-learning",
            "arxiv": "1509.06461",
            "year": 2015,
            "url": "http://arxiv.org/abs/1509.06461v3",
            "abstract": "The popular Q-learning algorithm is known to overestimate action values under\ncertain conditions. It was not previously known whether, in practice, such\noverestimations are common, whether they harm performance, and whether they can\ngenerally be prevented. In this paper, we answer all these questions\naffirmatively. In particular, we first show that the recent DQN algorithm,\nwhich combines Q-learning with a deep neural network, suffers from substantial\noverestimations in some games in the Atari 2600 domain. We then show that the\nidea behind the Double Q-learning algorithm, which was introduced in a tabular\nsetting, can be generalized to work with large-scale function approximation. We\npropose a specific adaptation to the DQN algorithm and show that the resulting\nalgorithm not only reduces the observed overestimations, as hypothesized, but\nthat this also leads to much better performance on several games.",
            "authors": [
                "Hado van Hasselt",
                "Arthur Guez",
                "David Silver"
            ]
        }
    ],
    "domain": {
        "domain_type": "Playing Games",
        "domain_prob": 0.986243540968548
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            }
        ]
    }
}