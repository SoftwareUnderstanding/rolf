{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Reinforcement Learning with Model-Agnostic Meta-Learning (MAML)",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "dragen1860",
                "owner_type": "User",
                "name": "MAML-Pytorch-RL",
                "url": "https://github.com/dragen1860/MAML-Pytorch-RL",
                "stars": 8,
                "pushed_at": "2018-08-16 05:13:13+00:00",
                "created_at": "2018-08-04 08:41:41+00:00",
                "language": "Python",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "797c2d43edd2cd347fd83f9394dd51983503063b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dragen1860/MAML-Pytorch-RL/blob/master/.gitignore"
                    }
                },
                "size": 1226
            },
            {
                "type": "code",
                "name": ".idea",
                "sha": "d6147e359758887400f1ddb5459541e4959b5929",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dragen1860/MAML-Pytorch-RL/tree/master/.idea"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "bc42f606c099eecac4b25605cd022e2528e5702c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dragen1860/MAML-Pytorch-RL/blob/master/LICENSE"
                    }
                },
                "size": 1070
            },
            {
                "type": "code",
                "name": "_assets",
                "sha": "4730aca80f34115a77fa0bf34beeb1a7d61b7ee2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dragen1860/MAML-Pytorch-RL/tree/master/_assets"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "aae806343723980dbe83860802a5f5bfa974b90c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dragen1860/MAML-Pytorch-RL/blob/master/main.py"
                    }
                },
                "size": 6338
            },
            {
                "type": "code",
                "name": "maml_rl",
                "sha": "fee5c322a0443aa550277ec945defdab241f38ff",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dragen1860/MAML-Pytorch-RL/tree/master/maml_rl"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "0210b7a3b207452122ed4f8344df02011d562757",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dragen1860/MAML-Pytorch-RL/blob/master/requirements.txt"
                    }
                },
                "size": 56
            },
            {
                "type": "code",
                "name": "test",
                "sha": "1e4ef22c3a4374046fc1f803ed5e838d949edbee",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dragen1860/MAML-Pytorch-RL/tree/master/test"
                    }
                },
                "num_files": 2
            }
        ]
    },
    "authors": [
        {
            "name": "Tristan Deleu",
            "github_id": "tristandeleu"
        },
        {
            "name": "Arian Hosseini",
            "email": "arian.hosseini9@gmail.com",
            "github_id": "arianhosseini"
        },
        {
            "name": "Jackie Loong",
            "github_id": "dragen1860"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/dragen1860/MAML-Pytorch-RL",
            "stars": 8,
            "issues": true,
            "readme": "# Reinforcement Learning with Model-Agnostic Meta-Learning (MAML)\n\n![HalfCheetahDir](https://raw.githubusercontent.com/tristandeleu/pytorch-maml-rl/master/_assets/halfcheetahdir.gif)\n\nImplementation of Model-Agnostic Meta-Learning (MAML) applied on Reinforcement Learning problems in Pytorch. This repository includes environments introduced in ([Duan et al., 2016](https://arxiv.org/abs/1611.02779), [Finn et al., 2017](https://arxiv.org/abs/1703.03400)): multi-armed bandits, tabular MDPs, continuous control with MuJoCo, and 2D navigation task.\n\n## Getting started\nTo avoid any conflict with your existing Python setup, and to keep this project self-contained, it is suggested to work in a virtual environment with [`virtualenv`](http://docs.python-guide.org/en/latest/dev/virtualenvs/). To install `virtualenv`:\n```\npip install --upgrade virtualenv\n```\nCreate a virtual environment, activate it and install the requirements in [`requirements.txt`](requirements.txt).\n```\nvirtualenv venv\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\n## Usage\nYou can use the [`main.py`](main.py) script in order to run reinforcement learning experiments with MAML. This script was tested with Python 3.5. Note that some environments may also work with Python 2.7 (all experiments besides MuJoCo-based environments).\n```\npython main.py --env-name HalfCheetahDir-v1 --num-workers 8 --fast-lr 0.1 --max-kl 0.01 --fast-batch-size 20 --meta-batch-size 40 --num-layers 2 --hidden-size 100 --num-batches 1000 --gamma 0.99 --tau 1.0 --cg-damping 1e-5 --ls-max-steps 15 --output-folder maml-halfcheetah-dir --device cuda\n```\n\n## References\nThis project is, for the most part, a reproduction of the original implementation [cbfinn/maml_rl](https://github.com/cbfinn/maml_rl/) in Pytorch. These experiments are based on the paper\n> Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. _International Conference on Machine Learning (ICML)_, 2017 [[ArXiv](https://arxiv.org/abs/1703.03400)]\n\nIf you want to cite this paper\n```\n@article{DBLP:journals/corr/FinnAL17,\n  author    = {Chelsea Finn and Pieter Abbeel and Sergey Levine},\n  title     = {Model-{A}gnostic {M}eta-{L}earning for {F}ast {A}daptation of {D}eep {N}etworks},\n  journal   = {International Conference on Machine Learning (ICML)},\n  year      = {2017},\n  url       = {http://arxiv.org/abs/1703.03400}\n}\n```\n",
            "readme_url": "https://github.com/dragen1860/MAML-Pytorch-RL",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning",
            "arxiv": "1611.02779",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.02779v2",
            "abstract": "Deep reinforcement learning (deep RL) has been successful in learning\nsophisticated behaviors automatically; however, the learning process requires a\nhuge number of trials. In contrast, animals can learn new tasks in just a few\ntrials, benefiting from their prior knowledge about the world. This paper seeks\nto bridge this gap. Rather than designing a \"fast\" reinforcement learning\nalgorithm, we propose to represent it as a recurrent neural network (RNN) and\nlearn it from data. In our proposed method, RL$^2$, the algorithm is encoded in\nthe weights of the RNN, which are learned slowly through a general-purpose\n(\"slow\") RL algorithm. The RNN receives all information a typical RL algorithm\nwould receive, including observations, actions, rewards, and termination flags;\nand it retains its state across episodes in a given Markov Decision Process\n(MDP). The activations of the RNN store the state of the \"fast\" RL algorithm on\nthe current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both\nsmall-scale and large-scale problems. On the small-scale side, we train it to\nsolve randomly generated multi-arm bandit problems and finite MDPs. After\nRL$^2$ is trained, its performance on new MDPs is close to human-designed\nalgorithms with optimality guarantees. On the large-scale side, we test RL$^2$\non a vision-based navigation task and show that it scales up to\nhigh-dimensional problems.",
            "authors": [
                "Yan Duan",
                "John Schulman",
                "Xi Chen",
                "Peter L. Bartlett",
                "Ilya Sutskever",
                "Pieter Abbeel"
            ]
        },
        {
            "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
            "arxiv": "1703.03400",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.03400v3",
            "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the\nsense that it is compatible with any model trained with gradient descent and\napplicable to a variety of different learning problems, including\nclassification, regression, and reinforcement learning. The goal of\nmeta-learning is to train a model on a variety of learning tasks, such that it\ncan solve new learning tasks using only a small number of training samples. In\nour approach, the parameters of the model are explicitly trained such that a\nsmall number of gradient steps with a small amount of training data from a new\ntask will produce good generalization performance on that task. In effect, our\nmethod trains the model to be easy to fine-tune. We demonstrate that this\napproach leads to state-of-the-art performance on two few-shot image\nclassification benchmarks, produces good results on few-shot regression, and\naccelerates fine-tuning for policy gradient reinforcement learning with neural\nnetwork policies.",
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ]
        },
        {
            "url": "http://arxiv.org/abs/1703.03400",
            "year": "2017",
            "journal": "International Conference on Machine Learning (ICML)",
            "title": "Model-{A}gnostic {M}eta-{L}earning for {F}ast {A}daptation of {D}eep {N}etworks",
            "author": [
                "Finn, Chelsea",
                "Abbeel, Pieter",
                "Levine, Sergey"
            ],
            "ENTRYTYPE": "article",
            "ID": "DBLP:journals/corr/FinnAL17",
            "authors": [
                "Finn, Chelsea",
                "Abbeel, Pieter",
                "Levine, Sergey"
            ]
        }
    ],
    "domain": {
        "domain_type": "Playing Games",
        "domain_prob": 0.9675491577130212
    }
}