{
    "visibility": {
        "visibility": "public",
        "license": "GNU General Public License v2.0"
    },
    "name": "Seq2seq",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "farizrahman4u",
                "owner_type": "User",
                "name": "seq2seq",
                "url": "https://github.com/farizrahman4u/seq2seq",
                "stars": 3099,
                "pushed_at": "2019-02-26 02:48:31+00:00",
                "created_at": "2015-11-07 07:37:12+00:00",
                "language": "Python",
                "description": "Sequence to Sequence Learning with Keras",
                "license": "GNU General Public License v2.0",
                "frameworks": [
                    "Keras"
                ]
            },
            {
                "type": "code",
                "name": ".cache",
                "sha": "f3634f1dd2669303a02c32025d8253678ae053bd",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/farizrahman4u/seq2seq/tree/master/.cache"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "4b58b24bf647dc5703cd6b83e7f4406120b656e1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/farizrahman4u/seq2seq/blob/master/.gitignore"
                    }
                },
                "size": 67
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "8cdb8451d9b90c1d4000c6f22eb86471a4568be6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/farizrahman4u/seq2seq/blob/master/LICENSE"
                    }
                },
                "size": 18047
            },
            {
                "type": "code",
                "name": "seq2seq",
                "sha": "02dec5850c5fe5104f9057e50132242ebe8526f3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/farizrahman4u/seq2seq/tree/master/seq2seq"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "e1fe3cabebc0921c855ba0b2cb70b8c5318233f8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/farizrahman4u/seq2seq/blob/master/setup.py"
                    }
                },
                "size": 546
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "f82d834c8b06bf93c43ba26f08dbc1a0bbcddae5",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/farizrahman4u/seq2seq/tree/master/tests"
                    }
                },
                "num_files": 1
            }
        ]
    },
    "authors": [
        {
            "name": "Fariz Rahman",
            "email": "farizrahman4u@gmail.com",
            "github_id": "farizrahman4u"
        },
        {
            "name": "Abhai Kollara Dilip",
            "github_id": "abhaikollara"
        },
        {
            "name": "Abin Simon",
            "email": "mail@meain.io",
            "github_id": "meain"
        },
        {
            "name": "Nicolas",
            "email": "nickolas.ivanov@gmail.com",
            "github_id": "nicolas-ivanov"
        },
        {
            "name": "Mike Clark",
            "email": "github@wassname.org",
            "github_id": "wassname"
        },
        {
            "name": "Carl Thom\u00e9",
            "email": "carlthome@gmail.com",
            "github_id": "carlthome"
        },
        {
            "name": "Matt Lee",
            "github_id": "mathewlee11"
        },
        {
            "name": "Philipp Dowling",
            "github_id": "phdowling"
        },
        {
            "name": "Igor Shalyminov",
            "email": "ishalyminov@gmail.com",
            "github_id": "ishalyminov"
        },
        {
            "name": "WarpTime",
            "github_id": "WarpTime"
        },
        {
            "name": "wobu",
            "email": "buchnerw@googlemail.com",
            "github_id": "wobu"
        },
        {
            "name": "Alex Khlivnuik",
            "email": "libraua@gmail.com",
            "github_id": "libraua"
        },
        {
            "name": "Bruno Alano",
            "email": "bruno@monoscope.io",
            "github_id": "brunoalano"
        },
        {
            "name": "dsx4602",
            "github_id": "dsx4602"
        },
        {
            "name": "Kieran Gorman",
            "email": "kieran@kjgorman.com",
            "github_id": "kjgorman"
        }
    ],
    "tags": [],
    "description": "Sequence to Sequence Learning with Keras",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/farizrahman4u/seq2seq",
            "stars": 3099,
            "issues": true,
            "readme": "# Seq2seq\nSequence to Sequence Learning with Keras\n\n\n\n**Hi!** You have just found Seq2Seq. Seq2Seq is a sequence to sequence learning add-on for the python deep learning library [Keras](http://www.keras.io). Using Seq2Seq, you can build and train sequence-to-sequence neural network models in Keras. Such models are useful for machine translation, chatbots (see [[4]](http://arxiv.org/pdf/1506.05869v1.pdf)), parsers, or whatever that comes to your mind.\n\n\n  ![seq2seq](http://i64.tinypic.com/30136te.png)\n\n# Getting started\n\nSeq2Seq contains modular and reusable layers that you can use to build your own seq2seq models as well as built-in models that work out of the box. Seq2Seq models can be compiled as they are or added as layers to a bigger model. Every Seq2Seq model has 2 primary layers : the encoder and the  decoder. Generally, the encoder encodes the input  sequence to an internal representation called 'context vector' which is used by the decoder to generate the output sequence. The lengths of input and output sequences can be different, as there is no explicit one on one relation between the input and output sequences. In addition to the encoder and decoder layers, a Seq2Seq model may also contain layers such as the left-stack (Stacked LSTMs on the encoder side), the right-stack (Stacked LSTMs on the decoder side), resizers (for shape compatibility between the encoder and the decoder) and dropout layers to avoid overfitting. The source code is heavily documented, so lets go straight to the examples:\n\n**A simple Seq2Seq model:**\n\n```python\nimport seq2seq\nfrom seq2seq.models import SimpleSeq2Seq\n\nmodel = SimpleSeq2Seq(input_dim=5, hidden_dim=10, output_length=8, output_dim=8)\nmodel.compile(loss='mse', optimizer='rmsprop')\n```\nThat's it! You have successfully compiled a minimal Seq2Seq model! Next, let's build a 6 layer deep Seq2Seq model (3 layers for encoding, 3 layers for decoding).\n\n**Deep Seq2Seq models:**\n\n```python\nimport seq2seq\nfrom seq2seq.models import SimpleSeq2Seq\n\nmodel = SimpleSeq2Seq(input_dim=5, hidden_dim=10, output_length=8, output_dim=8, depth=3)\nmodel.compile(loss='mse', optimizer='rmsprop')\n```\nNotice that we have specified the depth for both encoder and decoder as 3, and your model has a total depth of 3 + 3 = 6. You can also specify different depths for the encoder and the decoder. Example:\n\n```python\nimport seq2seq\nfrom seq2seq.models import SimpleSeq2Seq\n\nmodel = SimpleSeq2Seq(input_dim=5, hidden_dim=10, output_length=8, output_dim=20, depth=(4, 5))\nmodel.compile(loss='mse', optimizer='rmsprop')\n```\n\nNotice that the depth is specified as tuple, `(4, 5)`. Which means your encoder will be 4 layers deep whereas your decoder will be 5 layers deep. And your model will have a total depth of 4 + 5 = 9.\n\n**Advanced Seq2Seq models:**\n\nUntil now, you have been using the `SimpleSeq2Seq` model, which is a very minimalistic model. In the actual Seq2Seq implementation described in [[1]](http://arxiv.org/abs/1409.3215), the hidden state of the encoder is transferred to decoder. Also, the output of decoder at each timestep becomes the input to the decoder at the next time step. To make things more complicated, the hidden state is propogated throughout the LSTM stack. But you  have no reason to worry, as we have a built-in model that does all that out of the box. Example:\n\n```python\nimport seq2seq\nfrom seq2seq.models import Seq2Seq\n\nmodel = Seq2Seq(batch_input_shape=(16, 7, 5), hidden_dim=10, output_length=8, output_dim=20, depth=4)\nmodel.compile(loss='mse', optimizer='rmsprop')\n```\n\nNote that we had to specify the complete input shape, including the samples dimensions. This is because we need a static hidden state(similar to a stateful RNN) for transferring it across layers. (Update : Full input shape is not required in the latest version, since we switched to Recurrent Shop backend). By the way, Seq2Seq models also support the `stateful` argument, in case you need it.\n\nYou can also experiment with the hidden state propogation turned  off. Simply set the arguments `broadcast_state` and `inner_broadcast_state` to `False`.\n\n**Peeky Seq2seq model**:\n\nLet's not stop there. Let's build a model similar to [cho et al 2014](http://arxiv.org/abs/1406.1078), where the decoder gets a 'peek' at the context vector at every timestep.\n\n![cho et al 2014](http://i64.tinypic.com/302aqhi.png)\n\nTo achieve this, simply add the argument `peek=True`:\n\n```python\nimport seq2seq\nfrom seq2seq.models import Seq2Seq\n\nmodel = Seq2Seq(batch_input_shape=(16, 7, 5), hidden_dim=10, output_length=8, output_dim=20, depth=4, peek=True)\nmodel.compile(loss='mse', optimizer='rmsprop')\n```\n\n**Seq2seq model with attention:**\n\n![Attention Seq2seq](http://i64.tinypic.com/a2rw3d.png)\n\nLet's not stop there either. In all the models described above, there is no allignment between the input sequence elements and the output sequence elements. But for machine translation, learning a soft allignment between the input and output sequences imporves performance.[[3]](http://arxiv.org/pdf/1409.0473v6.pdf). The Seq2seq framework includes a ready made attention model which does the same. Note that in the attention model, there is no hidden state propogation, and a bidirectional LSTM encoder is used by default. Example:\n\n```python\nimport seq2seq\nfrom seq2seq.models import AttentionSeq2Seq\n\nmodel = AttentionSeq2Seq(input_dim=5, input_length=7, hidden_dim=10, output_length=8, output_dim=20, depth=4)\nmodel.compile(loss='mse', optimizer='rmsprop')\n```\n\nAs you can see, in the attention model you need not specify the samples dimension as there are no static hidden states involved(But you have to if you are building a stateful Seq2seq model).\nNote:  You  can set the argument `bidirectional=False` if you wish not to use a bidirectional encoder.\n\n# Final Words\n\nThat's all for now. Hope you love this library. For any questions you might have, create an issue and I will get in touch. You can also contribute to this project by reporting bugs, adding new examples, datasets or models.\n\n**Installation:**\n\n```sudo pip install git+https://github.com/farizrahman4u/seq2seq.git```\n\n\n**Requirements:**\n\n* [Keras](https://keras.io)\n* [Recurrent Shop](https://www.github.com/farizrahman4u/recurrentshop)\n\n\n**Working Example:**\n\n* [Training Seq2seq with movie subtitles](https://github.com/nicolas-ivanov/debug_seq2seq)  - Thanks to [Nicolas Ivanov](https://github.com/nicolas-ivanov)\n\n**Papers:**\n\n* [ [1] Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n* [ [2] Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation](http://arxiv.org/pdf/1406.1078.pdf)\n* [ [3] Neural Machine Translation by Jointly Learning to Align and Translate](http://arxiv.org/pdf/1409.0473v6.pdf)\n* [ [4] A Neural Conversational Model](http://arxiv.org/pdf/1506.05869v1.pdf)\n\n\n",
            "readme_url": "https://github.com/farizrahman4u/seq2seq",
            "frameworks": [
                "Keras"
            ]
        }
    ],
    "references": [
        {
            "title": "Sequence to Sequence Learning with Neural Networks",
            "arxiv": "1409.3215",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.3215v3",
            "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\na vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT'14 dataset, the translations produced by\nthe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\nnot have difficulty on long sentences. For comparison, a phrase-based SMT\nsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on\nthis task. The LSTM also learned sensible phrase and sentence representations\nthat are sensitive to word order and are relatively invariant to the active and\nthe passive voice. Finally, we found that reversing the order of the words in\nall source sentences (but not target sentences) improved the LSTM's performance\nmarkedly, because doing so introduced many short term dependencies between the\nsource and the target sentence which made the optimization problem easier.",
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V. Le"
            ]
        },
        {
            "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
            "arxiv": "1406.1078",
            "year": 2014,
            "url": "http://arxiv.org/abs/1406.1078v3",
            "abstract": "In this paper, we propose a novel neural network model called RNN\nEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNN\nencodes a sequence of symbols into a fixed-length vector representation, and\nthe other decodes the representation into another sequence of symbols. The\nencoder and decoder of the proposed model are jointly trained to maximize the\nconditional probability of a target sequence given a source sequence. The\nperformance of a statistical machine translation system is empirically found to\nimprove by using the conditional probabilities of phrase pairs computed by the\nRNN Encoder-Decoder as an additional feature in the existing log-linear model.\nQualitatively, we show that the proposed model learns a semantically and\nsyntactically meaningful representation of linguistic phrases.",
            "authors": [
                "Kyunghyun Cho",
                "Bart van Merrienboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ]
        },
        {
            "title": "Keras",
            "url": "https://keras.io"
        },
        {
            "title": "Recurrent Shop",
            "url": "https://www.github.com/farizrahman4u/recurrentshop"
        },
        {
            "title": "Training Seq2seq with movie subtitles",
            "url": "https://github.com/nicolas-ivanov/debug_seq2seq"
        },
        {
            "title": " [1] Sequence to Sequence Learning with Neural Networks",
            "url": "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"
        },
        {
            "title": " [3] Neural Machine Translation by Jointly Learning to Align and Translate",
            "url": "http://arxiv.org/pdf/1409.0473v6.pdf"
        },
        {
            "title": " [4] A Neural Conversational Model",
            "url": "http://arxiv.org/pdf/1506.05869v1.pdf"
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999852543663099,
        "task": "Machine Translation",
        "task_prob": 0.9878723825627928
    }
}