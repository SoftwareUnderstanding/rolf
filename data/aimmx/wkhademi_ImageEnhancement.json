{
    "visibility": {
        "visibility": "public"
    },
    "name": "ImageEnhancement",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "wkhademi",
                "owner_type": "User",
                "name": "ImageEnhancement",
                "url": "https://github.com/wkhademi/ImageEnhancement",
                "stars": 45,
                "pushed_at": "2020-12-16 06:41:14+00:00",
                "created_at": "2020-04-17 17:48:58+00:00",
                "language": "Python",
                "description": "Various models for handling underexposure, overexposure, super-resolution, shadow removal, etc.",
                "frameworks": [
                    "Keras",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "3adfc62c83466f9894d4f8cb7063f0f0026d6649",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wkhademi/ImageEnhancement/blob/master/.gitignore"
                    }
                },
                "size": 64
            },
            {
                "type": "code",
                "name": "dataloaders",
                "sha": "e4f6aedc3a25bac1d97d1f5ca476232edba50675",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wkhademi/ImageEnhancement/tree/master/dataloaders"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "datasets",
                "sha": "df0fa18027e08ac00d53a08511f83197de03bc37",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wkhademi/ImageEnhancement/tree/master/datasets"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "download_cyclegan_dataset.sh",
                "sha": "9950fd90ed44fa1780d7571b129299b50029aed7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wkhademi/ImageEnhancement/blob/master/download_cyclegan_dataset.sh"
                    }
                },
                "size": 1331
            },
            {
                "type": "code",
                "name": "models",
                "sha": "7d4fcd6f339e4bcc60f479b2e24924c2f94d1768",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wkhademi/ImageEnhancement/tree/master/models"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "options",
                "sha": "d6a1841379a5dbce81d545f5eed829ba262f380f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wkhademi/ImageEnhancement/tree/master/options"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "test",
                "sha": "7c858de7a5f19e5766dc1437c5bed96355d965b8",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wkhademi/ImageEnhancement/tree/master/test"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "train",
                "sha": "f257daf351689e3af33d408a4f65c5e4f9a07d51",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wkhademi/ImageEnhancement/tree/master/train"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "utils",
                "sha": "1ba7edf7cd9e6ca4d2121600543f0a87939408c0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wkhademi/ImageEnhancement/tree/master/utils"
                    }
                },
                "num_files": 6
            }
        ]
    },
    "authors": [
        {
            "name": "Wesley Khademi",
            "email": "khademiw@oregonstate.edu",
            "github_id": "wkhademi"
        }
    ],
    "tags": [
        "image-enhancement",
        "cyclegan",
        "deshadownet",
        "enlightengan",
        "tensorflow",
        "generative-adversarial-network",
        "deep-learning",
        "srgan",
        "shadow-removal",
        "super-resolution"
    ],
    "description": "Various models for handling underexposure, overexposure, super-resolution, shadow removal, etc.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/wkhademi/ImageEnhancement",
            "stars": 45,
            "issues": true,
            "readme": "# ImageEnhancement\nVarious models for handling underexposure, overexposure, super-resolution, shadow removal, etc.\n\n## Dependencies\n- Python 3.6\n- TensorFlow v1.15\n- OpenCV\n- Pillow\n- scikit-image\n\n## Models\nImplementations of the following models are provided:\n- CycleGAN by Zhu et al.: [Paper](https://arxiv.org/pdf/1703.10593.pdf) | [Code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\n- Mask-ShadowGAN by Hu et al.: [Paper](https://arxiv.org/pdf/1903.10683.pdf) | [Code](https://github.com/xw-hu/Mask-ShadowGAN)\n- EnlightenGAN by Jiang et al.: [Paper](https://arxiv.org/pdf/1906.06972.pdf) | [Code](https://github.com/TAMU-VITA/EnlightenGAN)\n- DeShadowNet by Liangqiong et al. (in progress): [Paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Qu_DeshadowNet_A_Multi-Context_CVPR_2017_paper.pdf) | [Code](https://github.com/Liangqiong/DeShadowNet)\n- SRGAN by Dong et al.: [Paper](https://arxiv.org/pdf/1609.04802.pdf) | [Code](https://github.com/tensorlayer/srgan)\n\n## Datasets\n- Download a CycleGAN dataset using:\n   ```\n   bash ./download_cyclegan_dataset.sh [apple2orange|summer2winter_yosemite|horse2zebra|monet2photo|cezanne2photo|ukiyoe2photo|vangogh2photo|maps|cityscapes|facades|iphone2dslr_flower|ae_photos]\n   ```\n- Download the Unpaired Shadow Removal (USR) dataset for shadow removal from: [USR Dataset](https://drive.google.com/file/d/1PPAX0W4eyfn1cUrb2aBefnbrmhB1htoJ/view)\n- Download the ISTD dataset for shadow removal from: [ISTD Dataset](https://drive.google.com/file/d/1I0qw-65KBA6np8vIZzO6oeiOvcDBttAY/view)\n- Download the EnlightenGAN dataset for low-light image enhancement from: [Google Drive](https://drive.google.com/drive/folders/1fwqz8-RnTfxgIIkebFG2Ej3jQFsYECh0)\n- Download the DIV2K - bicubic downscaling x4 dataset for super-resolution from: [Train Low-Res](https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X4.zip) | [Train High-Res](https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip) | [Test Low-Res](https://data.vision.ee.ethz.ch/cvl/DIV2K/validation_release/DIV2K_test_LR_bicubic_X4.zip)\n\n## Updating PYTHONPATH\nTo ensure all modules in repo can be found you must update your **PYTHONPATH** environment variable:\n```\nexport PYTHONPATH=$PYTHONPATH:/path/to/ImageEnhancement\n```\n\n## Training\n#### CycleGAN\nThe CycleGAN model takes approximately 20 hours to train to completion using a Tesla V100 GPU. To train run:\n```\npython train/cyclegan_train.py --dirA /path/to/dataA  --dirB /path/to/dataB --batch_size 1 --lr 0.0002 --layer_norm_type instance --weight_init_gain 0.02\n```\n\n#### Mask-ShadowGAN\nThe Mask-ShadowGAN model takes approximately 24 hours to train to completion using a Tesla V100 GPU. To train run:\n```\npython train/maskshadowgan_train.py --dirA /path/to/shadow_data --dirB /path/to/shadow_free_data --batch_size 1 --lr 0.0002 --layer_norm_type instance --weight_init_gain 0.02 --lambda_ident 0.5\n```\n\n#### EnlightenGAN\nTo train run:\n```\npython train/enlightengan_train.py --dirA /path/to/low_light_data --dirB /path/to/normal_data --weight_init_gain 0.02 --scale_size 320 --crop_size 320 --patchD_3 5 --self_attention --times_residual --patchD --vgg --patch_vgg --use_ragan --hybrid_loss\n```\n\n#### DeShadowNet\nTo be added...\n\n#### SRGAN\nTo train run:\n```\npython train/srgan_train.py --dir /path/to/high_res_data --batch_size 16 --scale_size 96 --crop_size 384 --weight_init_gain 0.02 --beta1 0.9\n```\n\nTo continue training from a saved checkpoint, add the following argument to the end of the command line arguments passed into the training script you are running:\n```\n--load_model /checkpoint_dir (e.g. /20022019-0801)\n```\n\n## Testing\n#### CycleGAN\nTo test the CycleGAN model run:\n```\npython test/cyclegan_test.py --dir /path/to/dataA --batch_size 1 --layer_norm_type instance --load_model /checkpoint_dir --sample_directory /path/to/save/samples/to\n```\n\n#### Mask-ShadowGAN\nTo test the Mask-ShadowGAN model run:\n```\npython test/maskshadowgan_test.py --dir /path/to/shadow_data --batch_size 1 --layer_norm_type instance --load_model /checkpoint_dir --sample_directory /path/to/save/samples/to\n```\n\n#### EnlightenGAN\nTo test the EnlightenGAN model run:\n```\npython test/enlightengan_test.py --dir /path/to/low_light_data --batch_size 1 --scale_size 320 --crop_size 320 --load_model /checkpoint_dir --sample_directory /path/to/save/samples/to --self_attention --times_residual\n```\n\n#### DeShadowNet\nTo be added...\n\n#### SRGAN\nTo test the SRGAN model run:\n```\npython test/srgan_test.py --dir /path/to/low_res_data --batch_size 1 --load_model /checkpoint_dir --sample_directory /path/to/save/samples/to\n```\n",
            "readme_url": "https://github.com/wkhademi/ImageEnhancement",
            "frameworks": [
                "Keras",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
            "arxiv": "1703.10593",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.10593v7",
            "abstract": "Image-to-image translation is a class of vision and graphics problems where\nthe goal is to learn the mapping between an input image and an output image\nusing a training set of aligned image pairs. However, for many tasks, paired\ntraining data will not be available. We present an approach for learning to\ntranslate an image from a source domain $X$ to a target domain $Y$ in the\nabsence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$\nsuch that the distribution of images from $G(X)$ is indistinguishable from the\ndistribution $Y$ using an adversarial loss. Because this mapping is highly\nunder-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$\nand introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice\nversa). Qualitative results are presented on several tasks where paired\ntraining data does not exist, including collection style transfer, object\ntransfiguration, season transfer, photo enhancement, etc. Quantitative\ncomparisons against several prior methods demonstrate the superiority of our\napproach.",
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A. Efros"
            ]
        },
        {
            "title": "EnlightenGAN: Deep Light Enhancement without Paired Supervision",
            "arxiv": "1906.06972",
            "year": 2019,
            "url": "http://arxiv.org/abs/1906.06972v2",
            "abstract": "Deep learning-based methods have achieved remarkable success in image\nrestoration and enhancement, but are they still competitive when there is a\nlack of paired training data? As one such example, this paper explores the\nlow-light image enhancement problem, where in practice it is extremely\nchallenging to simultaneously take a low-light and a normal-light photo of the\nsame visual scene. We propose a highly effective unsupervised generative\nadversarial network, dubbed EnlightenGAN, that can be trained without\nlow/normal-light image pairs, yet proves to generalize very well on various\nreal-world test images. Instead of supervising the learning using ground truth\ndata, we propose to regularize the unpaired training using the information\nextracted from the input itself, and benchmark a series of innovations for the\nlow-light image enhancement problem, including a global-local discriminator\nstructure, a self-regularized perceptual loss fusion, and attention mechanism.\nThrough extensive experiments, our proposed approach outperforms recent methods\nunder a variety of metrics in terms of visual quality and subjective user\nstudy. Thanks to the great flexibility brought by unpaired training,\nEnlightenGAN is demonstrated to be easily adaptable to enhancing real-world\nimages from various domains. The code is available at\n\\url{https://github.com/yueruchen/EnlightenGAN}",
            "authors": [
                "Yifan Jiang",
                "Xinyu Gong",
                "Ding Liu",
                "Yu Cheng",
                "Chen Fang",
                "Xiaohui Shen",
                "Jianchao Yang",
                "Pan Zhou",
                "Zhangyang Wang"
            ]
        },
        {
            "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
            "arxiv": "1609.04802",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.04802v5",
            "abstract": "Despite the breakthroughs in accuracy and speed of single image\nsuper-resolution using faster and deeper convolutional neural networks, one\ncentral problem remains largely unsolved: how do we recover the finer texture\ndetails when we super-resolve at large upscaling factors? The behavior of\noptimization-based super-resolution methods is principally driven by the choice\nof the objective function. Recent work has largely focused on minimizing the\nmean squared reconstruction error. The resulting estimates have high peak\nsignal-to-noise ratios, but they are often lacking high-frequency details and\nare perceptually unsatisfying in the sense that they fail to match the fidelity\nexpected at the higher resolution. In this paper, we present SRGAN, a\ngenerative adversarial network (GAN) for image super-resolution (SR). To our\nknowledge, it is the first framework capable of inferring photo-realistic\nnatural images for 4x upscaling factors. To achieve this, we propose a\nperceptual loss function which consists of an adversarial loss and a content\nloss. The adversarial loss pushes our solution to the natural image manifold\nusing a discriminator network that is trained to differentiate between the\nsuper-resolved images and original photo-realistic images. In addition, we use\na content loss motivated by perceptual similarity instead of similarity in\npixel space. Our deep residual network is able to recover photo-realistic\ntextures from heavily downsampled images on public benchmarks. An extensive\nmean-opinion-score (MOS) test shows hugely significant gains in perceptual\nquality using SRGAN. The MOS scores obtained with SRGAN are closer to those of\nthe original high-resolution images than to those obtained with any\nstate-of-the-art method.",
            "authors": [
                "Christian Ledig",
                "Lucas Theis",
                "Ferenc Huszar",
                "Jose Caballero",
                "Andrew Cunningham",
                "Alejandro Acosta",
                "Andrew Aitken",
                "Alykhan Tejani",
                "Johannes Totz",
                "Zehan Wang",
                "Wenzhe Shi"
            ]
        },
        {
            "title": "Mask-ShadowGAN: Learning to Remove Shadows from Unpaired Data",
            "arxiv": "1903.10683",
            "year": 2019,
            "url": "http://arxiv.org/abs/1903.10683v3",
            "abstract": "This paper presents a new method for shadow removal using unpaired data,\nenabling us to avoid tedious annotations and obtain more diverse training\nsamples. However, directly employing adversarial learning and cycle-consistency\nconstraints is insufficient to learn the underlying relationship between the\nshadow and shadow-free domains, since the mapping between shadow and\nshadow-free images is not simply one-to-one. To address the problem, we\nformulate Mask-ShadowGAN, a new deep framework that automatically learns to\nproduce a shadow mask from the input shadow image and then takes the mask to\nguide the shadow generation via re-formulated cycle-consistency constraints.\nParticularly, the framework simultaneously learns to produce shadow masks and\nlearns to remove shadows, to maximize the overall performance. Also, we\nprepared an unpaired dataset for shadow removal and demonstrated the\neffectiveness of Mask-ShadowGAN on various experiments, even it was trained on\nunpaired data.",
            "authors": [
                "Xiaowei Hu",
                "Yitong Jiang",
                "Chi-Wing Fu",
                "Pheng-Ann Heng"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "USR Dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://drive.google.com/file/d/1PPAX0W4eyfn1cUrb2aBefnbrmhB1htoJ/view"
                    }
                }
            },
            {
                "name": "ISTD Dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://drive.google.com/file/d/1I0qw-65KBA6np8vIZzO6oeiOvcDBttAY/view"
                    }
                }
            },
            {
                "name": "Cityscapes"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999733854642839,
        "task": "Image-to-Image Translation",
        "task_prob": 0.9489155747276865
    }
}