{
    "visibility": {
        "visibility": "public"
    },
    "name": "Show and Tell: A Neural Image Caption Generator",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "stoensin",
                "owner_type": "User",
                "name": "IC",
                "url": "https://github.com/stoensin/IC",
                "stars": 1,
                "pushed_at": "2019-08-27 08:52:45+00:00",
                "created_at": "2018-09-25 02:53:02+00:00",
                "language": "Python",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "fb46913cc7a5994c4324de50829c95d7858c30f4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/stoensin/IC/blob/master/.gitignore"
                    }
                },
                "size": 100
            },
            {
                "type": "code",
                "name": "WORKSPACE",
                "sha": "22da718b06f9c61be4ffdf45e48919ed4a5f17ae",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/stoensin/IC/blob/master/WORKSPACE"
                    }
                },
                "size": 27
            },
            {
                "type": "code",
                "name": "g3doc",
                "sha": "245f6670d00a290605e7b3377fb3a6bd64e6e15d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/stoensin/IC/tree/master/g3doc"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "im2txt",
                "sha": "11a2c6d2df2c985a045f0e42161467b930766e00",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/stoensin/IC/tree/master/im2txt"
                    }
                },
                "num_files": 12
            }
        ]
    },
    "authors": [
        {
            "name": "choya",
            "github_id": "choya"
        },
        {
            "name": "Yang",
            "email": "laver7@163.com",
            "github_id": "stoensin"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/stoensin/IC",
            "stars": 1,
            "issues": true,
            "readme": "# Show and Tell: A Neural Image Caption Generator\n\nA TensorFlow implementation of the image-to-text model described in the paper:\n\n\"Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning\nChallenge.\"\n\nOriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan.\n\n*IEEE transactions on pattern analysis and machine intelligence (2016).*\n\nFull text available at: http://arxiv.org/abs/1609.06647\n\n## Contact\n***Author:*** Chris Shallue\n\n***Pull requests and issues:*** @cshallue\n\n## Contents\n* [Model Overview](#model-overview)\n    * [Introduction](#introduction)\n    * [Architecture](#architecture)\n* [Getting Started](#getting-started)\n    * [A Note on Hardware and Training Time](#a-note-on-hardware-and-training-time)\n    * [Install Required Packages](#install-required-packages)\n    * [Prepare the Training Data](#prepare-the-training-data)\n    * [Download the Inception v3 Checkpoint](#download-the-inception-v3-checkpoint)\n* [Training a Model](#training-a-model)\n    * [Initial Training](#initial-training)\n    * [Fine Tune the Inception v3 Model](#fine-tune-the-inception-v3-model)\n* [Generating Captions](#generating-captions)\n\n## Model Overview\n\n### Introduction\n\nThe *Show and Tell* model is a deep neural network that learns how to describe\nthe content of images. For example:\n\n![Example captions](g3doc/example_captions.jpg)\n\n### Architecture\n\nThe *Show and Tell* model is an example of an *encoder-decoder* neural network.\nIt works by first \"encoding\" an image into a fixed-length vector representation,\nand then \"decoding\" the representation into a natural language description.\n\nThe image encoder is a deep convolutional neural network. This type of\nnetwork is widely used for image tasks and is currently state-of-the-art for\nobject recognition and detection. Our particular choice of network is the\n[*Inception v3*](http://arxiv.org/abs/1512.00567) image recognition model\npretrained on the\n[ILSVRC-2012-CLS](http://www.image-net.org/challenges/LSVRC/2012/) image\nclassification dataset.\n\nThe decoder is a long short-term memory (LSTM) network. This type of network is\ncommonly used for sequence modeling tasks such as language modeling and machine\ntranslation. In the *Show and Tell* model, the LSTM network is trained as a\nlanguage model conditioned on the image encoding.\n\nWords in the captions are represented with an embedding model. Each word in the\nvocabulary is associated with a fixed-length vector representation that is\nlearned during training.\n\nThe following diagram illustrates the model architecture.\n\n![Show and Tell Architecture](g3doc/show_and_tell_architecture.png)\n\nIn this diagram, \\{*s*<sub>0</sub>, *s*<sub>1</sub>, ..., *s*<sub>*N*-1</sub>\\}\nare the words of the caption and \\{*w*<sub>*e*</sub>*s*<sub>0</sub>,\n*w*<sub>*e*</sub>*s*<sub>1</sub>, ..., *w*<sub>*e*</sub>*s*<sub>*N*-1</sub>\\}\nare their corresponding word embedding vectors. The outputs \\{*p*<sub>1</sub>,\n*p*<sub>2</sub>, ..., *p*<sub>*N*</sub>\\} of the LSTM are probability\ndistributions generated by the model for the next word in the sentence. The\nterms \\{log *p*<sub>1</sub>(*s*<sub>1</sub>),\nlog *p*<sub>2</sub>(*s*<sub>2</sub>), ...,\nlog *p*<sub>*N*</sub>(*s*<sub>*N*</sub>)\\} are the log-likelihoods of the\ncorrect word at each step; the negated sum of these terms is the minimization\nobjective of the model.\n\nDuring the first phase of training the parameters of the *Inception v3* model\nare kept fixed: it is simply a static image encoder function. A single trainable\nlayer is added on top of the *Inception v3* model to transform the image\nembedding into the word embedding vector space. The model is trained with\nrespect to the parameters of the word embeddings, the parameters of the layer on\ntop of *Inception v3* and the parameters of the LSTM. In the second phase of\ntraining, all parameters - including the parameters of *Inception v3* - are\ntrained to jointly fine-tune the image encoder and the LSTM.\n\nGiven a trained model and an image we use *beam search* to generate captions for\nthat image. Captions are generated word-by-word, where at each step *t* we use\nthe set of sentences already generated with length *t* - 1 to generate a new set\nof sentences with length *t*. We keep only the top *k* candidates at each step,\nwhere the hyperparameter *k* is called the *beam size*. We have found the best\nperformance with *k* = 3.\n\n## Getting Started\n\n### A Note on Hardware and Training Time\n\nThe time required to train the *Show and Tell* model depends on your specific\nhardware and computational capacity. In this guide we assume you will be running\ntraining on a single machine with a GPU. In our experience on an NVIDIA Tesla\nK20m GPU the initial training phase takes 1-2 weeks. The second training phase\nmay take several additional weeks to achieve peak performance (but you can stop\nthis phase early and still get reasonable results).\n\nIt is possible to achieve a speed-up by implementing distributed training across\na cluster of machines with GPUs, but that is not covered in this guide.\n\nWhilst it is possible to run this code on a CPU, beware that this may be\napproximately 10 times slower.\n\n### Install Required Packages\nFirst ensure that you have installed the following required packages:\n\n* **Bazel** ([instructions](http://bazel.io/docs/install.html))\n* **TensorFlow** 1.0 or greater ([instructions](https://www.tensorflow.org/install/))\n* **NumPy** ([instructions](http://www.scipy.org/install.html))\n* **Natural Language Toolkit (NLTK)**:\n    * First install NLTK ([instructions](http://www.nltk.org/install.html))\n    * Then install the NLTK data package \"punkt\" ([instructions](http://www.nltk.org/data.html))\n* **Unzip**\n### Prepare the Training Data\n\nTo train the model you will need to provide training data in native TFRecord\nformat. The TFRecord format consists of a set of sharded files containing\nserialized `tf.SequenceExample` protocol buffers. Each `tf.SequenceExample`\nproto contains an image (JPEG format), a caption and metadata such as the image\nid.\n\nEach caption is a list of words. During preprocessing, a dictionary is created\nthat assigns each word in the vocabulary to an integer-valued id. Each caption\nis encoded as a list of integer word ids in the `tf.SequenceExample` protos.\n\nWe have provided a script to download and preprocess the [MSCOCO](http://mscoco.org/) image captioning data set into this format. Downloading\nand preprocessing the data may take several hours depending on your network and\ncomputer speed. Please be patient.\n\nBefore running the script, ensure that your hard disk has at least 150GB of\navailable space for storing the downloaded and processed data.\n\n```shell\n# Location to save the MSCOCO data.\nMSCOCO_DIR=\"${HOME}/im2txt/data/mscoco\"\n\n# Build the preprocessing script.\ncd research/im2txt\nbazel build //im2txt:download_and_preprocess_mscoco\n\n# Run the preprocessing script.\nbazel-bin/im2txt/download_and_preprocess_mscoco \"${MSCOCO_DIR}\"\n```\n\nThe final line of the output should read:\n\n```\n2016-09-01 16:47:47.296630: Finished processing all 20267 image-caption pairs in data set 'test'.\n```\n\nWhen the script finishes you will find 256 training, 4 validation and 8 testing\nfiles in `DATA_DIR`. The files will match the patterns `train-?????-of-00256`,\n`val-?????-of-00004` and `test-?????-of-00008`, respectively.\n\n### Download the Inception v3 Checkpoint\n\nThe *Show and Tell* model requires a pretrained *Inception v3* checkpoint file\nto initialize the parameters of its image encoder submodel.\n\nThis checkpoint file is provided by the\n[TensorFlow-Slim image classification library](https://github.com/tensorflow/models/tree/master/research/slim#tensorflow-slim-image-classification-library)\nwhich provides a suite of pre-trained image classification models. You can read\nmore about the models provided by the library\n[here](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models).\n\n\nRun the following commands to download the *Inception v3* checkpoint.\n\n```shell\n# Location to save the Inception v3 checkpoint.\nINCEPTION_DIR=\"${HOME}/im2txt/data\"\nmkdir -p ${INCEPTION_DIR}\n\nwget \"http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\"\ntar -xvf \"inception_v3_2016_08_28.tar.gz\" -C ${INCEPTION_DIR}\nrm \"inception_v3_2016_08_28.tar.gz\"\n```\n\nNote that the *Inception v3* checkpoint will only be used for initializing the\nparameters of the *Show and Tell* model. Once the *Show and Tell* model starts\ntraining it will save its own checkpoint files containing the values of all its\nparameters (including copies of the *Inception v3* parameters). If training is\nstopped and restarted, the parameter values will be restored from the latest\n*Show and Tell* checkpoint and the *Inception v3* checkpoint will be ignored. In\nother words, the *Inception v3* checkpoint is only used in the 0-th global step\n(initialization) of training the *Show and Tell* model.\n\n## Training a Model\n\n### Initial Training\n\nRun the training script.\n\n```shell\n# Directory containing preprocessed MSCOCO data.\nMSCOCO_DIR=\"${HOME}/im2txt/data/mscoco\"\n\n# Inception v3 checkpoint file.\nINCEPTION_CHECKPOINT=\"${HOME}/im2txt/data/inception_v3.ckpt\"\n\n# Directory to save the model.\nMODEL_DIR=\"${HOME}/im2txt/model\"\n\n# Build the model.\ncd research/im2txt\nbazel build -c opt //im2txt/...\n\n# Run the training script.\nbazel-bin/im2txt/train \\\n  --input_file_pattern=\"${MSCOCO_DIR}/train-?????-of-00256\" \\\n  --inception_checkpoint_file=\"${INCEPTION_CHECKPOINT}\" \\\n  --train_dir=\"${MODEL_DIR}/train\" \\\n  --train_inception=false \\\n  --number_of_steps=1000000\n```\n\nRun the evaluation script in a separate process. This will log evaluation\nmetrics to TensorBoard which allows training progress to be monitored in\nreal-time.\n\nNote that you may run out of memory if you run the evaluation script on the same\nGPU as the training script. You can run the command\n`export CUDA_VISIBLE_DEVICES=\"\"` to force the evaluation script to run on CPU.\nIf evaluation runs too slowly on CPU, you can decrease the value of\n`--num_eval_examples`.\n\n```shell\nMSCOCO_DIR=\"${HOME}/im2txt/data/mscoco\"\nMODEL_DIR=\"${HOME}/im2txt/model\"\n\n# Ignore GPU devices (only necessary if your GPU is currently memory\n# constrained, for example, by running the training script).\nexport CUDA_VISIBLE_DEVICES=\"\"\n\n# Run the evaluation script. This will run in a loop, periodically loading the\n# latest model checkpoint file and computing evaluation metrics.\nbazel-bin/im2txt/evaluate \\\n  --input_file_pattern=\"${MSCOCO_DIR}/val-?????-of-00004\" \\\n  --checkpoint_dir=\"${MODEL_DIR}/train\" \\\n  --eval_dir=\"${MODEL_DIR}/eval\"\n```\n\nRun a TensorBoard server in a separate process for real-time monitoring of\ntraining progress and evaluation metrics.\n\n```shell\nMODEL_DIR=\"${HOME}/im2txt/model\"\n\n# Run a TensorBoard server.\ntensorboard --logdir=\"${MODEL_DIR}\"\n```\n\n### Fine Tune the Inception v3 Model\n\nYour model will already be able to generate reasonable captions after the first\nphase of training. Try it out! (See [Generating Captions](#generating-captions)).\n\nYou can further improve the performance of the model by running a\nsecond training phase to jointly fine-tune the parameters of the *Inception v3*\nimage submodel and the LSTM.\n\n```shell\n# Restart the training script with --train_inception=true.\nbazel-bin/im2txt/train \\\n  --input_file_pattern=\"${MSCOCO_DIR}/train-?????-of-00256\" \\\n  --train_dir=\"${MODEL_DIR}/train\" \\\n  --train_inception=true \\\n  --number_of_steps=3000000  # Additional 2M steps (assuming 1M in initial training).\n```\n\nNote that training will proceed much slower now, and the model will continue to\nimprove by a small amount for a long time. We have found that it will improve\nslowly for an additional 2-2.5 million steps before it begins to overfit. This\nmay take several weeks on a single GPU. If you don't care about absolutely\noptimal performance then feel free to halt training sooner by stopping the\ntraining script or passing a smaller value to the flag `--number_of_steps`. Your\nmodel will still work reasonably well.\n\n## Generating Captions\n\nYour trained *Show and Tell* model can generate captions for any JPEG image! The\nfollowing command line will generate captions for an image from the test set.\n\n```shell\n# Path to checkpoint file or a directory containing checkpoint files. Passing\n# a directory will only work if there is also a file named 'checkpoint' which\n# lists the available checkpoints in the directory. It will not work if you\n# point to a directory with just a copy of a model checkpoint: in that case,\n# you will need to pass the checkpoint path explicitly.\nCHECKPOINT_PATH=\"${HOME}/im2txt/model/train\"\n\n# Vocabulary file generated by the preprocessing script.\nVOCAB_FILE=\"${HOME}/im2txt/data/mscoco/word_counts.txt\"\n\n# JPEG image file to caption.\nIMAGE_FILE=\"${HOME}/im2txt/data/mscoco/raw-data/val2014/COCO_val2014_000000224477.jpg\"\n\n# Build the inference binary.\ncd research/im2txt\nbazel build -c opt //im2txt:run_inference\n\n# Ignore GPU devices (only necessary if your GPU is currently memory\n# constrained, for example, by running the training script).\nexport CUDA_VISIBLE_DEVICES=\"\"\n\n# Run inference to generate captions.\nbazel-bin/im2txt/run_inference \\\n  --checkpoint_path=${CHECKPOINT_PATH} \\\n  --vocab_file=${VOCAB_FILE} \\\n  --input_files=${IMAGE_FILE}\n```\n\nExample output:\n\n```\nCaptions for image COCO_val2014_000000224477.jpg:\n  0) a man riding a wave on top of a surfboard . (p=0.040413)\n  1) a person riding a surf board on a wave (p=0.017452)\n  2) a man riding a wave on a surfboard in the ocean . (p=0.005743)\n```\n\nNote: you may get different results. Some variation between different models is\nexpected.\n\nHere is the image:\n\n![Surfer](g3doc/COCO_val2014_000000224477.jpg)\n",
            "readme_url": "https://github.com/stoensin/IC",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Rethinking the Inception Architecture for Computer Vision",
            "arxiv": "1512.00567",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.00567v3",
            "abstract": "Convolutional networks are at the core of most state-of-the-art computer\nvision solutions for a wide variety of tasks. Since 2014 very deep\nconvolutional networks started to become mainstream, yielding substantial gains\nin various benchmarks. Although increased model size and computational cost\ntend to translate to immediate quality gains for most tasks (as long as enough\nlabeled data is provided for training), computational efficiency and low\nparameter count are still enabling factors for various use cases such as mobile\nvision and big-data scenarios. Here we explore ways to scale up networks in\nways that aim at utilizing the added computation as efficiently as possible by\nsuitably factorized convolutions and aggressive regularization. We benchmark\nour methods on the ILSVRC 2012 classification challenge validation set\ndemonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6%\ntop-5 error for single frame evaluation using a network with a computational\ncost of 5 billion multiply-adds per inference and with using less than 25\nmillion parameters. With an ensemble of 4 models and multi-crop evaluation, we\nreport 3.5% top-5 error on the validation set (3.6% error on the test set) and\n17.3% top-1 error on the validation set.",
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jonathon Shlens",
                "Zbigniew Wojna"
            ]
        },
        {
            "title": "Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge",
            "arxiv": "1609.06647",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.06647v1",
            "abstract": "Automatically describing the content of an image is a fundamental problem in\nartificial intelligence that connects computer vision and natural language\nprocessing. In this paper, we present a generative model based on a deep\nrecurrent architecture that combines recent advances in computer vision and\nmachine translation and that can be used to generate natural sentences\ndescribing an image. The model is trained to maximize the likelihood of the\ntarget description sentence given the training image. Experiments on several\ndatasets show the accuracy of the model and the fluency of the language it\nlearns solely from image descriptions. Our model is often quite accurate, which\nwe verify both qualitatively and quantitatively. Finally, given the recent\nsurge of interest in this task, a competition was organized in 2015 using the\nnewly released COCO dataset. We describe and analyze the various improvements\nwe applied to our own baseline and show the resulting performance in the\ncompetition, which we won ex-aequo with a team from Microsoft Research, and\nprovide an open source implementation in TensorFlow.",
            "authors": [
                "Oriol Vinyals",
                "Alexander Toshev",
                "Samy Bengio",
                "Dumitru Erhan"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "MSCOCO"
            },
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9991979094537298,
        "task": "Image Classification",
        "task_prob": 0.988016084273044
    }
}