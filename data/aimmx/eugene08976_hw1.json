{
    "visibility": {
        "visibility": "public"
    },
    "name": "ISA525700 Computer Vision for Visual EffectsHomework 1 (Color-Transfer and Texture-Transfer)",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "eugene08976",
                "owner_type": "User",
                "name": "hw1",
                "url": "https://github.com/eugene08976/hw1",
                "stars": 0,
                "pushed_at": "2019-04-16 04:27:04+00:00",
                "created_at": "2019-04-01 11:28:58+00:00",
                "language": null,
                "frameworks": []
            }
        ]
    },
    "authors": [
        {
            "name": "EugeneLan",
            "github_id": "eugene08976"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/eugene08976/hw1",
            "stars": 0,
            "issues": true,
            "readme": "ISA525700 Computer Vision for Visual Effects<br/>Homework 1 (Color-Transfer and Texture-Transfer)\n===\n\n## [Cycle GAN](https://arxiv.org/pdf/1703.10593.pdf)\n### Introduction\nCycle gan\u8ddf\u50b3\u7d71\u7684gan\u505a\u5716\u50cf\u8f49\u63db\u7684\u65b9\u5f0f\u4e0d\u540c\uff0c\u5b83\u4e0d\u9700\u8981\u914d\u5c0d\u7684\u6578\u64da\u96c6(paired image data set)\uff1b\u5229\u7528\u5169\u500bgenerator\u3001discrimnator\u548c\u8f49\u63db\u7684\u4e00\u81f4\u6027(consistency)\uff0ccycle gan\u53ea\u9700\u8981\u4e0d\u540c\u98a8\u683c\u7684unpaired image data set\u5373\u53ef\u904b\u4f5c\u3002\n\n### Architecture\nCycle gan\u7e3d\u5171\u6709\u5169\u500bGenertoar(G, F)\uff0c\u5206\u5225\u628a\u5716\u50cf\u5f9eDomain X\u5230Domain Y\uff0c\u8ddf\u53cd\u5411\uff1b\u540c\u6642\u4e5f\u6709\u5169\u500b\u76f8\u5c0d\u61c9\u7684Discriminator(DY, DX)\u3002\n\n\u4e0d\u904e\uff0c\u56e0\u70bainput\u70ba\u672a\u914d\u5c0d\u6578\u64da\u96c6\uff0c\u56e0\u6b64\u5728\u98a8\u683c\u8f49\u63db\u5f8c(X -> Y')\uff0c\u9084\u9700\u8981\u5c07\u7d50\u679c\u518d\u9006\u8f49\u63db\u56de\u4f86(Y' -> X')\uff0c\u4e26\u505a\u6bd4\u8f03\u4ee5\u78ba\u4fdd\u8f49\u63db\u904e\u5f8c\u7d50\u69cb\u76f8\u4f3c\u3002\n\n![](https://i.imgur.com/vqnmh1F.png)\n\n### Loss Function\n```\nL(G, F, DX, DY) = Lgan(G, DY, X, Y) + Lgan(F, DX, Y, X) +\u3000\u03bbLcyc(G, F) \n```\n_Lcyc(G, F) = Ex\u223c     pdata(x)[\u2016F(G(x))\u2212x\u20161] + Ey\u223cpdata(y)[\u2016G(F(y))\u2212y\u20161], whcih represents loss of structure cmparison_\n\n### Training Process\n![](https://i.imgur.com/9UhfBha.png)\n\n\n### Inference Cycle GAN in personal image\n#### orange2apple\n\n\nresult v.s. original image\n\n\n![](https://i.imgur.com/CFhqolS.png)\n\n![](https://i.imgur.com/r7Wg1lO.png)\n\n#### apple2orange\n\n\nresult v.s. original image\n\n\n![](https://i.imgur.com/vwa18ZX.png)\n\n![](https://i.imgur.com/hQtVrfa.png)\n\n![](https://i.imgur.com/uAqtVHE.png)\n\n#### \n\n## Other Methods\n### [Color Transfer Between Images](https://www.cs.tau.ac.il/~turkel/imagepapers/ColorTransfer.pdf)\n\n#### Introduction\n\u8a72\u8ad6\u6587\u5229\u7528\u7c21\u55ae\u7684\u7d71\u8a08\u5206\u6790\uff0c\u5c07\u4e00\u5f35\u5716\u7684\u984f\u8272\u7279\u5fb5\u8f49\u79fb\u5230\u53e6\u5916\u4e00\u5f35\u5716\u4e0a\uff0c\u5176\u4e2d\uff0c\u8272\u5f69\u6821\u6b63\u7684\u90e8\u5206\u4e3b\u8981\u662f\u85c9\u7531\u9078\u64c7\u5408\u9069\u7684source image\uff0c\u4e26\u5c07\u5176\u7279\u5fb5\u61c9\u7528\u5230target image\u4e0a\u4f86\u5be6\u73fe\u3002\n\n#### Algorithm\n1. \u5c07source image\u548ctarget image\u7531RGB\u8f49\u70baLAB\n2. \u8a08\u7b97source image\u548ctarget image\u5728LAB color space\u4e09\u500bchannel\u7684\u6a19\u6e96\u5dee\u548c\u5e73\u5747\u503c\n3. \u5c07source image\u7684pixel\u503c\u6e1b\u53bb\u5e73\u5747pixel\u503c\uff0c\u518d\u9664\u4ee5\u6a19\u6e96\u5dee\uff0c\u4e4b\u5f8c\u52a0\u4e0atarget image\u7684\u5e73\u5747\u503c\uff0c\u5f97\u5230\u6700\u5f8c\u7684LAB\u503c\n4. \u5f9eLAB\u8f49\u63db\u70baRGB\n\n#### Concept\nRGB\u4e09\u500bchannel\u5177\u6709\u76f8\u7576\u5f37\u7684\u95dc\u806f\u6027\uff0c\u800c\u505acolor transfer\u7684\u540c\u6642\uff0c\u9069\u7576\u5730\u6539\u8b8a\u4e09\u500bchannel\u6bd4\u8f03\u56f0\u96e3\uff0c\u56e0\u6b64\u9078\u64c7\u5229\u7528\u4e09\u500bchannel\u4e92\u4e0d\u76f8\u95dc\u7684LAB color space\u4f86\u9032\u884c\u8a08\u7b97\uff0c\u904e\u7a0b\u4e2d\uff0c\u628a\u6574\u500b\u5206\u4f48\u8f49\u63db\uff0c\u4f7fLAB color space\u7684\u6bcf\u500b\u7dad\u5ea6\u90fd\u662f\u539f\u672cRGB \u503c\u7684\u7dda\u6027\u7d44\u5408\uff0c\u6bd4\u8f03\u6703\u9867\u53ca\u76f8\u4f3c\u984f\u8272\u9593\u7684\u76f8\u5c0d\u95dc\u4fc2\u3002\n\n#### Example\n|![](https://i.imgur.com/RrztFYY.jpg)|![](https://i.imgur.com/APtBT2Q.jpg)|![](https://i.imgur.com/j4ttZKo.jpg)|\n| ----------------- | --------------- | --------------- |\n\n### [A Neural Algorithm of Artistic Style](https://arxiv.org/pdf/1508.06576.pdf)\n#### Introduction\n\n\u8a72\u8ad6\u6587\u900f\u904e\u5377\u7a4d\u795e\u7d93\u7db2\u8def\uff0c\u5c07\u5716\u7247\u7684\u5167\u5bb9\u53ca\u98a8\u683c\u5206\u958b\u4e26\u91cd\u5efa\uff0c\u63d0\u4f9b\u4e00\u500bstyle transfer\u7684\u505a\u6cd5\u3002\n#### Architecture\n![](https://i.imgur.com/kuZTYgs.png)\n##### Concept\n\u5229\u7528pre-trained VGG19 model\u63d0\u53d6\u5716\u7247\u4e0d\u540clayer\u7684\u7279\u5fb5\uff0c\u4f5c\u70ba\u5716\u50cf\u548c\u98a8\u683c\u7684\u7279\u5fb5\u3002\n- Content:\u5728network\u4e2d\uff0c\u8f03\u4f4e\u5c64\u7d1a\u4fdd\u7559\u8f03\u591a\u7684\u539f\u59cb\u5716\u50cf\uff0c\u800c\u8f03\u9ad8\u5c64\u7d1a\u4e2d\u96d6\u7136\u6c92\u6709\u7d30\u7bc0\u7684pixel\u4f46\u4fdd\u7559\u4e86high level\u7684\u5167\u5bb9feature\uff0c\u56e0\u6b64\u53ef\u4ee5\u5229\u7528\u9ad8\u5c64\u7684\u7279\u5fb5\u4f86\u5c0d\u5716\u50cf\u505arecontruct\u3002\n- Style:\u5728\u539f\u672c\u7684CNN\u67b6\u69cb\u4e0a\u5efa\u7acb\u65b0\u7684feature space\u4f86\u63d0\u53d6\u98a8\u683c\u7684\u7279\u5fb5\u3002\u4f5c\u6cd5\u70ba\u8a08\u7b97\u4e0d\u540clayer\u6240\u7522\u751f\u7684\u4e0d\u540cfeature\u4e4b\u9593\u7684correlations\u3002\u5728reconstruction\u4e2d\uff0c\u8a72\u7d50\u679c\u78ba\u5be6\u7372\u5f97\u7684\u539f\u59cb\u5f71\u50cf\u7684texture\u3001color\u7b49\u7279\u5fb5\u3002\n\n#### Loss Function\n- Loss Function = content loss + style loss\n- Content loss : \u539f\u5716\u8207\u9810\u6e2c\u5716\u7684content feature\u5dee\u8ddd\n- Style loss : \u539f\u5716\u8207\u9810\u6e2c\u5716\u7684style feature\u5dee\u8ddd\n\n#### Flow\n![](https://i.imgur.com/zVpm139.png)\n$G^{l}_{ij}$ is the inner product between the vectorised feature maps of the initial image $i$ and $j$ in layer $l$,\n$w_{l}$ is the weight of each style layers\n$A_l$ is that of the style image\n$F_l$ is layer-wise content features of the initial image\n$P_l$ is that of the content image\n* $\\alpha$ and $\\beta$ is the content weight and   style weight, respectively that controls the weighting factors for content and style reconstruction.\n\n#### Example\n![](https://i.imgur.com/felJKFp.jpg)\n\n![](https://i.imgur.com/FkezUlt.png)\n\n\n\n### [Universal Style Transfer](https://arxiv.org/pdf/1705.08086.pdf)\n#### Introduction\n\u8a72\u8ad6\u6587\u63d0\u51fa\u4e86\u4e00\u500b\u901a\u7528\u7684reconstruction network\uff0c\u5e0c\u671b\u80fd\u5920\u5c0d\u8f38\u5165\u7684\u4efb\u610fstyle\u9032\u884ctransfer\uff0c\u800c\u4e0d\u9700\u8981\u91cd\u65b0train model\uff1b\u63db\u53e5\u8a71\u8aaa\uff0c\u5c31\u662f\u5e0c\u671b\u80fd\u5920\u4f7f\u7528\u4efb\u610f\u7684reference image\u4f86\u9032\u884cstyle transfer\uff0c\u64fa\u812b\u50b3\u7d71\u7684style transfer\u5c0d\u65bcstyle\u548ccontent loss\u9700\u8981\u901a\u904e\u5c0dlayer\u7684\u5617\u8a66\u53c3\u6578\uff0c\u4f86\u5f97\u5230\u4e00\u500b\u548cstyle\u8f03\u7232\u5339\u914d\u7684\u8868\u8ff0\u7e94\u80fd\u6709\u8f03\u597d\u7684\u6548\u679c\uff0c\u4e14\u91dd\u5c0d\u4e0d\u540c\u7684style\u9019\u4e00\u6b65\u9a5f\u9700\u8981\u91cd\u65b0training\u9019\u6a23\u7684\u7f3a\u9ede\u3002\n\u8a72\u8ad6\u6587\u63d0\u51fa\u4e86Whitening & Coloring transform layer (WCT layer)\uff0c\u5b83\u7684\u5be6\u4f5c\u89c0\u5ff5\u5728\u65bc\uff0c\u5c0d\u65bc\u4efb\u4f55\u4e00\u7a2estyle image(reference image)\uff0c\u8981\u80fd\u5920\u4f7fcontent\u8868\u73fe\u51fastyle\u7684\u98a8\u683c\uff0c\u53ea\u9700\u5728feature map\u4e0a\u5206\u5e03\u8868\u5fb5\u4e00\u81f4\u3002\n\u9996\u5148\uff0c\u5c07feature map\u6e1b\u53bb\u5e73\u5747\u503c\uff0c\u7136\u5f8c\u4e58\u4e0a\u5c0d\u81ea\u5df1\u7684\u5354\u65b9\u5dee\u77e9\u9663\u7684\u9006\u77e9\u9663\uff0c\u4f86\u9032\u884cwhitening\u7684\u52d5\u4f5c\uff0c\u4ee5\u5229\u5c07feature map\u62c9\u5230\u4e00\u500b\u767d\u8a71\u7684\u5206\u5e03\u7a7a\u9593\u3002\u7136\u5f8c\u900f\u904e\u5c0dreference image\u53d6\u5f97feature map\u7684coloring\u5354\u65b9\u5dee\u77e9\u9663\u7684\u65b9\u5f0f\uff0c\u5c07\u5176\u4e58\u4ee5content image whitening\u5f8c\u7684\u7d50\u679c\uff0c\u4e26\u52a0\u4e0a\u5e73\u5747\u503c\uff0c\u5c31\u53ef\u4ee5\u5c07content image whitening\u5f8c\u7684feature map\u7a7a\u9593\u8f49\u79fb\u5230reference image\u5716\u7247\u4e0a\u5e73\u5747\u5206\u5e03\uff1b\u6700\u5f8c\uff0c\u900f\u904eStylization Weight Control \u7684\u516c\u5f0f\uff1a\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\widehat{f_{cs}}&space;=&space;\\alpha&space;\\widehat{f_{cs}}&space;&plus;&space;(1&space;-&space;\\alpha)\\widehat{f_c}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\widehat{f_{cs}}&space;=&space;\\alpha&space;\\widehat{f_{cs}}&space;&plus;&space;(1&space;-&space;\\alpha)\\widehat{f_c}\" title=\"\\widehat{f_{cs}} = \\alpha \\widehat{f_{cs}} + (1 - \\alpha)\\widehat{f_c}\" /></a>\n\n\u5c31\u53ef\u4ee5\u5b8c\u6210\u5c07reference image\u6574\u5408input image\u7684\u52d5\u4f5c\u3002\n\n#### Pipeline\n![](https://i.imgur.com/V6eutky.png)\n\n#### Loss Function\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=L&space;=&space;\\left&space;\\|&space;I_{0}&space;-&space;I_{i}\\right&space;\\|_{a}^{b}&space;&plus;&space;\\lambda&space;\\left&space;\\|&space;\\Phi&space;(I_{0})&space;-&space;\\Phi&space;(I_{i})\\right&space;\\|_{a}^{b}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L&space;=&space;\\left&space;\\|&space;I_{0}&space;-&space;I_{i}\\right&space;\\|_{a}^{b}&space;&plus;&space;\\lambda&space;\\left&space;\\|&space;\\Phi&space;(I_{0})&space;-&space;\\Phi&space;(I_{i})\\right&space;\\|_{a}^{b}\" title=\"L = \\left \\| I_{0} - I_{i}\\right \\|_{a}^{b} + \\lambda \\left \\| \\Phi (I_{0}) - \\Phi (I_{i})\\right \\|_{a}^{b}\" /></a>\n\n#### Example\n![](https://i.imgur.com/0n5p0oR.png)\n\n\n### Comparisons \n#### photo2monet\n| content  | style    | Cycle GAN (with monet style)| Neural Algorithm|  Universal| color transfer| \n| ----------------- | --------------- |--------------- | --------------- |--------------- | --------------- | \n|![](https://i.imgur.com/QKJGL2S.jpg) |![](https://i.imgur.com/DfxqM53.jpg)|![](https://i.imgur.com/3QEVtf2.png)|![](https://i.imgur.com/SkBQ9Ld.jpg)|![](https://i.imgur.com/JCvdIM1.jpg)|![](https://i.imgur.com/2BfmXFP.jpg)|\n|![](https://i.imgur.com/yY5xJ4I.jpg)|![](https://i.imgur.com/DfxqM53.jpg)|![](https://i.imgur.com/CqO1e7S.png)|![](https://i.imgur.com/oWwDxsn.jpg)|![](https://i.imgur.com/HYbzKdI.jpg)|![](https://i.imgur.com/PRdClBu.jpg)|\n|![](https://i.imgur.com/CsHBjda.jpg)|![](https://i.imgur.com/DfxqM53.jpg)|![](https://i.imgur.com/mfBttGE.png)|![](https://i.imgur.com/vQlLvVi.jpg)|![](https://i.imgur.com/pH2wO1k.jpg)|![](https://i.imgur.com/R0U2VHf.jpg)|\n|![](https://i.imgur.com/xSwyzfy.jpg)|![](https://i.imgur.com/DfxqM53.jpg)|![](https://i.imgur.com/wyaTbrS.png)|![](https://i.imgur.com/fSnrbKI.jpg)|![](https://i.imgur.com/ENnqJrJ.jpg)|![](https://i.imgur.com/t3VX7rv.jpg)|\n|![](https://i.imgur.com/QKJGL2S.jpg)|![](https://i.imgur.com/MqSFc8o.jpg)|![](https://i.imgur.com/3QEVtf2.png)|![](https://i.imgur.com/QyUaFj9.jpg)|![](https://i.imgur.com/HxEPSTj.jpg)|![](https://i.imgur.com/5LundSa.jpg)|\n|![](https://i.imgur.com/yY5xJ4I.jpg)|![](https://i.imgur.com/MqSFc8o.jpg)|![](https://i.imgur.com/CqO1e7S.png)|![](https://i.imgur.com/Cv0qqo4.jpg)|![](https://i.imgur.com/FRbpL1q.jpg)|![](https://i.imgur.com/BCB7kWi.jpg)|\n|![](https://i.imgur.com/CsHBjda.jpg)|![](https://i.imgur.com/MqSFc8o.jpg)|![](https://i.imgur.com/mfBttGE.png)|![](https://i.imgur.com/RwD8Ydx.jpg)|![](https://i.imgur.com/2WomYI2.jpg)|![](https://i.imgur.com/t3Xn9MP.jpg)|\n|![](https://i.imgur.com/DgAhzHT.jpg)|![](https://i.imgur.com/MqSFc8o.jpg)|![](https://i.imgur.com/wyaTbrS.png)|![](https://i.imgur.com/VvTOuo1.jpg)|![](https://i.imgur.com/dxNVGAH.jpg)|![](https://i.imgur.com/iQYs7er.jpg)|\n\n\n#### Features\n\n| Method    |  Aspect  |  Algorithm   | Input | Output\n| ----------------- | --------------- | --------------- | --------------- | ---------------\n| Cycle GAN  | color transfer, style transfer...   | Generative and Discriminative model   | two sets of unpaired image     | general style  image\n| Neural | style transfer  | neural network   |   Content Image v.s. Style Image  | single-image-style image\n| Universal | style transfer   | neural network   | Content Image v.s. Style Image      | single-image-style image\n Color Transfer| Color Transfer| Statistical Transform| source image v.s. target image| single-image-color image\n\n#### Conclusion\n- Traditional:\n    - Color transfer: \u4e3b\u8981\u662f\u5229\u7528\u6578\u5b78\u7684\u65b9\u6cd5\u5c07target image\u7dda\u6027\u8f49\u63db\u70ba\u8207source image\u76f8\u4f3c\u7684\u8272\u5f69\u4e3b\u8abf\uff0c\u8f03\u80fd\u91dd\u5c0d\u4f9d\u8cf4\u8272\u5f69\u7684\u98a8\u683c\uff0c\u800c\u4e0d\u9069\u7528\u65bc\u7b46\u89f8\u3001\u82b1\u7d0b\u7b49\u5176\u4ed6\u7279\u5fb5\u3002\n- GAN \uff1a\n    - Cycle GAN: \u5728\u67b6\u69cb\u4e0a\uff0ccycle GAN\u6240\u5b78\u7fd2\u5230\u7684\u70ba\u5982\u4f55\u7522\u751f\u6307\u5b9a\u7684\u67d0\u4e00\u7a2etexture,style\u7684\u5716\u7247\uff08from training data)\uff0c\u5176\u65b9\u6cd5\u901a\u904eunpaired image\u7d93\u904eGEN\u548cDIS\u4f86\u53cd\u8986\u66f4\u65b0\u9054\u6210\uff0c\u9019\u7a2e\u67b6\u69cb\u9664\u4e86\u80fd\u8655\u7406style\u5916\uff0c\u5728\u5176\u4ed6case\u4e0a\u4e5f\u80fd\u6709\u597d\u7684\u7d50\u679c\u3002\n- CNN model : \n    - Neural Style Transfer : \u900f\u904eCNN model\u5c07\u5716\u7247\u7684\u5169\u7a2efeature\u7684\u63d0\u53d6\u9032\u884creconstruction\uff0c\u6240\u91dd\u5c0d\u7684\u70ba\u67d0\u4e00\u5f35\u5716\u7684\u98a8\u683c\uff0c\u4e26\u7279\u904econtent\u3001style factor\u4f86\u6c7a\u5b9astyle \u6bd4\u91cd\u4f86\u5b8c\u6210\u98a8\u683c\u8f49\u63db\u3002\n    - Universal Style Transfer : \u5728\u4e00\u500bcontent layer\u4e2d\u4f7f\u7528whitening and coloring transforms\uff0c\u540c\u6642\u589e\u52a0\u4e00\u500bpre-trained\u7684general encoder-decoder network\uff0c\u4f7f\u540c\u4e00\u500bmodel\u53ef\u4ee5\u61c9\u7528\u5728\u4e0d\u540cstyle\uff0c\u9054\u5230Universal\u7684\u6548\u679c\u3002 \n\n\n",
            "readme_url": "https://github.com/eugene08976/hw1",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
            "arxiv": "1703.10593",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.10593v7",
            "abstract": "Image-to-image translation is a class of vision and graphics problems where\nthe goal is to learn the mapping between an input image and an output image\nusing a training set of aligned image pairs. However, for many tasks, paired\ntraining data will not be available. We present an approach for learning to\ntranslate an image from a source domain $X$ to a target domain $Y$ in the\nabsence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$\nsuch that the distribution of images from $G(X)$ is indistinguishable from the\ndistribution $Y$ using an adversarial loss. Because this mapping is highly\nunder-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$\nand introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice\nversa). Qualitative results are presented on several tasks where paired\ntraining data does not exist, including collection style transfer, object\ntransfiguration, season transfer, photo enhancement, etc. Quantitative\ncomparisons against several prior methods demonstrate the superiority of our\napproach.",
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A. Efros"
            ]
        },
        {
            "title": "Universal Style Transfer via Feature Transforms",
            "arxiv": "1705.08086",
            "year": 2017,
            "url": "http://arxiv.org/abs/1705.08086v2",
            "abstract": "Universal style transfer aims to transfer arbitrary visual styles to content\nimages. Existing feed-forward based methods, while enjoying the inference\nefficiency, are mainly limited by inability of generalizing to unseen styles or\ncompromised visual quality. In this paper, we present a simple yet effective\nmethod that tackles these limitations without training on any pre-defined\nstyles. The key ingredient of our method is a pair of feature transforms,\nwhitening and coloring, that are embedded to an image reconstruction network.\nThe whitening and coloring transforms reflect a direct matching of feature\ncovariance of the content image to a given style image, which shares similar\nspirits with the optimization of Gram matrix based cost in neural style\ntransfer. We demonstrate the effectiveness of our algorithm by generating\nhigh-quality stylized images with comparisons to a number of recent methods. We\nalso analyze our method by visualizing the whitened features and synthesizing\ntextures via simple feature coloring.",
            "authors": [
                "Yijun Li",
                "Chen Fang",
                "Jimei Yang",
                "Zhaowen Wang",
                "Xin Lu",
                "Ming-Hsuan Yang"
            ]
        },
        {
            "title": "A Neural Algorithm of Artistic Style",
            "arxiv": "1508.06576",
            "year": 2015,
            "url": "http://arxiv.org/abs/1508.06576v2",
            "abstract": "In fine art, especially painting, humans have mastered the skill to create\nunique visual experiences through composing a complex interplay between the\ncontent and style of an image. Thus far the algorithmic basis of this process\nis unknown and there exists no artificial system with similar capabilities.\nHowever, in other key areas of visual perception such as object and face\nrecognition near-human performance was recently demonstrated by a class of\nbiologically inspired vision models called Deep Neural Networks. Here we\nintroduce an artificial system based on a Deep Neural Network that creates\nartistic images of high perceptual quality. The system uses neural\nrepresentations to separate and recombine content and style of arbitrary\nimages, providing a neural algorithm for the creation of artistic images.\nMoreover, in light of the striking similarities between performance-optimised\nartificial neural networks and biological vision, our work offers a path\nforward to an algorithmic understanding of how humans create and perceive\nartistic imagery.",
            "authors": [
                "Leon A. Gatys",
                "Alexander S. Ecker",
                "Matthias Bethge"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999888707749697,
        "task": "Image-to-Image Translation",
        "task_prob": 0.9371836834004083
    }
}