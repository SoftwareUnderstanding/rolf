{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "E2E-Keyword-Spotting",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "bozliu",
                "owner_type": "User",
                "name": "E2E-Keyword-Spotting",
                "url": "https://github.com/bozliu/E2E-Keyword-Spotting",
                "stars": 0,
                "pushed_at": "2021-04-19 12:29:22+00:00",
                "created_at": "2021-02-10 06:46:19+00:00",
                "language": "Python",
                "description": "Wake-Up Keyword Detection With End To End Deep Neural Networks",
                "license": "MIT License",
                "frameworks": [
                    "scikit-learn",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "be189ff72b1a416b215889185c6e478af5899459",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bozliu/E2E-Keyword-Spotting/blob/main/LICENSE"
                    }
                },
                "size": 1063
            },
            {
                "type": "code",
                "name": "config.py",
                "sha": "1fc3b881d20b8cfe8d3daf35b5d947148db7cdec",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bozliu/E2E-Keyword-Spotting/blob/main/config.py"
                    }
                },
                "size": 2482
            },
            {
                "type": "code",
                "name": "datasets",
                "sha": "4d0e3d59c0b4ba96f1f300aa22d3938acef235c0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bozliu/E2E-Keyword-Spotting/tree/main/datasets"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "environment.yml",
                "sha": "4fe57cd285dfafe644a324227c7782d687be44d4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bozliu/E2E-Keyword-Spotting/blob/main/environment.yml"
                    }
                },
                "size": 4907
            },
            {
                "type": "code",
                "name": "images",
                "sha": "fa10096df21b6f85c08f05eb47666bbaf5eecb41",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bozliu/E2E-Keyword-Spotting/tree/main/images"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "kws",
                "sha": "c8acfbe0b3324ed0b0ddcd2b44def56c7817fe2c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bozliu/E2E-Keyword-Spotting/tree/main/kws"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "b00147208e3a54f1d84a78c43fab96b41fd265e8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bozliu/E2E-Keyword-Spotting/blob/main/requirements.txt"
                    }
                },
                "size": 2308
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "734cc15651baacc5c1f4339e3da7b085b674d2fb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bozliu/E2E-Keyword-Spotting/blob/main/train.py"
                    }
                },
                "size": 12149
            }
        ]
    },
    "authors": [
        {
            "name": "bozliu",
            "github_id": "bozliu"
        }
    ],
    "tags": [],
    "description": "Wake-Up Keyword Detection With End To End Deep Neural Networks",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/bozliu/E2E-Keyword-Spotting",
            "stars": 0,
            "issues": false,
            "readme": "# E2E-Keyword-Spotting\n\nJoint End to End Approaches to Improving Far-field Wake-up Keyword Detection\n\n## :wrench: Dependencies and Installation\n\n- Python >= 3.7 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html))\n- [PyTorch >= 1.3](https://pytorch.org/)\n- NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads)\n\n\n1. Install dependent packages\n\n    ```bash\n    cd E2E-Keyword-Spotting\n    pip install -r requirements.txt\n    ```\n2. Or use conda \n    ```bash\n    cd E2E-Keyword-Spotting\n    conda env create -f environment.yaml\n    ```\n\n## :turtle: Dataset Preparation\n\n#### How to Use\nDataset is from [Google Speech Command](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) published in  [arxiv](https://arxiv.org/abs/1804.03209).\n* Data Pre-processing (Has already been done)\n1. According to the file, dataset has already been splited into three folders, train, test, and valid. \n1. The splited [Google Speech Command dataset](https://drive.google.com/file/d/1InqR8n7l5Qj6voJREpcjHYWHVTKG-BbB/view?usp=sharing) is saved in Google Drive folder. \n    \n## :computer: Train and Test\n### Training commands\n- **Single GPU Training**: \n```\npython train.py\n```\n- **Distributed Training**: \n```\nCUDA_VISIBLE_DEVICES=0,1 python train.py\n```\n### Test commands\n```\npython test.py \n```\n## Neural Network Architectures\n### General Architecture\n<img src=\"https://github.com/bozliu/E2E-Keyword-Spotting/blob/main/images/Standard%20E2E%20Architecture.png \" width=\"100%\">\n\n1. Multi-head Attention\n<img src=\"https://github.com/bozliu/E2E-Keyword-Spotting/blob/main/images/Multi-Head%20Attention%20Architecture.png\" width=\"100%\">\n* Encoder: GRU/LSM \n* Attention Heads: 8\n* GRU hidden nodes: 128/256/512\n* GRU layers: 1/2/3\n* Increasing GRU hidden layers nodes can increase the performance much better than increasing hidden layers \n\n2. VGG19/VGG16/VGG13/VGG11 with/without batch normalization\n3. Deep Residual Neural Network ('resnet18', 'resnet34', 'resnet50') \n4. Wide Residual Networks ('wideresnet28_10') imported from the [repository](https://github.com/xternalz/WideResNet-pytorch/blob/master/wideresnet.py)\n5. Dual Path Networks from [arxiv](https://arxiv.org/abs/1707.01629) \n6. Densely Connected Convolutional Networks from [arxiv](https://arxiv.org/abs/1608.06993)\n\n## Result\n\n### Model Parameters \n<img src=\"images/model_parameters.png\" width=\"100%\">\n\n### Best Accuracy Training Process\n![image](https://github.com/bozliu/E2E-Keyword-Spotting/blob/main/images/best%20accuracy.png)\n\n### Best Loss Training Process\n![image](https://github.com/bozliu/E2E-Keyword-Spotting/blob/main/images/best%20loss.png)\n\n## Files Description  \n\u251c\u2500\u2500 kws   \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 metrics    \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fnr_fpr.py  \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py  \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 models   \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 attention.py  \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 crnn.py  \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 densenet.py  \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dpn.py  \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py  \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 resnet.py   \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 resnext.py  \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 treasure_net.py  \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 vgg.py  \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 wideresnet.py  \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 transforms  \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 utils.py  \n\u251c\u2500\u2500 config.py  \n\n* *./kws/metrics* : Evaluation matrics, defining the False Rejection Rate (FRR) and False Alarm Rate (FAR) for keyword spotting\n* *./kws/models* : Diffferent network architecture \n* *.config.py* : Configuration about parameters and hyperparameters\n",
            "readme_url": "https://github.com/bozliu/E2E-Keyword-Spotting",
            "frameworks": [
                "scikit-learn",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Dual Path Networks",
            "arxiv": "1707.01629",
            "year": 2017,
            "url": "http://arxiv.org/abs/1707.01629v2",
            "abstract": "In this work, we present a simple, highly efficient and modularized Dual Path\nNetwork (DPN) for image classification which presents a new topology of\nconnection paths internally. By revealing the equivalence of the\nstate-of-the-art Residual Network (ResNet) and Densely Convolutional Network\n(DenseNet) within the HORNN framework, we find that ResNet enables feature\nre-usage while DenseNet enables new features exploration which are both\nimportant for learning good representations. To enjoy the benefits from both\npath topologies, our proposed Dual Path Network shares common features while\nmaintaining the flexibility to explore new features through dual path\narchitectures. Extensive experiments on three benchmark datasets, ImagNet-1k,\nPlaces365 and PASCAL VOC, clearly demonstrate superior performance of the\nproposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset,\na shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model\nsize, 25% less computational cost and 8% lower memory consumption, and a deeper\nDPN (DPN-131) further pushes the state-of-the-art single model performance with\nabout 2 times faster training speed. Experiments on the Places365 large-scale\nscene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation\ndataset also demonstrate its consistently better performance than DenseNet,\nResNet and the latest ResNeXt model over various applications.",
            "authors": [
                "Yunpeng Chen",
                "Jianan Li",
                "Huaxin Xiao",
                "Xiaojie Jin",
                "Shuicheng Yan",
                "Jiashi Feng"
            ]
        },
        {
            "title": "Densely Connected Convolutional Networks",
            "arxiv": "1608.06993",
            "year": 2016,
            "url": "http://arxiv.org/abs/1608.06993v5",
            "abstract": "Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high performance. Code and\npre-trained models are available at https://github.com/liuzhuang13/DenseNet .",
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens van der Maaten",
                "Kilian Q. Weinberger"
            ]
        },
        {
            "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition",
            "arxiv": "1804.03209",
            "year": 2018,
            "url": "http://arxiv.org/abs/1804.03209v1",
            "abstract": "Describes an audio dataset of spoken words designed to help train and\nevaluate keyword spotting systems. Discusses why this task is an interesting\nchallenge, and why it requires a specialized dataset that is different from\nconventional datasets used for automatic speech recognition of full sentences.\nSuggests a methodology for reproducible and comparable accuracy metrics for\nthis task. Describes how the data was collected and verified, what it contains,\nprevious versions and properties. Concludes by reporting baseline results of\nmodels trained on this dataset.",
            "authors": [
                "Pete Warden"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Google Speech Command dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://drive.google.com/file/d/1InqR8n7l5Qj6voJREpcjHYWHVTKG-BbB/view?usp=sharing"
                    }
                }
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "CIFAR-10"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999208379329542,
        "task": "Image Classification",
        "task_prob": 0.966864459777329
    }
}