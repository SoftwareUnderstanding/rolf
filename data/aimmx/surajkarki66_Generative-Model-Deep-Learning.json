{
    "visibility": {
        "visibility": "public"
    },
    "name": "Generative-Model-Deep-Learning",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "surajkarki66",
                "owner_type": "User",
                "name": "Generative-Model-Deep-Learning",
                "url": "https://github.com/surajkarki66/Generative-Model-Deep-Learning",
                "stars": 2,
                "pushed_at": "2020-08-22 16:25:53+00:00",
                "created_at": "2020-08-07 18:15:34+00:00",
                "language": "Jupyter Notebook",
                "description": "In this repository i'm gonna walk you through the origin of generative model.",
                "frameworks": [
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".vscode",
                "sha": "65c6c46571d0870fad39ced3ccbbdfcadb658a11",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/surajkarki66/Generative-Model-Deep-Learning/tree/master/.vscode"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "Autoencoders",
                "sha": "5fbbf3241c689926b558f4bbb9e6205cd9ea7027",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/surajkarki66/Generative-Model-Deep-Learning/tree/master/Autoencoders"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "Generative Adversarial Network",
                "sha": "9b82f1a8fc495d38cb592534a5c1d30042e9a6a3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/surajkarki66/Generative-Model-Deep-Learning/tree/master/Generative Adversarial Network"
                    }
                },
                "num_files": 7
            }
        ]
    },
    "tags": [
        "deep-learning",
        "gan",
        "tensorflow",
        "pytorch"
    ],
    "description": "In this repository i'm gonna walk you through the origin of generative model.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/surajkarki66/Generative-Model-Deep-Learning",
            "stars": 2,
            "issues": true,
            "readme": "# Generative-Model-Deep-Learning\n\n## How to train a GAN ? Tips and tricks to make GAN training stable.\n\n### 1) Normalize the inputs:\n\n- Normalize the images between -1 and 1.\n- Use `tanh` activation function in the last layer of generator.\n\n### 2) Loss function:\n\nIn GAN papers, the loss function to optimize G is `min (log 1-D)`, but in practice folks practically use `max log D`\n\n- because the first formulation has vanishing gradients early on\n- Goodfellow et. al (2014)\n\nIn practice, works well:\n\n- Flip labels when training generator: real = fake, fake = real\n\n### 3) Use a spherical Z:\n\n- Dont sample from a Uniform distribution\n- When doing interpolations, do the interpolation via a great circle, rather than a straight line from point A to point B\n- Tom White's [Sampling Generative Networks](https://arxiv.org/abs/1609.04468) ref code https://github.com/dribnet/plat has more details\n\n### 4) Batch Normalization:\n\n- Construct different mini-batches for real and fake, i.e. each mini-batch needs to contain only all real images or all generated images.\n- when batchnorm is not an option use instance normalization (for each sample, subtract mean and divide by standard deviation).\n\n### 5) Avoid Sparse Gradients: ReLU, MaxPool\n\n- the stability of the GAN game suffers if you have sparse gradients\n- LeakyReLU = good (in both G and D)\n- For Downsampling, use: Average Pooling, Conv2d + stride\n- For Upsampling, use: PixelShuffle, ConvTranspose2d + stride\n  - PixelShuffle: https://arxiv.org/abs/1609.05158\n\n### 6) Use Soft and Noisy labels\n\n- Label Smoothing, i.e. if you have two target labels: Real=1 and Fake=0, then for each incoming sample, if it is real, then replace the label with a random number between 0.7 and 1.2, and if it is a fake sample, replace it with 0.0 and 0.3 (for example).\n  - Salimans et. al. 2016\n- make the labels the noisy for the discriminator: occasionally flip the labels when training the discriminator\n- fake_labels = `tf.random.uniform(shape=[25, 1], minval=0, maxval=0.3, dtype=tf.float32)`\n- real_labels = `tf.random.uniform(shape=[25, 1], minval=0.7, maxval=1.2, dtype=tf.float32)`\n\n### 7) Use stability tricks from RL\n\n- Experience Replay\n  - Keep a replay buffer of past generations and occassionally show them\n  - Keep checkpoints from the past of G and D and occassionaly swap them out for a few iterations\n- All stability tricks that work for deep deterministic policy gradients\n\n### 8) Use the ADAM Optimizer\n\n- optim.Adam rules!\n  - See Radford et. al. 2015\n- Use SGD for discriminator and ADAM for generator\n\n### 9) Track failures early\n\n- D loss goes to 0: failure mode\n- check norms of gradients: if they are over 100 things are screwing up\n- when things are working, D loss has low variance and goes down over time vs having huge variance and spiking\n- if loss of generator steadily decreases, then it's fooling D with garbage (says martin)\n\n### 10) Add noise to inputs, decay over time\n\n- Add some artificial noise to inputs to D (Arjovsky et. al., Huszar, 2016)\n  - http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/\n  - https://openreview.net/forum?id=Hk4_qw5xe\n- adding gaussian noise to every layer of generator (Zhao et. al. EBGAN)\n  - Improved GANs: OpenAI code also has it (commented out)\n\n### 11) Use Dropouts in G in both train and test phase\n\n- Provide noise in the form of dropout (50%).\n- Apply on several layers of our generator at both training and test time\n- https://arxiv.org/pdf/1611.07004v1.pdf\n",
            "readme_url": "https://github.com/surajkarki66/Generative-Model-Deep-Learning",
            "frameworks": [
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network",
            "arxiv": "1609.05158",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.05158v2",
            "abstract": "Recently, several models based on deep neural networks have achieved great\nsuccess in terms of both reconstruction accuracy and computational performance\nfor single image super-resolution. In these methods, the low resolution (LR)\ninput image is upscaled to the high resolution (HR) space using a single\nfilter, commonly bicubic interpolation, before reconstruction. This means that\nthe super-resolution (SR) operation is performed in HR space. We demonstrate\nthat this is sub-optimal and adds computational complexity. In this paper, we\npresent the first convolutional neural network (CNN) capable of real-time SR of\n1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN\narchitecture where the feature maps are extracted in the LR space. In addition,\nwe introduce an efficient sub-pixel convolution layer which learns an array of\nupscaling filters to upscale the final LR feature maps into the HR output. By\ndoing so, we effectively replace the handcrafted bicubic filter in the SR\npipeline with more complex upscaling filters specifically trained for each\nfeature map, whilst also reducing the computational complexity of the overall\nSR operation. We evaluate the proposed approach using images and videos from\npublicly available datasets and show that it performs significantly better\n(+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster\nthan previous CNN-based methods.",
            "authors": [
                "Wenzhe Shi",
                "Jose Caballero",
                "Ferenc Husz\u00e1r",
                "Johannes Totz",
                "Andrew P. Aitken",
                "Rob Bishop",
                "Daniel Rueckert",
                "Zehan Wang"
            ]
        },
        {
            "title": "Sampling Generative Networks",
            "arxiv": "1609.04468",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.04468v3",
            "abstract": "We introduce several techniques for sampling and visualizing the latent\nspaces of generative models. Replacing linear interpolation with spherical\nlinear interpolation prevents diverging from a model's prior distribution and\nproduces sharper samples. J-Diagrams and MINE grids are introduced as\nvisualizations of manifolds created by analogies and nearest neighbors. We\ndemonstrate two new techniques for deriving attribute vectors: bias-corrected\nvectors with data replication and synthetic vectors with data augmentation.\nBinary classification using attribute vectors is presented as a technique\nsupporting quantitative analysis of the latent space. Most techniques are\nintended to be independent of model type and examples are shown on both\nVariational Autoencoders and Generative Adversarial Networks.",
            "authors": [
                "Tom White"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999035349661438,
        "task": "Image-to-Image Translation",
        "task_prob": 0.8753328746589588
    }
}