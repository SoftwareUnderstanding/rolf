{
    "visibility": {
        "visibility": "public"
    },
    "name": "Adaptive Notes Generator :bookmark_tabs:",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "chakravarthi-v",
                "owner_type": "User",
                "name": "Polaroid-1",
                "url": "https://github.com/chakravarthi-v/Polaroid-1",
                "stars": 0,
                "pushed_at": "2021-04-14 08:02:12+00:00",
                "created_at": "2021-09-28 06:03:49+00:00",
                "language": null,
                "frameworks": [
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "App",
                "sha": "a4ab30de002ef3e67d4f5223be0352bf31b1c7c8",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/chakravarthi-v/Polaroid-1/tree/main/App"
                    }
                },
                "num_files": 15
            },
            {
                "type": "code",
                "name": "ML",
                "sha": "61ce0c6d2ab8701740a7ee0ebf2e20adb4d55d91",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/chakravarthi-v/Polaroid-1/tree/main/ML"
                    }
                },
                "num_files": 7
            }
        ]
    },
    "authors": [
        {
            "name": "chakravarthi",
            "github_id": "chakravarthi-v"
        },
        {
            "name": "Abhigyan Singh",
            "github_id": "Blazikengr8"
        },
        {
            "name": "KushGrandhi",
            "github_id": "KushGrandhi"
        },
        {
            "name": "Gautam J",
            "email": "gautam.jayapal@gmail.com",
            "github_id": "Gautam-J"
        },
        {
            "name": "Payel",
            "github_id": "pp2659"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/chakravarthi-v/Polaroid-1",
            "stars": 0,
            "issues": false,
            "readme": "# Adaptive Notes Generator :bookmark_tabs:\n\n## > **Description:**\n\n**Adaptive Notes Generator** *is a tool that helps us attend online classes effectively:star_struck:. Due to the Online class culture, taking notes in pen and paper is not a good idea, the only options left are to click screenshots or struggle to note down everything in your notebook:unamused:. Our application will make your life easier, once a meeting video:film_projector: is provided, we will create the notes that will save you time:stopwatch: of research and gathering resources. We will divide your meeting into useful segments and add additional data to make it easy to understand any concept.:bookmark_tabs::bookmark_tabs:*\n\n## > **Problem we are Solving:** \n\n*During the Pandemic, many meetings were moved to online platforms:computer:, and still, continue using it. The Transition from blackboard:white_square_button: to PowerPoint:desktop_computer: has come with some problems, some of them are as follows:* \n\n**1. Not being able to keep up the pace:hourglass_flowing_sand::hourglass_flowing_sand: due to concise information on each slide.**\n\n**2. Not having the ability to write:black_nib::black_nib: effectively what the teacher explains.**\n\n*We plan to address these issues through our project:innocent::wink:.*\n\n---\n\n**Named Entity Recognition system** *features a sophisticated word embedding strategy using subword features and \"Bloom\" embeddings, a deep convolutional neural network with residual connections, and a novel transition-based approach to named entity parsing. The system is designed to give a good balance of efficiency, accuracy and adaptability.*\n\nSource: https://spacy.io/universe/project/video-spacys-ner-model\n\n**BART** *is a denoising autoencoder for pretraining sequence-to-sequence models. It is trained by corrupting text with an arbitrary noising function, and learning a model to reconstruct the original text. It uses a standard Transformer-based neural machine translation architecture. It uses a standard seq2seq/NMT architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT). This means the encoder's attention mask is fully visible, like BERT, and the decoder's attention mask is causal, like GPT2.*\n\nSource: https://arxiv.org/abs/1910.13461\n\n![75300fd2-7bcd-4536-9a05-d2d9c5fc7eec](https://user-images.githubusercontent.com/60737264/111874912-2bb46900-89bd-11eb-9f96-96495cab4fad.jpg)\n\nSource: https://paperswithcode.com/method/bart?hcb=1\n\n![1b721bdf-8a62-420e-b8f4-9b3b466a8ff2](https://user-images.githubusercontent.com/60737264/111874942-3c64df00-89bd-11eb-990c-b94127d58a57.jpg)\n\nSource: https://pytorch.org/hub/snakers4_silero-models_stt/?hcb=1\n\n## > **Problems faced:**\n* Overwriting unwanted branches while testing\n* Finding accurate speech to text model\n* Dealing with **cloud space**\n* Download and upload from ML backend\n* Didn't have enough **computational resource** to run bigger models\n* **Compressing** files upload video\n* Download button in PDF viewer\n",
            "readme_url": "https://github.com/chakravarthi-v/Polaroid-1",
            "frameworks": [
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
            "arxiv": "1910.13461",
            "year": 2019,
            "url": "http://arxiv.org/abs/1910.13461v1",
            "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence\nmodels. BART is trained by (1) corrupting text with an arbitrary noising\nfunction, and (2) learning a model to reconstruct the original text. It uses a\nstandard Tranformer-based neural machine translation architecture which,\ndespite its simplicity, can be seen as generalizing BERT (due to the\nbidirectional encoder), GPT (with the left-to-right decoder), and many other\nmore recent pretraining schemes. We evaluate a number of noising approaches,\nfinding the best performance by both randomly shuffling the order of the\noriginal sentences and using a novel in-filling scheme, where spans of text are\nreplaced with a single mask token. BART is particularly effective when fine\ntuned for text generation but also works well for comprehension tasks. It\nmatches the performance of RoBERTa with comparable training resources on GLUE\nand SQuAD, achieves new state-of-the-art results on a range of abstractive\ndialogue, question answering, and summarization tasks, with gains of up to 6\nROUGE. BART also provides a 1.1 BLEU increase over a back-translation system\nfor machine translation, with only target language pretraining. We also report\nablation experiments that replicate other pretraining schemes within the BART\nframework, to better measure which factors most influence end-task performance.",
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999714163328398,
        "task": "Machine Translation",
        "task_prob": 0.9769392702733852
    },
    "training": {
        "datasets": [
            {
                "name": "SQuAD"
            },
            {
                "name": "GLUE"
            }
        ]
    }
}