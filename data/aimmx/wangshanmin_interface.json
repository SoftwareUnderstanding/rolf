{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "InsightFace: 2D and 3D Face Analysis Project",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "wangshanmin",
                "owner_type": "User",
                "name": "interface",
                "url": "https://github.com/wangshanmin/interface",
                "stars": 1,
                "pushed_at": "2019-09-25 04:04:45+00:00",
                "created_at": "2019-06-26 11:31:23+00:00",
                "language": "Python",
                "license": "MIT License",
                "frameworks": [
                    "Caffe",
                    "MXNet",
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "7bbc71c09205c78d790739d246bbe4f9f1881c17",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/blob/master/.gitignore"
                    }
                },
                "size": 1157
            },
            {
                "type": "code",
                "name": ".gitmodules",
                "sha": "6c4c7f9803d071e278b5dd1dc9f81302b6aa3c21",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/blob/master/.gitmodules"
                    }
                },
                "size": 101
            },
            {
                "type": "code",
                "name": "3rdparty",
                "sha": "2312845078645b801cba42a371ee4b771564d588",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/3rdparty"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "Evaluation",
                "sha": "7939fd2b103265b9f3bcf1b22e0db629120bce40",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/Evaluation"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "1562c52dff26c5d846489922bd15eeb8c16f1a80",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/blob/master/LICENSE"
                    }
                },
                "size": 1082
            },
            {
                "type": "code",
                "name": "RetinaFace",
                "sha": "0979d10ee0a6f680f78091fdf59fd86ef61db0be",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/RetinaFace"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "alignment",
                "sha": "5e3bc63ede382fc9ff0b7cec8d7da81a39341416",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/alignment"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "common",
                "sha": "7885830deb11618f0dda37e49de1ad11707b9d31",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/common"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "cpp-align",
                "sha": "f73026c3e46093a8a122a563f2c53ac510872feb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/cpp-align"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "datasets",
                "sha": "e886b4d17e9105e7c80505255bbf636b70368da0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/datasets"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "deploy",
                "sha": "ef4a59295835d9a123668a7734074d30ea513803",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/deploy"
                    }
                },
                "num_files": 11
            },
            {
                "type": "code",
                "name": "gender-age",
                "sha": "bc965a4213925fda9364280f0a07fe936577eea4",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/gender-age"
                    }
                },
                "num_files": 13
            },
            {
                "type": "code",
                "name": "gluon",
                "sha": "022869099714a51cca67605428945e403b0b89c1",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/gluon"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "iccv19-challenge",
                "sha": "93020dc09288c8b04677ae25a087ae23f4341e8c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/iccv19-challenge"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "interface",
                "sha": "6d124286cd366b1153bfe89b551ed18eb8c51eb1",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/interface"
                    }
                },
                "num_files": 20
            },
            {
                "type": "code",
                "name": "models",
                "sha": "38f6eff72fd6deb603d9cb0b9099e94ad078a41e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/models"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "recognition",
                "sha": "62d8edda0ee4620e60ebacc9955b62131adfa1e7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/recognition"
                    }
                },
                "num_files": 12
            },
            {
                "type": "code",
                "name": "resources",
                "sha": "0deedc1aa464c0596d26c2bb1653f5db37527a97",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/resources"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "sample-images",
                "sha": "ef62819e7d2a87df8add1b5f0d0ef9e70c4e3d4c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/sample-images"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "src",
                "sha": "bd7cc075d3ff1428f66f620a5d0903a3053ba8c3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/wangshanmin/interface/tree/master/src"
                    }
                },
                "num_files": 17
            }
        ]
    },
    "authors": [
        {
            "name": "Jia Guo",
            "email": "guojia@gmail.com",
            "github_id": "nttstar"
        },
        {
            "name": "JiankangDeng",
            "email": "j.deng16@imperial.ac.uk",
            "github_id": "jiankangdeng"
        },
        {
            "name": "wangshanmin",
            "github_id": "wangshanmin"
        },
        {
            "name": "jingyang2017",
            "github_id": "jingyang2017"
        },
        {
            "name": "Yingfeng",
            "email": "yingfeng.zhang@gmail.com",
            "github_id": "yingfeng"
        },
        {
            "name": "Yuxin Wu",
            "github_id": "ppwwyyxx"
        },
        {
            "name": "Jack Yu",
            "email": "455501914@qq.com",
            "github_id": "szad670401"
        },
        {
            "name": "TengQi Ye",
            "email": "yetengqi@gmail.com",
            "github_id": "tengerye"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/wangshanmin/interface",
            "stars": 1,
            "issues": true,
            "readme": "\n# InsightFace: 2D and 3D Face Analysis Project\n\nBy Jia Guo and [Jiankang Deng](https://jiankangdeng.github.io/)\n\n## License\n\nThe code of InsightFace is released under the MIT License. There is no limitation for both acadmic and commercial usage.\n\n## ArcFace Video Demo\n\n[![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)\n\nPlease click the image to watch the Youtube video. For Bilibili users, click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).\n\n## Recent Update\n\n**`2019.04.30`**: Our Face detector ([RetinaFace](https://github.com/deepinsight/insightface/tree/master/RetinaFace)) obtains state-of-the-art results on [the WiderFace dataset](http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html).\n\n**`2019.04.14`**: We will launch a [Light-weight Face Recognition challenge/workshop](https://ibug.doc.ic.ac.uk/resources/lightweight-face-recognition-challenge-workshop/) on ICCV 2019.\n\n**`2019.04.04`**: Arcface achieved state-of-the-art performance (7/109) on the NIST Face Recognition Vendor Test (FRVT) (1:1 verification)\n[report](https://www.nist.gov/sites/default/files/documents/2019/04/04/frvt_report_2019_04_04.pdf) (name: Imperial-000 and Imperial-001). Our solution is based on [MS1MV2+DeepGlintAsian, ResNet100, ArcFace loss]. \n\n**`2019.02.08`**: Please check [https://github.com/deepinsight/insightface/tree/master/recognition](https://github.com/deepinsight/insightface/tree/master/recognition) for our parallel training code which can easily and efficiently support one million identities on a single machine (8* 1080ti).\n\n**`2018.12.13`**: Inference acceleration [TVM-Benchmark](https://github.com/deepinsight/insightface/wiki/TVM-Benchmark).\n\n**`2018.10.28`**: Light-weight attribute model [Gender-Age](https://github.com/deepinsight/insightface/tree/master/gender-age). About 1MB, 10ms on single CPU core. Gender accuracy 96% on validation set and 4.1 age MAE.\n\n**`2018.10.16`**: We achieved state-of-the-art performance on [Trillionpairs](http://trillionpairs.deepglint.com/results) (name: nttstar) and [IQIYI_VID](http://challenge.ai.iqiyi.com/detail?raceId=5afc36639689443e8f815f9e) (name: WitcheR). \n\n## Contents\n[Deep Face Recognition](#deep-face-recognition)\n- [Introduction](#introduction)\n- [Training Data](#training-data)\n- [Train](#train)\n- [Pretrained Models](#pretrained-models)\n- [Verification Results On Combined Margin](#verification-results-on-combined-margin)\n- [Test on MegaFace](#test-on-megaface)\n- [512-D Feature Embedding](#512-d-feature-embedding)\n- [Third-party Re-implementation](#third-party-re-implementation)\n\n[Face Alignment](#face-alignment)\n\n[Face Detection](#face-detection)\n\n[Citation](#citation)\n\n[Contact](#contact)\n\n## Deep Face Recognition\n\n### Introduction\n\nIn this repository, we provide training data, network settings and loss designs for deep face recognition.\nThe training data includes the normalised MS1M, VGG2 and CASIA-Webface datasets, which were already packed in MXNet binary format.\nThe network backbones include ResNet, MobilefaceNet, MobileNet, InceptionResNet_v2, DenseNet, DPN.\nThe loss functions include Softmax, SphereFace, CosineFace, ArcFace and Triplet (Euclidean/Angular) Loss.\n\n\n![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)\n\nOur method, ArcFace, was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository, you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.\n\n### Training Data\n\nAll face images are aligned by [MTCNN](https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html) and cropped to 112x112:\n\nPlease check [Dataset-Zoo](https://github.com/deepinsight/insightface/wiki/Dataset-Zoo) for detail information and dataset downloading.\n\n\n* Please check *src/data/face2rec2.py* on how to build a binary face dataset. Any public available *MTCNN* can be used to align the faces, and the performance should not change. We will improve the face normalisation step by full pose alignment methods recently.\n\n### Train\n\n1. Install `MXNet` with GPU support (Python 2.7).\n\n```\npip install mxnet-cu90\n```\n\n2. Clone the InsightFace repository. We call the directory insightface as *`INSIGHTFACE_ROOT`*.\n\n```\ngit clone --recursive https://github.com/deepinsight/insightface.git\n```\n\n3. Download the training set (`MS1M-Arcface`) and place it in *`$INSIGHTFACE_ROOT/datasets/`*. Each training dataset includes at least following 6 files:\n\n```Shell\n    faces_emore/\n       train.idx\n       train.rec\n       property\n       lfw.bin\n       cfp_fp.bin\n       agedb_30.bin\n```\n\nThe first three files are the training dataset while the last three files are verification sets.\n\n4. Train deep face recognition models.\nIn this part, we assume you are in the directory *`$INSIGHTFACE_ROOT/recognition/`*.\n```Shell\nexport MXNET_CPU_WORKER_NTHREADS=24\nexport MXNET_ENGINE_TYPE=ThreadedEnginePerDevice\n```\n\nPlace and edit config file:\n```Shell\ncp sample_config.py config.py\nvim config.py # edit dataset path etc..\n```\n\nWe give some examples below. Our experiments were conducted on the Tesla P40 GPU.\n\n(1). Train ArcFace with LResNet100E-IR.\n\n```Shell\nCUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r100 --loss arcface --dataset emore\n```\nIt will output verification results of *LFW*, *CFP-FP* and *AgeDB-30* every 2000 batches. You can check all options in *config.py*.\nThis model can achieve *LFW 99.80+* and *MegaFace 98.3%+*.\n\n(2). Train CosineFace with LResNet50E-IR.\n\n```Shell\nCUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r50 --loss cosface --dataset emore\n```\n\n(3). Train Softmax with LMobileNet-GAP.\n\n```Shell\nCUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network m1 --loss softmax --dataset emore\n```\n\n(4). Fine-turn the above Softmax model with Triplet loss.\n\n```Shell\nCUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network m1 --loss triplet --lr 0.005 --pretrained ./models/m1-softmax-emore,1\n```\n\n\n5. Verification results.\n\n*LResNet100E-IR* network trained on *MS1M-Arcface* dataset with ArcFace loss:\n\n| Method  | LFW(%) | CFP-FP(%) | AgeDB-30(%) |  \n| ------- | ------ | --------- | ----------- |  \n|  Ours   | 99.80+ | 98.0+     | 98.20+      |   \n\n\n\n### Pretrained Models\n\nYou can use `$INSIGHTFACE/src/eval/verification.py` to test all the pre-trained models.\n\n**Please check [Model-Zoo](https://github.com/deepinsight/insightface/wiki/Model-Zoo) for more pretrained models.**\n\n\n\n### Verification Results on Combined Margin\n\nA combined margin method was proposed as a function of target logits value and original `\u03b8`:\n\n```\nCOM(\u03b8) = cos(m_1*\u03b8+m_2) - m_3\n```\n\nFor training with `m1=1.0, m2=0.3, m3=0.2`, run following command:\n```\nCUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_softmax.py --network r100 --loss combined --dataset emore\n```\n\nResults by using ``MS1M-IBUG(MS1M-V1)``\n\n| Method           | m1   | m2   | m3   | LFW   | CFP-FP | AgeDB-30 |\n| ---------------- | ---- | ---- | ---- | ----- | ------ | -------- |\n| W&F Norm Softmax | 1    | 0    | 0    | 99.28 | 88.50  | 95.13    |\n| SphereFace       | 1.5  | 0    | 0    | 99.76 | 94.17  | 97.30    |\n| CosineFace       | 1    | 0    | 0.35 | 99.80 | 94.4   | 97.91    |\n| ArcFace          | 1    | 0.5  | 0    | 99.83 | 94.04  | 98.08    |\n| Combined Margin  | 1.2  | 0.4  | 0    | 99.80 | 94.08  | 98.05    |\n| Combined Margin  | 1.1  | 0    | 0.35 | 99.81 | 94.50  | 98.08    |\n| Combined Margin  | 1    | 0.3  | 0.2  | 99.83 | 94.51  | 98.13    |\n| Combined Margin  | 0.9  | 0.4  | 0.15 | 99.83 | 94.20  | 98.16    |\n\n### Test on MegaFace\n\nPlease check *`$INSIGHTFACE_ROOT/Evaluation/megaface/`* to evaluate the model accuracy on Megaface. All aligned images were already provided.\n\n\n### 512-D Feature Embedding\n\nIn this part, we assume you are in the directory *`$INSIGHTFACE_ROOT/deploy/`*. The input face image should be generally centre cropped. We use *RNet+ONet* of *MTCNN* to further align the image before sending it to the feature embedding network.\n\n1. Prepare a pre-trained model.\n2. Put the model under *`$INSIGHTFACE_ROOT/models/`*. For example, *`$INSIGHTFACE_ROOT/models/model-r100-ii`*.\n3. Run the test script *`$INSIGHTFACE_ROOT/deploy/test.py`*.\n\nFor single cropped face image(112x112), total inference time is only 17ms on our testing server(Intel E5-2660 @ 2.00GHz, Tesla M40, *LResNet34E-IR*).\n\n### Third-party Re-implementation\n\n- TensorFlow: [InsightFace_TF](https://github.com/auroua/InsightFace_TF)\n- TensorFlow: [tf-insightface](https://github.com/AIInAi/tf-insightface)\n- PyTorch: [InsightFace_Pytorch](https://github.com/TreB1eN/InsightFace_Pytorch)\n- PyTorch: [arcface-pytorch](https://github.com/ronghuaiyang/arcface-pytorch)\n- Caffe: [arcface-caffe](https://github.com/xialuxi/arcface-caffe)\n- Caffe: [CombinedMargin-caffe](https://github.com/gehaocool/CombinedMargin-caffe)\n- Tensorflow: [InsightFace-tensorflow](https://github.com/luckycallor/InsightFace-tensorflow)\n\n\n## Face Alignment\n\nPlease check the [Menpo](https://github.com/jiankangdeng/MenpoBenchmark) Benchmark and [Dense U-Net](https://github.com/deepinsight/insightface/tree/master/alignment) for more details.\n\n## Face Detection\n\nPlease check [RetinaFace](https://github.com/deepinsight/insightface/tree/master/RetinaFace) for more details.\n\n## Citation\n\nIf you find *InsightFace* useful in your research, please consider to cite the following related papers:\n\n```\n@inproceedings{deng2019retinaface,\ntitle={RetinaFace: Single-stage Dense Face Localisation in the Wild},\nauthor={Deng, Jiankang and Guo, Jia and Yuxiang, Zhou and Jinke Yu and Irene Kotsia and Zafeiriou, Stefanos},\nbooktitle={arxiv},\nyear={2019}\n}\n\n@inproceedings{guo2018stacked,\n  title={Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment},\n  author={Guo, Jia and Deng, Jiankang and Xue, Niannan and Zafeiriou, Stefanos},\n  booktitle={BMVC},\n  year={2018}\n}\n\n@article{deng2018menpo,\n  title={The Menpo benchmark for multi-pose 2D and 3D facial landmark localisation and tracking},\n  author={Deng, Jiankang and Roussos, Anastasios and Chrysos, Grigorios and Ververas, Evangelos and Kotsia, Irene and Shen, Jie and Zafeiriou, Stefanos},\n  journal={IJCV},\n  year={2018}\n}\n\n@inproceedings{deng2018arcface,\ntitle={ArcFace: Additive Angular Margin Loss for Deep Face Recognition},\nauthor={Deng, Jiankang and Guo, Jia and Niannan, Xue and Zafeiriou, Stefanos},\nbooktitle={CVPR},\nyear={2019}\n}\n```\n\n## Contact\n\n```\n[Jia Guo](guojia[at]gmail.com)\n[Jiankang Deng](jiankangdeng[at]gmail.com)\n```\n",
            "readme_url": "https://github.com/wangshanmin/interface",
            "frameworks": [
                "Caffe",
                "MXNet",
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition",
            "arxiv": "1801.07698",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.07698v3",
            "abstract": "One of the main challenges in feature learning using Deep Convolutional\nNeural Networks (DCNNs) for large-scale face recognition is the design of\nappropriate loss functions that enhance discriminative power. Centre loss\npenalises the distance between the deep features and their corresponding class\ncentres in the Euclidean space to achieve intra-class compactness. SphereFace\nassumes that the linear transformation matrix in the last fully connected layer\ncan be used as a representation of the class centres in an angular space and\npenalises the angles between the deep features and their corresponding weights\nin a multiplicative way. Recently, a popular line of research is to incorporate\nmargins in well-established loss functions in order to maximise face class\nseparability. In this paper, we propose an Additive Angular Margin Loss\n(ArcFace) to obtain highly discriminative features for face recognition. The\nproposed ArcFace has a clear geometric interpretation due to the exact\ncorrespondence to the geodesic distance on the hypersphere. We present arguably\nthe most extensive experimental evaluation of all the recent state-of-the-art\nface recognition methods on over 10 face recognition benchmarks including a new\nlarge-scale image database with trillion level of pairs and a large-scale video\ndataset. We show that ArcFace consistently outperforms the state-of-the-art and\ncan be easily implemented with negligible computational overhead. We release\nall refined training data, training codes, pre-trained models and training\nlogs, which will help reproduce the results in this paper.",
            "authors": [
                "Jiankang Deng",
                "Jia Guo",
                "Niannan Xue",
                "Stefanos Zafeiriou"
            ]
        },
        {
            "year": "2019",
            "booktitle": "arxiv",
            "author": [
                "Deng, Jiankang",
                "Guo, Jia",
                "Yuxiang, Zhou",
                "Yu, Jinke",
                "Kotsia, Irene",
                "Zafeiriou, Stefanos"
            ],
            "title": "RetinaFace: Single-stage Dense Face Localisation in the Wild",
            "ENTRYTYPE": "inproceedings",
            "ID": "deng2019retinaface",
            "authors": [
                "Deng, Jiankang",
                "Guo, Jia",
                "Yuxiang, Zhou",
                "Yu, Jinke",
                "Kotsia, Irene",
                "Zafeiriou, Stefanos"
            ]
        },
        {
            "year": "2018",
            "booktitle": "BMVC",
            "author": [
                "Guo, Jia",
                "Deng, Jiankang",
                "Xue, Niannan",
                "Zafeiriou, Stefanos"
            ],
            "title": "Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment",
            "ENTRYTYPE": "inproceedings",
            "ID": "guo2018stacked",
            "authors": [
                "Guo, Jia",
                "Deng, Jiankang",
                "Xue, Niannan",
                "Zafeiriou, Stefanos"
            ]
        },
        {
            "year": "2018",
            "journal": "IJCV",
            "author": [
                "Deng, Jiankang",
                "Roussos, Anastasios",
                "Chrysos, Grigorios",
                "Ververas, Evangelos",
                "Kotsia, Irene",
                "Shen, Jie",
                "Zafeiriou, Stefanos"
            ],
            "title": "The Menpo benchmark for multi-pose 2D and 3D facial landmark localisation and tracking",
            "ENTRYTYPE": "article",
            "ID": "deng2018menpo",
            "authors": [
                "Deng, Jiankang",
                "Roussos, Anastasios",
                "Chrysos, Grigorios",
                "Ververas, Evangelos",
                "Kotsia, Irene",
                "Shen, Jie",
                "Zafeiriou, Stefanos"
            ]
        },
        {
            "year": "2019",
            "booktitle": "CVPR",
            "author": [
                "Deng, Jiankang",
                "Guo, Jia",
                "Niannan, Xue",
                "Zafeiriou, Stefanos"
            ],
            "title": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition",
            "ENTRYTYPE": "inproceedings",
            "ID": "deng2018arcface",
            "authors": [
                "Deng, Jiankang",
                "Guo, Jia",
                "Niannan, Xue",
                "Zafeiriou, Stefanos"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "the WiderFace dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html"
                    }
                }
            },
            {
                "name": "Dataset-Zoo",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/deepinsight/insightface/wiki/Dataset-Zoo"
                    }
                }
            },
            {
                "name": "MegaFace"
            },
            {
                "name": "WebFace"
            },
            {
                "name": "LFW"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999991978978912,
        "task": "Face Verification",
        "task_prob": 0.989104941837821
    }
}