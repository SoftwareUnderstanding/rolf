{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "**GPT2-Pytorch with Text-Generator**",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "graykode",
                "owner_type": "User",
                "name": "gpt-2-Pytorch",
                "url": "https://github.com/graykode/gpt-2-Pytorch",
                "stars": 717,
                "pushed_at": "2019-07-08 15:24:35+00:00",
                "created_at": "2019-02-18 08:06:33+00:00",
                "language": "Python",
                "description": "Simple Text-Generator with OpenAI gpt-2 Pytorch Implementation",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "f12f44a3d5d05d477f63b751ba192bd3803a84f3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/graykode/gpt-2-Pytorch/blob/master/.gitignore"
                    }
                },
                "size": 52
            },
            {
                "type": "code",
                "name": "GPT2",
                "sha": "09352c6f80d17b394e94253f30b6df9b53136e1b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/graykode/gpt-2-Pytorch/tree/master/GPT2"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "GPT2_Pytorch.ipynb",
                "sha": "4ef537dac02c9e4ea6e5817847771b9d20efa66a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/graykode/gpt-2-Pytorch/blob/master/GPT2_Pytorch.ipynb"
                    }
                },
                "size": 6739
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "bc5de2fe77a184f2d35237517aafd8016962ca3a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/graykode/gpt-2-Pytorch/blob/master/LICENSE"
                    }
                },
                "size": 1102
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "c2b0649910fa60d7389d56877aaf20e3075d7932",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/graykode/gpt-2-Pytorch/blob/master/main.py"
                    }
                },
                "size": 2956
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "6ec436bf3b10caf79b056283d03d0c00135c4212",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/graykode/gpt-2-Pytorch/blob/master/requirements.txt"
                    }
                },
                "size": 15
            }
        ]
    },
    "authors": [
        {
            "name": "Tae-Hwan Jung",
            "email": "nlkey2022@gmail.com",
            "github_id": "graykode"
        },
        {
            "name": "Raveen Beemsingh",
            "email": "raveen.b@gmail.com",
            "github_id": "raveenb"
        }
    ],
    "tags": [
        "gpt-2",
        "pytorch",
        "implementation",
        "nlp",
        "text-generator",
        "story-telling",
        "gpt2",
        "natural-language-processing"
    ],
    "description": "Simple Text-Generator with OpenAI gpt-2 Pytorch Implementation",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/graykode/gpt-2-Pytorch",
            "stars": 717,
            "issues": true,
            "readme": "## **GPT2-Pytorch with Text-Generator**\n\n<p align=\"center\"><img width=\"100\" src=\"https://media-thumbs.golden.com/OLqzmrmwAzY1P7Sl29k2T9WjJdM=/200x200/smart/golden-storage-production.s3.amazonaws.com/topic_images/e08914afa10a4179893eeb07cb5e4713.png\" /></p>\n\n**Better Language Models and Their Implications**\n\n> Our model, called GPT-2 (a successor to [GPT](https://blog.openai.com/language-unsupervised/)), was trained simply to predict the next word in 40GB of Internet text. Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much [smaller model](https://github.com/openai/gpt-2) for researchers to experiment with, as well as a [technical paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). from [openAI Blog](https://blog.openai.com/better-language-models/)\n\nThis repository is simple implementation GPT-2 about **text-generator** in **Pytorch** with **compress code**\n\n- The original repertoire is [openai/gpt-2](https://github.com/openai/gpt-2). Also You can Read Paper about gpt-2, [\"Language Models are Unsupervised Multitask Learners\"](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). To Understand more detail concept, I recommend papers about Transformer Model.\n- Good implementation GPT-2 in Pytorch which I referred to, [huggingface/pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT), You can see more detail implementation in huggingface repository.\n\n- Transformer(Self-Attention) Paper : [Attention Is All You Need(2017)](https://arxiv.org/abs/1706.03762)\n- First OpenAi-GPT Paper : [Improving Language Understanding by Generative Pre-Training(2018)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\n- See [OpenAI Blog](https://blog.openai.com/better-language-models/) about GPT-2 and Paper\n\n\n\n## Quick Start\n\n1. download GPT2 pre-trained model in Pytorch which huggingface/pytorch-pretrained-BERT already made! (Thanks for sharing! it's help my problem transferring tensorflow(ckpt) file to Pytorch Model!)\n```shell\n$ git clone https://github.com/graykode/gpt-2-Pytorch && cd gpt-2-Pytorch\n# download huggingface's pytorch model \n$ curl --output gpt2-pytorch_model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\n# setup requirements, if using mac os, then run additional setup as descibed below\n$ pip install -r requirements.txt\n```\n\n\n2. Now, You can run like this.\n\n- Text from Book 1984, George Orwell\n\n```shell\n$ python main.py --text \"It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.\"\n```\n\n3. Also You can Quick Starting in [Google Colab](https://colab.research.google.com/github/graykode/gpt-2-Pytorch/blob/master/GPT2_Pytorch.ipynb)\n\n\n\n## Option\n\n- `--text` : sentence to begin with.\n- `--quiet` : not print all of the extraneous stuff like the \"================\"\n- `--nsamples` : number of sample sampled in batch when multinomial function use\n- `--unconditional` : If true, unconditional generation.\n- `--batch_size` : number of batch size\n- `--length` : sentence length (< number of context)\n- `--temperature`:  the thermodynamic temperature in distribution `(default 0.7)`\n- `--top_k`  : Returns the top k largest elements of the given input tensor along a given dimension. `(default 40)`\n\nSee more detail option about `temperature` and `top_k` in [here](https://github.com/openai/gpt-2#gpt-2-samples)\n\n\n\n## Dependencies\n\n- Pytorch 0.41+\n- regex 2017.4.5\n\n### Mac OS Setup\n```shell\n$ python3 -m venv venv\n$ source venv/bin/activate\n$ pip install torch tqdm\n$ brew install libomp\n$ export LC_ALL=en_US.UTF-8\n$ export LANG=en_US.UTF-8\n$ pip install -r requirements.txt\n```\n\n## Author\n\n- Tae Hwan Jung(Jeff Jung) @graykode\n- Author Email : [nlkey2022@gmail.com](mailto:nlkey2022@gmail.com)\n\n\n\n## License\n\n- OpenAi/GPT2 follow MIT license, huggingface/pytorch-pretrained-BERT is Apache license. \n- I follow MIT license with original GPT2 repository\n\n\n\n## Acknowledgement\n\n[Jeff Wu(@WuTheFWasThat)](https://github.com/WuTheFWasThat), [Thomas Wolf(@thomwolf)](https://github.com/thomwolf) for allowing referring code.",
            "readme_url": "https://github.com/graykode/gpt-2-Pytorch",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Attention Is All You Need",
            "arxiv": "1706.03762",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.03762v5",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999313296052807,
        "task": "Machine Translation",
        "task_prob": 0.9511926577009263
    }
}