Text;Label;Repo
"""This is a Project for Continuous Control Deep Reinforcement Learning Nanodegree @ Udacity. The Task is to follow a target with a multijoint robot arm. A DDPG model is applied to accomplish the task. The model achieves the desired +30 score on average per episode.   Download the appropriate Unity environment for your system from above github repository.  Create a python 3.6* environment  containing the following packages   pytorch 0.4*   Clone repo  Update environment path in you copy (Continuous_Control.ipynb)  Run the Continuous_Control.ipynb notebook   """;Reinforcement Learning;https://github.com/petsol/ContinuousControl_UnityAgent_DDPG_Udacity
"""- To install  `cd` into the root directory and type `pip install -e .`  - To interactively view moving to landmark scenario (see others in ./scenarios/): `bin/interactive.py --scenario simple.py`  - Known dependencies: Python (3.5.4)  OpenAI gym (0.10.5)  numpy (1.14.5)  - To use the environments  look at the code for importing them in `make_env.py`.   """;Reinforcement Learning;https://github.com/lachisis/multiagent-particle-envs
"""| 2 | Federated Learning on MNIST using PyTorch + PySyft | Day 4 |   Link ::link:   https://github.com/gargarchit/PATE_Analysis   with Accuracy 92.363%   Link:link:: https://github.com/gargarchit/Federated-Learning-on-MNIST   :heavy_check_mark:Make a Repo Link: https://github.com/gargarchit/60DaysOfUdacity to manage my Daily Updates on #60daysofudacity   Link:link:: https://github.com/gargarchit/Federated-Learning-on-MNIST   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity   Link:link:: https://github.com/gargarchit/Federated-Learning-on-MNIST   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   Dataset Link: :link: https://www.kaggle.com/c/virtual-hack   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :bettertogether::dart:Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :link: https://secureprivataischolar.slack.com/archives/CKC6MNSRG/p1566486761374100   :bettertogether::dart:Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :bettertogether::dart:Updated: https://github.com/gargarchit/60DaysOfUdacity    :bettertogether::dart:Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart:Updated my #60daysofudacity posts :link: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   """;Computer Vision;https://github.com/gargarchit/60DaysOfUdacity
"""GPU: 1  8GB  GM204GL [Tesla M60]   """;General;https://github.com/paniabhisek/maxout
"""torch  tensorboard (2.1 or higher)  biopython (see [requirements.txt](requirements.txt) for the complete list). Install with: ``` pip3 install -r requirements.txt [--user] ```  Be sure that your PYTHONPATH environment variable has the deepH3-distances-orientations/ directory. On linux  use the following command: ``` export PYTHONPATH=""$PYTHONPATH:/absolute/path/to/deepH3-distances-orientations"" ```   downloaded from SAbDab  run:  cd deeph3   cd deeph3   cd deeph3   """;General;https://github.com/Graylab/deepH3-distances-orientations
"""Dependecies to run the notebook:  - scikitlearn pip install sklearn  - networkx pip install networkx  - grakel pip install grakel-dev   """;Graphs;https://github.com/FilippoMB/Benchmark_dataset_for_graph_classification
"""```bash $ pip install g-mlp-gpt ```   ```python import torch from g_mlp_gpt import gMLPGPT  model = gMLPGPT(     num_tokens = 20000      dim = 512      depth = 4      seq_len = 1024      window = (128  256  512  1024) #: window sizes for each depth )  x = torch.randint(0  20000  (1  1000)) logits = model(x) #: (1  1000  20000) ```  16k context length  ```python import torch from g_mlp_gpt import gMLPGPT  model = gMLPGPT(     num_tokens = 20000      dim = 512      seq_len = 16384      reversible = True     #: reversible networks     act = nn.Tanh()       #: tanh activation for spatial gating     depth = 12      window = (         128          128          256          512          1024          1024          (2048  2)     #: window size of 2048  axial of 2         (2048  2)          (4096  4)          (4096  4)          (8192  8)     #: window size of 8192  axial of 8         (8192  8)     ) ).cuda()  x = torch.randint(0  20000  (1  16384)).cuda() logits = model(x) #: (1  16384  20000) ```   """;Computer Vision;https://github.com/lucidrains/g-mlp-gpt
"""Residual Attention Layer Transformer  shortened as RealFormer  is a transformer variant that incorporatess residual skip connections to allow previous attention scores to pass through the entire network. It outperforms canonical transformers on a variety of tasks and datasets  including masked language modeling (MLM)  [GLUE](https://gluebenchmark.com)  and [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/).   git clone https://github.com/jaketae/realformer.git  Navigate to the cloned directory. You can start using the model via   Just like torch.nn.TransformerEncoder  the RealFormerEncoder does not include any embedding layers. It is recommended that you implemenet positional encoding schemes (e.g. sinusodial tables  learnable embeddings) as needed.   """;Natural Language Processing;https://github.com/jaketae/realformer
"""Python  Pytorch   """;General;https://github.com/mminamina/Transfer_Learning_NaimishNet---Keypoints_Detection
"""You can download    """;Computer Vision;https://github.com/Atharva-Phatak/GAN_MNIST
"""<a href=""https://media.giphy.com/media/Ri2xsHaPlKv2sbv4pz/giphy.gif""><img src=""https://media.giphy.com/media/Ri2xsHaPlKv2sbv4pz/giphy.gif""/></a>   """;Computer Vision;https://github.com/UrosOgrizovic/SimpleGoogleQuickdraw
"""For other dataset follow instruction here. For other pre-processing run following command :   """;Audio;https://github.com/rishikksh20/FastSpeech2
"""1. Clone the repo    ```sh    git clone https://github.com/leokster/CVAE.git    ``` 2. Install NPM packages    ```sh    npm install    ```     <!-- ROADMAP -->  Commit your Changes (git commit -m 'Add some AmazingFeature')   To get a local copy up and running follow these simple steps.   """;Computer Vision;https://github.com/leokster/CVAE
"""This is a standard train-dev-test split on all the 8732 datapoints from the dataset.  <br />   """;Computer Vision;https://github.com/nitinvwaran/UrbanSound8K-audio-classification-with-ResNet
"""env virtual environment   """;Natural Language Processing;https://github.com/Jun-Zhang-32108/Sentiment-Analysis
"""This is an implementation of submitted paper:   Desai  S. V.  Balasubramanian  V. N.  Fukatsu  T.  Ninomiya  S.  & Guo  W. (2019). Automatic estimation of heading date of paddy rice using deep learning. Plant Methods  15(1)  76. https://doi.org/10.1186/s13007-019-0457-1   To plan the perfect time for harvest of rice crops  we detect and quantify the flowering of paddy rice. The dataset of rice crop images used in this work is taken from [1].   """;Computer Vision;https://github.com/svdesai/heading-date-estimation
"""- <a href=""https://arxiv.org/pdf/1512.03385.pdf"" target=""_blank"">Residual Network 101</a> - <a href=""https://arxiv.org/pdf/1512.00567.pdf"">Inception V3</a> - <a href=""https://arxiv.org/pdf/1602.07261.pdf"">Inception ResNet V2</a>  """;General;https://github.com/JanMarcelKezmann/Residual-Network-Architectures
"""``` pip install -r requirements.txt pip install pafy youtube-dl  ```  For the tflite runtime  you can either use tensorflow `pip install tensorflow` or the [TensorFlow Runtime](https://www.tensorflow.org/lite/guide/python)   Source: https://www.flickr.com/photos/32413914@N00/1475776461/    * **Image inference**:    ```  python imageLaneDetection.py   ```     * **Webcam inference**:    ```  python webcamLaneDetection.py  ```     * **Video inference**:    ```  python videoLaneDetection.py  ```   """;Computer Vision;https://github.com/ibaiGorordo/TfLite-Ultra-Fast-Lane-Detection-Inference
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/Shraddha2013/darknett
"""Set up a generator and discriminator model  ```python from models import Generator  Discriminator generator = Generator(img_size=(32  32  1)  latent_dim=100  dim=16) discriminator = Discriminator(img_size=(32  32  1)  dim=16) ```  The generator and discriminator are built to automatically scale with image sizes  so you can easily use images from your own dataset.  Train the generator and discriminator with the WGAN-GP loss  ```python import torch #: Initialize optimizers G_optimizer = torch.optim.Adam(generator.parameters()  lr=1e-4  betas=(.9  .99)) D_optimizer = torch.optim.Adam(discriminator.parameters()  lr=1e-4  betas=(.9  .99))  #: Set up trainer from training import Trainer trainer = Trainer(generator  discriminator  G_optimizer  D_optimizer                    use_cuda=torch.cuda.is_available())  #: Train model for 200 epochs trainer.train(data_loader  epochs=200  save_training_gif=True) ```  This will train the models and generate a gif of the training progress.  Note that WGAN-GPs take a *long* time to converge. Even on MNIST it takes about 50 epochs to start seeing decent results. For more information and a full example on MNIST  check out `main.py`.   """;Computer Vision;https://github.com/EmilienDupont/wgan-gp
"""Machine translation is a natural language processing task that aims to translate natural languages using computers automatically. Recent several years have witnessed the rapid development of end-to-end neural machine translation  which has become the new mainstream method in practical MT systems.  THUMT is an open-source toolkit for neural machine translation developed by [the Natural Language Processing Group at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/site2/index.php?lang=en). The website of THUMT is: [http://thumt.thunlp.org/](http://thumt.thunlp.org/).   The online demo of THUMT is available at [http://translate.thumt.cn/](http://101.6.5.207:3892/). The languages involved include Ancient Chinese  Arabic  Chinese  English  French  German  Indonesian  Japanese  Portugese  Russian  and Spanish.   """;Natural Language Processing;https://github.com/wangmz15/Chinese-Error-Correction-with-THUMT
"""This code was tested with Pytoch 1.2.0  CUDA 10.1  Python 3.6 and Ubuntu 16.04 with a 2080Ti GPU  - Install Pytoch 1.2.0  torchvision  and other dependencies from [http://pytorch.org](http://pytorch.org) - Install python libraries [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate) for visualization   ``` pip install visdom dominate ``` - Clone this repo (we suggest to only clone the depth 1 version):  ``` git clone https://github.com/idealwhite/tdanet --depth 1 cd tdanet ``` - Download the dataset and pre-processed files as in following steps.    to dataset/ directory as specified in config.bird.yml/config.coco.yml.  Download the pre-trained models bird inpainting or coco inpainting and put them undercheckpoints/ directory.  Install the PyQt5 for GUI operation  pip install PyQt5   <img src='https://github.com/idealwhite/tdanet/blob/master/images/inpainting_example.png' align=""center"">   <img src='https://github.com/idealwhite/tdanet/blob/master/images/manipulation_example.png' align=""center"">   ``` python train.py --name tda_bird  --gpu_ids 0 --model tdanet --mask_type 0 1 2 3 --img_file ./datasets/CUB_200_2011/train.flist --mask_file ./datasets/CUB_200_2011/train_mask.flist --text_config config.bird.yml ``` - **Important:** Add ```--mask_type``` in options/base_options.py for different training masks. ```--mask_file``` path is needed for **object mask**  use train_mask.flist for CUB and image_mask_coco_all.json for COCO. ```--text_config``` refer to the yml configuration file for text setup  ```--img_file``` is the image file dir or file list. - To view training results and loss plots  run ```python -m visdom.server``` and copy the URL [http://localhost:8097](http://localhost:8097). - Training models will be saved under the **./checkpoints** folder. - More training options can be found in **./options** folder. - **Suggestion:** use mask type 0 1 2 3 for CUB dataset and 0 1 2 4 for COCO dataset. Train more than 2000 epochs for CUB and 200 epochs for COCO.    Test  ``` python test.py --name tda_bird  --img_file datasets/CUB_200_2011/test.flist --results_dir results/tda_bird  --mask_file datasets/CUB_200_2011/test_mask.flist --mask_type 3 --no_shuffle --gpu_ids 0 --nsampling 1 --no_variance ``` **Note**:  - Remember to add  the ```--no_variance``` option to get better performance.   - For COCO object mask  use image_mask_coco_all.json as the mask file..  A ```eval_tda_bird.flist``` will be generated after the test. Then in the evaluation  this file is used as the ground truth file list:  ``` python evaluation.py --batch_test 60 --ground_truth_path eval_tda_bird.flist --save_path results/tda_bird ``` - Add ```--ground_truth_path``` to the dir of ground truth image path or list. ```--save_path``` as the result dir.    """;General;https://github.com/idealwhite/tdanet
"""- Install keras with tensorflow backend and dependencies from https://keras.io/#installation - Install python packages `jupyter-notebook`  `scikit-image` and `opencv`  ```bash pip install scikit-image pip install jupyter pip install opencv-python ``` - Install [livelossplot](https://github.com/stared/livelossplot)(optional) - a live monitor during training.  - Clone this repo: ```bash git clone https://github.com/isrugeek/semcolour cd semcolour ```  """;Computer Vision;https://github.com/isrugeek/semcolour
"""You can download pretrained models here:   """;General;https://github.com/a07458666/simsiamfacenet
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   Install <a href=""https://www.tensorflow.org/get_started/os_setup"" target=""_blank"">TensorFlow</a>. You may also need to install h5py. The code has been tested with Python 2.7  TensorFlow 1.0.1  CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04.  If you are using PyTorch  you can find a third-party pytorch implementation <a href=""https://github.com/fxia22/pointnet.pytorch"" target=""_blank"">here</a>.  To install h5py for Python: ```bash sudo apt-get install libhdf5-dev sudo pip install h5py ```   Created by <a href=""http://charlesrqi.com"" target=""_blank"">Charles R. Qi</a>  <a href=""http://ai.stanford.edu/~haosu/"" target=""_blank"">Hao Su</a>  <a href=""http://cs.stanford.edu/~kaichun/"" target=""_blank"">Kaichun Mo</a>  <a href=""http://geometry.stanford.edu/member/guibas/"" target=""_blank"">Leonidas J. Guibas</a> from Stanford University.  ![prediction example](https://github.com/charlesq34/pointnet/blob/master/doc/teaser.png)   cd part_seg  sh download_data.sh   To train a model to classify point clouds sampled from 3D shapes:      python train.py  Log files and network parameters will be saved to `log` folder in default. Point clouds of <a href=""http://modelnet.cs.princeton.edu/"" target=""_blank"">ModelNet40</a> models in HDF5 files will be automatically downloaded (416MB) to the data folder. Each point cloud contains 2048 points uniformly sampled from a shape surface. Each cloud is zero-mean and normalized into an unit sphere. There are also text files in `data/modelnet40_ply_hdf5_2048` specifying the ids of shapes in h5 files.  To see HELP for the training script:      python train.py -h  We can use TensorBoard to view the network architecture and monitor the training progress.      tensorboard --logdir log  After the above training  we can evaluate the model and output some visualizations of the error cases.      python evaluate.py --visu  Point clouds that are wrongly classified will be saved to `dump` folder in default. We visualize the point cloud by rendering it into three-view images.  If you'd like to prepare your own data  you can refer to some helper functions in `utils/data_prep_util.py` for saving and loading HDF5 files.   * <a href=""http://stanford.edu/~rqi/pointnet2/"" target=""_blank"">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a> by Qi et al. (NIPS 2017) A hierarchical feature learning framework on point clouds. The PointNet++ architecture applies PointNet recursively on a nested partitioning of the input point set. It also proposes novel layers for point clouds with non-uniform densities. * <a href=""http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w13/Engelmann_Exploring_Spatial_Context_ICCV_2017_paper.pdf"" target=""_blank"">Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</a> by Engelmann et al. (ICCV 2017 workshop). This work extends PointNet for large-scale scene segmentation. * <a href=""https://arxiv.org/abs/1710.04954"" target=""_blank"">PCPNET: Learning Local Shape Properties from Raw Point Clouds</a> by Guerrero et al. (arXiv). The work adapts PointNet for local geometric properties (e.g. normal and curvature) estimation in noisy point clouds. * <a href=""https://arxiv.org/abs/1711.06396"" target=""_blank"">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a> by Zhou et al. from Apple (arXiv) This work studies 3D object detection using LiDAR point clouds. It splits space into voxels  use PointNet to learn local voxel features and then use 3D CNN for region proposal  object classification and 3D bounding box estimation. * <a href=""https://arxiv.org/abs/1711.08488"" target=""_blank"">Frustum PointNets for 3D Object Detection from RGB-D Data</a> by Qi et al. (arXiv) A novel framework for 3D object detection with RGB-D data. The method proposed has achieved first place on KITTI 3D object detection benchmark on all categories (last checked on 11/30/2017).  """;Computer Vision;https://github.com/abdullahozer11/Segmentation-and-Classification-of-Objects-in-Point-Clouds
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   Install <a href=""https://www.tensorflow.org/get_started/os_setup"" target=""_blank"">TensorFlow</a>. You may also need to install h5py. The code has been tested with Python 2.7  TensorFlow 1.0.1  CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04.  If you are using PyTorch  you can find a third-party pytorch implementation <a href=""https://github.com/fxia22/pointnet.pytorch"" target=""_blank"">here</a>.  To install h5py for Python: ```bash sudo apt-get install libhdf5-dev sudo pip install h5py ```   Created by <a href=""http://charlesrqi.com"" target=""_blank"">Charles R. Qi</a>  <a href=""http://ai.stanford.edu/~haosu/"" target=""_blank"">Hao Su</a>  <a href=""http://cs.stanford.edu/~kaichun/"" target=""_blank"">Kaichun Mo</a>  <a href=""http://geometry.stanford.edu/member/guibas/"" target=""_blank"">Leonidas J. Guibas</a> from Stanford University.  ![prediction example](https://github.com/charlesq34/pointnet/blob/master/doc/teaser.png)   cd part_seg  sh download_data.sh   To train a model to classify point clouds sampled from 3D shapes:      python train.py  Log files and network parameters will be saved to `log` folder in default. Point clouds of <a href=""http://modelnet.cs.princeton.edu/"" target=""_blank"">ModelNet40</a> models in HDF5 files will be automatically downloaded (416MB) to the data folder. Each point cloud contains 2048 points uniformly sampled from a shape surface. Each cloud is zero-mean and normalized into an unit sphere. There are also text files in `data/modelnet40_ply_hdf5_2048` specifying the ids of shapes in h5 files.  To see HELP for the training script:      python train.py -h  We can use TensorBoard to view the network architecture and monitor the training progress.      tensorboard --logdir log  After the above training  we can evaluate the model and output some visualizations of the error cases.      python evaluate.py --visu  Point clouds that are wrongly classified will be saved to `dump` folder in default. We visualize the point cloud by rendering it into three-view images.  If you'd like to prepare your own data  you can refer to some helper functions in `utils/data_prep_util.py` for saving and loading HDF5 files.   * <a href=""http://stanford.edu/~rqi/pointnet2/"" target=""_blank"">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a> by Qi et al. (NIPS 2017) A hierarchical feature learning framework on point clouds. The PointNet++ architecture applies PointNet recursively on a nested partitioning of the input point set. It also proposes novel layers for point clouds with non-uniform densities. * <a href=""http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w13/Engelmann_Exploring_Spatial_Context_ICCV_2017_paper.pdf"" target=""_blank"">Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</a> by Engelmann et al. (ICCV 2017 workshop). This work extends PointNet for large-scale scene segmentation. * <a href=""https://arxiv.org/abs/1710.04954"" target=""_blank"">PCPNET: Learning Local Shape Properties from Raw Point Clouds</a> by Guerrero et al. (arXiv). The work adapts PointNet for local geometric properties (e.g. normal and curvature) estimation in noisy point clouds. * <a href=""https://arxiv.org/abs/1711.06396"" target=""_blank"">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a> by Zhou et al. from Apple (arXiv) This work studies 3D object detection using LiDAR point clouds. It splits space into voxels  use PointNet to learn local voxel features and then use 3D CNN for region proposal  object classification and 3D bounding box estimation. * <a href=""https://arxiv.org/abs/1711.08488"" target=""_blank"">Frustum PointNets for 3D Object Detection from RGB-D Data</a> by Qi et al. (arXiv) A novel framework for 3D object detection with RGB-D data. The method proposed has achieved first place on KITTI 3D object detection benchmark on all categories (last checked on 11/30/2017).  """;Computer Vision;https://github.com/KhusDM/PointNetTree
"""A basic example: ```python import fid  generator = ... #: Your code for creating the GAN generator noise = ... #: Your generator inputs (usually random noise) real_images = ... #: Your training dataset  #: change (0 1) to the range of values in your dataset fd = fid.FrechetInceptionDistance(generator  (0 1))  gan_fid = fd(real_images  noise) ```  If you already have the means and covariances: ```python gan_fid = fid.frechet_distance(mean1  cov1  mean2  cov2) ```   """;General;https://github.com/jleinonen/keras-fid
"""- `mkdir result`   - download your basketball video in the directory `dataset/`   ├── pytorch_YOLOv4                    pytorch-YOLOv4 source code  │   ├── weights                       need to download weights     <img src=""https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/pic/gif_highlight.gif"" width=""267"" height=""225""/>   conda create --name py36_env python=3.6  conda activate py36_env  cd AI_basketball_games_video_editor   mkdir pytorch_YOLOv4/weights/   mkdir pytorch_YOLOv4/weights/   ```sh git clone https://github.com/OwlTing/AI_basketball_games_video_editor.git ```   ```sh python video_editor.py --video_path VIDEO_PATH --output_path OUTPUT_PATH --output_video_name OUTPUT_VIDEO_NAME [OPTIONS]  #: example python video_editor.py --video_path dataset/basketball_demo.mp4 --output_path result/demo --output_video_name out_demo.mp4 ```  - It will generate `your_output_video_name.mp4 obj_log_name.data` in the directory `result/`  - If you had finished extracting features. You can use `--read_flag 1` to read log for different output video mode.   - If you use pytorch yolov4 object detector engine `--inference_detector pytorch`.     For image input size  you can select any inference_size = (height  width) in      height = 320 + 96 * n  n in {0  1  2  3  ...}     width = 320 + 96 * m  m in {0  1  2  3  ...}     Exmaple `--inference_size (1184  1184)` or `--inference_size (704  704)`     Default inference_size is (1184  1184)    - If you use tensorrt yolov4 object detector engine `--inference_detector tensorrt`.     For image input size  you only can select `--inference_size (1184  1184)`.     Tensorrt engine 3x faster than pytorch engine fps.  - You can use `--output_mode shot` to select different output video mode.   ```   output video mode     full            show person basketball basketball_hoop frame_information     basketball      show basketball basketball_hoop frame_information     shot            show basketball shot frame_information     standard        show frame_information     clean           only cutting video   ``` ![image](https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/pic/output_mode_clean.jpg) ![image](https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/pic/output_mode_full.jpg) ![image](https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/pic/output_mode_basketball.jpg) ![image](https://github.com/OwlTing/AI_basketball_games_video_editor/blob/master/pic/output_mode_shot.jpg)  - You can refer the command-line options.   ```   optional arguments:   -h  --help                                       show this help message and exit      --video_path VIDEO_PATH                          input video path (default: None)                                                       --output_path OUTPUT_PATH                        output folder path (default: None)                                                       --output_video_name OUTPUT_VIDEO_NAME            output video name (default: None)                                                       --highlight_flag HIGHLIGHT_FLAG                  select 1 with auto-generated highlight or                                                     0 without auto-generated highlight (default: 1)                                                       --output_mode OUTPUT_MODE                        output video mode                                                     full       show person basketball basketball_hoop frame_information                                                     basketball show basketball basketball_hoop frame_information                                                     shot       show basketball shot frame_information                                                     standard   show frame_information                                                     clean      only cutting video (default: shot)                                                       --process_frame_init PROCESS_FRAME_INIT          start processing frame (default: 0)                                                       --process_frame_final PROCESS_FRAME_FINAL        end processing frame. If process_frame_final < 0                                                      use video final frame (default: -1)                                                       --obj_log_name OBJ_LOG_NAME                      save frame information and obj detect result                                                     (default: obj_log_name.data)                                                       --save_step SAVE_STEP                            save obj log for each frame step (default: 2000)                                                       --weight_path WEIGHT_PATH                        Yolov4 weight path (default: pytorch_YOLOv4/weights/yolov4-basketball.weights)                                                       --cfg_path CFG_PATH                              Yolov4 cfg path (default: pytorch_YOLOv4/cfg/yolov4-basketball.cfg)      --num_classes NUM_CLASSES                        num classes = 3 (person/basketball/basketball_hoop) (default: 3)                                                       --namesfile_path NAMESFILE_PATH                  Yolov4 class names path (default: pytorch_YOLOv4/data/basketball_obj.names)                                                       --inference_detector INFERENCE_DETECTOR          object detector engine. You can select pytorch or tensorrt (default: pytorch)                                                       --inference_size INFERENCE_SIZE                  Image input size for inference                                                     If you use pytorch yolov4 object detector engine                                                     height = 320 + 96 * n  n in {0  1  2  3  ...}                                                     width = 320 + 96 * m  m in {0  1  2  3  ...}                                                     inference_size= (height  width)                                                                                                         If you use tensorrt yolov4 object detector engine Image input size for                                                    inference only with inference_size = (1184  1184) (default: (1184  1184))                                                       --read_flag READ_FLAG                            read log mode flag If you had finished extracting features. You can use                                                     select 1 to read log for different output video mode. (default: 0)                                                                                                        --cut_frame CUT_FRAME                            cut frame range around shot frame index for highlight video (default: 50)     ```  Reference: - https://github.com/Tianxiaomo/pytorch-YOLOv4 - https://github.com/eriklindernoren/PyTorch-YOLOv3 - https://github.com/marvis/pytorch-caffe-darknet-convert - https://github.com/marvis/pytorch-yolo3  - Paper Yolo v4: https://arxiv.org/abs/2004.10934 - Source code Yolo v4:https://github.com/AlexeyAB/darknet - More details: http://pjreddie.com/darknet/yolo/  ``` @article{yolov4    title={YOLOv4: YOLOv4: Optimal Speed and Accuracy of Object Detection}    author={Alexey Bochkovskiy  Chien-Yao Wang  Hong-Yuan Mark Liao}    journal = {arXiv}    year={2020} } ```  Contact:   Issues should be raised directly in the repository.   If you are very interested in this project  please feel free to contact me (george_chen@owlting.com).    """;Computer Vision;https://github.com/OwlTing/AI_basketball_games_video_editor
"""If you failed to build the .so file  make sure you didn't set your Tensorflow in development mode. I ran into this problem just now  and once I switch back to the pip package install directory  I was able to build again.   The makefile assumes your cuda library is installed in /usr/local/cuda  if you installed it somewhere else  you can change the part -L /usr/local/cuda/lib64/ in the last line of the makefile to -L [your cuda install path]/lib64/   This may not be required (may even raise error) if you are using newer version of TensorFlow.   """;General;https://github.com/MycChiu/fast-LayerNorm-TF
"""Ensure you have [CUDA](https://developer.nvidia.com/cuda-toolkit) and [cudNN](https://developer.nvidia.com/cudnn) installed. This has only been tested up to CUDA v9.0.  To install all depedencies  follow the instructions located at the top of the [Ubuntu 16.04 Installation Guide](https://github.com/BVLC/caffe/wiki/Ubuntu-16.04-Installation-Guide). This installation guide goes on to discuss installation using the Makefile  which is recommended for Python users. If you with to use Caffe with C++  jump ahead to [here](#cmake).   This method is recommended if you wish to use Caffe with Python! Make sure to add the caffe-segnet-cudnn7/python folder to your PYTHONPATH. Follow the remaining instructions from the Ubuntu 16.04 Installation Guide.  Some of the necessary changes to the Makefile have already been made  however  the above guide does provide a good reference for installation dependencies and build steps.   To install Caffe using the community-created CMake file  perform the following:  Note: This will install Caffe to /usr/local. If you wish to install it elsewhere  set the CMAKE_INSTALL_PREFIX  but ensure that any project using it will be able to find it.  mkdir cmake_build  cd cmake_build   make runtest  sudo make install   To use caffe-segnet-cudnn7 in a CMake project  you can now add the following lines to your project's CMakeLists.txt:   find_package(Caffe REQUIRED)   """;Computer Vision;https://github.com/sducournau/caffe-segnet-cudnn7
"""- TODO: 	- Load vocabulary. 	- Perform decoding after the translation. ---  """;General;https://github.com/leejieun51/transformer
"""device: 'cuda' or 'cpu'  depending if you want to train on GPU or not   """;General;https://github.com/zhengant/vae
"""We provide code to test our model on Human3.6M. Please follow the instructions below to setup and use our code. The typical procedure is 1) apply the ConvNet models using a torch script through command line and then 2) run a MATLAB script (from folder matlab) for visualization or evaluation. To run this code  make sure the following are installed:   Then  to visualize the output  you can use the MATLAB script:   Then  to conclude the evaluation  you need to run the next MATLAB script:   """;Computer Vision;https://github.com/geopavlakos/ordinal-pose3d
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/hhuaibo/darknet
"""This repo is now deprecated  I am migrating to the latest Gluon-CV which is more user friendly and has a lot more algorithms in development. This repo will not receive active development  however  you can continue use it with the mxnet 1.1.0(probably 1.2.0).  Now this repo is internally synchronized up to data with offical mxnet backend. pip install mxnet will work for this repo as well in most cases.   Update to the latest version according to caffe version  with 5% mAP increase.   Extra operators are now in mxnet/src/operator/contrib  symbols are modified. Please use Release-v0.2-beta for old models.   * Download the PASCAL VOC dataset  skip this step if you already have one.   cd /path/to/where_you_store_datasets/   : cd /path/to/mxnet-ssd  bash tools/prepare_pascal.sh  : or if you are using windows   : cd /path/to/mxnet-ssd   : cd /path/to/mxnet-ssd   Converter from caffe is available at /path/to/mxnet-ssd/tools/caffe_converter   cd /path/to/mxnet-ssd/tools/caffe_converter   now you can execute commands the same way as you would  if you'd install mxnet on your own computer.   If you chose to work with dockers  you have it installed in the pre-built image you've downloaded. otherwise  follow here for installation steps.   cd /res   ![demo1](https://cloud.githubusercontent.com/assets/3307514/19171057/8e1a0cc4-8be0-11e6-9d8f-088c25353b40.png) ![demo2](https://cloud.githubusercontent.com/assets/3307514/19171063/91ec2792-8be0-11e6-983c-773bd6868fa8.png) ![demo3](https://cloud.githubusercontent.com/assets/3307514/19171086/a9346842-8be0-11e6-8011-c17716b22ad3.png)   * Option #1 - install using 'Docker'. if you are not familiar with this technology  there is a 'Docker' section below. you can get the latest image: ``` docker pull daviddocker78/mxnet-ssd:gpu_0.12.0_cuda9 ``` * You will need python modules: `cv2`  `matplotlib` and `numpy`. If you use mxnet-python api  you probably have already got them. You can install them via pip or package manegers  such as `apt-get`: ``` sudo apt-get install python-opencv python-matplotlib python-numpy ``` * Clone this repo: ``` #: if you don't have git  install it via apt or homebrew/yum based on your system sudo apt-get install git #: cd where you would like to clone this repo cd ~ git clone --recursive https://github.com/zhreshold/mxnet-ssd.git #: make sure you clone this with --recursive #: if not done correctly or you are using downloaded repo  pull them all via: #: git submodule update --recursive --init cd mxnet-ssd/mxnet ``` * (Skip this step if you have offcial MXNet installed.) Build MXNet: `cd /path/to/mxnet-ssd/mxnet`. Follow the official instructions [here](http://mxnet.io/get_started/install.html). ``` #: for Ubuntu/Debian cp make/config.mk ./config.mk #: modify it if necessary ``` Remember to enable CUDA if you want to be able to train  since CPU training is insanely slow. Using CUDNN is optional  but highly recommanded.   * Download the pretrained model: [`ssd_resnet50_0712.zip`](https://github.com/zhreshold/mxnet-ssd/releases/download/v0.6/resnet50_ssd_512_voc0712_trainval.zip)  and extract to `model/` directory. * Run ``` #: cd /path/to/mxnet-ssd python demo.py --gpu 0 #: play with examples: python demo.py --epoch 0 --images ./data/demo/dog.jpg --thresh 0.5 python demo.py --cpu --network resnet50 --data-shape 512 #: wait for library to load for the first time ``` * Check `python demo.py --help` for more options.   """;Computer Vision;https://github.com/zhreshold/mxnet-ssd
"""This allows you to greatly simplify the model  since it does not have to deal with the manual placement of tensors.  Instead  you just specify which GPU you'd like to use in the beginning of your script.   : Horovod: pin GPU to be used to process local rank (one GPU per process)   Congratulations!  If you made it this far  your fashion_mnist.py should now be fully distributed.  To verify  you can run the following command in the terminal  which should produce no output:   In this tutorial  you will learn how to apply Horovod to a [WideResNet](https://arxiv.org/abs/1605.07146) model  trained on the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset.   In `fashion_mnist.py`  we're using the filename of the last checkpoint to determine the epoch to resume training from in case of a failure:  ![image](https://user-images.githubusercontent.com/16640218/54185268-d35d3f00-4465-11e9-99eb-96d4b99f1d38.png)  As you scale your workload to multi-node  some of your workers may not have access to the filesystem containing the checkpoint.  For that reason  we make the first worker to determine the epoch to restart from  and *broadcast* that information to the rest of the workers.  To broadcast the starting epoch from the first worker  add the following code:  ```python #: Horovod: broadcast resume_from_epoch from rank 0 (which will have #: checkpoints) to other ranks. resume_from_epoch = hvd.broadcast(resume_from_epoch  0  name='resume_from_epoch') ```  ![image](https://user-images.githubusercontent.com/16640218/53534072-2de3bc00-3ab2-11e9-8cf1-7531542e3202.png) (see line 52-54)   """;Computer Vision;https://github.com/darkreapyre/HaaS-GitOps
"""``` import os import keras.backend as K  from data import DATA_SET_DIR from elmo.lm_generator import LMDataGenerator from elmo.model import ELMo  parameters = {     'multi_processing': False      'n_threads': 4      'cuDNN': True if len(K.tensorflow_backend._get_available_gpus()) else False      'train_dataset': 'wikitext-2/wiki.train.tokens'      'valid_dataset': 'wikitext-2/wiki.valid.tokens'      'test_dataset': 'wikitext-2/wiki.test.tokens'      'vocab': 'wikitext-2/wiki.vocab'      'vocab_size': 28914      'num_sampled': 1000      'charset_size': 262      'sentence_maxlen': 100      'token_maxlen': 50      'token_encoding': 'word'      'epochs': 10      'patience': 2      'batch_size': 1      'clip_value': 5      'cell_clip': 5      'proj_clip': 5      'lr': 0.2      'shuffle': True      'n_lstm_layers': 2      'n_highway_layers': 2      'cnn_filters': [[1  32]                      [2  32]                      [3  64]                      [4  128]                      [5  256]                      [6  512]                      [7  512]                     ]      'lstm_units_size': 400      'hidden_units_size': 200      'char_embedding_size': 16      'dropout_rate': 0.1      'word_dropout_rate': 0.05      'weight_tying': True  }  #: Set-up Generators train_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['train_dataset'])                                    os.path.join(DATA_SET_DIR  parameters['vocab'])                                    sentence_maxlen=parameters['sentence_maxlen']                                    token_maxlen=parameters['token_maxlen']                                    batch_size=parameters['batch_size']                                    shuffle=parameters['shuffle']                                    token_encoding=parameters['token_encoding'])  val_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['valid_dataset'])                                  os.path.join(DATA_SET_DIR  parameters['vocab'])                                  sentence_maxlen=parameters['sentence_maxlen']                                  token_maxlen=parameters['token_maxlen']                                  batch_size=parameters['batch_size']                                  shuffle=parameters['shuffle']                                  token_encoding=parameters['token_encoding'])  test_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['test_dataset'])                                  os.path.join(DATA_SET_DIR  parameters['vocab'])                                  sentence_maxlen=parameters['sentence_maxlen']                                  token_maxlen=parameters['token_maxlen']                                  batch_size=parameters['batch_size']                                  shuffle=parameters['shuffle']                                  token_encoding=parameters['token_encoding'])  #: Compile ELMo elmo_model = ELMo(parameters) elmo_model.compile_elmo(print_summary=True)  #: Train ELMo elmo_model.train(train_data=train_generator  valid_data=val_generator)  #: Persist ELMo Bidirectional Language Model in disk elmo_model.save(sampled_softmax=False)  #: Evaluate Bidirectional Language Model elmo_model.evaluate(test_generator)  #: Build ELMo meta-model to deploy for production and persist in disk elmo_model.wrap_multi_elmo_encoder(print_summary=True  save=True)  ```   """;Natural Language Processing;https://github.com/yangrui123/Hidden
"""    $ cd ./data/WFLW        $ cd ./data/300W        $ cd ./data/300VW     $ cd ./data      $ sh train.sh   tip: please install resnest to use ResNest models.   |    Name         |*   Pytorch -> onnx -> onnx_sim    Make sure pip3 install onnx-simplifier   How to build :https://github.com/Tencent/ncnn/wiki/how-to-build      $ cd ncnn/build/tools/onnx   """;Computer Vision;https://github.com/github-luffy/PFLD_68points_Pytorch
"""bash train.ende.sh   bash train.ende.ft.sh   """;General;https://github.com/ictnlp/TLAT-NMT
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   Install <a href=""https://www.tensorflow.org/get_started/os_setup"" target=""_blank"">TensorFlow</a>. You may also need to install h5py. The code has been tested with Python 2.7  TensorFlow 1.0.1  CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04.  If you are using PyTorch  you can find a third-party pytorch implementation <a href=""https://github.com/fxia22/pointnet.pytorch"" target=""_blank"">here</a>.  To install h5py for Python: ```bash sudo apt-get install libhdf5-dev sudo pip install h5py ```   Created by <a href=""http://charlesrqi.com"" target=""_blank"">Charles R. Qi</a>  <a href=""http://ai.stanford.edu/~haosu/"" target=""_blank"">Hao Su</a>  <a href=""http://cs.stanford.edu/~kaichun/"" target=""_blank"">Kaichun Mo</a>  <a href=""http://geometry.stanford.edu/member/guibas/"" target=""_blank"">Leonidas J. Guibas</a> from Stanford University.  ![prediction example](https://github.com/charlesq34/pointnet/blob/master/doc/teaser.png)   cd part_seg  sh download_data.sh   To train a model to classify point clouds sampled from 3D shapes:      python train.py  Log files and network parameters will be saved to `log` folder in default. Point clouds of <a href=""http://modelnet.cs.princeton.edu/"" target=""_blank"">ModelNet40</a> models in HDF5 files will be automatically downloaded (416MB) to the data folder. Each point cloud contains 2048 points uniformly sampled from a shape surface. Each cloud is zero-mean and normalized into an unit sphere. There are also text files in `data/modelnet40_ply_hdf5_2048` specifying the ids of shapes in h5 files.  To see HELP for the training script:      python train.py -h  We can use TensorBoard to view the network architecture and monitor the training progress.      tensorboard --logdir log  After the above training  we can evaluate the model and output some visualizations of the error cases.      python evaluate.py --visu  Point clouds that are wrongly classified will be saved to `dump` folder in default. We visualize the point cloud by rendering it into three-view images.  If you'd like to prepare your own data  you can refer to some helper functions in `utils/data_prep_util.py` for saving and loading HDF5 files.   * <a href=""http://stanford.edu/~rqi/pointnet2/"" target=""_blank"">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a> by Qi et al. (NIPS 2017) A hierarchical feature learning framework on point clouds. The PointNet++ architecture applies PointNet recursively on a nested partitioning of the input point set. It also proposes novel layers for point clouds with non-uniform densities. * <a href=""http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w13/Engelmann_Exploring_Spatial_Context_ICCV_2017_paper.pdf"" target=""_blank"">Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</a> by Engelmann et al. (ICCV 2017 workshop). This work extends PointNet for large-scale scene segmentation. * <a href=""https://arxiv.org/abs/1710.04954"" target=""_blank"">PCPNET: Learning Local Shape Properties from Raw Point Clouds</a> by Guerrero et al. (arXiv). The work adapts PointNet for local geometric properties (e.g. normal and curvature) estimation in noisy point clouds. * <a href=""https://arxiv.org/abs/1711.06396"" target=""_blank"">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a> by Zhou et al. from Apple (arXiv) This work studies 3D object detection using LiDAR point clouds. It splits space into voxels  use PointNet to learn local voxel features and then use 3D CNN for region proposal  object classification and 3D bounding box estimation. * <a href=""https://arxiv.org/abs/1711.08488"" target=""_blank"">Frustum PointNets for 3D Object Detection from RGB-D Data</a> by Qi et al. (arXiv) A novel framework for 3D object detection with RGB-D data. The method proposed has achieved first place on KITTI 3D object detection benchmark on all categories (last checked on 11/30/2017).  """;Computer Vision;https://github.com/Yang2446/pointnet
"""```bash #: To train the model: sh train.sh #: To evaluate the model: (1)please first download the val data in https://github.com/ZhaoJ9014/face.evoLVe.PyTorch. (2)set the checkpoint dir in config.py sh evaluate.sh ``` You can change the experimental setting by simply modifying the parameter in the config.py   """;Computer Vision;https://github.com/HuangYG123/CurricularFace
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/mithunpaul08/bert_tensorflow
"""https://pjreddie.com/darknet/yolo/ \   """;Computer Vision;https://github.com/chihyanghsu0805/object_detection_yolo
"""Dependencies: TensorFlow  PrettyTensor  numpy  matplotlib   """;General;https://github.com/tdrussell/IllustrationGAN
"""<b> Installation: </b>  One can easily install GANify using the PIP:  <i>pip install ganify==1.0.10</i>   <img width=""600"" height=""500"" src=""https://github.com/arnonbruno/ganify/blob/master/ganify.gif"">   """;Computer Vision;https://github.com/arnonbruno/ganify
"""A dataset for book recommendations: ten thousand books  one million ratings   *  [UFLDL Tutorial 1](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial) *  [UFLDL Tutorial 2](http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/) *  [Deep Learning for NLP (without Magic)](http://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial) *  [A Deep Learning Tutorial: From Perceptrons to Deep Networks](http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks) *  [Deep Learning from the Bottom up](http://www.metacademy.org/roadmaps/rgrosse/deep_learning) *  [Theano Tutorial](http://deeplearning.net/tutorial/deeplearning.pdf) *  [TensorFlow tutorials](https://github.com/nlintz/TensorFlow-Tutorials)  More details in [tutorials](tutorials.md)   """;Computer Vision;https://github.com/dineshzende/awesome-deep-learning-resources
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/deeplearnproject/darknet
"""Modify the `data_dir` in the json file to a directory which has a bunch of wave files with the same sampling rate   then your are good to go. The mel-spectrogram will be computed on the fly.  ```json {   ""data_loader"": {     ""type"": ""RandomWaveFileLoader""      ""args"": {       ""data_dir"": ""/your/data/wave/files""        ""batch_size"": 8        ""num_workers"": 2        ""segment"": 16000     }   } } ```  ``` python train.py -c config.json ```   """;Audio;https://github.com/yoyololicon/constant-memory-waveglow
"""Keras 2.2.0  Tensorflow 1.8.0  Ubuntu 16.04  Python 3.5   """;Computer Vision;https://github.com/VinGPan/paper_implementations
"""Simply run:  `python train_agent.py`  for default args. Changeable args are: ``` --env: String of environment name (Default: HalfCheetah-v2) --alg: String of policy optimizer (Default: td3; Choices: {td3  sac  tds}) --yaml_config: String of YAML config file for either TD3  SAC or TDS (Default: None) --seed: Int of seed (Default: 100) --use_obs_filter: Boolean that is true when used (seems to degrade performance  Default: False) --update_every_n_steps: Int of how many env steps we take before optimizing the agent (Default: 1  ratio of steps v.s. backprop is tied to 1:1) --n_random_actions: Int of how many random steps we take to 'seed' the replay pool (Default: 10000) --n_collect_steps: Int of how steps we collect before training  (Default: 1000) --n_evals: Int of how many episodes we run an evaluation for (Default: 1) --checkpoint_interval: Int of how often to checkpoint model (i.e.  saving  making gifs) --save_model: Boolean that is true when used  saving the model parameters --make_gif: Boolean that is true when used; makes --save_replay_pool: Boolean that saves the replay pool along with the agent parameters (defaults to False  as this is very costly memory-wise) --load_model_path: Path to the directory where model .pt files were saved; loads and resumes training from that snapshot ```   """;General;https://github.com/fiorenza2/OffCon3
""":white_check_mark: :tada: :tada::tada:  https://github.com/NVIDIA/trt-samples-for-hackathon-cn/blob/master/hackathon/TRT-Hackathon-2021-summary.md :tada::tada::tada:   在Docker镜像内需要安装相应的Python库，可以在项目下执行pip3 install -r requirements.txt -i  http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com进行安装.   CUDA 11.2， cuDNN-8.1.  系统信息为：Linux version 4.15.0-139-generic (buildd@lgw01-amd64-035) (gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)) #143-Ubuntu SMP Tue Mar 16 01:30:17 UTC 2021.   ├── checkpoint  #: DETR Pytorch 模型，开源代码该部分仅提供模型下载链接   ├── requirements.txt   #: Python package list   : pytorch to onnx   DETR参考官方REPO:  https://github.com/facebookresearch/detr   cd cpp   -- Found OpenCV: /workspace/opencv-4.5.2/build (found version ""4.5.2"")    -- Found CUDA: /usr/local/cuda (found version ""11.2"")    """;Computer Vision;https://github.com/DataXujing/TensorRT-DETR
"""source code: https://github.com/AlexeyAB/darknet   useful links: https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&sk=6039748846bbcf1d960c3061542591d7   """;Computer Vision;https://github.com/Veternal1226/YOLO_darknet
"""If you want to modify the functions inside `pyclouds` you can do a development install by typing ``` python setup.py develop ```   To use the GPU with Keras which is strongly recommended if you want to train a neural network (see below)  you have to replace `keras` with `keras-gpu` in the requirements.txt file. Unfortunately it doesn't always work right out of the box depending on your system.  If you only want to use the pretrained models  you can also do this on CPU.   We highly recommend using Anaconda to manage your Python packages.   To start from scratch  create a new conda environment with the required packages by typing  conda create -n my-new-environment --file requirements.txt  python setup.py install  If you already have a conda environment and simply want to make sure you have all the necessary packages installed for this repository  do:  conda install --file requirements.txt  python setup.py install   In addition to the packages in the requirements.txt file you will need to install the following:   pip install keras-resnet  git clone git@github.com:fizyr/keras-retinanet.git  cd keras-retinanet  python setup.py install   WARNING: fastai requires Python 3.7. I used a different conda environment for the fastai experiments. Follow the instructions on the fastai documentation.   """;General;https://github.com/raspstephan/sugar-flower-fish-or-gravel
"""FoveaBox is an accurate  flexible and completely anchor-free object detection system for object detection framework  as presented in our paper [https://arxiv.org/abs/1904.03797](https://arxiv.org/abs/1904.03797): Different from previous anchor-based methods  FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility  and (b) producing category-agnostic bounding box for each position that potentially contains an object.  <div align=""center"">   <img src=""demo/foveabox.jpg"" width=""300px"" />   <p>FoveaBox detection process.</p> </div>    This FoveaBox implementation is based on [mmdetection](https://github.com/open-mmlab/mmdetection). Therefore the installation is the same as original mmdetection.  Please check [INSTALL.md](INSTALL.md) for installation instructions.    ./tools/dist_train.sh ${CONFIG_FILE} ${GPU_NUM} [optional arguments]   """;Computer Vision;https://github.com/taokong/FoveaBox
""" Once you've finished training your model using affinelayer's **pixpix.py** script  you can use the **export** mode to export only the generator to a new folder:  ``` python pix2pix.py --mode export --output_dir exported_model --input_dir your_model --checkpoint your_model ```  To create a .pict file and quantize your model  use the **export-checkpoint.py** script  ``` python tools/export-checkpoint.py --checkpoint exported_model --output_file model.pict ```  The default setting for number of filters in the first convolutional layer of your generator (--ngf) and your discriminator (--ndf) is set to 64. When using these settings  the quantized model will be around 50 MB in size and require significant processing power.  For simple use in javascript I recommend setting -ngf to 32 or 16 before training  which will result in a final model size of around 13 MB and 3 MB. This will significantly increase the generator's speed  but also reduce the quality of the generated image.   Original pix2pix paper: https://arxiv.org/abs/1611.07004  """;Computer Vision;https://github.com/AlliBalliBaba/PixFace
"""download [wall.alphacoders.com](https://wall.alphacoders.com) thumb image      cd bw2color     python tools/wallpaper.py  crop downloaded image to 256x256 and convert to grey  ```bash cd bw2color #: basic use python tools/preprocess.py #: set data dir python tools/preprocess.py --data [data dir] --save  [output dir] #: set process method python tools/preprocess.py --method gray python tools/preprocess.py --method sketch --mod_path [mod file path] ```  generate tfrecord      python generateds.py      train      python backward.py  test      python test.py   """;Computer Vision;https://github.com/Montia/bw2color
"""- Dont sample from a Uniform distribution  ![cube](images/cube.png ""Cube"")  - Sample from a gaussian distribution  ![sphere](images/sphere.png ""Sphere"")  - When doing interpolations  do the interpolation via a great circle  rather than a straight line from point A to point B - Tom White's [Sampling Generative Networks](https://arxiv.org/abs/1609.04468) ref code https://github.com/dribnet/plat has more details    - Label Smoothing  i.e. if you have two target labels: Real=1 and Fake=0  then for each incoming sample  if it is real  then replace the label with a random number between 0.7 and 1.2  and if it is a fake sample  replace it with 0.0 and 0.3 (for example).   - Salimans et. al. 2016 - make the labels the noisy for the discriminator: occasionally flip the labels when training the discriminator   - Experience Replay   - Keep a replay buffer of past generations and occassionally show them   - Keep checkpoints from the past of G and D and occassionaly swap them out for a few iterations - All stability tricks that work for deep deterministic policy gradients - See Pfau & Vinyals (2016)   - optim.Adam rules!   - See Radford et. al. 2015 - Use SGD for discriminator and ADAM for generator   - if you have labels available  training the discriminator to also classify the samples: auxillary GANs   - Provide noise in the form of dropout (50%). - Apply on several layers of our generator at both training and test time - https://arxiv.org/pdf/1611.07004v1.pdf    """;Computer Vision;https://github.com/linxi159/Tips-and-tricks-to-train-GANs
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/tjwei/stylegan2_workshop
"""python 3  pytorch 3.6   """;Computer Vision;https://github.com/ssumin6/real_nvp
"""How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN >= v7  CUDA >= 7.5   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio 2017 Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv #:replace with opencv[cuda] in case you want to use cuda-accelerated openCV   Build with the Powershell script build.ps1 or use the ""Open Folder"" functionality of Visual Studio 2017. In the first option  if you want to use Visual Studio  you will find a custom solution created for you by CMake after the build containing all the appropriate config flags for your system.  If you have MSVS 2015  CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 0 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox:      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/yytang2012/darknet
"""This FCOS implementation is based on [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark). Therefore the installation is the same as original maskrcnn-benchmark.  Please check [INSTALL.md](INSTALL.md) for installation instructions. You may also want to see the original [README.md](MASKRCNN_README.md) of maskrcnn-benchmark.   For users who only want to use FCOS as an object detector in their projects  they can install it by pip. To do so  run: ``` pip install torch  #: install pytorch if you do not have it pip install git+https://github.com/tianzhi0549/FCOS.git #: run this command line for a demo  fcos https://github.com/tianzhi0549/FCOS/raw/master/demo/images/COCO_val2014_000000000885.jpg ``` Please check out [here](fcos/bin/fcos) for the interface usage.   For your convenience  we provide the following trained models (more models are coming soon).   We update batch normalization for MobileNet based models. If you want to use SyncBN  please install pytorch 1.1 or later.   Note that:   Once the installation is done  you can follow the below steps to run a quick demo.           """;Computer Vision;https://github.com/Adelaide-AI-Group/FCOS
"""The foundational model that we use is Inception-V3 from Keras' pretrained models. However  we cut it off until 'mixed7' layer  and then add our own layers.  Read more about this model at: - https://keras.io/api/applications/inceptionv3/ - https://arxiv.org/abs/1512.00567     There are only 3 important files in this repository. - `modeling.ipynb` is a jupyter notebook which can be run on Google Colab (with GPU for faster training). It contains step-by-step on how to create the image classifier and export the model.  - `model_inception_weights.h5` is the trained weights of our deep learning model's layers. This is used to load the model in our web app. - `apps.py` is the python file to deploy our web app in Streamlit.     ├── setup.sh  ├── requirements.txt   To run the project locally  you can download this repo and type   ``` streamlit run apps.py ```  To view the project as a deployed online web app hosted by Heroku  you can check out with [this link](https://rps-myarist.herokuapp.com/)  ![heroku gif](Images/heroku.gif)     """;Computer Vision;https://github.com/myarist/Rock-Paper-Scissors
"""You can run cifar10_train.py and see how it works from the screen output (the code will download the data for you if you don't have it yet). It’s better to speicify version identifier before running  since the training logs  checkpoints  and error.csv file will be saved in the folder with name logs_$version. You can do this by command line: `python cifar10_train.py --version='test'`. You may also change the version number inside the hyper_parameters.py file  The training and validation error will be output on the screen. They can also be viewed using tensorboard. Use `tensorboard --logdir='logs_$version'` command to pull them out. (For e.g. If the version is ‘test’  the logdir should be ‘logs_test’.)  The relevant statistics of each layer can be found on tensorboard.     Run the following commands in the command line:   """;Computer Vision;https://github.com/wenxinxu/resnet-in-tensorflow
"""refrence that i followed: https://machinelearningmastery.com/cyclegan-tutorial-with-keras/   CycleGAN github: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix   if you are egyptian or someone who can't open medium website you can use this extension:   drive: https://drive.google.com/drive/folders/1jNj-ao5Ybb5HxuKZ3ZFKmmn04Sx2aOsv?usp=sharing   """;General;https://github.com/NDCHIRO/Cartoonizer
"""This repository contains solution of NER task based on PyTorch [reimplementation](https://github.com/huggingface/pytorch-pretrained-BERT) of [Google's TensorFlow repository for the BERT model](https://github.com/google-research/bert) that was released together with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin  Ming-Wei Chang  Kenton Lee and Kristina Toutanova.  This implementation can load any pre-trained TensorFlow checkpoint for BERT (in particular [Google's pre-trained models](https://github.com/google-research/bert)).  Old version is in ""old"" branch.   from modules.models.bert_models import BERTBiLSTMAttnCRF   from modules.train.train import NerLearner   from modules.analyze_utils.plot_metrics import get_bert_span_report  from modules.analyze_utils.main_metrics import precision_recall_f1   """;Natural Language Processing;https://github.com/sberbank-ai/ner-bert
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M and VGG2 datasets  which were already packed in the MxNet binary format. The network backbones include ResNet  InceptionResNet_v2  DenseNet  DPN and MobiletNet. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss. * loss-type=0:  Softmax * loss-type=1:  SphereFace * loss-type=2:  CosineFace * loss-type=4:  ArcFace * loss-type=5:  Combined Margin * loss-type=12: TripletLoss  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu80   git clone --recursive https://github.com/deepinsight/insightface.git   """;General;https://github.com/eric-erki/insightface
"""Do we need new vocab or can we just use one   """;General;https://github.com/iejMac/ScriptWriter
"""PyTorch implementation https://github.com/ducha-aiki/LSUV-pytorch   """;General;https://github.com/ducha-aiki/LSUV-keras
"""In this research project  to solve real world problems with machine learning  I noted that there is a limit to the traditional Deep Learning application  which is highly dependent on existing datasets because it is still difficult to obtain enough labled data.  The basis for judgment must be clear in the biomedical field  so I decided to use image data among various types for the reason of being visualized intuitively.  Using just one labeled image data for training  I wanted to categorize a lot of unseen data based on it by the basic concept of one shot learning through reinforcement learning.  In this project  I redefined the one shot image segmentation problem as a reinforcement learning and solved it using PPO. I found that there was actually a dramatic performance.  <p align=""center""> <img src=""oneshotgo/data/res/un.png"" width=70%/> </p>   ``` git clone https://github.com/decoderkurt/research_project_school_of_ai_2019.git cd research_project_school_of_ai_2019 pip install -e . ```  python -m baselines.run --alg=ppo2 --env=OneShotGo-v0 --save_path=""YourOwnOneShotGo10M""  python -m baselines.run --alg=ppo2 --env=OneShotGo-v0 --load_path=""OneShotGo10M""   """;Reinforcement Learning;https://github.com/decoderkurt/research_project_school_of_ai_2019
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla T4'  total_memory=15079MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/Lmonster/darknet
"""![data-example](images/data-example.png)  Dataset consists of collected from public available chest X-Ray (CXR) images. Overall amount of images is 800 meanwhile labeled only 704 of them. Whole dataset was randomly divided into train (0.8 of total) validation (0.1 splited from train) and test parts. Splits were saved into ```splits.pk```.  The main task is to implement pixel-wise segmentation on the available data to detect lung area. Download link on the dataset https://drive.google.com/file/d/1ffbbyoPf-I3Y0iGbBahXpWqYdGd7xxQQ/view.   """;Computer Vision;https://github.com/IlliaOvcharenko/lung-segmentation
"""You can install this package from PyPI:  ```sh pip install gmlp-flax ```  Or directly from GitHub:  ```sh pip install --upgrade git+https://github.com/SauravMaheshkar/gMLP.git ```   conda env create --name &lt;env-name&gt; sauravmaheshkar/gmlp  conda activate &lt;env-name&gt;   ```python import jax from gmlp_flax import gMLP  random_key = jax.random.PRNGKey(0)  x = jax.random.randint(key=random_key  minval=0  maxval=20000  shape=(1  1000))  init_rngs = {""params"": random_key}  gMLP(num_tokens=20000  dim=512  depth=4).init(init_rngs x) ```   """;Computer Vision;https://github.com/SauravMaheshkar/gMLP
"""git clone & change DIR  $ git clone https://github.com/J911/WRN-pytorch  $ cd WRN-pytorch   """;Computer Vision;https://github.com/J911/WRN-pytorch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/coco60/bert-test
"""Drive link to dataset : https://drive.google.com/open?id=1Hr1OuftLZ0reDac1yJ2wAspwE9gnUQEi  bg : Forest  park  seashore and road. Total images = 100; Channels = RGB; Size = 905KB; Resolution = 224X224  fg : Men  Women  Children and combination of men-women  women-children and men-children. Total images = 100; Channels = RGB; Size = 576KB; Resolution = 160X160. Gimp is used to remove the background in foreground images(made transparent). Understood difference between white bg and transparent bg.  fg_bg : Randomly placed each fg 40 times(with flips) over each bg. Total images = [100X100X(20X2)] 400K. Channels = RGB; Size = 2.2GB; Resolution = 224X224  fg_bg_mask : fg is converted from RGB to black and overlaid on top of black background. This is done along with step 3 (in the same for loop). Total images = 400K. Size = 1.6GB; Resolution = 224X224  fg_bg_depth : Tweaks with respect to image input folder and save have been made from the shared Dense Depth code. Image loading is done on CPU while prediction is done on GPU. Need to load the data as well in GPU for fast processing. 2000 images takes 15 minutes hence working on optimizations. Could have done this in the same for loop along with steps 3 and 4.  <img src = ""Data_Samples_Depth_Model.png"">  Link to codes :  Overlap and mask : https://github.com/aimbsg/EVA4_S14/blob/master/EVA4_S14_Overlap_And_Mask.ipynb  Dense depth model : https://github.com/aimbsg/EVA4_S14/blob/master/EVA4_S14_Dense_depth_model.ipynb   Link to code : https://github.com/aimbsg/EVA4_S15/blob/master/EVA4_S15_Custom_Dataset_Depth_Prediction.ipynb   """;General;https://github.com/aimbsg/EVA4_S15
"""The model is built in PyTorch 1.1.0 and tested on Ubuntu 16.04 environment (Python3.7  CUDA9.0  cuDNN7.5).  For installing  follow these intructions ``` conda create -n pytorch1 python=3.7 conda activate pytorch1 conda install pytorch=1.1 torchvision=0.3 cudatoolkit=9.0 -c pytorch pip install matplotlib scikit-image opencv-python yacs joblib natsort h5py tqdm ```  Install warmup scheduler  ``` cd pytorch-gradual-warmup-lr; python setup.py install; cd .. ```   """;Computer Vision;https://github.com/swz30/MPRNet
"""Inspired by the structure of Receptive Fields (RFs) in human visual systems  we propose a novel RF Block (RFB) module  which takes the relationship between the size and eccentricity of RFs into account  to enhance the discriminability and robustness of features. We further  assemble the RFB module to the top of SSD with a lightweight CNN model  constructing the RFB Net detector. You can use the code to train/evaluate the RFB Net for object detection. For more details  please refer to our [arXiv paper](https://arxiv.org/pdf/1711.07767.pdf).   <img align=""right"" src=""https://github.com/ruinmessi/RFBNet/blob/master/doc/rfb.png"">  &nbsp; &nbsp;   - Install [PyTorch-0.2.0](http://pytorch.org/) by selecting your environment on the website and running the appropriate command. - Clone this repository. This repository is mainly based on [ssd.pytorch](https://github.com/amdegroot/ssd.pytorch) and [Chainer-ssd](https://github.com/Hakuyume/chainer-ssd)  a huge thank to them.   * Note: We currently only support Python 3+. - Compile the nms and coco tools: ```Shell ./make.sh ``` *Note*: Check you GPU architecture support in utils/build.py  line 131. Default is: ```  'nvcc': ['-arch=sm_52'  ``` - Install [pyinn](https://github.com/szagoruyko/pyinn) for MobileNet backbone: ```Shell pip install git+https://github.com/szagoruyko/pyinn.git@master ``` - Then download the dataset by following the [instructions](#download-voc2007-trainval--test) below and install opencv.  ```Shell conda install opencv ``` Note: For training  we currently  support [VOC](http://host.robots.ox.ac.uk/pascal/VOC/) and [COCO](http://mscoco.org/).    Install the MS COCO dataset at /path/to/coco from official website  default is ~/data/COCO. Following the instructions to prepare minival2014 and valminusminival2014 annotations. All label files (.json) should be under the COCO/annotations/ folder. It should have this basic structure   First download the fc-reduced VGG-16 PyTorch base network weights at:    https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth   mkdir weights  cd weights   -v: choose backbone version  RFB_VGG  RFB_E_VGG or RFB_mobile.   """;General;https://github.com/ZTao-z/multiflow-resnet-ssd
"""Manual: https://github.com/AlexeyAB/darknet/wiki   Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example   PyTorch > ONNX:    TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like CUDA  cudnn  ZED and build against those. It will also create a shared object library file to use darknet for code development.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows.  Install Visual Studio 2017 or 2019. In case you need to download it  please go here: Visual Studio Community  Install CUDA (at least v10.0) enabling VS Integration during installation.   PS Code\&gt;              git clone https://github.com/microsoft/vcpkg  PS Code\&gt;              cd vcpkg   PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base cuda cudnn]:x64-windows for a quicker install of dependencies  PS Code\vcpkg&gt;         cd ..  PS Code\&gt;              git clone https://github.com/AlexeyAB/darknet  PS Code\&gt;              cd darknet   Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   in Python: https://github.com/tzutalin/labelImg  in Python: https://github.com/Cartucho/OpenLabeling   in JavaScript: https://github.com/opencv/cvat   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux   * using `build.sh` or   * build `darknet` using `cmake` or   * set `LIBSO=1` in the `Makefile` and do `make` * on Windows   * using `build.ps1` or   * build `darknet` using `cmake` or   * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs:  * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h   * Python examples using the C API:     * https://github.com/AlexeyAB/darknet/blob/master/darknet.py     * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py  * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp   * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp  ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`      * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)  `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42)  ```cpp struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false);         std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/Demonhesusheng/darknet_v2
"""conda install -c conda-forge wandb   """;General;https://github.com/somepago/saint
"""OS version: Ubuntu 18.04  NVIDIA diver version: 465.27  Cuda version: 11.3  Cudnn version:   Python version: 3.6.9  Python packages installation:   pip3 install -i https://mirrors.aliyun.com/pypi/simple  -r requirements.txt  sh run/train_example.sh  sh run/test.sh   pytorch-image-models: https://github.com/rwightman/pytorch-image-models  Vit-Pytorch: https://github.com/lucidrains/vit-pytorch    DeiT: https://github.com/facebookresearch/deit  Swin-Transformer: https://github.com/microsoft/Swin-Transformer   """;General;https://github.com/holdfire/CLS
"""To speed up data loading for medical images which are typically 3D in Nifti/Nrrd/mhd format  h5 files are recommended for PyTorch dataloader. After train/test split  merge all 2D slices of subjects from each set to form a [N w h 1] array  where N is the number of total slices  (w h) is the image size. Save arrays from source and target domains as a group 'src' (for source domain) and 'trg' (for target domain) in the h5 file.    python mains/train.py --name 'seg_renorm_cyclegan'\   """;General;https://github.com/mengweiren/segmentation-renormalized-harmonization
"""Clone the Github repo:  ``` git clone https://github.com/travismyers19/Vulnerable_Buildings ```  Change current directory to the project directory:  ``` cd Vulnerable_Buildings ```  All command line commands below assume that the user is in the project directory unless otherwise specified.   Activate the tensorflow 2.0 with Python 3.6 environment:  source activate tensorflow2_p36  Install Streamlit:  pip install streamlit  Create a conda environment from ""configs/environment.yml:  conda env create -f Configs/environment.yml  Activate the conda environment:  source activate tensorflow2_p36  Install from requirements.txt:  pip install -r requirements.txt   Set the following variables within the train.sh bash script:   """;Computer Vision;https://github.com/travismyers19/Vulnerable_Buildings
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/Caesarzhang/bert-zh
"""cd Handwritten-Character-Recognition<br/>   cd Handwritten-Character-Recognition<br/>   """;Computer Vision;https://github.com/Shantanu48114860/Handwritten-Character-Recognition
"""3. Run FaceSwap_GAN_v2.2_train_test.ipynb to train  models.   """;General;https://github.com/shaoanlu/faceswap-GAN
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla T4'  total_memory=15079MB)   Using CUDA device0 _CudaDeviceProperties(name='Tesla T4'  total_memory=15079MB)   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/yrouphail/yolov3
"""You have to download the pretrained models and put them in output/ckpt/LJSpeech/ or output/ckpt/AISHELL3.   Alternately  you can align the corpus by yourself.    """;Audio;https://github.com/zhangbo2008/fastSpeeck2_chinese_train
"""For setup  a Makefile is provided:  ```shell script make install ```  Or  you can manually run:  ```shell script #: Install System Requirements python3 -m pip install -r requirements.txt ```  In either case  you should delete the `.cloud` directory. Either the Makefile will do it for you or  you can manually delete it. It contains operations that I use personally when working with the Google API  so unless you are working with any of the same Google APIs  you should delete it.   To use the repository  it can be directly cloned from the command line:  ```shell script git clone --recurse-submodules https://github.com/amogh7joshi/engagement-detection.git ```   Then  use the scripts provided in the scripts directory to install the necessary data:  1. To install the model and caffemodel files for the DNN  use the getdata.sh script.    Once the datasets are preprocessed  they can be called through the following functions:  ```python from data.load_data import get_fer2013_data from data.load_data import get_ckplus_data  #: Load the training  validation  and testing data (repeat with other datasets). X_train  X_validation  X_test  y_train  y_validation  y_test = get_fer2013_data() ```  For more information  visit the `data` subdirectory.  The other tools in the Makefile are for convenience purposes only when committing to this repository   in addition to the `editconstant.sh` script. Do not use them unless you are committing to your own repository.  The `info.json` file contains the relevant locations of the cascade classifiers and DNN model files. You can replace the current locations with those on your computer  and then load the detectors as follows.  ```python from util.info import load_info  #: Set the `eyes` option to true if you want to load the eye cascade. cascade_face  cascade_eyes  net = load_info(eyes = True) ```   ![GitHub Workflow Status](https://img.shields.io/github/workflow/status/amogh7joshi/chemsolve/CodeQL)  Currently  all models have been configured to work with the `fer2013` and `ck+` datasets.  **Model Training**: Run the `trainmodel.py` script. You can edit the number of epochs in the argparse argument at the top of the file. Alternatively  you can run itt from the command line using the flags as mentioned by the  argparse arguments. Model weights will be saved to the `data/model` directory  and at the completion of the training  the best model will be moved into the `data/savedmodels` directory. The json file containing the model architecture will also be saved there. You can control what models to keep in the `data/savedmodels` directory manually.  **Model Testing**: Run the `testmodel.py` script. You can edit which model weights and architecture you want to use at the  location at the top of the file. From there  you can run `model.evaluate` on the pre-loaded training and testing data   you can run `model.predict` on any custom images you want to test  or run any other operations with the model.  A confusion matrix is also present  which will display if `plt.show()` is uncommented.  **Live Emotion Detection**: Run the `videoclassification.py` script. If you already have a trained model  set it at the top of the  script  and it will detect emotions live. For just facial detection  run the `facevideo.py` script. You can choose which detector you want to use  as described at the top of the file. If you want to save images  set the `-s` flag to `True`  and they will save to a  custom directory `imageruntest` at the top-level. More information is included at the top of the file.   **Image Emotion Detection**: Run the `emotionclassification.py` script. Choose the images you want to detect emotions on and place their paths in  the `userimages` variable. If running from the command line  then write out the paths to each of the images when running the script. Optionally  if you just want facial detection   run the `facedetect.py` script. If running from the command line  then read the argument information at the top of the file.  Otherwise  insert the paths of the images that you want to detect faces from into a list called `user_images` midway through the file. The changed images will save to a custom directory called `modded`  but you can change that from the `savedir` variable. For each image inputted  the script will output the same image with a bounding box around the faces detected from the image.   """;General;https://github.com/amogh7joshi/engagement-detection
"""Train a WideResNet-16-1 on CIFAR10:  ``` python train.py --config configs/WRN-16-1-scratch-CIFAR10.json ```  Train a WideResNet-40-2 on SVHN:  ``` python train.py --config configs/WRN-40-2-scratch-SVHN.json ```   """;Computer Vision;https://github.com/AlexandrosFerles/Wide-Residual-Networks-PyTorch
"""For Python3 users  you need to replace pip with pip3:   sudo apt-get install python3-pyqt4  * OpenCV3 with Python3: see the installation instruction.   bash ./models/scripts/download_dcgan_model.sh outdoor_64   bash models/scripts/download_alexnet.sh conv4   * Install the python libraries. (See [Requirements](https://github.com/junyanz/iGAN#requirements)). * Download the code from GitHub: ```bash git clone https://github.com/junyanz/iGAN cd iGAN ``` * Download the model. (See `Model Zoo` for details): ``` bash bash ./models/scripts/download_dcgan_model.sh outdoor_64 ```  * Run the python script: ``` bash THEANO_FLAGS='device=gpu0  floatX=float32  nvcc.fastmath=True' python iGAN_main.py --model_name outdoor_64 ```   """;Computer Vision;https://github.com/junyanz/iGAN
"""A development version based on FPN.    cd $PATH_ROOT/libs/box_utils/cython_utils   cd $PATH_ROOT/tools                       --GPU='0'  cd $PATH_ROOT/tools                  --GPU='0'   2、make tfrecord   cd $PATH_ROOT/tools   4、multi-gpu train  cd $PATH_ROOT/tools   cd $PATH_ROOT/output/summary   """;Computer Vision;https://github.com/DetectionTeamUCAS/FPN_Tensorflow
"""The easiest way to use this module is to install it with the `vl_contrib`  package manager:  ``` vl_contrib('install'  'mcnSENets') ; vl_contrib('setup'  'mcnSENets') ; vl_contrib('test'  'mcnSENets') ; % optional ```   The ordering of the imagenet labels differs from the standard ordering commonly found in caffe  pytorch etc.  These are remapped automically in the evaluation code.  The mapping between the synsets indices can be found [here](misc/label_map.txt).  """;Computer Vision;https://github.com/albanie/mcnSENets
"""The code was developed on Python 3.6.4  with pytorch 0.4.0 (CUDA V8.0  CuDNN 6.0) and all experiments were run on a GeForce GTX 1080 core.   """;General;https://github.com/tdmeeste/SparseSeqModels
"""The pickled data is a dictionary with 4 key/value pairs:  - `'features'` is a 4D array containing raw pixel data of the traffic sign images  (num examples  width  height  channels). - `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id. - `'sizes'` is a list containing tuples  (width  height) representing the original width and height the image. - `'coords'` is a list containing tuples  (x1  y1  x2  y2) representing coordinates of a bounding box around the sign in the image.  **First  we will use `numpy` provide the number of images in each subset  in addition to the image size  and the number of unique classes.** Number of training examples:  34799 Number of testing examples:  12630 Number of validation examples:  4410 Image data shape = (32  32  3) Number of classes = 43  **Then  we used `matplotlib` plot sample images from each subset.**   <figure>  <img src=""./traffic-signs-data/Screenshots/Train.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>   <figure>  <img src=""./traffic-signs-data/Screenshots/Test.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  <figure>  <img src=""./traffic-signs-data/Screenshots/Valid.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>   **And finally  we will use `numpy` to plot a histogram of the count of images in each unique class.**   <figure>  <img src=""./traffic-signs-data/Screenshots/TrainHist.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  <figure>  <img src=""./traffic-signs-data/Screenshots/TestHist.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  <figure>  <img src=""./traffic-signs-data/Screenshots/ValidHist.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  ---   Now  we'll use the testing set to measure the accuracy of the model over unknown examples. We've been able to reach a **Test accuracy of 97.6%**. A remarkable performance.  Now we'll plot the confusion matrix to see where the model actually fails.  <figure>  <img src=""./traffic-signs-data/Screenshots/cm.png"" width=""1072"" alt=""Combined Image"" />  <figcaption>  <p></p>   </figcaption> </figure>  We observe some clusters in the confusion matrix above. It turns out that the various speed limits are sometimes misclassified among themselves. Similarly  traffic signs with traingular shape are misclassified among themselves. We can further improve on the model using hierarchical CNNs to first identify broader groups (like speed signs) and then have CNNs to classify finer features (such as the actual speed limit).  ---   Python 3.6.2  TensorFlow 0.12.1 (GPU support)   """;Computer Vision;https://github.com/mohamedameen93/German-Traffic-Sign-Classification-Using-TensorFlow
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.  git clone https://github.com/affinelayer/pix2pix-tensorflow.git  cd pix2pix-tensorflow   ```sh #: clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git cd pix2pix-tensorflow #: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir facades_train \   --max_epochs 200 \   --input_dir facades/train \   --which_direction BtoA #: test the model python pix2pix.py \   --mode test \   --output_dir facades_test \   --input_dir facades/val \   --checkpoint facades_train ```  The test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.  If you have Docker installed  you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:  ```sh #: train the model python tools/dockrun.py python pix2pix.py \       --mode train \       --output_dir facades_train \       --max_epochs 200 \       --input_dir facades/train \       --which_direction BtoA #: test the model python tools/dockrun.py python pix2pix.py \       --mode test \       --output_dir facades_test \       --input_dir facades/val \       --checkpoint facades_train ```   <img src=""docs/combine.png"" width=""900px""/>  ```sh #: Resize source images python tools/process.py \   --input_dir photos/original \   --operation resize \   --output_dir photos/resized #: Create images with blank centers python tools/process.py \   --input_dir photos/resized \   --operation blank \   --output_dir photos/blank #: Combine resized images with blanked images python tools/process.py \   --input_dir photos/resized \   --b_dir photos/blank \   --operation combine \   --output_dir photos/combined #: Split into train/val set python tools/split.py \   --dir photos/combined ```  The folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.   """;General;https://github.com/bxck75/piss-ant-pix2pix
"""1. [requirements.txt](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/requirements.txt) - Includes all the required libraries for the Conda Environment. 2. [model.py](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/model.py) - Defines the QNetwork which is the nonlinear function approximator to calculate the value actions based directly on observation from the environment. 3. [dqn_agent.py](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/dqn_agent.py) -  Defines the Agent that uses Deep Learning to find the optimal parameters for the function approximators  determines the best action to take and maximizes the overall or total reward. 4. [Navigation.ipynb](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/Navigation.ipynb) - The main file that trains the Deep Q-Network and shows the trained agent in action. This file can be run in the Conda environment.       - I adopted Double Deep Q-Network structure<sup>1  2</sup> with three fully connected layers. If a single network is used  the Q-functions values change at each step of training  and then the value estimates can quickly spiral out of control. I used a target network to represent the old Q-function  which is used to compute the loss of every action during training.   The environment was solved in 463 episodes  with the average reward score of 13 to indicate solving the environment.   1. Create the Conda Environment      a. Install [`miniconda`](http://conda.pydata.org/miniconda.html) on your computer  by selecting the latest Python version for your operating system. If you already have `conda` or `miniconda` installed  you should be able to skip this step and move on to step b.      **Download** the latest version of `miniconda` that matches your system.      |        | Linux | Mac | Windows |     |--------|-------|-----|---------|     | 64-bit | [64-bit (bash installer)][lin64] | [64-bit (bash installer)][mac64] | [64-bit (exe installer)][win64]     | 32-bit | [32-bit (bash installer)][lin32] |  | [32-bit (exe installer)][win32]      [win64]: https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86_64.exe     [win32]: https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86.exe     [mac64]: https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh     [lin64]: https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh     [lin32]: https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86.sh      **Install** [miniconda](http://conda.pydata.org/miniconda.html) on your machine. Detailed instructions:      - **Linux:** http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install     - **Mac:** http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install     - **Windows:** http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install      b. Install git and clone the repository.      For working with Github from a terminal window  you can download git with the command:     ```     conda install git     ```     To clone the repository  run the following command:     ```     cd PATH_OF_DIRECTORY     git clone https://github.com/iDataist/Navigation-with-Deep-Q-Network     ```     c. Create local environment      - Create (and activate) a new environment  named `dqn-env` with Python 3.7. If prompted to proceed with the install `(Proceed [y]/n)` type y.          - __Linux__ or __Mac__:         ```         conda create -n dqn-env python=3.7         conda activate dqn-env         ```         - __Windows__:         ```         conda create --name dqn-env python=3.7         conda activate dqn-env         ```          At this point your command line should look something like: `(dqn-env) <User>:USER_DIR <user>$`. The `(dqn-env)` indicates that your environment has been activated  and you can proceed with further package installations.      - Install a few required pip packages  which are specified in the requirements text file. Be sure to run the command from the project root directory since the requirements.txt file is there.         ```         pip install -r requirements.txt         ipython3 kernel install --name dqn-env --user         ```     - Open Jupyter Notebook  and open the Navigation.ipynb file.         ```         jupyter notebook         ``` 2. Download the Unity Environment     Download the environment from one of the links below.  You need only select the environment that matches your operating system:     - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip)     - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip)     - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip)     - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)      (_For Windows users_) Check out [this link](https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64) if you need help with determining if your computer is running a 32-bit version or 64-bit version of the Windows operating system.      (_For AWS_) If you'd like to train the agent on AWS (and have not [enabled a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md))  then please use [this link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux_NoVis.zip) to obtain the environment.   """;Reinforcement Learning;https://github.com/iDataist/Navigation-with-Deep-Q-Network
"""$ brew install cmake boost boost-python sdl2 swig wget  $ cd $your_work_directory  $ git clone https://github.com/ageron/tiny-dqn.git  $ cd tiny-dqn  $ pip install --user --upgrade pip  $ pip install --user --upgrade -r requirements.txt   $ apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig  $ cd $your_work_directory  $ git clone https://github.com/ageron/tiny-dqn.git  $ cd tiny-dqn  $ vim requirements.txt  $ pip install --user --upgrade pip  $ pip install --user --upgrade -r requirements.txt   To train the model:      python tiny_dqn.py -v --number-steps 1000000  The model is saved to `my_dqn.ckpt` by default. To view it in action  run:      python tiny_dqn.py --test --render  For more options:      python tiny_dqn.py --help   """;Reinforcement Learning;https://github.com/jonaths/tf-dqn
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/binhdv92/darknet
"""<img src=""https://github.com/goldhuang/SRGAN-PyTorch/blob/master/results/11.png"">   """;General;https://github.com/goldhuang/SRGAN-PyTorch
"""Install dependencies with ```pip install -r requirements.txt```   """;Computer Vision;https://github.com/yifeiwang77/repgan
"""This repository contains a TensorFlow-based implementation of **[4PP-EUSR (""Deep learning-based image super-resolution considering quantitative and perceptual quality"")](http://arxiv.org/abs/1809.04789)**  which considers both the quantitative (e.g.  PSNR) and perceptual quality (e.g.  NIQE) of the upscaled images. Our method **won the 2nd place and got the highest human opinion score for Region 2** in the [2018 PIRM Challenge on Perceptual Image Super-resolution at ECCV 2018](https://arxiv.org/abs/1809.07517).  ![BSD100 - 37073](figures/bsd100_37073.png) ※ The perceptual index is calculated by ""0.5 * ((10 - [Ma](https://sites.google.com/site/chaoma99/sr-metric)) + [NIQE](https://doi.org/10.1109/LSP.2012.2227726))""  which is used in the [PIRM Challenge](https://www.pirm2018.org/PIRM-SR.html). Lower is better.  Followings are the performance comparison evaluated on the [BSD100](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/) dataset.  ![BSD100 PSNR vs. NIQE](figures/bsd100_psnr_niqe.png)  Method | PSNR (dB) (↓) | Perceptual Index ------------ | :---: | :---: [EDSR](https://github.com/thstkdgus35/EDSR-PyTorch) | 27.796 | 5.326 [MDSR](https://github.com/thstkdgus35/EDSR-PyTorch) | 27.771 | 5.424 [EUSR](https://github.com/ghgh3269/EUSR-Tensorflow) | 27.674 | 5.307 [SRResNet-MSE](https://arxiv.org/abs/1609.04802) | 27.601 | 5.217 **4PP-EUSR (PIRM Challenge)** | **26.569** | **2.683** [SRResNet-VGG22](https://arxiv.org/abs/1609.04802) | 26.322 | 5.183 [SRGAN-MSE](https://arxiv.org/abs/1609.04802) | 25.981 | 2.802 Bicubic interpolation | 25.957 | 6.995 [SRGAN-VGG22](https://arxiv.org/abs/1609.04802) | 25.697 | 2.631 [SRGAN-VGG54](https://arxiv.org/abs/1609.04802) | 25.176 | 2.351 [CX](https://arxiv.org/abs/1803.04626) | 24.581 | 2.250  Please cite following papers when you use the code  pre-trained models  or results: - J.-H. Choi  J.-H. Kim  M. Cheon  J.-S. Lee: **Deep learning-based image super-resolution considering quantitative and perceptual quality**. Neurocomputing (In Press) [[Paper]](https://doi.org/10.1016/j.neucom.2019.06.103) [[arXiv]](http://arxiv.org/abs/1809.04789) ``` @article{choi2018deep    title={Deep learning-based image super-resolution considering quantitative and perceptual quality}    author={Choi  Jun-Ho and Kim  Jun-Hyuk and Cheon  Manri and Lee  Jong-Seok}    journal={Neurocomputing}    year={2019}    publisher={Elsevier} } ``` - J.-H. Kim  J.-S. Lee: **Deep residual network with enhanced upscaling module for super-resolution**. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops  pp. 913-921 (2018) [[Paper]](http://openaccess.thecvf.com/content_cvpr_2018_workshops/w13/html/Kim_Deep_Residual_Network_CVPR_2018_paper.html) ``` @inproceedings{kim2018deep    title={Deep residual network with enhanced upscaling module for super-resolution}    author={Kim  Jun-Hyuk and Lee  Jong-Seok}    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops}    year={2018} } ```   Following are the brief instructions.   """;Computer Vision;https://github.com/idearibosome/tf-perceptual-eusr
"""If you'd like to submit a pull request  you'll need to clone the repository;   """;Natural Language Processing;https://github.com/google-research/google-research
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/pingheng001/Cnn-Bert
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='GeForce RTX 2080 Ti'  total_memory=11019MB)   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/Atom-min/MLBigWork
"""``` pip install fast_gat ```  Alternatively   ``` git clone https://github.com/tatp22/pytorch-fast-GAT.git cd fast_gat ```   Right now  there exist two different versions of GAT: one for sparse graphs  and one for dense graphs. The idea in the end is to use only the dense version  since the sparse version runs slower. It is currently not possible to use the dense version on very large graphs  since it creates a matrix of size `(n n)`  which will quickly drain the system's memory.  As an example  this is how to use the sparse version:  ```python import torch from fast_gat import GraphAttentionNetwork  nodes = torch.tensor([[0.1  0.2  0.3]  [0.4  0.5  0.6]  [0.7  0.8  0.9]  [1.0  1.1  1.2]]  dtype= torch.float) edges = {0: {1 2}  1: {0 2 3}  2: {0 1}  3: {1}}  depth = 3 heads = 3 input_dim = 3 inner_dim = 2  net = GraphAttentionNetwork(depth  heads  input_dim  inner_dim)  output = net(nodes  edges) print(output) ```  A point of interest here that one may notice is that the modules assume the graph is directed and that the edges have already been processed such that the nodes are zero indexed.   """;Natural Language Processing;https://github.com/tatp22/pytorch-fast-GAT
"""`pip install -r requirements.txt` <br>    This repository contains PyTorch examples of Vision transformer (https://arxiv.org/abs/2010.11929). <br> Code is originally based on https://github.com/lucidrains/vit-pytorch.   """;Computer Vision;https://github.com/gnoses/ViT_examples
"""It is tested with pytorch-1.0.   """;Computer Vision;https://github.com/GOD-GOD-Autonomous-Vehicle/self-pointnet
"""To train the HR Generator you must run:   Original SRGAN: https://github.com/twhui/SRGAN-PyTorch  Other SRGAN: https://github.com/kunalrdeshmukh/SRGAN   https://twitter.com/spain_ai_ <br>   If you have any doubts  you can feel free to contact me.   """;Computer Vision;https://github.com/AntonioAlgaida/Edge.SRGAN
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   Create tfrecord  clone this repo  then   * Clone this repo   !git clone https://github.com/skyflynil/stylegan2.git  %cd stylegan2   create your dataset for train  !mkdir dataset   * https://github.com/NVlabs/stylegan2  * https://github.com/akanimax/msg-stylegan-tf   """;General;https://github.com/cwvisuals/FleshDigressions
"""For calculating the Q Values we used a Neural Network  rather than just showing the current state to the network  the next 4 possible states(left  right  up  down) were also shown. This intuition was inspired from Monte Carlo Tree Search where game is played till the end to determine the Q-Values.  For data preprocessing log2 normalisation  training was done using the Bellman's Equation. The policy used was Epsilon greedy  to allow exploration the value of epsilon was annealed down by 5%.    """;Reinforcement Learning;https://github.com/Ishan-Kumar2/Reinforcement-Learning-on-2048
""" - Pytorch   """;General;https://github.com/tree-park/transformer_lm
"""To train the network  use the following command:  ```python main.py [-n=7] [--res-option='B'] [--use-dropout]```   """;Computer Vision;https://github.com/KellerJordan/ResNet-PyTorch-CIFAR10
"""We explore several different adaptive finetuning strategies in this repository. One thing that is common to all the strategies is the use of a policy network to determine which parts of the model to finetune/drop based on the input images-text pair. The chosen policy network is relatively very small when compared to the original VLBERT/LXMERT network. The policy network is optimized using Gumbel Softmax which relieves the argmax constraints to softmax while backpropagation.   The experiments presented are conducted on VLBERT and LXMERT. Detailed instructions to reproduce the experiments  comparisons and results are shown in the respective folders ``VLBERT`` and ``LXMERT``. Additionally  I have provided the links for Wandb workspaces for experiments on both the architectures \[[VLBERT](https://wandb.ai/shnik/adaptive-finetuning?workspace=user-shnik)  [LXMERT](https://wandb.ai/shnik/adaptive-finetuning-lxmert?workspace=user-shnik)\]. You can find the results  visualizations  training procedures  configs etc. in detail there.   """;Natural Language Processing;https://github.com/itsShnik/adaptively-finetuning-transformers
"""TBA   TBA   """;Natural Language Processing;https://github.com/evtaktasheva/dependency_extraction
"""```shell script pip install torch-fidelity ```  See also: [Installing the latest GitHub code](https://torch-fidelity.readthedocs.io/en/latest/installation.html#nightly-version)    fidelity --gpu 0 --isc --input1 cifar10-train   fidelity --gpu 0 --isc --input1 ~/images/     --gpu 0 \   Below are three examples of using torch-fidelity to evaluate metrics from the command line. See more examples in the  documentation.   When it comes to tracking the performance of generative models as they train  evaluating metrics after every epoch  becomes prohibitively expensive due to long computation times.  `torch_fidelity` tackles this problem by making full use  of caching to avoid recomputing common features and per-metric statistics whenever possible.  Computing all metrics for 50000 32x32 generated images and `cifar10-train` takes only 2 min 26 seconds on NVIDIA P100  GPU  compared to >10 min if using original codebases.  Thus  computing metrics 20 times over the whole training cycle makes overall training time just one hour longer.  In the following example  assume unconditional image generation setting with CIFAR-10  and the generative model  `generator`  which takes a 128-dimensional standard normal noise vector.  First  import the module:  ```python import torch_fidelity ```  Add the following lines at the end of epoch evaluation: ```python wrapped_generator = torch_fidelity.GenerativeModelModuleWrapper(generator  128  'normal'  0)  metrics_dict = torch_fidelity.calculate_metrics(     input1=wrapped_generator       input2='cifar10-train'       cuda=True       isc=True       fid=True       kid=True       verbose=False  ) ```  The resulting dictionary with computed metrics can logged directly to tensorboard  wandb  or console:   ```python print(metrics_dict) ```  Output:  ```python {     'inception_score_mean': 11.23678       'inception_score_std': 0.09514061       'frechet_inception_distance': 18.12198      'kernel_inception_distance_mean': 0.01369556       'kernel_inception_distance_std': 0.001310059 } ```  See also: [Full API reference](https://torch-fidelity.readthedocs.io/en/latest/api.html)   Refer to [sngan_cifar10.py](examples/sngan_cifar10.py) for a complete training example.  Evolution of fixed generator latents in the example:  ![Evolution of fixed generator latents](doc/img/sngan-cifar10.gif)  A generator checkpoint resulting from training the example can be downloaded  [here](https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/example-sngan-cifar10-generator.pth).    """;General;https://github.com/toshas/torch-fidelity
"""The code has been tested using:  Caffe Version 1.0.    Python 3.6   """;General;https://github.com/krutikabapat/Deep-Learning-based-OpenPose-Estimation
"""For more advanced usage  please refer to [INSTALL](docs/INSTALL.md) to set up more libraries needed for distributed training and sparse convolution.   ```bash #: basic python libraries conda create --name centerpoint python=3.6 conda activate centerpoint conda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=10.0 -c pytorch git clone https://github.com/tianweiy/CenterPoint.git cd CenterPoint pip install -r requirements.txt  #: add CenterPoint to PYTHONPATH by adding the following line to ~/.bashrc (change the path accordingly) export PYTHONPATH=""${PYTHONPATH}:PATH_TO_CENTERPOINT"" ```  First download the model (By default  [centerpoint_pillar_512](https://drive.google.com/file/d/1ubWKx3Jg1AqF93qqWIZxgGXTycQ77qM3/view?usp=sharing)) from the [Model Zoo](docs/MODEL_ZOO.md) and put it in ```work_dirs/centerpoint_pillar_512_demo```.   We provide a driving sequence clip from the [nuScenes dataset](https://www.nuscenes.org). Donwload the [folder](https://drive.google.com/file/d/1bK-xeq5UwJzpPfVDhICDJeKiU1QVZwtI/view?usp=sharing) and put in the main directory.      Then run a demo by ```python tools/demo.py```. If setup corectly  you will see an output video like (red is gt objects  blue is the prediction):   <p align=""center""> <img src='docs/demo.gif' align=""center"" height=""350px""> </p>    We provide a demo with PointPillars model for 3D object detection on the nuScenes dataset.    """;Computer Vision;https://github.com/chowkamlee81/CentrePointNet
"""Pip install this package  pip install antialiased-cnns  Or clone this repository and install requirements (notably  PyTorch)  cd antialiased-cnns  pip install -r requirements.txt   The antialiased_cnns module contains the BlurPool class  which does blur+subsampling. Run pip install antialiased-cnns or copy the antialiased_cnns subdirectory.   Run `pip install antialiased-cnns`  ```python import antialiased_cnns model = antialiased_cnns.resnet50(pretrained=True)  ``` <!-- model.load_state_dict(torch.load('resnet50_lpf4-994b528f.pth.tar')['state_dict'])  """;Computer Vision;https://github.com/adobe/antialiased-cnns
"""This is the official code of [High-Resolution Representations for Object Detection](https://arxiv.org/pdf/1904.04514.pdf). We extend the high-resolution representation (HRNet) [1] by augmenting the high-resolution representation by aggregating the (upsampled) representations from all the parallel convolutions  leading to stronger representations. We build a multi-level representation from the high resolution and apply it to the Faster R-CNN  Mask R-CNN and Cascade R-CNN framework. This proposed approach achieves superior results to existing single-model networks on COCO object detection. The code is based on [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark)  <div align=center>  ![](images/hrnetv2p.png)  </div>    1. Install PyTorch 1.0 following the [official instructions](https://pytorch.org/)  2. Install `pycocotools` ````bash git clone https://github.com/cocodataset/cocoapi.git \  && cd cocoapi/PythonAPI \  && python setup.py build_ext install \  && cd ../../ ```` 3. Install `HRNet-MaskRCNN-Benchmark`  ````bash  git clone https://github.com/HRNet/HRNet-MaskRCNN-Benchmark.git cd HRNet-MaskRCNN-Benchmark python setup.py build develop pip install -r requirements.txt ````  for more details  see [INSTALL.md](INSTALL.md)    cd HRNet-MaskRCNN-Benchmark   mkdir hrnetv2_pretrained   """;Computer Vision;https://github.com/HRNet/HRNet-MaskRCNN-Benchmark
"""python train.py --name ""default"" --config ""config/default.yaml""   """;Computer Vision;https://github.com/rishikksh20/ResUnet
"""      - 'name'   - save_path : Path for saving the models.   https://github.com/reedscot/icml2016 (the authors version)   """;Computer Vision;https://github.com/aelnouby/Text-to-Image-Synthesis
""" ``` git clone https://github.com/AutodeskAILab/BRepNet.git cd BRepNet conda env create -f environment.yml conda activate brepnet ```  For GPU training you will need to change the pytorch install to include your cuda version.  i.e. for cuda 11.1 ``` conda install pytorch cudatoolkit=11.1 -c pytorch -c conda-forge ``` or for cuda 11.0 ``` conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=11.0 -c pytorch ```  For training with multiple workers you may hit errors of the form `OSError: [Errno 24] Too many open files`.  In this case you need to increase the number of available file handles on the machine using  ``` ulimit -Sn 10000 ``` I find I need to set the limit to 10000 for 10 worker threads.   cd BRepNet/  python -m pipeline.quickstart --dataset_dir /path/to/where_you_keep_data/s2.0.0 --num_workers 5   You are then ready to train the model.  The quickstart script should exit telling you a default command to use which should be something like  python -m train.train \   python -m train.train --help   train/reproduce_paper_results.sh /path/to/where_you_keep_data/s2.0.0   cd BRepNet   """;Computer Vision;https://github.com/AutodeskAILab/BRepNet
"""It is tested with pytorch-1.0.   """;Computer Vision;https://github.com/hz-ants/pointnet.pytorch
"""This repo will not receive active development  however  you can continue use it with the mxnet 1.1.0(probably 1.2.0).   cd /path/to/where_you_store_datasets/   : cd /path/to/mxnet-ssd  bash tools/prepare_pascal.sh  : or if you are using windows   ![demo1](https://user-images.githubusercontent.com/3307514/28980832-29bb0262-7904-11e7-83e3-a5fec65e0c70.png)   - Build from source  this is required because this example is not merged  some custom operators are not presented in official MXNet. [Instructions](http://mxnet.io/get_started/install.html) - Install required packages: `cv2`  `matplotlib`   - Download the pretrained [model](https://github.com/zhreshold/mxnet-yolo/releases/download/0.1-alpha/yolo2_darknet19_416_pascalvoc0712_trainval.zip)(darknet as backbone)  or this [model](https://github.com/zhreshold/mxnet-yolo/releases/download/v0.2.0/yolo2_resnet50_voc0712_trainval.tar.gz)(resnet50 as backbone) and extract to `model/` directory. - Run ``` #: cd /path/to/mxnet-yolo python demo.py --cpu #: available options python demo.py -h ```   """;Computer Vision;https://github.com/zhreshold/mxnet-yolo
"""    [Code] https://github.com/NVIDIA/tacotron2<br>       [Code] https://github.com/NVIDIA/OpenSeq2Seq<br>       [Code] https://github.com/Kyubyong/dc_tts<br>       [Code] https://github.com/keithito/tacotron<br>      [Code] https://github.com/MycroftAI/mimic2<br>       [Code] https://github.com/mozilla/TTS<br>       [Code] https://github.com/stanfordnlp/GloVe<br>       [Code] https://github.com/yanggeng1995/GAN-TTS<br>       [DeepVoice 2] https://github.com/jdbermeol/deep_voice_2<br>      [DeepVoice 3] https://github.com/r9y9/deepvoice3_pytorch<br>   """;General;https://github.com/izzajalandoni/tts_models
"""I used the pandas library to calculate summary statistics of the traffic signs data set:  * Number of training examples = 34799 * Number of validation examples = 4410 * Number of testing examples = 12630 * Image data shape = (32  32  3) * Number of classes = 43   Here are the results of the prediction:  | Image			        |     Prediction	        					|  |:---------------------:|:---------------------------------------------:|  | Children crossing     | Children crossing  							|  | Right-of-way          | Right-of-way									| | Priority road			| Priority road									| | Turn right ahead 		| Turn right ahead				 				| | Road work 			| Road work         							|   The model was able to correctly guess 5 of the 5 traffic signs  which gives an accuracy of 100%  which is close to 98% from the test set.   My final model results were: * training set accuracy of 99%+ * validation set accuracy of 99%+ * test set accuracy of 98%+  If a well known architecture was chosen: * Inception v4 was chosen for traffic sign classifier. * This is a very suitable architecture because it has a very high accuracy for classifier (general inception v4 can be used to classify 1000 classes)  and it is quite efficient. * It can be concluded this model works very well because all 3 data sets have very high accuracies  which means the model is not under or over fitting (balanced variance and bias).  Additional visualization of the validation accuracy is analyzed to understand what works or not.  Below is the validation set recall and precision by class.  Note that class 16 has a low recall (false negative)  meaning images from class 16 were predicted as some other clases.  In precision chart  class 41 has a low value (false positive).  It is likely that many class 16 images were misclassified as class 41.  ![alt text][image9] ![alt text][image10]  Images were pulled from classes 16 and 41 and quickly one can see that some class 16 images have red circular borders are quite faded so they could be similar to class 41 images.  Below are classes 16 (left) and 41 (right) sample images.  ![alt text][image11]   First  I normalized all training images to have float32 from 0 to 1.  I found the accuracy increases faster than -1 to 1 during network training. The matricies below simply show one random image before and after normalization.  Before normalization:  [[51 36 45 ... 80 79 73]   [47 34 38 ... 64 75 79]    [45 32 38 ... 61 68 71]    ...    [43 38 34 ... 46 42 37]    [44 36 31 ... 36 33 35]    [41 36 38 ... 52 48 50]]   After normalization:  [[0.1849315  0.08219178 0.14383562 ... 0.38356164 0.37671232 0.33561644]   [0.15753424 0.06849315 0.09589041 ... 0.2739726  0.34931508 0.37671232]    [0.14383562 0.05479452 0.09589041 ... 0.25342464 0.30136988 0.3219178 ]    ...    [0.13013698 0.09589041 0.06849315 ... 0.15068494 0.12328767 0.0890411 ]    [0.1369863  0.08219178 0.04794521 ... 0.08219178 0.06164384 0.07534247]    [0.11643836 0.08219178 0.09589041 ... 0.19178082 0.16438356 0.1780822 ]]     As mentioned before  data augmentation is applied to even out image quantity difference among classes  and to include variations of same images.  * sharpen or smoothing * random rotate image * random stretch/squeeze image  * random darken partial image * random move image  Here is an example of a traffic sign image before and after augmentation.  The image is stretched horizontally and partially darkened at the bottom.  ![alt text][image4] ![alt text][image5]  When all training images are added up  the quantity shows:  ![alt text][image6]  As a last step  the training set is shuffled to remove any order.    I used the pandas library to calculate summary statistics of the traffic signs data set:  * Number of training examples = 34799 * Number of validation examples = 4410 * Number of testing examples = 12630 * Image data shape = (32  32  3) * Number of classes = 43   You're reading it! and here is a link to my [project code](https://github.com/waynecoffee9/Traffic-Sign-Classifier/blob/master/CarND-Traffic-Sign-Classifier-Project/Traffic_Sign_Classifier-inception.ipynb) If you are unable to view it under github  use https://nbviewer.jupyter.org/ and paste the link to view.   You're reading it! and here is a link to my [project code](https://github.com/waynecoffee9/Traffic-Sign-Classifier/blob/master/CarND-Traffic-Sign-Classifier-Project/Traffic_Sign_Classifier-inception.ipynb) If you are unable to view it under github  use https://nbviewer.jupyter.org/ and paste the link to view.   First  I normalized all training images to have float32 from 0 to 1.  I found the accuracy increases faster than -1 to 1 during network training. The matricies below simply show one random image before and after normalization.  Before normalization:  [[51 36 45 ... 80 79 73]   [47 34 38 ... 64 75 79]    [45 32 38 ... 61 68 71]    ...    [43 38 34 ... 46 42 37]    [44 36 31 ... 36 33 35]    [41 36 38 ... 52 48 50]]   After normalization:  [[0.1849315  0.08219178 0.14383562 ... 0.38356164 0.37671232 0.33561644]   [0.15753424 0.06849315 0.09589041 ... 0.2739726  0.34931508 0.37671232]    [0.14383562 0.05479452 0.09589041 ... 0.25342464 0.30136988 0.3219178 ]    ...    [0.13013698 0.09589041 0.06849315 ... 0.15068494 0.12328767 0.0890411 ]    [0.1369863  0.08219178 0.04794521 ... 0.08219178 0.06164384 0.07534247]    [0.11643836 0.08219178 0.09589041 ... 0.19178082 0.16438356 0.1780822 ]]     As mentioned before  data augmentation is applied to even out image quantity difference among classes  and to include variations of same images.  * sharpen or smoothing * random rotate image * random stretch/squeeze image  * random darken partial image * random move image  Here is an example of a traffic sign image before and after augmentation.  The image is stretched horizontally and partially darkened at the bottom.  ![alt text][image4] ![alt text][image5]  When all training images are added up  the quantity shows:  ![alt text][image6]  As a last step  the training set is shuffled to remove any order.    My final model results were: * training set accuracy of 99%+ * validation set accuracy of 99%+ * test set accuracy of 98%+  If a well known architecture was chosen: * Inception v4 was chosen for traffic sign classifier. * This is a very suitable architecture because it has a very high accuracy for classifier (general inception v4 can be used to classify 1000 classes)  and it is quite efficient. * It can be concluded this model works very well because all 3 data sets have very high accuracies  which means the model is not under or over fitting (balanced variance and bias).  Additional visualization of the validation accuracy is analyzed to understand what works or not.  Below is the validation set recall and precision by class.  Note that class 16 has a low recall (false negative)  meaning images from class 16 were predicted as some other clases.  In precision chart  class 41 has a low value (false positive).  It is likely that many class 16 images were misclassified as class 41.  ![alt text][image9] ![alt text][image10]  Images were pulled from classes 16 and 41 and quickly one can see that some class 16 images have red circular borders are quite faded so they could be similar to class 41 images.  Below are classes 16 (left) and 41 (right) sample images.  ![alt text][image11]   Here are some of the visualized feature maps evaluated on the first new image (children crossing).  It seems some feature maps picked up the shape of the triangle.  Some feature maps picked up the shape of the human figures inside the triangle.  Some feature maps picked up the blue sky on the left.  ![alt text][image18] ![alt text][image19] ![alt text][image20]  """;Computer Vision;https://github.com/waynecoffee9/Traffic-Sign-Classifier
"""Start self-supervised training with `python train.py configs/cpc-base.yml`.  Then  you can evaluate model on speaker verification (EER  minDCF) with `python evaluate.py configs/cpc-base.yml`.   """;General;https://github.com/theolepage/ssl-for-slr
"""Source PDF   You can start according to the default arguments by `python main.py`. Or specify the arguments: ```python python main.py --arch cond_cifar_resnet --num_layers 56 --num_experts 3 --dataset cifar10 --num_classes 10 ```    """;Computer Vision;https://github.com/prstrive/CondConv-tensorflow
"""All the parameters related to training/decoding will be stored in a yaml file. Hyperparameter tuning and massive experiment and can be managed easily this way. See [config files](config/) for the exact format and examples.   Before you start  make sure all the packages required listed above are installed correctly  See the instructions on the Preprocess wiki page for preprocessing instructions.   Note that the arguments--ckpdir=XXX --ckpt=XXX``` needs to be set correctly for the above command to run properly.   """;Natural Language Processing;https://github.com/andi611/Mockingjay-Speech-Representation
"""Before you start you can try the demo.   ```bash wget -P models https://storage.googleapis.com/models-hao/mobilenet-v1-ssd-mp-0_675.pth wget -P models https://storage.googleapis.com/models-hao/voc-model-labels.txt python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt  ```  ```bash wget -P models https://storage.googleapis.com/models-hao/mobilenet_v1_ssd_caffe2/mobilenet-v1-ssd_init_net.pb wget -P models https://storage.googleapis.com/models-hao/mobilenet_v1_ssd_caffe2/mobilenet-v1-ssd_predict_net.pb python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt  ```  You can see a decent speed boost by using Caffe2.   ```bash wget -P models https://storage.googleapis.com/models-hao/mb2-ssd-lite-mp-0_686.pth wget -P models https://storage.googleapis.com/models-hao/voc-model-labels.txt python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt  ```  The above MobileNetV2 SSD-Lite model is not ONNX-Compatible  as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu  I will upload the corresponding Pytorch and Caffe2 models.  You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However  MobileNetV2 is faster on mobile devices.   """;Computer Vision;https://github.com/qfgaohao/pytorch-ssd
"""1. Run `pip install -r requirements.txt` to install the necessary dependencies. 2. Run `python launch_script.py` to start training the Chess Engine.   python-chess - For handling the chess environment and gameplay.   """;Reinforcement Learning;https://github.com/saikrishna-1996/deep_pepper_chess
"""--name ffhq_aegan_wplus_decoupled \   --name ffhq_aegan_wplus_joint \   --name ffhq_gan \   --name ffhq_alae_wtied_recw=1_mlpd=4 \   """;General;https://github.com/phymhan/stylegan2-pytorch
"""Teknik yang digunakan dalam penyiapan data *(Data Preparation)* yaitu: - **Split Data** atau pembagian dataset menjadi data latih dan data uji menggunakan bantuan `ImageDataGenerator`. Pembagian dataset ini bertujuan agar nantinya dapat digunakan untuk melatih dan mengevaluasi kinerja model. Pada proyek ini  80% dataset digunakan untuk melatih model  dan 20% sisanya digunakan untuk mengevaluasi model. - **Image Augmentation** merupakan teknik menghasilkan gambar baru untuk melatih model *deep learning* [[6](https://medium.com/analytics-vidhya/image-augmentation-9b7be3972e27)]. Pada proyek ini  augmentasi gambar adalah dengan menambahkan layer untuk melakukan *fliping*  *rotating*  *zoom*  dan *scaling*.   Source | Kaggle   """;General;https://github.com/yusufsugiono/klasifikasi-gambar-bunga
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/wdlctc/darknet
"""http://118.190.96.120/iDentisty/client/   """;General;https://github.com/yangboz/TransferLearning4Dentist
"""executing the ` unet_learner` function will give you the modified unet with dropout. using the `DropOutAlexnet` class will give you the alexnet architecture with dropout added.   """;General;https://github.com/aredier/monte_carlo_dropout
"""In an effort to determine the high-risk customers  the bank observed 10 000 customers over six months. The dataset was partitioned into a training set of 8 000 samples and a testing set of 2 000 samples. They collected a variety of datapoints they figured would be indicative of retention (or  more technically  ""churn""). An excerpt of  the dataset is provided below  |RowNumber   	| CustomerID  	| Surname  	| CreditScore  	| Geography  	| Gender  	| Age  	| Tenure  	| Balance  	| NumOfProducts  	| HasCrCard  	| IsActiveMember  	| EstimatedSalary  	| Exited  	| |---	        |---	          |---	      |---	          |---	        |---	      |---	  |---	      |---	      |---	            |---	        |---	              |---	              |---	      | | 1   	      | 15634602  	  | Hargrave  | 619   	      | France    	| Female  	| 42   	| 2       	| 0       	| 1             	| 1          	| 1               	| 101348.88       	| 1  	      | | 49   	      | 15766205  	  | Yin       | 550   	      | Germany    	| Male    	| 38   	| 2       	| 103391.38 | 1             	| 0          	| 1               	| 90878.13         	| 0  	      |  I didn't collect this dataset. So far as I know  it was generated. If you're interested in obtaining the full dataset  please reach out to me and I can send it to you. I won't be hosting it in this repository to protect the work of its creator.   Python 3.6.6   """;Computer Vision;https://github.com/patconrey/ANN-Example
"""To get the environment installed correctly  you will first need to clone [rllab](https://github.com/rll/rllab)  and have its path added to your PYTHONPATH environment variable.  1. Clone rllab ``` cd <installation_path_of_your_choice> git clone https://github.com/rll/rllab.git cd rllab git checkout b3a28992eca103cab3cb58363dd7a4bb07f250a0 export PYTHONPATH=$(pwd):${PYTHONPATH} ```  2. [Download](https://www.roboti.us/index.html) and copy mujoco files to rllab path:   If you're running on OSX  download https://www.roboti.us/download/mjpro131_osx.zip instead  and copy the `.dylib` files instead of `.so` files. ``` mkdir -p /tmp/mujoco_tmp && cd /tmp/mujoco_tmp wget -P . https://www.roboti.us/download/mjpro131_linux.zip unzip mjpro131_linux.zip mkdir <installation_path_of_your_choice>/rllab/vendor/mujoco cp ./mjpro131/bin/libmujoco131.so <installation_path_of_your_choice>/rllab/vendor/mujoco cp ./mjpro131/bin/libglfw.so.3 <installation_path_of_your_choice>/rllab/vendor/mujoco cd .. rm -rf /tmp/mujoco_tmp ```  3. Copy your Mujoco license key (mjkey.txt) to rllab path: ``` cp <mujoco_key_folder>/mjkey.txt <installation_path_of_your_choice>/rllab/vendor/mujoco ```  4. Clone sac ``` cd <installation_path_of_your_choice> git clone https://github.com/haarnoja/sac.git cd sac ```  5. Create and activate conda environment ``` cd sac conda env create -f environment.yml source activate sac ```  The environment should be ready to run. See examples section for examples of how to train and simulate the agents.  Finally  to deactivate and remove the conda environment: ``` source deactivate conda remove --name sac --all ```   If you want to run the Mujoco environments  the docker environment needs to know where to find your Mujoco license key (`mjkey.txt`). You can either copy your key into `<PATH_TO_THIS_REPOSITY>/.mujoco/mjkey.txt`  or you can specify the path to the key in your environment variables:  ``` export MUJOCO_LICENSE_PATH=<path_to_mujoco>/mjkey.txt ```  Once that's done  you can run the Docker container with  ``` docker-compose up ```  Docker compose creates a Docker container named `soft-actor-critic` and automatically sets the needed environment variables and volumes.  You can access the container with the typical Docker [exec](https://docs.docker.com/engine/reference/commandline/exec/)-command  i.e.  ``` docker exec -it soft-actor-critic bash ```  See examples section for examples of how to train and simulate the agents.  To clean up the setup: ``` docker-compose down ```   Soft Actor-Critic can be run either locally or through Docker.   """;Reinforcement Learning;https://github.com/haarnoja/sac
"""The demos in this folder are designed to give straightforward samples of using TensorFlow in mobile applications.  Inference is done using the [TensorFlow Android Inference Interface](../../../tensorflow/contrib/android)  which may be built separately if you want a standalone library to drop into your existing application. Object tracking and YUV -> RGB conversion is handled by libtensorflow_demo.so.  A device running Android 5.0 (API 21) or higher is required to run the demo due to the use of the camera2 API  although the native libraries themselves can run on API >= 14 devices.   Make sure that adb debugging is enabled on your Android 5.0 (API 21) or later device  then after building use the following command from your workspace root to install the APK:  ```bash adb install -r bazel-bin/tensorflow/examples/android/tensorflow_demo.apk ```   The TensorFlow `GraphDef`s that contain the model definitions and weights are not packaged in the repo because of their size. They are downloaded automatically and packaged with the APK by Bazel via a new_http_archive defined in `WORKSPACE` during the build process  and by Gradle via download-models.gradle.  **Optional**: If you wish to place the models in your assets manually  remove all of the `model_files` entries from the `assets` list in `tensorflow_demo` found in the `[BUILD](BUILD)` file. Then download and extract the archives yourself to the `assets` directory in the source tree:  ```bash BASE_URL=https://storage.googleapis.com/download.tensorflow.org/models for MODEL_ZIP in inception5h.zip mobile_multibox_v1a.zip stylize_v1.zip do   curl -L ${BASE_URL}/${MODEL_ZIP} -o /tmp/${MODEL_ZIP}   unzip /tmp/${MODEL_ZIP} -d tensorflow/examples/android/assets/ done ```  This will extract the models and their associated metadata files to the local assets/ directory.  If you are using Gradle  make sure to remove download-models.gradle reference from build.gradle after your manually download models; otherwise gradle might download models again and overwrite your models.   Bazel is the primary build system for TensorFlow. To build with Bazel  it and the Android NDK and SDK must be installed on your system.  1. Install the latest version of Bazel as per the instructions [on the Bazel website](https://bazel.build/versions/master/docs/install.html). 2. The Android NDK is required to build the native (C/C++) TensorFlow code.         The current recommended version is 12b  which may be found         [here](https://developer.android.com/ndk/downloads/older_releases.html#ndk-12b-downloads). 3. The Android SDK and build tools may be obtained         [here](https://developer.android.com/tools/revisions/build-tools.html)          or alternatively as part of         [Android Studio](https://developer.android.com/studio/index.html). Build         tools API >= 23 is required to build the TF Android demo (though it will         run on API >= 21 devices).   nightly build   that Windows users download the  prebuilt binaries   you may build the APK. Run this from your workspace root:   If you get build errors about protocol buffers  run  git submodule update --init and make sure that you've modified your WORKSPACE   make sure that you can build with Bazel following the above directions. Then   look at build.gradle and make sure that the path to Bazel   Android Studio project. Click through installing all the Gradle extensions it  requests  and you should be able to have Android Studio build the demo like any   This folder contains an example application utilizing TensorFlow for Android devices.   Once the app is installed it can be started via the ""TF Classify""  ""TF Detect"" and ""TF Stylize"" icons  which have the orange TensorFlow logo as their icon.  While running the activities  pressing the volume keys on your device will toggle debug visualizations on/off  rendering additional info to the screen that may be useful for development purposes.   Pick your preferred approach below. At the moment  we have full support for Bazel  and partial support for gradle  cmake  make  and Android Studio.  As a first step for all build types  clone the TensorFlow repo with:  ``` git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git ```  Note that `--recurse-submodules` is necessary to prevent some issues with protobuf compilation.   """;General;https://github.com/GenesisDCarmen/C_Reconocimiento_Facial
"""In order to get the TF official weights. * Create a Colab file in your Google Drive. * Mount the drive with  ``` from google.colab import drive drive.mount('/content/drive/') ``` * Run this command in your created Colab  ```!gsutil cp -r gs://cloud-tpu-checkpoints/efficientnet ""/content/drive/My Drive/effnets/""``` * Download the effnets folder from your drive and extract somewhere.  * Extract the weights from each layer to a directory by. ```python from extract_weights import extract_tensors_from_checkpoint_file  extract_tensors_from_checkpoint_file('efficientnet-b0/model.ckpt-109400')  #: change this line to your extracted directory ```  * Each TF weights directory should be like. ``` best_eval.txt checkpoint model.ckpt-12345.data-00000-of-00001 model.ckpt-12345.index model.ckpt-12345.meta ```    Use `model.ckpt-12345` in this case.  * Create the hdf5 weights by run the `load_weights.py`  make sure to change `WEIGHTS_DIR` and `model_name` first.  * The example of testing a panda image is in `main.py`   """;Computer Vision;https://github.com/DableUTeeF/keras-efficientnet
"""```bash $ pip install siren-pytorch ```   A SIREN based multi-layered neural network  ```python import torch from torch import nn from siren_pytorch import SirenNet  net = SirenNet(     dim_in = 2                         #: input dimension  ex. 2d coor     dim_hidden = 256                   #: hidden dimension     dim_out = 3                        #: output dimension  ex. rgb value     num_layers = 5                     #: number of layers     final_activation = nn.Sigmoid()    #: activation of final layer (nn.Identity() for direct output)     w0_initial = 30.                   #: different signals may require different omega_0 in the first layer - this is a hyperparameter )  coor = torch.randn(1  2) net(coor) #: (1  3) <- rgb value ```  One SIREN layer  ```python import torch from siren_pytorch import Siren  neuron = Siren(     dim_in = 3      dim_out = 256 )  coor = torch.randn(1  3) neuron(coor) #: (1  256) ```  Sine activation (just a wrapper around `torch.sin`)  ```python import torch from siren_pytorch import Sine  act = Sine(1.) coor = torch.randn(1  2) act(coor) ```  Wrapper to train on a specific image of specified height and width from a given `SirenNet`  and then to subsequently generate.   ```python import torch from torch import nn from siren_pytorch import SirenNet  SirenWrapper  net = SirenNet(     dim_in = 2                         #: input dimension  ex. 2d coor     dim_hidden = 256                   #: hidden dimension     dim_out = 3                        #: output dimension  ex. rgb value     num_layers = 5                     #: number of layers     w0_initial = 30.                   #: different signals may require different omega_0 in the first layer - this is a hyperparameter )  wrapper = SirenWrapper(     net      image_width = 256      image_height = 256 )  img = torch.randn(1  3  256  256) loss = wrapper(img) loss.backward()  #: after much training ... #: simply invoke the wrapper without passing in anything  pred_img = wrapper() #: (1  3  256  256) ```   """;General;https://github.com/lucidrains/siren-pytorch
"""See https://github.com/bioinf-jku/TTUR for the original implementation using Tensorflow.   You can still use this version if you want a quick FID estimate without installing Tensorflow.   Requirements: - python3 - MXNet - GluonCV - numpy - scipy  To compute the FID score between two datasets  where images of each dataset are contained in an individual folder: ``` ./fid_score.py path/to/dataset1 path/to/dataset2 ```  To run the evaluation on GPU  use the flag `--gpu`.    """;General;https://github.com/djl11/mxnet-fid
"""There are two ways to install the Image Super-Resolution package:  - Install ISR from PyPI (recommended): ``` pip install ISR ``` - Install ISR from the GitHub source: ``` git clone https://github.com/idealo/image-super-resolution cd image-super-resolution python setup.py install ```   pip install 'h5py==2.10.0' --force-reinstall   To bump up the version  use   """;Computer Vision;https://github.com/idealo/image-super-resolution
"""And then fetch the repository and the install dependencies (including `jaxlib` with TPU support) as usual:  ```bash git clone --depth=1 --branch=master https://github.com/google-research/vision_transformer cd vision_transformer pip3 install virtualenv python3 -m virtualenv env . env/bin/activate ```  If you're connected to a VM with GPUs attached  install JAX with the following command:  ```bash pip3 install --upgrade jax jaxlib \     -f https://storage.googleapis.com/jax-releases/jax_releases.html ```  If you're connected to a VM with TPUs attached  install JAX with the following command:  ```bash pip3 install --upgrade jax jaxlib ```  For both GPUs and TPUs  then proceed to install the remaining dependencies and check that accelerators can indeed show up in JAX:  ```bash pip install -r vit_jax/requirements.txt #: Check that JAX can connect to attached accelerators: python -c 'import jax; print(jax.devices())' ```  And finally execute one of the commands mentioned in the section [fine-tuning a model](#fine-tuning-a-model).    Make sure you have `Python>=3.6` installed on your machine.  For installing [JAX](https://github.com/google/jax)  follow the instructions provided in the corresponding repository linked here. Note that installation instructions for GPU differs slightly from the instructions for CPU.  Then  install python dependencies by running: ``` pip install -r vit_jax/requirements.txt ```  For more details refer to the section [Running on cloud](#running-on-cloud) below.    Setup VM   Note: As for now (6/20/21) Google Colab only supports a single GPU (Nvidia   You can find all these models in the following storage bucket:   For example  if you would like to download the ViT-B/16 pre-trained on  imagenet21k run the following command:   For installation follow the same steps as above.   You can use the following commands to setup a VM with GPUs on Google Cloud:   : Below settings have been tested with this repository. You can choose other   Alternatively  you can use the following similar commands to set up a Cloud VM       --version v2-alpha   """;Computer Vision;https://github.com/google-research/vision_transformer
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/SCismycat/bert_code_view
"""This repo holds the code for [DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution](https://arxiv.org/pdf/2006.02334.pdf). The project is based on [mmdetection codebase](https://github.com/open-mmlab/mmdetection). Please refer to [mmdetection readme](README.mmdet.md) for installation and running scripts. The code is tested with PyTorch 1.4.0. It may not run with other versions. See [conda_env.md](conda_env.md) for the versions of all the packages.   """;Computer Vision;https://github.com/joe-siyuan-qiao/DetectoRS
""" ```bash git clone https://github.com/JCBrouwer/maua-stylegan2 cd maua-stylegan2 pip install -r requirements.txt ```  Alternatively  check out this [Colab Notebook](https://colab.research.google.com/drive/1Ig1EXfmBC01qik11Q32P0ZffFtNipiBR)   or (in e.g. a jupyter notebook):   """;General;https://github.com/JCBrouwer/maua-stylegan2
"""experimental code setup partly based on this pytorch implementation   """;Reinforcement Learning;https://github.com/niklasschmitz/DeepQLearning
"""- The wrong connection and lack joint error is 256px smaller than 321px. Sometimes there is missing part.  - In terms of time  256px is almost twice as fast as 321px.  - PifPaf is often wrong in the wrong connection  missing part error. In addition  MPII and COCO dataset 256px and 321px focus on other error.  - In sports environment  PifPaf focuses on wrong connection error (60-70% of total error).   The recent methods of estimating the human posture in two-dimensional space based on deep learning have shown better applicability and results than before. However  the problem also faced many different challenges such as in crowds  resolution  lighting  ... In this project  I analyze and evaluate pros and cons of the article “Pifpaf: Composite Fields For Human Pose Estimation ”  the author has focused on solving the challenge of occluded and low resolution. I analyze on different datasets: COCO dataset  in addition I analyze errors on 1000 MPII images  2000 images of sports dataset collected by us. Thereby having a more general view of the article  and thereby giving directions to develop research to improve the article.   This project based on paper : https://arxiv.org/abs/1903.06593 and code: https://github.com/vita-epfl/openpifpaf  I analyze and evaluate on datasets:4000 images COCO test-dev 2000 images sports 1000 images MPII datasets:  Because the author focused on solving the challenge on low resolution and obscuration. I resized the images to 3 different resolutions: 256px  321px  641px. The author have suggested that 321px is the best but I want to experiment with 256px more  then I can conclude that 321px is the best or not.  Dataset: https://drive.google.com/drive/folders/19xFqlgraUi7BZp9VgBUY_UfC6u4gYNyY?usp=sharing  Analysis: https://drive.google.com/drive/folders/1BUkAbabOqjFUhi2_RlYds0vFS7Wg4ryB?usp=sharing   """;Computer Vision;https://github.com/thanhtrung98/Pose_estimation
"""To run *node2vec* on Zachary's karate club network  execute the following command from the project home directory:<br/> 	``python src/main.py --input graph/karate.edgelist --output emb/karate.emd``   """;Graphs;https://github.com/syyunn/node2vec
"""👯 Clone this repo:  $ git clone https://github.com/saimj7/People-Counting-in-Real-Time.git   """;Computer Vision;https://github.com/saimj7/People-Counting-in-Real-Time
"""The dataset is available at http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html   """;Computer Vision;https://github.com/polmonroig/faceGAN
"""To discover how to project a real image using the original StyleGAN2 implementation  run: -   [`stylegan2_projecting_images.ipynb`][stylegan2_projecting_images] [![Open In Colab][colab-badge]][stylegan2_projecting_images]  To process the projection of **a batch** of images  using either `W(1 *)` (original) or `W(18 *)` (extended)  run: -   [`stylegan2_projecting_images_with_my_fork.ipynb`][stylegan2_projecting_images_with_fork] [![Open In Colab][colab-badge]][stylegan2_projecting_images_with_fork]  To edit latent vectors of projected images  run: -   [`stylegan2_editing_latent_vectors.ipynb`][stylegan2_editing_latent_vectors] [![Open In Colab][colab-badge]][stylegan2_editing_latent_vectors]  For more information about `W(1 *)` and `W(18 *)`  please refer to the [the original paper][stylegan2-paper] (section 5 on page 7):  > Inverting the synthesis network $g$ is an interesting problem that has many applications. > Manipulating a given image in the latent feature space requires finding a matching latent code $w$ for it first.  The following is about `W(18 *)`: > Previous research suggests that instead of finding a common latent code $w$  the results improve if a separate $w$ is chosen for each layer of the generator. > The same approach was used in an early encoder implementation.  The following is about `W(1 *)`  which is the approach used in the original implementation: > While extending the latent space in this fashion finds a closer match to a given image  it also enables projecting arbitrary images that should have no latent representation. > Instead  we concentrate on finding latent codes in the original  unextended latent space  as these correspond to images that the generator could have produced.   """;Computer Vision;https://github.com/woctezuma/stylegan2-projecting-images
"""chmod +x play.sh   You can use the following commands to train the model from scrach or finetuning(if pretrained weight file is provided).  chmod +x train.sh     """;Reinforcement Learning;https://github.com/qiankun214/DQN-FlappyBird-python3
"""Unpack the downloaded tar.gz file using:  tar -xzf acllmdb.tar.gz   Inside your virtual environment launch the *jupyter notebook*  and open the notebook file (with *.ipynb* extension)  then change the kernel to the one created in the preceding step (<env_name>). Now you are ready. Follow me through the tutorial.    """;Natural Language Processing;https://github.com/radoslawkrolikowski/sentiment-analysis-pytorch
""">Input: [96 x 96 x 3] >Patch: [24 x 24 x 3]  >Overlap size 12. 1 input image is composed with 49 patches.  >Flow: Input Image -> Make Patches -> Encoding -> Pixel CNN -> CPC  >Encoder: ResDense Block + Global AVG Pool  No Pooling Layer(Conv Only)  Batch Norm(Fine Tune Only) and Self Attention.  >***ResDense Block is*** dense convolution block with residual connection(See JointCenterLoss).   ***Contact: seonghobaek@gmail.com***  """;General;https://github.com/SeonghoBaek/CPC
"""      DARWISH Marwan : https://github.com/DMarwan          KHCHICHE Zakaria : https://github.com/zakariakhchiche          NAMOUS Seifeddine : https://github.com/seifnamous   Dashboard : SB Admin 2 : https://github.com/BlackrockDigital/startbootstrap-sb-admin-2   2) pip install -r requierements.txt   """;Computer Vision;https://github.com/DMarwan/NutricIA
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/Caesarzhang/bert-zh
"""```bash $ pip install res-mlp-pytorch ```   ```python import torch from res_mlp_pytorch import ResMLP  model = ResMLP(     image_size = 256      patch_size = 16      dim = 512      depth = 12      num_classes = 1000 )  img = torch.randn(1  3  256  256) pred = model(img) #: (1  1000) ```  Rectangular image  ```python import torch from res_mlp_pytorch import ResMLP  model = ResMLP(     image_size = (128  256)  #: (128 x 256)     patch_size = 16      dim = 512      depth = 12      num_classes = 1000 )  img = torch.randn(1  3  128  256) pred = model(img) #: (1  1000) ```   """;General;https://github.com/lucidrains/res-mlp-pytorch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/MichaelZhouwang/LMlexsub
"""This repository is not meant to be a library which you can install and use as it is  but rather as a ML project code which you can freely fork and modify to fit your particular needs.   You can find information about all the available arguments via `python main.py --help`. For example  you can train the Wine dataset on a heterogeneous VAE with default arguments using:  ```{bash} python main.py -model=vae -dataset=datasets/Wine -seed=2 -miss-perc=20 -miss-suffix=1 ```   """;Computer Vision;https://github.com/adrianjav/heterogeneous_vaes
"""Our UGC data include comments posted on news articles collected from 3 major Israeli news sites  between January 2020 to August 2020. The total size of the data is ~150 MB  including over 7 millions words and 350K sentences. ~2000 sentences were annotated by crowd members (3-10 annotators per sentence) for overall sentiment (polarity) and [eight emotions](https://en.wikipedia.org/wiki/Robert_Plutchik#Plutchik's_wheel_of_emotions): anger  disgust  expectation   fear  happy  sadness  surprise and trust.  The percentage of sentences in which each emotion appeared is found in the table below.  |       | anger | disgust | expectation | fear | happy | sadness | surprise | trust | sentiment | |------:|------:|--------:|------------:|-----:|------:|--------:|---------:|------:|-----------| | **ratio** |  0.78 |    0.83 |        0.58 | 0.45 |  0.12 |    0.59 |     0.17 |  0.11 | 0.25      |   	sentiment_analysis = pipeline( 	    ""sentiment-analysis""  	    model=""avichr/heBERT_sentiment_analysis""  	    tokenizer=""avichr/heBERT_sentiment_analysis""  	    return_all_scores = True 	) 	 	sentiment_analysis('אני מתלבט מה לאכול לארוחת צהריים')	 	>>>  [[{'label': 'natural'  'score': 0.9978172183036804}  	>>>  {'label': 'positive'  'score': 0.0014792329166084528}  	>>>  {'label': 'negative'  'score': 0.0007035882445052266}]]  	sentiment_analysis('קפה זה טעים') 	>>>  [[{'label': 'natural'  'score': 0.00047328314394690096}  	>>>  {'label': 'possitive'  'score': 0.9994067549705505}  	>>>  {'label': 'negetive'  'score': 0.00011996887042187154}]]  	sentiment_analysis('אני לא אוהב את העולם') 	>>>  [[{'label': 'natural'  'score': 9.214012970915064e-05}   	>>>  {'label': 'possitive'  'score': 8.876807987689972e-05}   	>>>  {'label': 'negetive'  'score': 0.9998190999031067}]]  	 Our model is also available on AWS! for more information visit [AWS' git](https://github.com/aws-samples/aws-lambda-docker-serverless-inference/tree/main/hebert-sentiment-analysis-inference-docker-lambda)   """;Natural Language Processing;https://github.com/avichaychriqui/HeBERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/faizansuhail89/bert
"""This is an official implementation of [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808). We present a new architecture  named Convolutional vision Transformers (CvT)  that improves Vision Transformers (ViT) in performance and efficienty by introducing convolutions into ViT to yield the best of both disignes. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding  and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (e.g. shift  scale  and distortion invariance) while maintaining the merits of Transformers (e.g. dynamic attention  global context  and better generalization). We validate CvT by conducting extensive experiments  showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k  with fewer parameters and lower FLOPs. In addition  performance gains are maintained when pretrained on larger dataset (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained on ImageNet-22k  our CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k val set. Finally  our results show that the positional encoding  a crucial component in existing Vision Transformers  can be safely removed in our model  simplifying the design for higher resolution vision tasks.   ![](figures/pipeline.svg)   Please prepare the data as following:  ``` sh |-DATASET   |-imagenet     |-train     | |-class1     | | |-img1.jpg     | | |-img2.jpg     | | |-...     | |-class2     | | |-img3.jpg     | | |-...     | |-class3     | | |-img4.jpg     | | |-...     | |-...     |-val       |-class1       | |-img5.jpg       | |-...       |-class2       | |-img6.jpg       | |-...       |-class3       | |-img7.jpg       | |-...       |-... ```    Assuming that you have installed PyTroch and TorchVision  if not  please follow the [officiall instruction](https://pytorch.org/) to install them firstly.  Intall the dependencies using cmd:  ``` sh python -m pip install -r requirements.txt --user -q ```  The code is developed and tested using pytorch 1.7.1. Other versions of pytorch are not fully tested.   | CvT-13 | 384x384    | 20M   | 16.3   | 83.0  |   | CvT-13  | 384x384    | 20M   | 16.3   | 83.3  |   bash run.sh -g 8 -t train --cfg experiments/imagenet/cvt/cvt-13-224x224.yaml  You can also modify the config paramters by the command line. For example  if you want to change the lr rate to 0.1  you can run the command:  bash run.sh -g 8 -t train --cfg experiments/imagenet/cvt/cvt-13-224x224.yaml TRAIN.LR 0.1   bash run.sh -t test --cfg experiments/imagenet/cvt/cvt-13-224x224.yaml TEST.MODEL_FILE ${PRETRAINED_MODLE_FILE}   a CLA and decorate the PR appropriately (e.g.  status check  comment). Simply follow the instructions   """;Computer Vision;https://github.com/microsoft/CvT
"""4.1 hyperparameters newed - with_restart - cosine_decay_steps  - cosine_decay_mult  4.2 proposal hyperparamters - lr_policy: ""poly"" - base_lr: 0.001 - momentum: 0.9 - momentum2: 0.999 - weight_decay: 0.0005 - with_restart: true (false will set yita = 1 fixedly) - cosine_decay_steps: 10000 (change it to observe results) - cosine_decay_mult: 2 - type: ""AdamWR""  4.3 others to take note  if you use lr_policy of step  you should take note hyperparameter of gamma  which may make loss value boomed if you use proposal value  0.1. Perhaps  you can set gamma to 0.9 instead.   """;General;https://github.com/Yagami123/Caffe-AdamW-AdamWR
"""@ikostrikov for pytorch-a3c   """;General;https://github.com/Kaixhin/NoisyNet-A3C
"""Dependencies: python>=3.6  numpy  pandas  torch>=1.3  tqdm.  ```shell $ git clone https://github.com/massquantity/DBRL.git ```  After downloading the data  unzip and put them into the `DBRL/dbrl/resources` folder. The original dataset consists  of three tables: `user.csv`  `item.csv`  `user_behavior.csv` . We'll first need to filter some users with too few interactions and merge all features together  and this is accomplished by `run_prepare_data.py`. Then we'll pretrain embeddings for every user and item by running `run_pretrain_embeddings.py` :  ```shell $ cd DBRL/dbrl $ python run_prepare_data.py $ python run_pretrain_embeddings.py --lr 0.001 --n_epochs 4 ```  You can tune the `lr` and `n_epochs` hyper-parameters to get better evaluate loss. Then we begin to train the model. Currently there are three algorithms in `DBRL`  so we can choose one of them:  ```shell $ python run_reinforce.py --n_epochs 5 --lr 1e-5 $ python run_ddpg.py --n_epochs 5 --lr 1e-5 $ python run_bcq.py --n_epochs 5 --lr 1e-5 ```  At this point  the `DBRL/resources` should contains at least 6 files:  + `model_xxx.pt`  the trained pytorch model. + `tianchi.csv`  the transformed dataset. + `tianchi_user_embeddings.npy`  the pretrained user embeddings in numpy `npy` format. + `tianchi_item_embeddings.npy`  the pretrained item embeddings in numpy `npy` format. + `user_map.json`  a json file that maps original user ids to ids used in the model. + `item_map.json`  a json file that maps original item ids to ids used in the model.       """;Reinforcement Learning;https://github.com/massquantity/DBRL
"""We tested each methods with miniImagenet dataset  which can be found in https://github.com/renmengye/few-shot-ssl-public   First  download miniImagenet data and run generate_dataset_from_pkl.py with dataroot  python generate_dataset_from_pkl --datadir ""downloaded pkl dir""  Then  you can run script file to train the model.    """;General;https://github.com/minseop-aitrics/FewshotLearning
"""* The original paper of this code is: https://arxiv.org/abs/1603.00748 * The code is mainly based on: https://github.com/carpedm20/NAF-tensorflow/ * Additionally I added the prioritized experience replay: https://arxiv.org/abs/1511.05952 * Using the OpenAI baseline implementation: https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py  Thanks openAI and Kim!    """;Reinforcement Learning;https://github.com/MathPhysSim/PER-NAF
"""````shell !pip install -U git+https://github.com/Klassikcat/project-NEXTLab-CNN-EfficientNet ````       ┖ loader.py             #: get train/valid/test dataset from directory       ┖ tfrecordMaker         #: Make TFRecord   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1kumrFpGtc3K2hfOW8VRj4-AfuQFV4HIR?usp=sharing)   """;Computer Vision;https://github.com/Klassikcat/project-NEXTLab-CNN-EfficientNet
"""The standard WGAN is available in the file WGAN_std.py  The gradient penalty WGAN is available in the file WGAN_gradientpenalty.py  ```python ```  ----------------------------   """;Computer Vision;https://github.com/dagrate/gan_network
"""- Supported platforms: Mac and Linux with Python 3.5.4  check `python --version` to make sure you have the correct version of python installed.    - If you're using [miniconda](https://docs.conda.io/en/latest/miniconda.html) (recommended) you can create an environment with `conda create --name multiagent-particle-envs python=3.5.4`    - This release requires `gym==0.10.5`  for newer versions of gym try using [jarbus's fork of this repo](https://github.com/jarbus/multiagent-particle-envs/tree/pullreq) instead  - To install  `cd` into the root directory and type `pip install -e .`  - To interactively view moving to landmark scenario (see others in ./scenarios/): `bin/interactive.py --scenario simple.py`  - To use the environments  look at the code for importing them in `make_env.py`.   """;Reinforcement Learning;https://github.com/mauricemager/multiagent-robot
"""So you can simply download the reconstructed images from here or here and then update the dataset.   """;Computer Vision;https://github.com/ShaofengZou/A-CNN-Based-Blind-Denoising-Method
"""$ mkdir MyDir <br/>  $ cd MyDir <br/>  $ wget https://github.com/Qengineering/TensorFlow_Lite_Segmentation_RPi_32/archive/refs/heads/master.zip <br/>   """;Computer Vision;https://github.com/Qengineering/TensorFlow_Lite_Segmentation_RPi_32-bit
"""1. Move SNorm_tf_keras.py to your code's dir. 2. Import layers like   `from SpectralNormalizationKeras import DenseSN  ConvSN1D  ConvSN2D  ConvSN3D` 3. Use them like normal layers (but only in discriminators mentioned in the paper).   """;Computer Vision;https://github.com/GrayXu/SpectralNormalization-TF-Keras
"""Py**T**orch **Im**age **M**odels (`timm`) is a collection of image models  layers  utilities  optimizers  schedulers  data-loaders / augmentations  and reference training / validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.  The work of many others is present here. I've tried to make sure all source material is acknowledged via links to github  arxiv papers  etc in the README  documentation  and code docstrings. Please let me know if I missed anything.   Awesome PyTorch Resources   Update ByoaNet attention modules   Add Swin Transformer models and weights from https://github.com/microsoft/Swin-Transformer   First 0.4.x PyPi release w/ NFNets (& related)  ByoB (GPU-Efficient  RepVGG  etc).   Tested with PyTorch 1.8 release. Updated CI to use 1.8.   Fix a few bugs introduced since last pypi release   A full version of the list below with source links can be found in the documentation.   PyTorch w/ single GPU single process (AMP optional)   fused&lt;name&gt; optimizers by name with NVIDIA Apex installed   Kornia - https://github.com/kornia/kornia  RepDistiller - https://github.com/HobbitLong/RepDistiller  torchdistill - https://github.com/yoshitomo-matsubara/torchdistill  PyTorch Metric Learning - https://github.com/KevinMusgrave/pytorch-metric-learning  fastai - https://github.com/fastai/fastai   My current [documentation](https://rwightman.github.io/pytorch-image-models/) for `timm` covers the basics.  [timmdocs](https://fastai.github.io/timmdocs/) is quickly becoming a much more comprehensive set of documentation for `timm`. A big thanks to [Aman Arora](https://github.com/amaarora) for his efforts creating timmdocs.  [paperswithcode](https://paperswithcode.com/lib/timm) is a good resource for browsing the models within `timm`.   * Detectron2 - https://github.com/facebookresearch/detectron2 * Segmentation Models (Semantic) - https://github.com/qubvel/segmentation_models.pytorch * EfficientDet (Obj Det  Semantic soon) - https://github.com/rwightman/efficientdet-pytorch   """;General;https://github.com/rwightman/pytorch-image-models
"""<a href=""https://giphy.com/gifs/1jaMfIL5LHFAdrjM3h""> <img width=399px src=""https://media.giphy.com/media/1jaMfIL5LHFAdrjM3h/giphy.gif"" title=""Cartpole demo""/></a>  """;General;https://github.com/JonasRSV/DQN
"""You can start a multithreaded analogy test by running:      Let's say you have a big model with 100 dimension. First  create a precomputed nn file using the command mentioned above (This is optional but saves time in multiple uses). In order to create a 60 dimension model with using the distillation method  run:    ```  $ ./fasttext skipgram -input ./your_data -output ./small_model_dim60 -dim 60 -distillFrom ./models/my_big_model_dim100.bin -precomputedNN ./nnfiles/my_big_model_dim100.nn -outputSmoothing  ```  This will train a new model that is distilled from your big model.      ```  $ ./fasttext skipgram -input ./your_data -output ./small_model_dim60 -dim 60 -distillFrom ./models/my_big_model_dim100.bin -precomputedNN ./nnfiles/my_big_model_dim100.nn -inputSmoothing ```   """;Natural Language Processing;https://github.com/Omerktn/fastText-iterative
"""```python python main.py ```   """;Computer Vision;https://github.com/SongDark/generate_normal
"""- Install PyTorch and dependencies from http://pytorch.org - Install Torch vision from the source. ```bash git clone https://github.com/pytorch/vision cd vision python setup.py install ``` - Install python libraries [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate). ```bash pip install visdom pip install dominate ``` - Clone this repo: ```bash git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix cd pytorch-CycleGAN-and-pix2pix ```   <a href=""https://github.com/yunjey/mnist-svhn-transfer"">[Minimal PyTorch]</a> (by yunjey)   <a href=""https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN"">[Mxnet]</a> (by Ldpe2G)    <a href=""https://github.com/taey16/pix2pixBEGAN.pytorch"">[Pytorch]</a> (by taey16)   - Test the model (`bash ./scripts/test_pix2pix.sh`):bash   More example scripts can be found at scripts directory.  You can download a pretrained model (e.g. horse2zebra) with the following script:  bash pretrained_models/download_cyclegan_model.sh horse2zebra   bash ./datasets/download_cyclegan_dataset.sh horse2zebra   ``` bash   bash pretrained_models/download_pix2pix_model.sh facades_label2photo   CPU/GPU (default --gpu_ids 0): set--gpu_ids -1 to use CPU mode; set --gpu_ids 0 1 2 for multi-GPU mode. You need a large batch size (e.g. --batchSize 32) to benefit from multiple GPUs.     bash ./datasets/download_cyclegan_dataset.sh dataset_name   bash ./datasets/download_pix2pix_dataset.sh dataset_name   """;Computer Vision;https://github.com/tomgillooly/geogan
"""bash run.sh train ConE wn18rr 0 1 1024 50 500 10 0.5 0.001 40000 4 -de \   bash run.sh category ConE wn18rr 0 1 1024 50 500 0.1 0.5 0.001 20000 4 -de \   Knowledge Graph Data:  - `entities.dict`: a dictionary mapping entities to unique ids  - `relations.dict`: a dictionary mapping relations to unique ids  - `train.txt`: the KGE model is trained to fit this data set  - `valid.txt`: create a blank file if no validation data is available  - `test.txt`: the KGE model is evaluated on this data set  - `relation_category.txt`: a dictionary mapping relations to their type (1-1 indicates non-hierarchical  1-M indicates hyponym  M-1 indicates hypernym)  required for ConE model  - `class_test_X.txt`: Test data for ancestor-descendant prediction task  *X*=easy: 0% inferred descendant pairs  *X*=medium: 50% inferred descendant pairs  *X*=hard: 100% inferred descendant pairs  - `lca_test_X.txt`: LCA prediction under *X*-hop is evaluated on this data set   """;General;https://github.com/snap-stanford/ConE
"""This repository contains the code we used to pre-process the data. There are files for extraction:  - `data-preprocessing/gigaword-extract.py` - `data-preprocessing/cord-extract.py` - `data-preprocessing/wiki-extract.py` - `data-preprocessing/nanc-extract.py` - `data-preprocessing/irc-extract.py`  And files for tokenising with Stanza and converting numbers:  - `data-preprocessing/tokenise.py` - `data-preprocessing/convert_nums.py`  We also include a script that reads the LDC PTB tgz file and produces our version of the PTB:  - `data-preprocessing/make-non-unk-ptb.py`  For example  to prepare the data in the same way we did  run these two commands (where `treebank_3_LDC99T42.tgz` must be downloaded from the LDC).  ``` ./data-preprocessing/make-non-unk-ptb.py --prefix ptb.std. treebank_3_LDC99T42.tgz ./data-preprocessing/make-non-unk-ptb.py --prefix ptb.rare. --no-unks treebank_3_LDC99T42.tgz ```   Install PyTorch 0.1.12_2   For data setup  run ./getdata.sh.   PyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed.   """;Sequential;https://github.com/jkkummerfeld/emnlp20lm
"""To train and evaluate a biLM  you need to provide:  * a vocabulary file * a set of training files * a set of heldout files  The vocabulary file is a a text file with one token per line.  It must also include the special tokens `<S>`  `</S>` and `<UNK>` (case sensitive) in the file.  <i>IMPORTANT</i>: the vocabulary file should be sorted in descending order by token count in your training data.  The first three lines should be the special tokens (`<S>`  `</S>` and `<UNK>`)  then the most common token in the training data  ending with the least common token.  <i>NOTE</i>: the vocabulary file used in training may differ from the one use for prediction.  The training data should be randomly split into many training files  each containing one slice of the data.  Each file contains pre-tokenized and white space separated text  one sentence per line. Don't include the `<S>` or `</S>` tokens in your training data.  All tokenization/normalization is done before training a model  so both the vocabulary file and training files should include normalized tokens. As the default settings use a fully character based token representation  in general we do not recommend any normalization other then tokenization.  Finally  reserve a small amount of the training data as heldout data for evaluating the trained biLM.   To run the image  you must use nvidia-docker  because this repository requires GPUs. ``` sudo nvidia-docker run -t allennlp/bilm-tf:training-gpu ```   Install python version 3.5 or later  tensorflow version 1.2 and h5py:  ``` pip install tensorflow-gpu==1.2 h5py python setup.py install ```  Ensure the tests pass in your environment by running: ``` python -m unittest discover tests/ ```   To pretrain models  please use bin/train_elmo_wikilink.py. Note that this implementation requires installing TensorFlow in your environment.   You may also find it easier to use the version provided in Tensorflow Hub if you just like to make predictions.   See the instructions above for using the output from Step #4 in downstream models.   As a result of the training method (see above)  the LSTMs are stateful  and carry their state forward from batch to batch. Consequently  this introduces a small amount of non-determinism  expecially for the first two batches.   Simple methods like average and max pooling of the word level ELMo representations across sentences works well  often outperforming supervised methods on benchmark datasets. See ""Evaluation of sentence embeddings in downstream and linguistic probing tasks""  Perone et al  2018 [arxiv link](https://arxiv.org/abs/1806.06259).    """;Natural Language Processing;https://github.com/mingdachen/bilm-tf
"""discrimator_real_dim)  name=”real_discrminator_input” )   name = “generator_input_z”)   """;Computer Vision;https://github.com/yuanxiaoyu0402/deep
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/nachiketaa/bert
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/solapark/da_yolo
"""Please refer to [SETUP.md](docs/TUTORIALS/SETUP.md)  [SOT_SETUP.md](docs/TUTORIALS/SOT_SETUP.md)  SOT_SETUP.md: instructions for setting-up   VOS_SETUP.md: instructions for setting-up   ```Bash #: demo with web camera python3 ./demo/main/video/sot_video.py --config 'experiments/siamfcpp/test/vot/siamfcpp_alexnet.yaml' --device cuda --video ""webcam""   #: demo with video file  and dump result into video file (optional) python3 ./demo/main/video/sot_video.py --config 'experiments/siamfcpp/test/vot/siamfcpp_alexnet.yaml' --device cuda --video $video_dir/demo.mp4 --output $dump_path/result.mp4  #: demo with extracted image files  and dump result into image files (optional) python3 ./demo/main/video/sot_video.py --config 'experiments/siamfcpp/test/vot/siamfcpp_alexnet.yaml' --device cuda --video $video_dir/*.jpg --output $dump_dir ```   """;Computer Vision;https://github.com/MegviiDetection/video_analyst
"""Install PyTorch 0.2   For data setup  run ./getdata.sh.   PyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed.   ``` @InProceedings{P18-1221    author = 	""Parvez  Md Rizwan 		and Chakraborty  Saikat 		and Ray  Baishakhi 		and Chang  Kai-Wei""    title = 	""Building Language Models for Text with Named Entities""    booktitle = 	""Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)""    year = 	""2018""    publisher = 	""Association for Computational Linguistics""    pages = 	""2373--2383""    location = 	""Melbourne  Australia""    url = 	""http://aclweb.org/anthology/P18-1221"" } ```    """;General;https://github.com/uclanlp/NamedEntityLanguageModel
"""Start to train  type the command as follow:    If you don't own one GPU  remove the --cuda option  but you had better get one  becaue it is faster than CPU !   Please firstly install [Anaconda](https://anaconda.org)  if you not understand how to install whole procedures on Ubuntu system  you can take this [link](https://stackoverflow.com/questions/28852841/install-anaconda-on-ubuntu-via-command-line) as reference.   After finishing  you need to create an Anaconda environment using the environment.yml file.  ``` conda env create -f environment.yml ```  After you create the environment  activate it. ``` source activate hw1 ```  Our current implementation supports GPU Card (Such as GTX-1060 up)  you need to have one GPU (like GTX-1080-ti) and have CUDA libraries installed on your machine.   **(Don't use VMs running on Ubuntu Operation  because VMs can not get the real GPU card)**   """;General;https://github.com/Qi-Xian/color-transfer_HW1
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/appcoreopc/berty
"""This code is built on top of [Dassl.pytorch](https://github.com/KaiyangZhou/Dassl.pytorch). Please follow the instructions provided in https://github.com/KaiyangZhou/Dassl.pytorch to install the `dassl` environment  as well as to prepare the datasets  PACS and OfficeHome. The five random labeled-unlabeled splits can be downloaded at the following links: [pacs](https://drive.google.com/file/d/1PSliZDI9D-_Wrr3tfRzGVtN2cpM1892p/view?usp=sharing)  [officehome](https://drive.google.com/file/d/1hASLWAfkf4qj-WXU5cx9uw9rQDsDvSyO/view?usp=sharing). The splits need to be extracted to the two datasets' folders. Assume you put the datasets under the directory `$DATA`  the structure should look like ``` $DATA/ |–– pacs/ |   |–– images/ |   |–– splits/ |   |–– splits_ssdg/ |–– office_home_dg/ |   |–– art/ |   |–– clipart/ |   |–– product/ |   |–– real_world/ |   |–– splits_ssdg/ ```  The style augmentation is based on [AdaIN](https://arxiv.org/abs/1703.06868) and the implementation is based on this code https://github.com/naoto0804/pytorch-AdaIN. Please download the weights of the decoder and the VGG from https://github.com/naoto0804/pytorch-AdaIN and put them under a new folder `ssdg-benchmark/weights`.   """;General;https://github.com/KaiyangZhou/ssdg-benchmark
"""Follow the instructions in `code/baselines/README.md`   ```conda env create -f environment.yml```   cd code/  conda activate bayesianrex   cd code/scripts/  bash strip_to_embedding_networks.sh ../../pretrained_networks/ breakout_pretrained.params   cd code/   cd code/   cd code/scripts/   ``` python run_test.py --env_id BreakoutNoFrameskip-v4 --env_type atari --model_path ../models/breakout/checkpoints/03600 --record_video --episode_count 1 --render ```  You can omit the last flag --record_video. When it is turned on  then the videos will be recorded in a videos/ directory below the current directory. If --render is omitted then it will simply print returns to the command line.    """;Reinforcement Learning;https://github.com/dsbrown1331/bayesianrex
"""1.Download the converted pretrained vgg16_reduced model here.  2.Prepare VOC datasets and generate .rec files by using tools/prepare_pascal.sh  3.Set TDM or DSSD mode in function get_config from symbol/symbol_factory.py.By default  DSSD mode is used please set all configs by your needs.   4.start training  ``` python train.py ``` or choice a bash file which provide in ./script/ to run some default parameters setting such like try the two stage training strategy. ``` bash scripts/stage1_dssd_train_res_voc.sh ```    1. Download model  available at [here](https://pan.baidu.com/s/1htX4Egc)  and place it in the model folder.  2. run demo.py    """;Computer Vision;https://github.com/MTCloudVision/mxnet-dssd
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/Shraddha2013/customyolo
"""The ""unbatched"" version should not be used anymore. If you've downloaded this code previously  please update it immediately to  the new version. The old version included a bug!  If you're looking for a pytorch implementation we recommend https://github.com/mseitzer/pytorch-fid  Requirements: TF 1.1+  Python 3.x   Improved WGAN (WGAN-GP) implementation forked from https://github.com/igul222/improved_wgan_training   """;General;https://github.com/bioinf-jku/TTUR
"""R-CNN is a state-of-the-art visual object detection system that combines bottom-up region proposals with rich features computed by a convolutional neural network. At the time of its release  R-CNN improved the previous best detection performance on PASCAL VOC 2012 by 30% relative  going from 40.9% to 53.3% mean average precision. Unlike the previous best results  R-CNN achieves this performance without using contextual rescoring or an ensemble of feature types.  R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1311.2524) and will appear in a forthcoming CVPR 2014 paper.   0. Download the training  validation  test data and VOCdevkit:    <pre>   wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtest_06-Nov-2007.tar   wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCdevkit_08-Jun-2007.tar   </pre>  0. Extract all of these tars into one directory  it's called `VOCdevkit`.     <pre>   tar xvf VOCtrainval_06-Nov-2007.tar   tar xvf VOCtest_06-Nov-2007.tar   tar xvf VOCdevkit_08-Jun-2007.tar   </pre>  0. It should have this basic structure:    <pre>   VOCdevkit/                           % development kit   VOCdevkit/VOCcode/                   % VOC utility code   VOCdevkit/VOC2007                    % image sets  annotations  etc.   ... and several other directories ...   </pre>  0. I use a symlink to hook the R-CNN codebase to the PASCAL VOC dataset:    <pre>   ln -sf /your/path/to/voc2007/VOCdevkit /path/to/rcnn/datasets/VOCdevkit2007   </pre>   0. **Prerequisites**    0. MATLAB (tested with 2012b on 64-bit Linux)   0. Caffe's [prerequisites](http://caffe.berkeleyvision.org/installation.html#prequequisites) 0. **Install Caffe** (this is the most complicated part)   0. R-CNN has been checked for compatability against Caffe release v0.999. *It has not been updated to work with the current Caffe master.*   0. Download [Caffe v0.999](https://github.com/BVLC/caffe/archive/v0.999.tar.gz)   0. Follow the [Caffe installation instructions](http://caffe.berkeleyvision.org/installation.html)   0. Let's call the place where you installed caffe `$CAFFE_ROOT` (you can run `export CAFFE_ROOT=$(pwd)`)   0. **Important:** Make sure to compile the Caffe MATLAB wrapper  which is not built by default: `make matcaffe`   1. **Important:** Make sure to run `cd $CAFFE_ROOT/data/ilsvrc12 && ./get_ilsvrc_aux.sh` to download the ImageNet image mean 0. **Install R-CNN**   0. Get the R-CNN source code by cloning the repository: `git clone https://github.com/rbgirshick/rcnn.git`   0. Now change into the R-CNN source code directory: `cd rcnn`   0. R-CNN expects to find Caffe in `external/caffe`  so create a symlink: `ln -sf $CAFFE_ROOT external/caffe`   0. Start MATLAB (make sure you're still in the `rcnn` directory): `matlab`   0. You'll be prompted to download the [Selective Search](http://disi.unitn.it/~uijlings/MyHomepage/index.php#page=projects1) code  which we cannot redistribute. Afterwards  you should see the message `R-CNN startup done` followed by the MATLAB prompt `>>`.   0. Run the build script: `>> rcnn_build()` (builds [liblinear](http://www.csie.ntu.edu.tw/~cjlin/liblinear/) and [Selective Search](http://www.science.uva.nl/research/publications/2013/UijlingsIJCV2013/)). Don't worry if you see compiler warnings while building liblinear  this is normal on my system.   0. Check that Caffe and MATLAB wrapper are set up correctly (this code should run without error): `>> key = caffe('get_init_key');` (expected output is key = -2)   0. Download the model package  which includes precompute models (see below).  **Common issues:** You may need to set an `LD_LIBRARY_PATH` before you start MATLAB. If you see a message like ""Invalid MEX-file '/path/to/rcnn/external/caffe/matlab/caffe/caffe.mexa64': libmkl_rt.so: cannot open shared object file: No such file or directory"" then make sure that CUDA and MKL are in your `LD_LIBRARY_PATH`. On my system  I use:      export LD_LIBRARY_PATH=/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64      Start MATLAB in the rcnn directory   """;Computer Vision;https://github.com/rbgirshick/rcnn
"""For this project  the unity ML [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment is used . The agent is trained to play [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis)     [![Unity ML-Agents Tennis Environment](https://video.udacity-data.com/topher/2018/May/5af7955a_tennis/tennis.png)Unity ML-Agents Tennis Environment](https://classroom.udacity.com/nanodegrees/nd893-ent/parts/0ba70f95-986b-400c-9b2e-59366cca2a49/modules/83e3a45a-a815-4dca-82bc-c6f1b46ac8cd/lessons/c03538e3-4024-41c5-9baa-3be2d91f250c/concepts/da65c741-cdeb-4f34-bb56-d8977385596e#)    In this environment  two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net  it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds  it receives a reward of -0.01. Thus  the goal of each agent is to keep the ball in play.  The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own  local observation. Two continuous actions are available  corresponding to movement toward (or away from) the net  and jumping.  The task is episodic  and in order to solve the environment  your agents must get an average score of +0.5 (over 100 consecutive episodes  after taking the maximum over both agents). Specifically   - After each episode  we add up the rewards that each agent received (without discounting)  to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores. - This yields a single **score** for each episode.  The environment is considered solved  when the average (over 100 episodes) of those **scores** is at least +0.5.   Unity Environment is already built and made available as part of Deep Reinforcement Learning course at Udacity.  1. Download the environment from one of the links below.  You need only select the environment that matches your operating system:  2. Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip) 3. Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip) 4. Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip) 5. Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip) 6. Place the file in the DRLND GitHub repository  in the `p2_continuous-control/` folder  and unzip (or decompress) the file.   Then  place the file in the `p3_collab-compet/` folder in the DRLND GitHub repository  and unzip (or decompress) the file.  (*For Windows users*) Check out [this link](https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64) if you need help with determining if your computer is running a 32-bit version or 64-bit version of the Windows operating system.  (*For AWS*) If you'd like to train the agent on AWS (and have not [enabled a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md))  then please use [this link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux_NoVis.zip) to obtain the ""headless"" version of the environment. You will **not** be able to watch the agent without enabling a virtual screen  but you will be able to train the agent. (*To watch the agent  you should follow the instructions to [enable a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md)  and then download the environment for the **Linux** operating system above.*)     To set up your python environment to run the code in this repository  follow the instructions below.  1. Create (and activate) a new environment with Python 3.6.     - **Linux** or **Mac**:     ```    conda create --name drlnd python=3.6    source activate drlnd    ```     - **Windows**:     ```    conda create --name drlnd python=3.6     activate drlnd    ```  2. Follow the instructions in [this repository](https://github.com/openai/gym) to perform a minimal install of OpenAI gym.     - Next  install the **classic control** environment group by following the instructions [here](https://github.com/openai/gym#classic-control).    - Then  install the **box2d** environment group by following the instructions [here](https://github.com/openai/gym#box2d).  3. Clone the repository (if you haven't already!)  and navigate to the `python/` folder. Then  install several dependencies.  ``` git clone https://github.com/udacity/deep-reinforcement-learning.git cd deep-reinforcement-learning/python pip install . ```  1. Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment.  ``` python -m ipykernel install --user --name drlnd --display-name ""drlnd"" ```  1. Before running code in a notebook  change the kernel to match the `drlnd` environment by using the drop-down `Kernel` menu.  [![Kernel](https://user-images.githubusercontent.com/10624937/42386929-76f671f0-8106-11e8-9376-f17da2ae852e.png)](https://user-images.githubusercontent.com/10624937/42386929-76f671f0-8106-11e8-9376-f17da2ae852e.png)     """;Reinforcement Learning;https://github.com/Zorrorulz/MultiAgentDDPG-Tennis
"""- Dont sample from a Uniform distribution  ![cube](images/cube.png ""Cube"")  - Sample from a gaussian distribution  ![sphere](images/sphere.png ""Sphere"")  - When doing interpolations  do the interpolation via a great circle  rather than a straight line from point A to point B - Tom White's [Sampling Generative Networks](https://arxiv.org/abs/1609.04468) ref code https://github.com/dribnet/plat has more details    - Label Smoothing  i.e. if you have two target labels: Real=1 and Fake=0  then for each incoming sample  if it is real  then replace the label with a random number between 0.7 and 1.2  and if it is a fake sample  replace it with 0.0 and 0.3 (for example).   - Salimans et. al. 2016 - make the labels the noisy for the discriminator: occasionally flip the labels when training the discriminator   - Experience Replay   - Keep a replay buffer of past generations and occassionally show them   - Keep checkpoints from the past of G and D and occassionaly swap them out for a few iterations - All stability tricks that work for deep deterministic policy gradients - See Pfau & Vinyals (2016)   - optim.Adam rules!   - See Radford et. al. 2015 - Use SGD for discriminator and ADAM for generator   - if you have labels available  training the discriminator to also classify the samples: auxillary GANs   - Provide noise in the form of dropout (50%). - Apply on several layers of our generator at both training and test time - https://arxiv.org/pdf/1611.07004v1.pdf    """;General;https://github.com/linxi159/GAN-training-tricks
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.    GPUs: K80 ($0.14/hr)  T4 ($0.11/hr)  V100 ($0.74/hr) CUDA with Nvidia Apex FP16/32     Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [Notebook](https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb) <a href=""https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a> * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) << highly recommended * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart)  ![Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/yolov3?logo=docker) * [A TensorRT Implementation of YOLOv3 and YOLOv4](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp)     """;Computer Vision;https://github.com/xinshasha/YoloV3
"""the need to let the user interact freely with the environment  in a build-your-own-adventure style.    """;Natural Language Processing;https://github.com/macco3k/deepstories
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/Eltomad/insightface
"""You can download needed packages via pip using the `requirements.txt` file:  ```python   pip3 install -r requirements.txt ```   - [Jetson Xavier NX](https://developer.nvidia.com/embedded/learn/get-started-jetson-xavier-nx-devkit) with [JetPack 4.4](https://developer.nvidia.com/jetpack-sdk-44-archive) (CUDA 10.2  TensorRT 7.1.3  cuDNN 8.0) - [Install Tensorflow 1.15.3](https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html)     - [Jetson Xavier NX](https://developer.nvidia.com/embedded/learn/get-started-jetson-xavier-nx-devkit) with [JetPack 4.4](https://developer.nvidia.com/jetpack-sdk-44-archive) (CUDA 10.2  TensorRT 7.1.3  cuDNN 8.0) - [Install Tensorflow 1.15.3](https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html)     """;Computer Vision;https://github.com/preste-nakam/AI_whiteboard
"""Environment GYM : https://gym.openai.com/   """;Reinforcement Learning;https://github.com/Gouet/PPO-gym
"""`pip install -r requirements.txt`  cd datasets   bash preprocess.sh $IMAGENET_TRAIN_DIR $PREPROCESSED_DATA_DIR   Consecutive category morphing movies: - (5x5 panels 128px images) https://www.youtube.com/watch?v=q3yy5Fxs7Lc   - (10x10 panels 128px images) https://www.youtube.com/watch?v=83D_3WXpPjQ   (If you want to use pretrained models for the image generation  please download the model from [link](https://drive.google.com/drive/folders/1xZoL48uFOCnTxNGdknEYqE5YX0ZyoUej?usp=sharing) and set the `snapshot` argument to the path to the downloaded pretrained model file (.npz).)   """;General;https://github.com/pfnet-research/sngan_projection
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   """;Computer Vision;https://github.com/latentgnn/maskrcnn-benchmark-latentgnn
"""Download the dataset and unzip it into your current working directory. This will create a directory called “maps” with the following structure:   """;Computer Vision;https://github.com/AquibPy/Pix2Pix-Conditional-GANs
"""If you think the above implementation is a little bit difficult to understand  or simply want to play with some easy experiments  we have also provided a toy example to help you get start with. This toy example `vgg16_cifar_toy.py` includes a VGG-16 model evaluated on CIFAR-100 dataset  and it provides with two flags only: `gpu`: the training GPU ID; and `mode`: shape-adaptor  and human  representing human-defined network and shape adaptor network  respectively. We hard-coded the network structure  shape adaptor hyper-parameters  and removed all other redundant code to maximise readability.  Different from the full version in  `model_training.py` where we insert shape adaptors uniformly across all network layers  shape adaptors in this toy example are attached based on the human-defined resizing layer locations (for readability purpose  no prominent performance change).      """;Computer Vision;https://github.com/lorenmt/shape-adaptor
"""Before you proceed  pip install -r ./requirements.txt   """;Natural Language Processing;https://github.com/alexandra-chron/ntua-slp-wassa-iest2018
"""**Adaptive Notes Generator** *is a tool that helps us attend online classes effectively:star_struck:. Due to the Online class culture  taking notes in pen and paper is not a good idea  the only options left are to click screenshots or struggle to note down everything in your notebook:unamused:. Our application will make your life easier  once a meeting video:film_projector: is provided  we will create the notes that will save you time:stopwatch: of research and gathering resources. We will divide your meeting into useful segments and add additional data to make it easy to understand any concept.:bookmark_tabs::bookmark_tabs:*   """;Sequential;https://github.com/chakravarthi-v/Polaroid-1
"""GPUs: K80 ($0.14/hr)  T4 ($0.11/hr)  V100 ($0.74/hr) CUDA with Nvidia Apex FP16/32     Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [Notebook](https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb) <a href=""https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a> * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) << highly recommended * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart)  ![Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/yolov3?logo=docker) * [A TensorRT Implementation of YOLOv3 and YOLOv4](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp)     """;Computer Vision;https://github.com/Jiachenyin1/Yolov3-U-attention-permuting
"""FoveaBox is an accurate  flexible and completely anchor-free object detection system for object detection framework  as presented in our paper [https://arxiv.org/abs/1904.03797](https://arxiv.org/abs/1904.03797): Different from previous anchor-based methods  FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility  and (b) producing category-agnostic bounding box for each position that potentially contains an object.  <div align=""center"">   <img src=""demo/foveabox.jpg"" width=""300px"" />   <p>FoveaBox detection process.</p> </div>   This FoveaBox implementation is based on [mmdetection](https://github.com/open-mmlab/mmdetection). Therefore the installation is the same as original mmdetection.  Please check [INSTALL.md](INSTALL.md) for installation instructions.    ./tools/dist_train.sh ${CONFIG_FILE} ${GPU_NUM} [optional arguments]   """;Computer Vision;https://github.com/marinarierav-uab/foveabox
"""We also need the to install [pycocotools](https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools)  on which the CocoDataset torchvision module is dependent.   pycocotool requires Cython  so we'll install that first  with:  ```bash pip3 install Cython ```  Then we can install pycocotools itself with:  ```bash pip3 install pycocotools ```   Prerequisite steps:  Download the COCO Detection Dataset  Install pycocotools   A copy of this project can be cloned from here - but don't forget to follow the prerequisite steps below.   If you have limited GPU memory  you may need to reduce your batch size.     num_workers: 4        #: This will depend on your CPU  you probably have at least 4 cores     - name: COCO Train 2017           #: Human-readable name         - name: COCO Image            #: Human-readable name             - name: CocoDetection                      - name: COCO Label            #: Human-readable name   dataset name         - name: COCO Image             - name: CocoDetection         - name: COCO Label             - name: SourcePointer    name: YOLO                              #: Select YOLO   Additionally  our source code for [YOLO](https://github.com/Deeplodocus/deeplodocus/blob/master/deeplodocus/app/models/yolo.py) and [Darknet](https://github.com/Deeplodocus/deeplodocus/blob/master/deeplodocus/app/models/darknet.py) can be found on GitHub.     name: ObjectLoss                        #: Name of the Python class to use     name: BoxLoss                           #: Name of the Python class to use     name: ClassLoss                         #: Name of the Python class to use   name: ""Adam""   !!! note      name: Train Transform Manager       - config/transformers/output.yaml     #: Path to output transformer       - config/transformers/output.yaml     #: Path to output transformer   name: Transformer for COCO input         name: reformat_pointer         name: resize   name: Output Transformer       name: Concatenate       name: NonMaximumSuppression       name: Visualize   """;Computer Vision;https://github.com/Deeplodocus/COCO-with-YOLO
"""_Accompanying repository for the paper [Aspect-Controlled Neural Argument Generation](https://aclanthology.org/2021.naacl-main.34/)._  We rely on arguments in our daily lives to deliver our opinions and base them on evidence  making them more convincing  in turn. However  finding and formulating arguments can be challenging.  To tackle this challenge  we trained a language model (based on the CTRL by  [Keskar et al. (2019)](https://arxiv.org/abs/1909.05858)) for argument generation that can be controlled on a fine-grained  level to generate sentence-level arguments for a given topic  stance  and aspect.  We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset  with 5 032 arguments annotated with aspects. We release this dataset  as well as the training data for the argument generation model  its weights  and the arguments generated with the model.  The following figure shows how the argument generation model was trained:  ![Image description](arg_gen_pipeline.png)  (1) We gather several million documents for eight different topics from two large data sources.  All sentences are classified into pro-  con-  and non-arguments. We detect aspects of all arguments with a model trained on a novel dataset and concatenate arguments with the same topic  stance  and aspect into training documents.  (2) We use the collected classified data to condition the CTRL model on the topics  stances  and aspects of all gathered arguments. (3) At inference  passing the control code  _[Topic]_ _[Stance]_ _[Aspect]_ will generate an argument that follows these commands.   In order to prepare training documents and fine-tune the model  you can use the _prepare_documents.py_ as described in  [I. Use our pipeline (with ArgumenText API)  step d.](#d-prepare-training-documents)  if you keep your classified data in the following format:  - The file should be named _merged.jsonl_ and located in the directory _training_data/[INDEX_NAME]/[TOPIC_NAME]/processed/_   where [INDEX_NAME] is the data source from where the samples were gathered and [TOPIC_NAME] the name of the respective search query for this data. - Each line represents a training sample in the following format:              {""id"": id of the sample  starting with 0 (int)  ""stance"": ""Argument_against"" or ""Argument_for""  depending on the stance (string)  ""sent"": The argument sentence (string)  ""aspect_string"": A list of aspects for this argument (list of string)}      The _prepare_documents.py_ appends all arguments that have the same topic  stance  and (stemmed) aspect to a training document:       python prepare_documents.py --max_sents [MAX_SENTS] --topic [TOPIC_NAME] --index [INDEX_NAME] --max_aspect_cluster_size [MAX_ASPECT_CLUSTER_SIZE] --min_aspect_cluster_size [MIN_ASPECT_CLUSTER_SIZE]  [MAX_SENTS] sets the maximum number of arguments to use (evenly devided between pro and con arguments if possible) and [MIN_ASPECT_CLUSTER_SIZE]/[MAX_ASPECT_CLUSTER_SIZE] sets the min/max number of allowed arguments to append for a  single training document. The final documents are stored in folder _training_data/[INDEX_NAME]/[TOPIC_NAME]/final/_.  The script _prepare_all_documents.sh_ can be used to automate the process.  Finally  to create training sequences from the documents and start fine-tuning the model  please download our fine-tuned  weights (see [Download section](#downloads)) and follow  [B. Use given training data to reproduce/fine-tune the model](#b-use-given-training-data-to-reproducefine-tune-the-model)   Steps 4-5.  _IMPORTANT_: In addition to the training documents  a file with all control codes based on the training  documents is created at _training_data/[INDEX_NAME]/[TOPIC_NAME]/generation_data/control_codes.jsonl_. This file holds all control codes to generate arguments from after fine-tuning has finished.    The code was tested with `Python3.6`. Install all requirements with           pip install -r requirements.txt       and follow the instructions in the [original Readme at _Usage_](README_original.md#usage)  _Step 1 and 2_.   The Argument Aspect Detection dataset can be downloaded from here   pro-/con-/non-Arguments. The following command starts the classification:   The following command starts the aspect detection:   In the following  we describe three approaches to use the aspect-controlled neural argument generation model:  [A. Use model for generation only](#a-use-model-for-generation-only)  [B. Use available training data to reproduce/fine-tune the model](#b-use-given-training-data-to-reproducefine-tune-the-model)  [C. Use your own data to fine-tune a new aspect-controlled neural argument generation model](#c-use-your-own-data-to-fine-tune-a-new-aspect-controlled-neural-argument-generation-model)   In order to generate arguments  please first download the weights for the models (download script at _scripts/download_weights.sh_).  Run the model via `python generation.py --model_dir reddit_seqlen256_v1` for the model trained on Reddit-comments data or `python generation.py --model_dir cc_seqlen256_v1` for the model trained on Common-Crawl data.  After loading is complete  type in a control code  for example `nuclear energy CON waste`  to generate arguments  that follow this control code. To get better results for the first generated argument  you can end the control code with  a period or colon (""."" or "":""). For more details  please refer to the paper.   _Note_: Allowed control codes for each topic and data source can be found in the _training_data_ folder.   In order to fine-tune the model as we have done in our work  please follow these steps:  1. Download the pre-trained weights from the original paper  ([original Readme at _Usage_](README_original.md#usage)  _Step 3_).  2. Download the training data (see [Downloads section](#downloads). You need the file _reddit_training_data.7z_ or _cc_training_data.7z_.  Depending on the source (cc or reddit)  put the archives either into folder _training_data/common-crawl-en/_ or _training_data/redditcomments-en/_  and unzip via:          7za x [FILENAME].7z  3. To reproduce the same training documents from the training data as we used for fine-tuning  please use the script at _training_utils/pipeline/prepare_documents_all.sh_ and adapt the _INDEX_ parameter. Depending on your hardware  the training document generation can take an hour or more to compute.  4. Lastly  TFRecords need to be generated from all training documents. To do so  please run:          python make_tf_records_multitag.py --files_folder [FOLDER] --sequence_len 256               [FOLDER] needs to point to the folder of the training documents        e.g. _training_data/common-crawl-en/abortion/final/_. After generating  the number of      training sequences generated for this specific topic is printed. Use this to determine the      number of steps the model should be trained on. The TFRecords are stored in folder _training_utils_.  5. Train the model:          python training.py --model_dir [WEIGHTS FOLDER] --iterations [NUMBER OF TRAINING STEPS]              The model takes the generated TFRecords automatically from the _training_utils_ folder.     Please note that the weights in [WEIGHTS FOLDER] will be overwritten. For generation with     the newly fine-tuned model  follow the instructions in ""_A. Use model for generation only_"".   To ease the process of gathering your own training data  we add our implementation of the pipeline described in our publication (see [I. Use our pipeline (with ArgumenText API)](#i-use-our-pipeline-with-argumentext-api)). To label sentences as arguments and to identify their stances and aspects  we use the [ArgumenText-API](https://api.argumentsearch.com). Alternatively  you can also train your own models  (see [II. Create your own pipeline (without ArgumenText API)](#ii-create-your-own-pipeline-without-argumentext-api)).   Please [request](https://api.argumentsearch.com/en/api_registration) a userID and apiKey for the  [ArgumenText-API](https://api.argumentsearch.com). Write both id and key in the respective constants at _training_utils/pipeline/credentials.py_.   """;Natural Language Processing;https://github.com/UKPLab/controlled-argument-generation
"""The repository contains the entire project (including all the preprocessing) for one-stage space-time video super-resolution with Zooming Slow-Mo.  Zooming Slow-Mo is a recently proposed joint video frame interpolation (VFI) and video super-resolution (VSR) method  which directly synthesizes an HR slow-motion video from an LFR  LR video. It is going to be published in [CVPR 2020](http://cvpr2020.thecvf.com/). The most up-to-date paper with supplementary materials can be found at [arXiv](https://arxiv.org/abs/2002.11616).  In Zooming Slow-Mo  we firstly temporally interpolate features of the missing LR frame by the proposed feature temporal interpolation network. Then  we propose a deformable ConvLSTM to align and aggregate temporal information simultaneously. Finally  a deep reconstruction network is adopted to predict HR slow-motion video frames. If our proposed architectures also help your research  please consider citing our paper.  Zooming Slow-Mo achieves state-of-the-art performance by PSNR and SSIM in Vid4  Vimeo test sets.  ![framework](./dump/framework.png)   The [test.py](codes/test.py) script also provides modes for evaluation on the following test sets: `Vid4`  `SPMC`  etc. We evaluate PSNR and SSIM on the Y-channels in YCrCb color space. The commands are the same with the ones above. All you need to do is the change the data_mode and corresponding path of the standard test set.   1. Download the original training + test set of `Vimeo-septuplet` (82 GB).  ```Shell wget http://data.csail.mit.edu/tofu/dataset/vimeo_septuplet.zip apt-get install unzip unzip vimeo_septuplet.zip ```  2. Split the `Vimeo-septuplet` into a training set and a test set  make sure you change the dataset's path to your download path in script  also you need to run for the training set and test set separately:  ```Shell cd $ZOOMING_ROOT/codes/data_scripts/sep_vimeo_list.py ```  This will create `train` and `test` folders in the directory of **`vimeo_septuplet/sequences`**. The folder structure is as follows:  ``` vimeo_septuplet ├── sequences     ├── 00001         ├── 0266             ├── im1.png             ├── ...             ├── im7.png         ├── 0268...     ├── 00002... ├── readme.txt ├──sep_trainlist.txt ├── sep_testlist.txt ```  3. Generate low resolution (LR) images. You can either do this via MATLAB or Python (remember to configure the input and output path):  ```Matlab #: In Matlab Command Window run $ZOOMING_ROOT/codes/data_scripts/generate_LR_Vimeo90K.m ```  ```Shell python $ZOOMING_ROOT/codes/data_scripts/generate_mod_LR_bic.py ```  4. Create the LMDB files for faster I/O speed. Note that you need to configure your input and output path in the following script:  ```Shell python $ZOOMING_ROOT/codes/data_scripts/create_lmdb_mp.py ```  The structure of generated lmdb folder is as follows:  ``` Vimeo7_train.lmdb ├── data.mdb ├── lock.mdb ├── meta_info.txt ```   Install the required packages: `pip install -r requirements.txt`  First  make sure your machine has a GPU  which is required for the DCNv2 module.  1. Clone the Zooming Slow-Mo repository. We'll call the directory that you cloned Zooming Slow-Mo as ZOOMING_ROOT.  ```Shell git clone --recursive https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020.git ```  2. Compile the DCNv2:  ```Shell cd $ZOOMING_ROOT/codes/models/modules/DCNv2 bash make.sh         #: build python test.py    #: run examples and gradient check ```  Please make sure the test script finishes successfully without any errors before running the following experiments.   cd $ZOOMING_ROOT/codes   We also write the above commands to a Shell script  so you can directly run:   bash zsm_my_video.sh   cd $ZOOMING_ROOT/codes   <table>   <thead>     <tr>       <td>Input&nbsp;&nbsp;&nbsp;&nbsp;</td>       <td>Output</td>     </tr>   </thead>   <tr>     <td colspan=""2"">       <a href=""https://youtu.be/8mgD8JxBOus"">         <img src=""dump/demo720.gif"" alt=""Demo GIF"">         </img>       </a>     </td>   </tr> </table>   """;Computer Vision;https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020
"""Welcome to the first ""Norwegian Blue Parrot"" project article. For each AI project  I write the code and show you how it works. You can test it using a mobile phone  tablet  and laptop (on the website  https://nbp3-webclient-2020.web.app/). Furthermore  you can see for yourself the effectiveness and the shortcoming of each AI model.  <img src=""https://nbp3-webclient-2020.web.app/image/undraw_true_love_cy8x.svg"" width=""60%"" style=""margin:2rem;"" />  I have developed dozens of AI projects  and with each AI model  I learn a bit more insights  into the world of Artificial Intelligence.  The audience or point of view  (POV) for demystifying AI is primary for AI enthusiasts and friends with a curious mind. However  I am a full-stack programmer  a solution architect  and an AI scientist  and therefore  I will not shy away from the math and the coding  but I will remain steadfast to the primary POV.  Before digging into the technicalities  we will have fun test-driving  the AI model. After we have a firm grasp on ""what"" we are trying to demystify  we will travel a full journey from coding to gathering the data. We will take a break to understand the particular data biases   both the conscious biases and the unforeseen consequences. We will reach the journey conclusion at the use-cases  i.e.  what is the purpose of this AI project  and what other possibilities could this AI model can be used either ethically or  feloniously.  > - The domain experts or the AI scientists do not  create the ""test"" data set. Unlike the ""valid "" the ""test"" is not part of the AI model development process. - It is not an AI Kaggle's competition (https://www.fast.ai/2017/11/13/validation-sets/); therefore  the ""test"" is to validate the overall project goal and not limited to the AI model testing. Refer to the ""Objective #4"" bullet-point. - The ""test"" data set is a sample of the real-world photos  uses during the user-testing phase. It can be any pictures  such as  cat dress up as a dog  wolf  selfie  tree  or hamburger. The salient point is that the domain expert or the AI scientists would not have thought of using these pictures. - The purpose of the ""test"" is exposing the AI model intentional biases  and adventitious biases. - The ""test"" will reveal the model generalization  outside the ""farm"" setting  e.g.  a picture of a dog walking in a city  an indoor pig  or a sculpture of a horse. - As relevant as the ""test"" to the project goal  the result does not discredit the model calculated accuracy using the ""valid"" data set.  ![test data set image](https://nbp3-webclient-2020.web.app/image/test.jpg)  9.  - The ""train data-image set"" is for the model to learn  and the ""valid data-image set"" is for the model to compare for accuracy. - Rachel Thomas wrote an excellent article  ""How (and why) to create a good validation set "" on how to choose a valid data set. - A rule of thumb is setting aside 20%  of the data-images for the validation set. - Some of Kaggle's competitions use ""valid "" and ""test"" data set interchangeably. In this series  the ""test"" is different from the ""valid"" data set.  ![train and valid data set imge](https://nbp3-webclient-2020.web.app/image/valid3.jpg)  8.  The ""k2fa"" AI project is for kids to learn about farm animals. Think of ""k2fa""  as kids wandering through a farm and naming the animal that they see.  More concisely  ""k2fa"" is Deep Learning Convolutional Neural Network (CNN) image classification model.  Before charging ahead with an in-depth explanation  let's test the ""k2fa"" kids. Show the ""k2fa"" kids a picture  and they will identify it. Scroll down  to the ""Prediction section"" and have fun.   Separate the ""train"" from ""valid"" by doing "".split_by_folder(train='train'  valid='valid')"" command.  Assign the label from the folder-name  i.e. using "".label_from_folder()"" command.   The ""k2fa"" AI project is for kids to learn about farm animals. Think of ""k2fa""  as kids wandering through a farm and naming the animal that they see.  More concisely  ""k2fa"" is Deep Learning Convolutional Neural Network (CNN) image classification model.  Before charging ahead with an in-depth explanation  let's test the ""k2fa"" kids. Show the ""k2fa"" kids a picture  and they will identify it. Scroll down  to the ""Prediction section"" and have fun.   A sample result is as follows.  <img src=""https://media-exp1.licdn.com/dms/image/C4D12AQFcyk7e7VszMg/article-inline_image-shrink_1000_1488/0?e=1602115200&v=beta&t=fZ-OJsgyyT6W7LqdU4s4wkTtja5leFpx74uqUEHT7BM"" width=""80%"" style=""margin:2rem !important;"" />  <img src=""https://media-exp1.licdn.com/dms/image/C5612AQFsxUUUvbJ4Ow/article-inline_image-shrink_1000_1488/0?e=1602115200&v=beta&t=c2Ue9AWM7GAvYpz82d3kRTMXsxG_ubFPAmE48IW1_UU"" width=""80%"" style=""margin:2rem !important;"" />   - The ""percentage per one_fit_cycle"" (pct_start) rate is 0.3. - The ""momentum"" rate is list(0.95  0.85). - The ""dropout"" rate is 0.5.  7.  ""Use case"" is a consulting terminology for ""what are the steps leading to the desired outcome from the user's point of view."" For ""k2fa "" the first ""happy path "" use-case is as follows.  1. Kids download the ""k2fa"" app on iPhone or Android phones. 1. Kids start playing the ""scavenger hunt""  game with their friends. 1. Kids run around a farm  taking pictures of farm animals. 1. ""k2fa"" verifies the farm animal pictures. 1. A kid  who has the shortest time finding all thirteen animals  wins the round.  <img src=""https://nbp3-webclient-2020.web.app/image/undraw_order_confirmed_aaw7.svg"" width=""60%"">  The second ""k2fa"" use-case is as follows.  1. Kids download the ""k2fa"" app on iPhone or Android phones. 1. Kids start playing the ""picture charades""  game with their friends  where each player or a team taking pictures of themselves  making faces  paint faces  acting poses  or dress up as the animal. If it is a team  each team member can choose a different animal. 1. ""k2fa"" judges whether the impression passed or failed. 1. A kid or team  who has the shortest time  wins the round.  <img src=""https://nbp3-webclient-2020.web.app/image/undraw_social_influencer_sgsv.svg"" width=""60%"">  The use-case's error condition and recovery are for analysts and the QA team to ponder  such as false-positive or cheating  by taking pictures of the online image. As said before  ""K2fa"" is an anodyne subject  and therefore  there is no known unintentional consequence.  If ""k2fa"" is in a science fiction story  then ""k2fa"" could be an army of robots    roaming the farm protecting livestock and killing foxes  bears  and coyotes but not tigers. It is because tigers are identifying as ""false-positive horses.""  > """;Computer Vision;https://github.com/duchaba/Norwegian_Blue_Parrot_k2fa_AI
"""I used the shape() property to get the shapes of of training  validation and test datasets. Shape can also be used to find the shape of traffic sign images. Number of classes can be found out using signnames.csv or finding unique entries in the training set - I use the latter  * The size of training set is 34799 * The size of the validation set is 4410 * The size of test set is 12630 * The shape of a traffic sign image is (32  32  3) * The number of unique classes/labels in the data set is 43   Here are the results of the prediction:  | Image			        |     Prediction	        					|  |:---------------------:|:---------------------------------------------:|  | Right-of-way at the next intersection      		| Right-of-way at the next intersection   									|  | Bumpy road     			| Bumpy road 										| | Slippery road					| Slippery road										| | Road work	      		| Road work					 				| | Children crossing			| Children crossing      							| | Speed limit (60km/h) | Speed limit (60km/h)      |   The model was able to correctly guess 6 out of 6 traffic signs  which gives an accuracy of 100%. This compares favorably to the accuracy on the test set of 98%   My final model results were: * training set accuracy of 100% * validation set accuracy of 98.8%  * test set accuracy of 98.0%   I tried the following architectures:  1. LeNet (shown in lecture) and enhancements to it including adapting learning rate  dropout for different layers  etc. This is present as a function in my final submission. 2. A VGGNet [7] like architecture (not that deep  but employing same padding level) with 3 convolution layers (convolution+batch norm+RELU+max pooling) and two fully connected layers with adaptive learning rate and dropouts. I excluded this in the final submission . 2. Sermanet architecture shown in [1]. I tried two flavors of it and immediately saw improvement. The main idea here is to ""short-circuit"" the output of the first convolutional layer directly into the fully connected layer. I saw a marked improvment in the convergence time with this method. The validation accuracy in every run was ~0.97 in just 3 epochs. For the final submission  I let it run for 100 epochs. **The final architecture is based on this and described below. The implementation is SermaNet2() in my submission.**  My journey to submission was long and I spent a lot of time experimenting with hyperparameters: * I started with the LeNet architecture and tried to study the rest of the design components like data augmentation  weight initialization  learning rate  dropout  batch normalization as described in next few bullets.  * I started with initial weight optimization study and quickly observed that covergence rate was heavily dependent on initialization hyperparameters of mean/standard deviation and also distribution (truncated gaussian v/s gaussian). I ended up using Xavier initialization after which I never had to worry about weight initialization. * The second hyperparameter I played with was learning rate. I saw marginal improvement on using learning rate adaptation of reducing it by 0.1 every 20 epochs and continued using it for the rest of the project. I kept a flag to turn off adaptation every now and then to test its effectiveness. * With the above steps  the model continued to overfit the training data with ~94% accuracy on validation data. I introduced dropout into the model and the validation accuracy improved to ~96%. * I added batch normalization and it improved convergence rate. I kept a flag and experimented with turning it off and on. * I wanted to further improve the accuracy and started looking at other architectures like GoogleNet  SermaNet  VGGNet  etc. I implemented SermaNet to the best of my understanding with much smaller feature sizes than in the paper. For example  the paper uses 108-108 filter depth and I used 12-32 filter depth in my submission. I implemented two different flavors of the concatenation layer - one concatenating the second layer with the output of a third convolutional layer and another concatenating output of first and second convolution layer. The latter has lesser parameters and gives better performance and was used for the final submission. * In the end I tried the VGGNet-like architecture mentioned above  though it gave me slightly lower accuracy than the final submission.     Inspired by [1]  I tried two image equalization techniques - histogram equalization and CLAHE (Contrast Limited Adaptive Histogram Equalization) applied to grayscale images [2]. Both these techniques improve the contrast in the image as shown in figure below (figure shows original image  histogram equalized image and CLAHE filtered image from left to right). The 70 in the image is hardly visible in the first image  however  the equalization techniques enhance the image immensely.  ![Equalization techniques considered][image4]  I decided to use CLAHE (on grayscale images) for data preprocessing here because histogram equalization does not work well when there are large intensity variations in an image. This is easier to demonstrate on larger images but a couple of examples where histogram equalization does not work well are shown below (as before  figure shows original image  histogram equalized image and CLAHE filtered image from left to right).  ![Children crossing][image5]  ![Bumpy road][image6]  Additionally  I tried a few data augmentation techniques and ended up using the following augmentations: * Image rotated randomly in the range +/-[5  15] degrees and then scaled by 0.9 or 1.1 * Randomly perturbed in both horizontal and vertical directions by [-2  2] pixels * Motion blurred with a kernel of size 2  The figure below shows the original RGB image and four processed images used for training (CLAHE filtered grayscale image  scaled and roated  randomly perturbed  and motion blurred)  ![augmentation][image14]  Note that the augmentation is applied to grayscaled and CLAHE filtered images. This gives a dataset that is four times the original dataset. Note that each copy of training set image is augmented to produce 4 images and I do not selectively choose certain image categories to augment. Such datasets may represent natural distributions and thus it may not be a good idea to augment unevenly. This is because Augmentation should increase the robustness of the model when seeing unseen images.  I centred the image around the pixel mean and normalized with the standard deviation because I wanted to center the data around zero and have similar ranges for the pixels. Images under different light variations can have largely different pixel values and we desire the network to learn other features in the image than the light conditions  thus centering around the mean and normalization helps the learning process. Normalization also ensures similar values of gradient while doing backpropagation and helps prevent gradient saturation (not too relevant here because image data is already upper bounded).      I used the shape() property to get the shapes of of training  validation and test datasets. Shape can also be used to find the shape of traffic sign images. Number of classes can be found out using signnames.csv or finding unique entries in the training set - I use the latter  * The size of training set is 34799 * The size of the validation set is 4410 * The size of test set is 12630 * The shape of a traffic sign image is (32  32  3) * The number of unique classes/labels in the data set is 43   You're reading it! and here is a link to my [project code](https://github.com/utsawk/CarND-Traffic-Sign-Classifier-Project/blob/master/Traffic_Sign_Classifier.ipynb)   You're reading it! and here is a link to my [project code](https://github.com/utsawk/CarND-Traffic-Sign-Classifier-Project/blob/master/Traffic_Sign_Classifier.ipynb)   Inspired by [1]  I tried two image equalization techniques - histogram equalization and CLAHE (Contrast Limited Adaptive Histogram Equalization) applied to grayscale images [2]. Both these techniques improve the contrast in the image as shown in figure below (figure shows original image  histogram equalized image and CLAHE filtered image from left to right). The 70 in the image is hardly visible in the first image  however  the equalization techniques enhance the image immensely.  ![Equalization techniques considered][image4]  I decided to use CLAHE (on grayscale images) for data preprocessing here because histogram equalization does not work well when there are large intensity variations in an image. This is easier to demonstrate on larger images but a couple of examples where histogram equalization does not work well are shown below (as before  figure shows original image  histogram equalized image and CLAHE filtered image from left to right).  ![Children crossing][image5]  ![Bumpy road][image6]  Additionally  I tried a few data augmentation techniques and ended up using the following augmentations: * Image rotated randomly in the range +/-[5  15] degrees and then scaled by 0.9 or 1.1 * Randomly perturbed in both horizontal and vertical directions by [-2  2] pixels * Motion blurred with a kernel of size 2  The figure below shows the original RGB image and four processed images used for training (CLAHE filtered grayscale image  scaled and roated  randomly perturbed  and motion blurred)  ![augmentation][image14]  Note that the augmentation is applied to grayscaled and CLAHE filtered images. This gives a dataset that is four times the original dataset. Note that each copy of training set image is augmented to produce 4 images and I do not selectively choose certain image categories to augment. Such datasets may represent natural distributions and thus it may not be a good idea to augment unevenly. This is because Augmentation should increase the robustness of the model when seeing unseen images.  I centred the image around the pixel mean and normalized with the standard deviation because I wanted to center the data around zero and have similar ranges for the pixels. Images under different light variations can have largely different pixel values and we desire the network to learn other features in the image than the light conditions  thus centering around the mean and normalization helps the learning process. Normalization also ensures similar values of gradient while doing backpropagation and helps prevent gradient saturation (not too relevant here because image data is already upper bounded).      My final model results were: * training set accuracy of 100% * validation set accuracy of 98.8%  * test set accuracy of 98.0%   I tried the following architectures:  1. LeNet (shown in lecture) and enhancements to it including adapting learning rate  dropout for different layers  etc. This is present as a function in my final submission. 2. A VGGNet [7] like architecture (not that deep  but employing same padding level) with 3 convolution layers (convolution+batch norm+RELU+max pooling) and two fully connected layers with adaptive learning rate and dropouts. I excluded this in the final submission . 2. Sermanet architecture shown in [1]. I tried two flavors of it and immediately saw improvement. The main idea here is to ""short-circuit"" the output of the first convolutional layer directly into the fully connected layer. I saw a marked improvment in the convergence time with this method. The validation accuracy in every run was ~0.97 in just 3 epochs. For the final submission  I let it run for 100 epochs. **The final architecture is based on this and described below. The implementation is SermaNet2() in my submission.**  My journey to submission was long and I spent a lot of time experimenting with hyperparameters: * I started with the LeNet architecture and tried to study the rest of the design components like data augmentation  weight initialization  learning rate  dropout  batch normalization as described in next few bullets.  * I started with initial weight optimization study and quickly observed that covergence rate was heavily dependent on initialization hyperparameters of mean/standard deviation and also distribution (truncated gaussian v/s gaussian). I ended up using Xavier initialization after which I never had to worry about weight initialization. * The second hyperparameter I played with was learning rate. I saw marginal improvement on using learning rate adaptation of reducing it by 0.1 every 20 epochs and continued using it for the rest of the project. I kept a flag to turn off adaptation every now and then to test its effectiveness. * With the above steps  the model continued to overfit the training data with ~94% accuracy on validation data. I introduced dropout into the model and the validation accuracy improved to ~96%. * I added batch normalization and it improved convergence rate. I kept a flag and experimented with turning it off and on. * I wanted to further improve the accuracy and started looking at other architectures like GoogleNet  SermaNet  VGGNet  etc. I implemented SermaNet to the best of my understanding with much smaller feature sizes than in the paper. For example  the paper uses 108-108 filter depth and I used 12-32 filter depth in my submission. I implemented two different flavors of the concatenation layer - one concatenating the second layer with the output of a third convolutional layer and another concatenating output of first and second convolution layer. The latter has lesser parameters and gives better performance and was used for the final submission. * In the end I tried the VGGNet-like architecture mentioned above  though it gave me slightly lower accuracy than the final submission.     """;General;https://github.com/utsawk/CarND-Traffic-Sign-Classifier-Project
"""1. Archive your training data and upload it to an S3 bucket 2. Provision your EC2 instance (I used an Ubuntu AMI) 3. Log into your EC2 instance via SSH 4. Install the aws CLI client and configure it:  ```bash sudo snap install aws-cli --classic aws configure ```  You will then have to enter your AWS access keys  which you can retrieve from the management console under AWS Management Console > Profile > My Security Credentials > Access Keys  Then  run these commands  or maybe put them in a shell script and execute that:  ```bash mkdir data curl -O https://bootstrap.pypa.io/get-pip.py sudo apt-get install python3-distutils python3 get-pip.py pip3 install stylegan2_pytorch export PATH=$PATH:/home/ubuntu/.local/bin aws s3 sync s3://<Your bucket name> ~/data cd data tar -xf ../train.tar.gz ```  Now you should be able to train by simplying calling `stylegan2_pytorch [args]`.  Notes:  * If you have a lot of training data  you may need to provision extra block storage via EBS. * Also  you may need to spread your data across multiple archives. * You should run this on a `screen` window so it won't terminate once you log out of the SSH session.   You will need a machine with a GPU and CUDA installed. Then pip install the package like this  ```bash $ pip install stylegan2_pytorch ```  If you are using a windows machine  the following commands reportedly works.  ```bash $ conda install pytorch torchvision -c python $ pip install stylegan2_pytorch ```   ```bash $ stylegan2_pytorch --data /path/to/images ```  That's it. Sample images will be saved to `results/default` and models will be saved periodically to `models/default`.   You can specify the name of your project with  ```bash $ stylegan2_pytorch --data /path/to/images --name my-project-name ```  You can also specify the location where intermediate results and model checkpoints should be stored with  ```bash $ stylegan2_pytorch --data /path/to/images --name my-project-name --results_dir /path/to/results/dir --models_dir /path/to/models/dir ```  By default  if the training gets cut off  it will automatically resume from the last checkpointed file. If you want to restart with new settings  just add a `new` flag  ```bash $ stylegan2_pytorch --new --data /path/to/images --name my-project-name --image-size 512 --batch-size 1 --gradient-accumulate-every 16 --network-capacity 10 ```  Once you have finished training  you can generate images from your latest checkpoint like so.  ```bash $ stylegan2_pytorch  --generate ```  To generate a video of a interpolation through two random points in latent space.  ```bash $ stylegan2_pytorch --generate-interpolation ```  To save each individual frame of the interpolation  ```bash $ stylegan2_pytorch --generate-interpolation --save-frames ```  If a previous checkpoint contained a better generator  (which often happens as generators start degrading towards the end of training)  you can load from a previous checkpoint with another flag  ```bash $ stylegan2_pytorch --generate --load-from {checkpoint number} ```   """;Computer Vision;https://github.com/Bakikii/stylegan2-pytorch23
"""You can download the dataset from Kaggle   """;Computer Vision;https://github.com/lxy000719/ResNet-50-pneumonia-classification
"""Tensorflow Tutorials using Jupyter Notebook   https://github.com/krasserm/machine-learning-notebooks  Stanford Machine Learning course exercises implemented with scikit-learn   """;Natural Language Processing;https://github.com/jiyuan/ainote
"""To run convert_deeplab_resnet.py  deeplab v2 caffe and pytorch (python 2.7) are required.   To run init_net_surgery .py  deeplab v2 caffe and pytorch (python 2.7) are required.   To run train.py  pytorch (python 2.7) is required.   Note that this repository has been tested with python 2.7 only.  """;Computer Vision;https://github.com/isht7/pytorch-deeplab-resnet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/appcoreopc/berty
"""To train a generative model on source1 for instance  one should give the following path: DATASET_FOLDER/source1   flow_builder : build flow using Transformed Distribution from Tensorflow-probability   """;Computer Vision;https://github.com/SamArgt/AudioSourceSep
"""- Preprocess the training data. Pretrain the baseline model with your data. Read [here](https://fairseq.readthedocs.io/en/latest/getting_started.html#training-a-new-model) for more instructions.  - Evaluate the importance of the neurons.   Run it for each language pair to calculate the importance.  ``` bash importanceModel.sh ```  or  ``` #:!/bin/bash  save_dir={checkpoints}/model.pt langs=lang1 lang2 lang3 lang4 #: All languages pairs of your model such as ""en-zh zh-en en-ar ar-en"" lang=lang1 #: Current language pair  such as ""en-zh""  python importance.py data-bin/{data} \        --arch multilingual_transformer  --reset-optimizer \        --encoder-langtok ""tgt"" \        --task multilingual_translation --lang-pairs $langs \        --share-encoders --share-decoders \        --share-all-embeddings \        --focus-lang $lang --fp16 \        --max-tokens 2048  --save-dir $save_dir ```  - Generate mask matrix based on the importance value    ``` python importance_mask.py ```       - Fine-tuning the model with language-specific mask matrix  ``` bash run.sh ```  or  ``` #: !/bin/bash model={path_to_ckpt} #:model=test  python3 train.py data-bin/{data}} \     --arch multilingual_transformer \     --fp16 \     --encoder-langtok ""tgt"" \     --restore-file /path_to_baseline/model.pt \     --task multilingual_translation --lang-pairs $langs \     --share-encoders --share-decoders \     --share-all-embeddings --share-decoder-input-output-embed \     --reset-lr-scheduler --reset-optimizer \     --optimizer adam --adam-betas '(0.9  0.98)' \     --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 \     --lr 0.0001 --min-lr 1e-09 --ddp-backend=no_c10d \     --dropout 0.1 \     --weight-decay 0.0 --clip-norm 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 4096  --update-freq 2 \     --no-progress-bar --log-format json --log-interval 20 \     --save-dir checkpoints/$model |tee -a  logs/$model.log ```  - Generate the language-specific translation   ``` #: !/bin/bash t=$src f=$tgt cat tst.$t-$f.$t \ python interactive.py $DATABIN --path $model_path \         --task multilingual_translation --source-lang $t --target-lang $f \         --encoder-langtok ""tgt"" \         --buffer-size 2000 --lang-pairs $langs \         --beam 4 --batch-size 128 --lenpen 0.6 --remove-bpe \         --log-format=none > $OUT_DIR/tst.$t-$f ```    """;General;https://github.com/ictnlp/NA-MNMT
"""| provenance | https://github.com/flyyufelix/DenseNet-Keras |    """;Computer Vision;https://github.com/modelhub-ai/densenet
"""Follow the steps in the installation documentation in [doc/installation.md](doc/installation.md).     """;General;https://github.com/jreisam/Unity-OpenPose-Edutable
"""The goal of **pycls** is to provide a simple and flexible codebase for image classification. It is designed to support rapid implementation and evaluation of research ideas. **pycls** also provides a large collection of baseline results ([Model Zoo](MODEL_ZOO.md)).  The codebase supports efficient single-machine multi-gpu training  powered by the PyTorch distributed package  and provides implementations of standard models including [ResNet](https://arxiv.org/abs/1512.03385)  [ResNeXt](https://arxiv.org/abs/1611.05431)  [EfficientNet](https://arxiv.org/abs/1905.11946)  and [RegNet](https://arxiv.org/abs/2003.13678).   """;General;https://github.com/facebookresearch/pycls
"""To extract and run the network in Code::Blocks <br/> $ mkdir *MyDir* <br/> $ cd *MyDir* <br/> $ wget https://github.com/Qengineering/NanoDet-ncnn-Jetson-Nano/archive/refs/heads/main.zip <br/> $ unzip -j master.zip <br/> Remove master.zip  LICENSE and README.md as they are no longer needed. <br/>  $ rm master.zip <br/> $ rm LICENSE <br/> $ rm README.md <br/> <br/> Your *MyDir* folder must now look like this: <br/>  James.mp4 <br/> parking.jpg <br/> busstop.jpg <br/> NanoDet.cpb <br/> NanoDet.cpp <br/> nanodet_m.bin <br/> nanodet_m.param <br/>  ------------   """;General;https://github.com/Qengineering/NanoDet-ncnn-Jetson-Nano
"""source activate infomax  bash vision_traineval.sh   If you want to save GPU memory  you can train layers sequentially  one at a time  by setting the module to be trained (0-2)  e.g.  bash    source activate infomax  bash audio_traineval.sh   If you want to save GPU memory  you can train layers sequentially  one at a time  by setting the layer to be trained (0-5)  e.g.  bash    """;General;https://github.com/loeweX/Greedy_InfoMax
"""https://github.com/hassony2/kinetics_i3d_pytorch <- i3d code  https://github.com/Tushar-N/pytorch-resnet3d <- non local code   If you see the error below  fix the code.    """;Computer Vision;https://github.com/seominseok0429/inception-I3D-NON-LOCAL
"""[This page](https://bazel.build/) describes how to install the Bazel build and test tool on your machine.   ```shell pip install six pip install absl-py pip install inflection pip install wrapt pip install numpy pip install dm-sonnet pip install tensorflow-gpu pip install tensorflow-probability-gpu pip install pygame ```   ```shell wget https://github.com/opencv/opencv/archive/2.4.13.6.zip unzip 2.4.13.6.zip cd opencv-2.4.13.6 mkdir build cd build cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local .. make -j7 sudo make install sudo ldconfig cd ../.. ```   ```shell git clone https://github.com/google/clif.git cd clif ./INSTALL.sh cd .. ```   For detailed information see: https://github.com/protocolbuffers/protobuf/blob/master/src/README.md  ```shell git clone https://github.com/protocolbuffers/protobuf.git cd protobuf git submodule update --init --recursive ./autogen.sh ./configure make -j7 sudo make install sudo ldconfig cd python python setup.py build sudo python setup.py install cd ../.. ```   ```shell sudo apt-get install autoconf automake libtool curl make g++ unzip virtualenv python-virtualenv cmake subversion pkg-config libpython-dev libcairo2-dev libboost-all-dev python-pip libssl-dev pip install setuptools pip install pyparsing ```   navigation agents in the StreetLearn environment by using a TensorFlow   streetlearn/python/environment A Python-based interface for calling the       Python using pygame  that instantiates the StreetLearn environment on the   build has only been tested running on Ubuntu 18.04.   git clone https://github.com/deepmind/streetlearn.git  cd streetlearn   bazel build streetlearn:streetlearn_engine_py   bazel build streetlearn/python/human_agent:all   The Python StreetLearn environment follows the specifications from   start_pano: The pano ID string to start from. The graph will be build       requested from the environment: ['view_image'  'graph_image'  'yaw'    The following games are available in the StreetLearn environment:   """;Reinforcement Learning;https://github.com/windstrip/DeepMind-StreetLearn
"""Human visual system starts from lower visual area and proceed to the higher areas. However  it is not a full story. Our lower visual areas are largely affected by various higher visual area interactively.   ![Retino and Non-retino images][incongOccluded]    """;Computer Vision;https://github.com/Ohyeon5/MultiscaleSegmentation
"""Accurately ranking the vast number of candidate detections is crucial for dense object detectors to achieve high performance. In this work  we propose to learn IoU-aware classification scores (**IACS**) that simultaneously represent the object presence confidence and localization accuracy  to produce a more accurate ranking of detections in dense object detectors. In particular  we design a new loss function  named **Varifocal Loss (VFL)**  for training a dense object detector to predict the IACS  and a new efficient star-shaped bounding box feature representation (the features at nine yellow sampling points) for estimating the IACS and refining coarse bounding boxes. Combining these two new components and a bounding box refinement branch  we build a new IoU-aware dense object detector based on the FCOS+ATSS architecture  what we call **VarifocalNet** or **VFNet** for short. Extensive experiments on MS COCO benchmark show that our VFNet consistently surpasses the strong baseline by ~2.0 AP with different backbones. Our best model VFNet-X-1200 with Res2Net-101-DCN reaches a single-model single-scale AP of **55.1** on COCO `test-dev`  achieving the state-of-the-art performance among various object detectors.  <div align=""center"">   <img src=""VFNet.png"" width=""600px"" />   <p>Learning to Predict the IoU-aware Classification Score.</p> </div>   - This VarifocalNet implementation is based on [MMDetection](https://github.com/open-mmlab/mmdetection). Therefore the installation is the same as original MMDetection.  - Please check [get_started.md](docs/get_started.md) for installation. Note that you should change the version of PyTorch and CUDA to yours when installing **mmcv** in `step 3` and clone this repo instead of MMdetection in `step 4`.  - If you run into problems with `pycocotools`  please install it by:    ```   pip install ""git+https://github.com/open-mmlab/cocoapi.git#:subdirectory=pycocotools""   ```   2020.10.29 This repo has been refactored so that users can pull the latest updates from the upstream official MMDetection repo. The previous one can be found in the old branch.   Assuming you have put the COCO dataset into data/coco/ and have downloaded the models into the checkpoints/  you can now evaluate the models on the COCO val2017 split:   The following command line will train vfnet_r50_fpn_1x_coco on 8 GPUs:   Once the installation is done  you can follow the steps below to run a quick demo.  - Download the [model](https://drive.google.com/file/d/1aF3Fi5rYeMqSC3Ndo4VEqjPXk4fcOfjt/view?usp=sharing) and put it into one folder under the root directory of this project  say  `checkpoints/`. - Go to the root directory of this project in terminal and activate the corresponding virtual environment. - Run    ```   python demo/image_demo.py demo/demo.jpg configs/vfnet/vfnet_r50_fpn_1x_coco.py checkpoints/vfnet_r50_1x_41.6.pth   ```    and you should see an image with detections.   Please see [exist_data_model.md](docs/1_exist_data_model.md) for the basic usage of MMDetection. They also provide [colab tutorial](demo/MMDet_Tutorial.ipynb) for beginners.  For troubleshooting  please refer to [faq.md](docs/faq.md)   """;General;https://github.com/hyz-xmaster/VarifocalNet
"""```python from Mixup import mixup  batch_x  batch_y = mixup(alpha  batch_x  batch_y) ```   """;Computer Vision;https://github.com/Yangget/Mixup_All-use
"""1. Clone this repository and place it in `~`. 2. Run `pip install -r requirements.txt` - Note: packages are pinned to exact versions but may work with older versions.  Compatibility is untested for versions not explicitly listed in `requirements.txt`.   The repository containts the following modules:   """;Natural Language Processing;https://github.com/griff4692/LMC
"""JDet is an object detection benchmark based on [Jittor](https://github.com/Jittor/jittor)  and mainly focus on aerial image object detection (oriented object detection).   <!-- **Features** - Automatic compilation. Our framwork is based on Jittor  which means we don't need to Manual compilation for these code with CUDA and C++. -  -->  <!-- Framework details are avaliable in the [framework.md](docs/framework.md) -->  JDet environment requirements:  * System: **Linux**(e.g. Ubuntu/CentOS/Arch)  **macOS**  or **Windows Subsystem of Linux (WSL)** * Python version >= 3.7 * CPU compiler (require at least one of the following)     * g++ (>=5.4.0)     * clang (>=8.0) * GPU compiler (optional)     * nvcc (>=10.0 for g++ or >=10.2 for clang) * GPU library: cudnn-dev (recommend tar file installation  [reference link](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux-tar))  **Step 1: Install the requirements** ```shell git clone https://github.com/Jittor/JDet cd JDet python -m pip install -r requirements.txt ``` If you have any installation problems for Jittor  please refer to [Jittor](https://github.com/Jittor/jittor)  **Step 2: Install JDet**   ```shell cd JDet #: suggest this  python setup.py develop #: or python setup.py install ``` If you don't have permission for install please add ```--user```.  Or use ```PYTHONPATH```:  You can add ```export PYTHONPATH=$PYTHONPATH:{you_own_path}/JDet/python``` into ```.bashrc```  and run ```shell source .bashrc ```   You can also build your own dataset by convert your datas to DOTA format.   mkdir $PROJECT_PATH$  cd $PROJECT_PATH$   mkdir configs   :heavy_check_mark: COCO   """;General;https://github.com/Jittor/JDet
"""faster_rcnn_pytorch | Faster RCNN with PyTorch   """;Computer Vision;https://github.com/busyboxs/Some-resources-useful-for-me
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/liuxiangchao369/VRS-training
"""Please see the DATASETs.md for the details.    The train settings require 8 GPU with at least 11GB memory.    sh ./scripts/train/train_cityscapes_sfnet_res18.sh   """;Computer Vision;https://github.com/lxtGH/SFSegNets
"""Official Implementation of our ReStyle paper for both training and evaluation. ReStyle introduces an iterative refinement mechanism which can be applied over different StyleGAN encoders for solving the StyleGAN inversion task.    In this work  we use rosinality's [StyleGAN2 implementation](https://github.com/rosinality/stylegan2-pytorch).  If you wish to use your own generator trained using NVIDIA's implementation there are a few options we recommend: 1. Using NVIDIA's StyleGAN2 / StyleGAN-ADA TensorFlow implementation.      You can then convert the TensorFlow `.pkl` checkpoints to the supported format using the conversion script found in [rosinality's implementation](https://github.com/rosinality/stylegan2-pytorch#convert-weight-from-official-checkpoints). 2. Using NVIDIA's StyleGAN-ADA PyTorch implementation.      You can then convert the PyTorch `.pkl` checkpoints to the supported format using the conversion script created by [Justin Pinkney](https://github.com/justinpinkney) found in [dvschultz's fork](https://github.com/dvschultz/stylegan2-ada-pytorch/blob/main/SG2_ADA_PT_to_Rosinality.ipynb).     Once you have the converted `.pt` files  you should be ready to use them in this repository.      In order to train ReStyle on your own data  you should perform the following steps:  1. Update `configs/paths_config.py` with the necessary data paths and model paths for training and inference. ``` dataset_paths = {     'train_data': '/path/to/train/data'     'test_data': '/path/to/test/data'  } ``` 2. Configure a new dataset under the `DATASETS` variable defined in `configs/data_configs.py`. There  you should define  the source/target data paths for the train and test sets as well as the transforms to be used for training and inference. ``` DATASETS = { 	'my_data_encode': { 		'transforms': transforms_config.EncodeTransforms    #: can define a custom transform  if desired 		'train_source_root': dataset_paths['train_data']  		'train_target_root': dataset_paths['train_data']  		'test_source_root': dataset_paths['test_data']  		'test_target_root': dataset_paths['test_data']  	} } ``` 3. To train with your newly defined dataset  simply use the flag `--dataset_type my_data_encode`.   - Dependencies:   We recommend running this repository using [Anaconda](https://docs.anaconda.com/anaconda/install/).  All dependencies for defining the environment are provided in `environment/restyle_env.yaml`.   Please download the pretrained models from the following links.   Note: all StyleGAN models are converted from the official TensorFlow models to PyTorch using the conversion script from rosinality.  By default  we assume that all auxiliary models are downloaded and saved to the directory pretrained_models.    Additionally  if you have tensorboard installed  you can visualize tensorboard logs in opts.exp_dir/logs.   notebook can be run using Google Colab here.   you can run the following:    If you wish to run inference using these two models and the bootstrapping technique you may run the following:    """;Computer Vision;https://github.com/yuval-alaluf/restyle-encoder
"""The read_data() function adapt the test.jpg to a tensor with shape          Tensor [1  3  224  224]          To fit as input for the future ResNet-34 classification model  ![](Prepare_1_3_224_224_tensor_from_test_jpg.png)   http://yann.lecun.com/exdb/mnist/   """;General;https://github.com/ollewelin/libtorch-GPU-CNN-test-MNIST-with-Batchnorm
"""Before running any of the code  be sure to install the requirements.  Links to the datasets are below.  python -m virtualenv env  source env/bin/activate  python -m pip install -r requirements.txt   Links from the lecture slides are listed below  under the name of the lecture they are from.   Maestro MIDI Dataset:  https://magenta.tensorflow.org/datasets/maestro   ECG dataset from Kaggle:  https://www.kaggle.com/shayanfazeli/heartbeat   """;General;https://github.com/nlinc1905/dsilt-tsa
"""The model has been developed using PyTorch library. The Pytorch library is available over the main page: https://pytorch.org/   conda install pytorch torchvision -c pytorch  In addition to PyTorch  in this repository has been used also Numpy. Numpy is already installed in Anaconda  otherwise you can use:   PyTorch   Numpy    To set up your python environment to run the code in this repository  follow the instructions below.  1. Create (and activate) a new environment with Python 3.6.      - __Linux__ or __Mac__:      ```bash     conda create --name drlnd python=3.6     source activate drlnd     ```     - __Windows__:      ```bash     conda create --name drlnd python=3.6      activate drlnd     ```      2. Follow the instructions in [this repository](https://github.com/openai/gym) to perform a minimal install of OpenAI gym.       - Next  install the **classic control** environment group by following the instructions [here](https://github.com/openai/gym#classic-control).     - Then  install the **box2d** environment group by following the instructions [here](https://github.com/openai/gym#box2d).      3. Clone the repository (if you haven't already!)  and navigate to the `python/` folder.  Then  install several dependencies.  ```bash git clone https://github.com/udacity/deep-reinforcement-learning.git cd deep-reinforcement-learning/python pip install . ``` 4. Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment.   ```bash python -m ipykernel install --user --name drlnd --display-name ""drlnd"" ```  5. Before running code in a notebook  change the kernel to match the `drlnd` environment by using the drop-down `Kernel` menu.    """;Reinforcement Learning;https://github.com/IvanVigor/MADDPG-Unity
"""Use Python 3.6.  Install:     install anaconda      create environment (use scripts/bash create_env.sh) or     pip install -r requirements.txt or      use Docker       """;Computer Vision;https://github.com/tatigabru/kaggle-lyft
"""#: You can also use the dense version of GAT   ``` $ python main.py ```   """;Graphs;https://github.com/marblet/gat-pytorch
"""cd cnn   You'll need to make sure to specify the correct checkpoint directory  which will change every run.   To generate the charts above simply run python cosine_power_annealing.py from the /cnn directory.   Instructions for acquiring PTB and WT2 can be found here. While CIFAR-10 can be automatically downloaded by torchvision  ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions here.   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/ahundt/sharpDARTS
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;Computer Vision;https://github.com/tjwei/stylegan2_workshop
"""            --ckpt_dir [checkpoint directory] \               --ckpt_dir [checkpoint directory] \   %load_ext tensorboard --> Activate Tensorboard in Jupyter Notebook or Google Colab    """;Computer Vision;https://github.com/hyunjoonbok/Unet-pytorch
"""This is an official pytorch implementation of [*Deep High-Resolution Representation Learning for Human Pose Estimation*](https://arxiv.org/abs/1902.09212).  In this work  we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods **recover high-resolution representations from low-resolution representations** produced by a high-to-low resolution network. Instead  our proposed network **maintains high-resolution representations** through the whole process. We start from a high-resolution subnetwork as the first stage  gradually add high-to-low resolution subnetworks one by one to form more stages  and connect the mutli-resolution subnetworks **in parallel**. We conduct **repeated multi-scale fusions** such that each of the high-to-low resolution representations receives information from other parallel representations over and over  leading to rich high-resolution representations. As a result  the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. </br>  ![Illustrating the architecture of the proposed HRNet](/figures/hrnet.png)  **For MPII data**  please download from [MPII Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/). The original annotation files are in matlab format. We have converted them into json format  you also need to download them from [OneDrive](https://1drv.ms/f/s!AhIXJn_J-blW00SqrairNetmeVu4) or [GoogleDrive](https://drive.google.com/drive/folders/1En_VqmStnsXMdldXA6qpqEyDQulnmS3a?usp=sharing). Extract them under {POSE_ROOT}/data  and make them look like this: ``` ${POSE_ROOT} |-- data `-- |-- mpii     `-- |-- annot         |   |-- gt_valid.mat         |   |-- test.json         |   |-- train.json         |   |-- trainval.json         |   `-- valid.json         `-- images             |-- 000001163.jpg             |-- 000003072.jpg ```  **For COCO data**  please download from [COCO download](http://cocodataset.org/#download)  2017 Train/Val is needed for COCO keypoints training and validation. We also provide person detection result of COCO val2017 and test-dev2017 to reproduce our multi-person pose estimation results. Please download from [OneDrive](https://1drv.ms/f/s!AhIXJn_J-blWzzDXoz5BeFl8sWM-) or [GoogleDrive](https://drive.google.com/drive/folders/1fRUDNUDxe9fjqcRZ2bnF_TKMlO0nB_dk?usp=sharing). Download and extract them under {POSE_ROOT}/data  and make them look like this: ``` ${POSE_ROOT} |-- data `-- |-- coco     `-- |-- annotations         |   |-- person_keypoints_train2017.json         |   `-- person_keypoints_val2017.json         |-- person_detection_results         |   |-- COCO_val2017_detections_AP_H_56_person.json         |   |-- COCO_test-dev2017_detections_AP_H_609_person.json         `-- images             |-- train2017             |   |-- 000000000009.jpg             |   |-- 000000000025.jpg             |   |-- 000000000030.jpg             |   |-- ...              `-- val2017                 |-- 000000000139.jpg                 |-- 000000000285.jpg                 |-- 000000000632.jpg                 |-- ...  ```   4. Init output(training model output directory) and log(tensorboard log directory) directory:     ```    mkdir output     mkdir log    ```     Your directory tree should look like this:     ```    ${POSE_ROOT}    ├── data    ├── experiments    ├── lib    ├── log    ├── models    ├── output    ├── tools     ├── README.md    └── requirements.txt    ```  6. Download pretrained models from our model zoo([GoogleDrive](https://drive.google.com/drive/folders/1hOTihvbyIxsm5ygDpbUuJ7O_tzv4oXjC?usp=sharing) or [OneDrive](https://1drv.ms/f/s!AhIXJn_J-blW231MH2krnmLq5kkQ))    ```    ${POSE_ROOT}     `-- models         `-- pytorch             |-- imagenet             |   |-- hrnet_w32-36af842e.pth             |   |-- hrnet_w48-8ef0771d.pth             |   |-- resnet50-19c8e357.pth             |   |-- resnet101-5d3b4d8f.pth             |   `-- resnet152-b121ed2d.pth             |-- pose_coco             |   |-- pose_hrnet_w32_256x192.pth             |   |-- pose_hrnet_w32_384x288.pth             |   |-- pose_hrnet_w48_256x192.pth             |   |-- pose_hrnet_w48_384x288.pth             |   |-- pose_resnet_101_256x192.pth             |   |-- pose_resnet_101_384x288.pth             |   |-- pose_resnet_152_256x192.pth             |   |-- pose_resnet_152_384x288.pth             |   |-- pose_resnet_50_256x192.pth             |   `-- pose_resnet_50_384x288.pth             `-- pose_mpii                 |-- pose_hrnet_w32_256x256.pth                 |-- pose_hrnet_w48_256x256.pth                 |-- pose_resnet_101_256x256.pth                 |-- pose_resnet_152_256x256.pth                 `-- pose_resnet_50_256x256.pth     ```      1. Install pytorch >= v1.0.0 following [official instruction](https://pytorch.org/).    **Note that if you use pytorch's version < v1.0.0  you should following the instruction at <https://github.com/Microsoft/human-pose-estimation.pytorch> to disable cudnn's implementations of BatchNorm layer. We encourage you to use higher pytorch's version(>=v1.0.0)** 2. Clone this repo  and we'll call the directory that you cloned as ${POSE_ROOT}. 3. Install dependencies:    ```    pip install -r requirements.txt    ``` 4. Make libs:    ```    cd ${POSE_ROOT}/lib    make    ``` 5. Install [COCOAPI](https://github.com/cocodataset/cocoapi):    ```    #: COCOAPI=/path/to/clone/cocoapi    git clone https://github.com/cocodataset/cocoapi.git $COCOAPI    cd $COCOAPI/PythonAPI    #: Install into global site-packages    make install    #: Alternatively  if you do not have permissions or prefer    #: not to install the COCO API into global site-packages    python3 setup.py install --user    ```    Note that instructions like  The code is developed using python 3.6 on Ubuntu 16.04. NVIDIA GPUs are needed. The code is developed and tested using 4 NVIDIA P100 GPU cards. Other platforms or GPU cards are not fully tested.       TEST.MODEL_FILE models/pytorch/pose_mpii/pose_hrnet_w32_256x256.pth       --cfg experiments/coco/hrnet/w32_256x192_adam_lr1e-3.yaml \      TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w32_256x192.pth \       --cfg experiments/coco/hrnet/w32_256x192_adam_lr1e-3.yaml \   """;Computer Vision;https://github.com/leoxiaobin/deep-high-resolution-net.pytorch
"""It was rendered using blender and the following <a href=""https://gist.github.com/LuanAdemi/6aac83f06d8d4394abc22e450af18a41"">script</a>.   https://github.com/maciejczyzewski/neural-chessboard    """;Computer Vision;https://github.com/LuanAdemi/VisualGo
"""Note that the following models are with bias wd = 0.   """;General;https://github.com/implus/PytorchInsight
"""``` pip install jupyter_nbextensions_configurator  jupyter nbextensions_configurator enable --user  pip install jupyter contrib_nbextensions  pip install autopep8  pip install npm  pip install nodejs  jupyter contrib nbextension install --user  ```  Read below for more useful tools for data-science https://towardsdatascience.com/bringing-the-best-out-of-jupyter-notebooks-for-data-science-f0871519ca29   ``` pip install qgrid jupyter nbextension enable --py --sys-prefix qgrid ```    :Note that http urls will not be displayed. Only https are allowed inside the Iframe   pip install cython  pip freeze &gt; requirements.txt  You will then find the requirements.txt file in your current working directory  Then install:  pip install -r requirements.txt  Ignore errors when installing packages via requirements.txt  FOR /F %p IN (requirements.txt) DO pip install %p  For Conda install  you can use  $ FOR /F ""delims=~"" %f in (requirements.txt) DO conda install --yes ""%f"" || pip install ""%f""   Run following command in the console:   2.  Create a new folder  then download your Export into HTML to that new folder. You will see a new folder created within that folder called ""nameofyourfile_files""  it has all of the files that you need like your jquery  MathJax  etc..   """;Computer Vision;https://github.com/noobpython/Notes
"""The codebases are built on top of [Detectron2](https://github.com/facebookresearch/detectron2) and [DETR](https://github.com/facebookresearch/detr).   Install and build libs  git clone https://github.com/PeizeSun/SparseR-CNN.git  cd SparseR-CNN  python setup.py build develop  Link coco dataset path to SparseR-CNN/datasets/coco  mkdir -p datasets/coco   """;Computer Vision;https://github.com/henbucuoshanghai/sparsercnn
"""Follow the instructions for installing SpinningUp  you might also need environment libraries such as Gym and MuJoCo.   To run Peng's Q(lambda) with delayed environment (with k=3)  n-step buffer with n=5  run the following   To run n-step with delayed environment (with k=3)  n-step buffer with n=5  run the following   To run Retrace with delayed environment (with k=3)  n-step buffer with n=5  just set lambda=1.0 and run the following   """;Reinforcement Learning;https://github.com/robintyh1/icml2021-pengqlambda
"""1. Archive your training data and upload it to an S3 bucket 2. Provision your EC2 instance (I used an Ubuntu AMI) 3. Log into your EC2 instance via SSH 4. Install the aws CLI client and configure it:  ```bash sudo snap install aws-cli --classic aws configure ```  You will then have to enter your AWS access keys  which you can retrieve from the management console under AWS Management Console > Profile > My Security Credentials > Access Keys  Then  run these commands  or maybe put them in a shell script and execute that:  ```bash mkdir data curl -O https://bootstrap.pypa.io/get-pip.py sudo apt-get install python3-distutils python3 get-pip.py pip3 install stylegan2_pytorch export PATH=$PATH:/home/ubuntu/.local/bin aws s3 sync s3://<Your bucket name> ~/data cd data tar -xf ../train.tar.gz ```  Now you should be able to train by simplying calling `stylegan2_pytorch [args]`.  Notes:  * If you have a lot of training data  you may need to provision extra block storage via EBS. * Also  you may need to spread your data across multiple archives. * You should run this on a `screen` window so it won't terminate once you log out of the SSH session.   You will need a machine with a GPU and CUDA installed. Then pip install the package like this  ```bash $ pip install stylegan2_pytorch ```  If you are using a windows machine  the following commands reportedly works.  ```bash $ conda install pytorch torchvision -c python $ pip install stylegan2_pytorch ```   Thanks to <a href=""https://github.com/GetsEclectic"">GetsEclectic</a>  you can now calculate the FID score periodically! Again  made super simple with one extra argument  as shown below.  Firstly  install the pytorch_fid package  $ pip install pytorch-fid   If you would like to sample images programmatically  you can do so with the following simple ModelLoader class.       base_dir = '/path/to/directory'    #: path to where you invoked the command line tool   Then  you need to make sure you have <a href=""https://docs.docker.com/get-docker/"">Docker installed</a>. Following the instructions at <a href=""https://github.com/aimhubio/aim"">Aim</a>  you execute the following in your terminal.   ```bash $ stylegan2_pytorch --data /path/to/images ```  That's it. Sample images will be saved to `results/default` and models will be saved periodically to `models/default`.   You can specify the name of your project with  ```bash $ stylegan2_pytorch --data /path/to/images --name my-project-name ```  You can also specify the location where intermediate results and model checkpoints should be stored with  ```bash $ stylegan2_pytorch --data /path/to/images --name my-project-name --results_dir /path/to/results/dir --models_dir /path/to/models/dir ```  You can increase the network capacity (which defaults to `16`) to improve generation results  at the cost of more memory.  ```bash $ stylegan2_pytorch --data /path/to/images --network-capacity 256 ```  By default  if the training gets cut off  it will automatically resume from the last checkpointed file. If you want to restart with new settings  just add a `new` flag  ```bash $ stylegan2_pytorch --new --data /path/to/images --name my-project-name --image-size 512 --batch-size 1 --gradient-accumulate-every 16 --network-capacity 10 ```  Once you have finished training  you can generate images from your latest checkpoint like so.  ```bash $ stylegan2_pytorch  --generate ```  To generate a video of a interpolation through two random points in latent space.  ```bash $ stylegan2_pytorch --generate-interpolation --interpolation-num-steps 100 ```  To save each individual frame of the interpolation  ```bash $ stylegan2_pytorch --generate-interpolation --save-frames ```  If a previous checkpoint contained a better generator  (which often happens as generators start degrading towards the end of training)  you can load from a previous checkpoint with another flag  ```bash $ stylegan2_pytorch --generate --load-from {checkpoint number} ```  A technique used in both StyleGAN and BigGAN is truncating the latent values so that their values fall close to the mean. The small the truncation value  the better the samples will appear at the cost of sample variety. You can control this with the `--trunc-psi`  where values typically fall between `0.5` and `1`. It is set at `0.75` as default  ```bash $ stylegan2_pytorch --generate --trunc-psi 0.5 ```   """;Computer Vision;https://github.com/Di-Is/stylegan2-ada-pytorch
"""Yolo v4 source code: https://github.com/AlexeyAB/darknet   Useful links: https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&sk=6039748846bbcf1d960c3061542591d7   """;Computer Vision;https://github.com/ZhiqiWang12/try_darknet
"""* /Data/ - includes all necessary data for solving task on ParaPhraser + scripts that make files with word embeddings (wordvec_rv_3.txt) and train/test data (train_PP.tsv and test_PP.tsv) * /Word2Vec/ - includes scripts for method Word2Vec + TF-IDF weighting * /BIMPM/ - includes scripts for method BiMPM   Application of NLP methods on russian dataset ParaPhraser for paraphrase identification problem.   """;Natural Language Processing;https://github.com/MariBax/Paraphrase-Identification
"""``` yolo.ai ├── cfg                 #: The directory where model config file is located (darknet  efficientnet  etc) ├── tests               #: Implmentation test cases ├── yolo                #: YOLO implementation code bases │   ├── data            #: Base data directory │   │   ├── datasets    #: Contain datasets such as pascal-voc │   │   └── transforms  #: Custom transforms for Dataset │   ├── models          #: Base model directory │   │   ├── arch        #: YOLO model assembly place │   │   ├── backbones   #: All backbone network gathering here │   │   ├── detectors   #: Assembly of all types of detectors │   │   ├── losses      #: The gathering place of all loss functions │   │   ├── metrics     #: Metrics functions for bounding boxes and losses │   │   └── modules     #: Individuals modules for network building │   └── utils           #: Utilites file for visualization and network ```  """;Computer Vision;https://github.com/DavianYang/yolo.ai
"""To use training/evaluating scripts as well as all models  you need to clone the repository and install dependencies: ``` git clone git@github.com:osmr/imgclsmob.git pip install -r requirements.txt ```   - pytorchcv for PyTorch    - PyTorch models    """;Sequential;https://github.com/osmr/imgclsmob
"""Python 3.7.7  pip install -r requirements.txt   device - использовать CPU или GPU;   device - использовать CPU или GPU.   """;Computer Vision;https://github.com/elis2496/maxup_implementation
"""English | [简体中文](README_zh-CN.md)  [![build](https://github.com/open-mmlab/mmocr/workflows/build/badge.svg)](https://github.com/open-mmlab/mmocr/actions) [![docs](https://readthedocs.org/projects/mmocr/badge/?version=latest)](https://mmocr.readthedocs.io/en/latest/?badge=latest) [![codecov](https://codecov.io/gh/open-mmlab/mmocr/branch/main/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmocr) [![license](https://img.shields.io/github/license/open-mmlab/mmocr.svg)](https://github.com/open-mmlab/mmocr/blob/main/LICENSE) [![PyPI](https://badge.fury.io/py/mmocr.svg)](https://pypi.org/project/mmocr/) [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmocr.svg)](https://github.com/open-mmlab/mmocr/issues) [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmocr.svg)](https://github.com/open-mmlab/mmocr/issues)  MMOCR is an open-source toolbox based on PyTorch and mmdetection for text detection  text recognition  and the corresponding downstream tasks including key information extraction. It is part of the [OpenMMLab](https://openmmlab.com/) project.  The main branch works with **PyTorch 1.6+**.  Documentation: https://mmocr.readthedocs.io/en/latest/.  <div align=""left"">   <img src=""resources/illustration.jpg""/> </div>   Please refer to our [Install Guide](https://mmocr.readthedocs.io/en/latest/install.html).   Please see [Getting Started](https://mmocr.readthedocs.io/en/latest/getting_started.html) for the basic usage of MMOCR.   """;Computer Vision;https://github.com/open-mmlab/mmocr
"""```bash git clone https://github.com/ShaojieJiang/tldr.git ~/tldr cd ~/tldr; python setup.py develop ```   The following command trains a transformer model with TLDR loss by default  on the DailyDialog dataset: ```bash python examples/train_model.py -m transformer/generator -t dailydialog ```  With arguments `--weight-level sequence` and `--weight-func fl`  you can then train a model using sequence-level weighting and Focal Loss  respectively.  For using our proposed Seq2seq with pre attention  please specify `-m seq2seq` and add argument `--attention-time pre`.  > :information_source: Our implementation of pre-attention is different than > that implemented in the original ParlAI framework.   """;General;https://github.com/ShaojieJiang/tldr
"""Open the python interpreter by typing `python` in your anaconda terminal and copy and run the following code ``` import fastai.utils fastai.utils.show_install(1) ```  If you get an output that looks something like this   you have successfully installed all libraries ``` === Software === python        : 3.8.2 fastai        : 1.0.61 fastprogress  : 0.2.3 torch         : 1.5.0 nvidia driver : 446.14 torch cuda    : 10.2 / is available torch cudnn   : 7604 / is enabled  === Hardware === nvidia gpus   : 1 torch devices : 1   - gpu0      : 8192MB | GeForce GTX 1070 Ti ```  If your output does not match this   you can find troubleshooting steps here : https://docs.fast.ai/troubleshoot.html   * The tool is written in **python** programming language and uses the **fastai** library which runs on top of **pytorch** framework. Hence you will need to install the latest version of **pytorch** along with **fastai** library.  * Nvidia GPU is required  * **I recommend using anaconda for installing pytorch and fastai**   Install Anaconda with python verion 3.x for your appropriate OS   Open Anaconda terminal and create a new environment if needed. ( Optional but recommended to avoid conflicts )  Install Pytorch framework by running the following command in the anaconda terminal https://pytorch.org/  conda install pytorch torchvision cudatoolkit=10.2 -c pytorch  Install the fastai library by running this command in anaconda terminal  pip install fastai   To Train your own model   you can use the train.ipynb jupyter notebook.   * To use this tool   first download the repo. * Open anaconda terminal and navigate to the downloaded folder using the `cd` command followed by the path of the folder ``` cd name-of-subfolder/sub-subfolder/ ``` * Activate the environment where you installed fastai library ( Optional )   """;Computer Vision;https://github.com/AtharvBhat/Plus-Ultra
"""1. Prepare python environment `python3 -m venv env` 1. Enter to the env `source env/bin/activate` 2. Install library `pip install -r requirements.txt`   Run python3 -m visdom.server in one terminal   """;General;https://github.com/chameleonTK/continual-learning-for-HAR
"""cd data   cd ..   """;Sequential;https://github.com/kaiidams/voice100
"""We included two Jupyter notebooks to demonstrate how the HDF5 datasets are created * For the medium scale datasets view `create_hdf_benchmarking_datasets.ipynb`. You will need `pytorch`  `ogb==1.1.1` and `dgl==0.4.2` libraries to run the notebook. The notebook is also runnable on Google Colaboratory. * For the large scale pcqm4m dataset view `create_hdf_pcqm4m.ipynb`. You will need `pytorch`  `ogb>=1.3.0` and `rdkit>=2019.03.1` to run the notebook.   This is the official implementation of the **Edge-augmented Graph Transformer (EGT)** as described in https://arxiv.org/abs/2108.03348  which augments the Transformer architecture with residual edge channels. The resultant architecture can directly process graph-structured data and acheives good results on supervised graph-learning tasks as presented by [Dwivedi et al.](https://arxiv.org/abs/2003.00982). It also achieves good performance on the large-scale [PCQM4M-LSC](https://arxiv.org/abs/2103.09430) (`0.1263 MAE` on val) dataset. EGT beats convolutional/message-passing graph neural networks on a wide range of supervised tasks and thus demonstrates that convolutional aggregation is not an essential inductive bias for graphs.   """;Natural Language Processing;https://github.com/shamim-hussain/egt
"""Requirements: - python3 - pytorch - torchvision - numpy - scipy - scikit-learn - Pillow  To compute the FID or KID score between two datasets with features extracted from inception net:  * Ensure that you have saved both datasets as numpy files (`.npy`) in channels-first format  i.e. `(no. of images  channels  height  width)`  ``` python fid_score.py --true path/to/real/images.npy --fake path/to/gan/generated.npy --gpu ID ``` ``` python kid_score.py --true path/to/real/images.npy --fake path/to/gan/generated.npy --gpu ID ```   """;General;https://github.com/abdulfatir/gan-metrics-pytorch
"""The pre-trained vgg_16.ckpt could be downloaded from http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz   """;Computer Vision;https://github.com/Stick-To/SSD-tensorflow
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/guoyaohua/BERT-Chinese-Annotation
"""To do this  you can use the following script: https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh   """;General;https://github.com/d-li14/mobilenetv3.pytorch
"""We use the implementation done by `lucidrains <https://github.com/lucidrains/lambda-networks>`_  .. code-block:: bash     pip install lambda-networks          """;Computer Vision;https://github.com/rajatsaini0294/lambda-mobilenets
"""For example  to train ViP with 8 GPU on a single node  run:   bash ./distributed_train.sh vip-t-001 configs/vip_t_bs1024.yaml 8   bash ./distributed_train.sh vip-s-001 configs/vip_s_bs1024.yaml 8   bash ./distributed_train.sh vip-m-001 configs/vip_m_bs1024.yaml 8   bash ./distributed_train.sh vip-b-001 configs/vip_b_bs1024.yaml 8   """;General;https://github.com/kevin-ssy/ViP
"""Download VGG19 model: bash models/download_vgg.sh   """;Computer Vision;https://github.com/eridgd/WCT-TF
"""DataParallel support to run on multi-gpu (single node) systems   Download and pre-process the celebA-HQ dataset. We recommend using the following code: https://github.com/nperraud/download-celebA-HQ   """;Computer Vision;https://github.com/hukkelas/progan-pytorch
"""1. Download CIFAR-100 dataset(the Python version) from [here](http://www.cs.toronto.edu/~kriz/cifar.html) 2. Unpack it to  ./DenseNet-Tensorflow  3. Run ```python3 DenseNet.py```  """;General;https://github.com/UESBTC/DenseNet-Tensorflow
"""You need Tensorlofw 2.x and numpy  You can install Tensorflow and numpy below code   pip install tensorflow  If you want to use gpu   pip install tensorflow-gpu  And you also need numpy  pip install numpy   git clone https://github.com/yw0nam/DenseNet   cd DenseNet   """;Computer Vision;https://github.com/yw0nam/DenseNet
"""This is the code base for our work of Generative Map at <https://arxiv.org/abs/1902.11124>.   It is an effort to combine generative model (in particular  [Variational Auto-Encoders](https://arxiv.org/abs/1312.6114)) and the classic Kalman filter for generation with localization.   For more details  please refer to [our paper on arXiv](https://arxiv.org/abs/1902.11124).   For training  one example use can be found below. The `PATH_TO_DATA_DIR` points to the data folder  for example in 7-Scenes it is the folder where you can find `TrainSplit.txt`  `TestSplit.txt`  and folders of `seq-xx`.   ``` python train.py --data_dir <PATH_TO_DATA_DIR> ```  For evaluation  one example of estimation with generation using a constant model is   ``` python eval.py --model_dir <PATH_TO_YOUR_MODEL_DIR_WITH_CHECKPOINTS> --generation_mode 2 --no_model ```   ``` usage: train.py [-h] [--training_size TRAINING_SIZE]                 [--dim_observation DIM_OBSERVATION] [--batch_size BATCH_SIZE]                 [--num_epochs NUM_EPOCHS] [--save_every SAVE_EVERY]                 [--model_dir MODEL_DIR] [--data_dir DATA_DIR]                 [--reconstruct_accuracy RECONSTRUCT_ACCURACY]                 [--dim_input DIM_INPUT DIM_INPUT DIM_INPUT]                 [--dim_reconstruct DIM_RECONSTRUCT DIM_RECONSTRUCT DIM_RECONSTRUCT]                 [--learning_rate LEARNING_RATE] [--use_robotcar]  optional arguments:   -h  --help            show this help message and exit   --training_size TRAINING_SIZE                         size of the training set  sampled from the given                         sequences   --dim_observation DIM_OBSERVATION                         dimension of the latent representation (""observation"")   --batch_size BATCH_SIZE                         mini-batch size for training   --num_epochs NUM_EPOCHS                         number of epochs for training   --save_every SAVE_EVERY                         intermediate saving frequency by epochs   --model_dir MODEL_DIR                         directory to save current model to   --data_dir DATA_DIR   directory to load the dataset   --reconstruct_accuracy RECONSTRUCT_ACCURACY                         how accurate should the reconstruction be  float                          negative and positive  the smaller the more accurate   --dim_input DIM_INPUT DIM_INPUT DIM_INPUT                         the height  width  and number of channel for input                         image  separated by space   --dim_reconstruct DIM_RECONSTRUCT DIM_RECONSTRUCT DIM_RECONSTRUCT                         the height  width  and number of channel for                         reconstructed image  separated by space   --learning_rate LEARNING_RATE                         learning rate   --use_robotcar        if the robotcar data loader will be used  since it is                         stored in a different format. ```    ``` usage: eval.py [-h] [--model_dir MODEL_DIR] [--state_var STATE_VAR]                [--visualize_dir VISUALIZE_DIR] [--video] [--no_model]                [--generation_mode GENERATION_MODE] [--all] [--seq_idx SEQ_IDX]                [--deviation DEVIATION]  optional arguments:   -h  --help            show this help message and exit   --model_dir MODEL_DIR                         directory to load model from. If not provided  the                         most recent dir in the default path will be used                          raise an Error if failed.   --state_var STATE_VAR                         estimated state transition variance for the model used                         by extended Kalman filter  it is assumed to be                         independent and stay the same for all dimension   --visualize_dir VISUALIZE_DIR                         directory to save the visualization output + video (if                         apply)   --video               if a video should be recorded comparing real and                         estimated reconstructed images  together with                         localization results   --no_model            if the true state transition will be hidden to test                         localization based only on images.   --generation_mode GENERATION_MODE                         0: equidistant sample image generation; 1: Kalman                         filter estimation for localization; 2: perform both   --all                 evaluate all the validation sequences and report the                         performance w/o a picture.   --seq_idx SEQ_IDX     evaluate the given indexed sequence in the text.   --deviation DEVIATION                         deviation across all state dimension for robust test. ```    """;General;https://github.com/Mingpan/generative_map
"""[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.   Getting the source code   $ wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip   $ cd fastText-0.1.0  $ make   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ mkdir build &amp;&amp; cd build &amp;&amp; cmake ..  $ make &amp;&amp; make install   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ pip install .   You can also quantize a supervised model to reduce its memory usage with the following command:   You can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.  There is also the master branch that contains all of our most recent work  but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.   This library has two main use cases: word representation learning and text classification. These were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).   """;Natural Language Processing;https://github.com/romik9999/fasttext-1925f09ed3
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/LIU1514/Yolov3-
"""Simply run either: ``` python generate_lemons.py --type=healthy ``` or: ``` python generate_lemons.py --type=unhealthy ```  To change the weights  line 44 defines which model to use: ``` model = load_model('lemons_generator_1500.h5') ``` In the form: 'lemons_generator_*epochnum*.h5'  Then  1000 jpg files will be generated of lemons that are either healthy or unhealthy   """;Computer Vision;https://github.com/jordan-bird/synthetic-fruit-image-generator
"""- eval_input_reader 里面的shuffle， 这个是跟eval步骤的数据reader有关，如果不使用GPU进行训练的话，这里需要从false改成true，不然会导致错误，详细内容参阅 https://github.com/tensorflow/models/issues/1936   windows 命令行不支持 * 操作符。   """;Computer Vision;https://github.com/pessimiss/ai100-w8-master
"""``` pip install -r requirements.txt ``` For the PyTorch installation  please follow [this guide](http://pytorch.org).   The output on 5k iteration (for default configuration): ![2017-11-15 21-42-34](https://user-images.githubusercontent.com/3521007/32853968-22e3659c-ca4e-11e7-9eeb-c04663f33388.png) 5 minute after: ![2017-11-15 21-47-37](https://user-images.githubusercontent.com/3521007/32854134-b179687e-ca4e-11e7-81cd-eaf52ecc71fb.png) 2 hours after: ![2017-11-16 00-07-49](https://user-images.githubusercontent.com/3521007/32860436-a795ea62-ca62-11e7-979f-a65512605dbe.png)   """;Computer Vision;https://github.com/Shito0907/cat_gen
"""Grunnet at tf 1.6.0 ikke støtter alle cuda versjoner måtte jeg installere cuda 9.0: sudo apt-get -y install cuda-toolkit-9.0 -->   Set up to run on python 3   <a href='g3doc/installation.md'>Installation.</a><br>   To get help with issues you may encounter while using the DeepLab Tensorflow implementation  create a new question on [StackOverflow](https://stackoverflow.com/) with the tags ""tensorflow"" and ""deeplab"".  Please report bugs (i.e.  broken code  not usage questions) to the tensorflow/models GitHub [issue tracker](https://github.com/tensorflow/models/issues)  prefixing the issue name with ""deeplab"".   """;General;https://github.com/mathildor/DeepLab-v3
"""- tensorflow/tensorflow:tensorflow:2.4.0-gpu-jupyter   - Pytorch : Stable (1.7.1) - Linux - Python - CUDA (11.0)   - Using Single GPU (not tested on cpu only)   """;Computer Vision;https://github.com/Jasonlee1995/DeepLab_v1
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.    GPUs: K80 ($0.14/hr)  T4 ($0.11/hr)  V100 ($0.74/hr) CUDA with Nvidia Apex FP16/32     Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [Notebook](https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb) <a href=""https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a> * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) << highly recommended * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart)  ![Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/yolov3?logo=docker) * [A TensorRT Implementation of YOLOv3 and YOLOv4](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp)     """;Computer Vision;https://github.com/liuji93/yolo_v3
"""Install via pip: ```bash pip install efficientnet_pytorch ```  Or install from source: ```bash git clone https://github.com/lukemelas/EfficientNet-PyTorch cd EfficientNet-Pytorch pip install -e . ```   Install with pip install efficientnet_pytorch and load a pretrained EfficientNet with:   <img src=""https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnetv2-image.png"" width=""100%"" />   Upgrade the pip package with pip install --upgrade efficientnet-pytorch   Also: fixed a CUDA/CPU bug (#32)   At the moment  you can easily:   If you have any feature requests or questions  feel free to leave them as GitHub issues!   |    Name         |*   Below is a simple  complete example. It may also be found as a jupyter notebook in `examples/simple` or as a [Colab Notebook](https://colab.research.google.com/drive/1Jw28xZ1NJq4Cja4jLe6tJ6_F5lCzElb4).  We assume that in your current directory  there is a `img.jpg` file and a `labels_map.txt` file (ImageNet class names). These are both included in `examples/simple`.  ```python import json from PIL import Image import torch from torchvision import transforms  from efficientnet_pytorch import EfficientNet model = EfficientNet.from_pretrained('efficientnet-b0')  #: Preprocess image tfms = transforms.Compose([transforms.Resize(224)  transforms.ToTensor()      transforms.Normalize([0.485  0.456  0.406]  [0.229  0.224  0.225]) ]) img = tfms(Image.open('img.jpg')).unsqueeze(0) print(img.shape) #: torch.Size([1  3  224  224])  #: Load ImageNet class names labels_map = json.load(open('labels_map.txt')) labels_map = [labels_map[str(i)] for i in range(1000)]  #: Classify model.eval() with torch.no_grad():     outputs = model(img)  #: Print predictions print('-----') for idx in torch.topk(outputs  k=5).indices.squeeze(0).tolist():     prob = torch.softmax(outputs  dim=1)[0  idx].item()     print('{label:<75} ({p:.2f}%)'.format(label=labels_map[idx]  p=prob*100)) ```   You can easily extract features with `model.extract_features`: ```python from efficientnet_pytorch import EfficientNet model = EfficientNet.from_pretrained('efficientnet-b0')  #: ... image preprocessing as in the classification example ... print(img.shape) #: torch.Size([1  3  224  224])  features = model.extract_features(img) print(features.shape) #: torch.Size([1  1280  7  7]) ```   Exporting to ONNX for deploying to production is now simple: ```python import torch from efficientnet_pytorch import EfficientNet  model = EfficientNet.from_pretrained('efficientnet-b1') dummy_input = torch.randn(10  3  240  240)  model.set_swish(memory_efficient=False) torch.onnx.export(model  dummy_input  ""test-b1.onnx""  verbose=True) ```  [Here](https://colab.research.google.com/drive/1rOAEXeXHaA8uo3aG2YcFDHItlRJMV0VP) is a Colab example.    """;Computer Vision;https://github.com/lukemelas/EfficientNet-PyTorch
"""found here:- https://github.com/openai/multiagent-particle-envs.   """;Reinforcement Learning;https://github.com/marwanihab/RL_Testing_Noise_ASRN
"""[ ] Create one self-contained notebook   """;Natural Language Processing;https://github.com/jackbandy/deep_learning_ulmfit
"""Reproduce the results with the pre-trained model:  	python ./experiments/testing/ROLO_network_test_all.py  Or download the results at [Results](http://).  Run video Demo:  	./python ROLO_demo_test.py    """;Computer Vision;https://github.com/Guanghan/ROLO
"""Supports PyTorch 1.0   To run the segmentation demo  just type:  ``` python segmentation_demo.py ```  To run the detection demo  run the following command: ```  python detection_demo.py  OR   python detection_demo.py --live ```  For other supported arguments  please see the corresponding files.    """;General;https://github.com/sacmehta/EdgeNets
"""```bash $ pip install mixture_of_experts ```   ```python import torch from torch import nn from mixture_of_experts import MoE  moe = MoE(     dim = 512      num_experts = 16                #: increase the experts (#: parameters) of your model without increasing computation     hidden_dim = 512 * 4            #: size of hidden dimension in each expert  defaults to 4 * dimension     activation = nn.LeakyReLU       #: use your preferred activation  will default to GELU     second_policy_train = 'random'  #: in top_2 gating  policy for whether to use a second-place expert     second_policy_eval = 'random'   #: all (always) | none (never) | threshold (if gate value > the given threshold) | random (if gate value > threshold * random_uniform(0  1))     second_threshold_train = 0.2      second_threshold_eval = 0.2      capacity_factor_train = 1.25    #: experts have fixed capacity per batch. we need some extra capacity in case gating is not perfectly balanced.     capacity_factor_eval = 2.       #: capacity_factor_* should be set to a value >=1     loss_coef = 1e-2                #: multiplier on the auxiliary expert balancing auxiliary loss )  inputs = torch.randn(4  1024  512) out  aux_loss = moe(inputs) #: (4  1024  512)  (1 ) ```  The above should suffice for a single machine  but if you want a heirarchical mixture of experts (2 levels)  as used in the GShard paper  please follow the instructions below  ```python import torch from mixture_of_experts import HeirarchicalMoE  moe = HeirarchicalMoE(     dim = 512      num_experts = (4  4)        #: 4 gates on the first layer  then 4 experts on the second  equaling 16 experts )  inputs = torch.randn(4  1024  512) out  aux_loss = moe(inputs) #: (4  1024  512)  (1 ) ```  1 billion parameters  ```python import torch from mixture_of_experts import HeirarchicalMoE  moe = HeirarchicalMoE(     dim = 512      num_experts = (22  22) ).cuda()  inputs = torch.randn(1  1024  512).cuda() out  aux_loss = moe(inputs)  total_params = sum(p.numel() for p in moe.parameters()) print(f'number of parameters - {total_params}') ```  If you want some more sophisticated network for the experts  you can define your own and pass it into the `MoE` class as `experts`  ```python import torch from torch import nn from mixture_of_experts import MoE  #: a 3 layered MLP as the experts  class Experts(nn.Module):     def __init__(self  dim  num_experts = 16):         super().__init__()         self.w1 = nn.Parameter(torch.randn(num_experts  dim  dim * 4))         self.w2 = nn.Parameter(torch.randn(num_experts  dim * 4  dim * 4))         self.w3 = nn.Parameter(torch.randn(num_experts  dim * 4  dim))         self.act = nn.LeakyReLU(inplace = True)      def forward(self  x):         hidden1 = self.act(torch.einsum('end edh->enh'  x  self.w1))         hidden2 = self.act(torch.einsum('end edh->enh'  hidden1  self.w2))         out = torch.einsum('end edh->enh'  hidden2  self.w3)         return out  experts = Experts(512  num_experts = 16)  moe = MoE(     dim = 512      num_experts = 16      experts = experts )  inputs = torch.randn(4  1024  512) out  aux_loss = moe(inputs) #: (4  1024  512)  (1 ) ```   """;General;https://github.com/lucidrains/mixture-of-experts
"""Here collects my study notes  including programming languages  deep learning / machine learning papers  mathematical simulation notes  from a biochem major perspective.   I will focus on the following five directions:   """;Reinforcement Learning;https://github.com/liyiyuian/Deep-Learning
"""We are using Python framework - Pytorch  for writing our code. Basic computational and data manipulation libraries are used like NumPy  SciPy  Pandas  Matplotlib  torchvision. It is not advised to go through all these libraries and then start coding. Best apporach is to start coding and understand it parallelly.   """;Computer Vision;https://github.com/JitindraFartiyal/Object-Detection
"""```bash $ bash download.sh CelebA or $ bash download.sh LSUN ```    $ git clone https://github.com/heykeetae/Self-Attention-GAN.git  $ cd Self-Attention-GAN   $ cd samples/sagan_celeb   $ cd samples/sagan_lsun   """;General;https://github.com/heykeetae/Self-Attention-GAN
"""Simply run `lumi --help`.   First  clone the repo on your machine and then install with `pip`:  ```bash git clone https://github.com/tryolabs/luminoth.git cd luminoth pip install -e . ```   Just install from PyPI:  ```bash pip install luminoth ```  Optionally  Luminoth can also install TensorFlow for you if you install it with `pip install luminoth[tf]` or `pip install luminoth[tf-gpu]`  depending on the version of TensorFlow you wish to use.   Luminoth currently supports Python 2.7 and 3.4–3.6.   If you wish to train using Google Cloud ML Engine  the optional dependencies must be installed:  pip install luminoth[gcloud]   By default summary and graph logs are saved to jobs/ under the current directory. You can use TensorBoard by running:  tensorboard --logdir path/to/jobs   There is one main command line interface which you can use with the `lumi` command. Whenever you are confused on how you are supposed to do something just type:  `lumi --help` or `lumi <subcommand> --help`  and a list of available options with descriptions will show up.   """;General;https://github.com/tryolabs/luminoth
"""This repository is build for the proposed self-attention network (SAN)  which contains full training and testing code. The implementation of SA module with optimized CUDA kernels are also included.  <p align=""center""><img src=""./figure/sa.jpg"" width=""400""/></p>   1. Requirement:     - Hardware: tested with 8 x Quadro RTX 6000 (24G).    - Software: tested with PyTorch 1.4.0  Python3.7  CUDA 10.1  [CuPy](https://cupy.chainer.org/) 10.1  [tensorboardX](https://github.com/lanpa/tensorboardX).  2. Clone the repository:     ```shell    git clone https://github.com/hszhao/SAN.git    ```     3. Train:     - Download and [prepare](https://github.com/facebookarchive/fb.resnet.torch/blob/master/INSTALL.md) the ImageNet dataset (ILSVRC2012) and symlink the path to it as follows (you can alternatively modify the relevant path specified in folder `config`):       ```      cd SAN      mkdir -p dataset      ln -s /path_to_ILSVRC2012_dataset dataset/ILSVRC2012      ```     - Specify the gpus (usually 8 gpus are adopted) used in config and then do training:       ```      sh tool/train.sh imagenet san10_pairwise      ```          - If you are using [SLURM](https://slurm.schedmd.com/documentation.html) for nodes manager  uncomment lines in `train.sh` and then do training:       ```shell      sbatch tool/train.sh imagenet san10_pairwise      ```  4. Test:     - Download trained SAN [models](https://drive.google.com/open?id=19ZJn48UpiF_j8e1-UU9UZ4AVfLjmwPzH) and put them under folder specified in config or modify the specified paths  and then do testing:       ```shell      sh tool/test.sh imagenet san10_pairwise      ```     5. Visualization:     - [tensorboardX](https://github.com/lanpa/tensorboardX) incorporated for better visualization regarding curves:       ```shell      tensorboard --logdir=exp/imagenet      ```  6. Other:     - Resources: GoogleDrive [LINK](https://drive.google.com/open?id=19ZJn48UpiF_j8e1-UU9UZ4AVfLjmwPzH) contains shared models.   """;Computer Vision;https://github.com/hszhao/SAN
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;General;https://github.com/ShaharrHorn/Detection-Game-logic
"""This application is an implementation of People tracking using Tensorflow implementation of MobileNet V3 + SSD[1] and DeepSort[2]. This could very well be extended to any other objects for tracking like cars or animals.   """;Computer Vision;https://github.com/kshitij1489/object-tracking
"""The camera calibration code for this step could be found on the [Step_1_Camera_Calibration notebook](Step_1_Camera_Calibration.ipynb).  I start by preparing ""object points""  which will be the (x  y  z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x  y) plane at z=0  such that the object points are the same for each calibration image.  Thus  `objp` is just a replicated array of coordinates  and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x  y) pixel position of each of the corners in the image plane with each successful chessboard detection.    I then used the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  I applied this distortion correction to the test image using the `cv2.undistort()` function.  Using `cv2.findChessboardCorners`  the corners points are stored in an array `imgpoints` for each calibration image where the chessboard could be found. The object points will always be the same as the known coordinates of the chessboard with zero as 'z' coordinate because the chessboard is flat. The object points are stored in an array called `objpoints`.   I then used the output objpoints and imgpoints to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera` function. I applied this distortion correction to the test image using the `cv2.undistort` function and obtained this result:  ![Camera calibratio undistort result](./results_images/camera_calibration_undistort_result.PNG)  The camera calibration and distortion coefficients(mtx  dist) are stored using `pickle`(in folder ./pickled_data/camera_calibration.p) to be used on the main [notebook](Project.%20Advance%20Lane%20Lines.ipynb)  A short description of the images in [Step_1_Camera_Calibration notebook](Step_1_Camera_Calibration.ipynb) to compute the camera calibration matrix and distortion coefficients given a set of chessboard images: ``` readChessImages(): Load calibration images(chess boards in different angles from our camera_cal folder) showImages(calibrationImages): Method for displaying the images findingPoints(calibrationImages): Method for detecting the points in chess images (objpoints and imgpoints) getCoefficients(objpoints  imgpoints  originalImage): Method for returning camera matrix  mtx  and the distortion coefficients dist  HOW TO USE THESE FUNCTIONS:   #: Read images calibrationImages = readChessImages() #: Return object points and image points objpoints  imgpoints  originalImages  outimages = findingPoints(calibrationImages) #: Save coefficients into pickle originalImage = originalImages[10]  #:Getting the coefficients we need for camera calibration mtx dist = getCoefficients(objpoints  imgpoints  originalImage) ```  To demonstrate this step  I will describe how I apply the distortion correction to one of the test images like this one: ![Camera calibration](./results_images/original-to-undistort.png)    The code used to experiment with color  gradients  and thresholds could be found on the [Step 2. Color Transform and Gradients Threshold notebook](Step_2_Color_Transform_and_Gradients_Threshold.ipynb).  A color transformation to HLS was done  and the S channel was selected because it shows more contracts on the lane lines. Here are the results for all the testing images: ![S_channel_result](./results_images/s_channel.png)  Next step was to calculte the gradients: - Sobel X and Sobel Y: `In [7]` and `In [8]` - Magnitude : `In [10]` - Gradient direction : `In [12]` - Combination of all the above (Sobel X and Sobel Y) or (Magnitude and Gradient): `In [13]`  After applying thresholds here are the results: ![Results combined](./results_images/show_compared_results.png)  We can observe that the combination of these gradients leads to a ""noisy"" binary images. For this reason  on the main project  presented in the [Project. Advance Lane Lines notebook](Project.%20Advance%20Lane%20Lines.ipynb) notebook  only the combination of `Sobel X` and `Sobel Y` was used to continue with the pipeline.    The perspective transformation code could be found on [Step_3 Apply Perspective transformation notebook(Step_3_Apply_Perspective_transformation.ipynb).   Four points where selected on the first image as the source of the perspective transformation. Those points are highlighted on the following image: ![Straight lane lines](./results_images/draw_lines.png)  How we draw the vertices can be found in the method `get_Rectangle_Vertices(image)` in `In [4]`: ``` imgHeight = int(image.shape[0]) imgWidth = int(image.shape[1])  vertice_1 = (190  imgHeight) vertice_2 = (int(imgWidth/2 - imgWidth*.038)  int(imgHeight/2 + imgHeight*.08 + 30)) vertice_3 = (int(imgWidth/2 + imgWidth*.038)  int(imgHeight/2 + imgHeight*.08 + 30)) vertice_4 = (imgWidth-160  imgHeight) vertices = (vertice_1  vertice_2  vertice_3  vertice_4) ``` Using `cv2.getPerspectiveTransform`  a transformation matrix was calculated  and an inverse transformation matrix was also calculated to map the points back to the original space (In [6]). The result of the transformation on a test image is the following: ![Perspective transformation of the lines](./results_images/image_side_by_side.png)  The transformation matrix and the inverse transformation matrix was stored using `pickle` to be used on the main project. The following picture shows the binary images results after the perspective transformation: ![Binary images transformed](./results_images/result_perspective.png)  ![Detect Lines & Determine Curvature](./results_images/lane_curvature.PNG)  Radius of Curvature  awsome tutorial here: https://www.intmath.com/applications-differentiation/8-radius-curvature.php   To display the lane lines on the image  the polynomials where evaluated on a lineal space of the Y coordinates.  The generated points where mapped back to the image space using the inverse transformation matrix generated by the perspective transformation. The following images are examples of this mapping: ![Lane lines fit](./results_images/pipeline.png)   """;Computer Vision;https://github.com/Ceachi/Project-Self-Driving-Car-Advanced-Lane-Lines
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;Computer Vision;https://github.com/cbhower/style-gan-music-video
"""Install PyTorch 0.4   The codebase is now PyTorch 0.4 compatible for most use cases (a big shoutout to https://github.com/shawntan for a fairly comprehensive PR https://github.com/salesforce/awd-lstm-lm/pull/43). Mild readjustments to hyperparameters may be necessary to obtain quoted performance. If you desire exact reproducibility (or wish to run on PyTorch 0.3 or lower)  we suggest using an older commit of this repository. We are still working on pointer  finetune and generate functionalities.   For data setup  run ./getdata.sh.   PyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed.   """;Sequential;https://github.com/AtheMathmo/lookahead-lstm
"""- Dont sample from a Uniform distribution  ![cube](images/cube.png ""Cube"")  - Sample from a gaussian distribution  ![sphere](images/sphere.png ""Sphere"")  - When doing interpolations  do the interpolation via a great circle  rather than a straight line from point A to point B - Tom White's [Sampling Generative Networks](https://arxiv.org/abs/1609.04468) ref code https://github.com/dribnet/plat has more details    - Label Smoothing  i.e. if you have two target labels: Real=1 and Fake=0  then for each incoming sample  if it is real  then replace the label with a random number between 0.7 and 1.2  and if it is a fake sample  replace it with 0.0 and 0.3 (for example).   - Salimans et. al. 2016 - make the labels the noisy for the discriminator: occasionally flip the labels when training the discriminator   - Experience Replay   - Keep a replay buffer of past generations and occassionally show them   - Keep checkpoints from the past of G and D and occassionaly swap them out for a few iterations - All stability tricks that work for deep deterministic policy gradients - See Pfau & Vinyals (2016)   - optim.Adam rules!   - See Radford et. al. 2015 - Use SGD for discriminator and ADAM for generator   - if you have labels available  training the discriminator to also classify the samples: auxillary GANs   - Provide noise in the form of dropout (50%). - Apply on several layers of our generator at both training and test time - https://arxiv.org/pdf/1611.07004v1.pdf    """;Computer Vision;https://github.com/kingcheng2000/GAN
"""This project is built around the wonderful Fast.AI library.  Prereqs  in summary: * **Fast.AI 1.0.51** (and its dependencies) * **Jupyter Lab** `conda install -c conda-forge jupyterlab` * **Tensorboard** (i.e. install Tensorflow) and **TensorboardX** (https://github.com/lanpa/tensorboardX).  I guess you don't *have* to but man  life is so much better with it.  FastAI now comes with built in support for this- you just  need to install the prereqs: `conda install -c anaconda tensorflow-gpu` and `pip install tensorboardX` * **ImageNet** – Only if you're training  of course. It has proven to be a great dataset for my purposes.  http://www.image-net.org/download-images  --------------------------  You should now be able to do a simple install with Anaconda. Here are the steps:  Open the command line and navigate to the root folder you wish to install.  Then type the following commands  ```console git clone https://github.com/jantic/DeOldify.git DeOldify cd DeOldify conda env create -f environment.yml ``` Then start running with these commands: ```console source activate deoldify jupyter lab ```  From there you can start running the notebooks in Jupyter Lab  via the url they provide you in the console.     Get more updates on Twitter <img src=""resource_images/Twitter_Social_Icon_Rounded_Square_Color.svg"" width=""16"">   git clone https://github.com/jantic/DeOldify.git DeOldify   cd DeOldify &amp;&amp; docker build -t deoldify_jupyter -f Dockerfile .   git clone https://github.com/jantic/DeOldify.git DeOldify   [![](http://img.youtube.com/vi/l3UXXid04Ys/0.jpg)](http://www.youtube.com/watch?v=l3UXXid04Ys """")   [![](http://img.youtube.com/vi/EXn-n2iqEjI/0.jpg)](http://www.youtube.com/watch?v=EXn-n2iqEjI """")  -----------------------   ""Migrant Mother"" by Dorothea Lange (1936)  ![MigrantMother](resource_images/10_dorothea-lange_artistic_RF25_compared_sm.jpg)   Woman relaxing in her livingroom in Sweden (1920)  ![SwedenLivingRoom](resource_images/0_LivingRoom1920Sweden_artistic_RF46_compared_sm.jpg)   ""Toffs and Toughs"" by Jimmy Sime (1937)  ![ClassDivide](resource_images/1_ClassDivide1930sBrittain_artistic_RF30_compared_sm.jpg)   Thanksgiving Maskers (1911)  ![ThanksgivingMaskers](resource_images/2_1911ThanksgivingMaskers_artistic_RF36_compared_sm.jpg)   Glen Echo Madame Careta Gypsy Camp in Maryland (1925)  ![GypsyCamp](resource_images/3_1925GypsyCampMaryland_artistic_RF45_compared_sm.jpg)   ""Mr. and Mrs. Lemuel Smith and their younger children in their farm house  Carroll County  Georgia."" (1941)  ![GeorgiaFarmhouse](resource_images/4_1941GeorgiaFarmhouse_stable_RF43_compared_sm.jpg)    ""Building the Golden Gate Bridge"" (est 1937)  ![GoldenGateBridge](resource_images/5_GoldenGateConstruction_stable_RF45_compared_sm.jpg) <sub>NOTE:  What you might be wondering is while this render looks cool  are the colors accurate? The original photo certainly makes it look like the towers of the bridge could be white. We looked into this and it turns out the answer is no- the towers were already covered in red primer by this time. So that's something to keep in mind- historical accuracy remains a huge challenge!</sub>   ""Terrasse de café  Paris"" (1925)  ![CafeParis](resource_images/6_CafeTerrace1925Paris_artistic_RF37_compared_sm.jpg)   Norwegian Bride (est late 1890s)  ![NorwegianBride](resource_images/7_NorwegianBride1890s_artistic_RF40_compared_sm.jpg)   Zitkála-Šá (Lakota: Red Bird)  also known as Gertrude Simmons Bonnin (1898)  ![NativeWoman](resource_images/8_NativeWoman1898_artistic_RF19_compared_sm.jpg)   Chinese Opium Smokers (1880)  ![OpiumReal](resource_images/9_ChinaOpiumc1880_artistic_RF43_compared_sm.jpg)  -------------------------  So that's the gist of this project – I'm looking to make old photos and film look reeeeaaally good with GANs  and more importantly  make the project *useful*.  In the meantime though this is going to be my baby and I'll be actively updating and improving the code over the foreseeable future.  I'll try to make this as user-friendly as possible  but I'm sure there's going to be hiccups along the way.    Oh and I swear I'll document the code properly...eventually.  Admittedly I'm *one of those* people who believes in ""self documenting code"" (LOL).  -----------------------   The easiest way to get started is to go straight to the Colab notebooks:   Image [<img src=""https://colab.research.google.com/assets/colab-badge.svg"" align=""center"">](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb) | Video [<img src=""https://colab.research.google.com/assets/colab-badge.svg"" align=""center"">](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb)   Special thanks to Matt Robinson and María Benavente for their image Colab notebook contributions  and Robert Bell for the video Colab notebook work!  -----------------------   """;Computer Vision;https://github.com/recluse27/Colorizator
""" ``` git clone https://github.com/float256/rectified-adam-keras.git cd rectified-adam-keras pip3 install . ```  Required Libraries   """;General;https://github.com/float256/rectified-adam-keras
"""We use Slurm and submitit (pip install submitit). To train on 2 nodes with 8 GPUs each (total 16 GPUs):   If you choose not to specify --pretrained_weights  then DINO reference weights are used by default. If you want instead to evaluate checkpoints from a run of your own  you can run for example:   Please verify that you're using pytorch version 1.7.1 since we are not able to reproduce the results with most recent pytorch 1.8.1 at the moment.   cd $HOME  git clone https://github.com/davisvideochallenge/davis-2017 &amp;&amp; cd davis-2017   git clone https://github.com/davisvideochallenge/davis2017-evaluation $HOME/davis2017-evaluation   """;Computer Vision;https://github.com/facebookresearch/dino
"""Python 3.5  Tensorflow-gpu version:  1.4.0rc2    """;Computer Vision;https://github.com/MINGUKKANG/Adversarial-AutoEncoder
"""python B0_global.py (training requires CUDA enabled GPU with VRAM of at least 4GB - tested on machine with 8GB VRAM)   NOTE: These instructions are for use on a Windows machine. These may not work for Linux  MacOS  or other Operating Systems   These papers can be found at the following links:   * COCO   ![](graphics/B0_KERNEL9/trnacc_B0.png)   ![](graphics/B0_KERNEL9/trnloss_B0.png)  We can clearly see from the above graphs that there is a logarithmic relationship between epoch and accuracy/loss.   """;Computer Vision;https://github.com/JacobM184/EfficientNet-for-Gun-detection
"""Some optimization options might be disabled in the precompiled pip versions. Try to enable all optimizations to get maximum performances. You have to compile the frameworks by yourself.   Mobilenet V2 is a safe choice. It have been tested in various problems  and you can find implementations and pretrained models for most deep learning frameworks. Also  the operations in Mobilenet are highly optimized in the frameworks.   You should be careful when you try a new architecture. A new architecture having fewer parameters can be slower when the architecture is not optimized for CPUs and mobile environments.    GPU is definitely faster than CPU. It is true for mobile phones. TF-lite [https://www.tensorflow.org/lite/performance/gpu] supports mobile GPUs.   Weight quantization is a good way to reduces inference latency without much loss in accuracy. See links below.   TF lite [https://www.tensorflow.org/lite/performance/post_training_quantization]   QNNPACK for Caffe2 [https://code.fb.com/ml-applications/qnnpack/]  Be aware that the operations in your network should have implementations for quantization.    """;Computer Vision;https://github.com/youngwoo-yoon/Practical-tips-for-lightweight-deep-learning
"""Faster R-CNN has two networks: region proposal network (RPN) for generating region proposals and a network using these proposals to detect objects. The main different here with Fast R-CNN is that the later uses selective search to generate region proposals. The time cost of generating region proposals is much smaller in RPN than selective search  when RPN shares the most computation with the object detection network. Briefly  RPN ranks region boxes (called anchors) and proposes the ones most likely containing objects.   """;Computer Vision;https://github.com/KushaalShroff/Signature-and-Annotation-detection
"""1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   Requirements: hardware  Basic installation   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   To train and test a Faster R-CNN detector using the **alternating optimization** algorithm from our NIPS 2015 paper  use `experiments/scripts/faster_rcnn_alt_opt.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  (""alt opt"" refers to the alternating optimization training algorithm described in the NIPS paper.)  To train and test a Faster R-CNN detector using the **approximate joint training** method  use `experiments/scripts/faster_rcnn_end2end.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  This method trains the RPN module jointly with the Fast R-CNN network  rather than alternating between training the two. It results in faster (~ 1.5x speedup) training times and similar detection accuracy. See these [slides](https://www.dropbox.com/s/xtr4yd4i5e0vw8g/iccv15_tutorial_training_rbg.pdf?dl=0) for more details.  Artifacts generated by the scripts in `tools` are written in this directory.  Trained Fast R-CNN networks are saved under:  ``` output/<experiment directory>/<dataset name>/ ```  Test outputs are saved under:  ``` output/<experiment directory>/<dataset name>/<network snapshot name>/ ```  """;Computer Vision;https://github.com/GAOwy123/py-faster-rcnn
"""Download the convolutionalized VGG-16 weights   If you would like to use one of the provided trained models for transfer learning (i.e. fine-tune one of the trained models on your own dataset)  there is a Jupyter notebook tutorial that helps you sub-sample the trained weights so that they are compatible with your dataset  see further below.   Download the weights for the convolutionalized VGG-16 or for one of the trained original models provided below.   Below are some prediction examples of the fully trained original SSD300 ""07+12"" model (i.e. trained on Pascal VOC2007 `trainval` and VOC2012 `trainval`). The predictions were made on Pascal VOC2007 `test`.  | | | |---|---| | ![img01](./examples/trained_ssd300_pascalVOC2007_test_pred_05_no_gt.png) | ![img01](./examples/trained_ssd300_pascalVOC2007_test_pred_04_no_gt.png) | | ![img01](./examples/trained_ssd300_pascalVOC2007_test_pred_01_no_gt.png) | ![img01](./examples/trained_ssd300_pascalVOC2007_test_pred_02_no_gt.png) |  Here are some prediction examples of an SSD7 (i.e. the small 7-layer version) partially trained on two road traffic datasets released by [Udacity](https://github.com/udacity/self-driving-car/tree/master/annotations) with roughly 20 000 images in total and 5 object categories (more info in [`ssd7_training.ipynb`](ssd7_training.ipynb)). The predictions you see below were made after 10 000 training steps at batch size 32. Admittedly  cars are comparatively easy objects to detect and I picked a few of the better examples  but it is nonetheless remarkable what such a small model can do after only 10 000 training iterations.  | | | |---|---| | ![img01](./examples/ssd7_udacity_traffic_pred_01.png) | ![img01](./examples/ssd7_udacity_traffic_pred_02.png) | | ![img01](./examples/ssd7_udacity_traffic_pred_03.png) | ![img01](./examples/ssd7_udacity_traffic_pred_04.png) |   This repository provides Jupyter notebook tutorials that explain training  inference and evaluation  and there are a bunch of explanations in the subsequent sections that complement the notebooks.  How to use a trained model for inference: * [`ssd300_inference.ipynb`](ssd300_inference.ipynb) * [`ssd512_inference.ipynb`](ssd512_inference.ipynb)  How to train a model: * [`ssd300_training.ipynb`](ssd300_training.ipynb) * [`ssd7_training.ipynb`](ssd7_training.ipynb)  How to use one of the provided trained models for transfer learning on your own dataset: * [Read below](#how-to-fine-tune-one-of-the-trained-models-on-your-own-dataset)  How to evaluate a trained model: * In general: [`ssd300_evaluation.ipynb`](ssd300_evaluation.ipynb) * On MS COCO: [`ssd300_evaluation_COCO.ipynb`](ssd300_evaluation_COCO.ipynb)  How to use the data generator: * The data generator used here has its own repository with a detailed tutorial [here](https://github.com/pierluigiferrari/data_generator_object_detection_2d)   """;General;https://github.com/pierluigiferrari/ssd_keras
"""All the parameters related to training/decoding will be stored in a yaml file. Hyperparameter tuning and experiments can be managed easily this way. See [documentation and examples](config/) for the exact format. **Note that the example configs provided were not fine-tuned**  you may want to write your own config for best performance.    You may checkout some example log files with TensorBoard by downloading them from coming soon   - Single GPU   |cudnn-ctc| Use CuDNN as the backend of PyTorch CTC. Unstable  see this issue  not sure if solved in latest Pytorch with cudnn version > 7.6|   """;General;https://github.com/s3prl/End-to-end-ASR-Pytorch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   This is a fork of the original (Google's) BERT implementation.   * Add Multi-GPU support with Horovod  This [blog](https://lambdalabs.com/blog/bert-multi-gpu-implementation-using-tensorflow-and-horovod-with-code/) explains all the changes we made to the original implementation.  __Install__ Please first [install Horovod](https://github.com/uber/horovod#install)  __Run__ See the commands in each section to run BERT with Multi-GPUs:  * [Sentence (and sentence-pair) classification tasks](#sentencepair)  * [SQuAD 1.1](#squad1.1)  * [SQuAD 2.0](#squad2.0)  * [Pre-training](#pretraining)      PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/dbonner/tf1_bert
"""- Clone this repo: ```bash git clone https://github.com/ImagingLab/Colorizing-with-GANs.git cd Colorizing-with-GANs ``` - Install Tensorflow and dependencies from https://www.tensorflow.org/install/ - Install python requirements: ```bash pip install -r requirements.txt ```   """;Computer Vision;https://github.com/ImagingLab/Colorizing-with-GANs
"""* PyTorch 1.0.1   * imageio 2.5.0   TODO   """;Computer Vision;https://github.com/kilgore92/Probabalistic-U-Net
"""The experiments provided in this code base include active learning on standard vision based datasets (classification) and UCI datasets (regression). The following experiments are provided (see section 7 of the paper):  1. Active learning for regression: run the following command        ```./scripts/run_active_regression.sh DATASET ACQ CORESET```        where      - ```DATASET``` may be one of ```{yacht  boston  energy  power  year}``` (determines the dataset to be used).     - ```ACQ```  may be one of ```{BALD  Entropy  ACS}``` (determines the acquisition function to be used).     - ```CORESET``` may be one of ```{Argmax  Random  Best  FW}``` (determines the querying strategy to be used)        For example  to run the proposed method on the boston dataset  please run:          ```./scripts/run_active_regression.sh boston ACS FW```      This will automatically generate an experimental directory with the appropriate name  and place results from 40 seeds in     the directory. Hyper-parameters for the experiments can all be found in the main body of the paper.      2. Active learning for regression (with projections -- should be used for large datasets e.g.  year and power): run the following command        ```./scripts/run_active_regression_projections.sh DATASET NUM_PROJECTIONS```        where      - ```DATASET``` may be one of ```{yacht  boston  power  protein  year}``` (determines the dataset to be used).     - ```NUM_PROJECTIONS```  is an integer (determines the number of samples used to estimate values).        For example  to run the proposed method on the year dataset  please run:          ```./scripts/run_active_regression.sh year 10```      This will automatically generate an experimental directory with the appropriate name  and place results from 40 seeds in     the directory. Hyper-parameters for the experiments can all be found in the main body of the paper.      3. Active learning for classification (using standard active learning methods): run the following command     ```./scripts/run_active_torchvision.sh ACQ CORESET DATASET ```           where      - ```ACQ```  may be one of ```{BALD  Entropy}``` (determines the acquisition function to be used).     - ```CORESET``` may be one of ```{Argmax  Random  Best}``` (determines the querying strategy to be used)     - ```DATASET``` may be one of ```{cifar10  svhn  fashion_mnist}``` (determines the dataset to be used).         For example  to run greedy BALD on CIFAR10  run the following command:        ```./scripts/run_active_torchvision.sh BALD Argmax cifar10```        This will automaticall generate an experimental directory with an appropriate name  and place results from    5 runs in the directory.  4. Active learning for classification (using projections as in section 5 of the paper): run the following command      ```./scripts/run_active_torchvision_projections.sh CORESET DATASET ```          where      -```CORESET``` may be one of ```{Argmax  Random  Best  FW}``` (determines the querying strategy to be used)     -```DATASET``` may be one of ```{cifar10  svhn  fashion_mnist}``` (determines the dataset to be used).          For example  to run the proposed method on the CIFAR10 dataset  please run:          ```./scripts/run_active_torchvision_projections.sh FW cifar10```          This will automaticall generate an experimental directory with an appropriate name  and place results from     5 runs in the directory.       """;General;https://github.com/rpinsler/active-bayesian-coresets
"""YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled):                               path/  #: directory   * [Train Custom Data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)&nbsp; 🚀 RECOMMENDED * [Tips for Best Training Results](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results)&nbsp; ☘️ RECOMMENDED * [Weights & Biases Logging](https://github.com/ultralytics/yolov5/issues/1289)&nbsp; 🌟 NEW * [Supervisely Ecosystem](https://github.com/ultralytics/yolov5/issues/2518)&nbsp; 🌟 NEW * [Multi-GPU Training](https://github.com/ultralytics/yolov5/issues/475) * [PyTorch Hub](https://github.com/ultralytics/yolov5/issues/36)&nbsp; ⭐ NEW * [ONNX and TorchScript Export](https://github.com/ultralytics/yolov5/issues/251) * [Test-Time Augmentation (TTA)](https://github.com/ultralytics/yolov5/issues/303) * [Model Ensembling](https://github.com/ultralytics/yolov5/issues/318) * [Model Pruning/Sparsity](https://github.com/ultralytics/yolov5/issues/304) * [Hyperparameter Evolution](https://github.com/ultralytics/yolov5/issues/607) * [Transfer Learning with Frozen Layers](https://github.com/ultralytics/yolov5/issues/1314)&nbsp; ⭐ NEW * [TensorRT Deployment](https://github.com/wang-xinyu/tensorrtx)    """;Computer Vision;https://github.com/wslerry/yolov5
"""A general YOLOv4/v3/v2 object detection pipeline inherited from [keras-yolo3-Mobilenet](https://github.com/Adamdad/keras-YOLOv3-mobilenet)/[keras-yolo3](https://github.com/qqwweee/keras-yolo3) and [YAD2K](https://github.com/allanzelener/YAD2K). Implement with tf.keras  including data collection/annotation  model training/tuning  model evaluation and on device deployment. Support different architecture and different technologies:   1. custom dataset 디렉토리 스트럭쳐      ```      .      ├── custom_data      │   └── WASTE2020      │       ├── Annotations      │       │   ├── single_can      │       │   └── single_vinyl      │       └── Images      │           ├── single_can      │           └── single_vinyl      ```      - Annotations의 subdirectories 안에 VOC포맷의 xml 파일들      - Images의 subdirectories 안에 png 파일들  2. configs/custom_classes.txt 추가해주기      ```      configs      ├── VOC2012_person_test.txt      ├── coco_classes.txt      ├── custom_classes.txt      ├── tiny_yolo3_anchors.txt      ├── voc_classes.txt      ├── yolo2-tiny-voc_anchors.txt      ├── yolo2-tiny_anchors.txt      ├── yolo2-voc_anchors.txt      ├── yolo2_anchors.txt      ├── yolo3_anchors.txt      └── yolo4_anchors.txt      ```      #: cd tools/dataset_converter/ && python voc_annotation.py -h                              both      You can merge these train &amp; val annotation file as your need. For example  following cmd will creat 07/12 combined trainval dataset:      #: cd tools/dataset_converter/ && python coco_annotation.py -h   If you want to download PascalVOC or COCO dataset  refer to Dockerfile for cmd     --gpu_num GPU_NUM     Number of GPU to use  default=1   You can also use Tensorboard to monitor the loss trend during train:   If you're evaluating with MSCOCO dataset  you can further use pycoco_eval.py with the generated txt detection result and COCO GT annotation to get official COCO AP with pycocotools:   The test environment is   Python 3.6.8   1.  VOC -> YOLO 포맷      ```      cd tools/dataset_converter && python custom_voc_annotation.py      ``` 2. Train       Example      ```      python train.py --annotation_file tools/dataset_converter/2020_train.txt --transfer_epoch 200 --total_epoch 250 --val_split 0.2 --classes_path configs/custom_classes.txt --eval_online --model_type yolo3_efficientnet      ```   1. Install requirements on Ubuntu 16.04/18.04:  ``` #: apt install python3-opencv #: pip install Cython #: pip install -r requirements.txt ```  2. Download Related Darknet/YOLOv2/v3/v4 weights from [YOLO website](http://pjreddie.com/darknet/yolo/) and [AlexeyAB/darknet](https://github.com/AlexeyAB/darknet). 3. Convert the Darknet YOLO model to a Keras model. 4. Run YOLO detection on your image or video  default using Tiny YOLOv3 model.  ``` #: wget -O weights/darknet53.conv.74.weights https://pjreddie.com/media/files/darknet53.conv.74 #: wget -O weights/darknet19_448.conv.23.weights https://pjreddie.com/media/files/darknet19_448.conv.23 #: wget -O weights/yolov3.weights https://pjreddie.com/media/files/yolov3.weights #: wget -O weights/yolov3-tiny.weights https://pjreddie.com/media/files/yolov3-tiny.weights #: wget -O weights/yolov3-spp.weights https://pjreddie.com/media/files/yolov3-spp.weights #: wget -O weights/yolov2.weights http://pjreddie.com/media/files/yolo.weights #: wget -O weights/yolov2-voc.weights http://pjreddie.com/media/files/yolo-voc.weights #: wget -O weights/yolov2-tiny.weights https://pjreddie.com/media/files/yolov2-tiny.weights #: wget -O weights/yolov2-tiny-voc.weights https://pjreddie.com/media/files/yolov2-tiny-voc.weights  #:#:#: manually download csdarknet53-omega_final.weights from https://drive.google.com/open?id=18jCwaL4SJ-jOvXrZNGHJ5yz44g9zi8Hm #: wget -O weights/yolov4.weights https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights  #: python tools/model_converter/convert.py cfg/yolov3.cfg weights/yolov3.weights weights/yolov3.h5 #: python tools/model_converter/convert.py cfg/yolov3-tiny.cfg weights/yolov3-tiny.weights weights/yolov3-tiny.h5 #: python tools/model_converter/convert.py cfg/yolov3-spp.cfg weights/yolov3-spp.weights weights/yolov3-spp.h5 #: python tools/model_converter/convert.py cfg/yolov2.cfg weights/yolov2.weights weights/yolov2.h5 #: python tools/model_converter/convert.py cfg/yolov2-voc.cfg weights/yolov2-voc.weights weights/yolov2-voc.h5 #: python tools/model_converter/convert.py cfg/yolov2-tiny.cfg weights/yolov2-tiny.weights weights/yolov2-tiny.h5 #: python tools/model_converter/convert.py cfg/yolov2-tiny-voc.cfg weights/yolov2-tiny-voc.weights weights/yolov2-tiny-voc.h5 #: python tools/model_converter/convert.py cfg/darknet53.cfg weights/darknet53.conv.74.weights weights/darknet53.h5 #: python tools/model_converter/convert.py cfg/darknet19_448_body.cfg weights/darknet19_448.conv.23.weights weights/darknet19.h5  #: python tools/model_converter/convert.py cfg/csdarknet53-omega.cfg weights/csdarknet53-omega_final.weights weights/cspdarknet53.h5  #:#:#: make sure to reorder output tensors for YOLOv4 cfg and weights file #: python tools/model_converter/convert.py --yolo4_reorder cfg/yolov4.cfg weights/yolov4.weights weights/yolov4.h5  #: python yolo.py --image #: python yolo.py --input=<your video file> ``` For other model  just do in a similar way  but specify different model type  weights path and anchor path with `--model_type`  `--weights_path` and `--anchors_path`.  Image detection sample:  <p align=""center"">   <img src=""assets/dog_inference.jpg"">   <img src=""assets/kite_inference.jpg""> </p>   1. [yolo.py](https://github.com/david8862/keras-YOLOv3-model-set/blob/master/yolo.py) > * Demo script for trained model  image detection mode ``` #: python yolo.py --model_type=yolo3_mobilenet_lite --weights_path=model.h5 --anchors_path=configs/yolo3_anchors.txt --classes_path=configs/voc_classes.txt --model_image_size=416x416 --image ``` video detection mode ``` #: python yolo.py --model_type=yolo3_mobilenet_lite --weights_path=model.h5 --anchors_path=configs/yolo3_anchors.txt --classes_path=configs/voc_classes.txt --model_image_size=416x416 --input=test.mp4 ``` For video detection mode  you can use ""input=0"" to capture live video from web camera and ""output=<video name>"" to dump out detection result to another video   """;Computer Vision;https://github.com/symoon94/YOLO-keras
"""In this Repo I implemented the paper ""Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization"" (https://arxiv.org/abs/1703.06868)      The colab Notebook can be found here: https://colab.research.google.com/drive/1-7E5haL8VhaajmUXBlpR7ynH8Ge7ywjR#scrollTo=YLa2rTxLUxAF     """;General;https://github.com/Jwrede/neural_style_transfer
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  BERT is based on the Transformer architecture introduced in [Attention is all you need](https://arxiv.org/abs/1706.03762).  Googles academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  Googles Github repository where the original english models can be found here: [https://github.com/google-research/bert](https://github.com/google-research/bert).  Included in the downloads below are PyTorch versions of the models based on the work of  NLP researchers from HuggingFace. [PyTorch version of BERT available](https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/)   The links to the models are here (right-click  'Save link as...' on the name):   """;Natural Language Processing;https://github.com/af-ai-center/bert
"""Build with NDK-17b   ./gradlew build  adb install app/build/outputs/apk/release/app-release.apk           runtime: cpu+gpu       bash      git clone https://github.com/XiaoMi/mace      cd mace      git checkout v0.11.0-rc1       sh   """;Computer Vision;https://github.com/nolanliou/PeopleSegmentationDemo
"""git clone https://github.com/NVIDIA/trt-samples-for-hackathon-cn.git  cd build   """;Natural Language Processing;https://github.com/trthackthonFighters/ConvBert
"""train.sh : modify the hyper-parameters for training. run bash train.sh   """;Reinforcement Learning;https://github.com/anhtu293/NeurIPS-2019-Challenge
"""<a name=""problems_and_applications""/>   <a name=""ideas_for_papers""/>   <a name=""theoretic_questions""/>   +II. Верно ли  что для любой матрицы близостей найдется такая константа  которая при добавлении делает матрицу метрической — да  верно  это доказывается в статье [1  Agarwal]  после такого как матрица стала метрической  из неё автоматически можно получить представления объектов(https://kaggle.com/kolosov/isotonic-grader) - но здесь есть важная оговорка  что исходная матрица близостей должна быть симметричной  <a name=""papers_plan""/>   <a name=""literature""/>   Как с этим работать — https://github.com/akutuzov/webvectors/blob/master/preprocessing/rusvectores_tutorial.ipynb   <a name=""links""/>   """;Natural Language Processing;https://github.com/obj2vec/obj2vec
"""Python 3.6   """;General;https://github.com/jxz542189/dmn_plus
"""sh download_data.sh   """;Computer Vision;https://github.com/LebronGG/PointNet
"""The *Show and Tell* model is a deep neural network that learns how to describe the content of images. For example:  ![Example captions](g3doc/example_captions.jpg)   To train the model you will need to provide training data in native TFRecord format. The TFRecord format consists of a set of sharded files containing serialized `tf.SequenceExample` protocol buffers. Each `tf.SequenceExample` proto contains an image (JPEG format)  a caption and metadata such as the image id.  Each caption is a list of words. During preprocessing  a dictionary is created that assigns each word in the vocabulary to an integer-valued id. Each caption is encoded as a list of integer word ids in the `tf.SequenceExample` protos.  We have provided a script to download and preprocess the [MSCOCO](http://mscoco.org/) image captioning data set into this format. Downloading and preprocessing the data may take several hours depending on your network and computer speed. Please be patient.  Before running the script  ensure that your hard disk has at least 150GB of available space for storing the downloaded and processed data.  ```shell #: Location to save the MSCOCO data. MSCOCO_DIR=""${HOME}/im2txt/data/mscoco""  #: Build the preprocessing script. cd research/im2txt bazel build //im2txt:download_and_preprocess_mscoco  #: Run the preprocessing script. bazel-bin/im2txt/download_and_preprocess_mscoco ""${MSCOCO_DIR}"" ```  The final line of the output should read:  ``` 2016-09-01 16:47:47.296630: Finished processing all 20267 image-caption pairs in data set 'test'. ```  When the script finishes you will find 256 training  4 validation and 8 testing files in `DATA_DIR`. The files will match the patterns `train-?????-of-00256`  `val-?????-of-00004` and `test-?????-of-00008`  respectively.   First ensure that you have installed the following required packages:  * **Bazel** ([instructions](http://bazel.io/docs/install.html)) * **TensorFlow** 1.0 or greater ([instructions](https://www.tensorflow.org/install/)) * **NumPy** ([instructions](http://www.scipy.org/install.html)) * **Natural Language Toolkit (NLTK)**:     * First install NLTK ([instructions](http://www.nltk.org/install.html))     * Then install the NLTK data package ""punkt"" ([instructions](http://www.nltk.org/data.html)) * **Unzip**  Install Required Packages   Download the Inception v3 Checkpoint   cd research/im2txt   Note that you may run out of memory if you run the evaluation script on the same  GPU as the training script. You can run the command   : Ignore GPU devices (only necessary if your GPU is currently memory   : you will need to pass the checkpoint path explicitly.   cd research/im2txt   : Ignore GPU devices (only necessary if your GPU is currently memory   A TensorFlow implementation of the image-to-text model described in the paper:  ""Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge.""  Oriol Vinyals  Alexander Toshev  Samy Bengio  Dumitru Erhan.  *IEEE transactions on pattern analysis and machine intelligence (2016).*  Full text available at: http://arxiv.org/abs/1609.06647   """;Computer Vision;https://github.com/brandontrabucco/im2txt_match
"""``` git clone https://github.com/yoshitomo-matsubara/torchdistill.git cd torchdistill/ pip3 install -e . #: or use pipenv pipenv install ""-e ."" ```   ``` pip3 install torchdistill #: or use pipenv pipenv install torchdistill ```   - Python 3.6 >= - pipenv (optional)   If you find models on PyTorch Hub or GitHub repositories supporting PyTorch Hub        name: 'resnest50d'   Executable code can be found in [examples/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/) such as - [Image classification](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/image_classification.py): ImageNet (ILSVRC 2012)  CIFAR-10  CIFAR-100  etc - [Object detection](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/object_detection.py): COCO 2017  etc - [Semantic segmentation](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/semantic_segmentation.py): COCO 2017  PASCAL VOC  etc - [Text classification](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/hf_transformers/text_classification.py): GLUE  etc  For CIFAR-10 and CIFAR-100  some models are reimplemented and available as pretrained models in ***torchdistill***.  More details can be found [here](https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.1.1).    Some Transformer models fine-tuned by ***torchdistill*** for GLUE tasks are available at [Hugging Face Model Hub](https://huggingface.co/yoshitomo-matsubara).  Sample GLUE benchmark results and details can be found [here](https://github.com/yoshitomo-matsubara/torchdistill/tree/master/examples/hf_transformers#sample-benchmark-results-and-fine-tuned-models).   The following examples are available in [demo/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/demo/).  Note that the examples are for Google Colab users. Usually  [examples/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/) would be a better reference  if you have your own GPU(s).   """;General;https://github.com/yoshitomo-matsubara/torchdistill
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/malin9402/retiface
"""- To install  `cd` into the root directory and type `pip install -e .`  - Known dependencies: Python (3.5.4)  OpenAI gym (0.10.5)  tensorflow (1.8.0)  numpy (1.14.5)   """;Reinforcement Learning;https://github.com/openai/maddpg
"""Labeling Tool : https://github.com/AlexeyAB/Yolo_mark  Darknet (Yolov4) : https://github.com/AlexeyAB/darknet   발표영상 : https://www.youtube.com/watch?v=H3-SVf0Ps4c   1. `git clone https://github.com/AlexeyAB/darknet` 2. `cd darknet` 3. 사용하는 환경에 맞게 Makefile 설정  `vi Makefile` ``` GPU=0		#: GPU 사용 시 1로 변경 CUDNN=0		#: cuDNN 사용 시 1로 변경 (NVIDIA) CUDNN_HALF=0 OPENCV=0	#: OpenCV 사용 시 1로 변경 AVX=0 OPENMP=0 LIBSO=1         #: libdarknet.so 생성  ... ... ``` 4. `make` - 기본 패키지 : make  gcc  pkg-config (없다면 `sudo apt-get install …`로 설치)    5. `data/*`  `cfg/yolov4-ANPR.cfg`  ` backup/yolov4-ANPR.weights` 다운로드    """;Computer Vision;https://github.com/Dodant/ANPR-with-Yolov4
"""<p> Make sure you have the following dependencies before running the web app</p>   <p>To run the web app use on port 5000 </p>   """;Computer Vision;https://github.com/isheunesutembo/TB-Computer-Aided-Diagnosis-Using-Deep-Learning
"""Benchmarking GNN: https://github.com/graphdeeplearning/benchmarking-gnns   GNN-RecSys: https://github.com/je-dbl/GNN-RecSys   1. [**Learning from Non-Binary Constituency Trees via Tensor Decomposition**](https://github.com/danielecastellana22/tensor-tree-nn)  COLING'20  *Daniele Castellana  Davide Bacciu*   1. [**Generalising Recursive Neural Models by Tensor Decomposition**](https://github.com/danielecastellana22/tensor-tree-nn)  IJCNN'20  *Daniele Castellana  Davide Bacciu*   DGL provides a plenty of learning materials for all kinds of users from ML researcher to domain experts. The [Blitz Introduction to DGL](https://docs.dgl.ai/tutorials/blitz/index.html) is a 120-minute tour of the basics of graph machine learning. The [User Guide](https://docs.dgl.ai/guide/index.html) explains in more details the concepts of graphs as well as the training methodology. All of them include code snippets in DGL that are runnable and ready to be plugged into one’s own pipeline.   Users can install DGL from [pip and conda](https://www.dgl.ai/pages/start.html). Advanced users can follow the [instructions](https://docs.dgl.ai/install/index.html#install-from-source) to install from source.  For absolute beginners  start with [the Blitz Introduction to DGL](https://docs.dgl.ai/tutorials/blitz/index.html). It covers the basic concepts of common graph machine learning tasks and a step-by-step on building Graph Neural Networks (GNNs) to solve them.  For acquainted users who wish to learn more   * Learn DGL by [example implementations](https://www.dgl.ai/) of popular GNN models. * Read the [User Guide](https://docs.dgl.ai/guide/index.html) ([中文版链接](https://docs.dgl.ai/guide_cn/index.html))  which explains the concepts and usage of DGL in much more details. * Go through the tutorials for advanced features like [stochastic training of GNNs](https://docs.dgl.ai/tutorials/large/index.html)  training on [multi-GPU](https://docs.dgl.ai/tutorials/multi/index.html) or [multi-machine](https://docs.dgl.ai/tutorials/dist/index.html). * [Study classical papers](https://docs.dgl.ai/tutorials/models/index.html) on graph machine learning alongside DGL. * Search for the usage of a specific API in the [API reference manual](https://docs.dgl.ai/api/python/index.html)  which organizes all DGL APIs by their namespace.  All the learning materials are available at our [documentation site](https://docs.dgl.ai/). If you are new to deep learning in general  check out the open source book [Dive into Deep Learning](https://d2l.ai/).    We provide multiple channels to connect you to the community of the DGL developers  users  and the general GNN academic researchers:  * Our Slack channel  [click to join](https://join.slack.com/t/deep-graph-library/shared_invite/zt-eb4ict1g-xcg3PhZAFAB8p6dtKuP6xQ) * Our discussion forum: https://discuss.dgl.ai/ * Our [Zhihu blog (in Chinese)](https://www.zhihu.com/column/c_1070749881013936128) * Monthly GNN User Group online seminar ([event link](https://www.eventbrite.com/e/graph-neural-networks-user-group-tickets-137512275919?utm-medium=discovery&utm-campaign=social&utm-content=attendeeshare&aff=escb&utm-source=cp&utm-term=listing) | [past videos](https://www.youtube.com/channel/UCnmuSDY1pTlaFH1WRQElfTg))  Take the survey [here](https://forms.gle/Ej3jHCocACmb49Gp8) and leave any feedback to make DGL better fit for your needs. Thanks!   """;General;https://github.com/dmlc/dgl
"""Follow the steps in the installation documentation in [doc/installation.md](doc/installation.md).     """;General;https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin
"""[![Run on Ainize](https://ainize.ai/static/images/run_on_ainize_button.svg)](https://ainize.web.app/redirect?git_repo=github.com/kmswlee/ainized-PointRend)  PointRend is image segmentation as rendering  The PointRend neural network module performs point-based segementation predictions at adaptively selected locations based on  an iterative subvision algorithm.  PointRend achieves higher sharpness on tricky object boundaries such as fingers than Mask R-CNN  and can be added on both semantic and instance segmentation.   This module show intermediate results. So if you want to use Point Rend  apply Point Rend to your instance segmentation or semantic segmentation project.   Ainize is done in the following steps: 1. click 'default'. 2. click 'try it out' and first  input mask-image file and second  input original-image file. 3. click 'submit' button.    this is dockerized  it can be run using docker commands.   """;Computer Vision;https://github.com/kmswlee/ainized-PointRend
"""we will use the Celeb-A Faces dataset which can be downloaded at the linked site     Download link:   Baidu Drive  https://pan.baidu.com/s/1eSNpdRG#list/path=%2F   The resulting directory structure should be:     bash run.sh   """;Computer Vision;https://github.com/darr/DCGAN
"""bert and multi_cased_L-12_H-768_A-12 should be extracted this directory.   ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ multi_cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip) |____ data		            #: train data (EnglishBERTdata3Labels  TurkishNERdata3Labels  TurkishNERdata7Labels) |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ run_ner.sh    		    #: run model and eval result  ```    There are 3 different data sets available.  ``` EnglishBERTdata3Labels      #: Labels : I-PER B-PER I-ORG B-ORG I-LOC B-LOC TurkishNERdata3Labels       #: Labels : PERSON LOCATION ORGANIZATION TurkishNERdata7Labels       #: Labels : PERSON LOCATION ORGANIZATION DATE TIME PERCENT MONEY ```   ``` bash run_ner.sh ```    """;Natural Language Processing;https://github.com/teghub/TurkishNER-BERT
"""  Recorded Video(Output)   |      SMS Received :-----------------:|:-----------------------: ![recorded video](/img/park.gif) | ![received SMS](/img/Screenshot%20from%202019-08-07%2002-07-53.png)     """;Computer Vision;https://github.com/Mrnoorsingh/car-parking
"""![alt text](https://github.com/saeedabi1/deep_learning_project/blob/master/pictures/Screen%20Shot%202020-05-23%20at%205.08.25%20PM.png?raw=true)    ![alt text](https://github.com/saeedabi1/deep_learning_project/blob/master/pictures/Screen%20Shot%202020-05-23%20at%205.08.39%20PM.png?raw=true)    Attention mechanisms are broadly used in present image captioning encoder / decoder frameworks  where at each step a weighted average is generated on encoded vectors to direct the process of caption decoding.  However  the decoder has no knowledge of whether or how well the vector being attended and the attention question being given are related  which may result in the decoder providing erroneous results.  Image captioning  that is to say generating natural automatic descriptions of language images are useful for visually impaired images and for the quest of natural language related pictures.  It is significantly more demanding than traditional vision tasks recognition of objects and classification of images for two guidelines.  First  well formed structured output space natural language sentences are considerably more challenging than just a set of class labels to predict.  Secondly  this dynamic output space enables a more thin understanding of the visual scenario  and therefore also a more informative one visual scene analysis to do well on this task.   Taking the one with the highest probability   """;General;https://github.com/varun-bhaseen/Image-caption-generation-using-attention-model
"""python ideepe.py [-h] [--posi <postive_sequecne_file>] <br>                  [--nega <negative_sequecne_file>] [--model_type MODEL_TYPE] <br>                  [--out_file OUT_FILE] [--motif MOTIF] [--train TRAIN] <br>                  [--model_file MODEL_FILE] [--predict PREDICT] [--motif_dir MOTIF_DIR]<br>                  [--testfile TESTFILE] [--maxsize MAXSIZE] [--channel CHANNEL] <br>                  [--window_size WINDOW_SIZE] [--local LOCAL] [--glob GLOB] <br>                  [--ensemble ENSEMBLE] [--batch_size BATCH_SIZE] <br>                  [--num_filters NUM_FILTERS] [--n_epochs N_EPOCHS] <br> It supports model training  testing and different model structure  MODEL_TYPE can be CNN  CNN-LSTM and ResNet  DenseNet.   Take ALKBH5 as an example  if you want to predict the binding sites for RBP ALKBH5 using ensembling local and global CNNs  and the default model is ensembling model. <br> You first need train the model for RBP ALKBH5  then the trained model is used to predict binding probability of this RBP for your sequences. The follwoing CLI will train a ensembling model using local and global CNNs  which are trained using positves and negatives derived from CLIP-seq. <br>  """;General;https://github.com/xypan1232/iDeepE
"""This model is a semantic image segmentation model  which assigns label to each pixel of an image to partition different objects into segments. The whole model is composed of two parts  namely backbone part and classifier part. The backbone part is resnet101 which has been pre-trained  and the classifier part (DeepLabV3+ head  implemented by https://github.com/jfzhang95/pytorch-deeplab-xception using PyTorch) is fine-tuned based on this specific task.    ![](https://github.com/sdyy6211/Dissertation_Image_segmentation/blob/20210302/gitpic/original_image.jpg) |:--:|  | *An example of training image* |    ![](https://github.com/sdyy6211/Dissertation_Image_segmentation/blob/20210302/gitpic/original_image.jpg) |:--:|  | *An example of training image* |    ![](https://github.com/sdyy6211/Dissertation_Image_segmentation/blob/20210302/gitpic/label.png) |:--:|  | *The label of overall classes for the first model* |  ![](https://github.com/sdyy6211/Dissertation_Image_segmentation/blob/20210302/gitpic/segmentated_label.png) |:--:|  | *The label of binary classes for the second model to refine prediction* |  This process involves downloading masks from the Labelbox. These codes correspond to the file of *segmentation_data_processing.ipynb*   """;General;https://github.com/sdyy6211/plant-segmentation
"""Getting the source code   $ wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip   $ cd fastText-0.9.2  $ make   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ mkdir build &amp;&amp; cd build &amp;&amp; cmake ..  $ make &amp;&amp; make install   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ pip install .   You can also quantize a supervised model to reduce its memory usage with the following command:   You can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.  There is also the master branch that contains all of our most recent work  but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.   This library has two main use cases: word representation learning and text classification. These were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).   """;Natural Language Processing;https://github.com/facebookresearch/fastText
"""For DS-TD3: Dynamic Sparse training of TD3 algorithm  ``` python main.py --env HalfCheetah-v3 --policy DS-TD3 ```  For Static-TD3 ``` python main.py --env HalfCheetah-v3 --policy StaticSparseTD3 ```  For TD3 ``` python main.py --env HalfCheetah-v3 --policy TD3 ```   """;General;https://github.com/GhadaSokar/Dynamic-Sparse-Training-for-Deep-Reinforcement-Learning
"""Instructions for acquiring PTB and WT2 can be found here. While CIFAR-10 can be automatically downloaded by torchvision  ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions here.   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/jiangyingjunn/i-darts
"""- better heuristics for testing the networks  - the addition of Alpha-Beta pruning algorithms which for some of the games also takes into account the depth in the search tree to provide a stronger heuristics and to test the network capacity of generalization.  - the games start from random position when pitting the network  in this case the network is evaluated better  as games tend to be different. Also  in this way we can see how well the network generalize.  - using Dirichlet noise  this repo manages to randomize(to a certain degree) even the games that are generated in self-play  so the games are more unique  and the network tends to learn better.  - the networks used are almost like those in AlphaZero and AlphaGo Zero  with minor tweaks in order to be able to run them on resource-constrained system(systems with limited RAM  GPU memory  computation power  etc.)  - the MCTS is reset after each game  so neither player has any advantage of having a developed tree and moving first. In certain games the second player has an advantage. This advantage combined with a developed tree makes the game easy for the second player. - Othello game is updated in order to take a draw into account.  - this implementation provides means of tracking the progress of the network through the training. This info is provided as the number of games won lost or which resulted in a draw in each epoch against Greedy  Random and Alpha-Beta pruning. However  you can turn this feature off.  - this implementation provide complex neural networks that can be used for every game and are capable of learning any game. The project mentioned above uses very small networks which are unsuitable to learn more complex games  thus not being general enough to be used for all games.   """;Reinforcement Learning;https://github.com/MerceaOtniel/HybridAlpha
"""In this implementation example  the original hyper-parameters specified by the original paper are set. Feel free to play with other hyper-parameters:  ```python from torchvision.models import resnet18  model = resnet18()  learner = SimSiam(model)  opt = torch.optim.Adam(learner.parameters()  lr=0.001)  criterion = NegativeCosineSimilarity()  def sample_unlabelled_images():     return torch.randn(20  3  256  256)  for _ in range(100):     images1 = sample_unlabelled_images()     images2 = images1*0.9     p1  p2  z1  z2 = learner(images1  images2).values()     loss = criterion(p1  p2  z1  z2)     opt.zero_grad()     loss.backward()     opt.step()     print(_+1 loss) ```     """;General;https://github.com/SaeedShurrab/SimSiam-pytorch
"""Using your favorite virtual environment manage  set a new environment and start by installing the requirement by running the following command  ``` pip install -r requirement.txt ``` This command will automatically install the library `ssd_keras` You can verify that the library is installed by using the command ``` $ pip list | grep ssd_keras ``` If the library is not installed  you can installed manually launching the following command being inside the repo  ``` pip install -e ./ssd_keras/. ```  This will install the library locally without dublicating the files of the library  once the requirement are installed you can run the following command to make sure that all needed libraries are installed  ├── requirements.txt ==> Requirement to be install   """;Computer Vision;https://github.com/YELKHATTABI/MAS_Clean
"""To run the code  adjust the hyperparameters in main.py and run  ``` python3 main.py ```  This will train the GAN on the MNIST dataset. The necessary dependencies are contained in requirements.txt.   """;General;https://github.com/mcclow12/wgan-gp-pytorch
"""Training can be observed via tensorboard (run tensorboard --logdir=./Graph from command line)   As you can see  as a first step  the environment needs to be created. As a second step  different agents need to be   To contribute do the following:  Get Pycharm and build the virtual python environment. Use can do: pip install -r requirements.txt   Clone your fork to your local machine. You can do this directly from pycharm: VCS --> check out from version control --> git   Make your edits.   Push your changes to your origin (your fork) (CTRL+SHIFT+K)   - nb_steps_warmup = 75   - nb_steps = 10000   """;Reinforcement Learning;https://github.com/dickreuter/neuron_poker
"""- Install [PyTorch](http://pytorch.org/)(version v0.3) by selecting your environment on the website and running the appropriate command. - Please install cv2 as well. I recommend using anaconda 3.6 and it's opnecv package. - You will also need Matlab. If you have distributed computing license then it would be faster otherwise it should also be fine.  Just replace `parfor` with simple `for` in Matlab scripts. I would be happy to accept a PR for python version of this part. - Clone this repository.    * Note: We currently only support Python 3+ with Pytorch version v0.2 on Linux system. - We currently only support [UCF24](http://www.thumos.info/download.html) with [revised annotaions](https://github.com/gurkirt/corrected-UCF101-Annots) released with our paper  we will try to add [JHMDB21](http://jhmdb.is.tue.mpg.de/) as soon as possible  but can't promise  you can check out our [BMVC2016 code](https://bitbucket.org/sahasuman/bmvc2016_code) to get started your experiments on JHMDB21. - To simulate the same training and evaluation setup we provide extracted `rgb` images from videos along with optical flow images (both `brox flow` and `real-time flow`) computed for the UCF24 dataset. You can download it from my [google drive link](https://drive.google.com/file/d/1o2l6nYhd-0DDXGP-IPReBP4y1ffVmGSE/view?usp=sharing) - We also support [Visdom](https://github.com/facebookresearch/visdom) for visualization of loss and frame-meanAP on subset during training.   * To use Visdom in the browser:    ```Shell   #: First install Python server and client    pip install visdom   #: Start the server (probably in a screen or tmux)   python -m visdom.server --port=8097   ```   * Then (during training) navigate to http://localhost:8097/ (see the Training section below for more details).   for publication. I have forked the version of SSD-CAFFE which I used to generate results for paper  you try that if you want to use caffe. You can use that repo if like caffe other I would recommend using this version.   We build on Pytorch implementation of SSD by Max deGroot  Ellis Brown.   The previous version was in pytorch 0.2. The current one works on pytorch 1.2.    <a href='#installation'>Installation</a>   By default  we assume that you have downloaded that dataset.       Let's assume that you extracted dataset in /home/user/ucf24/ directory then your train command from the root directory of this repo is going to be:    For instructions on Visdom usage/installation  see the <a href='#installation'>Installation</a> section. By default  it is off.  If you don't like to use visdom then you always keep track of train using logfile which is saved under save_root directory   To compute frame-mAP you can use frameAP.m script. You will need to specify data_root  data_root.    - NMS is performed once in python then again in Matlab; one has to do that on GPU in python   Also  Feynman27 pushed a python version of the incremental_linking   """;Computer Vision;https://github.com/gurkirt/realtime-action-detection
"""Note that this could be relying on a different ""take"" than you would expect. For example  you could have answered ลูกเขย in the second example because it  is the one associated with male gender.   """;Sequential;https://github.com/cstorm125/thai2fit
"""``` pip install torch_cka ```  ```python from torch_cka import CKA model1 = resnet18(pretrained=True)  #: Or any neural network of your choice model2 = resnet34(pretrained=True)  dataloader = DataLoader(your_dataset                           batch_size=batch_size  #: according to your device memory                         shuffle=False)  #: Don't forget to seed your dataloader  cka = CKA(model1  model2            model1_name=""ResNet18""    #: good idea to provide names to avoid confusion           model2_name=""ResNet34""               model1_layers=layer_names_resnet18  #: List of layers to extract features from           model2_layers=layer_names_resnet34  #: extracts all layer features by default           device='cuda')  cka.compare(dataloader) #: secondary dataloader is optional  results = cka.export()  #: returns a dict that contains model names  layer names                         #: and the CKA matrix ```   `torch_cka` can be used with any pytorch model (subclass of `nn.Module`) and can be used with pretrained models available from popular sources like torchHub  timm  huggingface etc. Some examples of where this package can come in handy are illustrated below.   """;Computer Vision;https://github.com/AntixK/PyTorch-Model-Compare
"""|PyTorch | 0.56 | 20.6|   Train and evaluate our models through `main.py`.  ```sh main.py:   --config: Training configuration.     (default: 'None')   --eval_folder: The folder name for storing evaluation results     (default: 'eval')   --mode: <train|eval>: Running mode: train or eval   --workdir: Working directory ```  * `config` is the path to the config file. Our prescribed config files are provided in `configs/`. They are formatted according to [`ml_collections`](https://github.com/google/ml_collections) and should be quite self-explanatory.    **Naming conventions of config files**: the path of a config file is a combination of the following dimensions:   *  dataset: One of `cifar10`  `celeba`  `celebahq`  `celebahq_256`  `ffhq_256`  `celebahq`  `ffhq`.   * model: One of `ncsn`  `ncsnv2`  `ncsnpp`  `ddpm`  `ddpmpp`.   * continuous: train the model with continuously sampled time steps.   *  `workdir` is the path that stores all artifacts of one experiment  like checkpoints  samples  and evaluation results.  * `eval_folder` is the name of a subfolder in `workdir` that stores all artifacts of the evaluation process  like meta checkpoints for pre-emption prevention  image samples  and numpy dumps of quantitative results.  * `mode` is either ""train"" or ""eval"". When set to ""train""  it starts the training of a new model  or resumes the training of an old model if its meta-checkpoints (for resuming running after pre-emption in a cloud environment) exist in `workdir/checkpoints-meta` . When set to ""eval""  it can do an arbitrary combination of the following    * Evaluate the loss function on the test / validation dataset.    * Generate a fixed number of samples and compute its Inception score  FID  or KID. Prior to evaluation  stats files must have already been downloaded/computed and stored in `assets/stats`.    * Compute the log-likelihood on the training or test dataset.    These functionalities can be configured through config files  or more conveniently  through the command-line support of the `ml_collections` package. For example  to generate samples and evaluate sample quality  supply the  `--config.eval.enable_sampling` flag; to compute log-likelihoods  supply the `--config.eval.enable_bpd` flag  and specify `--config.eval.dataset=train/test` to indicate whether to compute the likelihoods on the training or test dataset.   | Link | Description| |:----:|:-----| |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1dRR_0gNRmfLtPavX2APzUggBuXyjWW55?usp=sharing)  | Load our pretrained checkpoints and play with sampling  likelihood computation  and controllable synthesis (JAX + FLAX)| |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17lTrPLTt_0EDXa4hkbHmbAFQEkpRDZnh?usp=sharing) | Load our pretrained checkpoints and play with sampling  likelihood computation  and controllable synthesis (PyTorch) | |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1SeXMpILhkJPjXUaesvzEhc3Ke6Zl_zxJ?usp=sharing)  | Tutorial of score-based generative models in JAX + FLAX | |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing)| Tutorial of score-based generative models in PyTorch |    """;Computer Vision;https://github.com/yang-song/score_sde
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/chandu7077/mybert
"""Python 3.5+ is recommended. Install the dependencies with the following command: ``` pip install -r requirements.txt ```  Then  I recommend training a model in a simple environment  like Cartpole. Use this command to do that. This will also log the results to tensorboard and save the model (Training takes about 5 min. on my 2018 Macbook Pro): ``` py runner.py --env-id='CartPole-v1' --learning-rate='lambda x: x * 1e-4' --shared-network='fc3'  --num-batches=500 --tb-path='./tbs/CartPole-v1/' --log-every=1 --save-path='./models/CartPole-v1/' --save-every=50 ```  Use this command to watch your trained model play: ``` py runner.py --env-id='CartPole-v1' --mode='test' --restore-path='./models/CartPole-v1/-500' --shared-network='fc3' ```  Launch your Tensorboard: ``` tensorboard --logdir='./tbs/CartPole-v1/' ```   """;Reinforcement Learning;https://github.com/michael-snower/ppo
"""python 3.6+      pip install torch==1.5.1  torchvision==0.6.1   #Higher versions of PyTorch also tested OK          pip install pycocotools       pip install numpy    pip install scipy     cd pytorch_solov2/       python setup.py develop      #*Install the original FocalLoss of SoloV2*  **2021-05-17 update**     Completely remove the dependency on MMCV  **2020-10-13 update**           Improve the evaluation code  save it as the picture after instance segmentation  and add video test code.      **2020-07-23update**     The implementation of FocalLoss in the latest version of MMCV-Full is different from that in the original SOLO version (the processing label of the background class is different). If the MMCV-Full FocalLoss is used for training  although the loss is reduced  the actual prediction is not accurate.  So replace it with the original FocalLoss implementation.  ``` python setup.py develop ``` After the replacement  retraining  loss and prediction are normal.     if use COCO dataset     ln -s /path/coco2017 coco      'name': 'COCO 2017'    ```python        'name': 'casia-SPT 2020'    #:dataset's name        'name': 'solov2_base'         'num_gpus': 1                        #:only support single GPU for now       'name': 'solov2_base'     ```Python    链接：https://pan.baidu.com/s/1MCVkAeKwTua-m9g1NLyRpw    """;Computer Vision;https://github.com/OpenFirework/pytorch_solov2
"""The goal of this tasks consists of generating a text about a specific topic or situation. We used two models in this experiment: GPT-2 and a Transformer model.  On the one hand  despite GPT-2 was only trained to predict the next token in a sentence  it surprisingly learned basic competence in some tasks like translating between languages and answering questions. On the other hand  we implemented a basic Transformer encoder-decoder architecture to analyse whether it is able to generate a similar text to the training data.   In Question-Answering tasks  the model receives a text and a question regarding to the text  and it should mark the beginning and end of the answer in the text.  In this case  it is necessary to fine-tune BERT. We will use XQuAD (Cross-lingual Question Answering Dataset) to fine-tune it  which consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 together with their professional translations into 10 languages. Source: https://github.com/deepmind/xquad   BERT's authors trained it in two tasks and one of them is **Masked Language Model** (MLM). The task consists of mask some input tokens randomly  and then try to predict those random tokens. Hence  we do not need to pretrain BERT and we can directly test it.  In this repository  I have implemented **a basic script to infer the masked token** in a sentence.  Read the BERT paper: https://arxiv.org/pdf/1810.04805.pdf   - https://www.kaggle.com/dimasmunoz/infer-masked-token-with-bert  Note: These commands have been tested in MacOS and Git Bash (Windows).   sh manager.sh bert   $ sh manager.sh bert   """;Natural Language Processing;https://github.com/DimasDMM/transformers
"""Thanks to Kaggle and a lot of amazing data enthusiasm people sharing their notebooks so I had a chance to learn Transformer and really use it to a real-world task!         Saint+ is a **Transformer** based knowledge-tracing model which takes students' exercise history information to predict future performance. As classical Transformer  it has an Encoder-Decoder structure that Encoder applied self-attention to a stream of exercise embeddings; Decoder applied self-attention to responses embeddings and encoder-decoder attention to encoder output.  git clone https://github.com/Chang-Chia-Chi/SaintPlus-Knowledge-Tracing-Pytorch.git      cd SaintPlus-Knowledge-Tracing-Pytorch      """;Natural Language Processing;https://github.com/Chang-Chia-Chi/SaintPlus-Knowledge-Tracing-Pytorch
"""|ResNet50v1+BN | 76.00% | 92.98% | --  | stepwise decay | -- |[PyTorch Vision]|   """;General;https://github.com/switchablenorms/Switchable-Normalization
"""1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   Requirements: hardware  Basic installation   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   To train and test a Faster R-CNN detector using the **alternating optimization** algorithm from our NIPS 2015 paper  use `experiments/scripts/faster_rcnn_alt_opt.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  (""alt opt"" refers to the alternating optimization training algorithm described in the NIPS paper.)  To train and test a Faster R-CNN detector using the **approximate joint training** method  use `experiments/scripts/faster_rcnn_end2end.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  This method trains the RPN module jointly with the Fast R-CNN network  rather than alternating between training the two. It results in faster (~ 1.5x speedup) training times and similar detection accuracy. See these [slides](https://www.dropbox.com/s/xtr4yd4i5e0vw8g/iccv15_tutorial_training_rbg.pdf?dl=0) for more details.  Artifacts generated by the scripts in `tools` are written in this directory.  Trained Fast R-CNN networks are saved under:  ``` output/<experiment directory>/<dataset name>/ ```  Test outputs are saved under:  ``` output/<experiment directory>/<dataset name>/<network snapshot name>/ ```  """;Computer Vision;https://github.com/phtruongan/py-faster-rcnn-docker
"""Once downloaded just place it into the root directory of the repo and you're good to go.   Add automatic gpu/cpu selection   Make requirements.txt file   To use the model for style transfer use the command `python style.pt <path to content image> <path to style image>`.  The styled image will be saves as `output.jpg` in the currect directory.   """;Computer Vision;https://github.com/CellEight/Pytorch-Adaptive-Instance-Normalization
"""First  use your favorite models to get predictions for the test.txt data. Each prediction file should contain an integer prediction (0-5) for each sentence in test.txt (one per line). See example_pred_file.txt to ensure your file is similar.    ``` python3 analyze_predictions.py [prediction_files] ```  The script will print out the accuracy for each of the categories.   ``` python3 analyze_predictions.py example_pred.txt  Challenge dataset file: annotated.txt Testing predictions from example_pred.txt/ model               pos    neg    mixed    no-sent    spelling    desirable    idioms    strong    negated    w-know    amp.    comp.    irony    shift    emoji    modal    morph.    red.    vocab ----------------  -----  -----  -------  ---------  ----------  -----------  --------  --------  ---------  --------  ------  -------  -------  -------  -------  -------  --------  ------  ------- example_pred.txt   16.0   55.4     14.6        1.0        53.1         44.9      18.8      18.6       30.7      32.4    33.3     33.3     45.8     62.2     72.2     45.7       7.4    15.4     12.7  ```    """;Natural Language Processing;https://github.com/ltgoslo/assessing_and_probing_sentiment
"""```bash pip install --upgrade pip  pip install -r requirements.txt  #: Installs the wheel compatible with Cuda 11 and cudnn 8. pip install ""jax[cuda111]<=0.21.1"" -f https://storage.googleapis.com/jax-releases/jax_releases.html ```  Also  see other configurations for CUDA [here](https://github.com/google/jax#pip-installation-gpu-cuda).   For a PyTorch reimplementation see https://github.com/rail-berkeley/rlkit/tree/master/examples/iql   """;Reinforcement Learning;https://github.com/ikostrikov/implicit_q_learning
"""Yolo v4 source code: https://github.com/AlexeyAB/darknet  Useful links: https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&sk=6039748846bbcf1d960c3061542591d7   """;Computer Vision;https://github.com/Ashish-Sankritya/darknet_CNN
"""|Name | train | dev | test   """;Natural Language Processing;https://github.com/ky1994/SpeechRecognition
"""$ git clone http://github.com/lsds/Crossbow.git  $ cd Crossbow   $ ./scripts/build.sh   $ cd $CROSSBOW_HOME   $ ./scripts/benchmarks/resnet-50.sh   $ cd $CROSSBOW_HOME  $ ./scripts/datasets/mnist/prepare-mnist.sh  $ ./scripts/benchmarks/lenet.sh   """;General;https://github.com/lsds/Crossbow
"""  - No GPU     - Python 2.7   To train a copy task:  `python train.py --iterations=100000`   """;Sequential;https://github.com/camigord/Neural-Turing-Machine
"""python 3.7 / pytorch 1.2.0   """;Computer Vision;https://github.com/wangleihitcs/DeepLab-V1-PyTorch
"""the include shell script launch_train_sbatch.sh is setup to all training on the Enki Cluster.   2019-02-27 13:05:03.068185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0  1  2  3   2019-02-27 13:05:04.527345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 3   2019-02-27 13:05:04.528317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y Y Y   2019-02-27 13:05:04.529605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N Y Y   2019-02-27 13:05:04.530902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   Y Y N Y   2019-02-27 13:05:04.532257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   Y Y Y N   2019-02-27 13:45:03.442171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0  1  2  3   2019-02-27 13:45:05.405293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 2 3   2019-02-27 13:45:05.405302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y N N   2019-02-27 13:45:05.405307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N N N   2019-02-27 13:45:05.405311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 2:   N N N Y   2019-02-27 13:45:05.405315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 3:   N N Y N                           lmdb database to use for (Required)                           lmdb database to use for testing (Required)                           per gpu   """;Computer Vision;https://github.com/usnistgov/image-classification-resnet50
"""Start by cloning this repo:  ``` git clone https://github.com/delmalih/MIAS-mammography-obj-detection ```   conda create --name faster-r-cnn  conda activate faster-r-cnn  conda install ipython pip   pip install -r requirements.txt  cd ..  Then  run these commands (ignore if you have already done the FCOS installation) :   : install pytorch  conda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=9.0 -c pytorch   : install pycocotools  cd $INSTALL_DIR  git clone https://github.com/cocodataset/cocoapi.git  cd cocoapi/PythonAPI  python setup.py build_ext install  : install cityscapesScripts  cd $INSTALL_DIR  git clone https://github.com/mcordts/cityscapesScripts.git  cd cityscapesScripts/  python setup.py build_ext install  : install apex  cd $INSTALL_DIR  git clone https://github.com/NVIDIA/apex.git  cd apex  python setup.py install --cuda_ext --cpp_ext  : install PyTorch Detection  cd $INSTALL_DIR  git clone https://github.com/facebookresearch/maskrcnn-benchmark.git  cd maskrcnn-benchmark  python setup.py build develop  cd $INSTALL_DIR   conda create --name retinanet python=3.6  conda activate retinanet  conda install ipython pip   pip install -r requirements.txt  cd ..  pip install tensorflow-gpu==1.9  pip install keras==2.2.5  Then  run these commands :   : clone keras-retinanet repo  git clone https://github.com/fizyr/keras-retinanet  cd keras-retinanet  pip install .   conda create --name fcos  conda activate fcos  conda install ipython pip   pip install -r requirements.txt  cd ..  Then  follow these instructions   For example  to generate 10x augmented COCO annotations  run this command :                                       --output ../mias-db/COCO \   Go to the faster-r-cnn directory: cd faster-r-cnn  Change conda env: conda deactivate &amp;&amp; conda activate faster-r-cnn   cd retinanet  conda deactivate &amp;&amp; conda activate retinanet                   coco &lt;Path to the COCO dataset&gt;   tensorboard --logdir &lt;Path to the tensorboard directory&gt;   Follow these instructions   cd fcos  conda deactivate &amp;&amp; conda activate fcos   cd faster-r-cnn  conda deactivate &amp;&amp; conda activate faster-r-cnn   cd retinanet  conda deactivate &amp;&amp; conda activate retinanet                       coco &lt;Path to the COCO dataset&gt;  cd fcos  conda deactivate &amp;&amp; conda activate fcos   """;General;https://github.com/moallafatma/Breast_Cancer_Detection_Classification
"""To visualize the simulator in real time  you will need to install the OpenGL renderer  which is currently an independent C++ program. However this won't be possible if running Visual MPC on a remote server. In this case  there are other ways to inspect results (see `vismpc/README.md`).  These instructions have been tested on Mac OS X and Ubuntu 18.04. For some reason  we have not been able to get this working for Ubuntu 16.04. For Ubuntu 18.04  you might need sudo access for `make -j4 install`.  1. Navigate to `render/ext/libzmq`. Run ``` mkdir build cd build cmake .. make -j4 install ``` 2. Navigate to `render/ext/cppzmq`. Again run ``` mkdir build cd build cmake .. make -j4 install ``` 3. Navigate to `render`. Run ``` mkdir build cd build cmake .. make ```  Finally you should have an executable `clothsim` in `render/build`. **To test that it is working  go to `render/build` and run `./clothsim` on the command line. You should see an empty window appear. There should be no segmentation faults.** Occasionally I have seen it fail on installed machines  but normally rebooting fixes it.  To activate the renderer  set `init: render_opengl` to `True` in your cfg file.  Notes:  - If you make changes to `width`  `height`  or `render_port` in   `cfg/env_config.yaml`  please also update `num_width_points`    `num_height_points`  and `render_port` respectively in   `render/scene/pinned2.json`.  - It's easier to change the viewing angle by directly adjusting values in   `clothSimulator.cpp`  rather than with the mouse and GUI. When you adjust the   camera angles  be sure to re-compile the renderer using the instructions   above. You only need to re-compile `render`  not the other two.  [1]:https://github.com/openai/gym/tree/master/gym/envs [2]:https://github.com/openai/gym/pull/1314 [3]:https://arxiv.org/abs/2003.09044 [4]:https://sites.google.com/view/fabric-vsf [5]:https://github.com/BerkeleyAutomation/dvrk-vismpc/tree/master  1. Make a new virtualenv. For example  if you're using Python 2 and you put your environments in a directory `~/Envs`:     ```    virtualenv --python=python2 ~/Envs/py2-clothsim    ```  2. Run `pip install -r requirements.txt`.  3. Run `python setup.py install`. This should automatically ""cythonize"" the Python `.pyx` files and allow you to run `from gym_cloth import ...` regarless of current directory. You will need to re-run this command after making any changes in `gym_cloth/` or `vismpc/`  for example:     ```    python setup.py install ; python scripts/run.py    ```   This creates a gym environment based on our cloth simulator. The path directory   Python versions tested:  Python 2.7  Python 3.6   1. Make a new virtualenv. For example  if you're using Python 2 and you put your environments in a directory `~/Envs`:     ```    virtualenv --python=python2 ~/Envs/py2-clothsim    ```  2. Run `pip install -r requirements.txt`.  3. Run `python setup.py install`. This should automatically ""cythonize"" the Python `.pyx` files and allow you to run `from gym_cloth import ...` regarless of current directory. You will need to re-run this command after making any changes in `gym_cloth/` or `vismpc/`  for example:     ```    python setup.py install ; python scripts/run.py    ```   """;General;https://github.com/ryanhoque/fabric-vsf
"""```bash git clone git@github.com:philipperemy/keras-tcn.git cd keras-tcn virtualenv -p python3.6 venv source venv/bin/activate pip install -r requirements.txt #: change to tensorflow if you dont have a gpu. pip install . --upgrade #: install it as a package. ```  Note: Only compatible with Python 3 at the moment. Should be almost compatible with python 2.   pip install keras-tcn   Installation (Python 3)   pip install tox   """;Audio;https://github.com/ShotDownDiane/tcn-master
"""MobileNetV2 | 1.0 | 71.9 | 3 | 300M | [download](https://download.pytorch.org/models/mobilenet_v2-b0353104.pth)  MobileNetV3 | S\|L | 67.7`\|`74.0 | 3`\|`5 | 56M`\|`219M | [S](https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth)\|[L](https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth)   Datasets should have the following structure:   |__ COCO     <summary><strong>Requirements</strong></summary>  * python >= 3.6   Other requirements can be installed with `pip install -r requirements.txt`.   To train with a single GPU:   #:#: example using ade20k pretrained models   * https://github.com/CoinCheung/BiSeNet  * https://github.com/open-mmlab/mmsegmentation  * https://github.com/rwightman/pytorch-image-models   """;General;https://github.com/sithu31296/semantic-segmentation
"""This Repository includes YOLOv3 with some lightweight backbones (***ShuffleNetV2  GhostNet  VoVNet***)  some computer vision attention mechanism (***SE Block  CBAM Block  ECA Block***)  pruning quantization and distillation for GhostNet.  Attention : Single GPU will be better  If you need previous attention model or have any question  you can add my WeChat: AutrefoisLethe  python 3.7    pytorch >= 1.1.0     1. Download the datasets  place them in the ***data*** directory     2. Train the models by using following command (change the model structure by changing the cfg file)   ```   python3 train.py --data data/visdrone.data --batch-size 16 --cfg cfg/ghost-yolov3-visdrone.cfg --img-size 640 ``` 3. Detect objects using the trained model (place the pictures or videos in the ***samples*** directory)     ```   python3 detect.py --cfg cfg/ghostnet-yolov3-visdrone.cfg --weights weights/best.pt --data data/visdrone.data ``` 4. Results:   ![most](https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention/blob/master/output/most.png)   ![car](https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention/blob/master/output/car.png)   ![airplane](https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention/blob/master/output/airplane.png)    """;Computer Vision;https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention
""":#:  NAME   Example scripts for data set interface  training and testing various models are provided under `experiments/`. - N-class semantic segmentation with various architectures (your data) - Generative Adversarial Networks (MNIST) - Variational Autoencoders (MNIST) - Multi-instance / bagged labels (MNIST)  I've recently revised the way I structure experiments in order to separate my experiments from the structure of this repository. New examples coming soon ^(TM).   """;General;https://github.com/BioImageInformatics/tfmodels
"""pip install -r requirements.txt 安装所需安装包   """;Computer Vision;https://github.com/Shun14/enet
"""From PyPI:      pip install octconv  Bleeding edge version from github:      pip install git+https://github.com/braincreators/octconv.git#egg=octconv   ```python import torch from octconv import OctConv2d   #: (batch  channels  height  width) x = torch.rand(5  3  200  200)  conv1 = OctConv2d(in_channels=3  out_channels=10  kernel_size=3  alpha=(0.  0.5)  padding=1) conv2 = OctConv2d(in_channels=10  out_channels=20  kernel_size=7  alpha=(0.5  0.8)  padding=3) conv3 = OctConv2d(in_channels=20  out_channels=1  kernel_size=3  alpha=(0.8  0.)  padding=1)  out = conv3(conv2(conv1(x)))  #: shape: (5  1  200  200) ```   """;Computer Vision;https://github.com/braincreators/octconv
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;General;https://github.com/alannguyencs/maskrcnn
"""The requirements file has all the dependencies that are needed by MDETR.   We provide instructions how to install dependencies via conda. First  clone the repository locally: ``` git clone https://github.com/ashkamath/mdetr.git ```  Make a new conda env and activate it:  ``` conda create -n mdetr_env python=3.8 conda activate mdetr_env ```  Install the the packages in the requirements.txt: ``` pip install -r requirements.txt ```  Multinode training  Distributed training is available via Slurm and [submitit](https://github.com/facebookincubator/submitit): ``` pip install submitit ```    """;Computer Vision;https://github.com/ashkamath/mdetr
"""Manual: https://github.com/AlexeyAB/darknet/wiki   Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example   PyTorch > ONNX:    TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like CUDA  cudnn  ZED and build against those. It will also create a shared object library file to use darknet for code development.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows.  Install Visual Studio 2017 or 2019. In case you need to download it  please go here: Visual Studio Community  Install CUDA (at least v10.0) enabling VS Integration during installation.   PS Code\&gt;              git clone https://github.com/microsoft/vcpkg  PS Code\&gt;              cd vcpkg   PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base cuda cudnn]:x64-windows for a quicker install of dependencies  PS Code\vcpkg&gt;         cd ..  PS Code\&gt;              git clone https://github.com/AlexeyAB/darknet  PS Code\&gt;              cd darknet   Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   in Python: https://github.com/tzutalin/labelImg  in Python: https://github.com/Cartucho/OpenLabeling   in JavaScript: https://github.com/opencv/cvat   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux   * using `build.sh` or   * build `darknet` using `cmake` or   * set `LIBSO=1` in the `Makefile` and do `make` * on Windows   * using `build.ps1` or   * build `darknet` using `cmake` or   * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs:  * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h   * Python examples using the C API:     * https://github.com/AlexeyAB/darknet/blob/master/darknet.py     * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py  * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp   * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp  ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`      * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)  `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42)  ```cpp struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false);         std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/dipuchak95/dip_net
"""The main implementation of this code is available in `umap.parametric_umap` in the [UMAP repository](https://github.com/lmcinnes/umap) (v0.5+). Most people reading this will want to use that code  and can ignore this repository.   The code in this repository is the 'messy' version. It has custom training loops which are a bit more verbose and customizable. It might be more useful for integrating UMAP into your custom models.   The code can be installed with `python setup.py develop`. Though  unless you're just trying to reproduce our results  you'll probably just want to pick through the notebooks and tfumap folder for the code relevant to your project.   In addition  we have a more verbose Colab notebook to walk you through the algorithm:  Parametric UMAP (verbose) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1lpdCy7HkC5TRI9LfUtIHBBW8oRO86Nvi?usp=sharing)    """;General;https://github.com/timsainb/ParametricUMAP_paper
"""When *training with the backbone of [IBN-ResNet-50](https://arxiv.org/abs/1807.09441)*  you need to download the [ImageNet](http://www.image-net.org/) pre-trained model from this [link](https://drive.google.com/drive/folders/1thS2B8UOSBi_cJX6zRy6YYRwz_nVFI_S) and save it under the path of `logs/pretrained/`. ```shell mkdir logs && cd logs mkdir pretrained ``` The file tree should be ``` MMT/logs └── pretrained     └── resnet50_ibn_a.pth.tar ```   ```shell cd examples && mkdir data ``` Download the raw datasets [DukeMTMC-reID](https://arxiv.org/abs/1609.01775)  [Market-1501](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf)  [MSMT17](https://arxiv.org/abs/1711.08565)  and then unzip them under the directory like ``` MMT/examples/data ├── dukemtmc │   └── DukeMTMC-reID ├── market1501 │   └── Market-1501-v15.09.15 └── msmt17     └── MSMT17_V1 ```   ```shell git clone https://github.com/yxgeee/MMT.git cd MMT python setup.py install ```   sh scripts/pretrain.sh dukemtmc market1501 resnet50 1  sh scripts/pretrain.sh dukemtmc market1501 resnet50 2   sh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet50 500   Note that you could add --rr-gpu in the training scripts for faster clustering but requiring more GPU memory.   sh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet50   sh scripts/test.sh market1501 resnet50 logs/dukemtmcTOmarket1501/resnet50-MMT-500/model_best.pth.tar   sh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 500  sh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 700  sh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 900   sh scripts/train_baseline_dbscan.sh dukemtmc market1501 resnet50    Transferring from [DukeMTMC-reID](https://arxiv.org/abs/1609.01775) to [Market-1501](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf) on the backbone of [ResNet-50](https://arxiv.org/abs/1512.03385)  *i.e. Duke-to-Market (ResNet-50)*.   **Duke-to-Market (IBN-ResNet-50)** ```shell #: pre-training on the source domain sh scripts/pretrain.sh dukemtmc market1501 resnet_ibn50a 1 sh scripts/pretrain.sh dukemtmc market1501 resnet_ibn50a 2 #: end-to-end training with MMT-500 sh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet_ibn50a 500 #: or MMT-700 sh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet_ibn50a 700 #: or MMT-DBSCAN sh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet_ibn50a  #: testing the best model sh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-500/model_best.pth.tar sh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-700/model_best.pth.tar sh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-DBSCAN/model_best.pth.tar ``` **Duke-to-MSMT (ResNet-50)** ```shell #: pre-training on the source domain sh scripts/pretrain.sh dukemtmc msmt17 resnet50 1 sh scripts/pretrain.sh dukemtmc msmt17 resnet50 2 #: end-to-end training with MMT-500 sh scripts/train_mmt_kmeans.sh dukemtmc msmt17 resnet50 500 #: or MMT-1000 sh scripts/train_mmt_kmeans.sh dukemtmc msmt17 resnet50 1000 #: or MMT-DBSCAN sh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet50  #: testing the best model sh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-500/model_best.pth.tar sh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-1000/model_best.pth.tar sh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-DBSCAN/model_best.pth.tar ```   """;General;https://github.com/wangyuan249/Mymmt767
"""  * There have been minor changes with the 1.1.0 update. Now we support PyTorch 1.1.0 by default  and please use the legacy branch if you prefer older version.   Differences between Torch version   git clone https://github.com/thstkdgus35/EDSR-PyTorch  cd EDSR-PyTorch   We recommend you to pre-process the images before training. This step will decode all png files and save them as binaries. Use --ext sep_reset argument on your first run. You can skip the decoding part and use saved binaries with --ext sep argument.   cd src       #: You are now in */EDSR-PyTorch/src   Add --chop_forward argument to your script to enable it.   Now PyTorch 0.3.1 is a default. Use legacy/0.3.0 branch if you use the old version.   If you cannot make the binary pack  use the default setting (--ext img).   Please use the legacy/0.3.1 branch if you are using the old version of PyTorch.   with --pre_train download  pretrained models will be automatically downloaded from the server.   We support PyTorch 1.0.0. If you prefer the previous versions of PyTorch  use legacy branches.   You can test our super-resolution algorithm with your images. Place your images in ``test`` folder. (like ``test/<your_image>``) We support **png** and **jpeg** files.  Run the script in ``src`` folder. Before you run the demo  please uncomment the appropriate line in ```demo.sh``` that you want to execute. ```bash cd src       #: You are now in */EDSR-PyTorch/src sh demo.sh ```  You can find the result images from ```experiment/test/results``` folder.  | Model | Scale | File name (.pt) | Parameters | ****PSNR** | |  ---  |  ---  | ---       | ---        | ---  | | **EDSR** | 2 | EDSR_baseline_x2 | 1.37 M | 34.61 dB | | | | *EDSR_x2 | 40.7 M | 35.03 dB | | | 3 | EDSR_baseline_x3 | 1.55 M | 30.92 dB | | | | *EDSR_x3 | 43.7 M | 31.26 dB | | | 4 | EDSR_baseline_x4 | 1.52 M | 28.95 dB | | | | *EDSR_x4 | 43.1 M | 29.25 dB | | **MDSR** | 2 | MDSR_baseline | 3.23 M | 34.63 dB | | | | *MDSR | 7.95 M| 34.92 dB | | | 3 | MDSR_baseline | | 30.94 dB | | | | *MDSR | | 31.22 dB | | | 4 | MDSR_baseline | | 28.97 dB | | | | *MDSR | | 29.24 dB |  *Baseline models are in ``experiment/model``. Please download our final models from [here](https://cv.snu.ac.kr/research/EDSR/model_pytorch.tar) (542MB) **We measured PSNR using DIV2K 0801 ~ 0900  RGB channels  without self-ensemble. (scale + 2) pixels from the image boundary are ignored.  You can evaluate your models with widely-used benchmark datasets:  [Set5 - Bevilacqua et al. BMVC 2012](http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html)   [Set14 - Zeyde et al. LNCS 2010](https://sites.google.com/site/romanzeyde/research-interests)   [B100 - Martin et al. ICCV 2001](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/)   [Urban100 - Huang et al. CVPR 2015](https://sites.google.com/site/jbhuang0604/publications/struct_sr).  For these datasets  we first convert the result images to YCbCr color space and evaluate PSNR on the Y channel only. You can download [benchmark datasets](https://cv.snu.ac.kr/research/EDSR/benchmark.tar) (250MB). Set ``--dir_data <where_benchmark_folder_located>`` to evaluate the EDSR and MDSR with the benchmarks.  You can download some results from [here](https://cv.snu.ac.kr/research/EDSR/result_image/edsr-results.tar). The link contains **EDSR+_baseline_x4** and **EDSR+_x4**. Otherwise  you can easily generate result images with ``demo.sh`` scripts.   """;General;https://github.com/rkem1542/EDSR-pytorch
"""```bash $ pip install vit-pytorch ```   Vision Transformer - Pytorch   For a Pytorch implementation with pretrained models  please see Ross Wightman's repository <a href=""https://github.com/rwightman/pytorch-image-models"">here</a>.   You can also use the handy .to_vit method on the DistillableViT instance to get back a ViT instance.   Alternatively you can use one of several pre-defined models [2 4 6 7 8 14 16]   You can use it with the following code (ex. NesT-T)   from vit_pytorch.nest import NesT  nest = NesT(   pred = nest(img) #: (1  1000)   You can use it with the following code (ex. mobilevit_xs)   You can use it with the following code   $ pip install nystrom-attention   $ pip install x-transformers   ```python import torch from vit_pytorch import ViT  v = ViT(     image_size = 256      patch_size = 32      num_classes = 1000      dim = 1024      depth = 6      heads = 16      mlp_dim = 2048      dropout = 0.1      emb_dropout = 0.1 )  img = torch.randn(1  3  256  256)  preds = v(img) #: (1  1000) ```   """;General;https://github.com/lucidrains/vit-pytorch
"""GCN pytorch version >https://github.com/tkipf/pygcn   GAT pytorch version >https://github.com/Diego999/pyGAT   """;Graphs;https://github.com/liu6zijian/simplified-gcn-model
"""The lime package is on [PyPI](https://pypi.python.org/pypi/lime). Simply run:  ```sh pip install lime ```  Or clone the repository and run:  ```sh pip install . ```  We dropped python2 support in `0.2.0`  `0.1.1.37` was the last version before that.   For example usage for text classifiers  take a look at the following two tutorials (generated from ipython notebooks):  - [Basic usage  two class. We explain random forest classifiers.](https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html) - [Multiclass case](https://marcotcr.github.io/lime/tutorials/Lime%20-%20multiclass.html)  For classifiers that use numerical or categorical data  take a look at the following tutorial (this is newer  so please let me know if you find something wrong):  - [Tabular data](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html) - [Tabular data with H2O models](https://marcotcr.github.io/lime/tutorials/Tutorial_H2O_continuous_and_cat.html) - [Latin Hypercube Sampling](doc/notebooks/Latin%20Hypercube%20Sampling.ipynb)  For image classifiers:  - [Images - basic](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20images.html) - [Images - Faces](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20Faces%20and%20GradBoost.ipynb) - [Images with Keras](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20Image%20Classification%20Keras.ipynb) - [MNIST with random forests](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20MNIST%20and%20RF.ipynb) - [Images with PyTorch](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20images%20-%20Pytorch.ipynb)  For regression:  - [Simple regression](https://marcotcr.github.io/lime/tutorials/Using%2Blime%2Bfor%2Bregression.html)  Submodular Pick:  - [Submodular Pick](https://github.com/marcotcr/lime/tree/master/doc/notebooks/Submodular%20Pick%20examples.ipynb)  The raw (non-html) notebooks for these tutorials are available [here](https://github.com/marcotcr/lime/tree/master/doc/notebooks).  The API reference is available [here](https://lime-ml.readthedocs.io/en/latest/).   """;General;https://github.com/marcotcr/lime
"""Mixup is a generic and straightforward data augmentation principle. In essence  mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so  mixup regularizes the neural network to favor simple linear behavior in-between training examples.  This repository contains the implementation used for the results in our paper (https://arxiv.org/abs/1710.09412).   * A computer running macOS or Linux * For training new models  you'll also need a NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl) * Python version 3.6 * A [PyTorch installation](http://pytorch.org/)   """;Computer Vision;https://github.com/Ryoo72/dimension-wise_mixup
"""Districts and cities have periods of high demand for electricity  which raise electricity prices and the overall cost of the power distribution networks. Flattening  smoothening  and reducing the overall curve of electrical demand helps reduce operational and capital costs of electricity generation  transmission  and distribution networks. Demand response is the coordination of electricity consuming agents (i.e. buildings) in order to reshape the overall curve of electrical demand. CityLearn allows the easy implementation of reinforcement learning agents in a multi-agent setting to reshape their aggregated curve of electrical demand by controlling the storage of energy by every agent. Currently  CityLearn allows controlling the storage of domestic hot water (DHW)  and chilled water (for sensible cooling and dehumidification). CityLearn also includes models of air-to-water heat pumps  electric heaters  solar photovoltaic arrays  and the pre-computed energy loads of the buildings  which include space cooling  dehumidification  appliances  DHW  and solar generation.  All the costs are normalized by the costs a rule-based controller would have. Therefore  the different costs are normalized by different values and that is why the average_daily_peak cost may be higher than the peak_demand cost. To see the factors by which they are being normalized  you can run env.cost_rbc after the environment has run through at least one episode.   It's also worth mentioning that within your controller you can do any feature engineering you want using these observed variables that CityLearn returns. You need to use these observed variables from the  buildings_state_action_space.json only  but you can process them as you wish within your controller.   """;Reinforcement Learning;https://github.com/intelligent-environments-lab/CityLearn
"""pytorch-generative supports the following algorithms.    Notebook: https://github.com/EugenHotaj/pytorch-generative/blob/master/notebooks/style_transfer.ipynb <br>   Notebook: https://github.com/EugenHotaj/pytorch-generative/blob/master/notebooks/cppn.ipynb <br>   Supported models are implemented as PyTorch Modules and are easy to use:  ```python from pytorch_generative import models  model = models.ImageGPT(in_channels=1  in_size=28) ... model(data) ```  Alternatively  lower level building blocks in [pytorch_generative.nn](https://github.com/EugenHotaj/pytorch-generative/blob/master/pytorch_generative/nn.py) can be used to write models from scratch. For example  we implement a convolutional [ImageGPT](https://openai.com/blog/image-gpt/)-like model below:  ```python  from torch import nn  from pytorch_generative import nn as pg_nn   class TransformerBlock(nn.Module):   """"""An ImageGPT Transformer block.""""""    def __init__(self                  n_channels                  n_attention_heads):     """"""Initializes a new TransformerBlock instance.          Args:       n_channels: The number of input and output channels.       n_attention_heads: The number of attention heads to use.     """"""     super().__init__()     self._attn = pg_nn.MaskedAttention(         in_channels=n_channels          embed_channels=n_channels          out_channels=n_channels          n_heads=n_attention_heads          is_causal=False)     self._out = nn.Sequential(         nn.Conv2d(             in_channels=n_channels               out_channels=4*n_channels               kernel_size=1)          nn.GELU()          nn.Conv2d(             in_channels=4*n_channels               out_channels=n_channels               kernel_size=1))    def forward(self  x):     x = x + self._attn(x)     return x + self._out(x)   class ImageGPT(nn.Module):   """"""The ImageGPT Model.      Note that we don't use LayerNorm because it would break the model's    autoregressive property.   """"""      def __init__(self                        in_channels                 in_size                 n_transformer_blocks=8                 n_attention_heads=4                 n_embedding_channels=16):     """"""Initializes a new ImageGPT instance.          Args:       in_channels: The number of input channels.       in_size: Size of the input images. Used to create positional encodings.       n_transformer_blocks: Number of TransformerBlocks to use.       n_attention_heads: Number of attention heads to use.       n_embedding_channels: Number of attention embedding channels to use.     """"""     super().__init__()     self._pos = nn.Parameter(torch.zeros(1  in_channels  in_size  in_size))     self._input = pg_nn.MaskedConv2d(         is_causal=True          in_channels=in_channels          out_channels=n_embedding_channels          kernel_size=3          padding=1)     self._transformer = nn.Sequential(         *[TransformerBlock(n_channels=n_embedding_channels                           n_attention_heads=n_attention_heads)           for _ in range(n_transformer_blocks)])     self._out = nn.Conv2d(in_channels=n_embedding_channels                            out_channels=in_channels                            kernel_size=1)    def forward(self  x):     x = self._input(x + self._pos)     x = self._transformer(x)     return self._out(x) ```   """;Computer Vision;https://github.com/eyalbetzalel/pytorch-generative-v2
"""Before you proceed  pip install -r ./requirements.txt   """;Sequential;https://github.com/alexandra-chron/ntua-slp-wassa-iest2018
"""   https://arxiv.org/pdf/1512.02325.pdf  https://arxiv.org/pdf/1801.04381.pdf  ---  For a quick dive into the project  run  ``` https://github.com/eddiebarry/SingleShotDetector/blob/master/SSDLite.ipynb ```  in colab  ---      - train.py     - test.py     ``` python train.py ```  At the start of each epoch  model predictions are visualised and saved in   ``` './test_results/out_img/'  ```  by default  ---   """;Computer Vision;https://github.com/eddiebarry/SingleShotDetector
"""```python import torch import torch_blocksparse  #: Z: non-sparse batch dimension #: H: sparse batch dimension #: M: row dimension #: N: column dimension Z  H  M  N  K = 4  2  256  512  384 a = torch.rand((Z  H  M  K)  dtype=torch.float32).cuda() b = torch.rand((Z  H  K  N)  dtype=torch.float32).cuda() #: create sparsity layout block = 16 layout = torch.randint(0  2  (H  M//block  N//block)) #: create object for Sparse = trans(Dense) x Dense (sdd) #: some overhead there as it pre-computes look-up tables  #: internally needed by GPU kernels dot = torch_blocksparse.MatMul(layout  block  'sdd'  trans_a=True  trans_b=False) c = dot(a  b) #: create object for Sparse = softmax(Sparse) softmax = torch_blocksparse.Softmax(layout  block) d = softmax(c) ``` """;Natural Language Processing;https://github.com/ptillet/torch-blocksparse
"""( see https://github.com/johansatge/jpeg-autorotate )   """;Computer Vision;https://github.com/jason9693/cycleGAN-tensorflow-v2
"""- Được xây dựng trên framwork Pytorch.    Dữ liệu dùng để huấn luyện mô hình của đồ án chỉ sử dụng tập train của dữ liệu gốc  sau đó được phân chia thành 3 tập tương ứng là train  validation  test với tỉ lệ tương ứng là 60  20  20.   """;Natural Language Processing;https://github.com/yscope75/CS2225.CH2001020
"""Follow the instructions at the [Quickstart Guide](https://cloud.google.com/tpu/docs/quickstart) to get a GCE VM with access to Cloud TPU.  To run this model  you will need:  * A GCE VM instance with an associated Cloud TPU resource * A GCS bucket to store your training checkpoints * (Optional): The ImageNet training and validation data preprocessed into   TFRecord format  and stored in GCS.   If you would like to make any fixes or improvements to the models  please   If you do not have ImageNet dataset prepared  you can use a randomly generated   To run the same code on CPU/GPU  set the flag --use_tpu=False. This will use   created by CPU/GPU and TPU are all identical so it is possible to train on one   layers. The configuration can be controlled via --resnet_size. Bigger models   """;Computer Vision;https://github.com/serre-lab/gala_tpu
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/Osobarako/alduswarrensewell
"""1. Clone the repository and make sure that all the dependencies listed above are installed. 2. Download all the resources from [here](https://drive.google.com/open?id=1AcGulyTXcrsn6hStefD3M0MNrzkxV_1n) and place them in the *res/* directory 3. Download the pre-trained models from [here](https://drive.google.com/open?id=1ss9-4LEzuKC-p1s0lLa0XVu2_ERM-ynL) and place them in the *models/* directory 4. Go to the *src/* directory 5. For a thorough feature analysis  run: ```bash python feature_analysis.py ``` 6. For training and evaluating a traditional machine learning model  run: ```bash python ml_models.py ``` 7. For training and evaluating the embeddings (word and/or emojis/deepmojis)  run: ```bash python embeddings_model.py ``` 8. For training and evaluating various deep learning models  quickly implemented in Keras  run: ```bash python dl_models.py ``` 9. For training and evaluating the attention-based LSTM model implemented in TensorFlow  run: ```bash python tf_attention.py ```  By default  the dataset collected by [Ghosh and Veale (2016)](http://www.aclweb.org/anthology/W16-0425) is used  but this can be easily replaced by changing the *dataset* parameter in the code (as for all other parameters).   """;General;https://github.com/qq345736500/sarcasm
"""Instalación mediante pip:   Además  necesitaremos instalar el software de NVIDIA donde se puede encontrar la información aquí: https://www.tensorflow.org/install/gpu/   Sacada de https://www.tensorflow.org/install/source#linux   Parecido a la instalación mediante pip pero con más facilidades  para instalar Tensorflow gpu basta con poner el siguiente comando:  conda install tensorflow-gpu==1.13.1   Tiene 2 comandos ‘compile e ‘infer’  ‘compile’ compila el modelo entrenado (.tflite  .caffemodel) a kmodel.   tar -Jxf ncc-linux-x86_64.tar.xz  rm ncc-linux-x86_64.tar.xz   enlace aquí: https://github.com/kendryte/nncase/releases  la versión utilizada es  NNCase Converter v0.1.0 RC5   ./tflite2kmodel.sh workspace/mbnet75.tflite   """;General;https://github.com/SalvadorAlbarran/TFG2020
"""GPUs: K80 ($0.14/hr)  T4 ($0.11/hr)  V100 ($0.74/hr) CUDA with Nvidia Apex FP16/32     Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [Notebook](https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb) <a href=""https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a> * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) << highly recommended * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart)  ![Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/yolov3?logo=docker) * [A TensorRT Implementation of YOLOv3 and YOLOv4](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp)     """;Computer Vision;https://github.com/thanhtran98/yolov3_jetbot
"""We make VERSE available in two forms: fast  optimized C++ code that was used in the experiments  and more convenient python wrapper. Note that wrapper is still experimental and may not provide optimal performance.  For C++ executables: ```bash cd src && make; ``` should be enough on most platforms. If you need to change the default compiler (i.e. to Intel)  use: ```bash make CXX=icpc ```  VERSE is able to encompass diverse similarity measures under its model. For performance reasons  we have implemented three different similarities separately.  Use the command ```bash verse -input data/karate.bcsr -output karate.bin -dim 128 -alpha 0.85 -threads 4 -nsamples 3 ``` to run the default version (that corresponds to PPR similarity) with 128 embedding dimension  PPR alpha 0.85  using 3 negative samples.   1 2 3 4 5 6 7 8 9 11 12 13 14 18 20 22 32       (note  you must also specify the variable name of the adjacency matrix --matfile-variable-name)   We make VERSE available in two forms: fast  optimized C++ code that was used in the experiments  and more convenient python wrapper. Note that wrapper is still experimental and may not provide optimal performance.  For C++ executables: ```bash cd src && make; ``` should be enough on most platforms. If you need to change the default compiler (i.e. to Intel)  use: ```bash make CXX=icpc ```  VERSE is able to encompass diverse similarity measures under its model. For performance reasons  we have implemented three different similarities separately.  Use the command ```bash verse -input data/karate.bcsr -output karate.bin -dim 128 -alpha 0.85 -threads 4 -nsamples 3 ``` to run the default version (that corresponds to PPR similarity) with 128 embedding dimension  PPR alpha 0.85  using 3 negative samples.   """;Graphs;https://github.com/xgfs/verse
"""W0813 20:56:21.233371 54904 device_context.cc:422] device: 0  cuDNN Version: 7.6.   链接: https://pan.baidu.com/s/1CftnEt0nl1V6w6ApDzqkKA 提取码: dyvf   """;General;https://github.com/thinkall/deepfefm
"""    git clone git@github.com:RicardoZiTseng/dcnn-tensorflow.git     cd dcnn-tensorflow         python main.py   """;Graphs;https://github.com/RicardoZiTseng/dcnn-tensorflow
"""Basically this implementation converts DAUs into a single K x K kernel  and then uses a standard conv2d operation  which can be simply done by:  ```Python          F = 128 #: num output channels     G = 4 #: num DAUs per channel     S = 64 #: num input channels         max_kernel_size = 17          dau_w = tf.Variable(shape=(1 S G F))     dau_mu1 = tf.Variable(shape=(1 S G F))     dau_mu2 = tf.Variable(shape=(1 S G F))     dau_sigma = tf.Variable(shape=(1 S G F))           [X Y] = np.meshgrid(np.arange(max_kernel_size) np.arange(max_kernel_size))          X = np.reshape(X (max_kernel_size*max_kernel_size 1 1 1)) - int(max_kernel_size/2)     Y = np.reshape(Y (max_kernel_size*max_kernel_size 1 1 1)) - int(max_kernel_size/2)              #: Gaussian kernel     gauss_kernel = tf.exp(-1* (tf.pow(X - dau_mu1 2.0) + tf.pow(Y - dau_mu2 2.0)) / (2.0*tf.pow(dau_sigma 2.0)) name='gauss_kernel')     gauss_kernel_sum = tf.reduce_sum(gauss_kernel axis=0  keep_dims=True name='guass_kernel_sum')     gauss_kernel_norm = tf.divide(gauss_kernel  gauss_kernel_sum  name='gauss_kernel_norm')          #: normalize to sum of 1 and add weight     gauss_kernel_norm = tf.multiply(dau_w  gauss_kernel_norm name='gauss_kernel_weight')          #: sum over Gaussian units     gauss_kernel_norm = tf.reduce_sum(gauss_kernel_norm  axis=2  keep_dims=True name='gauss_kernel_sum_units')          #: convert to [Kw Kh S F] shape     gauss_kernel_norm = tf.reshape(gauss_kernel_norm  (max_kernel_size  max_kernel_size  gauss_kernel_norm.shape[1]  gauss_kernel_norm.shape[3]) name='gauss_kernel_reshape')            output = tf.nn.conv2d(inputs  gauss_kernel_norm)      ```   We provide a wrapper based on `tf.contrib.layer.conv2d()` API  that is also compatible/interchangeable with the `dau_conv2d` from [DAU-ConvNet](http://github.com/skokec/DAU-ConvNet).   Install using pip: ```bash sudo pip3 install https://github.com/skokec/DAU-ConvNet-TF/releases/download/v1.0/dau_conv_tf-1.0-py3-none-any.whl   ```  There are two available methods to use:   ```python from dau_conv_tf import dau_conv2d_tf  dau_conv2d_tf(inputs               filters  #: number of output filters              dau_units  #: number of DAU units per image axis  e.g  (2 2) for 4 DAUs per filter               max_kernel_size  #: maximal possible size of kernel that limits the offset of DAUs (highest value that can be used=17)                stride=1  #: only stride=1 supported               mu_learning_rate_factor=500  #: additional factor for gradients of mu1 and mu2              dau_unit_border_bound=1               data_format=None               activation_fn=tf.nn.relu               normalizer_fn=None               normalizer_params=None               weights_initializer=tf.random_normal_initializer(stddev=0.1)                weights_regularizer=None               mu1_initializer=None                mu1_regularizer=None                mu2_initializer=None               mu2_regularizer=None               sigma_initializer=None               sigma_regularizer=None               biases_initializer=tf.zeros_initializer()               biases_regularizer=None               reuse=None               variables_collections=None               outputs_collections=None               trainable=True               scope=None) ```   ```python from dau_conv_tf import DAUConv2dTF  DAUConv2dTF(filters  #: number of output filters            dau_units  #: number of DAU units per image axis  e.g  (2 2) for 4 DAUs total per one filter            max_kernel_size  #: maximal possible size of kernel that limits the offset of DAUs (highest value that can be used=17)            strides=1  #: only stride=1 supported            data_format='channels_first'  #: supports only 'channels_last'             activation=None             use_bias=True             weight_initializer=tf.random_normal_initializer(stddev=0.1)             mu1_initializer=None              mu2_initializer=None              sigma_initializer=None             bias_initializer=tf.zeros_initializer()             weight_regularizer=None             mu1_regularizer=None             mu2_regularizer=None             sigma_regularizer=None             bias_regularizer=None             activity_regularizer=None             weight_constraint=None             mu1_constraint=None             mu2_constraint=None             sigma_constraint=None             bias_constraint=None             trainable=True             mu_learning_rate_factor=500  #: additional factor for gradients of mu1 and mu2            dau_unit_border_bound=1               unit_testing=False  #: for competability between CPU and GPU version (where gradients of last edge need to be ignored) during unit testing            name=None) ```   """;Computer Vision;https://github.com/skokec/DAU-ConvNet-TF
"""**Swin Transformer** (the name `Swin` stands for **S**hifted **win**dow) is initially described in [arxiv](https://arxiv.org/abs/2103.14030)  which capably serves as a general-purpose backbone for computer vision. It is basically a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection.  Swin Transformer achieves strong performance on COCO object detection (`58.7 box AP` and `51.1 mask AP` on test-dev) and ADE20K semantic segmentation (`53.5 mIoU` on val)  surpassing previous models by a large margin.  ![teaser](figures/teaser.png)   a CLA and decorate the PR appropriately (e.g.  status check  comment). Simply follow the instructions   - For **Image Classification**  please see [get_started.md](get_started.md) for detailed instructions. - For **Object Detection and Instance Segmentation**  please see [Swin Transformer for Object Detection](https://github.com/SwinTransformer/Swin-Transformer-Object-Detection). - For **Semantic Segmentation**  please see [Swin Transformer for Semantic Segmentation](https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation). - For **Self-Supervised Learning**  please see [Transformer-SSL](https://github.com/SwinTransformer/Transformer-SSL). - For **Video Recognition**  please see [Video Swin Transformer](https://github.com/SwinTransformer/Video-Swin-Transformer).   ***In this pargraph  we cross link third-party repositories which use Swin and report results. You can let us know by raising an issue***   (`Note please report accuracy numbers and provide trained models in your new repository to facilitate others to get sense of correctness and model behavior`)  [12/21/2021] Swin Transformer for StyleGAN: [StyleSwin](https://github.com/microsoft/StyleSwin)  [12/13/2021] Swin Transformer for Face Recognition: [FaceX-Zoo](https://github.com/JDAI-CV/FaceX-Zoo)  [08/29/2021] Swin Transformer for Image Restoration: [SwinIR](https://github.com/JingyunLiang/SwinIR)  [08/12/2021] Swin Transformer for person reID: [https://github.com/layumi/Person_reID_baseline_pytorch](https://github.com/layumi/Person_reID_baseline_pytorch)  [06/29/2021] Swin-Transformer in PaddleClas and inference based on whl package: [https://github.com/PaddlePaddle/PaddleClas](https://github.com/PaddlePaddle/PaddleClas)  [04/14/2021] Swin for RetinaNet in Detectron: https://github.com/xiaohu2015/SwinT_detectron2.  [04/16/2021] Included in a famous model zoo: https://github.com/rwightman/pytorch-image-models.  [04/20/2021] Swin-Transformer classifier inference using TorchServe: https://github.com/kamalkraj/Swin-Transformer-Serve   """;Computer Vision;https://github.com/microsoft/Swin-Transformer
"""During experimentation  we used Linux machines with `conda` virtual environments  PyTorch 1.8 and CUDA 11.  Start by cloning this repo ```bash git clone https://github.com/v-iashin/SpecVQGAN.git ```  Next  install the environment. For your convenience  we provide both `conda` and `docker` environments.   BMVC 2021 – Oral Presentation  • [[Project Page](https://v-iashin.github.io/SpecVQGAN)] • [[ArXiv](http://arxiv.org/abs/2110.08791)] • [[BMVC Proceedings](https://www.bmvc2021-virtualconference.com/conference/papers/paper_1213.html)] • [[Poster (for PAISS)](https://v-iashin.github.io/images/specvqgan/poster.pdf)] • [[Presentation on YouTube](https://www.youtube.com/watch?v=Bucb3nAa398)] ([Can't watch YouTube?](https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/specvqgan_public/SpecVQGAN%20YouTube.mp4)) •  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1pxTIMweAKApJZ3ZFqyBee3HtMqFpnwQ0?usp=sharing)  <img src=""https://github.com/v-iashin/v-iashin.github.io/raw/master/images/specvqgan/specvqgan_vggsound_samples.jpg"" alt=""Generated Samples Using our Model"" width=""900"">  Listen for the samples on our [project page](https://v-iashin.github.io/SpecVQGAN).   Environment Preparation   conda env create -f conda_env.yml  Test your environment  conda activate specvqgan       python   or build it yourselfbash   name: down_audioset     - conda-forge     - pip=20.1.1=py38_1    - python=3.8.3=hcff3b4d_2     - pip:       - scipy==1.5.1   Install the environment: conda env create -f down_audioset.yml   | 212 Feats | BN Inception |  20.5 |        6.0 |          11.8 | 1c4e56077d737677eac524383e6d98d3 |  | 212 Feats |     ResNet50 | 20.8 |       6.2 |          11.8 | 6e553ea44c8bc7a3310961f74e7974ea |   We run our experiments on a relatively expensive hardware setup with four 40GB NVidia A100 but the models   Run it on a 12GB GPU as  cd ./specvqgan/modules/losses/vggishish   cd ./vocoder   We provide a multi-gpu command which can easily be applied on a multi-node setup by replacing --master_addr to your main machine and --node_rank for every worker's id (also see an sbatch script in ./evaluation/sbatch_sample.sh if you have a SLURM cluster at your disposal):   While the Spectrogram VQGAN was never designed to be a neural audio codec but it happened to be highly effective for this task. We can employ our Spectrogram VQGAN pre-trained on an open-domain dataset as a neural audio codec without a change  If you wish to apply the SpecVQGAN for audio compression for arbitrary audio  please see our Google Colab demo: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1K_-e6CRQFLk9Uq6O46FOsAYt63TeEdXf?usp=sharing).  Integrated to [Huggingface Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/SpecVQGAN_Neural_Audio_Codec)  We also alternatively provide a similar notebook in `./neural_audio_codec_demo.ipynb` to play with the demo on a local machine.   """;Computer Vision;https://github.com/v-iashin/SpecVQGAN
"""This project is built around the wonderful Fast.AI library.  Prereqs  in summary: * **Fast.AI 1.0.51** (and its dependencies).  If you use any higher version you'll see grid artifacts in rendering and tensorboard will malfunction. So yeah...don't do that. * **PyTorch 1.0.1** Not the latest version of PyTorch- that will not play nicely with the version of FastAI above.  Note however that the conda install of FastAI 1.0.51 grabs the latest PyTorch  which doesn't work.  This is patched over by our own conda install but fyi. * **Jupyter Lab** `conda install -c conda-forge jupyterlab` * **Tensorboard** (i.e. install Tensorflow) and **TensorboardX** (https://github.com/lanpa/tensorboardX).  I guess you don't *have* to but man  life is so much better with it.  FastAI now comes with built in support for this- you just  need to install the prereqs: `conda install -c anaconda tensorflow-gpu` and `pip install tensorboardX` * **ImageNet** – Only if you're training  of course. It has proven to be a great dataset for my purposes.  http://www.image-net.org/download-images  --------------------------  You should now be able to do a simple install with Anaconda. Here are the steps:  Open the command line and navigate to the root folder you wish to install.  Then type the following commands  ```console git clone https://github.com/jantic/DeOldify.git DeOldify cd DeOldify conda env create -f environment.yml ``` Then start running with these commands: ```console source activate deoldify jupyter lab ```  From there you can start running the notebooks in Jupyter Lab  via the url they provide you in the console.    **NOTE** You can also now do ""conda activate deoldify"" if you have the latest version of conda and in fact that's now recommended. But a lot of people don't have that yet so I'm not going to make it the default instruction here yet.   Get more updates on Twitter <img src=""resource_images/Twitter_Social_Icon_Rounded_Square_Color.svg"" width=""16"">   git clone https://github.com/jantic/DeOldify.git DeOldify   cd DeOldify &amp;&amp; docker build -t deoldify_jupyter -f Dockerfile .   git clone https://github.com/jantic/DeOldify.git DeOldify   [![](http://img.youtube.com/vi/l3UXXid04Ys/0.jpg)](http://www.youtube.com/watch?v=l3UXXid04Ys """")   [![](http://img.youtube.com/vi/EXn-n2iqEjI/0.jpg)](http://www.youtube.com/watch?v=EXn-n2iqEjI """")  -----------------------   ""Migrant Mother"" by Dorothea Lange (1936)  ![MigrantMother](https://i.imgur.com/Bt0vnke.jpg)   Woman relaxing in her livingroom in Sweden (1920)  ![SwedenLivingRoom](https://i.imgur.com/158d0oU.jpg)   ""Toffs and Toughs"" by Jimmy Sime (1937)  ![ClassDivide](https://i.imgur.com/VYuav4I.jpg)   Thanksgiving Maskers (1911)  ![ThanksgivingMaskers](https://i.imgur.com/n8qVJ5c.jpg)   Glen Echo Madame Careta Gypsy Camp in Maryland (1925)  ![GypsyCamp](https://i.imgur.com/1oYrJRI.jpg)   ""Mr. and Mrs. Lemuel Smith and their younger children in their farm house  Carroll County  Georgia."" (1941)  ![GeorgiaFarmhouse](https://i.imgur.com/I2j8ynm.jpg)    ""Building the Golden Gate Bridge"" (est 1937)  ![GoldenGateBridge](https://i.imgur.com/6SbFjfq.jpg) <sub>NOTE:  What you might be wondering is while this render looks cool  are the colors accurate? The original photo certainly makes it look like the towers of the bridge could be white. We looked into this and it turns out the answer is no- the towers were already covered in red primer by this time. So that's something to keep in mind- historical accuracy remains a huge challenge!</sub>   ""Terrasse de café  Paris"" (1925)  ![CafeParis](https://i.imgur.com/WprQwP5.jpg)   Norwegian Bride (est late 1890s)  ![NorwegianBride](https://i.imgur.com/MmtvrZm.jpg)   Zitkála-Šá (Lakota: Red Bird)  also known as Gertrude Simmons Bonnin (1898)  ![NativeWoman](https://i.imgur.com/zIGM043.jpg)   Chinese Opium Smokers (1880)  ![OpiumReal](https://i.imgur.com/lVGq8Vq.jpg)  -------------------------  So that's the gist of this project – I'm looking to make old photos and film look reeeeaaally good with GANs  and more importantly  make the project *useful*.  In the meantime though this is going to be my baby and I'll be actively updating and improving the code over the foreseeable future.  I'll try to make this as user-friendly as possible  but I'm sure there's going to be hiccups along the way.    Oh and I swear I'll document the code properly...eventually.  Admittedly I'm *one of those* people who believes in ""self documenting code"" (LOL).  -----------------------   The easiest way to get started is to go straight to the Colab notebooks:   Image [<img src=""https://colab.research.google.com/assets/colab-badge.svg"" align=""center"">](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb) | Video [<img src=""https://colab.research.google.com/assets/colab-badge.svg"" align=""center"">](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb)   Special thanks to Matt Robinson and María Benavente for their image Colab notebook contributions  and Robert Bell for the video Colab notebook work!  -----------------------   """;Computer Vision;https://github.com/yashwanyne/Robust-Video-Colorization-with-Self-Attention-based-Progressive-Generative-Adversarial-Networks
"""To install this repository  you can directly clone it from the command line:  ```shell script git clone https://github.com/amogh7joshi/plant-health-detection.git ```  Then  enter the repository and install system requirements.  ```shell script #: Enter the Repository cd plant-health-detection  #: Install System Requirements python3 -m pip install -r requirements.txt ```  Then  you can download the plant leaf health dataset from [this location](https://data.mendeley.com/datasets/hb74ynkjcn/1). It will take up around 6 GB of space. Move the folder containing the dataset into the `data` subdirectory. Then  run the `preprocess.sh` script in order to preprocess the data. It takes around 3 minutes to process each of `healthy|diseased` images  so the script splits them up if you  would like.   """;Computer Vision;https://github.com/amogh7joshi/plant-health-detection
"""``` $ python train.py model.block_type basic model.depth 110 run.outdir results ```   ``` $ python train.py model.block_type basic model.depth 110 model.remove_first_relu True model.add_last_bn True run.outdir results ```   """;Computer Vision;https://github.com/hysts/pytorch_resnet_preact
"""Aloception's packages are built on top of multiple libraries. Most of them are listed in the **requirements.txt** ``` pip install -r requirements.txt ```  Once the others packages are installed  you still need to install pytorch based on your hardware and environment configuration. Please  ref to the `pytorch website <https://pytorch.org/>`_  for this install.   pytorch  and  pytorch lightning.   <ul>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/getting_started.html"">Getting started</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/aloscene.html"">Aloscene: Computer vision with ease</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/alodataset.html"">Alodataset: Loading your vision datasets</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/alonet.html"">Alonet: Loading & training your models</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/augmented_tensor.html"">About augmented tensors</a></li> </ul>    <ul>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/data_setup.html"">How to setup your data?</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/training_detr.html"">Training Detr</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/finetuning_detr.html"">Finetuning DETR</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/training_panoptic.html"">Training Panoptic Head</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/training_deformable_detr.html"">Training Deformable DETR</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/finetuning_deformable_detr.html"">Finetuning Deformanble DETR</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/tensort_inference.html"">Exporting DETR / Deformable-DETR to TensorRT</a></li> </ul>   """;General;https://github.com/Visual-Behavior/aloception
"""A corpus is a set of documents that will be used to train the text generator. You will need to adequate your corpus according to one of the following formats:   You will need an **Anaconda Python 3** distribution. Then run the following commands to install the required python packages in your active environment:      make install      or  if you want to build the project with GPU support  run      make install-gpu   You can make use of neurowriter either through a dockerized version  or you may install it locally in your computer.    make build-image  to build the Neurowriter Docker image. If instead you want to build this image with GPU support  you will also need    make build-image-gpu   You will need to provide the following arguments:    that you are hiding you to fight the place that You-Know-Who would be to keep the snake to see you in the    The basic process to create a text generator is the following:  * Prepare a **corpus** of documents in a **proper format**. * **Tokenize** the corpus (optional  but strongly recommended). * Select a **model architecture** to learn from the corpus and run the **training** process. * Use the created model to generate new texts!   Pre-trained models are available for some of these examples: check the **samplemodels** folder.   """;General;https://github.com/albarji/neurowriter
"""```shell script pip install -r requirements.txt ```  Require python 3.6 + (to support huggingface [transformers](https://github.com/huggingface/transformers)).       bash tokenization/tokenize_wiki103_bert.bash       bash tokenization/tokenize_wiki_bert.bash   : bash scripts/extract_keys.bash $GPU_ID $MODEL_NAME  bash scripts/extract_keys.bash 0 bert_resnext    bash scripts/xmatching_benchmark.bash 0 bert_resnext   Note: --tokenizer-name must be provided in the script.       #: bash scripts/mpvokenize_wiki103.bash $USE_GPUS $SNAP_NAME      bash scripts/mpvokenize_wiki103.bash 0 1 2 3 bert_resnext       #: bash scripts/mpvokenize_wiki.bash $USE_GPUS $SNAP_NAME      bash scripts/mpvokenize_wiki.bash 0 1 2 3 bert_resnext   bash scripts/small_vlm_wiki103.bash 0 1 2 3 wiki103_bert_small   please install the nvidia/apex library with command:   git clone https://github.com/NVIDIA/apex  cd apex   the script scripts/small_vlm_wiki103.bash.   bash scripts/base_vlm_wiki.bash 0 1 2 3 wiki_bert_base   installing apex).   Other tasks could be evaluated following the setup here   bash scripts/run_glue_epochs.bash 0 1 2 3 snap/vlm/wiki103_bert_small --snaps 7                             bash scripts/run_glue_epochs.bash 0 snap/vlm/wiki103_bert_small --snaps 1   bash scripts/small_wiki103.bash 0 1 2 3 bert_small   Or you could directly using the script small_wiki103_glue.bash to    bash scripts/small_wiki103_glue.bash 0 1 2 3 bert_small   bash scripts/base_wiki.bash 0 1 2 3 bert_wiki   bash scripts/base_wiki_glue.bash 0 1 2 3 bert_wiki   For all results saved under `snap/` (whatever the dir names)  running the folloing command will print out all the results. ```bash python vlm/show_glue_results_epochs.py  ```  It will print results like ``` snap/vlm/test_finetune/glueepoch_checkpoint-epoch0019      RTE    MRPC   STS-B    CoLA   SST-2    QNLI     QQP    MNLI MNLI-MM    GLUE    54.51   84.72   87.18   52.32   90.02   88.36   87.16   81.92   82.57   78.75 snap/vlm/bert_6L_512H_wiki103_sharedheadctr_noshuffle/glueepoch_checkpoint-epoch0029      RTE    MRPC   STS-B    CoLA   SST-2    QNLI     QQP    MNLI MNLI-MM    GLUE    58.12   82.76   84.45   26.74   89.56   84.40   86.52   77.56   77.99   74.23 ```   """;Computer Vision;https://github.com/airsplay/vokenization
"""Which benefits:  - PCB structure  - PCB randomly update  - batchnorm  - random erasing  zero paddding crop  - warm-up learning rate  - global branch  - small batchsize  Which might helps:  - feature erasing  - feature mask  - tri-loss  - balanced sampling  - multi-gpu training (differs in BN layer)    Not working:  - adam  - am-softmax  - bias in FC layer or BN     This code depends on pytorch v0.4 and torchvision  run the  following command to install pytorch:  ``` pip install --user torch==0.4 torchvision==0.2.1 tensorflow==1.8 tensorboardX lmdb -i https://pypi.douban.com/simple/ ```   cd pytorch-reid-lite   cd ~/.local/bin   For following settings   GPU memory usage:   For following settings   GPU memory usage:   ""batch_size"": The batch_size PER GPU.  ""batches_dir"": The path to your dataset generated by the open platform.   ""parallels"": The GPU(s) to train your model on.   """;Computer Vision;https://github.com/xuxu116/pytorch-reid-lite
"""In the paper the anchor setting is `Ratios： [0.5 1 2] scales :[8 ]`  With the setting and P2~P6  all anchor sizes are  `[32 64 128 512 1024]` but this setting is suit for COCO dataset which has so many small targets.  But the voc dataset targets are range `[128 256 512]`.  So  we desgin the anchor setting:`Ratios： [0.5 1 2] scales :[8 16]`  this is very import for voc dataset.   cd caffe-fpn  mkdir build  cd build   make -j16 all  cd lib   ./experiments/scripts/FP_Net_end2end.sh 1 FPN pascal_voc  ./test.sh 1 FPN pascal_voc    ./experiments/scripts/FP_Net_end2end_merge_rcnn.sh 0 FPN pascal_voc   ./test_mergercnn.sh 0 FPN pascal_voc  0 1 is GPU id.   download  voc07 12 dataset `ResNet50.caffemodel` and rename to `ResNet50.v2.caffemodel`  ```bash cp ResNet50.v2.caffemodel data/pretrained_model/ ``` - OneDrive download: [link](https://onedrive.live.com/?authkey=%21AAFW2-FVoxeVRck&id=4006CBB8476FF777%2117887&cid=4006CBB8476FF777)  `In my expriments  the codes require ~10G GPU memory in training and ~6G in testing.  your can design the suit image size  mimbatch size and rcnn batch size for your GPUS.`  """;Computer Vision;https://github.com/rickyHong/FPN-repl
"""```python from ops import *   x = conv(x)   x = group_norm(x)  ```   """;General;https://github.com/taki0112/Group_Normalization-Tensorflow
"""Additionally  you will need to install the threads and optnet package:   luarocks install threads  luarocks install optnet   mkdir celebA; cd celebA   Also  you will need to download list_attr_celeba.txt from the same link  which is found under Anno folder.  unzip img_align_celeba.zip; cd ..       bash       bash   Download MNIST as a luarocks package: luarocks install mnist       bash       bash   """;Computer Vision;https://github.com/Guim3/IcGAN
"""To launch the training on EuroSAT (rgb or MS)  it is necessary to download the corresponding datasets. The `root_dir` variable in the corresponding `datasets/eurosat_dataset.py` and `datasets/eurosat_rgb_dataset.py` files shall be adjusted according to the dataset path.     <!-- Content of Repo -->  1. Get [miniconda](https://docs.conda.io/en/latest/miniconda.html) or similar 2. Clone the repo    ```sh    git clone https://github.com/gomezzz/MSMatch.git    ``` 3. Setup the environment. This will create a conda environment called `torchmatch`    ```sh    conda env create -f environment.yml    ```   EfficientNet PyTorch   imageio  numpy  pandas   environment.yml: conda environment file describing dependencies.    Commit your Changes (git commit -m 'Add some AmazingFeature')   This is a brief example of setting up MSMatch.   """;Computer Vision;https://github.com/gomezzz/MSMatch
"""You will first need to launch a Virtual Machine (VM) on Google Cloud. Details about launching the VM can be found at the [Google Cloud Documentation](http://cloud/compute/docs/instances/create-start-instance).  In order to run training or eval on Cloud TPUs  you must set up the following variables based on your project  zone and GCS bucket appropriately. Please refer to the [Cloud TPU Quickstart](https://cloud.google.com/tpu/docs/quickstart) guide for more details.  ```sh export PROJECT=your_project_name export ZONE=your_project_zone export BUCKET=gs://yourbucket/ export TPU_NAME=t5-tpu export DATA_DIR=""${BUCKET}/your_data_dir"" export MODEL_DIR=""${BUCKET}/your_model_dir"" ```  Please use the following command to create a TPU device in the Cloud VM.  ```sh ctpu up --name=$TPU_NAME --project=$PROJECT --zone=$ZONE --tpu-size=v3-8  \         --tpu-only   --tf-version=1.15 --noconf ```    To install the T5 package  simply run:  ```sh pip install t5[gcp] ```   You may either use a new or pre-existing `Task`  or you may load examples from a preprocessed TSV file.   Depending on your data source (see above)  you will need to prepare your data appropriately.   Make sure your files are accessible to the TPU (i.e.  are in a GCS bucket)  and you should be good to go!   You can also use greedy_decode.gin or sample_decode.gin instead of beam_search.gin in the command above.   You can also use beam_search.gin or greedy_decode.gin instead of sample_decode.gin in the command above.   In order to train using XManager  you can use the run_training script. To pre-train on the C4 dataset  you need to specify the mixture name  gfs_user  and xm resource alloc group.   Here we provide example usage for how to pre-train  fine-tune  evaluate  and decode from a model with our codebase. You can use these instructions to reproduce our results  fine-tune one of our released checkpoints with your own data and/or hyperparameters  or pre-train a model from scratch.   We provide several [convinient scripts](https://github.com/google-research/text-to-text-transfer-transformer/tree/master/t5/google/scripts) to use our library internally at Google. We describe the usage of these scripts in the sections below.   """;Natural Language Processing;https://github.com/Nimesh-Patel/text-to-text-transfer-transformer
"""- Download the ImageNet dataset and put them into the `{repo_root}/data/imagenet`.   Or you can specify the checkpoint path by modifying test.sh   * Install [PyTorch](http://pytorch.org/) * Clone the repo:   ```   git clone https://github.com/switchablenorms/Sparse_SwitchNorm.git   ```   """;General;https://github.com/switchablenorms/Sparse_SwitchNorm
"""You have to download the pretrained models and put them in output/ckpt/LJSpeech/   output/ckpt/AISHELL3  or output/ckpt/LibriTTS/.   Alternately  you can align the corpus by yourself.    """;Natural Language Processing;https://github.com/ming024/FastSpeech2
"""To use the notebooks  the following directory structure is expected:   """;Computer Vision;https://github.com/saragarci/synthetic-art
"""The datasets are taken from the original MAF repository. Follow the instructions to get them.  Tests check invertibility  you can run them as   """;General;https://github.com/ikostrikov/pytorch-flows
"""pip install &lt;pytorch-latest.whl url&gt;   """;Computer Vision;https://github.com/gan3sh500/local-relational-nets
"""LibriTTS train-clean-360 split tar.gz link   Following the format from NVIDIA/tacotron2  the metadata should be formatted as:   """;Computer Vision;https://github.com/mindslab-ai/univnet
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/artynet/darknet-alexeyAB
"""Create 2 folders `train` and `validation` in the `data` folder (which was created already). Then `Please copy` your images with the corresponding names into these folders.  - `train` folder was used for the training process - `validation` folder was used for validating training result after each epoch  This library use `image_dataset_from_directory` API from `Tensorflow 2.0` to load images. Make sure you have some understanding of how it works via [its document](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory).  Structure of these folders.  ``` main_directory/ ...class_a/ ......a_image_1.jpg ......a_image_2.jpg ...class_b/ ......b_image_1.jpg ......b_image_2.jpg ```   1. Make sure you have installed Miniconda. If not yet  see the setup document [here](https://conda.io/en/latest/user-guide/install/index.html#regular-installation).  2. Clone this repository: `git clone https://github.com/bangoc123/vit` 3. `cd` into `vit` and install dependencies package: `pip install -r requirements.txt`   """;Computer Vision;https://github.com/protonx-engineering/vit
"""```python pip install keras-tcn ```  ---  | Topic                            | Github                                                       | Colab                                                        | | -------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | | MNIST Dataset                    | [MNIST Dataset](https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_MNIST.ipynb) | [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ashishpatel26/tcn-keras-Examples/blob/master/TCN_MNIST.ipynb) | | IMDB Dataset                     | [IMDA Dataset](https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_IMDB.ipynb) | [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ashishpatel26/tcn-keras-Examples/blob/master/TCN_IMDB.ipynb) | | Time Series Dataset Milk         | [Time Series Dataset Milk](https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_TimeSeries_Approach.ipynb) | [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ashishpatel26/tcn-keras-Examples/blob/master/TCN_TimeSeries_Approach.ipynb) | | Many to Many Regression Approach | [MtoM Regression](https://colab.research.google.com/github/ashishpatel26/tcn-keras-Examples/blob/master/TCN_Many_to_Many_Regression.ipynb) | [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ashishpatel26/tcn-keras-Examples/blob/master/TCN_Many_to_Many_Regression.ipynb) | | Self Generated Dataset Approach  | [Self Generated Dataset](https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_Self_generated_Data_Training.ipynb) | [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ashishpatel26/tcn-keras-Examples/blob/master/TCN_Self_generated_Data_Training.ipynb) | | Cifar10 Image Classification | [Cifar10 Image Classification](https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_cifar10.ipynb) | [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ashishpatel26/tcn-keras-Examples/blob/master/TCN_cifar10.ipynb) |   Article : https://arxiv.org/pdf/1803.01271.pdf  github : https://github.com/philipperemy/keras-tcn   ***Note : This All Notebook Contains Step by Step code of TCN in different domain Application.***   """;Audio;https://github.com/ashishpatel26/tcn-keras-Examples
"""install mxnet 2.0.0 from here https://dist.mxnet.io/python/all <br> Install all packages with pip ``` pip install -r requirements.txt ```   """;Computer Vision;https://github.com/eitan3/nerf_gluon
"""This repository is a fork of the Nathan Sprague implementation of the deep Q-learning algorithm described in:  [Playing Atari with Deep Reinforcement Learning](http://arxiv.org/abs/1312.5602) Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Alex Graves  Ioannis Antonoglou  Daan Wierstra  Martin Riedmiller  and   Mnih  Volodymyr  et al. ""Human-level control through deep reinforcement learning."" Nature 518.7540 (2015): 529-533.  We use the DQN algorithm to learn the strategies for Atari games using the RAM state of the machine.   ./frameskip.sh breakout just_ram 8   ./dropout.sh &lt;rom name&gt; ram_dropout   You can try the models with l2-regularization using:   ./weight-decay.sh breakout big_ram   ./learningrate.sh breakout big_ram   """;Reinforcement Learning;https://github.com/sygi/deep_q_rl
"""Get the requirements pip install -r requirements.txt       * To run on GPU  use the flag --cuda True  otherwise do not use this option.   """;General;https://github.com/JoeRoussy/adaptive-attention-in-cv
"""Presented here is a project involving an implementation of a neural network architecture that has the aim of detecting rooftops in a typical 'object detection' task. But why is it called Zeus? For two main reasons; firstly Zeus is the god of the sky and we are detecting rooftops from aerial drone photographs. Secondly  because Zeus was the first god to inhabit Olympus and birth the future gods. This project was my first ambitious AI-related project and I hope like Zeus it will spawn many others that follow in its footsteps. I'm not going to dwell too long on the background  theory  or exact nature of neural networks. Many articles and resources are available for this and my attempt at rehashing this history would not do it justice. Instead  I will focus on the problems I faced with this project and how I overcame them. I find the most useful resources online (especially while attempting this project) were those that focussed on the implementation rather than getting caught up in theory. I will walk through the project and code in chronological order which also closely follows the order of implementation of each component.  A brief explainer on the origins of this project: originally it was a code test sent by a company  but I felt that I needed to finish it. As I progressed through the project I continued to learn about how exactly to implement a neural network and obtain results from working code. This was very valuable to me  as while I have education in neural network theory  the practical nature of actual implementation I feel is something you can only get from experience. As for the tools I chose  Mask-RCNN is a commonly used and well understood network that performs well in object detection tasks. Therefore  I decided to use Mask-RCNN as it had pre-trained weights available and would have lots of documentation (for specific resources used  see references). Faster R-CNN was also considered  but Mask R-CNN also has the option of outputting masks for each object (i.e. performing object segmentation as well as detection).   """;Computer Vision;https://github.com/Alexander-Whelan/Zeus
"""The pickled data is a dictionary with 4 key/value pairs:  - `'features'` is a 4D array containing raw pixel data of the traffic sign images  (num examples  width  height  channels). - `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id. - `'sizes'` is a list containing tuples  (width  height) representing the original width and height the image. - `'coords'` is a list containing tuples  (x1  y1  x2  y2) representing coordinates of a bounding box around the sign in the image.  **First  we will use `numpy` provide the number of images in each subset  in addition to the image size  and the number of unique classes.** Number of training examples:  34799 Number of testing examples:  12630 Number of validation examples:  4410 Image data shape = (32  32  3) Number of classes = 43  **Then  we used `matplotlib` plot sample images from each subset.**  **And finally  we will use `numpy` to plot a histogram of the count of images in each unique class.** ---   Now  we'll use the testing set to measure the accuracy of the model over unknown examples. We've been able to reach a **Test accuracy of 96.06%**. A remarkable performance.  Now we'll plot the confusion matrix to see where the model actually fails.  We observe some clusters in the confusion matrix above. It turns out that the various speed limits are sometimes misclassified among themselves. Similarly  traffic signs with traingular shape are misclassified among themselves. We can further improve on the model using hierarchical CNNs to first identify broader groups (like speed signs) and then have CNNs to classify finer features (such as the actual speed limit).  ---   Python 3.6.2  TensorFlow 0.12.1 (GPU support)   """;General;https://github.com/rahulsonone1234/Traffic-Sign-Recognition
"""Finetune can be installed directly from PyPI by using `pip`  ``` pip3 install finetune ```  or installed directly from source:  ```bash git clone -b master https://github.com/IndicoDataSolutions/finetune && cd finetune python3 setup.py develop              #: symlinks the git directory to your python path pip3 install tensorflow-gpu --upgrade #: or tensorflow-cpu python3 -m spacy download en          #: download spacy tokenizer ```  In order to run `finetune` on your host  you'll need a working copy of tensorflow-gpu >= 1.14.0 and up to date nvidia-driver versions.  You can optionally run the provided test suite to ensure installation completed successfully.  ```bash pip3 install pytest pytest ```    models = (Classifier  MultiLabelClassifier  MultiFieldClassifier  MultipleChoice  #: Classify one or more inputs into one or more classes            Regressor  OrdinalRegressor  MultifieldRegressor                        #: Regress on one or more inputs             DeploymentModel                                                         #: Wrapper to optimize your serialized models for a production environment   git clone https://github.com/IndicoDataSolutions/finetune && cd finetune   """;Natural Language Processing;https://github.com/IndicoDataSolutions/finetune
"""``` pip3 install Cpython cd data ./get_coco_2017.sh ```   [x] Build dataloader   """;Computer Vision;https://github.com/quocdat32461997/Mask_RCNN
"""```bash pip install -r requirements.txt ```  If you want to use GPU  follow [these instructions](https://www.tensorflow.org/install/) to install with pip3.  Make sure Keras is using Tensorflow and you have Python 3.6.3+. Depending on your environment  you may have to run python3/pip3 instead of python/pip.   Basic Usage ------------  For training model  execute `Self-Play`  `Trainer` and `Evaluator`.  **Note**: Make sure you are running the scripts from the top-level directory of this repo  i.e. `python src/chess_zero/run.py opt`  not `python run.py opt`.   Self-Play --------  ```bash python src/chess_zero/run.py self ```  When executed  Self-Play will start using BestModel. If the BestModel does not exist  new random model will be created and become BestModel.   GPU Memory   """;Reinforcement Learning;https://github.com/svikramank/chess-deepRL
"""  - Please refer [Code for Data Generation](https://github.com/twtygqyy/pytorch-SRResNet/tree/master/data) for creating training files.   - Data augmentations including flipping  rotation  downsizing are adopted.      --cuda                Use cuda?     --cuda             use cuda?   ``` usage: demo.py [-h] [--cuda] [--model MODEL] [--image IMAGE]                [--dataset DATASET] [--scale SCALE] [--gpus GPUS]  optional arguments:   -h  --help         show this help message and exit   --cuda             use cuda?   --model MODEL      model path   --image IMAGE      image name   --dataset DATASET  dataset name   --scale SCALE      scale factor  Default: 4   --gpus GPUS        gpu ids (default: 0) ``` We convert Set5 test set images to mat format using Matlab  for simple image reading An example of usage is shown as follows: ``` python demo.py --model model/model_srresnet.pth --dataset Set5 --image butterfly_GT --scale 4 --cuda ```   """;General;https://github.com/V0LsTeR/mySRResNet
"""Scripts are implemented on python3 and tensorflow r1.11 or r1.12. The core scripts are referred to the following repository.   """;Sequential;https://github.com/jiankaiwang/dnc-py3
"""This repository contains the author's implementation of ECCV 2018 paper ""License Plate Detection and Recognition in Unconstrained Scenarios"".  * Paper webpage: http://sergiomsilva.com/pubs/alpr-unconstrained/  If you use results produced by our code in any publication  please cite our paper:  ``` @INPROCEEDINGS{silva2018a    author={S. M. Silva and C. R. Jung}     booktitle={2018 European Conference on Computer Vision (ECCV)}     title={License Plate Detection and Recognition in Unconstrained Scenarios}     year={2018}     pages={580-596}     doi={10.1007/978-3-030-01258-8_36}     month={Sep} } ```   The following command can be used to train the network from scratch considering the data inside the train-detector folder:   $ mkdir models   """;Computer Vision;https://github.com/sergiomsilva/alpr-unconstrained
"""effcientNet backbones and pretrained weights from @rwightman(https://github.com/rwightman/gen-efficientnet-pytorch)   """;Computer Vision;https://github.com/SweetyTian/efficientdet
"""MMSegmentation is an open source semantic segmentation toolbox based on PyTorch. It is a part of the OpenMMLab project.  The master branch works with **PyTorch 1.5+**.  ![demo image](resources/seg_demo.gif)   Please refer to [get_started.md](docs/en/get_started.md#installation) for installation and [dataset_prepare.md](docs/en/dataset_prepare.md#prepare-datasets) for dataset preparation.   [x] COCO-Stuff 10k  [x] COCO-Stuff 164k   Please see [train.md](docs/en/train.md) and [inference.md](docs/en/inference.md) for the basic usage of MMSegmentation. There are also tutorials for [customizing dataset](docs/en/tutorials/customize_datasets.md)  [designing data pipeline](docs/en/tutorials/data_pipeline.md)  [customizing modules](docs/en/tutorials/customize_models.md)  and [customizing runtime](docs/en/tutorials/customize_runtime.md). We also provide many [training tricks](docs/en/tutorials/training_tricks.md) for better training and [useful tools](docs/en/useful_tools.md) for deployment.  A Colab tutorial is also provided. You may preview the notebook [here](demo/MMSegmentation_Tutorial.ipynb) or directly [run](https://colab.research.google.com/github/open-mmlab/mmsegmentation/blob/master/demo/MMSegmentation_Tutorial.ipynb) on Colab.   """;General;https://github.com/open-mmlab/mmsegmentation
"""PyTorch implementation https://github.com/ducha-aiki/LSUV-pytorch   """;General;https://github.com/x5675602/LSUVinit-keras
"""Manual: https://github.com/AlexeyAB/darknet/wiki   Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/  Pytorch - Scaled-YOLOv4: https://github.com/WongKinYiu/ScaledYOLOv4  TensorFlow: pip install yolov4 YOLOv4 on TensorFlow 2.0 / TFlite / Andriod: https://github.com/hunglc007/tensorflow-yolov4-tflite   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example   PyTorch > ONNX:    TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   Deepstream 5.0 / TensorRT for YOLOv4 https://github.com/NVIDIA-AI-IOT/yolov4_deepstream   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like CUDA  cudnn  ZED and build against those. It will also create a shared object library file to use darknet for code development.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   You also need to specify for which graphics card the code is generated. This is done by setting ARCH=. If you use a never version than CUDA 11 you further need to edit line 20 from Makefile and remove -gencode arch=compute_30 code=sm_30 \ as Kepler GPU support was dropped in CUDA 11. You can also drop the general ARCH= and just uncomment ARCH= for your graphics card.   This is the recommended approach to build Darknet on Windows.  Install Visual Studio 2017 or 2019. In case you need to download it  please go here: Visual Studio Community  Install CUDA (at least v10.0) enabling VS Integration during installation.   PS Code\&gt;              git clone https://github.com/microsoft/vcpkg  PS Code\&gt;              cd vcpkg   PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base cuda cudnn]:x64-windows for a quicker install of dependencies  PS Code\vcpkg&gt;         cd ..  PS Code\&gt;              git clone https://github.com/AlexeyAB/darknet  PS Code\&gt;              cd darknet   Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   in Python: https://github.com/tzutalin/labelImg  in Python: https://github.com/Cartucho/OpenLabeling   in JavaScript: https://github.com/opencv/cvat   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux   * using `build.sh` or   * build `darknet` using `cmake` or   * set `LIBSO=1` in the `Makefile` and do `make` * on Windows   * using `build.ps1` or   * build `darknet` using `cmake` or   * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs:  * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h   * Python examples using the C API:     * https://github.com/AlexeyAB/darknet/blob/master/darknet.py     * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py  * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp   * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp  ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`      * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)  `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42)  ```cpp struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false);         std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/iamsingh/darknet
"""[This page](https://bazel.build/) describes how to install the Bazel build and test tool on your machine.   ```shell pip install six pip install absl-py pip install inflection pip install wrapt pip install numpy pip install dm-sonnet pip install tensorflow-gpu pip install tensorflow-probability-gpu pip install pygame ```   ```shell wget https://github.com/opencv/opencv/archive/2.4.13.6.zip unzip 2.4.13.6.zip cd opencv-2.4.13.6 mkdir build cd build cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local .. make -j7 sudo make install sudo ldconfig cd ../.. ```   ```shell git clone https://github.com/google/clif.git cd clif ./INSTALL.sh cd .. ```   For detailed information see: https://github.com/protocolbuffers/protobuf/blob/master/src/README.md  ```shell git clone https://github.com/protocolbuffers/protobuf.git cd protobuf git submodule update --init --recursive ./autogen.sh ./configure make -j7 sudo make install sudo ldconfig cd python python setup.py build sudo python setup.py install cd ../.. ```   ```shell sudo apt-get install autoconf automake libtool curl make g++ unzip virtualenv python-virtualenv cmake subversion pkg-config libpython-dev libcairo2-dev libboost-all-dev python-pip libssl-dev pip install setuptools pip install pyparsing ```   navigation agents in the StreetLearn environment by using a TensorFlow   streetlearn/python/environment A Python-based interface for calling the       Python using pygame  that instantiates the StreetLearn environment on the   build has only been tested running on Ubuntu 18.04.   git clone https://github.com/deepmind/streetlearn.git  cd streetlearn   bazel build streetlearn:streetlearn_engine_py   bazel build streetlearn/python/human_agent:all   The Python StreetLearn environment follows the specifications from   start_pano: The pano ID string to start from. The graph will be build       requested from the environment: ['view_image'  'graph_image'  'yaw'    The following games are available in the StreetLearn environment:   """;General;https://github.com/windstrip/DeepMind-StreetLearn
"""We built our approach on [FCOS](https://arxiv.org/abs/1904.01355)  A simple and strong anchor-free object detector  with [ResNeSt](https://arxiv.org/abs/2004.08955) as our backbone  to detect embedded and isolated formulas.  We employed [ATSS](https://arxiv.org/abs/1912.02424) as our sampling strategy instead of random sampling to eliminate the effects of sample imbalance. Moreover  we observed and revealed the influence of different FPN levels on the detection result.  [Generalized Focal Loss](https://arxiv.org/abs/2006.04388) is adopted to our loss. Finally  with a series of useful tricks and model ensembles  our method was ranked 1st in the MFD task.  ![Random Sampling(left) ATSS(right)](https://github.com/Yuxiang1995/ICDAR2021_MFD/blob/main/resources/sampling_strategy.png) **Random Sampling(left) ATSS(right)**    Firstly  Firstly  you need to put the image files and the GT files into two separate folders as below.  ```shell Tr01 ├── gt │   ├── 0001125-color_page02.txt │   ├── 0001125-color_page05.txt │   ├── ... │   └── 0304067-color_page08.txt ├── img     ├── 0001125-page02.jpg     ├── 0001125-page05.jpg     ├── ...     └── 0304067-page08.jpg ```  Secondly  run [data_preprocess.py](https://github.com/Yuxiang1995/ICDAR2021_MFD/blob/main/tools/data_preprocess.py) to get coco format label.  Remember to change **'img_path'**  **'txt_path'**  **'dst_path'** and **'train_path'** to your own path.    ```shell python ./tools/data_preprocess.py ```  The new structure of data folder will become  ```shell Tr01 ├── gt │   ├── 0001125-color_page02.txt │   ├── 0001125-color_page05.txt │   ├── ... │   └── 0304067-color_page08.txt │ ├── gt_icdar │   ├── 0001125-color_page02.txt │   ├── 0001125-color_page05.txt │   ├── ... │   └── 0304067-color_page08.txt │    ├── img │   ├── 0001125-page02.jpg │   ├── 0001125-page05.jpg │   ├── ... │   └── 0304067-page08.jpg │ └── train_coco.json ```  Finally  change **'data_root'** in ./configs/_base_/datasets/formula_detection.py to your path.   1. Install PyTorch and torchvision following the [official instructions ](https://pytorch.org/)  e.g.       ```shell     pip install pytorch torchvision -c pytorch     ```      Note: Make sure that your compilation CUDA version and runtime CUDA version match.     You can check the supported CUDA version for precompiled packages on the [PyTorch website](https://pytorch.org/).      `E.g.1` If you have CUDA 10.1 installed under `/usr/local/cuda` and would like to install     PyTorch 1.5  you need to install the prebuilt PyTorch with CUDA 10.1.      ```shell     pip install pytorch cudatoolkit=10.1 torchvision -c pytorch     ```      `E.g. 2` If you have CUDA 9.2 installed under `/usr/local/cuda` and would like to install     PyTorch 1.3.1.  you need to install the prebuilt PyTorch with CUDA 9.2.      ```shell     pip install pytorch=1.3.1 cudatoolkit=9.2 torchvision=0.4.2 -c pytorch     ```      If you build PyTorch from source instead of installing the prebuilt pacakge      you can use more CUDA versions such as 9.0.     2. Install mmcv-full  we recommend you to install the pre-build package as below.      ```shell     pip install mmcv-full==latest+torch1.6.0+cu101 -f https://download.openmmlab.com/mmcv/dist/index.html     ```      See [here](https://github.com/open-mmlab/mmcv#install-with-pip) for different versions of MMCV compatible to different PyTorch and CUDA versions.     Optionally you can choose to compile mmcv from source by the following command      ```shell     git clone https://github.com/open-mmlab/mmcv.git     cd mmcv     MMCV_WITH_OPS=1 pip install -e .  #: package mmcv-full will be installed after this step     cd ..     ```      Or directly run      ```shell     pip install mmcv-full     ```     3. Install build requirements and then compile MMDetection.      ```shell     pip install -r requirements.txt     pip install tensorboard     pip install ensemble-boxes     pip install -v -e .  #: or ""python setup.py develop""     ```   train with single gpu on ResNeSt50   """;General;https://github.com/Yuxiang1995/ICDAR2021_MFD
"""We recommend that you use a virtual environment such as [conda (recommended)](https://conda.io/docs/user-guide/getting-started.html)  [virtualenv](https://virtualenv.pypa.io/en/stable/)  or [Pipenv](https://pipenv.readthedocs.io/en/latest/)  You can install the driver by cloning and pip-installing: ```bash     git clone https://github.com/BlueRiverTech/quanser-openai-driver.git     cd quanser-openai-driver     pip3 install -e . ```  Once you have that setup: Run the classical control baseline (ensure the Qube is connected to your computer)<br> ```bash python tests/test.py --env QubeSwingupEnv --controller flip ```    We have tested on Ubuntu 16.04 LTS and Ubuntu 18.04 LTS using Python 2.7 and Python 3.6.5<br>    Usage is very similar to most OpenAI gym environments but **requires** that you close the environment when finished. Without safely closing the Env  bad things may happen. Usually you will not be able to reopen the board.  This can be done with a context manager using a `with` statement ```python import gym from gym_brt import QubeSwingupEnv  num_episodes = 10 num_steps = 250  with QubeSwingupEnv() as env:     for episode in range(num_episodes):         state = env.reset()         for step in range(num_steps):             action = env.action_space.sample()             state  reward  done  _ = env.step(action) ```  Or can be closed manually by using `env.close()`. You can see an [example here](docs/alternatives.md#usage).    """;Reinforcement Learning;https://github.com/BlueRiverTech/quanser-openai-driver
"""your nodes if you want to target them by name in the restored version (when you   If you want to go deeper in the math  the one piece you are missing is the explanation   ``` > python3 rnn_train.py ``` The script **rnn_train.py** trains a language model on the complete works of William Shakespeare. You can also train on Tensorflow Python code. See comments in the file.  The file **rnn_train_stateistuple.py** implements the same model using  the state_is_tuple=True option in tf.nn.rnn_cell.MultiRNNCell (default). Training is supposedly faster (by ~10%) but handling the state as a tuple is a bit more cumbersome.  ``` > tensorboard --logdir=log ``` The training script **rnn_train.py** is set up to save training and validation data as ""Tensorboard summaries"" in the ""log"" folder. They can be visualised with Tensorboard. In the screenshot below  you can see the RNN being trained on 6 epochs of Shakespeare. The training and validation curves stay close together which means that overfitting is not a major issue here.  You can try to add some dropout (pkeep=0.8 for example) but it will not improve the situation much becasue it is already quite good.   ![Image](https://martin-gorner.github.io/tensorflow-rnn-shakespeare/tensorboard_screenshot.png) ``` > python3 rnn_play.py ```      The script **rnn_play.py** uses a trained checkpoint to generate a new ""Shakespeare"" play.   You can also generate new ""Tensorflow Python"" code. See comments in the file.  Checkpoint files can be downloaded from here:       [Fully trained](https://drive.google.com/file/d/0B5njS_LX6IsDc2lWTmtyanRpOHc/view?usp=sharing) on Shakespeare or Tensorflow Python source.        [Partially trained](https://drive.google.com/file/d/0B5njS_LX6IsDUlFsMkdhclNSazA/view?usp=sharing) to see how they make progress in training.  ``` > python3 -m unittest tests.py ``` Unit tests can be run with the command above.    ```          TITUS ANDRONICUS   ACT I   SCENE III	An ante-chamber. The COUNT's palace.   [Enter CLEOMENES  with the Lord SAY]   Chamberlain     Let me see your worshing in my hands.   LUCETTA     I am a sign of me  and sorrow sounds it.   [Enter CAPULET and LADY MACBETH]   What manner of mine is mad  and soon arise?   JULIA     What shall by these things were a secret fool      That still shall see me with the best and force?   Second Watchman     Ay  but we see them not at home: the strong and fair of thee      The seasons are as safe as the time will be a soul      That works out of this fearful sore of feather     To tell her with a storm of something storms     That have some men of man is now the subject.     What says the story  well say we have said to thee      That shall she not  though that the way of hearts      We have seen his service that we may be sad.   [Retains his house] ADRIANA What says my lord the Duke of Burgons of Tyre?   DOMITIUS ENOBARBUS     But  sir  you shall have such a sweet air from the state      There is not so much as you see the store      As if the base should be so foul as you.   DOMITIUS ENOY     If I do now  if you were not to seek to say      That you may be a soldier's father for the field.   [Exit]  ```  """;General;https://github.com/martin-gorner/tensorflow-rnn-shakespeare
"""  2. Will release a detailed version later.   cd eva/eval_quality and run eval_quality.sh (e.g.  ./eval_quality.sh  lfw).   install requirements.   Modify parameters in run.sh/run_dist.sh/run_dist_cos.sh and run it.  Note: Use Pytorch > 1.7 for this feature. Codes are mainly based on torchshard from Kaiyu Yue.   [Optional. Help needed as NAN can be reached during training.] Enable fp16 training by setiing --fp16 1 in run/run_dist.sh.  run run/run_dist.sh.   Pytorch: FaceX-Zoo from JDAI.  Pytorch: pt-femb-face-embeddings from Jonas Grebe   20210513: add instructions for finetuning with MagFace   """;General;https://github.com/IrvingMeng/MagFace
"""You can train your own RL model using:   """;Reinforcement Learning;https://github.com/komejisatori/ReinforcementCar
"""Implementation: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix   """;General;https://github.com/cvp19g2/cvp19g2
"""```python from coordconv import CoordConv1d  CoordConv2d  CoordConv3d  class Net(nn.Module):     def __init__(self):         super(Net  self).__init__()         self.coordconv = CoordConv2d(2  32  1  with_r=True)         self.conv1 = nn.Conv2d(32  64  1)         self.conv2 = nn.Conv2d(64  64  1)         self.conv3 = nn.Conv2d(64   1  1)         self.conv4 = nn.Conv2d( 1   1  1)      def forward(self  x):         x = self.coordconv(x)         x = F.relu(self.conv1(x))         x = F.relu(self.conv2(x))         x = F.relu(self.conv3(x))         x = self.conv4(x)         x = x.view(-1  64*64)         return x  device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") net = Net().to(device) ```   """;Computer Vision;https://github.com/walsvid/CoordConv
"""Machine translation is a natural language processing task that aims to translate natural languages using computers automatically. Recent several years have witnessed the rapid development of end-to-end neural machine translation  which has become the new mainstream method in practical MT systems.  THUMT is an open-source toolkit for neural machine translation developed by [the Natural Language Processing Group at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/site2/index.php?lang=en).    """;Natural Language Processing;https://github.com/insigh/THUMT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/halo090770/bert
"""Download weights   Download train dataset   """;Computer Vision;https://github.com/Lornatang/SRCNN-PyTorch
"""To run this app you will need the following installed:     - PyTorch    - EfficientNet-Pytorch (instructions on how to install here https://github.com/lukemelas/EfficientNet-PyTorch)  To use this app use the following bash commmands:     - git clone https://github.com/vladthesav/MoldAI.git     - cd MoldAI   """;Computer Vision;https://github.com/vladthesav/MoldAI
"""``` #: Train and test CIFAR 10 with mixup. python main_cifar10.py --mixup --exp='cifar10_nomixup' #: Train and test CIFAR 10 without mixup. python main_cifar10.py --exp='cifar10_nomixup' #: Train and test CIFAR 100 with mixup. python main_cifar100.py --mixup --exp='cifar100_mixup' #: Train and test CIFAR 100 without mixup. python main_cifar100.py --exp='cifar100_nomixup' ```  """;Computer Vision;https://github.com/leehomyc/mixup_pytorch
"""This is a prebuild darknet on Ubuntu 18.04 with OpenCV v3.2.0  CUDA V10.1.105 and CuDNN 7   Yolo v4 source code: https://github.com/AlexeyAB/darknet  Useful links: https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&sk=6039748846bbcf1d960c3061542591d7   """;Computer Vision;https://github.com/gihants/darknet_built
"""GPU 이용 가능하게 만들기   conda error 해결   """;General;https://github.com/drumpt/Transformer
"""GPUs: K80 ($0.14/hr)  T4 ($0.11/hr)  V100 ($0.74/hr) CUDA with Nvidia Apex FP16/32     Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [Notebook](https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb) <a href=""https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a> * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) << highly recommended * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart)  ![Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/yolov3?logo=docker) * [A TensorRT Implementation of YOLOv3 and YOLOv4](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp)     """;Computer Vision;https://github.com/zjjszj/yolov3
"""Usually it is straightforward to use the provided models on other datasets  but some cases require manual setup.   | Name      |   """;Computer Vision;https://github.com/akamaster/pytorch_resnet_cifar10
"""Instructions for acquiring PTB and WT2 can be found here. While CIFAR-10 can be automatically downloaded by torchvision  ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions here.   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/victoriaqiu/10707finalproject
"""Any pre-algorithm checks you perform on your DICOM   """;Computer Vision;https://github.com/Raghuvar/pneumonia_detection_using_X-rays
"""Speedy compiler for jupyter:   """;Computer Vision;https://github.com/Zhz1997/Singing-voice-speration-with-U-Net
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/DeligientSloth/QQsim
"""Given a mixed source signal  the task of source separation algorithm is to divide the signal into its original components. We test our method on music separation and specifically on the [MUSDB18 dataset](https://zenodo.org/record/1117372#.XiSY9Bco9QJ) where the sources consist of contemporary songs and the goal is to divide them into four stems:      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:drum::shark:&nbsp;&nbsp; **drums**      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:studio_microphone::rabbit2:&nbsp;&nbsp; **vocals**      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:guitar::eagle:&nbsp;&nbsp; **bass**      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:saxophone::snake:&nbsp;&nbsp; **other accompaniments**     Music source separation can not only be used as a preprocessing step to other MIR problems (like sound source identification)  but it can also be used more creatively: we can create backing tracks to any song for musical practice or just for fun (karaoke)  we can create ""smart"" equilizers that are able to make a new remix  or we can separate a single instrument to better study its intricacies (guitar players can more easily determine the exact chords for example).   <br>  <p align=""center"">   <img src=""img/spectrogram.png"" alt=""Spectrogram illustration."" width=""600""/>   </p>  <p align=""center"">   <sub><em>Illustration of a separated audio signal (projected on log-scaled spectrograms). The top spectrogram shows the mixed audio that is transformed into the four separated components at the bottom. Note that we use the spectrograms just to illustrate the task — our model operates directly on the audio waveforms.</em></sub> </p>  <br>   You can try an interactive demo of the pretrained model in [**Google Colab notebook**](https://colab.research.google.com/drive/1iVFGlRuhdpjtnO3a7ATzgd-lnPxtd4oT).  <br>   """;Audio;https://github.com/pfnet-research/meta-tasnet
"""SSD(Single Shot MultiBox Detector) is a state-of-art object detection algorithm  brought by Wei Liu and other wonderful guys  see [SSD: Single Shot MultiBox Detector @ arxiv](https://arxiv.org/abs/1512.02325)  recommended to read for better understanding.  Also  SSD currently performs good at PASCAL VOC Challenge  see [http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=3](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=3)   The code structures looks like below:  ``` 1. Aseets - Prior boxes  (prior_boxes_ssd300.pkl is the model pre-defined static prior boxes) 2. Data-set - VOC-2007 3. SSD Model - The training and the test scripts     - ssd_v2.py #: main model architecture using Keras 	- ssd_layers.py #: Normalize and PriorBox defenition 	- ssd_training.py #: MultiboxLoss Definition 	- ssd_utils.py #: Utilities including encode decode assign_boxes    4.  data-generator  #: customrized generator  which return proper training data structure 				            #: including image and assigned boxes(similar to input boxex)   - get_data_from_XML.py #: parse Annotations of PASCAL VOC  helper of generator      ```  """;Computer Vision;https://github.com/nirajdevpandey/Object-detection-and-localization-using-SSD-
"""代码（jupyter notebook与其导出的html文件）   """;Computer Vision;https://github.com/udacity/MLND-CN-Capstone-TGSImage
"""Install PyTorch 0.4   The codebase is now PyTorch 0.4 compatible for most use cases (a big shoutout to https://github.com/shawntan for a fairly comprehensive PR https://github.com/salesforce/awd-lstm-lm/pull/43). Mild readjustments to hyperparameters may be necessary to obtain quoted performance. If you desire exact reproducibility (or wish to run on PyTorch 0.3 or lower)  we suggest using an older commit of this repository. We are still working on pointer  finetune and generate functionalities.   For data setup  run ./getdata.sh.   PyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed.   """;General;https://github.com/philippwirth/awd-lstm-test
"""We are hosted in [PyPI](https://pypi.org/)  you can install the library using pip:  ```bash pip install biotorch ```  Or from source:  ```bash git clone https://github.com/jsalbert/biotorch.git cd biotorch script/setup ```   | Sign Symmetry | ['usf'  'brsf'  'frsf']  | [PyTorch]|   ```python from torchvision.models import alexnet from biotorch.module.biomodule import BioModule  model = BioModule(module=alexnet()  mode='frsf') ```   """;General;https://github.com/jsalbert/biotorch
"""Download the train/test splits here        Note: in_dir must be the path to the 2019 folder.    """;Computer Vision;https://github.com/bshall/VectorQuantizedCPC
"""Gradient accumulation: https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255   """;General;https://github.com/voqtuyen/GAN-Intuition
"""Any message you will get after you run the bot having the word ""show news"" in it will be treated as a request for latest headlines. The bot then fetches the latest news.   """;Computer Vision;https://github.com/skaravind/Whatsapp-Radiologist
"""For install and data preparation  please refer to the guidelines in [MMSegmentation v0.13.0](https://github.com/open-mmlab/mmsegmentation/tree/v0.13.0).  Other requirements: ```pip install timm==0.3.2```  An example (works for me): ```CUDA 10.1``` and  ```pytorch 1.7.1```   ``` pip install torchvision==0.8.2 pip install timm==0.3.2 pip install mmcv-full==1.2.7 pip install opencv-python==4.5.1.48 cd SegFormer && pip install -e . --user ```   ./tools/dist_test.sh local_configs/segformer/B1/segformer.b1.512x512.ade.160k.py /path/to/checkpoint_file <GPU_NUM>   tools/dist_test.sh local_configs/segformer/B1/segformer.b1.512x512.ade.160k.py /path/to/checkpoint_file <GPU_NUM> --aug-test   Example: train SegFormer-B1 on ADE20K:   SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers.<br> [Enze Xie](https://xieenze.github.io/)  [Wenhai Wang](https://whai362.github.io/)  [Zhiding Yu](https://chrisding.github.io/)  [Anima Anandkumar](http://tensorlab.cms.caltech.edu/users/anima/)  [Jose M. Alvarez](https://rsu.data61.csiro.au/people/jalvarez/)  and [Ping Luo](http://luoping.me/).<br> Technical Report 2021.  This repository contains the official Pytorch implementation of training & evaluation code and the pretrained models for [SegFormer](https://arxiv.org/abs/2105.15203).  SegFormer is a simple  efficient and powerful semantic segmentation method  as shown in Figure 1.  We use [MMSegmentation v0.13.0](https://github.com/open-mmlab/mmsegmentation/tree/v0.13.0) as the codebase.  🔥🔥 SegFormer is on [MMSegmentation](https://github.com/open-mmlab/mmsegmentation/tree/master/configs/segformer). 🔥🔥     """;Computer Vision;https://github.com/kikacaty/RAP_Benchmark
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/ghadahamed/darknet
"""If you run into any trouble with the setup/code or have any questions please contact me at st.dereka@gmail.com.   Clone the repo:  git clone https://github.com/stdereka/liverpool-ion-switching.git  cd liverpool-ion-switching   within a reasonable margin. Run following command:      more than one day with hardware setup I described above. Run:   """;Sequential;https://github.com/stdereka/liverpool-ion-switching
"""Here we introduce RainNet -- a convolutional neural network for radar-based precipitation nowcasting. RainNet was trained to predict continuous precipitation intensities at a lead time of five minutes  using several years of quality-controlled weather radar composites provided by the German Weather Service (DWD).   The source code of the RainNet model written using [_Keras_](https://keras.io) functional API is in the file `rainnet.py`.  The pretrained instance of `keras` `Model` for RainNet  as well as RainNet's pretrained weights are available on Zenodo:   [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3630429.svg)](https://doi.org/10.5281/zenodo.3630429)   **Prerequisites**:  * Python 3.6+   * Keras 2.2+   * h5py 2.8 * pretrained RainNet model (file `rainnet.h5`) and its weights (file `rainnet_weights.h5`) has to be downloaded from the corresponding [Zenodo repository](https://doi.org/10.5281/zenodo.3630429): [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3630429.svg)](https://doi.org/10.5281/zenodo.3630429)  There are two scenarios of how to use the pretrained RainNet model.  The first scenario allows you to load the RainNet's model architecture alongside pretrained weights using built-in `keras` `load_model` function:  ```python3 from keras.models import load_model  model = load_model(""rainnet.h5"") ```  The second scenario allows you to build the RainNet model from scratch using `rainnet` function from the module `rainnet.py` and then load pretrained weights stored in the file `rainnet_weights.h5`:  ```python3 from rainnet import rainnet  model = rainnet()  model.load_weights(""rainnet_weights.h5"") ```  Having the RainNet model in place  you can then use built-in `keras` [`Model class API`](https://keras.io/models/model/)  e.g.  for running the RainNet model or evaluating its skill.  Some available options are: * `model.predict(X)` -- runs the model with specified input data _X_ to obtain the corresponding nowcast. * `model.evaluate(X  y)` -- returns the loss value for the model in test mode. Here X stands for input data  and y specifies ground truth data.  * `model.summary()` -- creates a brief summary about model's layers and the number of parameters. * `model.get_layer(name  index)` -- retrieves a layer based on either its name or index.   In case you only want to train  RainNet from scratch (on RY or some different radar data archive)  you can avoid loading the provided pretrained weights.  There is also an option to specify the input data shape using `input_shape` argument (just have in mind that the spatial extent of input data has to be a multiple of 2<sup>n+1</sup>  where _n_ is the number of max pooling layers (4 for the vanilla RainNet)). Then  use the code as follows:  ```python3 from rainnet import rainnet  #: you can pass the specific input shape of (x y z)  #: where x and y provide the spatial extent   #: and z specifies how many previous radar scans #: you want to account for.   model = rainnet(input_shape=(x y z)) ```  You can find more examples of RainNet usage in the corresponding `examples` subfolder  which is provided as a part of this repository.   """;General;https://github.com/hydrogo/rainnet
"""To run the official vision app natively (i.e. without Docker)  first install Python prerequisites such as OpenCV  TensorFlow and COCO Python API: ```bash $ ck detect soft --tags=compiler python --full_path=`which python3` $ ck install package --tags=lib tensorflow v1.14 vcpu vprebuilt $ ck install package --tags=lib python-package cv2 $ ck install package --tags=tool coco api ```  Then  install the latest LoadGen package: ```bash $ ck install package --tags=mlperf inference source upstream.master $ ck install package --tags=lib python-package absl $ ck install package --tags=lib python-package mlperf loadgen ```  **NB:** The most important thing during installation is to select the same version of Python 3 (if you have more than one registered with CK). Check that each package ""needs"" exactly the same version of Python 3 after installation: ```bash $ ck show env --tags=lib tensorflow v1.14 vcpu vprebuilt Env UID:         Target OS: Bits: Name:                              Version: Tags: 087035468886d589   linux-64    64 TensorFlow library (prebuilt  cpu) 1.14.0   64bits channel-stable host-os-linux-64 lib needs-python needs-python-3.6.7 target-os-linux-64 tensorflow tensorflow-cpu tf tf-cpu v1 v1.14 v1.14.0 vcpu vprebuilt  $ ck show env --tags=lib python-package cv2 Env UID:         Target OS: Bits: Name:                 Version: Tags: 5f31d16b444d6b8c   linux-64    64 Python OpenCV library 3.6.7    64bits cv2 host-os-linux-64 lib needs-python needs-python-3.6.7 opencv python-package target-os-linux-64 v3 v3.6 v3.6.7  $ ck show env --tags=tool coco api Env UID:         Target OS: Bits: Name:            Version: Tags: 885a8f71bf1219da   linux-64    64 COCO dataset API master   64bits api coco compiled-by-gcc compiled-by-gcc-8.3.0 host-os-linux-64 needs-python needs-python-3.6.7 target-os-linux-64 tool v0 vmaster vtrunk  $ ck show env --tags=lib python-package mlperf loadgen Env UID:         Target OS: Bits: Name:                            Version: Tags: 462592cb2beeaf63   linux-64    64 MLPerf Inference LoadGen library master   64bits host-os-linux-64 lib loadgen mlperf mlperf-loadgen mlperf_loadgen needs-python needs-python-3.6.7 python-package target-os-linux-64 v0 vmaster ```  <a name=""modify_run_local""></a>  **NB:** It is currently necessary to create symbolic links if a model's file name is different from the one hardcoded in the application for each profile. For example  for the `tf-mobilenet` profile (which can be used both for the non-quantized and quantized MobileNet TF models)  the application specifies `mobilenet_v1_1.0_224_frozen.pb`   but the quantized model's file is `mobilenet_v1_1.0_224_quant_frozen.pb`.  <a name=""resnet""></a>  <a name=""imagenet""></a>  ```bash $ python -m pip install ck --user $ ck version V1.11.1 ```   Linux/MacOS:    Modify run_local.sh   <a name=""installation""></a>   <a name=""inference_0_5""></a>   <a name=""unofficial""></a>   <a name=""official_docker""></a>   $ ck install package --tags=image-classification dataset imagenet aux   <a name=""coco""></a>   <a name=""models""></a>   $ ./run_and_time.sh tf resnet cpu   <a name=""mobilenet""></a>   $ ./run_and_time.sh tf mobilenet cpu   <a name=""mobilenet_quant""></a>   $ ./run_and_time.sh tf mobilenet cpu   <a name=""ssd_mobilenet""></a>   $ ./run_and_time.sh tf ssd-mobilenet cpu   <a name=""ssd_mobilenet_quant""></a>   $ ./run_and_time.sh tf ssd-mobilenet cpu   <a name=""ssd_resnet""></a>   <a name=""official_native""></a>  <a name=""prereqs""></a>  Modify the run_local.sh script under v0.5/classification_and_detection as follows:  $ git diff  diff --git a/v0.5/classification_and_detection/run_local.sh b/v0.5/classification_and_detection/run_local.sh   --- a/v0.5/classification_and_detection/run_local.sh  +++ b/v0.5/classification_and_detection/run_local.sh        mkdir -p $OUTPUT_DIR   +ck virtual env --tag_groups=""lib tensorflow-cpu v1.14 vcpu vprebuilt lib python-package cv2 tool coco lib python-package mlperf loadgen"" \   See [above](#official_docker) for how to specify [datasets](#datasets) and [models](#models).   ```bash $ ck install package --tags=mlperf image-classification model tf mobilenet non-quantized $ export MODEL_DIR=`ck locate env --tags=model tf mobilenet non-quantized` $ export DATA_DIR=`ck locate env --tags=dataset imagenet val` $ export EXTRA_OPS=""--count 1024 --scenario Offline"" $ ./run_local.sh tf mobilenet cpu ... TestScenario.Offline qps=237.10  mean=3.3406  time=4.319  queries=1024  tiles=50.0:2.9683 80.0:4.2340 90.0:4.2692 95.0:4.2827 99.0:4.2932 99.9:4.2932 ```  """;General;https://github.com/ctuning/ck-mlperf
"""Data scientists often choose the uniform distribution or the normal distribution as the latent variable distribution when they build representative models of datasets. For example  the studies of the GANs [1] and the VAEs[2] used the uniform random distribution and the normal one  respectively.  As the approximate function implemented by neural networks is usually continuous  the topological structure of the latent variable distribution is preserved after the transformation from the latent variables space to the observable variables space. Given that the observed variables are distributed on a torus and that networks  for example the GANs  are trained with the latent variables sampled from the normal distribution  the structure of the projected distribution by the trained networks does not meet with the torus  even if residual error is small enough. Imagine another example where the observable variables follow a mixture distribution  of which clusters separate each other  trained variational autoencoder can encode the feature on the latent variable space with high precision  however  the decoded distribution consists of connected clusters since the latent variable is topologically equal with the ball. This means that the topology of the given dataset is not represented by the projection of the trained networks.  In this short text  we study the consequence of autoencoders' training due to  the topological mismatch. We use the SAE[4] as autoencoders  which is enhanced based on the WAE[3] owing to the sinkhorn algorithm.   """;General;https://github.com/allnightlight/ConditionalWassersteinAutoencoderPoweredBySinkhornDistance
"""If you want to download weights the link in ""weights.txt"" file you can check that otherwise if you want to train on your own coustom dataset you can use man losses like ciou losses giou loss if you want reference you can check it in ""losses.py"" file   """;Computer Vision;https://github.com/ravindra579/object-detection_yolov4
"""``` pip install . ```  You're ready to go!   ``` pip install -r requirements.txt ```   ``` pip install schnetpack ```   The best place to start is training a SchNetPack model on a common benchmark dataset.  The example scripts provided by SchNetPack are inserted into your PATH during installation.    The QM9 example scripts allows to train and evaluate both SchNet and wACSF neural networks. The training can be started using:  ``` spk_run.py train <schnet/wacsf> qm9 <dbpath> <modeldir> --split num_train num_val [--cuda] ```  where num_train and num_val need to be replaced by the number of training and validation datapoints respectively.  You can choose between SchNet and wACSF networks and have to provide a path to the database file and a path to a directory which will be used to store the model. If the database path does not exist  the data is downloaded and stored there. Please note that the database path must include the file extension .db. With the `--cuda` flag  you can activate GPU training. The default hyper-parameters should work fine  however  you can change them through command-line arguments.  Please refer to the help at `spk_run.py train <schnet/wacsf> --help`.   The training progress will be logged in `<modeldir>/log`  either as CSV  (default) or as TensorBoard event files. For the latter  TensorBoard needs to be installed to view the event files. This can be done by installing the version included in TensorFlow   ``` pip install tensorflow ```  or the [standalone version](https://github.com/dmlc/tensorboard).  To evaluate the trained model with the best validation error  call  ``` spk_run.py eval <datapath> <modeldir> --split test [--cuda] ```  which will run on the specified `--split` and write a result file `evaluation.txt` into the model directory.   """;General;https://github.com/atomistic-machine-learning/schnetpack
"""Python  Pytorch   """;Computer Vision;https://github.com/mminamina/Transfer_Learning_NaimishNet---Keypoints_Detection
"""Build vocabulary     Make batches through pytorch Dataloader     > Added pytorch 1.0 compatible notebook. It uses pytorch 1.1 and ignite training functions. Also better use of pytorch Dataset and Dataloader. Code is more compact and easy to understand.   """;General;https://github.com/hpanwar08/sentence-classification-pytorch
"""- Install torch and dependencies from https://github.com/torch/distro - Install tensorboardX from https://github.com/lanpa/tensorboardX - Clone this repo: ```bash git clone https://github.com/onedayatatime0923/Cycle_Mcd_Gan cd Cycle_Mcd_Gan ```   Download gtFine_trainvaltest.zip and leftImg8bit_trainvaltest.zip from https://www.cityscapes-dataset.com/downloads/   Note: the datastructure will become like this   cd checkpoints/cycle_mcd_da   """;General;https://github.com/onedayatatime0923/Cycle_Mcd_Gan
"""$ PROJ_DIR=~/shake_shake_chainer  #: assuming you clone this repo to your home directory  $ git clone https://github.com/motokimura/shake_shake_chainer.git $PROJ_DIR   $ bash build.sh   $ bash run.sh   $ bash exec.sh  : Now you should be inside the container already running. Start TensorBoard by following:   owruby's Pytorch implementation. [GitHub]   """;General;https://github.com/motokimura/shake_shake_chainer
"""创建 python 环境：  conda create -n paddle_env python=3.6   conda activate paddle_env  pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -U 库名 --default-time=1000 --user   python -m pip install paddlepaddle-gpu   进入 python 环境并测试：   git clone https://github.com/PaddlePaddle/PaddleDetection.git   pip install -r requirements.txt  指定当前 Python 路径然后测试：   pip install git+https://github.com/philferriere/cocoapi.git#:subdirectory=PythonAPI   X2Paddle 源码地址：https://github.com/PaddlePaddle/X2Paddle       x2paddle_128 = fluid.layers.relu(x2paddle_127  name='x2paddle_128')       x2paddle_132 = fluid.layers.relu(x2paddle_131  name='x2paddle_132')       x2paddle_135 = fluid.layers.elementwise_add(x=x2paddle_134  y=x2paddle_129  name='x2paddle_135')      x2paddle_136 = fluid.layers.relu(x2paddle_135  name='x2paddle_136')       x2paddle_139 = fluid.layers.relu(x2paddle_138  name='x2paddle_139')       x2paddle_142 = fluid.layers.elementwise_add(x=x2paddle_141  y=x2paddle_136  name='x2paddle_142')      x2paddle_143 = fluid.layers.relu(x2paddle_142  name='x2paddle_143')       x2paddle_146 = fluid.layers.relu(x2paddle_145  name='x2paddle_146')       x2paddle_151 = fluid.layers.elementwise_add(x=x2paddle_148  y=x2paddle_150  name='x2paddle_151')      x2paddle_152 = fluid.layers.relu(x2paddle_151  name='x2paddle_152')       x2paddle_155 = fluid.layers.relu(x2paddle_154  name='x2paddle_155')       x2paddle_158 = fluid.layers.elementwise_add(x=x2paddle_157  y=x2paddle_152  name='x2paddle_158')      x2paddle_159 = fluid.layers.relu(x2paddle_158  name='x2paddle_159')       x2paddle_162 = fluid.layers.relu(x2paddle_161  name='x2paddle_162')       x2paddle_167 = fluid.layers.elementwise_add(x=x2paddle_164  y=x2paddle_166  name='x2paddle_167')      x2paddle_168 = fluid.layers.relu(x2paddle_167  name='x2paddle_168')       x2paddle_171 = fluid.layers.relu(x2paddle_170  name='x2paddle_171')       x2paddle_174 = fluid.layers.elementwise_add(x=x2paddle_173  y=x2paddle_168  name='x2paddle_174')      x2paddle_175 = fluid.layers.relu(x2paddle_174  name='x2paddle_175')       x2paddle_178 = fluid.layers.relu(x2paddle_177  name='x2paddle_178')       x2paddle_183 = fluid.layers.elementwise_add(x=x2paddle_180  y=x2paddle_182  name='x2paddle_183')      x2paddle_184 = fluid.layers.relu(x2paddle_183  name='x2paddle_184')       x2paddle_187 = fluid.layers.relu(x2paddle_186  name='x2paddle_187')       x2paddle_190 = fluid.layers.elementwise_add(x=x2paddle_189  y=x2paddle_184  name='x2paddle_190')      x2paddle_191 = fluid.layers.relu(x2paddle_190  name='x2paddle_191')       x2paddle_200 = fluid.layers.transpose(x2paddle_199  perm=[0  2  1]  name='x2paddle_200')      x2paddle_201 = fluid.layers.matmul(x=x2paddle_199  y=x2paddle_200  name='x2paddle_201')       x2paddle_208 = fluid.layers.elementwise_add(x=x2paddle_206  y=x2paddle_207  name='x2paddle_208')      x2paddle_209 = fluid.layers.sqrt(x2paddle_208  name='x2paddle_209')       x2paddle_210 = fluid.layers.elementwise_add(x=x2paddle_210_mm  y=x2paddle_fc_bias  name='x2paddle_210')   output = exe.run(eval_program    """;Computer Vision;https://github.com/Sharpiless/yolov3-vehicle-detection-paddle
"""[x] Pytorch lightning   [ ] EM-Merger (TODO: use https://github.com/eg4000/SKU110K_CVPR19 to do postprocessing util)   You can make inference  guide bellow.   path  img  bboxes = d[0]   path  img  bboxes = d[0]       y1 = d[0]     y2 = d[1]     paths_b  xb  yb = d.collate_fn((y1  y2)) 	     anchors  loss = m(xb.cuda()  yb.cuda())     confidence_threshold = 0.05     iou_threshold = 0.5     bboxes  labels = utils.get_bboxes_from_anchors(anchors  confidence_threshold  iou_threshold  coco_dict) #COCO dict is id->class dictionary (f.e. 0->person)     #For first img     arr = utils.get_img_with_bboxes(xb[0].cpu()  bboxes[0].cpu()  resize=False  labels=labels[0])     Image.fromarray(arr)   """;Computer Vision;https://github.com/rahzaazhar/Test-Remote
"""pip install -r requirements.txt  conda env create -f environment.yml  conda activate yolov3-tf2   """;Computer Vision;https://github.com/ZackPashkin/YOLOv3-EfficientNet-EffYolo
"""To run the script  run run.sh. Otherwise  a manual approach can be taken as follows.   """;General;https://github.com/ptuls/tabnet-modified
"""  [MegaFace](http://megaface.cs.washington.edu/) dataset includes 1 027 060 faces  690 572 identities.   Challenge 1 is taken to test our model with 1 million distractors.   ![image](https://github.com/foamliu/InsightFace-v2/raw/master/images/megaface_stats.png)    MS-Celeb-1M dataset for training  3 804 846 faces over 85 164 identities.    <br>Note: we used the noises list proposed by InsightFace  at https://github.com/deepinsight/insightface.   """;General;https://github.com/foamliu/InsightFace-PyTorch
"""This code provides a framework to easily add architectures and datasets  in order to  train and deploy CNNs for a robot. It contains a full training pipeline in python using Tensorflow and OpenCV  and it also some C++ apps to deploy a frozen protobuf in ROS and standalone. The C++ library is made in a way which allows to add other backends (such as TensorRT and MvNCS)  but only Tensorflow and TensorRT are implemented for now. For now  we will keep it this way because we are mostly interested in deployment for the Jetson and Drive platforms  but if you have a specific need  we accept pull requests!  The networks included is based of of many other architectures (see below)  but not exactly a copy of any of them. As seen in the videos  they run very fast in both GPU and CPU  and they are designed with performance in mind  at the cost of a slight accuracy loss. Feel free to use it as a model to implement your own architecture.  All scripts have been tested on the following configurations: - x86 Ubuntu 16.04 with an NVIDIA GeForce 940MX GPU (nvidia-384  CUDA9  CUDNN7  TF 1.7  TensorRT3) - x86 Ubuntu 16.04 with an NVIDIA GTX1080Ti GPU (nvidia-375  CUDA9  CUDNN7  TF 1.7  TensorRT3) - x86 Ubuntu 16.04 and 14.04 with no GPU (TF 1.7  running on CPU in NHWC mode  no TensorRT support) - Jetson TX2 (full Jetpack 3.2)  We also provide a Dockerfile to make it easy to run without worrying about the dependencies  which is based on the official nvidia/cuda image containing cuda9 and cudnn7. In order to build and run this image with support for X11 (to display the results)  you can run this in the repo root directory ([nvidia-docker](https://github.com/NVIDIA/nvidia-docker) should be used instead of vainilla docker):  ```sh   $ docker pull tano297/bonnet:cuda9-cudnn7-tf17-trt304   $ nvidia-docker build -t bonnet .   $ nvidia-docker run -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v $HOME/.Xauthority:/home/developer/.Xauthority -v /home/$USER/data:/shared --net=host --pid=host --ipc=host bonnet /bin/bash ```  _-v /home/$USER/data:/share_ can be replaced to point to wherever you store the data and trained models  in order to include the data inside the container for inference/training.   (for the new Pytorch version  go here)   Persons (+coco people):   (at your option) any later version.   Coco: Link   queueing tool to share GPU  CPU and Memory resources in a multi-GPU environment.   """;Computer Vision;https://github.com/PRBonn/bonnet
"""In this work  3D image features are learned in a self-supervised way by training pointNet network to recognize the 3d rotation that been applied to an input image. Those learned features are being used for classification task learned on top of this base network.  In this repository I uploaded code and data.   You may need to install <a href=""https://www.tensorflow.org/get_started/os_setup"" target=""_blank"">TensorFlow</a>. You may also need to install h5py.  To install h5py for Python: ```bash sudo apt-get install libhdf5-dev sudo pip install h5py ```   To train a complete model to classify point clouds sampled from 3D shapes run the following bash script:  	!chmod 777 train.sh      run.sh       The training split into two parts and can be run with different parameters (see code):   self-suprevised task predicting randomly rotating images: 	 	python trainRotation.py --model_save_path=fourRotations --rotation_list=[0 3 5 6] (rotation_list default value is [0 3 5 6]. It is list of numbers in between 0-7 represents the pi/2 rotation upon the axis. i.e  3 represents X not rotate  Y rotate  Z rotate)  Then  Classifier is learned on top of previous network. Exampled usage: 	 	python trainClasiffiers.py \ 	--model_save_path=fc3_stop_gradient_4rotations \ 	--model_restore_path=fourRotations \ 	--fc_layers_number=3 \ 	--freeze_weights='True'  (Note: 'model_restore_path'  have to be consistent with the same name of 'model_save_path' parameter from trainRotation.py  as it use the trained parameters of previous network at initialization)  Log files and network parameters will be saved to `log` folder in default. Point clouds of <a href=""http://modelnet.cs.princeton.edu/"" target=""_blank"">ModelNet40</a> models in HDF5 files will be automatically downloaded (416MB) to the data folder. Each point cloud contains 2048 points uniformly sampled from a shape surface. Each cloud is zero-mean and normalized into an unit sphere. There are also text files in `data/modelnet40_ply_hdf5_2048` specifying the ids of shapes in h5 files.  To see HELP for the training script:  We can use TensorBoard to view the network architecture and monitor the training progress.      tensorboard --logdir log  """;Computer Vision;https://github.com/aviros/pointnet_totations
"""I recommend using the original code https://github.com/ikostrikov/pytorch-a2c-ppo-acktr or my personal RL code base https://github.com/ASzot/rl-toolkit    `python train.py` to train.   `python evaluate.py` to test.   """;Reinforcement Learning;https://github.com/ASzot/ppo-pytorch
"""To run these models  you will need to first install all dependencies. These can be located in the `requirements.txt` files of the model folder you wish to run.  Move into the model folder and run the following command:  `pip install -r requirements.txt`  To utilize `make` alongside this  you will need to install `funwithgans` itself as a package. You can do so from the root folder  using the following command which will install an editable version of the package that you can make changes to and immediately see the effects.  `pip install -e .`  Make will allow you to run the examples with a simpler command  while limiting the options you can change. They will oftentimes also perform additional steps  like cleaning up the output directories   or running other necessary stages.  In general  a `make` command will look something like:  `make dcgan-example`  The subsections below will provide the required `make` commands  if there are any to run the examples.  To remove this package when you are done  and to keep your environment clean  use:  `pip uninstall funwithgans`   """;Computer Vision;https://github.com/tylercroberts/funwithgans
"""* Install tesnorflow (skip this step if it's already installed test environment:tensorflow 2.3.0) * Install dependencies:  `pip install -r requirements.txt`   git clone https://github.com/wangermeng2021/EfficientDet-tensorflow2.git    cd EfficientDet-tensorflow2   """;Computer Vision;https://github.com/wangermeng2021/EfficientDet-tensorflow2
"""You can use the [`main.py`](main.py) script in order to train the algorithm with MAML. ``` python main.py --env-name 2DNavigation-v0 --num-workers 20 --fast-lr 0.1 --max-kl 0.01 --fast-batch-size 20 --meta-batch-size 40 --num-layers 2 --hidden-size 100 --num-batches 500 --gamma 0.99 --tau 1.0 --cg-damping 1e-5 --ls-max-steps 15 ```  To evaluate the trained agent  just run ``` python experiments.py ``` Both scripts were tested with Python 3.6.   """;General;https://github.com/MoritzTaylor/maml-rl-tf2
"""This is a MATLAB implementation of a 2 hidden-layers neural network that recognizes handwritten digit with 97% accuracy on MNIST database. The architecture and training parameters of the network is configurable (including number of layers  number of neurons in each layer  number of training rounds  learning rate  and mini-batch length). The network integrates input normalization  He weight initialization [1]  and Swish activation function [2].  https://youtu.be/iykVSmfFJzk  """;General;https://github.com/phogbinh/handwritten-digit-recognition
"""This is the demo program of LSGAN using tensorflow.   It needs input data like Food101 datasets.     """;General;https://github.com/masataka46/demo_LSGAN_TF
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/google-research/bert
"""We make use of some pretrained models  that can be downloaded [here](https://drive.google.com/file/d/1TA-UWYVDkCkNPOy1INjUU9321s-HA6RF/view?usp=sharing). They are a subset of the [models](https://drive.google.com/file/d/1aXTmN2AyNLdZ8zOeyLzpVbRHZRZD0fW0/view?usp=sharing) provided with the code of the original paper. They need to be unzipped and put in the `./pretrained` folder  in the root directory of the repo.  The dataset ([CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html)) is automatically downloaded via `torchvision.datasets` when first running the experiment  and will be saved in the `data/` folder (more info [here](https://pytorch.org/docs/stable/torchvision/datasets.html#cifar)).  The paper is implemented and tested using Python 3.7. Dependencies are listed in [requirements.txt](requirements.txt).  For the moment  it is possible to run the experiment using [VGG nets](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) and [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) as reference models and [GDAS](https://arxiv.org/pdf/1910.04465.pdf)  [WRN](https://arxiv.org/pdf/1605.07146.pdf) and [PyramidNet](https://arxiv.org/pdf/1610.02915.pdf) as victim models.  In order to test our implemenation  install the dependencies with `pip3 install --user --requirement requirements.txt`  and run the following command:  ```bash python run.py ```  This will run the experiment on line 5 of table II of our report  with the following settings:  - Reference models: AlexNet+VGGs - Victim model: GDAS - Number of images: 1000 - Maximum queries per image: 10000 - 0 seed    And hyperparameters:  - eta_g = 0.1 - eta = 1/255 - delta = 0.1 - tau = 1.0 - epsilon = 8/255  N.B.: it takes 7 hours 45 minutes to run on a Google Cloud Platform n1-highmem-8 virtual machine  with 8 vCPU  52 GB memory and an Nvidia Tesla T4.  Moreover  the following settings can be used to customize the experiment:  ```bash usage: run.py [-h] [-ds {Dataset.CIFAR_10}]                      [--reference-models {vgg11_bn vgg13_bn vgg16_bn vgg19_bn AlexNet_bn} [{vgg11_bn vgg13_bn vgg16_bn vgg19_bn AlexNet_bn} ...]]                      [--victim-model {gdas wrn pyramidnet}]                      [--loss {ExperimentLoss.CROSS_ENTROPY ExperimentLoss.NEG_LL}]                      [--tau TAU] [--epsilon EPSILON] [--delta DELTA]                      [--eta ETA] [--eta_g ETA_G] [--n-images N_IMAGES]                      [--image-limit IMAGE_LIMIT]                      [--compare-gradients COMPARE_GRADIENTS]                      [--check-success CHECK_SUCCESS]                      [--show-images SHOW_IMAGES] [--seed SEED]  optional arguments:   -h  --help            show this help message and exit   -ds {Dataset.CIFAR_10}  --dataset {Dataset.CIFAR_10}                         The dataset to be used.   --reference-models {vgg11_bn vgg13_bn vgg16_bn vgg19_bn AlexNet_bn} [{vgg11_bn vgg13_bn vgg16_bn vgg19_bn AlexNet_bn} ...]                         The reference models to be used.   --victim-model {gdas wrn pyramidnet}                         The model to be attacked.   --loss {ExperimentLoss.CROSS_ENTROPY ExperimentLoss.NEG_LL}                         The loss function to be used   --tau TAU             Bandit exploration.   --epsilon EPSILON     The norm budget.   --delta DELTA         Finite difference probe.   --eta ETA             Image learning rate.   --eta_g ETA_G         OCO learning rate.   --n-images N_IMAGES   The number of images on which the attack has to be run   --image-limit IMAGE_LIMIT                         Limit of iterations to be done for each image   --compare-gradients COMPARE_GRADIENTS                         Whether the program should output a comparison between                         the estimated and the true gradients.   --check-success CHECK_SUCCESS                         Whether the attack on each image should stop if it has                         been successful.   --show-images SHOW_IMAGES                         Whether each image to be attacked  and its                         corresponding adversarial examples should be shown   --seed SEED           The random seed with which the experiment should be                         run  to be used for reproducibility purposes. ```  In order to run an experiment on 100 images in which the loss of the true model and the cosine similarity between the estimated and true gradient  for all 5000 iterations per image  regardless of the success of the attack (i.e. the one used for figures 1 and 2 of our report)  you should run  ```bash python3 run.py --check-success=False --n-images=100 --compare-gradients=True ```  N.B.: it takes around 20 hours to run the experiment on the aforementioned machine.  The experiment results are saved in the `outputs/` folder  in a file named `YYYY-MM-DD.HH-MM.npy` a dictionary exported with `numpy.save()`. The format of the dictionary is:  ```python experiment_info = {     'experiment_baseline': {         'victim_model': victim_model_name          'reference_model_names': reference_model_names          'dataset': dataset     }      'hyperparameters': {         'tau': tau          'epsilon': epsilon          'delta': delta          'eta': eta          'eta_g': eta_g     }      'settings': {         'n_images': n_images          'image_limit': image_limit          'compare_gradients': compare_gradients          'gpu': #: If the GPU has been used for the experiment          'seed': seed     }      'results': {         'queries': #: The number of queries run         'total_time' #: The time it took to run the experiment         #: The following are present only if compare_gradients == True         'gradient_products': #: The cosine similarities for each image         'true_gradient_norms': #: The norms of the true gradients for each image         'estimated_gradient_norms': #: The norms of the estimated gradients for each image         'true_losses': #: The true losses each iteration         'common_signs': #: The percentages of common signs between true and est gradients         'subs_common_signs': #: The percentages of common signs between subsequent gradients } ```  The file can be imported in Python using `np.load(output_path  allow_pickle=True).item()`.   """;Computer Vision;https://github.com/epfl-ml-reproducers/subspace-attack-reproduction
"""Python 3  jupyter or jupyterlab   """;Computer Vision;https://github.com/henry32144/wgan-gp-tensorflow
"""```bash $ pip install En-transformer ```   ```python import torch from en_transformer import EnTransformer  model = EnTransformer(     dim = 512      depth = 4                #: depth     dim_head = 64            #: dimension per head     heads = 8                #: number of heads     edge_dim = 4             #: dimension of edge feature     neighbors = 64           #: only do attention between coordinates N nearest neighbors - set to 0 to turn off     talking_heads = True     #: use Shazeer's talking heads https://arxiv.org/abs/2003.02436     checkpoint = True        #: use checkpointing so one can increase depth at little memory cost (and increase neighbors attended to)     use_cross_product = True #: use cross product vectors (idea by @MattMcPartlon) )  feats = torch.randn(1  1024  512) coors = torch.randn(1  1024  3) edges = torch.randn(1  1024  1024  4)  mask = torch.ones(1  1024).bool()  feats  coors = model(feats  coors  edges  mask = mask)  #: (1  16  512)  (1  16  3) ```  Letting the network take care of both atomic and bond type embeddings  ```python import torch from en_transformer import EnTransformer  model = EnTransformer(     num_tokens = 10        #: number of unique nodes  say atoms     rel_pos_emb = True     #: set this to true if your sequence is not an unordered set. it will accelerate convergence     num_edge_tokens = 5    #: number of unique edges  say bond types     dim = 128      edge_dim = 16      depth = 3      heads = 4      dim_head = 32      neighbors = 8 )  atoms = torch.randint(0  10  (1  16))    #: 10 different types of atoms bonds = torch.randint(0  5  (1  16  16)) #: 5 different types of bonds (n x n) coors = torch.randn(1  16  3)            #: atomic spatial coordinates  feats_out  coors_out = model(atoms  coors  edges = bonds) #: (1  16  512)  (1  16  3) ```  If you would like to only attend to sparse neighbors  as defined by an adjacency matrix (say for atoms)  you have to set one more flag and then pass in the `N x N` adjacency matrix.  ```python import torch from en_transformer import EnTransformer  model = EnTransformer(     num_tokens = 10      dim = 512      depth = 1      heads = 4      dim_head = 32      neighbors = 0      only_sparse_neighbors = True     #: must be set to true     num_adj_degrees = 2              #: the number of degrees to derive from 1st degree neighbors passed in     adj_dim = 8                      #: whether to pass the adjacency degree information as an edge embedding )  atoms = torch.randint(0  10  (1  16)) coors = torch.randn(1  16  3)  #: naively assume a single chain of atoms i = torch.arange(atoms.shape[1]) adj_mat = (i[:  None] <= (i[None  :] + 1)) & (i[:  None] >= (i[None  :] - 1))  #: adjacency matrix must be passed in feats_out  coors_out = model(atoms  coors  adj_mat = adj_mat) #: (1  16  512)  (1  16  3) ```   To run a protein backbone coordinate denoising toy task  first install `sidechainnet`  ```bash $ pip install sidechainnet ```  Then  ```bash $ python denoise.py ```   """;General;https://github.com/lucidrains/En-transformer
"""This capstone project would be aimed at building an image classification model using convulated neural networks that will identify the following 4 categories of products: backpacks  clothing  footwear and watches. The source data to construct this model will be based on 20 000 images scraped from Amazon  the world's largest online retailer. Based on this  a recognition model will be used to correctly class a certain product image. Secondly  a recommendation system would be built to promote the closest matches based on a select product image.  Stakeholders will be the e-commerce companies and the user of the services themselves. It will help the company improve the effectiveness of potential transactions. It will also improve the user experience with more accuracy and also to avoid problems arising from wrongly identifying products. Often  users may want to source for similar looking items and a recommender would help to efficiently match a user's preferences to similar postings  giving rise to increased transactions and sales turnover.  The detailed solution to this would be to make use of unsupervised machine learning via neural networks in order to perform multi-classification of product categories. Dimensional clustering can also be used in order to match similar looking product images for the recommender system.  Metrics used to measure the performance would be accuracy using majority class and also compared to Imagenet pre-trained model performance  to be within 5 percentage points. Challenges foreseen would be lack of sufficient data  complex background noise or poor resolution images.   """;Computer Vision;https://github.com/Lester1711/DSI15_Capstone
"""Jupyter Notebook   DeepLearning con PyTorch en 60 minutos   """;General;https://github.com/dccuchile/CC6204
"""Install the `stn` package using:  ``` pip3 install stn ```  Then  you can call the STN layer as follows:  ```python from stn import spatial_transformer_network as transformer  out = transformer(input_feature_map  theta  out_dims) ```  **Parameters**  - `input_feature_map`: the output of the layer preceding the localization network. If the STN layer is the first layer of the network  then this corresponds to the input images. Shape should be (B  H  W  C). - `theta`: this is the output of the localization network. Shape should be (B  6) - `out_dims`: desired (H  W) of the output feature map. Useful for upsampling or downsampling. If not specified  then output dimensions will be equal to `input_feature_map` dimensions.   """;Computer Vision;https://github.com/kevinzakka/spatial-transformer-network
"""To train the network  for the new task  run the terminal command (inside the ""hand"" folder):    To get the evaluation on a new test dataset  unseen by the model  run the terminal command:   """;Computer Vision;https://github.com/alexalm4190/Mask_RCNN-Vizzy_Hand
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/sarrrrry/maskrcnn-benchmark
"""keras implementation of Generative Adversarial Network for MNIST Digit Generation  <!--  """;Computer Vision;https://github.com/FnSK4R17s/GAN
"""Manual: https://github.com/AlexeyAB/darknet/wiki   Requirements (and how to install dependencies)   Requirements for Windows  Linux and macOS   How to compile on Linux/macOS (using CMake)   How to compile on Linux (using make)  How to compile on Windows (using CMake)  How to compile on Windows (using vcpkg)  How to train with multi-GPU   tkDNN: https://github.com/ceccocats/tkDNN  OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile   - `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   - `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/  Pytorch - Scaled-YOLOv4: https://github.com/WongKinYiu/ScaledYOLOv4  TensorFlow: pip install yolov4 YOLOv4 on TensorFlow 2.0 / TFlite / Android: https://github.com/hunglc007/tensorflow-yolov4-tflite   OpenCV the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example  Intel OpenVINO 2021.2: supports YOLOv4 (NPU Myriad X / USB Neural Compute Stick / Arria FPGA): https://devmesh.intel.com/projects/openvino-yolov4-49c756 read this manual (old manual ) (for Scaled-YOLOv4 models use https://github.com/Chen-MingChang/pytorch_YOLO_OpenVINO_demo )  PyTorch > ONNX:   DirectML https://github.com/microsoft/DirectML/tree/master/Samples/yolov4  OpenCL (Intel  AMD  Mali GPUs for macOS & GNU/Linux) https://github.com/sowson/darknet   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like CUDA  cudnn  ZED and build against those. It will also create a shared object library file to use darknet for code development.  To update CMake on Ubuntu  it's better to follow guide here: https://apt.kitware.com/ or https://cmake.org/download/  git clone https://github.com/AlexeyAB/darknet  cd darknet  mkdir build_release  cd build_release   cmake --build . --target install --parallel 8  Install: Cmake  CUDA  cuDNN How to install dependencies  Install powershell for your OS (Linux or MacOS) (guide here).   git clone https://github.com/AlexeyAB/darknet  cd darknet  ./build.ps1 -UseVCPKG -EnableOPENCV -EnableCUDA -EnableCUDNN   remove option -UseVCPKG if you plan to manually provide OpenCV library to darknet or if you do not want to enable OpenCV integration  add option -EnableOPENCV_CUDA if you want to build OpenCV with CUDA support - very slow to build! (requires -UseVCPKG)  If you open the build.ps1 script at the beginning you will find all available switches.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   You also need to specify for which graphics card the code is generated. This is done by setting ARCH=. If you use a never version than CUDA 11 you further need to edit line 20 from Makefile and remove -gencode arch=compute_30 code=sm_30 \ as Kepler GPU support was dropped in CUDA 11. You can also drop the general ARCH= and just uncomment ARCH= for your graphics card.   MSVC: https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community  CMake GUI: Windows win64-x64 Installerhttps://cmake.org/download/   find the executable file darknet.exe in the output path to the binaries you specified  This is the recommended approach to build Darknet on Windows.  Install Visual Studio 2017 or 2019. In case you need to download it  please go here: Visual Studio Community. Remember to install English language pack  this is mandatory for vcpkg!  Install CUDA enabling VS Integration during installation.   git clone https://github.com/AlexeyAB/darknet  cd darknet  .\build.ps1 -UseVCPKG -EnableOPENCV -EnableCUDA -EnableCUDNN  (add option -EnableOPENCV_CUDA if you want to build OpenCV with CUDA support - very slow to build! - or remove options like -EnableCUDA or -EnableCUDNN if you are not interested in them). If you open the build.ps1 script at the beginning you will find all available switches.  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   in Python: https://github.com/tzutalin/labelImg  in Python: https://github.com/Cartucho/OpenLabeling   in JavaScript: https://github.com/opencv/cvat   If you use `build.ps1` script or the makefile (Linux only) you will find `darknet` in the root directory.  If you use the deprecated Visual Studio solutions  you will find `darknet` in the directory `\build\darknet\x64`.  If you customize build with CMake GUI  darknet executable will be installed in your preferred folder.  - Yolo v4 COCO - **image**: `./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` - **Output coordinates** of objects: `./darknet detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` - Yolo v4 COCO - **video**: `./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` - Yolo v4 COCO - **WebCam 0**: `./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` - Yolo v4 COCO for **net-videocam** - Smart WebCam: `./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` - Yolo v4 - **save result videofile res.avi**: `./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` - Yolo v3 **Tiny** COCO - video: `./darknet detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` - **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` - Yolo v3 Tiny **on GPU #1**: `./darknet detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` - Alternative method Yolo v3 COCO - image: `./darknet detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` - Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):     `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` - 186 MB Yolo9000 - image: `./darknet detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` - Remember to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app - To process a list of images `data/train.txt` and save results of detection to `result.json` file use:     `./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` - To process a list of images `data/train.txt` and save results of detection to `result.txt` use:     `./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` - Pseudo-labelling - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` - To calculate anchors: `./darknet detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` - To check accuracy mAP@IoU=50: `./darknet detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` - To check accuracy mAP@IoU=75: `./darknet detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   - on Linux   - using `build.sh` or   - build `darknet` using `cmake` or   - set `LIBSO=1` in the `Makefile` and do `make` - on Windows   - using `build.ps1` or   - build `darknet` using `cmake` or   - compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs:  - C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h   - Python examples using the C API:     - https://github.com/AlexeyAB/darknet/blob/master/darknet.py     - https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py  - C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp   - C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp  ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     - You should have installed **CUDA 10.2**     - To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      - you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`      - after launching your console application and entering the image file name - you will see info for each object:     `<obj_id> <left_x> <top_y> <width> <height> <probability>`     - to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     - you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)  `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42)  ```cpp struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false);         std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```   """;Computer Vision;https://github.com/AlexeyAB/darknet
"""Cloning the repo  ```shell $ git clone http://github.com/xuyuwei/resnet-tf $ cd resnet-tf ```  Setting up the virtualenv  installing TensorFlow (OS X) ```shell $ virtualenv venv $ source venv/bin/activate (venv)$ pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl ```  If you don't have virtualenv installed  run `pip install virtualenv`. Also  the cifar-10 data for python can be found at: https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz. Place the data in the main directory.  Start Training: ```shell (venv)$ python main.py  ```  This starts the training for ResNet-20  saving the progress after training every 512 images. To train a net of different depth  comment the line in `main.py` ``` net = models.resnet(X  20) ``` and uncomment the line initializing the appropriate model.   """;General;https://github.com/t0930198/backup
"""1) Install from <a href=""https://pypi.org/project/vistrans/"">PyPI</a>  ```Python pip install vistrans ``` 2) Install from <a href=""https://anaconda.org/nachiket273/vistrans"">Anaconda</a>  ```Python conda install -c nachiket273 vistrans ```   --------------------- pip install vistrans ``` 1) List Pretrained Models. ```Python from vistrans import BotNet BotNet.list_pretrained() ``` 2) Create Pretrained Models. ```Python from vistrans import BotNet model = BotNet.create_pretrained(name  img_size  in_ch  num_classes                                   n_heads  pos_enc_type) ``` 3) Create Custom Model ```Python from vistrans import BotNet model = BotNet.create_model(layers  img_size  in_ch  num_classes  groups                              norm_layer  n_heads  pos_enc_type) ```  #:#: Version 0.001 (03/04/2021) ----------------------------- [![PyPI version](https://badge.fury.io/py/vistrans.svg)](https://badge.fury.io/py/vistrans)  Pretrained Pytorch <a href=""https://arxiv.org/abs/2010.11929"">Vision Transformer</a> Models including following <br> * vit_s16_224 * vit_b16_224 * vit_b16_384 * vit_b32_384 * vit_l16_224 * vit_l16_384 * vit_l32_384 <br> Implementation based off <a href=https://github.com/google-research/vision_transformer>official jax repository</a> and <a href=""https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py"">timm's implementation</a><br>  #:#: Usage --------------------- 1) List Pretrained Models. ```Python from vistrans import VisionTransformer VisionTransformer.list_pretrained() ``` 2) Create Pretrained Models. ```Python from vistrans import VisionTransformer model = VisionTransformer.create_pretrained(name  img_size  in_ch  num_classes) ``` 3) Create Custom Model ```Python from vistrans import VisionTransformer model = VisionTransformer.create_model(img_size  patch_size  in_ch  num_classes                                         embed_dim  depth  num_heads  mlp_ratio                                         drop_rate  attention_drop_rate  hybrid                                         norm_layer  bias) ```  """;Computer Vision;https://github.com/nachiket273/VisTrans
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/aakashjhawar/traffic-sign-detection
"""   First  you can go to the official site(https://www.cityscapes-dataset.com/downloads/).    Then  download `leftImg8bit_trainvaltest.zip`(11GB) and `gtFine_trainvaltest.zip`(241MB). Make sure that you've registered your email on this site before downloading and that email is not a gmail account.   I personally used 2975 images as train data  500 images as both validation and test data. I saved configuration details in files located at `Csv` dir.   Pyramid Scene Parsing Network (PSPNet) is the model architecture proposed by this paper(https://arxiv.org/abs/1612.01105).     <img src=""https://user-images.githubusercontent.com/51239551/98388785-8ac0b380-2096-11eb-8a61-44401b1ec8b6.png"" width=""230""/> <img src=""https://user-images.githubusercontent.com/51239551/98388903-af1c9000-2096-11eb-88bf-fd5ce39b1d2c.png"" width=""230""/> <img src=""https://user-images.githubusercontent.com/51239551/98388922-b5ab0780-2096-11eb-920b-f768001eb05e.png"" width=""230""/>      Input Image(left)  Output Image(center)  Ground Truth(right)   I leave my own environment below. I tested it out on a single GPU.   Linux(Ubuntu 18.04.5 LTS)       generate the pickle file contaning hyperparameter values by running the following command.  ``` python config.py (-h) ```  you will see the pickle file in `Pkl` dir.   now you can start training the model.  ``` python train.py -p Pkl/***.pkl ```  After training is done  you could see prediction of pretrained model  ``` python infer.py -p Weights/***.pt ```   """;Computer Vision;https://github.com/Rintarooo/PSPNet
"""you can save your adjacent matrix using the code below   noted: The nodeID start from 0.<br>   ``` $ python main.py -c config/xx.ini ``` >noted: your can just checkout and modify config file or main.py to get what you want.  """;Graphs;https://github.com/suanrong/SDNE
"""Can we infer intentions and goals from a person's actions? As an example of this family of problems  we consider here whether it is possible to decipher what a person is searching for by decoding their eye movement behavior. We conducted two human psychophysics experiments on object arrays and natural images where we monitored subjects' eye movements while they were looking for a target object. Using as input the pattern of ""error"" fixations on non-target objects before the target was found  we developed a model (InferNet) whose goal was to infer what the target was. ""Error"" fixations share similar features with the sought target. The Infernet model uses a pre-trained 2D convolutional architecture to extract features from the error fixations and computes a 2D similarity map between the error fixation and all locations across the search image by modulating the search image via convolution across layers. InferNet consolidates the modulated response maps across layers via max pooling to keep track of the sub-patterns highly similar to features at error fixations and integrates these maps across all error fixations. InferNet successfully identifies the subject's goal and outperforms all the competitive null models  even without any object-specific training on the inference task.   [![problemintro](img/Capture.JPG)](img/Capture.JPG)  We present an example below. The figure shows the error fixations indicated by yellow number in the visual search process of one human subject (Column 1)  the inference maps predicted by our InferNet based on human error fixations (Column 2) and the inferred target location denoted by green numbers (Column 3) from the inference maps. Red color denotes higher probability of being the search target.  [![problemintro](img/1-teaser.gif)](img/1-teaser.gif)   Navigate to the repository folder. To pre-process fixation data  extract cropped fixation patches and save them in a folder  run ```preprocessFixation/GenerateFixationPatchObjectArray.m``` for object arrays in MATLAB. One can easily generalize the same processes to natural images by running ```preprocessFixation/GenerateFixationPatchObjectNaturalDesignWaldo.n``` .  To run our InferNet torch7 model (take fixation patches and search images as inputss  output likelihood maps for selected layer in [VGG16](https://arxiv.org/abs/1409.1556))  navigate to ```/torchMM``` and copy the following command in the command window: ``` th MMInferArray.lua ```  Navigate to ```/eval``` folder. Run ```ConsolidateAttentionMapArray.m``` in MATLAB to consolidate likelihood maps across layers and save them in a folder. Run ```ScoreBasedonErrorFixationsArray.m``` to generate final likelihood maps and calculate the number of guesses required given certain number of error fixations. All the evaluation results are saved in ```Mat``` folder. One can skip the steps above and run the following functions to plot the results presented in our paper.  - ```PlotScoreBasedonErrorFixationNumber_cummulative.m```: plot relative improvment performance for all computational models. Toggle ```type``` variable from ```array``` to ```naturaldesign``` for different plots in two datasets respectively. - ```PlotAblationArray.m```: plot relative improvment performance for all ablated models on object arrays. - ```PlotQualatativeArray.m```: plot visualization example of final likelihood maps on object arrays.   """;Computer Vision;https://github.com/kreimanlab/HumanIntentionInferenceZeroShot
"""This repository provides the official PyTorch implementation of UNAS that was presented at CVPR 2020.  The paper presents results in two search spaces including [DARTS](https://arxiv.org/abs/1806.09055)  and [ProxylessNAS](https://arxiv.org/abs/1812.00332) spaces. Our paper can be found [here](https://arxiv.org/abs/1912.07651).    We examined UNAS on the CIFAR-10  CIFAR-100  and ImageNet datasets. The CIFAR datasets will be downloaded automatically if you don't have a local copy. However  you may need to download the ImageNet 2012 dataset  and structure the dataset under  train and val subfloders. You can  follow [this page](https://github.com/pytorch/examples/tree/master/imagenet#requirements)  to structure the dataset. The data directory should be in the form:      data/         ├── train/             ├── n01440764/             ├── n01443537/             ├── ...         ├── val/             ├── n01440764/             ├── n01443537/             ├── ...             """;Computer Vision;https://github.com/NVlabs/unas
"""Our Bregman learning framework aims at training sparse neural networks in an inverse scale space manner  starting with very few parameters and gradually adding only relevant parameters during training. We train a neural network <img src=""https://latex.codecogs.com/svg.latex?f_\theta:\mathcal{X}\rightarrow\mathcal{Y}"" title=""net""/> parametrized by weights <img src=""https://latex.codecogs.com/svg.latex?\theta"" title=""weights""/> using the simple baseline algorithm <p align=""center"">       <img src=""https://latex.codecogs.com/svg.latex?\begin{cases}v\gets\ v-\tau\hat{\nabla}\mathcal{L}(\theta) \\\theta\gets\mathrm{prox}_{\delta\ J}(\delta\ v) \end{cases}"" title=""Update"" /> </p>  where  * <img src=""https://latex.codecogs.com/svg.latex?\mathcal{L}"" title=""loss""/> denotes a loss function with stochastic gradient <img src=""https://latex.codecogs.com/svg.latex?\hat{\nabla}\mathcal{L}"" title=""stochgrad""/>  * <img src=""https://latex.codecogs.com/svg.latex?J"" title=""J""/> is a sparsity-enforcing functional  e.g.  the <img src=""https://latex.codecogs.com/svg.latex?\ell_1"" title=""ell1""/>-norm  * <img src=""https://latex.codecogs.com/svg.latex?\mathrm{prox}_{\delta\ J}"" title=""prox""/> is the proximal operator of <img src=""https://latex.codecogs.com/svg.latex?J"" title=""J""/>.  Our algorithm is based on linearized Bregman iterations [[2]](#2) and is a simple extension of stochastic gradient descent which is recovered choosing <img src=""https://latex.codecogs.com/svg.latex?J=0"" title=""Jzero""/>. We also provide accelerations of our baseline algorithm using momentum and Adam [[3]](#3).   The variable <img src=""https://latex.codecogs.com/svg.latex?v"" title=""v""/> is a subgradient of <img src=""https://latex.codecogs.com/svg.latex?\theta"" title=""weights""/> with respect to the *elastic net* functional   <p align=""center"">       <img src=""https://latex.codecogs.com/svg.latex?J_\delta(\theta)=J(\theta)+\frac1\delta\|\theta\|^2"" title=""el-net""/> </p>  and stores the information which parameters are non-zero.   """;General;https://github.com/TimRoith/BregmanLearning
"""pip install pytorch-pretrained-bert   output_dir set as per your directory.   """;Natural Language Processing;https://github.com/hegebharat/sentiment-Analysis-for-German-Datasets
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/goldenbili/Bert_Test3
"""| class_id | name       | TP  | FP | ap     |   or you can get pretrained weights   <img src=""https://github.com/MINED30/Face_Mask_Detection_YOLO/blob/main/demo/ezgif-2-6ce114bf9fed.gif?raw=true"" width=""60%""> <img src=""https://github.com/MINED30/Face_Mask_Detection_YOLO/blob/main/demo/predictions%20(2).jpg?raw=true"" width=""60%""> <img src=""https://github.com/MINED30/Face_Mask_Detection_YOLO/blob/main/demo/ezgif-3-6e175c3b97a8.gif?raw=true"" width=""60%""> <img src=""https://raw.githubusercontent.com/MINED30/Face_Mask_Detection_YOLO/main/demo/predictions%20(1).jpg"" width=""60%"">   """;Computer Vision;https://github.com/MINED30/Face_Mask_Detection_YOLO
"""If you do not have the right dependencies to run this project  you can use our docker image which we used too to run these experiments on.  ``` docker pull etheleon/dotfiles docker run --runtime=nvidia -it etheleon/dotfiles ```    """;General;https://github.com/Neoanarika/Searching-for-activation-functions
"""In this project  my aim was to develop a model that could regenerate patched/covered parts of human faces  and achieve believable results. I used the [Celeb-A](https://www.kaggle.com/jessicali9530/celeba-dataset) dataset  and created a Generative Adversarial Network with a Denoising Autoencoder as the Generator and a Deep Convolutional Network as the Discriminator. I chose this architecture based on *Avery Allen and Wenchen Li*'s [Generative Adversarial Denoising Autoencoder for Face Completion](https://www.cc.gatech.edu/~hays/7476/projects/Avery_Wenchen/).  The Denoising Autoencoder has 'relu' activations in the middle layers while the output layer had a 'tanh' activation. Each Convolution layer was followed by a BatchNormalization layer. The Discriminator has 'LeakyReLU' activations for the Convolution part  with a BatchNormalization layer following every Conv layer. At the end  the output from the CNN segment was flattened and connected to a Dense layer with 1 node  having 'sigmoid' as the activation function. This would enable the discrimator to predict the probability that the input image is real.  I added distortions to the images in two ways:- - Added random Gaussian noise. - Added random sized Black Patches.  The entire training was done on a GTX 1080 GPU  and took about 12days.  The latest checkpoints and the saved generator and discriminator can be found [here](https://drive.google.com/drive/folders/13wUgCcENajkPZ4MHz2bHrJtQepyVDvtb?usp=sharing).  A few sample generated images are present in `saved_imgs`.   1) Create a new Python/Anaconda environment (optional but recommended). You might use the `environment.yml` file for this purpose (Skip Step-2 in that case).  2) Install the necessary packages. Refer to the packages mentioned in `environment.yml`.  3) Download the training checkpoints and saved generator and discriminator models from [here](https://drive.google.com/drive/folders/13wUgCcENajkPZ4MHz2bHrJtQepyVDvtb?usp=sharing).  4) Download the [Celeb-A](https://www.kaggle.com/jessicali9530/celeba-dataset) dataset  and place it in the directory in the following manner:- <pre> ├─── Patched-Face-Regeneration-GAN      ├─── ..      ├─── saved_imgs           ├─── training_checkpoints      ├─── saved_discriminator      ├─── saved_generator      ├─── discriminator.png      ├─── generator.png      ├─── inference_output.png      ├─── environment.yml      ├─── Face_Generation.ipynb          └─── images             └─── img_align_celeba                ├─── 000001.jpg                ├─── 000002.jpg                ├─── 000003.jpg                ├─── 000004.jpg                ├─── 000005.jpg                ├─── ..                ├─── ..                ├─── 202597.jpg                ├─── 202598.jpg                └─── 202599.jpg </pre>   """;Computer Vision;https://github.com/rdutta1999/Patched-Face-Regeneration-GAN
"""To install all necessary dependencies simply run:  ````shell script pip install -r requirements.txt ````   This repository showcases the mode collapse problem of GANs for different loss functions on a 2D toy example distribution.  <table>   <tr>     <td> Standard GAN loss </td>     <td> Non-saturating GAN loss </td>     <td> Non-saturating GAN loss + top-k </td>     <td> Hinge GAN loss </td>   </tr>    <tr>     <td> <img src=""/plots/standard.gif""  alt=""1"" width = 200px height = 150px ></td>     <td><img src=""/plots/non-saturating.gif"" alt=""2"" width = 200px height = 150px></td>     <td> <img src=""/plots/non-saturating_top_k.gif""  alt=""3"" width = 200px height = 150px ></td>     <td><img src=""/plots/hinge.gif"" alt=""4"" width = 200px height = 150px></td>   </tr>    <tr>     <td> Wasserstein GAN loss + spec. norm </td>     <td> Wasserstein GAN loss + GP </td>     <td> Wasserstein GAN loss GP + top-k </td>     <td> Least squares GAN loss </td>   </tr>    <tr>     <td> <img src=""/plots/wasserstein.gif""  alt=""5"" width = 200px height = 150px ></td>     <td><img src=""/plots/wasserstein-gp.gif"" alt=""6"" width = 200px height = 150px></td>     <td> <img src=""/plots/wasserstein-gp_top_k.gif""  alt=""7"" width = 200px height = 150px ></td>     <td><img src=""/plots/least-squares.gif"" alt=""8"" width = 200px height = 150px></td>   </td>   </tr> </table>   To reproduces the achieved results run the [`main.py`](main.py) script with the corresponding arguments.  |Argument | Default value | Info | | --- | :---: | --- | |`--device` | 'cuda' | Set device to be utilized (cuda or cpu) | |`--epochs` | 500 | Training epochs to be performed | |`--plot_frequency` | 10 | Frequency of epochs to produce plots | |`--lr` | 0.0001 | Learning rate to be applied | |`--latent_size` | 32 | Size of latent vector to be utilized | |`--samples` | 10000 | Number of samples from the real distribution | |`--batch_size` | 500 | Batch size to be utilized | |`--loss` | 'standard' | GAN loss function to be used (standard  non-saturating  hinge  wasserstein  wasserstein-gp or least-squares) | |`--spectral_norm` | False | If set spectral norm is utilized | |`--topk` | False | If set top-k training is utilized after 0.5 of the epochs to be performed |   """;Computer Vision;https://github.com/ChristophReich1996/Mode_Collapse
"""Follow these instructions to install the VGGFace from the paper (https://arxiv.org/pdf/1703.07332.pdf):   $ tar xvzf vgg_face_caffe.tar.gz  $ sudo apt install caffe-cuda  $ pip install mmdnn   If you have a problem with pickle  delete your numpy and reinstall numpy with version 1.16.1   cv2 (opencv-python)   - modify paths in params folder to reflect your path - preprocess.py: preprocess our data for faster inference and lighter dataset - train.py: initialize and train the network or continue training from trained network - embedder_inference.py: (Requires trained model) Run the embedder on videos or images of a person and get embedding vector in tar file  - fine_tuning_trainng.py: (Requires trained model and embedding vector) finetune a trained model - webcam_inference.py: (Requires trained model and embedding vector) run the model using person from embedding vector and webcam input  just inference - video_inference.py: just like webcam_inference but on a video  change the path of the video at the start of the file    """;General;https://github.com/times2049/talkinghead
"""Tested on CartPole & Pendulum   """;Reinforcement Learning;https://github.com/liampetti/DDPG
"""To train the HR Generator you must run:   Original SRGAN: https://github.com/twhui/SRGAN-PyTorch  Other SRGAN: https://github.com/kunalrdeshmukh/SRGAN   https://twitter.com/spain_ai_ <br>   If you have any doubts  you can feel free to contact me.   """;General;https://github.com/AntonioAlgaida/Edge.SRGAN
"""Remote sensing offers the unique opportunity to monitor the evolution of human settlements from space.   Using multi-spectral images from Landsat 5  Landsat 7  Landsat 8 along with nighttime images from DMSP-OLS and NPP-VIIRS  this project aims to:  - Build a model quantifying the footprint of cities and monitor their evolution over time. - Use the model to provide an empirical validation of the scaling law in the city size distribution  at difference scales (regional level  country level  worldwide).  Yearly images between 1992 to 2020 of Las Vegas  Nevada - USA   |RGB composites             |  Nighttime Lights        | |:-------------------------:|:-------------------------:| ![Alt Text](./data/demo/las_vegas_area_rgb.gif) |  ![Alt Text](./data/demo/las_vegas_area_nl.gif) |Image segmentation             |  Segmentation legend    | |![Alt Text](./data/demo/las_vegas_area_preds.gif) | ![Alt Text](./data/demo/legend.png) |   - Quantifying the urban growth of the Las Vegas area  Nevada - USA between 1992 and 2020* ![Alt Text](./data/demo/las_vegas_area_values.png)     - Quantifying the  evolution of the 10 biggest urban agglomerations in India (by surface area) between 2015 and 2020* ![Alt Text](./data/demo/india_top10_cities.png)  - Empirical validation of the Zipf's law for cities in India  quantifying the surface areas of +15000 cities in the country  between 2015 and 2020* (Log-Log scales) ![Alt Text](./data/demo/zipf_law_india.png) 	- Read more on [Self-Organized Criticality and Urban Development](https://www.researchgate.net/publication/26531729_Self-Organized_Criticality_and_Urban_Development) from Batty and Xie  1999.  _*2020 based on partial data as of May 2020_  _Important Note: computed areas for a specific city might differ from official reported numbers as this project quantifies the organic footprint of agglomerations as opposed to the administrative boundaries of cities._   """;Computer Vision;https://github.com/badrbmb/cities-watch
"""Regex tester: https://regex101.com/ <br/>   """;General;https://github.com/KeithGalli/pycon2020
"""Download [Market1501 Dataset](http://www.liangzheng.com.cn/Project/project_reid.html) [[Google]](https://drive.google.com/file/d/0B8-rUzbwVRk0c054eEozWG9COHM/view) [[Baidu]](https://pan.baidu.com/s/1ntIi2Op)  Preparation: Put the images with the same id in one folder. You may use  ```bash python prepare.py ``` Remember to change the dataset path to your own path.  Futhermore  you also can test our code on [DukeMTMC-reID Dataset]( [GoogleDriver](https://drive.google.com/open?id=1jjE85dRCMOgRtvJ5RQV9-Afs-2_5dY3O) or ([BaiduYun](https://pan.baidu.com/s/1jS0XM7Var5nQGcbf9xUztw) password: bhbh)). Our baseline code is not such high on DukeMTMC-reID **Rank@1=64.23%  mAP=43.92%**. Hyperparameters are need to be tuned.   - Install Pytorch from http://pytorch.org/ - Install Torchvision from the source ``` git clone https://github.com/pytorch/vision cd vision python setup.py install ``` - [Optinal] You may skip it. Install apex from the source ``` git clone https://github.com/NVIDIA/apex.git cd apex python setup.py install --cuda_ext --cpp_ext ``` Because pytorch and torchvision are ongoing projects.  Here we noted that our code is tested based on Pytorch 0.3.0/0.4.0/0.5.0/1.0.0 and Torchvision 0.2.0/0.2.1 .   - Float16 to save GPU memory based on apex   What's new: FP16 has been added. It can be used by simply added --fp16. You need to install apex and update your pytorch to 1.0.    | [DenseNet-121] | 90.17% | 74.02% | python train.py --name ft_net_dense --use_dense --train_all |   --gpu_ids which gpu to run.   --gpu_ids which gpu to run.   """;Computer Vision;https://github.com/wxb589/Person_reID_baseline_pytorch-master
"""Install rltorch from source. ``` git clone https://github.com/ku2482/rltorch.git cd rltorch pip install -e . ```   [--num_actors int(default 4)] [--cuda (optional)] \   You can implement Soft Actor-Critic[2  3] agent like this example here. Note that you need a license and mujoco_py to be installed.   [--cuda (optional)] [--seed int(default 0)]   """;Reinforcement Learning;https://github.com/ku2482/rltorch
"""```bash git clone https://github.com/EleutherAI/GPTNeo cd GPTNeo pip3 install -r requirements.txt ```  Create your VM through a google shell (https://ssh.cloud.google.com/) with ctpu up --vm-only so that it can connect to your Google bucket and TPUs and install the requirements with pip (see above).   You can also choose to train GPTNeo locally on your GPUs. To do so  you can omit the Google cloud setup steps above  and git clone the repo locally. Run through the Training Guide below  then when running main.py  you simply have to omit the tpu flag  and pass in GPU ids instead.   We recommend you use Huggingface's pretrained GPT2 tokenizer with our repo (instructions provided below)  but if you want to train a model with a different vocabulary size  we provide facilities to train your own tokenizer like so:       --base_dir ./path/to/your/txt/files \      --output_dir ./output/path \   : if it succeeded  you should see the message   If you have a dataset encoded using the pretrained gpt2 tokenizer  you can specify that like so:       ""path"": ""./path/to/your/*.tfrecords""    The &lt;dataset id&gt; will be the filename  excluding the .json  that you created above   To setup:   """;Natural Language Processing;https://github.com/EleutherAI/gpt-neo
"""This project is based on the [benchmarking-gnns](https://github.com/graphdeeplearning/benchmarking-gnns) repository.  [Follow these instructions](./docs/01_benchmark_installation.md) to install the benchmark and setup the environment.   <br>   """;Graphs;https://github.com/graphdeeplearning/graphtransformer
"""Commonly used training and testing datasets can be downloaded [here](https://github.com/xinntao/BasicSR/blob/master/docs/DatasetPreparation.md).   - Python 3 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux)) - [PyTorch >= 1.0](https://pytorch.org/) - NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads) - Python packages: `pip install numpy opencv-python lmdb pyyaml` - TensorBoard:    - PyTorch >= 1.1: `pip install tb-nightly future`   - PyTorch == 1.0: `pip install tensorboardX`     Training and testing codes are in ['codes/'](./codes/). Please see ['codes/README.md'](./codes/README.md) for basic usages.   """;Computer Vision;https://github.com/pkuxmq/Invertible-Image-Rescaling
"""git clone https://github.com/jaketae/res-mlp.git  Navigate to the cloned directory. You can start using the model via   """;Computer Vision;https://github.com/jaketae/res-mlp
"""Clone the repo   Download Infersent - Follow the steps here https://github.com/facebookresearch/InferSent.   To view the demo I have implement a api  run the file api.py . I attached the image below of the request from the postman  ![alt text](./img/demo.png)    """;Natural Language Processing;https://github.com/rajatgermany/qa-nlp
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;Computer Vision;https://github.com/vantupham/darknet
"""[PyTorch] (by Pavel Yakubovskiy)  [PyTorch] (by 4ui_iurz1)   [PyTorch] (by ZJUGiveLab)  [PyTorch] (by MontaEllis)   """;Computer Vision;https://github.com/MrGiovanni/UNetPlusPlus
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/saurabhnlp/bert
"""You should prepare lmdb dataset   """;Computer Vision;https://github.com/celdeldel/style_conditionnal_gan
"""git clone https://github.com/jaketae/res-mlp.git  Navigate to the cloned directory. You can start using the model via   """;General;https://github.com/jaketae/res-mlp
"""put the images in the folder named ""data"". They are used for training.          put the image in a folder named ""val"". They are used for validation.  when you set folders  training runs ""python main.py"".   after training  test runs ""python pred.py"" It is executed on the images in the folder named ""test"".    like this ``` main.py pred.py data   ├ 000.png   ├ aaa.png   ...   └ zzz.png val   ├ 111.png   ├ bbb.png   ...   └ xxx.png test   ├ 222.png   ├ ccc.png   ...   └ yyy.png ```    left:nearest right:output  <img src = 'output/115_val.png' >    """;Computer Vision;https://github.com/itsuki8914/srresnet4x
"""All unit tests in stable baselines3 can be run using `pytest` runner: ``` pip install pytest pytest-cov make pytest ```  You can also do a static type check using `pytype`: ``` pip install pytype make type ```  Codestyle check with `flake8`: ``` pip install flake8 make lint ```   Install the Stable Baselines3 package: ``` pip install stable-baselines3[extra] ``` **Note:** Some shells such as Zsh require quotation marks around brackets  i.e. `pip install 'stable-baselines3[extra]'` ([More Info](https://stackoverflow.com/a/30539963)).  This includes an optional dependencies like Tensorboard  OpenCV or `atari-py` to train on atari games. If you do not need those  you can use: ``` pip install stable-baselines3 ```  Please read the [documentation](https://stable-baselines3.readthedocs.io/) for more details and alternatives (from source  using docker).    **Note:** Stable-Baselines3 supports PyTorch >= 1.8.1.   A migration guide from SB2 to SB3 can be found in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html).   Github repo: https://github.com/DLR-RM/rl-baselines3-zoo   To install stable-baselines on Windows  please look at the documentation.   Most of the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms.  Here is a quick example of how to train and run PPO on a cartpole environment: ```python import gym  from stable_baselines3 import PPO  env = gym.make(""CartPole-v1"")  model = PPO(""MlpPolicy""  env  verbose=1) model.learn(total_timesteps=10000)  obs = env.reset() for i in range(1000):     action  _states = model.predict(obs  deterministic=True)     obs  reward  done  info = env.step(action)     env.render()     if done:       obs = env.reset()  env.close() ```  Or just train a model with a one liner if [the environment is registered in Gym](https://github.com/openai/gym/wiki/Environments) and if [the policy is registered](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html):  ```python from stable_baselines3 import PPO  model = PPO('MlpPolicy'  'CartPole-v1').learn(10000) ```  Please read the [documentation](https://stable-baselines3.readthedocs.io/) for more examples.    """;General;https://github.com/DLR-RM/stable-baselines3
"""    X = X.cuda()     softmax_model.cuda()     softmax_model.eval()     linear_model.cuda()     linear_model.eval()       softmax_model = builder.get()   linear_model = builder.get()   Dependencies & Installation  The fast transformers library has the following dependencies:   pip install --user pytorch-fast-transformers  Note: macOS users should ensure they have llvm and libomp installed.  Using the homebrew &lt;https://brew.sh&gt;_ package manager  this can be  accomplished by running brew install llvm libomp.   """;General;https://github.com/idiap/fast-transformers
"""This repository is an academical work on a new subject introduced by google researchers called:  Transformer for Image classification at scale. We worked at Georgia Tech Lorraine with the DREAM research team  a robotic laboratory  in in order to test this new image classification technique on a diatom dataset.  This technique called Vision Transformer was published in the folowing paper:  [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929).  Overview of the model given by Google: we split an image into fixed-size patches  linearly embed each of them  add position embeddings  and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification  we use the standard approach of adding an extra learnable ""classification token"" to the sequence.   Make sure you have `Python>=3.6` installed on your machine.  → Install venv package: ``` apt-get install python3-venv ``` → Create jax-ViT venv:   ``` python3 -m venv venv/jax-ViT ``` → Activate venv:  ``` source /venv/jax-ViT/bin/activate ``` → Upgrade pip before installing required package:  ``` python -m pip install --upgrade pip ``` → Install required package for jax-ViT into the venv: ```  pip3 install -r vit_jax/requirements.txt ``` → Install jax-GPU version:  ``` pip install --upgrade jax jaxlib==0.1.61+cuda110 -f https://storage.googleapis.com/jax-releases/jax_releases.html ``` (“cuda110” → means cuda v.11.0: change this according to the cuda version in your computer) → Clone Github code:  ``` git clone https://github.com/Thanusan19/Vision_Transformer.git ```  For more details on Jax  please check the [Jax GitHub repository](https://github.com/google/jax) Note that installation instructions for GPU differs slightly from the instructions for CPU.    You can find all these models in the following storage bucket:   Download one of the pre-trained model with the following command:   We ended having multiple branches depending on the use. The first one corresponds to our initial ViT implementation changes  and is capable of training on the diatom dataset and has data augmentation capabilities. The second one is ""cnn_model"" branch  which was used to test simple convolution and PCA based feature extractor. Finally the third one is ""resnet_vit"" branch which was used to test the resnet50 model as feature extractor.  Here are the general settings to check in the various implementations: - First make sure that `FINE_TUNE = True` in order to do fine-tuning - Set the following parameter in order to enable inference: `INFERENCE = True`. Also set the checkpoint's filepath  in: `params = checkpoint.load('../models/model_diatom_final_checkpoints.npz')` - If you want to train without the fine-tuned weights  use: `LOAD_FINE_TUNNED_CHECKPOINTS = False`. Also set the checkpoint's filepath: `checkpoints_file_path = ""../models/model_diatom_checkpoint_step_6000_with_data_aug.npz""` - Test a saved checkpoint accuracy by setting the following parameter: `CHECKPOINTS_TEST = True`. Also set the checkpoint's filepath: `  checkpoints_file_path = ""../models/model_diatom_final_checkpoints.npz""` - Choose the ViT model to train on with the `model` parameter. The basic model to use could be `model = 'ViT-B_16'`. See branch specific instructions for more details.  - Choose the dataset to load  e.g: `DATASET = 2`. Choose the dataset between:   - `0` for CIFAR-10    - `1` for dog and cats    - `2` for the diatom dataset. - Set the batch size and epochs according to your training resources and needs. We recommend the following parameters:   ```python   epochs = 100   batch_size = 256   warmup_steps = 5   decay_type = 'cosine'   grad_norm_clip = 1   accum_steps = 64  #: 64--> GPU3  #:8--> TPU   base_lr = 0.03 #: base learning rate   ``` - If you want to use data augmentation during training  change the `doDataAugmentation` parameter inside the corresponding call to the python data generator `MyDogsCats()`: `doDataAugmentation=True`. We recommend not using data augmentation on the train and validation sets.  Here is an example of parameters we recommend to use if you want to do fine-tuning with untrained fine-tuning weights  on the diatom dataset: ```python INFERENCE = False FINE_TUNE = True LOAD_FINE_TUNNED_CHECKPOINTS = False CHECKPOINTS_TEST = False DATASET = 2 #:to load diatom dataset batch_size = 256 #:can be set to 512 for no data augmentation and simple ViT model fine-tuning epochs = 100 warmup_steps = 5 decay_type = 'cosine' grad_norm_clip = 1 accum_steps = 64  #: 64--> GPU3  #:8--> TPU base_lr = 0.03 #:base learning rate ```  Once you have checked the specific recommendation for the specific branch  you can launch the training using: ``` cd vit_jax/ python vit_jax.py ``` __NB:__ Make sure you have activated the virtual environment before launching. (e.g. `source /venv/jax-ViT/bin/activate`)   """;Computer Vision;https://github.com/Thanusan19/Vision_Transformer
"""Run setup.py which installs required packages and steps you through downloading additional data: ``` python setup.py ``` You can download all trained models used in [this](https://www.aclweb.org/anthology/P19-1567) paper from [here](https://mega.nz/#!mI0iDCTI!qhKoBiQRY3rLg3K6nxAmd4ZMNEX4utFRvSby_0q2dwU). Each training contains two checkpoints  one for the validation loss minimum and another after 150 epochs. The data and the trainings folder structure match each other exactly.  Richard Csaky (If you need any help with running the code: ricsinaruto@hotmail.com)   The main file can be called from anywhere  but when specifying paths to directories you should give them from the root of the repository. ``` python code/main.py -h ``` <a><img src=""https://github.com/ricsinaruto/NeuralChatbots-DataFiltering/blob/master/docs/help.png"" align=""top"" height=""800"" ></a>     For the complete **documentation** visit the [wiki](https://github.com/ricsinaruto/NeuralChatbots-DataFiltering/wiki).   """;General;https://github.com/ricsinaruto/NeuralChatbots-DataFiltering
"""git clone https://github.com/dreamnotover/english_chinese_machine_translation_baseline.git   python 3.6    mkdir    t2t_tmp   t2t_data  raw_data  1、下载数据  链接: https://pan.baidu.com/s/18nRxRrUY0bOlYlPkCN2kyA 提取码: at2r    sh  ./prepare.sh  sh  ./data_gen.sh   big 模式   sh  train_big.sh  base 模式   sh  train_base.sh   big 模式   sh  decode_big.sh  base 模式   sh  decode_base.sh   """;Natural Language Processing;https://github.com/dreamnotover/english_chinese_machine_translation_baseline
"""This allows you to greatly simplify the model  since it does not have to deal with the manual placement of tensors.  Instead  you just specify which GPU you'd like to use in the beginning of your script.   : Horovod: pin GPU to be used to process local rank (one GPU per process)   Congratulations!  If you made it this far  your fashion_mnist.py should now be fully distributed.  To verify  you can run the following command in the terminal  which should produce no output:   In this tutorial  you will learn how to apply Horovod to a [WideResNet](https://arxiv.org/abs/1605.07146) model  trained on the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset.   In `fashion_mnist.py`  we're using the filename of the last checkpoint to determine the epoch to resume training from in case of a failure:  ![image](https://user-images.githubusercontent.com/16640218/54185268-d35d3f00-4465-11e9-99eb-96d4b99f1d38.png)  As you scale your workload to multi-node  some of your workers may not have access to the filesystem containing the checkpoint.  For that reason  we make the first worker to determine the epoch to restart from  and *broadcast* that information to the rest of the workers.  To broadcast the starting epoch from the first worker  add the following code:  ```python #: Horovod: broadcast resume_from_epoch from rank 0 (which will have #: checkpoints) to other ranks. resume_from_epoch = hvd.broadcast(resume_from_epoch  0  name='resume_from_epoch') ```  ![image](https://user-images.githubusercontent.com/16640218/53534072-2de3bc00-3ab2-11e9-8cf1-7531542e3202.png) (see line 52-54)   """;Computer Vision;https://github.com/darkreapyre/HaaS-KF
"""python DCGAN.pyで学習ループが回ります．その際epochごとにGeneratorのモデルがnpz形式で保存されます．変数max_epochを変更することで学習epoch数が変更出来ます．   """;Computer Vision;https://github.com/akjgskwi/DCGAN_implement
"""Object Cut is an online image background removal service that uses [BASNet](https://github.com/NathanUA/BASNet). Removing the background from an image is a common operation in the daily work of professional photographers and image editors. This process is usually a repeatable and manual task that requires a lot of effort and human time. However  thanks to [BASNet](https://github.com/NathanUA/BASNet)  one of the most robust and fastest performance deep learning models in image segmentation  Object Cut was able to turn it into an easy and automatic process.   It was built as an API to make it as easy as possible to integrate. APIs  also known as Application Programming Interfaces  are already a common way to integrate different types of solutions to improve systems without actually knowing what is happening inside. Specifically  RESTful APIs are a standard in the Software Engineering field for designing and specifying APIs. Making it substantially easier to adapt your desired APIs to your workflows.  <br> <p align=""center"">   <img alt=""Pipeline"" src=""docs/images/pipeline.png"" width=""100%""/> </p> <br>  Object Cut was born to power up the designing and image editing process from the people who work with images daily. Integrating the Object Cut API removes the necessity of understanding the complex inner workings behind it and automates the process of removing the background from images in a matter of seconds.       environment:   To run the server  please execute the following from the root directory:  1. Set up environment creating the `.env` file. This file must have this structure (without the brackets):      ```     SECRET_ACCESS={SECRET_ACCESS}     ```  2. Set up your Google Cloud Storage credentials  decrypt it using GPG with the needed passphrase and decompress it:      ```bash     gpg --quiet --batch --yes --decrypt --passphrase=""{{ GPG_PASSPHRASE }}"" --output ./multiplexer/keys/storage_key.tar ./multiplexer/keys/storage_key.tar.gpg     tar xvf ./multiplexer/keys/storage_key.tar -C ./multiplexer/keys     ```  3. Build everything in parallel:      ````bash    docker-compose build --parallel     ````  4. Deploy the whole stuck (multiplexer  inference and traefik) with just this command.      ```bash     docker-compose up -d --scale multiplexer=1 --scale inference=3     ```  _That's it_! You have ObjectCut running on port 80 routing traffic using _traefik_.   """;Computer Vision;https://github.com/AlbertSuarez/object-cut
"""This repository contains my own implementation of the MobileNet Convolutional Neural Network (CNN) developed in Python programming language with Keras and TensorFlow enabled for Graphic Processing Unit (GPU). The provided source-code contains two functions representing the implementations of the MobileNetV1 and MobileNetV2 architectures. A case study in a image set of two airplane models is also provided to evaluate the performance of the network.   Keras: version 2.2.5  TensorFlow: version 1.14.0   """;Computer Vision;https://github.com/danilojodas/MobileNet
"""You can simply train IGNNK on METR-LA from command line by   """;Graphs;https://github.com/Kaimaoge/IGNNK
"""Under construction...   Under construction...   """;Natural Language Processing;https://github.com/mits58/Pointer-Networks
"""In the Pong environment  the agent has three related elements: action  reward and state.  Actions: agent takes the action at time t; there are six actions including going up  down  staying put  fire the ball  etc. Rewards: agent/environment receives/produces reward  when the opponent fails to hit the ball back towards the agent or the agent get 21 points and win. State: environment updates state St  which is defined by four game frames’ interfaces stacking together - the Pong involves motion of two paddles and one ball  and background features that the agent need to learn at the game. The network  suggested by https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf  is used to approximate the action values  which consists of three convolutional neural networks followed by two dense layers. In addition to a network used for training  the other network  which is architecture identical with the first one  gets its weights by copying them from the train network periodically during training and is used to compute the action value label. The other network (called the target network by the paper) is set up to avoid instability in training.  The model is trained using the following three frameworks.    Matplotlib (https://matplotlib.org/) - The python package used to visualize the final result    The whole project is done in Google's colaboratory environment (https://colab.research.google.com/).   * run the ""Set Up Google Cloud GPU"" section first to set up the GPU for faster computation * run the sequent chunks of code to start training    """;Reinforcement Learning;https://github.com/gznyyb/deep_reinforcement_learning_Pong
"""Based on pointnet.pytorch   """;Computer Vision;https://github.com/yanxp/PointNet
"""<p align=""justify""> The following commands learn a neural network and score on the test set. Training a model on the default dataset.</p>  ```sh $ python src/main.py ``` <p align=""center""> <img style=""float: center;"" src=""clustergcn.gif""> </p>  Training a ClusterGCN model for a 100 epochs. ```sh $ python src/main.py --epochs 100 ``` Increasing the learning rate and the dropout. ```sh $ python src/main.py --learning-rate 0.1 --dropout 0.9 ``` Training a model with a different layer structure: ```sh $ python src/main.py --layers 64 64 ``` Training a random clustered model: ```sh $ python src/main.py --clustering-method random ```  **License**  - [GNU](https://github.com/benedekrozemberczki/ClusterGCN/blob/master/LICENSE)  """;Graphs;https://github.com/benedekrozemberczki/ClusterGCN
"""- [BERT for dummies](https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03)  tutorial on how to load the pre-trained BERT model in a PyTorch notebook and fine-tune it on your own dataset for solving a specific task by Michel Kana - [Creating a responsive graph with Angular and D3](https://medium.com/@jeanphilippelemieux/creating-a-responsive-graph-with-angular-and-d3-b45bb8065588)  tutorial on d3 + Angular by Jean-Philippe Lemieux - [Tensorflow Quickstart](https://fireship.io/lessons/tensorflow-js-quick-start/)  tutorial on Tensorflow on fireship.io - [Predicting Movie Reviews with BERT on TF Hub.ipynb](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=MC_w8SRqN0fr)  Jupyter Notebook on NLP classification task  - [Visualizing BERT embeddings with t-SNE](https://www.kaggle.com/mateiionita/visualizing-bert-embeddings-with-t-sne) - [Visualizing Algorithms](https://bost.ocks.org/mike/algorithms/)  blogpost on viz adapting Mike Bostock's talk at Eyeo 2014 - [The Illustrated BERT  ELMo  and co.](http://jalammar.github.io/illustrated-bert/)  blogpost on Google BERT by Jay Alammar - [Multi-label Text Classification using BERT – The Mighty Transformer](https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d)  blogpost on Google BERT by Kaushal Trivedi - [Introduction to Flair for NLP](https://www.analyticsvidhya.com/blog/2019/02/flair-nlp-library-python/)  blogpost on FlairNLP Framework by Sharoon Saxena  - [Visualization for Machine Learning (Google Brain)](https://www.youtube.com/watch?v=ulLx2iPTIcs)  video on ML Viz by Google - [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/)  article on t-SNE by Google Researchers - [How to deploy an Angular App](https://medium.com/coinmonks/how-to-deploy-an-angular-app-8db1af39f8c1)  blogpost on deploying Angular app via gh-pages by Yann Mulonda   """;Natural Language Processing;https://github.com/elenalenaelena/machine-learning-viz
"""Main requirements:  Python 3.7.6  PyTorch 1.2.0   conda env create --name awt --file=environment.yml   You will need the WikiText-2 (WT2) dataset. Follow the instructions in: AWD-LSTM to download it   You will need to install wordcloud for the words maps   """;Sequential;https://github.com/S-Abdelnabi/awt
"""To install PyTorch  see installation instructions on the [PyTorch website](pytorch.org).  To install Gym  see installation instructions on the [Gym GitHub repo](https://github.com/openai/gym).   All tutorials use Monte Carlo methods to train the CartPole-v1 environment with the goal of reaching a total episode reward of 475 averaged over the last 25 episodes. There are also alternate versions of some algorithms to show how to use those algorithms with other environments.  * 0 - [Introduction to Gym](https://github.com/bentrevett/pytorch-rl/blob/master/0%20-%20Introduction%20to%20Gym.ipynb)  * 1 - [Vanilla Policy Gradient (REINFORCE)](https://github.com/bentrevett/pytorch-rl/blob/master/1%20-%20Vanilla%20Policy%20Gradient%20(REINFORCE)%20[CartPole].ipynb)      This tutorial covers the workflow of a reinforcement learning project. We'll learn how to: create an environment  initialize a model to act as our policy  create a state/action/reward loop and update our policy. We update our policy with the [vanilla policy gradient algorithm](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)  also known as [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf).      * 2 - [Actor Critic](https://github.com/bentrevett/pytorch-rl/blob/master/2%20-%20Actor%20Critic.ipynb)      This tutorial introduces the family of [actor-critic](https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf) algorithms  which we will use for the next few tutorials.  * 3 - [Advantage Actor Critic (A2C)](https://github.com/bentrevett/pytorch-rl/blob/master/3%20-%20Advantage%20Actor%20Critic%20(A2C)%20[CartPole].ipynb)      We cover an improvement to the actor-critic framework  the [A2C](https://arxiv.org/abs/1602.01783) (advantage actor-critic) algorithm.      * 4 - [Generalized Advantage Estimation (GAE)](https://github.com/bentrevett/pytorch-rl/blob/master/4%20-%20Generalized%20Advantage%20Estimation%20(GAE)%20[CartPole].ipynb)      We improve on A2C by adding [GAE](https://arxiv.org/abs/1506.02438) (generalized advantage estimation).   * 5 - [Proximal Policy Evaluation](https://github.com/bentrevett/pytorch-rl/blob/master/5%20-%20Proximal%20Policy%20Optimization%20(PPO)%20[CartPole].ipynb)      We cover another improvement on A2C  [PPO](https://arxiv.org/abs/1707.06347) (proximal policy optimization).  Potential algorithms covered in future tutorials: DQN  ACER  ACKTR.   """;Reinforcement Learning;https://github.com/bentrevett/pytorch-rl
"""``` python -m visdom.server ```   You can download animal datasets from [here](https://drive.google.com/drive/folders/16A0aPG7takJRat5866sojS1s548gTdmN?usp=sharing). Note that for real images we can only provide drosophila dataset for now. For C. elegans and Zebra fish  we don't own the data. If you need those data  you can find the sources from our paper.  Put the download models in folder ``` ./dataset ```   Clone this repo: ``` git clone https://github.com/siyliepfl/deformation-aware-unpaired-image-translation.git ``` Install dependencies: ``` pip install -r requirements.txt ```   You can also train it by running:   You can also train our model on your custom dataset. You need to prepare 4 folders:    These instructions will get you a copy of the project up and running on your local machine.   """;Computer Vision;https://github.com/siyliepfl/deformation-aware-unpaired-image-translation
"""Can we automatically group images into semantically meaningful clusters when ground-truth annotations are absent? The task of unsupervised image classification remains an important  and open challenge in computer vision. Several recent approaches have tried to tackle this problem in an end-to-end fashion. In this paper  we deviate from recent works  and advocate a two-step approach where feature learning and clustering are decoupled.  We outperform state-of-the-art methods by large margins  in particular +26.6% on CIFAR10  +25.0% on CIFAR100-20 and +21.3% on STL10 in terms of classification accuracy.  Our method is the first to perform well on ImageNet (1000 classes). __Check out the benchmarks on the [Papers-with-code](https://paperswithcode.com/paper/learning-to-classify-images-without-labels) website for [Image Clustering](https://paperswithcode.com/task/image-clustering) and [Unsupervised Image Classification](https://paperswithcode.com/task/unsupervised-image-classification).__   The following files need to be adapted in order to run the code on your own machine: - Change the file paths to the datasets in `utils/mypath.py`  e.g. `/path/to/cifar10`. - Specify the output directory in `configs/env.yml`. All results will be stored under this directory.   Our experimental evaluation includes the following datasets: CIFAR10  CIFAR100-20  STL10 and ImageNet. The ImageNet dataset should be downloaded separately and saved to the path described in `utils/mypath.py`. Other datasets will be downloaded automatically and saved to the correct path when missing.   The code runs with recent Pytorch versions  e.g. 1.4.  Assuming [Anaconda](https://docs.anaconda.com/anaconda/install/)  the most important packages can be installed as: ```shell conda install pytorch=1.4.0 torchvision=0.5.0 cudatoolkit=10.0 -c pytorch conda install matplotlib scipy scikit-learn   #: For evaluation and confusion matrix visualization conda install faiss-gpu                       #: For efficient nearest neighbors search  conda install pyyaml easydict                 #: For using config files conda install termcolor                       #: For colored print statements ``` We refer to the `requirements.txt` file for an overview of the packages in the environment we used to produce our results.   |CIFAR100          | Download  |  |STL10             | Download  |   If you want to see another (more detailed) example for STL-10  checkout [TUTORIAL.md](https://github.com/wvangansbeke/Unsupervised-Classification/blob/master/TUTORIAL.md). It provides a detailed guide and includes visualizations and log files with the training progress.   """;General;https://github.com/wvangansbeke/Unsupervised-Classification
"""Python 3.7  Pytorch 1.0.0  fastai 1.0.52   Note: we recommend starting with a single GPU  as running multiple GPU will require additional hyperparameter tuning.   %run train.py --woof 1 --size 256 --bs 64 --mixup 0.2 --sa 1 --epoch 5  --lr 3e-3  - woof: 0 for Imagenette  1 for Imagewoof (dataset will download automatically) - size: image size - bs: batch size - mixup: 0 for no mixup data augmentation - sa: 1 if we use SimpleSelfAttention  otherwise 0 - sym: 1 if we add symmetry to SimpleSelfAttention (need to have sa=1) - epoch: number of epochs - lr: learning rate - lrfinder: 1 to run learning rate finder  don't train - dump: 1 to print model  don't train - arch: default is 'xresnet50' - gpu: gpu to train on (by default uses all available GPUs??) - log: name of csv file to save training log to (folder path is displayed when running)   For faster training on multiple GPUs  you can try running: python -m fastai.launch train.py (not tested much)    Results using the original self-attention layer are added as a reference.    | Model | Dataset | Image Size | Epochs | Learning Rate |  """;General;https://github.com/sdoria/SimpleSelfAttention
"""celebAHQ: https://github.com/nperraud/download-celebA-HQ  fashionGen: https://fashion-gen.com/   Your checkpoints will be dumped in output_networks/celebaHQ. You should get 1024x1024 generations at the end.   The above command will train the fashionGen model up resolution 256x256. If you want to train fashionGen on a specific sub-dataset for example CLOTHING  run:   To this you can add a ""config"" entry giving overrides to the standard configuration. See models/trainer/standard_configurations to see all possible options. For example:   Then  run your generation with:   You can add optional arguments:   """;General;https://github.com/facebookresearch/pytorch_GAN_zoo
"""2) One-Stage Detectors   To download a dataset from OID:  Step 1: Install git            git clone https://github.com/EscVM/OIDv4_Toolkit.git          !git clone https://github.com/EscVM/OIDv4_Toolkit.git   One will get the OID repo here: https://g.co/dataset/open-images   !pip install icrawler   The first thing we have to do is cloning and building the darknet.The following cells will clone darknet from AlexeyAB's famous repository  adjust the Makefile to enable OPENCV and GPU for darknet and then build darknet.  !git clone https://github.com/AlexeyAB/darknet   !wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137   Note that you would have to change your class names and paths given according to your model. Now remember that you have to give permission to your YOLO model for execution. That can be done using the following code:   """;Computer Vision;https://github.com/Abhi-899/YOLOV4-Custom-Object-Detection
"""This open source book is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See [LICENSE](LICENSE) file.  The sample and reference code within this open source book is made available under a modified MIT license. See the [LICENSE-SAMPLECODE](LICENSE-SAMPLECODE) file.  [Chinese version](https://github.com/d2l-ai/d2l-zh) | [Discuss and report issues](https://discuss.d2l.ai/) | [Code of conduct](CODE_OF_CONDUCT.md) | [Other Information](INFO.md)   """;Computer Vision;https://github.com/d2l-ai/d2l-en
"""DC-GAN with Images: https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_Keras_gan.ipynb     https://soundcloud.com/comeheretohavearave/datasetsound24   """;Computer Vision;https://github.com/djcustard/GestureGAN
"""| 1.    | ViT Pytorch (1.9K) ⭐                                 | https://github.com/lucidrains/vit-pytorch                    |   | 3.    | JeonWorld(161) ⭐                                     | https://github.com/jeonsworld/ViT-pytorch                    |  | 4.    | Vit Tensorflow2                                      | https://github.com/ashishpatel26/Vision-Transformer-Keras-Tensorflow-Pytorch-Examples/blob/main/Vision_Transformer_with_tf2.ipynb |  | 5.    | Vision Transformer Pytorch(80)⭐                      | https://github.com/asyml/vision-transformer-pytorch          |  | 6.    | Pretrained Vision Transformer (70)⭐                  | https://github.com/lukemelas/PyTorch-Pretrained-ViT          |   | 11.   | Vision Transformer       (20) ⭐                      | https://github.com/tahmid0007/VisionTransformer              |   | 13.   | VIT Pytorch(9) ⭐                                     | https://github.com/tczhangzhi/VisionTransformer-Pytorch      |   Tensorflow implementation of the Vision Transformer (ViT) presented in [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://openreview.net/pdf?id=YicbFdNTTy)  where the authors show that Transformers applied directly to image patches and pre-trained on large datasets work really well on image classification.  | Official Paper                                   | Official Code                                                | Article Step By Step                                         | | ------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | | [Download](https://arxiv.org/pdf/2010.11929.pdf) | [Code](https://github.com/google-research/vision_transformer) | [Article](https://jacobgil.github.io/deeplearning/vision-transformer-explainability) |  ---  [![img](https://github.com/emla2805/vision-transformer/raw/master/vit.png)](https://github.com/emla2805/vision-transformer/blob/master/vit.png)   <object data=""https://arxiv.org/pdf/2010.11929.pdf"" type=""application/pdf"" width=""700px"" height=""700px"">     <embed src=""https://arxiv.org/pdf/2010.11929.pdf"">         <p>Vision Transformer<a href=""https://arxiv.org/pdf/2010.11929.pdf"">Download PDF</a>.</p>     </embed> </object>   """;Computer Vision;https://github.com/ashishpatel26/Vision-Transformer-Keras-Tensorflow-Pytorch-Examples
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;General;https://github.com/jahutwb/DL_dosimetry
"""```bash $ pip install sinkhorn_transformer ```   You can follow the instructions here to set it correctly https://github.com/lucidrains/product-key-memory#learning-rates   A Sinkhorn Transformer based language model  ```python import torch from sinkhorn_transformer import SinkhornTransformerLM  model = SinkhornTransformerLM(     num_tokens = 20000      dim = 1024      heads = 8      depth = 12      max_seq_len = 8192      bucket_size = 128         #: size of the buckets     causal = False            #: auto-regressive or not     n_sortcut = 2             #: use sortcut to reduce memory complexity to linear     n_top_buckets = 2         #: sort specified number of key/value buckets to one query bucket. paper is at 1  defaults to 2     ff_chunks = 10            #: feedforward chunking  from Reformer paper     reversible = True         #: make network reversible  from Reformer paper     emb_dropout = 0.1         #: embedding dropout     ff_dropout = 0.1          #: feedforward dropout     attn_dropout = 0.1        #: post attention dropout     attn_layer_dropout = 0.1  #: post attention layer dropout     layer_dropout = 0.1       #: add layer dropout  from 'Reducing Transformer Depth on Demand' paper     weight_tie = True         #: tie layer parameters  from Albert paper     emb_dim = 128             #: embedding factorization  from Albert paper     dim_head = 64             #: be able to fix the dimension of each head  making it independent of the embedding dimension and the number of heads     ff_glu = True             #: use GLU in feedforward  from paper 'GLU Variants Improve Transformer'     n_local_attn_heads = 2    #: replace N heads with local attention  suggested to work well from Routing Transformer paper     pkm_layers = (4 7)        #: specify layers to use product key memory. paper shows 1 or 2 modules near the middle of the transformer is best     pkm_num_keys = 128        #: defaults to 128  but can be increased to 256 or 512 as memory allows )  x = torch.randint(0  20000  (1  2048)) model(x) #: (1  2048  20000) ```  A plain Sinkhorn Transformer  layers of sinkhorn attention  ```python import torch from sinkhorn_transformer import SinkhornTransformer  model = SinkhornTransformer(     dim = 1024      heads = 8      depth = 12      bucket_size = 128 )  x = torch.randn(1  2048  1024) model(x) #: (1  2048  1024) ```  Sinkhorn Encoder / Decoder Transformer  ```python import torch from sinkhorn_transformer import SinkhornTransformerLM  DE_SEQ_LEN = 4096 EN_SEQ_LEN = 4096  enc = SinkhornTransformerLM(     num_tokens = 20000      dim = 512      depth = 6      heads = 8      bucket_size = 128      max_seq_len = DE_SEQ_LEN      reversible = True      return_embeddings = True ).cuda()  dec = SinkhornTransformerLM(     num_tokens = 20000      dim = 512      depth = 6      causal = True      bucket_size = 128      max_seq_len = EN_SEQ_LEN      receives_context = True      context_bucket_size = 128   #: context key / values can be bucketed differently     reversible = True ).cuda()  x = torch.randint(0  20000  (1  DE_SEQ_LEN)).cuda() y = torch.randint(0  20000  (1  EN_SEQ_LEN)).cuda()  x_mask = torch.ones_like(x).bool().cuda() y_mask = torch.ones_like(y).bool().cuda()  context = enc(x  input_mask=x_mask) dec(y  context=context  input_mask=y_mask  context_mask=x_mask) #: (1  4096  20000) ```   """;Natural Language Processing;https://github.com/lucidrains/sinkhorn-transformer
"""**Windows portable version**: Simply download and use the latest version from the [Releases](https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases) section.  Otherwise  check [doc/installation.md](doc/installation.md) for instructions on how to build OpenPose from source.     OS: Ubuntu (14  16)  Windows (8  10)  Mac OSX  Nvidia TX2.   CUDA (Nvidia GPU)  OpenCL (AMD GPU)  and CPU-only (no GPU) versions.   Most users do not need the OpenPose C++/Python API  but can simply use the OpenPose Demo:  - **OpenPose Demo**: To easily process images/video/webcam and display/save the results. See [doc/demo_overview.md](doc/demo_overview.md). E.g.  run OpenPose in a video with: ``` #: Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi ```  - **Calibration toolbox**: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See [doc/modules/calibration_module.md](doc/modules/calibration_module.md).  - **OpenPose C++ API**: If you want to read a specific input  and/or add your custom post-processing function  and/or implement your own display/saving  check the C++ API tutorial on [examples/tutorial_api_cpp/](examples/tutorial_api_cpp/) and [doc/library_introduction.md](doc/library_introduction.md). You can create your custom code on [examples/user_code/](examples/user_code/) and quickly compile it with CMake when compiling the whole OpenPose project. Quickly **add your custom code**: See [examples/user_code/README.md](examples/user_code/README.md) for further details.  - **OpenPose Python API**: Analogously to the C++ API  find the tutorial for the Python API on [examples/tutorial_api_python/](examples/tutorial_api_python/).  - **Adding an extra module**: Check [doc/library_add_new_module.md](./doc/library_add_new_module.md).  - **Standalone face or hand detector**:     - **Face** keypoint detection **without body** keypoint detection: If you want to speed it up (but also reduce amount of detected faces)  check the OpenCV-face-detector approach in [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     - **Use your own face/hand detector**: You can use the hand and/or face keypoint detectors with your own face or hand detectors  rather than using the body detector. E.g.  useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     """;General;https://github.com/yongsheng268/OpenPose
"""::     pip install unet    .. image:: https://img.shields.io/pypi/v/unet.svg          :target: https://pypi.python.org/pypi/unet   """;Computer Vision;https://github.com/fepegar/unet
"""Download the data (PTB  WT2  WT103):  ```bash chmod +x get_data.sh ./get_data.sh ```  For emotions  add in `data/IEMOCAP/` the `all_features_cv` files.  We use python `3.6` with Pytorch `0.4.1`. To create a new python environement and install dependencies  run:  ```bash python3 -m virtualenv venv source venv/bin/activate pip3 install -r requirements.txt ```  You can check your setup by launching a quick training over one epoch with the following command:  ```bash python3 main_run.py --main-model awd-lstm --batch-size 20 --data data/penn --epochs 1 --nhid 5 --emsize 5 --nlayers 1 --bptt 5 ```  The program should exit without error and write the logs in the `logs/` folder. You can watch the logs with tensorboard by launching the following command:  ```bash tensorboard --logdir logs/ ```   Experiments were run on a Tesla P100 GPU. Results are very likely to differ based on the GPU used.   """;Sequential;https://github.com/nkcr/overlap-ml
"""First install Detectron2 following the official guide: [INSTALL.md](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md). Please use Detectron2 with commit id [9eb4831](https://github.com/facebookresearch/detectron2/commit/9eb4831f742ae6a13b8edb61d07b619392fb6543) for now. The incompatibility with the latest one will be fixed soon.  Then build AdelaiDet with:  ``` git clone https://github.com/aim-uofa/AdelaiDet.git cd AdelaiDet python setup.py build develop ```  Some projects may require special setup  please follow their own `README.md` in [configs](configs).   SOLO to be released (mmdet version)  SOLOv2 to be released (mmdet version)   Name | inf. time | e2e-hmean | det-hmean | download   setup the corresponding datasets following   Note that:   Model | Name |inf. time | box AP | mask AP | download --- |:---:|:---:|:---:|:---:|:---: Mask R-CNN | [R_101_3x](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml) | 10 FPS | 42.9 | 38.6 | BlendMask | [R_101_3x](configs/BlendMask/R_101_3x.yaml) | 11 FPS | 44.8 | 39.5 | [model](https://cloudstor.aarnet.edu.au/plus/s/e4fXrliAcMtyEBy/download) BlendMask | [R_101_dcni3_5x](configs/BlendMask/R_101_dcni3_5x.yaml) | 10 FPS | 46.8 | 41.1 | [model](https://cloudstor.aarnet.edu.au/plus/s/vbnKnQtaGlw8TKv/download)  For more models and information  please refer to BlendMask [README.md](configs/BlendMask/README.md).   Name | inf. time | box AP | mask AP | download --- |:---:|:---:|:---:|:---: [MEInst_R_50_3x](https://github.com/aim-uofa/AdelaiDet/configs/MEInst-InstanceSegmentation/MEInst_R_50_3x.yaml) | 12 FPS | 43.6 | 34.5 | [model](https://cloudstor.aarnet.edu.au/plus/s/1ID0DeuI9JsFQoG/download)  For more models and information  please refer to MEInst [README.md](configs/MEInst-InstanceSegmentation/README.md).   Name | inf. time | box AP | mask AP | download --- |:---:|:---:|:---:|:---: [CondInst_MS_R_50_1x](configs/CondInst/MS_R_50_1x.yaml) | 14 FPS | 39.7 | 35.7 | [model](https://cloudstor.aarnet.edu.au/plus/s/Trx1r4tLJja7sLT/download) [CondInst_MS_R_50_BiFPN_3x_sem](configs/CondInst/MS_R_50_BiFPN_3x_sem.yaml) | 13 FPS | 44.7 | 39.4 | [model](https://cloudstor.aarnet.edu.au/plus/s/9cAHjZtdaAGnb2Q/download) [CondInst_MS_R_101_3x](configs/CondInst/MS_R_101_3x.yaml) | 11 FPS | 43.3 | 38.6 | [model](https://cloudstor.aarnet.edu.au/plus/s/vWLiYm8OnrTSUD2/download) [CondInst_MS_R_101_BiFPN_3x_sem](configs/CondInst/MS_R_101_BiFPN_3x_sem.yaml) | 10 FPS | 45.7 | 40.2 | [model](https://cloudstor.aarnet.edu.au/plus/s/2p1ashxl54Su8vv/download)  For more models and information  please refer to CondInst [README.md](configs/CondInst/README.md).  Note that: - Inference time for all projects is measured on a NVIDIA 1080Ti with batch size 1. - APs are evaluated on COCO2017 val split unless specified.    """;Computer Vision;https://github.com/quangvy2703/ABCNet-ESRGAN-SRTEXT
"""These instructions will get you a copy of the project  and generates your own pitching overlay clip!   Get a copy of this project by simply running the git clone command.  ```git git clone https://github.com/chonyy/ML-auto-baseball-pitching-overlay.git ```   <p align=center>     <img src=""img/2_balls_new.gif""> </p> <p align=center>     <img src=""img/4-balls.gif""> </p> <p align=center>     <img src=""img/3-balls-new.gif""> </p> <p align=center>     <img src=""img/3-balls-diff.gif""> </p>   """;Computer Vision;https://github.com/chonyy/ML-auto-baseball-pitching-overlay
"""requirements.txt - File to install all the dependencies   Install Python3.5 (Should also work for python>3.5)  Then install the requirements by running  ``` bash $ pip3 install -r requirements.txt ```  Now to run the training code for binary classification  execute  ``` bash $ python3 bert_siamese.py -num_labels 2 ```  Now to run the training code for 6 class classification  execute  ``` bash $ python3 bert_siamese.py -num_labels 6 ```   """;General;https://github.com/manideep2510/siamese-BERT-fake-news-detection-LIAR
"""Based on the recommendation from HuggingFace  both finetuning and eval are 30% faster with ```--fp16```. For that you need to install ```apex```. ```bash $ git clone https://github.com/NVIDIA/apex $ cd apex $ pip install -v --disable-pip-version-check --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```   Tested with Python 3.7 via virtual environment. Clone the repo  go to the repo folder  setup the virtual environment  and install the required packages: ```bash $ python3.7 -m venv venv $ source venv/bin/activate $ pip install -r requirements.txt ```   $ wget https://cdn-datasets.huggingface.co/summarization/xsum.tar.gz  $ tar -xzvf xsum.tar.gz   """;Natural Language Processing;https://github.com/chz816/esacl
"""The `neural_tangents` (`nt`) package contains the following modules and functions:  * `stax` - primitives to construct neural networks like `Conv`  `Relu`  `serial`  `parallel` etc.  * `predict` - predictions with infinite networks:    * `predict.gradient_descent_mse` - inference with a single infinite width / linearized network trained on MSE loss with continuous gradient descent for an arbitrary finite or infinite (`t=None`) time. Computed in closed form.    * `predict.gradient_descent` - inference with a single infinite width / linearized network trained on arbitrary loss with continuous (momentum) gradient descent for an arbitrary finite time. Computed using an ODE solver.    * `predict.gradient_descent_mse_ensemble` - inference with an infinite ensemble of infinite width networks  either fully Bayesian (`get='nngp'`) or inference with MSE loss using continuous gradient descent (`get='ntk'`). Finite-time Bayesian inference (e.g. `t=1.  get='nngp'`) is interpreted as gradient descent on the top layer only [[11]](#11-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent)  since it converges to exact Gaussian process inference with NNGP (`t=None  get='nngp'`). Computed in closed form.    * `predict.gp_inference` - exact closed form Gaussian process inference using NNGP (`get='nngp'`)  NTK (`get='ntk'`)  or both (`get=('nngp'  'ntk')`). Equivalent to `predict.gradient_descent_mse_ensemble` with `t=None` (infinite training time)  but has a slightly different API (accepting precomputed kernel matrix `k_train_train` instead of `kernel_fn` and `x_train`).  * `monte_carlo_kernel_fn` - compute a Monte Carlo kernel estimate  of _any_ `(init_fn  apply_fn)`  not necessarily specified via `nt.stax`  enabling the kernel computation of infinite networks without closed-form expressions.  * Tools to investigate training dynamics of _wide but finite_ neural networks  like `linearize`  `taylor_expand`  `empirical_kernel_fn` and more. See [Training dynamics of wide but finite networks](#training-dynamics-of-wide-but-finite-networks) for details.    To use GPU  first follow [JAX's](https://www.github.com/google/jax/) GPU installation instructions. Otherwise  install JAX on CPU by running  ``` pip install jax jaxlib --upgrade ```  Once JAX is installed install Neural Tangents by running  ``` pip install neural-tangents ``` or  to use the bleeding-edge version from GitHub source   ``` git clone https://github.com/google/neural-tangents; cd neural-tangents pip install -e . ```  You can now run the examples (using [`tensorflow_datasets`](https://github.com/tensorflow/datasets)) and tests by calling:  ``` pip install tensorflow tensorflow-datasets more-itertools --upgrade  python examples/infinite_fcn.py python examples/weight_space.py python examples/function_space.py  set -e; for f in tests/*.py; do python $f; done ```    | Install guide   both = kernel_fn(x1  x2  ('nngp'  'ntk'))   y_test_nngp = predict_fn(x_test=x_test  get='nngp')   y_test_ntk = predict_fn(x_test=x_test  get='ntk')   both = predict_fn(x_test=x_test  get=('nngp'  'ntk'))   y_test_nngp  y_test_ntk = predict_fn(x_test=x_test  get=('nngp'  'ntk'))   Tested using version 0.2.1. All GPU results are per single accelerator.   ```python import jax.numpy as np import neural_tangents as nt  def apply_fn(params  x):   W  b = params   return np.dot(x  W) + b  W_0 = np.array([[1.  0.]  [0.  1.]]) b_0 = np.zeros((2 ))  apply_fn_lin = nt.linearize(apply_fn  (W_0  b_0)) W = np.array([[1.5  0.2]  [0.1  0.9]]) b = b_0 + 0.2  x = np.array([[0.3  0.2]  [0.4  0.5]  [1.2  0.2]]) logits = apply_fn_lin((W  b)  x)  #: (3  2) np.ndarray ```   ```python import jax.random as random import jax.numpy as np import neural_tangents as nt  def apply_fn(params  x):   W  b = params   return np.dot(x  W) + b  W_0 = np.array([[1.  0.]  [0.  1.]]) b_0 = np.zeros((2 )) params = (W_0  b_0)  key1  key2 = random.split(random.PRNGKey(1)  2) x_train = random.normal(key1  (3  2)) x_test = random.normal(key2  (4  2)) y_train = random.uniform(key1  shape=(3  2))  kernel_fn = nt.empirical_kernel_fn(apply_fn) ntk_train_train = kernel_fn(x_train  None  'ntk'  params) ntk_test_train = kernel_fn(x_test  x_train  'ntk'  params) mse_predictor = nt.predict.gradient_descent_mse(ntk_train_train  y_train)  t = 5. y_train_0 = apply_fn(params  x_train) y_test_0 = apply_fn(params  x_test) y_train_t  y_test_t = mse_predictor(t  y_train_0  y_test_0  ntk_test_train) #: (3  2) and (4  2) np.ndarray train and test outputs after `t` units of time #: training with continuous gradient descent ```   """;General;https://github.com/google/neural-tangents
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`  2. Download VOC2007 and VOC2012 dataset. By default  we assume the data is stored in `$HOME/data/`   ```Shell   #: Download the data.   cd $HOME/data   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar   #: Extract the data.   tar -xvf VOCtrainval_11-May-2012.tar   tar -xvf VOCtrainval_06-Nov-2007.tar   tar -xvf VOCtest_06-Nov-2007.tar   ```  3. Create the LMDB file.   ```Shell   cd $CAFFE_ROOT   #: Create the trainval.txt  test.txt  and test_name_size.txt in data/VOC0712/   ./data/VOC0712/create_list.sh   #: You can modify the parameters in create_data.sh if needed.   #: It will create lmdb files for trainval and test with encoded original image:   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb   #: and make soft links at examples/VOC0712/   ./data/VOC0712/create_data.sh   ```   1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`   ```Shell   git clone https://github.com/weiliu89/caffe.git   cd caffe   git checkout ssd   ```  2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.   ```Shell   #: Modify Makefile.config according to your Caffe installation.   cp Makefile.config.example Makefile.config   make -j8   #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.   make py   make test -j8   #: (Optional)   make runtest -j8   ```   COCO<sup>[1]</sup>: SSD300*  SSD512*  07+12+COCO: SSD300*  SSD512*  07++12+COCO: SSD300*  SSD512*  COCO models:   """;Computer Vision;https://github.com/xzgz/caffe-ssd
"""Download and extract ImageNet train and val images from http://image-net.org/. The directory structure is the standard layout for the torchvision [`datasets.ImageFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder)  and the training and validation data is expected to be in the `train/` folder and `val` folder respectively:  ``` /path/to/imagenet/   train/     class1/       img1.jpeg     class2/       img2.jpeg   val/     class1/       img3.jpeg     class/2       img4.jpeg ```   pip install submitit   First  clone the repository locally: ``` git clone https://github.com/facebookresearch/levit.git ``` Then  install PyTorch 1.7.0+ and torchvision 0.8.1+ and [pytorch-image-models](https://github.com/rwightman/pytorch-image-models):  ``` conda install -c pytorch pytorch torchvision pip install timm ```   """;Computer Vision;https://github.com/zongdaoming/TinyTransformer
"""Recently  channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neuralnetworks (CNNs). However  most existing methods dedicate to developing more sophisticated attention modules for achieving better performance which inevitably increase model complexity. To overcome the paradox of performance and complexity trade-off  this paper proposes an EfficientChannel Attention (ECA) module  which only involves a handful of parameters while bringing clear performance gain. By dissecting the channelattention module in SENet  we empirically show avoiding dimensionality reduction is important for learning channel attention  and appropriatecross-channel interaction can preserve performance while significantly decreasing model complexity. Therefore  we propose a localcross-channel interaction strategy without dimensionality reduction  which can be efficiently implemented via `1D` convolution. Furthermore we develop a method to adaptively select kernel size of `1D` convolution  determining coverage of local cross-channel interaction. Theproposed ECA module is efficient yet effective  e.g.  the parameters and computations of our modules against backbone of ResNet50 are 80 vs.24.37M and 4.7e-4 GFLOPs vs. 3.86 GFLOPs  respectively  and the performance boost is more than 2\% in terms of Top-1 accuracy. We extensivelyevaluate our ECA module on image classification  object detection and instance segmentation with backbones of ResNets and MobileNetV2. Theexperimental results show our module is more efficient while performing favorably against its counterparts.   |Model|Param.|FLOPs|AP|AP_50|AP_75|Pre trained models|Extract code|GoogleDrive| |:----|:----:|:---:|:--:|:-------:|:-------:|:----------------:|:----------:|:---------:| |Mask_R-CNN_ecanet50|44.18M|275.69G|35.6|58.1|37.7|[mask_rcnn_ecanet50_k3377_bs8_lr0.01](https://pan.baidu.com/s/1h_2PgK4PMHa2nTIqU3ZNkQ)|xe19|[mask_rcnn_ecanet50_k3377_bs8_lr0.01](https://drive.google.com/open?id=1z5sAqOD6zZzoVOyAd2VmBztmMXUKy-Nv)| |Mask_R-CNN_ecanet101|63.17M|351.83G|37.4|59.9|39.8|[mask_rcnn_ecanet101_k3357_bs8_lr0.01](https://pan.baidu.com/s/19gph8Sr5nv11_kCecEyu5w)|y5e9|[mask_rcnn_ecanet101_k3357_bs8_lr0.01](https://drive.google.com/open?id=1Rv-VjdfWOt5mE45M0lihgv59fqnCAHQq)| |RetinaNet_ecanet50|37.74M|239.43G|35.6|58.1|37.7|[RetinaNet_ecanet50_k3377_bs8_lr0.01](https://pan.baidu.com/s/1uL-EwILJVeW7O2O3oBmO0A)|my44|[RetinaNet_ecanet50_k3377_bs8_lr0.01](https://drive.google.com/open?id=1Pp6gndZFiZZo2BdbVmzUKQUtQ1m4kIbW)| |RetinaNet_ecanet101|56.74M|315.57G|37.4|59.9|39.8|[RetinaNet_ecanet101_k3357_bs8_lr0.01](https://pan.baidu.com/s/1Rt3ijSItumSlqHFjS4J51w)|2eu5|[RetinaNet_ecanet101_k3357_bs8_lr0.01](https://drive.google.com/open?id=1290duDtOGMpp3QrL47CRAq24oGPgbtct)|   """;General;https://github.com/BangguWu/ECANet
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`  2. Download VOC2007 and VOC2012 dataset. By default  we assume the data is stored in `$HOME/data/`   ```Shell   #: Download the data.   cd $HOME/data   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar   #: Extract the data.   tar -xvf VOCtrainval_11-May-2012.tar   tar -xvf VOCtrainval_06-Nov-2007.tar   tar -xvf VOCtest_06-Nov-2007.tar   ```  3. Create the LMDB file.   ```Shell   cd $CAFFE_ROOT   #: Create the trainval.txt  test.txt  and test_name_size.txt in data/VOC0712/   ./data/VOC0712/create_list.sh   #: You can modify the parameters in create_data.sh if needed.   #: It will create lmdb files for trainval and test with encoded original image:   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb   #: and make soft links at examples/VOC0712/   ./data/VOC0712/create_data.sh   ```   1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`   ```Shell   git clone https://github.com/weiliu89/caffe.git   cd caffe   git checkout ssd   ```  2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.   ```Shell   #: Modify Makefile.config according to your Caffe installation.   cp Makefile.config.example Makefile.config   make -j8   #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.   make py   make test -j8   #: (Optional)   make runtest -j8   ```   COCO<sup>[1]</sup>: SSD300*  SSD512*  07+12+COCO: SSD300*  SSD512*  07++12+COCO: SSD300*  SSD512*  COCO models:   """;Computer Vision;https://github.com/ml-inory/Caffe
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/Nstats/bert_senti_analysis_ch
"""1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   Requirements: hardware  Basic installation   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   To train and test a Faster R-CNN detector using the **alternating optimization** algorithm from our NIPS 2015 paper  use `experiments/scripts/faster_rcnn_alt_opt.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  (""alt opt"" refers to the alternating optimization training algorithm described in the NIPS paper.)  To train and test a Faster R-CNN detector using the **approximate joint training** method  use `experiments/scripts/faster_rcnn_end2end.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  This method trains the RPN module jointly with the Fast R-CNN network  rather than alternating between training the two. It results in faster (~ 1.5x speedup) training times and similar detection accuracy. See these [slides](https://www.dropbox.com/s/xtr4yd4i5e0vw8g/iccv15_tutorial_training_rbg.pdf?dl=0) for more details.  Artifacts generated by the scripts in `tools` are written in this directory.  Trained Fast R-CNN networks are saved under:  ``` output/<experiment directory>/<dataset name>/ ```  Test outputs are saved under:  ``` output/<experiment directory>/<dataset name>/<network snapshot name>/ ```  """;Computer Vision;https://github.com/xzgz/faster-rcnn
"""<img src=""./Report/25_inf.png"">     (first row ~ last row)<br>     0633: this flower has petals that are yellow with red blotches <br>     0194: the flower has white stringy petals with yellow and purple pollen tubes<br>     2014: this flower is pink in color with only one large petal<br>     4683: this flower is yellow in color with petals that are rounded<br>     3327: the flower has a several pieces of yellow colored petals that looks similar to its leaves  network's variable should be update.   <img src=""./Report/25_inf.png"">     (first row ~ last row)<br>     0633: this flower has petals that are yellow with red blotches <br>     0194: the flower has white stringy petals with yellow and purple pollen tubes<br>     2014: this flower is pink in color with only one large petal<br>     4683: this flower is yellow in color with petals that are rounded<br>     3327: the flower has a several pieces of yellow colored petals that looks similar to its leaves           """;Computer Vision;https://github.com/BeyondCloud/Comp04_ReverseImageCaption
"""This repository contains the code for fine-tuning a CLIP model [[Arxiv paper](https://arxiv.org/abs/2103.00020)][[OpenAI Github Repo](https://github.com/openai/CLIP)] on the [ROCO dataset](https://github.com/razorx89/roco-dataset)  a dataset made of radiology images and a caption. This work is done as a part of the [**Flax/Jax community week**](https://github.com/huggingface/transformers/blob/master/examples/research_projects/jax-projects/README.md#quickstart-flax-and-jax-in-transformers) organized by Hugging Face and Google.  [[🤗 Model card]](https://huggingface.co/flax-community/medclip-roco) [[Streamlit demo]](https://huggingface.co/spaces/kaushalya/medclip-roco)   This repo depends on the master branch of [Hugging Face - Transformers library](https://github.com/huggingface/transformers). First you need to clone the transformers repository and then install it locally (preferably inside a virtual environment) with `pip install -e "".[flax]""`.   You can try a Streamlit demo app that uses this model on [🤗 Spaces](https://huggingface.co/spaces/kaushalya/medclip-roco). You may have to signup for 🤗 Spaces private beta to access this app (screenshot shown below). ![Streamlit app](./assets/streamlit_app.png)   """;Computer Vision;https://github.com/Kaushalya/medclip
"""- Install QT:  ``` sudo apt-get install build-essential sudo apt-get install qt5-default qtcreator qt5-doc qt5-doc-html qtbase5-doc-html qtbase5-examples -y sudo /sbin/ldconfig -v ```  - Install OpenCV  ``` https://linuxize.com/post/how-to-install-opencv-on-ubuntu-18-04/ ```  - Install protobuf 3.6.1  ``` https://github.com/protocolbuffers/protobuf ```   I created an image of my SD card [here](https://1drv.ms/u/s!Av71xxzl6mYZgdZxDoxDnxR-sOERSw?e=S2o1uR). You can flash and run this image on Jetson Nano.  **Note:**  - The source code and binary files in this SD card image is older than in `master` branch. Please upgrade to the lastest source code and recompile on your device. - Use Alt+F4 to exit the GUI and start editing your source code. - Login information:     + Username: `smartcam`.     + Password: Open Terminal and type `sudo passwd smartcam` to change the password.   Note: The paths can be different on your computer.   sudo apt-get install qttools5-dev-tools libqt5svg5-dev qtmultimedia5-dev  Issue:  Need to specify CUDA root   You should change the path corresponding to your environment.   """;Computer Vision;https://github.com/vietanhdev/open-adas
"""I highly recommend to check a sychronous version and other algorithms: pytorch-a2c-ppo-acktr.   ```bash #: Works only wih Python 3. python3 main.py --env-name ""PongDeterministic-v4"" --num-processes 16 ```  This code runs evaluation in a separate thread in addition to 16 processes.   """;Reinforcement Learning;https://github.com/ikostrikov/pytorch-a3c
"""pip install -r requirements.txt   """;General;https://github.com/gaborvecsei/Barlow-Twins
""" This package can be installed from pip with:  ```  pip install delira-cycle-gan  ```  or from source via:  ```  pip install git+https://github.com/justusschock/delira_cycle_gan_pytorch  ```      <img src=""https://junyanz.github.io/CycleGAN/images/teaser_high_res.jpg"" width=""800""/>   """;General;https://github.com/justusschock/delira_cycle_gan_pytorch
"""Training GAN is hard. Models may never converge and mode collapses are common. <br>  When learning generative models  we assume the data we have comes from some unknown distribution <img src='./readme_images/pr.png' />. (The r stands for real) We want to learn a distribution <img src='./readme_images/ptheta.png' />​​ that approximates <img src='./readme_images/pr.png' />  where θ are the parameters of the distribution. <br> You can imagine two approaches for doing this. <br> - Directly learn the probability density function <img src='./readme_images/ptheta.png' />​​. We optimize <img src='./readme_images/ptheta.png' />​​ through maximum likelihood estimation.  - Learn a function that transforms an existing distribution Z into <img src='./readme_images/ptheta.png' />​​.   The first approach runs into problems. Given function <img src='./readme_images/ptheta.png' />​​​​  the MLE objective is <br> <img src='./readme_images/eqn13.png' />​​ <br> In the limit  this is equivalent to minimizing the KL-divergence. <br> <img src='./readme_images/eqn14.png' />​​ <br> <img src='./readme_images/eqn15.png' />​​ <br>  Variational Auto-Encoders (VAEs) and Generative Adversarial Networks (GANs) are well known examples of this approach.   """;Computer Vision;https://github.com/Mohammad-Rahmdel/WassersteinGAN-Tensorflow
"""  pip install kornia    pip install kornia[x]  #: to get the training API !     python setup.py install    pip install -e .    pip install git+https://github.com/kornia/kornia   GitHub Issues: bug reports  feature requests  install issues  RFCs  thoughts  etc. OPEN   Run our Jupyter notebooks [tutorials](https://kornia-tutorials.readthedocs.io/en/latest/) to learn to use the library.  <div align=""center"">   <a href=""https://colab.research.google.com/github/kornia/tutorials/blob/master/source/hello_world_tutorial.ipynb"" target=""_blank"">     <img src=""https://raw.githubusercontent.com/kornia/data/main/hello_world_arturito.png"" width=""75%"" height=""75%"">   </a> </div>  :triangular_flag_on_post: **Updates** - :white_check_mark: Integrated to [Huggingface Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See [Gradio Web Demo](https://huggingface.co/spaces/akhaliq/Kornia-LoFTR).   """;Computer Vision;https://github.com/kornia/kornia
"""  ![Summary](/images/summary.PNG)    """;Natural Language Processing;https://github.com/Raman-Raje/Machine-Reading-Comprehension-Neural-Question-Answer-
"""For this project  you will work with the [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment.  ![Trained Agent][image1]  In this environment  a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus  the goal of your agent is to maintain its position at the target location for as many time steps as possible.  The observation space consists of 33 variables corresponding to position  rotation  velocity  and angular velocities of the arm. Each action is a vector with four numbers  corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.   - The second version contains 20 identical agents  each with its own copy of the environment.     1. Download the environment from one of the links below.  You need only select the environment that matches your operating system:      - **_Version 1: One (1) Agent_**         - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Linux.zip)         - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher.app.zip)         - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Windows_x86.zip)         - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Windows_x86_64.zip)      - **_Version 2: Twenty (20) Agents_**         - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux.zip)         - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher.app.zip)         - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86.zip)         - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86_64.zip)          (_For Windows users_) Check out [this link](https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64) if you need help with determining if your computer is running a 32-bit version or 64-bit version of the Windows operating system.      (_For AWS_) If you'd like to train the agent on AWS (and have not [enabled a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md))  then please use [this link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Linux_NoVis.zip) (version 1) or [this link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux_NoVis.zip) (version 2) to obtain the ""headless"" version of the environment.  You will **not** be able to watch the agent without enabling a virtual screen  but you will be able to train the agent.  (_To watch the agent  you should follow the instructions to [enable a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md)  and then download the environment for the **Linux** operating system above._)  2. Place the file in the DRLND GitHub repository  in the `TD3_continuous_control/` folder  and unzip (or decompress) the file.  3. Rename the file to Reacher 4. Install environment      1. pip install matplotlib     2. pip install mlagents     3. pip install numpy     4. pip install tensorboardx     5. pip install tensorboard 5. In the `TD3_continuous_control/` folder run command: `python train.py --eval_load_best=True` or `python train.py --eval_load_best=True --slow_and_pretty=True` for a slow representation  """;Reinforcement Learning;https://github.com/Zartris/TD3_continuous_control
"""Install with:  pip install ""git+https://github.com/andrewlstewart/StereoNet_PyTorch""       https://github.com/zhixuanli/StereoNet/issues/12#issuecomment-508327106   """;General;https://github.com/andrewlstewart/StereoNet_PyTorch
"""MJPEG / imshow optional visual output;   You got the idea.   Sometimes you just do not need full OpenCV installation for object detection. I have such ANPR projet here: https://github.com/LdDl/license_plate_recognition  I guess when I'm done with stable core I might switch from Go's Darknet bindings to OpenCV one (since ODaM-project requires OpenCV installation obviously)   If you are here  then you are already helped a lot  since you noticed my existence :harold_face:   Need to enable CUDA (GPU) in every installation step where it's possible.  Install CUDA (we recommend version 10.2)      bash      wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin      sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600       sudo apt-get update      sudo apt-get -y install cuda       source ~/.bashrc  Install cuDNN (we recommend version v7.6.5 (November 18th  2019)  for CUDA 10.2)      Go to NVIDIA's site and download .deb package. After downloading .deb package install it:      bash       Do not forget to check if cuDNN installed properly:      bash       cd  $HOME/cudnn_samples_v7/mnistCUDNN       cd -       bash      git clone https://github.com/AlexeyAB/darknet      cd ./darknet       git checkout f056fc3b6a11528fa0522a468eca1e909b7004b7      #: Enable GPU acceleration      sed 's/GPU=0/GPU=1/' ./Makefile      #: Enable cuDNN       make       Alternatively you can use Makefile from go-darknet repository: https://github.com/LdDl/go-darknet/blob/master/Makefile   GoCV - instructions link.   You need to implement your gRPC server as following proto-file: https://github.com/LdDl/odam/blob/master/yolo_grpc.proto.   go install github.com/LdDl/odam/cmd/odam       ./download_data_v4.sh           ""source"": ""rtsp://127.0.0.1:554/h264""  #: Link to RTSP stream   """;Computer Vision;https://github.com/LdDl/odam
"""You can add more pretrained models from upscale.wiki.   """;General;https://github.com/olaviinha/NeuralImageSuperResolution
"""To get a local copy up and running follow these simple steps:  1. Clone the repo ```sh git clone https://github.com/LorenzoAgnolucci/BERT_for_ABSA.git ``` 2. Run ```pip install -r requirements.txt``` in the root folder of the repo to install the requirements    1. Run ```generate_datasets.py``` to build the datasets corresponding to each model or simply use the ones provided in ```data/sentihood/``` and ```data/semeval2014/```  2. Use the forms in ```BERT_for_ABSA.ipynb``` to choose the desired dataset type and task both for BERT-single and BERT-pair. Then fine-tune the model and evaluate it runnning the corresponding cells  3. Run the subsequent cells in ```BERT_for_ABSA.ipynb``` to fine-tune and evalaute the model   """;Natural Language Processing;https://github.com/LorenzoAgnolucci/BERT_for_ABSA
"""All code runs on Python 3.6.7 using [PyTorch version 1.0.0](https://github.com/pytorch/pytorch/tree/v1.0.0).  Our implementations build on the `torch.distributed` package in PyTorch  which provides an interface for exchanging tensors between multiple machines. The `torch.distributed` package in PyTorch v.1.0.0 can use different backends. We recommmend using NCCL for all algorithms (this is the default).  To install the Stochastic Gradient Push library  via pip: ```bash git clone https://github.com/facebookresearch/stochastic_gradient_push.git cd stochastic_gradient_push pip install . ```  If you want to use the parsing scripts to parse results  you can instead do: ```bash git clone https://github.com/facebookresearch/stochastic_gradient_push.git cd stochastic_gradient_push pip install -e .[parse] ```   The job_scripts/ directory contains the following files:   In all cases  the scripts will need to be editied/modified in order to run on your cluster/setup. They also contain instructions on how to modify the script  e.g.  to vary the number of nodes or other parameters.   """;Computer Vision;https://github.com/facebookresearch/stochastic_gradient_push
"""See https://github.com/arogers1/VAE_LSTM_Text_Encoding/blob/master/Variational%20Autoencoder%20LSTM%20Text%20Encoding.ipynb for a Jupyter notebook demoing the code on a dataset built from NLTK corpora.   """;General;https://github.com/arogers1/VAE_LSTM_Text_Encoding
"""Python 3.6  PyTorch  Tensorflow (for TensorBoard)   TensorBoard logger (https://github.com/TeamHG-Memex/tensorboard_logger)   $ sh setting_up_script.sh   """;General;https://github.com/AlexiaJM/Deep-learning-with-cats
"""- A complete checkpoint folder must be placed under `logs\`. Use the entire folder pytorch-lightning automatically saves.   [x] Rewrite using pytorch-lightning framework   """;Natural Language Processing;https://github.com/Rick-McCoy/Reformer-pytorch
"""Create a virtualenv using virtualenv python=python3 .venv. Run source .venv/bin/activate to start the environment  and deactivate to close it.  Install dependencies using pip install -r requirements.txt   The following information is needed in order to get this project up and running on your system.   """;Computer Vision;https://github.com/SDBurt/SRGAN-PT
"""Note:    Model   | CUDA<br/>/cuDNN | #Node | GPU Card<br/>(per node) | Batch Size<br/>(per GPU) | kvstore | GPU Mem<br/>(per GPU) | Training Speed*<br/>(per node)   - Download link: GoogleDrive   - Download link: GoogleDrive   - ~~Download link: [GoogleDrive: https://goo.gl/kNZC4j]~~  - Download link: GoogleDrive   """;Computer Vision;https://github.com/cypw/DPNs
"""Implementation with PyTorch 1.3.0 for multi-gpu DARTS https://arxiv.org/abs/1806.09055   This code is also based on https://github.com/alphadl/darts.pytorch1.1  yet with some bug fixed  so that it can perform better on multi-gpu.   cd rnn &amp;&amp; python train_search.py --gpu 0 1    #: for recurrent cells on PTB   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/AlvinIsonomia/darts.pytorch1.3_MultiGPU
"""Install TensorFlow version >= 1.13 for both GCE VM and Cloud.        """;Computer Vision;https://github.com/pikkaay/efficientnet_gpu
"""Install kitti_native_evaluation offline evaluation:  cd kitti_native_evaluation                 If you want to use a single GPU  you might also need to reduce the batch size by half to save GPU memory.   """;Computer Vision;https://github.com/WeijingShi/Point-GNN
"""  * <a href=""https://modeldepot.io/oandrienko/pspnet50-for-urban-scene-understanding"">**PSPNet50 Inference Notebook**</a><br>   * <a href=""https://modeldepot.io/oandrienko/icnet-for-fast-segmentation"">**ICNet Inference Notebook**</a><br>   """;Computer Vision;https://github.com/oandrienko/fast-semantic-segmentation
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/xuantruongdang/module-yolo
"""      probabilistic implementation of https://arxiv.org/abs/1410.5401 in julia        main.jl encapsulate of the function       con.jl contains definition of controller       utils.jl contains utility functions like content_finding  gated_interpolation etc        ntm.jl encapsulate all the modules       heads.jl contains read_head and write_head function's  """;Sequential;https://github.com/shanyaanand/ntm
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/solapark/darknet_partdet
"""``` pip install -r requirements.txt pip install pafy youtube-dl ```  For the tflite runtime  you can either use tensorflow(make sure it is version 2.6.0 or above) `pip install tensorflow==2.6.0` or the [TensorFlow Runtime binary](https://github.com/PINTO0309/TensorflowLite-bin)    * **Image inference**:    ```  python imageDepthEstimation.py   ```     * **Video inference**:    ```  python videoDepthEstimation.py  ```    * **DrivingStereo dataset inference**:    ```  python drivingStereoTest.py  ```      ![Hitnet stereo depth estimation on video Raspberry Pi 4](https://github.com/ibaiGorordo/TFLite-HITNET-Stereo-depth-estimation/blob/main/docs/img/Pi4tfliteHitnetDepthEstimation.gif)   """;Computer Vision;https://github.com/ibaiGorordo/TFLite-HITNET-Stereo-depth-estimation
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/hukangli/https-github.com-deepinsight-insightface
"""1. Clone this repo and install requirements     ```command    git clone https://github.com/L0SG/NanoFlow.git    cd NanoFlow    pip install -r requirements.txt    ```  2. Install [Apex] for mixed-precision training    Below are the example commands using nanoflow-h16-r128-emb512.json      insert `checkpoint_path: ""experiments/nanoflow-h16-r128-emb512/waveflow_5000""` in the config file then run    ```command    python train.py -c configs/nanoflow-h16-r128-emb512.json    ```     for loading averaged weights over 10 recent checkpoints  insert `checkpoint_path: ""experiments/nanoflow-h16-r128-emb512""` in the config file then run    ```command    python train.py -a 10 -c configs/nanoflow-h16-r128-emb512.json    ```     you can reset the optimizer and training scheduler (and keep the weights) by providing `--warm_start`    ```command    python train.py --warm_start -c configs/nanoflow-h16-r128-emb512.json    ```     4. Synthesize waveform from the trained model.     insert `checkpoint_path` in the config file and use `--synthesize` to `train.py`. The model generates waveform by looping over `test_files.txt`.    ```command    python train.py --synthesize -c configs/nanoflow-h16-r128-emb512.json    ```    if `fp16_run: true`  the model uses FP16 (half-precision) arithmetic for faster performance (on GPUs equipped with Tensor Cores).    """;General;https://github.com/L0SG/NanoFlow
"""This project is based on Python 3  [Tensorflow](https://www.tensorflow.org)  and the [OpenAI Gym environments](https://gym.openai.com). It's been tested on various Atari environments  although the basic algorithm can easily be applied to other scenarios.  To install the python requirements  run `pip3 install -r requirements.txt` (although you may want to create a [virtual environment](https://docs.python.org/3/tutorial/venv.html) first). The video recorder also requires [ffmepg](https://ffmpeg.org) which must be installed separately.  To run an environment  use e.g.      python3 atari_ppo.py --logdir=./logdata --pfile=../example-pong-params.json  With the example parameters  the agent should be able to win a perfect game of Pong in about 2 million frames  which closely matches the results from the OpenAI baseline implementation. Other environments can be used by modifying the parameters file. To view the training progress  use tensorboard:      tensorboard --logdir=./logdata    ![a game of pong](./pong.gif) ![a game of space invaders](./space_invaders.gif)    """;Reinforcement Learning;https://github.com/clwainwright/proximal_policy_optimization
"""Armstrong — 'That's one small step for a man  one giant leap for mankind'.   Links to Download:   Alternate Links to Download:   Version 1.0  released on 17/11/2017.   """;Computer Vision;https://github.com/curto2/c
"""If you want to configure one or more parameters use:   """;General;https://github.com/julik43/IIMAS-USCS
"""Install the Google compute platform on Linux   : Create environment variable for correct distribution   : Update the package list and install the Cloud SDK  sudo apt-get update && sudo apt-get install google-cloud-sdk   Conda install fastai packages  sudo /opt/anaconda3/bin/conda install -c fastai fastai   """;General;https://github.com/qiaolinhan/ws-preprocess
"""Supports Keras with Theano and Tensorflow backend. Due to recent report that Theano will no longer be updated  Tensorflow is the default backend for this project now.  Requires Pillow  imageio  sklearn  scipy  keras 2.3.1  tensorflow 1.15.0  **Note**: The project is going to be reworked. Therefore please refer to [Framework-Updates.md](https://github.com/titu1994/Image-Super-Resolution/blob/master/Framework-Update.md) to see the changes which will affect performance.  The model weights are already provided in the weights folder  therefore simply running :<br> `python main.py ""imgpath""`  where imgpath is a full path to the image.  The default model is DDSRCNN (dsr)  which outperforms the other three models. To switch models <br> `python main.py ""imgpath"" --model=""type""`  where type = `sr`  `esr`  `dsr`  `ddsr`  If the scaling factor needs to be altered then :<br> `python main.py ""imgpath"" --scale=s`  where s can be any number. Default `s = 2`  If the intermediate step (bilinear scaled image) is needed  then:<br> `python main.py ""imgpath"" --scale=s --save_intermediate=""True""`   There are 14 extra images provided in results  2 of which (Monarch Butterfly and Zebra) have been scaled using both bilinear  SRCNN  ESRCNN and DSRCNN.   """;General;https://github.com/titu1994/Image-Super-Resolution
"""Please follow the instructions of py-faster-rcnn [here](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to setup VOC and COCO datasets (Part of COCO is done). The steps involve downloading data and optionally creating soft links in the ``data`` folder. Since faster RCNN does not rely on pre-computed proposals  it is safe to ignore the steps that setup proposals.  If you find it useful  the ``data/cache`` folder created on my side is also shared [here](http://ladoga.graphics.cs.cmu.edu/xinleic/tf-faster-rcnn/cache.tgz).   1. Clone the repository   ```Shell   git clone https://github.com/endernewton/tf-faster-rcnn.git   ```  2. Update your -arch in setup script to match your GPU   ```Shell   cd tf-faster-rcnn/lib   #: Change the GPU architecture (-arch) if necessary   vim setup.py   ```    | GPU model  | Architecture |   | ------------- | ------------- |   | TitanX (Maxwell/Pascal) | sm_52 |   | GTX 960M | sm_50 |   | GTX 1080 (Ti) | sm_61 |   | Grid K520 (AWS g2.2xlarge) | sm_30 |   | Tesla K80 (AWS p2.xlarge) | sm_37 |    **Note**: You are welcome to contribute the settings on your end if you have made the code work properly on other GPUs. Also even if you are only using CPU tensorflow  GPU based code (for NMS) will be used by default  so please set **USE_GPU_NMS False** to get the correct output.   3. Build the Cython modules   ```Shell   make clean   make   cd ..   ```  4. Install the [Python COCO API](https://github.com/pdollar/coco). The code requires the API to access COCO dataset.   ```Shell   cd data   git clone https://github.com/pdollar/coco.git   cd coco/PythonAPI   make   cd ../../..   ```     - Due to the randomness in GPU training with Tensorflow especially for VOC  the best numbers are reported (with 2-3 attempts) here. According to my experience  for COCO you can almost always get a very close number (within ~0.2%) despite the randomness.      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz     tar -xzvf vgg_16_2016_08_28.tar.gz      cd ../..     For Resnet101  you can set up like:      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz     tar -xzvf resnet_v1_101_2016_08_28.tar.gz      cd ../..     ./experiments/scripts/train_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/train_faster_rcnn.sh 1 coco res101     ./experiments/scripts/test_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/test_faster_rcnn.sh 1 coco res101   1. Download pre-trained model   ```Shell   #: Resnet101 for voc pre-trained on 07+12 set   ./data/scripts/fetch_faster_rcnn_models.sh   ```   **Note**: if you cannot download the models through the link  or you want to try more models  you can check out the following solutions and optionally update the downloading script:   - Another server [here](http://xinlei.sp.cs.cmu.edu/xinleic/tf-faster-rcnn/).   - Google drive [here](https://drive.google.com/open?id=0B1_fAEgxdnvJSmF3YUlZcHFqWTQ).  2. Create a folder and a soft link to use the pre-trained model   ```Shell   NET=res101   TRAIN_IMDB=voc_2007_trainval+voc_2012_trainval   mkdir -p output/${NET}/${TRAIN_IMDB}   cd output/${NET}/${TRAIN_IMDB}   ln -s ../../../data/voc_2007_trainval+voc_2012_trainval ./default   cd ../../..   ```  3. Demo for testing on custom images   ```Shell   #: at repository root   GPU_ID=0   CUDA_VISIBLE_DEVICES=${GPU_ID} ./tools/demo.py   ```   **Note**: Resnet101 testing probably requires several gigabytes of memory  so if you encounter memory capacity issues  please install it with CPU support only. Refer to [Issue 25](https://github.com/endernewton/tf-faster-rcnn/issues/25).  4. Test with pre-trained Resnet101 models   ```Shell   GPU_ID=0   ./experiments/scripts/test_faster_rcnn.sh $GPU_ID pascal_voc_0712 res101   ```   **Note**: If you cannot get the reported numbers (79.8 on my side)  then probably the NMS function is compiled improperly  refer to [Issue 5](https://github.com/endernewton/tf-faster-rcnn/issues/5).   """;Computer Vision;https://github.com/wangzpeng/tf-faster-rcnn
"""Install PyTorch and ImageNet dataset following the [official PyTorch ImageNet training code](https://github.com/pytorch/examples/tree/master/imagenet).  This repo aims to be minimal modifications on that code. Check the modifications by: ``` diff main_moco.py <(curl https://raw.githubusercontent.com/pytorch/examples/master/imagenet/main.py) diff main_lincls.py <(curl https://raw.githubusercontent.com/pytorch/examples/master/imagenet/main.py) ```      --pretrained [your checkpoint path]/checkpoint_0199.pth.tar \   """;General;https://github.com/Evgeneus/MoCo
"""If the test file has sentiment labels  just run the following command:   And then  run the follow code to get the transferred review:bash   1. Run the http server to allow the js. script. ```bash python3 -m web/run_server.sh & ``` 2. Visit web/demo.htm to watch the demo.  ***************************************************************   """;General;https://github.com/sy-sunmoon/Clever-Commenter-Let-s-Try-More-Apps
"""<img src=""https://badges.frapsoft.com/os/v1/open-source.svg?v=103"">   <img src=""https://img.shields.io/badge/python-v3.7%2B-orange"" />   <p align=""justify"">On the terminal run the following commands-</p>           Install all dependencies           &lt;code&gt; pip install numpy &lt;/code&gt;           &lt;code&gt; pip install matplotlib &lt;/code&gt;   """;General;https://github.com/matakshay/Neural_Image_Caption_Generator
"""* cd math23k   * cd mawps   """;Sequential;https://github.com/2003pro/Graph2Tree
"""That concludes this brief introduction into the wonderful world of Reinforcement Learning. Hopefully this provides you with a good foundation of understanding of the topic and can help springboard you into starting RL projects of your own. <br><br> Addtionally  it is important to note that RL is extremely expensive computationally. To train the Atari games  for example  even with a high end GPU  it could take multiple days of constant training. That being said  more simple environments can be trained is as little as 30 minutes to 1 hour.  <br> <br> Also important to note is that RL models are highly unstable and unpredictable. Even using the same seed for random number generation  the results of training may vary wildly from one training session to another. While it may have only taken 500 games to train the first time  the second time it might take 1000 games to reach the same performance. <br><br> There are also many hyperparameters to adjust  from the epsilon values to the layers of the neural network to the size of the batch in experience replay. Finding the right balance of these variables can be time-consuming  adding to the time sink that RL has the possibility to become. <br><br> That being said  it is extremely fulfilling when you see that your AI has actually started to learn and you see scores start climbing. <br><br> Beware  you may just find yourself become emotional involved in the success of your model.   You may have heard that the two types of learning are supervised and unsupervised - either you are training a model to correctly assign labels or training a model to group similar items together. There is  however  a third breed of learning: reinforcement learning. Reinforcement learning seeks to train a model make a sequence of decisions.   Gym can be install using pip:  <b>pip install gym</b> <br>   Windows users  if you want to run Atari environments  you will have to also install this: <br><br>  <b>pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py</b>   env = gym.make('MsPacman-v4')   Calling <b>reset()</b> on the environment will do just that - reset it to a new game. <br> <br>   """;Reinforcement Learning;https://github.com/matthewsparr/Reinforcement-Learning-Lesson
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/paolanu/BERT_epitope
"""The articulated 3D pose of the human body is high-dimensional and complex.  Many applications make use of a prior distribution over valid human poses  but modeling this distribution is difficult. Here we present VPoser  a learning based variational human pose prior trained from a large dataset of human poses represented as SMPL bodies. This body prior can be used as an Inverse Kinematics (IK) solver for many tasks such as fitting a body model to images  as the main contribution of this repository for [SMPLify-X](https://smpl-x.is.tue.mpg.de/).  VPoser has the following features:   - defines a prior of SMPL pose parameters  - is end-to-end differentiable  - provides a way to penalize impossible poses while admitting valid ones  - effectively models correlations among the joints of the body  - introduces an efficient  low-dimensional  representation for human pose  - can be used to generate valid 3D human poses for data-dependent tasks   **Requirements** - Python 3.7 - [PyTorch 1.7.1](https://pytorch.org/get-started)  [comment]: <> (- [Torchgeometry 0.1.2]&#40;https://pypi.org/project/torchgeometry/0.1.2/&#41;)  [comment]: <> (- [Body Visualizer]&#40;https://github.com/nghorbani/body_visualizer&#41; for visualizations)     Clone this repo and run the following from the root folder: ```bash python install -r requirements.txt python setup.py develop ```   ![alt text](support_data/latent_interpolation_1.gif ""Interpolation of novel poses on the smoother VPoser latent space."") ![alt text](support_data/latent_interpolation_2.gif ""Interpolation of novel poses on the smoother VPoser latent space."")  * [VPoser Body poZ Space for SMPL Body Model Family](tutorials/vposer.ipynb) * [Sampling Novel Body Poses with VPoser](tutorials/vposer_sampling.ipynb)   """;Computer Vision;https://github.com/nghorbani/human_body_prior
"""```bash $ cd data $ bash download.sh CelebA (404 not found) #: or $ bash download.sh LSUN #: For Hearthstone player $ mkdir hearthstone-card-images $ cd hearthstone-card-images $ wget https://www.dropbox.com/s/vvaxb4maoj4ri34/hearthstone_card.zip?dl=0 $ unzip hearthstone_card.zip?dl=0  ```   $ git clone https://github.com/heykeetae/Self-Attention-GAN.git  $ cd Self-Attention-GAN  : for conda user  $ conda create -n sagan python=3.5  $ conda activate sagan  $ conda install pytorch=0.3.0  $ pip install -r requirements.txt   $ cd samples/sagan_celeb   $ cd samples/sagan_lsun   $ cd samples/sagan_hearth_at1   Colormap from opencv(https://docs.opencv.org/2.4/modules/contrib/doc/facerec/colormaps.html)   """;General;https://github.com/hinofafa/Self-Attention-HearthStone-GAN
"""Available | Source  | **   Available | Source  | **   Age | DenseNet121 | 10K |  ❌ |  VBM | download   Sex | DenseNet121 | 10K |  ❌️ | VBM | download   """;General;https://github.com/Duplums/bhb10k-dl-benchmark
"""or you can train with regular softmax:   """;General;https://github.com/rosinality/adaptive-softmax-pytorch
"""-   Acquire the data  e.g. as a snapshot called `256x256.zip` in [my data repository][data-repository]  -   Run [`StyleGAN2_ADA_training.ipynb`][colab-notebook-training] to train a StyleGAN2-ADA model from scratch. [![Open In Colab][colab-badge]][colab-notebook-training] -   Run [`StyleGAN2_ADA_image_sampling.ipynb`][colab-notebook-sampling] to generate images with a trained StyleGAN2-ADA model  [![Open In Colab][colab-badge]][colab-notebook-sampling] -   To automatically resume training from the latest checkpoint  you will have to use [my fork][stylegan2-ada-fork] of StyleGAN2-ADA.   """;General;https://github.com/woctezuma/steam-stylegan2-ada
"""Version 1.0 is available here: https://dl.fbaipublicfiles.com/anli/anli_v1.0.zip.   transformers==3.0.2 or later (tested: 3.0.2  3.1.0  4.0.0)   #: Note:   <a name=""albert"">ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli</a> | 76.0 | 57.0 | 57.0 | 73.6 | 58.6 | 53.4   """;Natural Language Processing;https://github.com/facebookresearch/anli
"""So that's the gist of this project – I'm looking to make old photos look reeeeaaally good with GANs  and more importantly  make the project *useful*.  And yes  I'm definitely interested in doing video  but first I need to sort out how to get this model under control with memory (it's a beast).  It'd be nice if the models didn't take two to three days to train on a 1080TI as well (typical of GANs  unfortunately). In the meantime though this is going to be my baby and I'll be actively updating and improving the code over the foreseable future.  I'll try to make this as user-friendly as possible  but I'm sure there's going to be hiccups along the way.    Oh and I swear I'll document the code properly...eventually.  Admittedly I'm *one of those* people who believes in ""self documenting code"" (LOL).   This project is built around the wonderful Fast.AI library.  Unfortunately  it's the -old- version and I have yet to upgrade it to the new version.  (That's definitely on the agenda.)  So prereqs  in summary: * ***Old* Fast.AI library**  After being buried in this project for two months I'm a bit lost as to what happened to the old Fast.AI library because the one marked ""old"" doesn't really look like the one I have.  This all changed in the past two months or so.  So if all else fails you should be able to use the one I forked here: https://github.com/jantic/fastai .  Again  getting upgraded to the latest Fast.AI is on the agenda fo sho  and I apologize in advance. * **Whatever dependencies Fast.AI has** – there's already convenient requirements.txt and environment.yml there. * **Pytorch 0.4.1** (needs spectral_norm  so  latest stable release is needed). * **Jupyter Lab** * **Tensorboard** (i.e. install Tensorflow) and **TensorboardX** (https://github.com/lanpa/tensorboardX).  I guess you don't *have* to but man  life is so much better with it.  And I've conveniently provided hooks/callbacks to automatically write all kinds of stuff to tensorboard for you already!  The notebooks have examples of these being instantiated (or commented out since I didn't really need the ones doing histograms of the model weights).  Noteably  progress images will be written to Tensorboard every 200 iterations by default  so you get a constant and convenient look at what the model is doing.  * **ImageNet** – It proved to be a great dataset for training.   * **BEEFY Graphics card**.  I'd really like to have more memory than the 11 GB in my GeForce 1080TI (11GB).  You'll have a tough time with less.  The Unet and Critic are ridiculously large but honestly I just kept getting better results the bigger I made them.    **For those wanting to start transforming their own images right away:** To start right away with your own images without training the model yourself (understandable)...well  you'll need me to upload pre-trained weights first.  I'm working on that now.  Once those are available  you'll be able to refer to them in the visualization notebooks. I'd use ColorizationVisualization.ipynb.  Basically you'd replace   colorizer_path = IMAGENET.parent/('bwc_rc_gen_192.h5')   With the weight file I upload for the generator (colorizer).  Then you'd just drop whatever images in the /test_images/ folder you want to run this against and you can visualize the results inside the notebook with lines like this:  vis.plot_transformed_image(""test_images/derp.jpg""  netG  md.val_ds  tfms=x_tfms  sz=500)  I'd keep the size around 500px  give or take  given you're running this on a gpu with plenty of memory (11 GB GeForce 1080Ti  for example).  If you have less than that  you'll have to go smaller or try running it on CPU.  I actually tried the latter but for some reason it was -really- absurdly slow and I didn't take the time to investigate why that was other than to find out that the Pytorch people were recommending building from source to get a big performance boost.  Yeah...I didn't want to bother at that point.    """;Computer Vision;https://github.com/Soldie/DeOldify-colorir-imagens-antigas
"""This plugin is meant to make it easy to replace the background in portrait images and video. It is using a neural network to predict the mask of the portrait and remove the background pixels. It's easily composable with other OBS plugins to replace the background with e.g. an image or a transparent color.  ![](demo.gif)  The models used for background detection are SINet: https://arxiv.org/abs/1911.09099 and MODNet: https://arxiv.org/pdf/2011.11961.pdf The pre-trained model weights were taken from: - https://github.com/anilsathyan7/Portrait-Segmentation/tree/master/SINet - https://github.com/ZHKKKe/MODNet  Some more information about how I built it: https://www.morethantechnical.com/2021/04/15/obs-plugin-for-portrait-background-removal-with-onnx-sinet-model/   ``` $ cmake .. -DobsPath=""$HOME\Downloads\obs-studio\"" $ cmake --build . --config Release $ cpack $ Expand-Archive .\obs-backgroundremoval-win64.zip -DestinationPath 'C:\Program Files\obs-studio\' -Force ```  To build with CUDA support  tell cmake to use the CUDA version of OnnxRuntime ``` $ cmake .. -DobsPath=""$HOME\Downloads\obs-studio\"" -DWITH_CUDA=ON ``` The rest of the build process is similar  but the result archive will be `obs-backgroundremoval-win64-cuda.zip`.  Install OpenCV via `vcpkg`: ``` $ mkdir build $ cd build $ git clone https://github.com/microsoft/vcpkg $ cd vcpkg $ .\bootstrap-vcpkg.bat $ .\vcpkg.exe install opencv[core]:x64-windows-static ```  Install Onnxruntime with NuGet: ``` $ cd build $ mkdir nuget $ Invoke-WebRequest https://dist.nuget.org/win-x86-commandline/latest/nuget.exe -UseBasicParsing -OutFile nuget.exe $ nuget.exe install Microsoft.ML.OnnxRuntime.DirectML -Version 1.7.0 $ nuget.exe install Microsoft.ML.OnnxRuntime.Gpu -Version 1.7.1 ```  Clone the OBS repo  `Downloads\ $ git clone --single-branch -b 27.0.1 git@github.com:obsproject/obs-studio.git`  to e.g. Downloads.   Unpack the package to the plugins directory of the system's Library folder (which is Apple's preffered way) ``` $ unzip -o obs-backgroundremoval-macosx.zip -d ""/Library/Application Support/obs-studio/plugins"" ```  or directly to your OBS install directory  e.g. ``` $ unzip -o obs-backgroundremoval-macosx.zip -d /Applications/OBS.app/Contents/ ```  The first is recommended as it preserves the plugins over the parallel installation of OBS versions (i.e. running the latest productive version and a release candidate) whereas the latter will also remove the plugin if you decide to delete the OBS application.   You may use homebrew: ``` $ brew install opencv onnxruntime ```  Or - you may also build a (very minimal) version of OpenCV and ONNX Runtime for static-linking  instead of the homebrew ones: ``` <root>/build/ $ ../scripts/makeOpenCV_osx.sh <root>/build/ $ ../scripts/makeOnnxruntime_osx.sh ``` Static linking should be more robust across versions of OSX  as well as building for 10.13.   Linux (Ubuntu  Arch)  The plugin was built and tested on Mac OSX  Windows and Ubuntu Linux. Help is appreciated in building on other OSs and formalizing the one-click installers.  If you install the desktop OBS app (https://obsproject.com/download) you already have the binaries   But you don't have the headers - so clone the main obs repo e.g. git clone --single-branch -b 27.1.3 git@github.com:obsproject/obs-studio.git (match the version number to your OBS install. Right now on OSX it's 27.1.3)  $ mkdir -p build &amp;&amp; cd build   $ apt install -y libobs-dev libopencv-dev language-pack-en wget git build-essential cmake  $ wget https://github.com/microsoft/onnxruntime/releases/download/v1.7.0/onnxruntime-linux-x64-1.7.0.tgz   Then build and install:  $ mkdir build &amp;&amp; cd build  $ cmake .. &amp;&amp; cmake --build . &amp;&amp; cmake --install .   $ cd scripts   : pacman -Sy --needed --noconfirm sudo fakeroot binutils gcc make   : sudo -u builduser bash -c 'cd /src/scripts && makepkg -s'   """;Computer Vision;https://github.com/royshil/obs-backgroundremoval
"""|pytorch-handbook|6.7k|pytorch handbook是一本开源的书籍，目标是帮助那些希望和使用PyTorch进行深度学习开发和研究的朋友快速入门，其中包含的Pytorch教程全部通过测试保证可以成功运行|   |tensorboardX|5.5k|tensorboard for pytorch (and chainer  mxnet  numpy  ...)|   |pretrained-models.pytorch|4.9k|Pretrained ConvNets for pytorch: NASNet  ResNeXt  ResNet  InceptionV4  InceptionResnetV2  Xception  DPN  etc.|   |keras-js|4.6k|Run Keras models in the browser  with GPU support using WebGL|   |practical-pytorch|3.5k|PyTorch tutorials demonstrating modern techniques with readable code|   |tensorflow-build|1.9k|TensorFlow binaries supporting AVX  FMA  SSE|   |tangent|1.9k|Source-to-Source Debuggable Derivatives in Pure Python|   |nnvm|1.6k|move to https://github.com/dmlc/tvm/|   """;Sequential;https://github.com/Aspire-Mayank/Top-Deep-Learning-Stuffs
"""Please follow the instructions of py-faster-rcnn [here](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to setup VOC and COCO datasets (Part of COCO is done). The steps involve downloading data and optionally creating soft links in the ``data`` folder. Since faster RCNN does not rely on pre-computed proposals  it is safe to ignore the steps that setup proposals.  If you find it useful  the ``data/cache`` folder created on my side is also shared [here](http://ladoga.graphics.cs.cmu.edu/xinleic/tf-faster-rcnn/cache.tgz).   1. Clone the repository   ```Shell   git clone https://github.com/endernewton/tf-faster-rcnn.git   ```  2. Update your -arch in setup script to match your GPU   ```Shell   cd tf-faster-rcnn/lib   #: Change the GPU architecture (-arch) if necessary   vim setup.py   ```    | GPU model  | Architecture |   | ------------- | ------------- |   | TitanX (Maxwell/Pascal) | sm_52 |   | GTX 960M | sm_50 |   | GTX 1080 (Ti) | sm_61 |   | Grid K520 (AWS g2.2xlarge) | sm_30 |   | Tesla K80 (AWS p2.xlarge) | sm_37 |    **Note**: You are welcome to contribute the settings on your end if you have made the code work properly on other GPUs. Also even if you are only using CPU tensorflow  GPU based code (for NMS) will be used by default  so please set **USE_GPU_NMS False** to get the correct output.   3. Build the Cython modules   ```Shell   make clean   make   cd ..   ```  4. Install the [Python COCO API](https://github.com/pdollar/coco). The code requires the API to access COCO dataset.   ```Shell   cd data   git clone https://github.com/pdollar/coco.git   cd coco/PythonAPI   make   cd ../../..   ```     - Due to the randomness in GPU training with Tensorflow especially for VOC  the best numbers are reported (with 2-3 attempts) here. According to my experience  for COCO you can almost always get a very close number (within ~0.2%) despite the randomness.      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz     tar -xzvf vgg_16_2016_08_28.tar.gz      cd ../..     For Resnet101  you can set up like:      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz     tar -xzvf resnet_v1_101_2016_08_28.tar.gz      cd ../..     ./experiments/scripts/train_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/train_faster_rcnn.sh 1 coco res101     ./experiments/scripts/test_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/test_faster_rcnn.sh 1 coco res101   1. Download pre-trained model   ```Shell   #: Resnet101 for voc pre-trained on 07+12 set   ./data/scripts/fetch_faster_rcnn_models.sh   ```   **Note**: if you cannot download the models through the link  or you want to try more models  you can check out the following solutions and optionally update the downloading script:   - Another server [here](http://xinlei.sp.cs.cmu.edu/xinleic/tf-faster-rcnn/).   - Google drive [here](https://drive.google.com/open?id=0B1_fAEgxdnvJSmF3YUlZcHFqWTQ).  2. Create a folder and a soft link to use the pre-trained model   ```Shell   NET=res101   TRAIN_IMDB=voc_2007_trainval+voc_2012_trainval   mkdir -p output/${NET}/${TRAIN_IMDB}   cd output/${NET}/${TRAIN_IMDB}   ln -s ../../../data/voc_2007_trainval+voc_2012_trainval ./default   cd ../../..   ```  3. Demo for testing on custom images   ```Shell   #: at repository root   GPU_ID=0   CUDA_VISIBLE_DEVICES=${GPU_ID} ./tools/demo.py   ```   **Note**: Resnet101 testing probably requires several gigabytes of memory  so if you encounter memory capacity issues  please install it with CPU support only. Refer to [Issue 25](https://github.com/endernewton/tf-faster-rcnn/issues/25).  4. Test with pre-trained Resnet101 models   ```Shell   GPU_ID=0   ./experiments/scripts/test_faster_rcnn.sh $GPU_ID pascal_voc_0712 res101   ```   **Note**: If you cannot get the reported numbers (79.8 on my side)  then probably the NMS function is compiled improperly  refer to [Issue 5](https://github.com/endernewton/tf-faster-rcnn/issues/5).   """;Computer Vision;https://github.com/liuqiang3/faster_rcnn
"""Link: https://github.com/qubvel   !pip install git+https://github.com/qubvel/classification_models.git   2. Classification_models : Github: https://github.com/qubvel/classification_models.git  3. Numpy  4. Matplotlib   """;Computer Vision;https://github.com/VinayBN8997/ResNet-CIFAR10
"""1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   Requirements: hardware  Basic installation   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   To train and test a Faster R-CNN detector using the **alternating optimization** algorithm from our NIPS 2015 paper  use `experiments/scripts/faster_rcnn_alt_opt.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  (""alt opt"" refers to the alternating optimization training algorithm described in the NIPS paper.)  To train and test a Faster R-CNN detector using the **approximate joint training** method  use `experiments/scripts/faster_rcnn_end2end.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  This method trains the RPN module jointly with the Fast R-CNN network  rather than alternating between training the two. It results in faster (~ 1.5x speedup) training times and similar detection accuracy. See these [slides](https://www.dropbox.com/s/xtr4yd4i5e0vw8g/iccv15_tutorial_training_rbg.pdf?dl=0) for more details.  Artifacts generated by the scripts in `tools` are written in this directory.  Trained Fast R-CNN networks are saved under:  ``` output/<experiment directory>/<dataset name>/ ```  Test outputs are saved under:  ``` output/<experiment directory>/<dataset name>/<network snapshot name>/ ```  """;Computer Vision;https://github.com/leonardhan1979/fasterRCNN
"""ORB-SLAM Mono with/without Good Feature https://github.com/ivalab/GF_ORB_SLAM  ORB-SLAM Stereo with/without Good Feature https://github.com/ivalab/gf_orb_slam2  SVO Mono & Stereo https://github.com/YipuZhao/rpg_svo  DSO Mono https://github.com/YipuZhao/DSO   ROVIO Mono https://github.com/YipuZhao/rovio  VINS-Mono https://github.com/YipuZhao/VINS-Mono  MSCKF Stereo https://github.com/YipuZhao/msckf_vio  OKVIS Stereo https://github.com/YipuZhao/okvis   """;General;https://github.com/ivalab/FullResults_GoodFeature
"""Please follow the instructions of py-faster-rcnn [here](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to setup VOC and COCO datasets (Part of COCO is done). The steps involve downloading data and optionally creating soft links in the ``data`` folder. Since faster RCNN does not rely on pre-computed proposals  it is safe to ignore the steps that setup proposals.  If you find it useful  the ``data/cache`` folder created on Xinlei's side is also shared [here](http://ladoga.graphics.cs.cmu.edu/xinleic/tf-faster-rcnn/cache.tgz).   1. Clone the repository   ```Shell   git clone https://github.com/ruotianluo/pytorch-faster-rcnn.git   ```  2. Choose your `-arch` option to match your GPU for step 3 and 4.    | GPU model  | Architecture |   | ------------- | ------------- |   | TitanX (Maxwell/Pascal) | sm_52 |   | GTX 960M | sm_50 |   | GTX 1080 (Ti) | sm_61 |   | Grid K520 (AWS g2.2xlarge) | sm_30 |   | Tesla K80 (AWS p2.xlarge) | sm_37 |    **Note**: You are welcome to contribute the settings on your end if you have made the code work properly on other GPUs.   3. Build RoiPooling module   ```   cd pytorch-faster-rcnn/lib/layer_utils/roi_pooling/src/cuda   echo ""Compiling roi_pooling kernels by nvcc...""   nvcc -c -o roi_pooling_kernel.cu.o roi_pooling_kernel.cu -x cu -Xcompiler -fPIC -arch=sm_52   cd ../../   python build.py   cd ../../../   ```   4. Build NMS   ```   cd lib/nms/src/cuda   echo ""Compiling nms kernels by nvcc...""   nvcc -c -o nms_kernel.cu.o nms_kernel.cu -x cu -Xcompiler -fPIC -arch=sm_52   cd ../../   python build.py   cd ../../   ```  5. Install the [Python COCO API](https://github.com/pdollar/coco). The code requires the API to access COCO dataset.   ```Shell   cd data   git clone https://github.com/pdollar/coco.git   cd coco/PythonAPI   make   cd ../../..   ```     - Due to the randomness in GPU training especially for VOC  the best numbers are reported (with 2-3 attempts) here. According to Xinlei's experience  for COCO you can almost always get a very close number (within ~0.2%) despite the randomness.      cd data/imagenet_weights     python #: open python in terminal and run the following Python code     ```Python      cd ../..     For Resnet101  you can set up like:Shell      cd data/imagenet_weights     #: download from my gdrive (link in pytorch-resnet)      cd ../..     ./experiments/scripts/train_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/train_faster_rcnn.sh 1 coco res101     ./experiments/scripts/test_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/test_faster_rcnn.sh 1 coco res101   1. Download pre-trained model (only google drive works)   <!-- ```Shell   #: Resnet101 for voc pre-trained on 07+12 set   #: ./data/scripts/fetch_faster_rcnn_models.sh   ```   **Note**: if you cannot download the models through the link  or you want to try more models  you can check out the following solutions and optionally update the downloading script: -->   - ~~Another server [here](http://gs11655.sp.cs.cmu.edu/xinleic/tf-faster-rcnn/).~~   - Google drive [here](https://drive.google.com/open?id=0B7fNdx_jAqhtNE10TDZDbFRuU0E).  **(Optional)** Instead of downloading my pretrained or converted model  you can also convert from tf-faster-rcnn model. You can download the tensorflow pretrained model from [tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn/#demo-and-test-with-pre-trained-models). Then run: ```Shell python tools/convert_from_tensorflow.py --tensorflow_model resnet_model.ckpt  python tools/convert_from_tensorflow_vgg.py --tensorflow_model vgg_model.ckpt ```  This script will create a `.pth` file with the same name in the same folder as the tensorflow model.  2. Create a folder and a soft link to use the pre-trained model   ```Shell   NET=res101   TRAIN_IMDB=voc_2007_trainval+voc_2012_trainval   mkdir -p output/${NET}/${TRAIN_IMDB}   cd output/${NET}/${TRAIN_IMDB}   ln -s ../../../data/voc_2007_trainval+voc_2012_trainval ./default   cd ../../..   ```  3. Demo for testing on custom images   ```Shell   #: at repository root   GPU_ID=0   CUDA_VISIBLE_DEVICES=${GPU_ID} ./tools/demo.py   ```   **Note**: Resnet101 testing probably requires several gigabytes of memory  so if you encounter memory capacity issues  please install it with CPU support only. Refer to [Issue 25](https://github.com/endernewton/tf-faster-rcnn/issues/25).  4. Test with pre-trained Resnet101 models   ```Shell   GPU_ID=0   ./experiments/scripts/test_faster_rcnn.sh $GPU_ID pascal_voc_0712 res101   ```   **Note**: If you cannot get the reported numbers (79.8 on my side)  then probably the NMS function is compiled improperly  refer to [Issue 5](https://github.com/endernewton/tf-faster-rcnn/issues/5).   """;Computer Vision;https://github.com/yanxp/rcnn-discovery
"""- LJSpeech：https://keithito.com/LJ-Speech-Dataset/   """;General;https://github.com/mitsu-h/deepvoice3
"""PyTorch is required as a prerequisite before installing OP Text. Head on over to the [getting started  page](https://pytorch.org/get-started/locally/) of their website and follow the installation instructions for your version of Python.   >!Currently only Python versions 3.6 and above are supported  Use one of the following commands to install OP Text:   pip install op_text  conda install op_text   The entire purpose of this package is to allow users to leverage the power of the transformer models available in HuggingFace's library without needing to understand how to use PyTorch.   """;Natural Language Processing;https://github.com/knuddy/op_text
"""Training data should be formatted as below: ``` source_sequence \t target_sequence source_sequence \t target_sequence ... ```  To prepare data: ``` python prepare.py training_data ```  To train: ``` python train.py model vocab.src vocab.tgt training_data.csv num_epoch ```  To predict: ``` python predict.py model.epochN vocab.src vocab.tgt test_data ```   """;General;https://github.com/threelittlemonkeys/transformer-pytorch
"""Download Windows/Linux/MacOS Executable for Intel/AMD/Nvidia GPU  This package includes all the binaries and models required. It is portable  so no CUDA or PyTorch runtime environment is needed :)   mkdir input_frames  mkdir output_frames   ffmpeg -framerate 48 -i output_frames/%08d.png -i audio.m4a -c:a copy -crf 20 -c:v libx264 -pix_fmt yuv420p output.mp4   Download and setup the Vulkan SDK from https://vulkan.lunarg.com/  For Linux distributions  you can either get the essential build requirements from package manager   dnf install vulkan-headers vulkan-loader-devel   apt-get install libvulkan-dev   git clone https://github.com/nihui/rife-ncnn-vulkan.git  cd rife-ncnn-vulkan  git submodule update --init --recursive  Build with CMake  You can pass -DUSE_STATIC_MOLTENVK=ON option to avoid linking the vulkan loader library on MacOS   mkdir build  cd build   cmake --build . -j 4   Input two frame images  output one interpolated frame image.   ```shell ./rife-ncnn-vulkan -0 0.jpg -1 1.jpg -o 01.jpg ./rife-ncnn-vulkan -i input_frames/ -o output_frames/ ```  Example below runs on CPU  Discrete GPU  and Integrated GPU all at the same time. Uses 2 threads for image decoding  4 threads for one CPU worker  4 threads for another CPU worker  2 threads for discrete GPU  1 thread for integrated GPU  and 4 threads for image encoding. ```shell ./rife-ncnn-vulkan -i input_frames/ -o output_frames/ -g -1 -1 0 1 -j 2:4 4 2 1:4 ```   ```console Usage: rife-ncnn-vulkan -0 infile -1 infile1 -o outfile [options]...        rife-ncnn-vulkan -i indir -o outdir [options]...    -h                   show this help   -v                   verbose output   -0 input0-path       input image0 path (jpg/png/webp)   -1 input1-path       input image1 path (jpg/png/webp)   -i input-path        input image directory (jpg/png/webp)   -o output-path       output image path (jpg/png/webp) or directory   -m model-path        rife model path (default=rife-HD)   -g gpu-id            gpu device to use (-1=cpu  default=auto) can be 0 1 2 for multi-gpu   -j load:proc:save    thread count for load/proc/save (default=1:2:2) can be 1:2 2 2:2 for multi-gpu   -x                   enable tta mode   -u                   enable UHD mode   -f pattern-format    output image filename pattern format (%08d.jpg/png/webp  default=ext/%08d.png) ```  - `input0-path`  `input1-path` and `output-path` accept file path - `input-path` and `output-path` accept file directory - `load:proc:save` = thread count for the three stages (image decoding + rife interpolation + image encoding)  using larger values may increase GPU usage and consume more GPU memory. You can tune this configuration with ""4:4:4"" for many small-size images  and ""2:2:2"" for large-size images. The default setting usually works fine for most situations. If you find that your GPU is hungry  try increasing thread count to achieve faster processing. - `pattern-format` = the filename pattern and format of the image to be output  png is better supported  however webp generally yields smaller file sizes  both are losslessly encoded  If you encounter a crash or error  try upgrading your GPU driver:  - Intel: https://downloadcenter.intel.com/product/80939/Graphics-Drivers - AMD: https://www.amd.com/en/support - NVIDIA: https://www.nvidia.com/Download/index.aspx   """;Computer Vision;https://github.com/nihui/rife-ncnn-vulkan
"""This repository is the official implementation for [Pose Recognition with Cascade Transformers](https://openaccess.thecvf.com/content/CVPR2021/html/Li_Pose_Recognition_With_Cascade_Transformers_CVPR_2021_paper.html). It proposes two types of cascade Transformers  as follows  for pose recognition.   """;Computer Vision;https://github.com/mlpc-ucsd/PRTR
"""Download and extract ImageNet train and val images from http://image-net.org/. The directory structure is the standard layout for the torchvision [`datasets.ImageFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder)  and the training and validation data is expected to be in the `train/` folder and `val` folder respectively:  ``` /path/to/imagenet/   train/     class1/       img1.jpeg     class2/       img2.jpeg   val/     class1/       img3.jpeg     class/2       img4.jpeg ```  Then  install PyTorch 1.7.0+ and torchvision 0.8.1+ and pytorch-image-models 0.3.2:   To evaluate the performance of Twins-SVT-L on ImageNet using one GPU  run   First  clone the repository locally: ``` git clone https://github.com/Meituan-AutoML/Twins.git ``` Then  install PyTorch 1.7.0+ and torchvision 0.8.1+ and [pytorch-image-models==0.3.2](https://github.com/rwightman/pytorch-image-models):  ``` conda install -c pytorch pytorch torchvision pip install timm==0.3.2 ```  """;Computer Vision;https://github.com/Meituan-AutoML/Twins
"""The solution is a web-service.  Users interact with it via a standard web browser on a smartphone or a desktop computer. Results are displayed on the screen as images and text and can be sent to the user's E-mail.  This solution can also be installed as a standalone program on a personal computer and can be used through a command-line interface.  Video presentation: https://youtu.be/_vcvxPtAzOM     This service is available at the address: http://angelina-reader.ru       ``` git clone --recursive https://github.com/IlyaOvodov/AngelinaReader.git cd AngelinaReader pip install --upgrade pip pip install -r requirements.txt wget -O weights/model.t7 http://angelina-reader.ovdv.ru/retina_chars_eced60.clr.008     python run_web_app.py ``` Windows: pip directory (i.e. `<python>\Scripts`) should be added to Path .    Be sure  `python` and `pip` start Python3 if both Python 3 and Python 2.7 are installed.     Open http://127.0.0.1:5000 in a browser. The main page of the application should be displayed.  To access the application from Internet forward port 80 to port 5000 of the server. It is not required to test the service locally (at http://127.0.0.1:5000 address).      or  in Windows:    """;General;https://github.com/IlyaOvodov/AngelinaReader
"""Datasets are stored as uncompressed ZIP archives containing uncompressed PNG files and a metadata file `dataset.json` for labels.  Custom datasets can be created from a folder containing images; see [`python dataset_tool.py --help`](./docs/dataset-tool-help.txt) for more information. Alternatively  the folder can also be used directly as a dataset  without running it through `dataset_tool.py` first  but doing so may lead to suboptimal performance.  Legacy TFRecords datasets are not supported &mdash; see below for instructions on how to convert them.  **FFHQ**:  Step 1: Download the [Flickr-Faces-HQ dataset](https://github.com/NVlabs/ffhq-dataset) as TFRecords.  Step 2: Extract images from TFRecords using `dataset_tool.py` from the [TensorFlow version of StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada/):  ```.bash #: Using dataset_tool.py from TensorFlow version at #: https://github.com/NVlabs/stylegan2-ada/ python ../stylegan2-ada/dataset_tool.py unpack \     --tfrecord_dir=~/ffhq-dataset/tfrecords/ffhq --output_dir=/tmp/ffhq-unpacked ```  Step 3: Create ZIP archive using `dataset_tool.py` from this repository:  ```.bash #: Original 1024x1024 resolution. python dataset_tool.py --source=/tmp/ffhq-unpacked --dest=~/datasets/ffhq.zip  #: Scaled down 256x256 resolution. python dataset_tool.py --source=/tmp/ffhq-unpacked --dest=~/datasets/ffhq256x256.zip \     --width=256 --height=256 ```  **MetFaces**: Download the [MetFaces dataset](https://github.com/NVlabs/metfaces-dataset) and create ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/metfaces/images --dest=~/datasets/metfaces.zip ```  **AFHQ**: Download the [AFHQ dataset](https://github.com/clovaai/stargan-v2/blob/master/README.md#animal-faces-hq-dataset-afhq) and create ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/afhq/train/cat --dest=~/datasets/afhqcat.zip python dataset_tool.py --source=~/downloads/afhq/train/dog --dest=~/datasets/afhqdog.zip python dataset_tool.py --source=~/downloads/afhq/train/wild --dest=~/datasets/afhqwild.zip ```  **CIFAR-10**: Download the [CIFAR-10 python version](https://www.cs.toronto.edu/~kriz/cifar.html) and convert to ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/cifar-10-python.tar.gz --dest=~/datasets/cifar10.zip ```  **LSUN**: Download the desired categories from the [LSUN project page](https://www.yf.io/p/lsun/) and convert to ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/lsun/raw/cat_lmdb --dest=~/datasets/lsuncat200k.zip \     --transform=center-crop --width=256 --height=256 --max_images=200000  python dataset_tool.py --source=~/downloads/lsun/raw/car_lmdb --dest=~/datasets/lsuncar200k.zip \     --transform=center-crop-wide --width=512 --height=384 --max_images=200000 ```  **BreCaHAD**:  Step 1: Download the [BreCaHAD dataset](https://figshare.com/articles/BreCaHAD_A_Dataset_for_Breast_Cancer_Histopathological_Annotation_and_Diagnosis/7379186).  Step 2: Extract 512x512 resolution crops using `dataset_tool.py` from the [TensorFlow version of StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada/):  ```.bash #: Using dataset_tool.py from TensorFlow version at #: https://github.com/NVlabs/stylegan2-ada/ python dataset_tool.py extract_brecahad_crops --cropsize=512 \     --output_dir=/tmp/brecahad-crops --brecahad_dir=~/downloads/brecahad/images ```  Step 3: Create ZIP archive using `dataset_tool.py` from this repository:  ```.bash python dataset_tool.py --source=/tmp/brecahad-crops --dest=~/datasets/brecahad.zip ```   * GPU memory usage is comparable to the TensorFlow version.   | stylegan2-ada-pytorch | Main directory hosted on Amazon S3       --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl       --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl   The above code requires torch_utils and dnnlib to be accessible via PYTHONPATH. It does not need source code for the networks themselves &mdash; their class definitions are loaded from the pickle via torch_utils.persistence.   The name of the output directory reflects the training configuration. For example  00000-mydataset-auto1 indicates that the base configuration was auto1  meaning that the hyperparameters were selected automatically for training on one GPU. The base configuration is controlled by --cfg:       --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl   We employ the following metrics in the ADA paper. Execution time and GPU memory usage is reported for one NVIDIA Tesla V100 GPU at 1024x1024 resolution:   Pre-trained networks are stored as `*.pkl` files that can be referenced using local filenames or URLs:  ```.bash #: Generate curated MetFaces images without truncation (Fig.10 left) python generate.py --outdir=out --trunc=1 --seeds=85 265 297 849 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl  #: Generate uncurated MetFaces images with truncation (Fig.12 upper left) python generate.py --outdir=out --trunc=0.7 --seeds=600-605 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl  #: Generate class conditional CIFAR-10 images (Fig.17 left  Car) python generate.py --outdir=out --seeds=0-35 --class=1 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl  #: Style mixing example python style_mixing.py --outdir=out --rows=85 100 75 458 1500 --cols=55 821 1789 293 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl ```  Outputs from the above commands are placed under `out/*.png`  controlled by `--outdir`. Downloaded network pickles are cached under `$HOME/.cache/dnnlib`  which can be overridden by setting the `DNNLIB_CACHE_DIR` environment variable. The default PyTorch extension build directory is `$HOME/.cache/torch_extensions`  which can be overridden by setting `TORCH_EXTENSIONS_DIR`.  **Docker**: You can run the above curated image example using Docker as follows:  ```.bash docker build --tag sg2ada:latest . ./docker_run.sh python3 generate.py --outdir=out --trunc=1 --seeds=85 265 297 849 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl ```  Note: The Docker image requires NVIDIA driver release `r455.23` or later.  **Legacy networks**: The above commands can load most of the network pickles created using the previous TensorFlow versions of StyleGAN2 and StyleGAN2-ADA. However  for future compatibility  we recommend converting such legacy pickles into the new format used by the PyTorch version:  ```.bash python legacy.py \     --source=https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl \     --dest=stylegan2-cat-config-f.pkl ```   """;Computer Vision;https://github.com/XieBaoshi/stylegan2
"""The easiest way to install deepface is to download it from [`PyPI`](https://pypi.org/project/deepface/). It's going to install the library itself and its prerequisites as well. The library is mainly based on TensorFlow and Keras.  ```python pip install deepface ```  Then you will be able to import the library and use its functionalities.  ```python from deepface import DeepFace ```  **Facial Recognition** - [`Demo`](https://youtu.be/WnUVYQP4h44)  A modern [**face recognition pipeline**](https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/) consists of 5 common stages: [detect](https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/)  [align](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/)  [normalize](https://sefiks.com/2020/11/20/facial-landmarks-for-face-recognition-with-dlib/)  [represent](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) and [verify](https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/). Deepface handles all these common stages in the background. You can just call its verification  find or analysis function with a single line of code.  **Face Verification** - [`Demo`](https://youtu.be/KRCvkNCOphE)  This function verifies face pairs as same person or different persons. It expects exact image paths as inputs. Passing numpy or based64 encoded images is also welcome.  ```python result = DeepFace.verify(img1_path = ""img1.jpg""  img2_path = ""img2.jpg"") ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-1.jpg"" width=""95%"" height=""95%""></p>  **Face recognition** - [`Demo`](https://youtu.be/Hrjp-EStM_s)  [Face recognition](https://sefiks.com/2020/05/25/large-scale-face-recognition-for-deep-learning/) requires applying face verification many times. Herein  deepface has an out-of-the-box find function to handle this action. It's going to look for the identity of input image in the database path and it will return pandas data frame as output.  ```python df = DeepFace.find(img_path = ""img1.jpg""  db_path = ""C:/workspace/my_db"") ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-6-v2.jpg"" width=""95%"" height=""95%""></p>  **Face recognition models** - [`Demo`](https://youtu.be/i_MOwvhbLdI)  Deepface is a **hybrid** face recognition package. It currently wraps many **state-of-the-art** face recognition models: [`VGG-Face`](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/)   [`Google FaceNet`](https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/)  [`OpenFace`](https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/)  [`Facebook DeepFace`](https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/)  [`DeepID`](https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/)  [`ArcFace`](https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/) and [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/). The default configuration uses VGG-Face model.  ```python models = [""VGG-Face""  ""Facenet""  ""Facenet512""  ""OpenFace""  ""DeepFace""  ""DeepID""  ""ArcFace""  ""Dlib""] result = DeepFace.verify(img1_path = ""img1.jpg""  img2_path = ""img2.jpg""  model_name = models[1]) df = DeepFace.find(img_path = ""img1.jpg""  db_path = ""C:/workspace/my_db""  model_name = models[1]) ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-wrapped-models.png"" width=""95%"" height=""95%""></p>  FaceNet  VGG-Face  ArcFace and Dlib [overperforms](https://youtu.be/i_MOwvhbLdI) than OpenFace  DeepFace and DeepID based on experiments. Supportively  FaceNet /w 512d got 99.65%; FaceNet /w 128d got 99.2%; ArcFace got 99.41%; Dlib got 99.38%; VGG-Face got 98.78%; DeepID got 97.05; OpenFace got 93.80% accuracy scores on [LFW data set](https://sefiks.com/2020/08/27/labeled-faces-in-the-wild-for-face-recognition/) whereas human beings could have just 97.53%.  **Similarity**  Face recognition models are regular [convolutional neural networks](https://sefiks.com/2018/03/23/convolutional-autoencoder-clustering-images-with-neural-networks/) and they are responsible to represent faces as vectors. We expect that a face pair of same person should be [more similar](https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/) than a face pair of different persons.  Similarity could be calculated by different metrics such as [Cosine Similarity](https://sefiks.com/2018/08/13/cosine-similarity-in-machine-learning/)  Euclidean Distance and L2 form. The default configuration uses cosine similarity.  ```python metrics = [""cosine""  ""euclidean""  ""euclidean_l2""] result = DeepFace.verify(img1_path = ""img1.jpg""  img2_path = ""img2.jpg""  distance_metric = metrics[1]) df = DeepFace.find(img_path = ""img1.jpg""  db_path = ""C:/workspace/my_db""  distance_metric = metrics[1]) ```  Euclidean L2 form [seems](https://youtu.be/i_MOwvhbLdI) to be more stable than cosine and regular Euclidean distance based on experiments.  **Facial Attribute Analysis** - [`Demo`](https://youtu.be/GT2UeN85BdA)  Deepface also comes with a strong facial attribute analysis module including [`age`](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/)  [`gender`](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/)  [`facial expression`](https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/) (including angry  fear  neutral  sad  disgust  happy and surprise) and [`race`](https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/) (including asian  white  middle eastern  indian  latino and black) predictions.  ```python obj = DeepFace.analyze(img_path = ""img4.jpg""  actions = ['age'  'gender'  'race'  'emotion']) ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-2.jpg"" width=""95%"" height=""95%""></p>  Age model got ± 4.65 MAE; gender model got 97.44% accuracy  96.29% precision and 95.05% recall as mentioned in its [tutorial](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/).  **Streaming and Real Time Analysis** - [`Demo`](https://youtu.be/-c9sSJcx6wI)  You can run deepface for real time videos as well. Stream function will access your webcam and apply both face recognition and facial attribute analysis. The function starts to analyze a frame if it can focus a face sequantially 5 frames. Then  it shows results 5 seconds.  ```python DeepFace.stream(db_path = ""C:/User/Sefik/Desktop/database"") ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-3.jpg"" width=""90%"" height=""90%""></p>  Even though face recognition is based on one-shot learning  you can use multiple face pictures of a person as well. You should rearrange your directory structure as illustrated below.  ```bash user ├── database │   ├── Alice │   │   ├── Alice1.jpg │   │   ├── Alice2.jpg │   ├── Bob │   │   ├── Bob.jpg ```  **Face Detectors** - [`Demo`](https://youtu.be/GZ2p2hj2H5k)  Face detection and alignment are early stages of a modern face recognition pipeline. Experiments show that just alignment increases the face recognition accuracy almost 1%. [`OpenCV`](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/)  [`SSD`](https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/)  [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/)   [`MTCNN`](https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/) and [`RetinaFace`](https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/) detectors are wrapped in deepface. OpenCV is the default detector.  ```python backends = ['opencv'  'ssd'  'dlib'  'mtcnn'  'retinaface']  #:face detection and alignment detected_face = DeepFace.detectFace(img_path = ""img.jpg""  detector_backend = backends[4])  #:face verification obj = DeepFace.verify(img1_path = ""img1.jpg""  img2_path = ""img2.jpg""  detector_backend = backends[4])  #:face recognition df = DeepFace.find(img_path = ""img.jpg""  db_path = ""my_db""  detector_backend = backends[4])  #:facial analysis demography = DeepFace.analyze(img_path = ""img4.jpg""  detector_backend = backends[4]) ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-detectors.png"" width=""90%"" height=""90%""></p>  [RetinaFace](https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/) and [MTCNN](https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/) seem to overperform in detection and alignment stages but they are slower than others. If the speed of your pipeline is more important  then you should use opencv or ssd. On the other hand  if you consider the accuracy  then you should use retinaface or mtcnn.  <!-- **Ensemble learning for face recognition** - [`Demo`](https://youtu.be/EIBJJJ0ECXU)  A face recognition task can be handled by several models and similarity metrics. Herein  deepface offers a [special boosting and combination solution](https://sefiks.com/2020/06/03/mastering-face-recognition-with-ensemble-learning/) to improve the accuracy of a face recognition task. This provides a huge improvement on accuracy metrics. On the other hand  this runs much slower than single models.  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-4.jpg"" width=""70%"" height=""70%""></p>  ```python resp_obj = DeepFace.verify(""img1.jpg""  ""img2.jpg""  model_name = ""Ensemble"") df = DeepFace.find(img_path = ""img1.jpg""  db_path = ""my_db""  model_name = ""Ensemble"") ``` -->  **API** - [`Demo`](https://youtu.be/HeKCQ6U9XmI)  Deepface serves an API as well. You can clone [`/api/api.py`](https://github.com/serengil/deepface/tree/master/api/api.py) and pass it to python command as an argument. This will get a rest service up. In this way  you can call deepface from an external system such as mobile app or web.  ``` python api.py ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-api.jpg"" width=""90%"" height=""90%""></p>  Face recognition  facial attribute analysis and vector representation functions are covered in the API. You are expected to call these functions as http post methods. Service endpoints will be `http://127.0.0.1:5000/verify` for face recognition  `http://127.0.0.1:5000/analyze` for facial attribute analysis  and `http://127.0.0.1:5000/represent` for vector representation. You should pass input images as base64 encoded string in this case. [Here](https://github.com/serengil/deepface/tree/master/api)  you can find a postman project.  **Tech Stack** - [`Vlog`](https://youtu.be/R8fHsL7u3eE)  [`Tutorial`](https://sefiks.com/2021/03/31/tech-stack-recommendations-for-face-recognition/)  Face recognition models represent facial images as vector embeddings. The idea behind facial recognition is that vectors should be more similar for same person than different persons. The question is that where and how to store facial embeddings in a large scale system. Herein  deepface offers a represention function to find vector embeddings from facial images.  ```python embedding = DeepFace.represent(img_path = ""img.jpg""  model_name = 'Facenet') ```  Tech stack is vast to store vector embeddings. To determine the right tool  you should consider your task such as face verification or face recognition  priority such as speed or confidence  and also data size.  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/tech-stack.png"" width=""90%"" height=""90%""></p>   """;Computer Vision;https://github.com/serengil/deepface
"""Please follow the installation instructions from our [TransFuser repository](https://github.com/autonomousvision/transfuser) to set up the CARLA simulator. The conda environment required for NEAT can be installed via: ```Shell conda env create -f environment.yml conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c nvidia ```  For running the AIM-VA baseline  you will additionally need to install [MMCV](https://mmcv.readthedocs.io/en/latest/get_started/installation.html) and [MMSegmentation](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/get_started.md#installation). ```Shell pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html pip install mmsegmentation ```   mkdir model_ckpt   Spin up a CARLA server (described above) and run the required agent. The required variables need to be set in leaderboard/scripts/run_evaluation.sh.   CUDA_VISIBLE_DEVICES=0 ./leaderboard/scripts/run_evaluation.sh   """;Computer Vision;https://github.com/autonomousvision/neat
"""1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   Requirements: hardware  Basic installation   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   To train and test a Faster R-CNN detector using the **alternating optimization** algorithm from our NIPS 2015 paper  use `experiments/scripts/faster_rcnn_alt_opt.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  (""alt opt"" refers to the alternating optimization training algorithm described in the NIPS paper.)  To train and test a Faster R-CNN detector using the **approximate joint training** method  use `experiments/scripts/faster_rcnn_end2end.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  This method trains the RPN module jointly with the Fast R-CNN network  rather than alternating between training the two. It results in faster (~ 1.5x speedup) training times and similar detection accuracy. See these [slides](https://www.dropbox.com/s/xtr4yd4i5e0vw8g/iccv15_tutorial_training_rbg.pdf?dl=0) for more details.  Artifacts generated by the scripts in `tools` are written in this directory.  Trained Fast R-CNN networks are saved under:  ``` output/<experiment directory>/<dataset name>/ ```  Test outputs are saved under:  ``` output/<experiment directory>/<dataset name>/<network snapshot name>/ ```  """;Computer Vision;https://github.com/krantirk/py-faster-rcnn
"""This code can't cope environment with continuous action   tensorflow-gpu (2.0.0a0)  numpy (1.16.4)   environment.py used to create a environment that let agent do something   you can also customize the environment   like process state to speed up convergence   """;Reinforcement Learning;https://github.com/UesugiErii/tf2-PPO-atari
""" The source code requires PyTorch 0.4.0 (there is known incompatible issue when using PyTorch 0.4.1  haven't tested on PyTorch 1.0). Python 3.5+ is needed (there is known incompatible issue when using Python 2.7).  The full list of arguments can be accessed using `--help`   """;Computer Vision;https://github.com/Yang-YiFan/DiracDeltaNet
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/cbhower/style-gan-music-video
"""Linux  | Windows ------------- | ------------- sudo apt-get install virtualenv / pip install virtualenv | pip install virtualenv virtualenv --python /usr/bin/python3.7 venv	  | virtualenv venv source venv/bin/activate  | venv\Scripts\activate.bat pip install -r requirements.txt  | pip install -r requirements.txt  pip install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp37-cp37m-linux_x86_64.whl (1)* | pip install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp37-cp37m-win_amd64.whl (1)* pip install torchvision  | pip install torchvision pip install torchtext==0.2.3  | pip install torchtext==0.2.3  python -m spacy download en_core_web_sm | python -m spacy download en_core_web_sm  (1)* replace ""cpu"" in link if you plan to use GPU: ""cu80"" for CUDA 8  ""cu90"" for CUDA 9.0  ""cu92"" for CUDA 9.2  ...  _If you require a newer version  please visit http://pytorch.org/ and follow their instructions to install the relevant pytorch binary._    Clone this repo with  ``` git clone --recursive https://github.com/davidalbertonogueira/NLP-tutorials git submodule update --depth 10 --recursive --init ```    """;Natural Language Processing;https://github.com/davidalbertonogueira/NLP-tutorials
"""The code reproduces the qualitative and quantitative experiments in the paper. The required dependencies are listed in dependencies.txt. Note that the INTERACTION dataset for the GL intersection has to be downloaded from: https://interaction-dataset.com/ and placed into the data directory. Then  a directory: data/INTERACTION-Dataset-DR-v1_1 should exist.  To run the experiments  please run the following files:   """;Computer Vision;https://github.com/sisl/MultiAgentVariationalOcclusionInference
"""This model consists of 2 generators and 2 discriminators. The two generators as U-net like CNNs. During the evaluation of the model  I directly used the pretrained salient objective detection model from Joker  https://github.com/Joker316701882/Salient-Object-Detection.  This is a project about image style transfer developed by Tao Liang  Tianrui Yu  Ke Han and Yifan Ruan. Our project contains three different models  one is in ""cycle_gan_unet"" directory which uses the u-net like cnn as generators  one is in ""Ukiyoe_codes"" directory which uses Resnet blocks as generators  which uses the model proposed in this paper https://arxiv.org/pdf/1703.10593.pdf  the other is in neural_style_transfer that implement sytle transfer using convolution neural network proposed in this paper https://arxiv.org/pdf/1508.06576.pdf.   Directly run the demo.ipynb notebook. You can see the original image and the transferred image.<br/> If you want to train the model by yourself  delete /baroque and /ukiyo_e directorys. And run the cycle_gan_model.ipynb notebook. You can set all the parameters in the initialization of the experiment class.   single_test.ipynb(for demo use):   run this notebook to show the Ukiyoe-style transfer result of 'test_image.jpg'. Make sure the image  latest_ukiyoe_G_A.pkl and './models' are in their original places<br/>  train.ipynb:  run this notebook to train a cycle-GAN that can transfer 'datasets/trainA' style to 'datasets/trainB' style. Training options can be found and revised in './options/train_options.py' and './options/base_options.py'<br/>  test.ipynb:  run this notebook to test the model in './checkpoints' file. Input the model name in './options/base_options.py'<br/> plot_losses.ipynb:   run this to plot losses given a loss log in './checkpoints'<br/>     Run the Neural_Style_Transfer.ipynb for demo.<br/> The notebook also stores model. If you want to change the network structure  choose one of content_layers_default and style_layers_default each and comment the others. For white noise input  consider decreasing the weight of style loss and increase the number of optimizing steps.     """;General;https://github.com/CarpdiemLiang/style_transfer
"""You can find the complete CRACT workshop here: https://ml-critique-correct.github.io    As many of the model variants have many filters in early layers  we have implemented a GPU memory usage estimate when the model gets build initially. Taking into account the batch size and model configuration  we automatically split the mini-batch into smaller chunks if the currently available GPU memory is exceeded. The gradients are then accumulated until the specified mini-batch size (default: 128) is reached before an optimizer update is done. While our code can be used with multiple GPUs (through use of PyTorch's DataParallel) this allows our code to be run on any single GPU.   We note that this check crashes occasionally and memory still gets exceeded because the theoretically allocated memory and the effective memory used by CUDNN differ. We came up with a rough heuristic for this  but *if anyone using our code has a proper solution to this issue  we will very much appreciate feedback  suggestions or a pull request!*.    """;General;https://github.com/MrtnMndt/Rethinking_CNN_Layerwise_Feature_Amounts
"""pip install tensorboard_logger   """;Computer Vision;https://github.com/andreasveit/densenet-pytorch
"""To avoid any conflict with your existing Python setup  and to keep this project self-contained  it is suggested to work in a virtual environment with [`virtualenv`](http://docs.python-guide.org/en/latest/dev/virtualenvs/). To install `virtualenv`: ``` pip install --upgrade virtualenv ``` Create a virtual environment  activate it and install the requirements in [`requirements.txt`](requirements.txt). ``` virtualenv venv source venv/bin/activate pip install -r requirements.txt ```   1) maml_rl/envs/__init__.py 	register your environment: 	 	register(     'TabularMDP-v1'      entry_point='maml_rl.envs.mdp-my:TabularMDPEnv'      kwargs={'num_states': 5  'num_actions': 3}      max_episode_steps=10 )  2) basic settings: /configs/maml/mdp/mdp-my.yaml  3) change the environment: /maml_rl/envs/mdp-my.py  4) train & test   """;General;https://github.com/GeorgeDUT/MetaRLSAS
"""```bash pip install youtokentome ```  Class youtokentome.BPE has the following methods:   YouTokenToMe supports the following commands:   With the --stream option  --n_threads will be ignored and all sentences will be processed one by one.     --output_type TEXT   'id' or 'subword'.  [required]   Let's start with a self-contained example.   ```python import random  import youtokentome as yttm  train_data_path = ""train_data.txt"" model_path = ""example.model""  #: Generating random file with training data #: 10000 lines with 100 characters in each line n_lines = 10000 n_characters = 100 with open(train_data_path  ""w"") as fout:     for _ in range(n_lines):         print("""".join([random.choice(""abcd "") for _ in range(n_characters)])  file=fout)  #: Generating random text test_text = """".join([random.choice(""abcde "") for _ in range(100)])  #: Training model yttm.BPE.train(data=train_data_path  vocab_size=5000  model=model_path)  #: Loading model bpe = yttm.BPE(model=model_path)  #: Two types of tokenization print(bpe.encode([test_text]  output_type=yttm.OutputType.ID)) print(bpe.encode([test_text]  output_type=yttm.OutputType.SUBWORD)) ```  &nbsp;  ```bash $ yttm bpe --data TRAINING_DATA_FILE --model OUTPUT_MODEL_FILE --vocab_size 2000 $ yttm encode --model OUTPUT_MODEL_FILE --output_type subword < TEST_DATA_FILE > ENCODED_DATA  ```    """;Natural Language Processing;https://github.com/VKCOM/YouTokenToMe
"""Do we need new vocab or can we just use one   """;Natural Language Processing;https://github.com/iejMac/ScriptWriter
"""You have to compile your model with focal loss. Sample: ``` model_prn.compile(optimizer=optimizer  loss=[focal_loss(alpha=.25  gamma=2)]) ```   """;General;https://github.com/mkocabas/focal-loss-keras
"""pip install tensorboard_logger   """;General;https://github.com/andreasveit/densenet-pytorch
"""A general YOLOv4/v3/v2 object detection pipeline inherited from [keras-yolo3-Mobilenet](https://github.com/Adamdad/keras-YOLOv3-mobilenet)/[keras-yolo3](https://github.com/qqwweee/keras-yolo3) and [YAD2K](https://github.com/allanzelener/YAD2K). Implement with tf.keras  including data collection/annotation  model training/tuning  model evaluation and on device deployment. Support different architecture and different technologies:      #: cd tools && python voc_annotation.py -h                              both      You can merge these train &amp; val annotation file as your need. For example  following cmd will creat 07/12 combined trainval dataset:      #: cd tools && python coco_annotation.py -h   If you want to download PascalVOC or COCO dataset  refer to Dockerfile for cmd     --gpu_num GPU_NUM     Number of GPU to use  default=1   You can also use Tensorboard to monitor the loss trend during train:   If you're evaluating with MSCOCO dataset  you can further use pycoco_eval.py with the generated txt detection result and COCO GT annotation to get official COCO AP with pycocotools:   : cd tools && python pycoco_eval.py -h   The test environment is   Python 3.6.8   1. Install requirements on Ubuntu 16.04/18.04:  ``` #: apt install python3-opencv #: pip install -r requirements.txt ```  2. Download Related Darknet/YOLOv2/v3/v4 weights from [YOLO website](http://pjreddie.com/darknet/yolo/) and [AlexeyAB/darknet](https://github.com/AlexeyAB/darknet). 3. Convert the Darknet YOLO model to a Keras model. 4. Run YOLO detection on your image or video  default using Tiny YOLOv3 model.  ``` #: wget -O weights/darknet53.conv.74.weights https://pjreddie.com/media/files/darknet53.conv.74 #: wget -O weights/darknet19_448.conv.23.weights https://pjreddie.com/media/files/darknet19_448.conv.23 #: wget -O weights/yolov3.weights https://pjreddie.com/media/files/yolov3.weights #: wget -O weights/yolov3-tiny.weights https://pjreddie.com/media/files/yolov3-tiny.weights #: wget -O weights/yolov3-spp.weights https://pjreddie.com/media/files/yolov3-spp.weights #: wget -O weights/yolov2.weights http://pjreddie.com/media/files/yolo.weights #: wget -O weights/yolov2-voc.weights http://pjreddie.com/media/files/yolo-voc.weights #: wget -O weights/yolov2-tiny.weights https://pjreddie.com/media/files/yolov2-tiny.weights #: wget -O weights/yolov2-tiny-voc.weights https://pjreddie.com/media/files/yolov2-tiny-voc.weights  #: wget -O weights/yolov4.weights https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights #: wget -O weights/yolov4.conv.137.weights https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137  #: python tools/convert.py cfg/yolov3.cfg weights/yolov3.weights weights/yolov3.h5 #: python tools/convert.py cfg/yolov3-tiny.cfg weights/yolov3-tiny.weights weights/yolov3-tiny.h5 #: python tools/convert.py cfg/yolov3-spp.cfg weights/yolov3-spp.weights weights/yolov3-spp.h5 #: python tools/convert.py cfg/yolov2.cfg weights/yolov2.weights weights/yolov2.h5 #: python tools/convert.py cfg/yolov2-voc.cfg weights/yolov2-voc.weights weights/yolov2-voc.h5 #: python tools/convert.py cfg/yolov2-tiny.cfg weights/yolov2-tiny.weights weights/yolov2-tiny.h5 #: python tools/convert.py cfg/yolov2-tiny-voc.cfg weights/yolov2-tiny-voc.weights weights/yolov2-tiny-voc.h5 #: python tools/convert.py cfg/darknet53.cfg weights/darknet53.conv.74.weights weights/darknet53.h5 #: python tools/convert.py cfg/darknet19_448_body.cfg weights/darknet19_448.conv.23.weights weights/darknet19.h5  #:#:#: make sure to reorder output tensors for YOLOv4 cfg and weights file #: python tools/convert.py --yolo4_reorder cfg/yolov4.cfg weights/yolov4.weights weights/yolov4.h5  #: python yolo.py --image #: python yolo.py --input=<your video file> ``` For other model  just do in a similar way  but specify different model type  weights path and anchor path with `--model_type`  `--weights_path` and `--anchors_path`.  Image detection sample:  <p align=""center"">   <img src=""assets/dog_inference.jpg"">   <img src=""assets/kite_inference.jpg""> </p>   1. [yolo.py](https://github.com/david8862/keras-YOLOv3-model-set/blob/master/yolo.py) > * Demo script for trained model  image detection mode ``` #: python yolo.py --model_type=yolo3_mobilenet_lite --weights_path=model.h5 --anchors_path=configs/yolo3_anchors.txt --classes_path=configs/voc_classes.txt --model_image_size=416x416 --image ``` video detection mode ``` #: python yolo.py --model_type=yolo3_mobilenet_lite --weights_path=model.h5 --anchors_path=configs/yolo3_anchors.txt --classes_path=configs/voc_classes.txt --model_image_size=416x416 --input=test.mp4 ``` For video detection mode  you can use ""input=0"" to capture live video from web camera and ""output=<video name>"" to dump out detection result to another video   """;Computer Vision;https://github.com/grifon-239/diploma
"""This repository contains the official code and pretrained models for [CoaT: Co-Scale Conv-Attentional Image Transformers](http://arxiv.org/abs/2104.06399). It introduces (1) a co-scale mechanism to realize fine-to-coarse  coarse-to-fine and cross-scale attention modeling and (2) an efficient conv-attention module to realize relative position encoding in the factorized attention.  <img src=""./figures/model-acc.svg"" alt=""Model Accuracy"" width=""600"" />  For more details  please refer to [CoaT: Co-Scale Conv-Attentional Image Transformers](http://arxiv.org/abs/2104.06399) by [Weijian Xu*](https://weijianxu.com/)  [Yifan Xu*](https://yfxu.com/)  [Tyler Chang](https://tylerachang.github.io/)  and [Zhuowen Tu](https://pages.ucsd.edu/~ztu/).   1. Clone the repo.    ```bash    git clone https://github.com/mlpc-ucsd/CoaT    cd CoaT    ```  2. Download ImageNet dataset (ILSVRC 2012) and extract.    ```bash    #: Create dataset folder.    mkdir -p ./data/ImageNet     #: Download the dataset (not shown here) and copy the files (assume the download path is in $DATASET_PATH).    cp $DATASET_PATH/ILSVRC2012_img_train.tar $DATASET_PATH/ILSVRC2012_img_val.tar $DATASET_PATH/ILSVRC2012_devkit_t12.tar.gz ./data/ImageNet     #: Extract the dataset.    python -c ""from torchvision.datasets import ImageNet; ImageNet('./data/ImageNet'  split='train')""    python -c ""from torchvision.datasets import ImageNet; ImageNet('./data/ImageNet'  split='val')""    #: After the extraction  you should observe `train` and `val` folders under ./data/ImageNet.    ```   1. Set up a new conda environment and activate it.    ```bash    #: Create an environment with Python 3.8.    conda create -n coat python==3.8    conda activate coat    ```  2. Install required packages.    ```bash    #: Install PyTorch 1.7.1 w/ CUDA 11.0.    pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html     #: Install timm 0.3.2.    pip install timm==0.3.2     #: Install einops.    pip install einops    ```   | Name | AP | AP50 | AP75 | APS | APM | APL |      ```bash     #: Download the pretrained checkpoint.     mkdir -p ./output/pretrained      #: Usage: bash ./scripts/eval.sh [model name] [output folder] [checkpoint path]     bash ./scripts/eval.sh coat_lite_tiny coat_lite_tiny_pretrained ./output/pretrained/coat_lite_tiny_e88e96b0.pth      bash      bash     #: Usage: bash ./scripts/train.sh [model name] [output folder]     bash ./scripts/train.sh coat_lite_tiny coat_lite_tiny      ```bash      bash ./scripts/train_extra_args.sh coat_lite_medium coat_lite_medium_384x384 \         --drop-path 0.2 \   The following usage is provided for the classification task using CoaT model. For the other tasks  please follow the corresponding readme  such as [instance segmentation](./tasks/mmdet/README.md) and [object detection](./tasks/Deformable-DETR/README.md).   """;Computer Vision;https://github.com/mlpc-ucsd/CoaT
"""This project help you can use PyramidNet_SakeDrop model just like how you use the PyramidNet model.   You only need join the model folder into your project and use the PyramidNet_SakeDrop model with the simple following codes:   ``` python from model import PyramidNet_ShakeDrop net = PyramidNet_ShakeDrop(depth=101 alpha=270 num_classes=1000) ```  """;General;https://github.com/Dragonsson/SakeDrop-Pytorch
"""GPU: Nvidia GTX 1080Ti   Python: 3.6.8  Pytorch: 1.3.0   ├── requirements.txt   - https://github.com/pianomania/infoGAN-pytorch  - https://github.com/Natsu6767/InfoGAN-PyTorch  - https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/infogan   ```bash @ project root pip install -r requirements.txt  #: with local visdom server python -m visdom.server python src/main.py --use_visdom True  #: with remote visdom server #:#: In remote machine python -m visdom.server #:#: In local machine python src/main.py --use_visdom True --visdom_server <http://your_server_ip>  #: See training process in http://localhost:8097 in the machine where your visdom server is running  #: without visdom logger python src/main.py --use_visdom False  ```   """;Computer Vision;https://github.com/JunsikChoi/Pytorch-InfoGAN-CR
"""It is tested with pytorch-1.0.   """;Computer Vision;https://github.com/mengxingshifen1218/pointnet.pytorch
"""cd cnn   Instructions for acquiring PTB and WT2 can be found here. While CIFAR-10 can be automatically downloaded by torchvision  ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions here.   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/adammenges/DARTS-PyTorch
"""1. Clone this repo and install requirements     ```command    git clone https://github.com/L0SG/NanoFlow.git    cd NanoFlow    pip install -r requirements.txt    ```  2. Install [Apex] for mixed-precision training    Below are the example commands using nanoflow-h16-r128-emb512.json      insert `checkpoint_path: ""experiments/nanoflow-h16-r128-emb512/waveflow_5000""` in the config file then run    ```command    python train.py -c configs/nanoflow-h16-r128-emb512.json    ```     for loading averaged weights over 10 recent checkpoints  insert `checkpoint_path: ""experiments/nanoflow-h16-r128-emb512""` in the config file then run    ```command    python train.py -a 10 -c configs/nanoflow-h16-r128-emb512.json    ```     you can reset the optimizer and training scheduler (and keep the weights) by providing `--warm_start`    ```command    python train.py --warm_start -c configs/nanoflow-h16-r128-emb512.json    ```     4. Synthesize waveform from the trained model.     insert `checkpoint_path` in the config file and use `--synthesize` to `train.py`. The model generates waveform by looping over `test_files.txt`.    ```command    python train.py --synthesize -c configs/nanoflow-h16-r128-emb512.json    ```    if `fp16_run: true`  the model uses FP16 (half-precision) arithmetic for faster performance (on GPUs equipped with Tensor Cores).    """;Computer Vision;https://github.com/L0SG/NanoFlow
"""__[step 1.] Prepare dataset__    The author of progressive GAN released CelebA-HQ dataset  and which Nash is working on over on the branch that i forked this from. For my version just make sure that all images are the children of that folder that you declare in Config.py. Also i warn you that if you use multiple classes  they should be similar as to not end up with attrocities.   ~~~ --------------------------------------------- The training data folder should look like :  <train_data_root>                 |--Your Folder                         |--image 1                         |--image 2                         |--image 3 ... --------------------------------------------- ~~~  __[step 2.] Prepare environment using virtualenv__      + you can easily set PyTorch (v0.3) and TensorFlow environment using virtualenv.   + CAUTION: if you have trouble installing PyTorch  install it mansually using pip. [[PyTorch Install]](http://pytorch.org/)   + For install please take your time and install all dependencies of PyTorch and also install tensorflow      ~~~   $ virtualenv --python=python2.7 venv   $ . venv/bin/activate   $ pip install -r requirements.txt   $ conda install pytorch torchvision -c pytorch   ~~~    __[step 3.] Run training__       + edit `config.py` to change parameters. (don't forget to change path to training images) + specify which gpu devices to be used  and change ""n_gpu"" option in `config.py` to support Multi-GPU training. + run and enjoy!    ~~~~   (example)   If using Single-GPU (device_id = 0):   $ vim config.py   -->   change ""n_gpu=1""   $ CUDA_VISIBLE_DEVICES=0 python trainer.py      If using Multi-GPUs (device id = 1 3 7):   $ vim config.py   -->   change ""n_gpu=3""   $ CUDA_VISIBLE_DEVICES=1 3 7 python trainer.py ~~~~      __[step 4.] Display on tensorboard__   (At the moment skip this part) + you can check the results on tensorboard.  <p align=""center""><img src=""https://puu.sh/ympU0/c38f4e7d33.png"" width=""700""></p>    <p align=""center""><img src=""https://puu.sh/ympUe/bf9b53dea8.png"" width=""700"" align=""center""></p>       ~~~   $ tensorboard --logdir repo/tensorboard --port 8888   $ <host_ip>:8888 at your browser.   ~~~       __[step 5.] Generate fake images using linear interpolation__    ~~~ CUDA_VISIBLE_DEVICES=0 python generate_interpolated.py ~~~        """;Computer Vision;https://github.com/nashory/pggan-pytorch
"""1. Archive your training data and upload it to an S3 bucket 2. Provision your EC2 instance (I used an Ubuntu AMI) 3. Log into your EC2 instance via SSH 4. Install the aws CLI client and configure it:  ```bash sudo snap install aws-cli --classic aws configure ```  You will then have to enter your AWS access keys  which you can retrieve from the management console under AWS Management Console > Profile > My Security Credentials > Access Keys  Then  run these commands  or maybe put them in a shell script and execute that:  ```bash mkdir data curl -O https://bootstrap.pypa.io/get-pip.py sudo apt-get install python3-distutils python3 get-pip.py pip3 install stylegan2_pytorch export PATH=$PATH:/home/ubuntu/.local/bin aws s3 sync s3://<Your bucket name> ~/data cd data tar -xf ../train.tar.gz ```  Now you should be able to train by simplying calling `stylegan2_pytorch [args]`.  Notes:  * If you have a lot of training data  you may need to provision extra block storage via EBS. * Also  you may need to spread your data across multiple archives. * You should run this on a `screen` window so it won't terminate once you log out of the SSH session.   You will need a machine with a GPU and CUDA installed. Then pip install the package like this  ```bash $ pip install stylegan2_pytorch ```  If you are using a windows machine  the following commands reportedly works.  ```bash $ conda install pytorch torchvision -c python $ pip install stylegan2_pytorch ```   ```bash $ stylegan2_pytorch --data /path/to/images ```  That's it. Sample images will be saved to `results/default` and models will be saved periodically to `models/default`.   You can specify the name of your project with  ```bash $ stylegan2_pytorch --data /path/to/images --name my-project-name ```  You can also specify the location where intermediate results and model checkpoints should be stored with  ```bash $ stylegan2_pytorch --data /path/to/images --name my-project-name --results_dir /path/to/results/dir --models_dir /path/to/models/dir ```  By default  if the training gets cut off  it will automatically resume from the last checkpointed file. If you want to restart with new settings  just add a `new` flag  ```bash $ stylegan2_pytorch --new --data /path/to/images --name my-project-name --image-size 512 --batch-size 1 --gradient-accumulate-every 16 --network-capacity 10 ```  Once you have finished training  you can generate images from your latest checkpoint like so.  ```bash $ stylegan2_pytorch  --generate ```  To generate a video of a interpolation through two random points in latent space.  ```bash $ stylegan2_pytorch --generate-interpolation ```  To save each individual frame of the interpolation  ```bash $ stylegan2_pytorch --generate-interpolation --save-frames ```  If a previous checkpoint contained a better generator  (which often happens as generators start degrading towards the end of training)  you can load from a previous checkpoint with another flag  ```bash $ stylegan2_pytorch --generate --load-from {checkpoint number} ```   """;Computer Vision;https://github.com/ShreyasArthur/StyleGAN-2-with-Urban-Plans
"""Use .sh files to execute Note that --realnvp use model RealNVP to model toy data   """;Computer Vision;https://github.com/Daniel-H-99/CRD
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/LIU1514/Yolov3-
"""0. [Requirements](#requirements) 0. [Usage](#usage) 0. [Example](#example) 0. [Customization](#customization) 0. [Citation](#citation)  <!---      cd network_utils       bash   In order to apply NetAdapt  run:                      python master.py [-h] [-gp GPUS [GPUS ...]] [-re] [-im INIT_MODEL_PATH]                  [-mi MAX_ITERS] [-lr FINETUNE_LR] [-bu BUDGET]                  [-bur BUDGET_RATIO] [-rt RESOURCE_TYPE]                  [-ir INIT_RESOURCE_REDUCTION]                  [-irr INIT_RESOURCE_REDUCTION_RATIO]                  [-rd RESOURCE_REDUCTION_DECAY]                  [-st SHORT_TERM_FINE_TUNE_ITERATION] [-lt LOOKUP_TABLE_PATH]                  [-dp DATASET_PATH] [-a ARCH] [-si SAVE_INTERVAL]                  working_folder input_data_shape input_data_shape                  input_data_shape  - `working_folder`: Root folder where models  related files and history information are saved. You can see how models are pruned progressively in `working_folder/master/history.txt`.  - `input_data_shape`: Input data shape (C  H  W) (default: 3 224 224). If you want to apply NetAdapt to different tasks  you might need to change data shape.  - `-h  --help`: Show this help message and exit.              - `-gp GPUS [GPUS ...]   --gpus GPUS [GPUS ...]`: Indices of available gpus (default: 0).              - `-re  --resume`: Resume from previous iteration. In order to resume  specify `--resume` and specify `working_folder` as the one you want to resume.  The resumed arguments will overwrite the arguments provided here. For example  if you want to simplify a model by pruning and finetuning for 30 iterations (under `working_folder`)  however  your program terminated after 20 iterations. Then you can use `--resume` to restore and continue for the last 10 iterations.              - `-im INIT_MODEL_PATH  --init_model_path INIT_MODEL_PATH`: Path to pretrained model.              - `-mi MAX_ITERS  --max_iters MAX_ITERS`: Maximum iteration of removing filters and short-term fine-tune (default: 10).              - `-lr FINETUNE_LR  --finetune_lr FINETUNE_LR`: Short-term fine-tune learning rate (default: 0.001).              - `-bu BUDGET  --budget BUDGET`: Resource constraint. If resource < `budget`  the process is terminated.              - `-bur BUDGET_RATIO  --budget_ratio BUDGET_RATIO`: If `--budget` is not specified  `buget` = `budget_ratio`\*(pretrained model resource) (default: 0.25).              - `-rt RESOURCE_TYPE  --resource_type RESOURCE_TYPE`: Resource constraint type (default: FLOPS). We currently support `FLOPS`  `WEIGHTS`  and `LATENCY` (device `cuda:0`). If you want to add other resource types  please modify `def compute_resource(...)` in `network_util` python files (e.g. `network_utils/network_utils_alexnet`).              - `-ir INIT_RESOURCE_REDUCTION  --init_resource_reduction INIT_RESOURCE_REDUCTION`: For each iteration  target resource = current resource - `init_resource_reduction`\*(`resource_reduction_decay`\*\*(iteration-1)).  - `-irr INIT_RESOURCE_REDUCTION_RATIO  --init_resource_reduction_ratio INIT_RESOURCE_REDUCTION_RATIO`: If `--init_resource_reduction` is not specified  `init_resource_reduction` = `init_resource_reduction_ratio`\*(pretrained model resource) (default: 0.025).  - `-rd RESOURCE_REDUCTION_DECAY  --resource_reduction_decay RESOURCE_REDUCTION_DECAY`: For each iteration  target resource = current resource - `init_resource_reduction`\*(`resource_reduction_decay`\*\*(iteration-1)) (default: 0.96).              - `-st SHORT_TERM_FINE_TUNE_ITERATION  --short_term_fine_tune_iteration SHORT_TERM_FINE_TUNE_ITERATION`: Short-term fine-tune iteration (default: 10).  - `-lt LOOKUP_TABLE_PATH  --lookup_table_path LOOKUP_TABLE_PATH`: Path to lookup table.  - `-dp DATASET_PATH  --dataset_path DATASET_PATH`: Path to dataset.  - `-a ARCH  --arch ARCH  network_utils`: Defines how networks are pruned  fine-tuned  and evaluated. If you want to use your own method  please see [**Customization**](#customization) and specify here. (default: alexnet)              - `-si SAVE_INTERVAL  --save_interval SAVE_INTERVAL`: Interval of iterations that all pruned models at the same iteration will be saved.  Use `-1` to save only the best model at each iteration.  Use `1` to save all models at each iteration. (default: -1).     <!---  We provide an example of applying **NetAdapt** to **AlexNet** on **CIFAR-10**.  If you want to apply the algorithm to different networks or even different tasks  please see the following [**Customization**](#customization) section.   The example consists of the following four steps: 1. training AlexNet on CIFAR-10; 2. measuring latency; 3. applying NetAdapt; 4. evaluation using adapted models.  1. **Training AlexNet on CIFAR-10.**      Train from scratch:     ```bash     python train.py data/ --dir models/alexnet/no-pretrain/checkpoint.pth.tar --arch alexnet     ```     Use Imagenet-pretrained model:     ```bash     python train.py data/ --dir models/alexnet/pretrain/checkpoint.pth.tar --pretrained --lr 0.01 --arch alexnet     ```     Evaluation:     ```bash     python eval.py data/ --dir models/alexnet/no-pretrain/checkpoint.pth.tar --arch alexnet     ```          ```bash     python train.py data/ --dir models/mobilenet/model.pth.tar --arch mobilenet     ``` 2. **Measuring Latency**      Here we build the latency lookup table for `cuda:0` device:     ```bash     python build_lookup_table.py --dir latency_lut/lut_alexnet.pkl --arch alexnet     ```     It measures latency of different layers contained in the network (i.e. **AlexNet** here).     For conv layers  the sampled numbers of feature channels are multiples of `MIN_CONV_FEATURE_SIZE`.     For fc layers  the sampled numbers of features are multiples of `MIN_FC_FEATURE_SIZE`.           3. **Applying NetAdapt**      Modify which GPUs will be utilized (`-gp`) in `netadapt_alexnet.sh` and run the script to apply NetAdapt to a pretrained model:     ```bash     sh netadapt_alexnet.sh     ```          After obtaining the adapted model  we need to finetune the model:     ```bash     python train.py data/ --arch alexnet --resume models/alexnet/no-pretrain/prune-by-mac/iter_100_best_model.pth.tar --dir models/alexnet/no-pretrain/checkpoint-adapt.pth.tar     ```          <p align=""center""> 	<img src=""fig/netadapt_algo.png"" alt=""photo not available"" width=""90%"" height=""90%"">     </p>  4. **Evaluation Using Adapted Models**      After applying NetAdapt to a pretrained model  we can evaluate this adapted model using:     ```bash     python eval.py data/ --dir models/alexnet/no-pretrain/checkpoint-adapt.pth.tar --arch alexnet     ```     The adapted model can be restored **without modifying the orignal python file**.  --->  We provide a simple example of applying **NetAdapt** to a very small [network](nets/helloworld.py):              sh scripts/netadapt_helloworld.sh  Detailed examples of applying **NetAdapt** to **AlexNet**/**MobileNet** on **CIFAR-10** are shown [**here (AlexNet)**](docs/alexnet/README.md) and [**here (MobileNet)**](docs/mobilenet/README.md).  <p align=""center""> <img src=""fig/netadapt_algo.png"" alt=""photo not available"" width=""90%"" height=""90%""> </p>  If you want to apply the algorithm to different networks or even different tasks  please see the following [**Customization**](#customization) section.    """;General;https://github.com/denru01/netadapt
""": setup optimizer   <b>All results below are without fine-tuning.</b>  <p align=""center""><b> BigGAN (z-space) - ImageNet (256x256) </b></p>  ![](./assets/biggan_comparison.png)  <p align=""center""><b> StyleGAN2 (z-space) - LSUN Cars (384x512) </b></p>  ![](./assets/stylegan2_cars.png)  <p align=""center""><b> StyleGAN2 (z-space) - FFHQ (1024x1024) </b></p>  ![](./assets/stylegan2_ffhq.png)    - <b>Install PyTorch</b>     Install the correct [PyTorch version](https://pytorch.org/) for your machine    - <b>Install the python dependencies</b>     Install the remaining dependencies via   ```bash   pip install -r requirements.txt   ``` - <b>Install pix2latent</b>   ```bash   git clone https://github.com/minyoungg/pix2latent   cd pix2latent   pip install .   ```   We provide several demo codes in `./examples/` for both [`BigGAN`](https://arxiv.org/abs/1809.11096) and [`StyleGAN2`](https://arxiv.org/abs/1912.04958). Note that the codebase has been tuned and developed on `BigGAN`.   ```bash > cd examples > python invert_biggan_adam.py --num_samples 4 ```  Using the `make_video` flag will save the optimization trajectory as a video. ```bash > python invert_biggan_adam.py --make_video --num_samples 4 ```  **(slow)** To optimize with `CMA-ES` or `BasinCMA`  we use [PyCMA](https://github.com/CMA-ES/pycma). Note that the PyCMA version of CMA-ES has a predefined number of samples to jointly evaluate (18 for BigGAN) and (22 for StyleGAN2).  ```bash > python invert_biggan_cma.py  > python invert_biggan_basincma.py  ```  **(fast)** Alternatively CMA-ES in [Nevergrad](https://github.com/facebookresearch/nevergrad) provides sample parallelization so you can set your own number of samples. Although this runs faster  we have observed the performance to be slightly worse. **(warning: performance depends on num_samples)**. ```bash > python invert_biggan_nevergrad.py --ng_method CMA --num_samples 4 > python invert_biggan_hybrid_nevergrad.py --ng_method CMA --num_samples 4 ```  Same applies to `StyleGAN2`. See `./examples/` for extensive list of examples.    """;General;https://github.com/minyoungg/pix2latent
"""PyTorch version of DeepCTR.   """;General;https://github.com/shenweichen/DeepCTR-Torch
"""<img src=""./instruction_images/Windows/Step_1_DD_Win.png"" width=""480px""></img>  Presuming Python is installed:  - Open command prompt and navigate to the directory of your current version of Python ```bash   pip install deep-daze ```   ```bash $ pip install deep-daze ```     This will require that you have an Nvidia GPU or AMD GPU   - Minimum Requirements: 4GB VRAM (Using VERY LOW settings  see usage instructions below)           (required) A phrase less than 77 tokens which you would like to visualize.   We get:   ```bash $ imagine ""a house in the forest"" ``` For Windows:  <img src=""./instruction_images/Windows/Step_2_DD_Win.png"" width=""480px""></img>  - Open command prompt as administrator ```bash   imagine ""a house in the forest"" ```  That's it.   If you have enough memory  you can get better quality by adding a `--deeper` flag  ```bash $ imagine ""shattered plates on the ground"" --deeper ```   If you have at least 16 GiB of vram available  you should be able to run these settings with some wiggle room. ```python imagine = Imagine(     text=text      num_layers=42      batch_size=64      gradient_accumulate_every=1  ) ```   ```python imagine = Imagine(     text=text      num_layers=24      batch_size=16      gradient_accumulate_every=2 ) ```   If you are desperate to run this on a card with less than 8 GiB vram  you can lower the image_width. ```python imagine = Imagine(     text=text      image_width=256      num_layers=16      batch_size=1      gradient_accumulate_every=16 #: Increase gradient_accumulate_every to correct for loss in low batch sizes ) ```   This is just a teaser. We will be able to generate images  sound  anything at will  with natural language. The holodeck is about to become real in our lifetimes.  Please join replication efforts for DALL-E for <a href=""https://github.com/lucidrains/dalle-pytorch"">Pytorch</a> or <a href=""https://github.com/EleutherAI/DALLE-mtf"">Mesh Tensorflow</a> if you are interested in furthering this technology.   """;General;https://github.com/lucidrains/deep-daze
"""pip install git+https://github.com/rcmalli/keras-squeezenet.git  pip install keras_squeezenet   ~~~python import numpy as np from keras_squeezenet import SqueezeNet from keras.applications.imagenet_utils import preprocess_input  decode_predictions from keras.preprocessing import image  model = SqueezeNet()  img = image.load_img('../images/cat.jpeg'  target_size=(227  227)) x = image.img_to_array(img) x = np.expand_dims(x  axis=0) x = preprocess_input(x)  preds = model.predict(x) print('Predicted:'  decode_predictions(preds))  ~~~    """;Computer Vision;https://github.com/rcmalli/keras-squeezenet
"""conda 4.9.2  TensorFlow GPU 1.15   NVIDIA GPU drivers 461.33   https://www.youtube.com/watch?v=ijOh_TxSuAE&ab_channel=%EC%9E%90%EB%9D%BC%EC%9E%90   """;Computer Vision;https://github.com/BonJunKu/ScooterHelmetDetector
"""This NAS-FCOS implementation is based on [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark). Therefore the installation is the same as original maskrcnn-benchmark.  Please check [INSTALL.md](INSTALL.md) for installation instructions. You may also want to see the original [README.md](MASKRCNN_README.md) of maskrcnn-benchmark.   The train command line on coco train:   For your convenience  we provide the following trained models (more models are coming soon).   """;Computer Vision;https://github.com/Lausannen/NAS-FCOS
"""If you'd like to submit a pull request  you'll need to clone the repository;   """;Computer Vision;https://github.com/google-research/google-research
"""**Hyperparameter sets** are encoded in [`HParams`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/utils/hparam.py) objects  and are registered with [`@registry.register_hparams`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/utils/registry.py). Every model and problem has a `HParams`. A basic set of hyperparameters are defined in [`common_hparams.py`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/layers/common_hparams.py) and hyperparameter set functions can compose other hyperparameter set functions.   ``` #: Assumes tensorflow or tensorflow-gpu installed pip install tensor2tensor  #: Installs with tensorflow-gpu requirement pip install tensor2tensor[tensorflow_gpu]  #: Installs with tensorflow (cpu) requirement pip install tensor2tensor[tensorflow] ```  Binaries:  ``` #: Data generator t2t-datagen  #: Trainer t2t-trainer --registry_help ```  Library usage:  ``` python -c ""from tensor2tensor.models.transformer import Transformer"" ```   Run on FloydHub   run either on Cloud TPUs or on 8-GPU machines; you might need  to modify the hyperparameters if you run on a different setup.   few steps (e.g.  --train_steps=2000).   You can get translations in the other direction by appending _rev to  the problem name  e.g.  for German-English use   pip install tensor2tensor   mkdir -p $DATA_DIR $TMP_DIR $TRAIN_DIR   [This iPython notebook](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb) explains T2T and runs in your browser using a free VM from Google  no installation needed. Alternatively  here is a one-command version that installs T2T  downloads MNIST  trains a model and evaluates it:  ``` pip install tensor2tensor && t2t-trainer \   --generate_data \   --data_dir=~/t2t_data \   --output_dir=~/t2t_train/mnist \   --problem=image_mnist \   --model=shake_shake \   --hparams_set=shake_shake_quick \   --train_steps=1000 \   --eval_steps=100 ```   """;Natural Language Processing;https://github.com/tensorflow/tensor2tensor
"""Local Setup (Use Conda : recommended)  https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html#installation     Backprop (Mathematical Version):   Pytorch Autograd:   https://www.youtube.com/watch?v=py5byOOHZM8    Based on following Dataset:   """;Computer Vision;https://github.com/mabhay3420/Deep-Into-CNN
"""You can train models with ./train.py.   Dataset: CelebA(https://www.kaggle.com/jessicali9530/celeba-dataset/version/2)   """;Computer Vision;https://github.com/RUTILEA/Chainer-StyleBasedGAN
"""<details> <summary>Installation</summary>  Step1. Install YOLOX. ```shell git clone git@github.com:jesse01/paddle-yolox.git ``` Step2. Install [apex](https://github.com/NVIDIA/apex).  ```shell #: skip this step if you don't want to train model. git clone https://github.com/NVIDIA/apex cd apex pip3 install -v --disable-pip-version-check --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ``` Step3. Install [pycocotools](https://github.com/cocodataset/cocoapi).  ```shell pip3 install cython; pip3 install 'git+https://github.com/cocodataset/cocoapi.git#:subdirectory=PythonAPI' ```  </details>  <details> <summary>Demo</summary>  Step1. Download a pretrained model from the benchmark table.  Step2. Use either -n or -f to specify your detector's config. For example:  ```shell python tools/demo.py image -n yolox-s -c /path/to/your/yolox_s.pth --path assets/dog.jpg --conf 0.30 --nms 0.45 --tsize 640 --save_result ``` or ```shell python tools/demo.py image -f exps/default/yolox_s.py -c /path/to/your/yolox_s.pth --path assets/dog.jpg --conf 0.25 --nms 0.45 --tsize 640 --save_result ``` Demo for video: ```shell python tools/demo.py video -n yolox-s -c /path/to/your/yolox_s.pth --path /path/to/your/video --conf 0.25 --nms 0.45 --tsize 640 --save_result ```   </details>  <details> <summary>Reproduce our results on COCO</summary>  Step1. Prepare COCO dataset ```shell cd <YOLOX_HOME> ln -s /path/to/your/COCO ./datasets/COCO ```  Step2. Reproduce our results on COCO by specifying -n:  ```shell python tools/train.py -n yolox-s -b 8 -o                          yolox-m                          yolox-l                          yolox-x ``` * -m: paddle.distributed.launch  multiple gpu training * -b: total batch size  the recommended number for -b is num-gpu * 8  **Multi GPU Training**  python -m paddle.distributed.launch tools/train.py   -n yolox-s -b 64 -o                                                         yolox-m                                                         yolox-l                                                         yolox-x  When using -f  the above commands are equivalent to:  ```shell python tools/train.py -f exps/default/yolox-s.py  -b 64 -o                          exps/default/yolox-m.py                          exps/default/yolox-l.py                          exps/default/yolox-x.py ```  </details>   <details> <summary>Evaluation</summary>  We support batch testing for fast evaluation:  ```shell python tools/eval.py -n  yolox-s -c yolox_s.pth -b 8 --conf 0.001 [--fuse]                          yolox-m                          yolox-l                          yolox-x ``` * --fuse: fuse conv and bn * -b: total batch size across on all GPUs  To reproduce speed test  we use the following command: ```shell python tools/eval.py -n  yolox-s -c yolox_s.pth -b 1 --conf 0.001 --fuse                          yolox-m                          yolox-l                          yolox-x ```  </details>    """;Computer Vision;https://github.com/jesse01/paddle-yolox
"""Datasets are stored as uncompressed ZIP archives containing uncompressed PNG files and a metadata file `dataset.json` for labels.  Custom datasets can be created from a folder containing images; see [`python dataset_tool.py --help`](./docs/dataset-tool-help.txt) for more information. Alternatively  the folder can also be used directly as a dataset  without running it through `dataset_tool.py` first  but doing so may lead to suboptimal performance.  Legacy TFRecords datasets are not supported &mdash; see below for instructions on how to convert them.  **FFHQ**:  Step 1: Download the [Flickr-Faces-HQ dataset](https://github.com/NVlabs/ffhq-dataset) as TFRecords.  Step 2: Extract images from TFRecords using `dataset_tool.py` from the [TensorFlow version of StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada/):  ```.bash #: Using dataset_tool.py from TensorFlow version at #: https://github.com/NVlabs/stylegan2-ada/ python ../stylegan2-ada/dataset_tool.py unpack \     --tfrecord_dir=~/ffhq-dataset/tfrecords/ffhq --output_dir=/tmp/ffhq-unpacked ```  Step 3: Create ZIP archive using `dataset_tool.py` from this repository:  ```.bash #: Original 1024x1024 resolution. python dataset_tool.py --source=/tmp/ffhq-unpacked --dest=~/datasets/ffhq.zip  #: Scaled down 256x256 resolution. python dataset_tool.py --source=/tmp/ffhq-unpacked --dest=~/datasets/ffhq256x256.zip \     --width=256 --height=256 ```  **MetFaces**: Download the [MetFaces dataset](https://github.com/NVlabs/metfaces-dataset) and create ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/metfaces/images --dest=~/datasets/metfaces.zip ```  **AFHQ**: Download the [AFHQ dataset](https://github.com/clovaai/stargan-v2/blob/master/README.md#animal-faces-hq-dataset-afhq) and create ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/afhq/train/cat --dest=~/datasets/afhqcat.zip python dataset_tool.py --source=~/downloads/afhq/train/dog --dest=~/datasets/afhqdog.zip python dataset_tool.py --source=~/downloads/afhq/train/wild --dest=~/datasets/afhqwild.zip ```  **CIFAR-10**: Download the [CIFAR-10 python version](https://www.cs.toronto.edu/~kriz/cifar.html) and convert to ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/cifar-10-python.tar.gz --dest=~/datasets/cifar10.zip ```  **LSUN**: Download the desired categories from the [LSUN project page](https://www.yf.io/p/lsun/) and convert to ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/lsun/raw/cat_lmdb --dest=~/datasets/lsuncat200k.zip \     --transform=center-crop --width=256 --height=256 --max_images=200000  python dataset_tool.py --source=~/downloads/lsun/raw/car_lmdb --dest=~/datasets/lsuncar200k.zip \     --transform=center-crop-wide --width=512 --height=384 --max_images=200000 ```  **BreCaHAD**:  Step 1: Download the [BreCaHAD dataset](https://figshare.com/articles/BreCaHAD_A_Dataset_for_Breast_Cancer_Histopathological_Annotation_and_Diagnosis/7379186).  Step 2: Extract 512x512 resolution crops using `dataset_tool.py` from the [TensorFlow version of StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada/):  ```.bash #: Using dataset_tool.py from TensorFlow version at #: https://github.com/NVlabs/stylegan2-ada/ python dataset_tool.py extract_brecahad_crops --cropsize=512 \     --output_dir=/tmp/brecahad-crops --brecahad_dir=~/downloads/brecahad/images ```  Step 3: Create ZIP archive using `dataset_tool.py` from this repository:  ```.bash python dataset_tool.py --source=/tmp/brecahad-crops --dest=~/datasets/brecahad.zip ```   * GPU memory usage is comparable to the TensorFlow version.   | stylegan2-ada-pytorch | Main directory hosted on Amazon S3       --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl       --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl   The above code requires torch_utils and dnnlib to be accessible via PYTHONPATH. It does not need source code for the networks themselves &mdash; their class definitions are loaded from the pickle via torch_utils.persistence.   The name of the output directory reflects the training configuration. For example  00000-mydataset-auto1 indicates that the base configuration was auto1  meaning that the hyperparameters were selected automatically for training on one GPU. The base configuration is controlled by --cfg:       --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl   We employ the following metrics in the ADA paper. Execution time and GPU memory usage is reported for one NVIDIA Tesla V100 GPU at 1024x1024 resolution:   Pre-trained networks are stored as `*.pkl` files that can be referenced using local filenames or URLs:  ```.bash #: Generate curated MetFaces images without truncation (Fig.10 left) python generate.py --outdir=out --trunc=1 --seeds=85 265 297 849 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl  #: Generate uncurated MetFaces images with truncation (Fig.12 upper left) python generate.py --outdir=out --trunc=0.7 --seeds=600-605 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl  #: Generate class conditional CIFAR-10 images (Fig.17 left  Car) python generate.py --outdir=out --seeds=0-35 --class=1 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl  #: Style mixing example python style_mixing.py --outdir=out --rows=85 100 75 458 1500 --cols=55 821 1789 293 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl ```  Outputs from the above commands are placed under `out/*.png`  controlled by `--outdir`. Downloaded network pickles are cached under `$HOME/.cache/dnnlib`  which can be overridden by setting the `DNNLIB_CACHE_DIR` environment variable. The default PyTorch extension build directory is `$HOME/.cache/torch_extensions`  which can be overridden by setting `TORCH_EXTENSIONS_DIR`.  **Docker**: You can run the above curated image example using Docker as follows:  ```.bash docker build --tag sg2ada:latest . ./docker_run.sh python3 generate.py --outdir=out --trunc=1 --seeds=85 265 297 849 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl ```  Note: The Docker image requires NVIDIA driver release `r455.23` or later.  **Legacy networks**: The above commands can load most of the network pickles created using the previous TensorFlow versions of StyleGAN2 and StyleGAN2-ADA. However  for future compatibility  we recommend converting such legacy pickles into the new format used by the PyTorch version:  ```.bash python legacy.py \     --source=https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl \     --dest=stylegan2-cat-config-f.pkl ```   """;General;https://github.com/XieBaoshi/stylegan2
"""Part-II: https://medium.com/swlh/abstractive-text-summarization-using-transformers-3e774cc42453   """;Natural Language Processing;https://github.com/rojagtap/abstractive_summarizer
"""python inception_score.py --image_folder IMAGE_FOLDER_PATH      """;General;https://github.com/hanzhanggit/StackGAN-inception-model
"""    conda create -y -n fastai python=3.6     conda activate fastai     pip install dataclasses gpustat     conda install -y -c pytorch pytorch torchvision cudatoolkit=9.0     conda install -y -c fastai fastai     conda install -y ipykernel nbconvert ipywidgets scikit-learn     conda install -y -c conda-forge jupytext     conda install -y -c conda-forge jupyterlab     conda install -y -c conda-forge altair vega_datasets   *Project Structure and Organization based off https://github.com/callahantiff/Abra-Collaboratory/*  Once you have access  you can download the .gz version of the data files with these commands - replace mimicusername with your actual username:   """;General;https://github.com/magic-lantern/nlp-transfer-learning
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/theQuert/inlpfun
"""4、tfplot 0.2.0 (optional)             2、(Recommend in this repo) Or you can choose to use a better backbone  refer to gluon2TF.     cd $PATH_ROOT/libs/box_utils/cython_utils  python setup.py build_ext --inplace (or make)  cd $PATH_ROOT/libs/box_utils/   2、Make tfrecord      3、Multi-gpu train  cd $PATH_ROOT/tools   cd $PATH_ROOT/tools   cd $PATH_ROOT/output/summary   """;General;https://github.com/DetectionTeamUCAS/RetinaNet_Tensorflow_Rotation
"""[![Watch the video](figures/video_figure.png)](https://www.youtube.com/watch?v=a_OeT8MXzWI&feature=youtu.be)   ![](figures/cnn_imagenet_new.png)  ![](figures/imagenet_80_acc.png)   [News] OFA is available via pip! Run pip install ofa to install the whole OFA codebase.    ```python """""" OFA Networks.     Example: ofa_network = ofa_net('ofa_mbv3_d234_e346_k357_w1.0'  pretrained=True) """"""  from ofa.model_zoo import ofa_net ofa_network = ofa_net(net_id  pretrained=True)      #: Randomly sample sub-networks from OFA network ofa_network.sample_active_subnet() random_subnet = ofa_network.get_active_subnet(preserve_weight=True)      #: Manually set the sub-network ofa_network.set_active_subnet(ks=7  e=6  d=4) manual_subnet = ofa_network.get_active_subnet(preserve_weight=True) ``` If the above scripts failed to download  you download it manually from [Google Drive](https://drive.google.com/drive/folders/10leLmIiMtaRu4J46KwrBaMydvQt0qFuI?usp=sharing) and put them under $HOME/.torch/ofa_nets/.   ```python """""" OFA Specialized Networks. Example: net  image_size = ofa_specialized('flops@595M_top1@80.0_finetune@75'  pretrained=True) """"""  from ofa.model_zoo import ofa_specialized net  image_size = ofa_specialized(net_id  pretrained=True) ``` If the above scripts failed to download  you download it manually from [Google Drive](https://drive.google.com/drive/folders/1ez-t_DAHDet2fqe9TZUTJmvrU-AwofAt?usp=sharing) and put them under $HOME/.torch/ofa_specialized/.   [![Watch the video](figures/ofa-tutorial.jpg)](https://www.youtube.com/watch?v=wrsid5tvuSM)    """;General;https://github.com/mit-han-lab/once-for-all
"""<img src=""./Report/25_inf.png"">     (first row ~ last row)<br>     0633: this flower has petals that are yellow with red blotches <br>     0194: the flower has white stringy petals with yellow and purple pollen tubes<br>     2014: this flower is pink in color with only one large petal<br>     4683: this flower is yellow in color with petals that are rounded<br>     3327: the flower has a several pieces of yellow colored petals that looks similar to its leaves  network's variable should be update.   <img src=""./Report/25_inf.png"">     (first row ~ last row)<br>     0633: this flower has petals that are yellow with red blotches <br>     0194: the flower has white stringy petals with yellow and purple pollen tubes<br>     2014: this flower is pink in color with only one large petal<br>     4683: this flower is yellow in color with petals that are rounded<br>     3327: the flower has a several pieces of yellow colored petals that looks similar to its leaves           """;General;https://github.com/BeyondCloud/Comp04_ReverseImageCaption
"""A dataset for book recommendations: ten thousand books  one million ratings   *  [UFLDL Tutorial 1](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial) *  [UFLDL Tutorial 2](http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/) *  [Deep Learning for NLP (without Magic)](http://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial) *  [A Deep Learning Tutorial: From Perceptrons to Deep Networks](http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks) *  [Deep Learning from the Bottom up](http://www.metacademy.org/roadmaps/rgrosse/deep_learning) *  [Theano Tutorial](http://deeplearning.net/tutorial/deeplearning.pdf) *  [TensorFlow tutorials](https://github.com/nlintz/TensorFlow-Tutorials) *  [Deep Learning with R in Motion](https://www.manning.com/livevideo/deep-learning-with-r-in-motion) *  [Grokking Deep Learning in Motion](https://www.manning.com/livevideo/grokking-deep-learning-in-motion)  More details in [tutorials](tutorials.md)   """;Computer Vision;https://github.com/sandeeplbscek/awesome-deeplearning-resources
"""  [![GitHub Issues](https://img.shields.io/github/issues/lucylow/En_francais_si_vous_plait-.svg)](https://github.com/lucylow/En_francais_si_vous_plait-/issues)    [![GitHub Pull Requests](https://img.shields.io/github/issues-pr/lucylow/En_francais_si_vous_plait-.svg)](https://github.com/lucylow/En_francais_si_vous_plait-/pulls)   ```python   cd data/  bash prepare-iwslt14.sh   ```python   $ mkdir -p trainings/fconv   ```python    """;Natural Language Processing;https://github.com/lucylow/En_francais_si_vous_plait-
"""The goal of this project is to develop a bot using Reinforcement Learning that can play Connect 4 against humans.   If you are interested in running the algorithm yourself  make sure to do the following (preferably in some Python environment):  - Clone the git repostiory - Set up and install [OpenAI Gym](http://gym.openai.com/docs/#installation) - Set up and install [keras-rl](https://github.com/keras-rl/keras-rl) - Install the Connect 4 enviornment. Run `pip install -e gym_connect4` from the root of the repository  To run the training  modify `train.py` to point to the right files and use the policies and algorithms you want. Then you just run `python train.py`.  To play against your bot  modify `train.py` so that you are only loading in the neural network weights  using the `Connect4VsHuman-v0` environment  and running `dqn.test(...)` so that you don't overwrite your files. Run `python train.py` to start playing against your bot.   """;Reinforcement Learning;https://github.com/RandyDeng/gym_connect4
"""This code tries to implement the Neural Turing Machine  as found in  https://arxiv.org/abs/1410.5401  as a backend neutral recurrent keras layer.  A very default experiment  the copy task  is provided  too.  In the end there is a TODO-List. Help would be appreciated!  NOTE: * There is a nicely formatted paper describing the rough idea of the NTM  implementation difficulties and which discusses the   copy experiment. It is available here in the repository as The_NTM_-_Introduction_And_Implementation.pdf.  * You may want to change the LOGDIR_BASE in testing_utils.py to something that works for you or just set a symbolic   link.    For a quick start on the copy task  type       python main.py -v ntm  while in a python enviroment which has tensorflow  keras and numpy. Having tensorflow-gpu is recommend  as everything is about 20x faster. In my case this experiment takes about 100 minutes on a NVIDIA GTX 1050 Ti. The -v is optional and offers much more detailed information about the achieved accuracy  and also after every training epoch. Logging data is written LOGDIR_BASE  which is ./logs/ by default. View them with tensorboard:      tensorboard --logdir ./logs  If you've luck and not had a terrible run (that can happen  unfortunately)  you now have a machine capable of copying a given sequence! I wonder if we could have achieved that any other way ...  These results are especially interesting compared to an LSTM model: Run      python main.py lstm  This builds 3 layers of LSTM with and goes through the same testing procedure as above  which for me resulted in a training time of approximately 1h (same GPU) and  (roughly) 100%  100%  94%  50%  50% accuracy at the respective test lengths. This shows that the NTM has advantages over LSTM in some cases. Especially considering the LSTM model has about 807.200 trainable parameters while the NTM had a mere 3100!   Have fun playing around  maybe with other controllers? dense  double_dense and lstm are build in.      (by default both are 1).   More or less minimal code example:      from keras.models import Sequential     from keras.optimizers import Adam     from ntm import NeuralTuringMachine as NTM      model = Sequential()     model.name = ""NTM_-_"" + controller_model.name      ntm = NTM(output_dim  n_slots=50  m_depth=20  shift_range=3                controller_model=None                return_sequences=True                input_shape=(None  input_dim)                 batch_size = 100)     model.add(ntm)      sgd = Adam(lr=learning_rate  clipnorm=clipnorm)     model.compile(loss='binary_crossentropy'  optimizer=sgd                     metrics = ['binary_accuracy']  sample_weight_mode=""temporal"")  What if we instead want a more complex controller? Design it  e.g. double LSTM:      controller = Sequential()     controller.name=ntm_controller_architecture     controller.add(LSTM(units=150                          stateful=True                          implementation=2     """;General;https://github.com/flomlo/ntm_keras
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/chen-xiong-yi/OwnBERT
"""**Windows portable version**: Simply download and use the latest version from the [Releases](https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases) section.  Otherwise  check [doc/installation.md](doc/installation.md) for instructions on how to build OpenPose from source.     OS: Ubuntu (14  16)  Windows (8  10)  Mac OSX  Nvidia TX2.   CUDA (Nvidia GPU)  OpenCL (AMD GPU)  and CPU-only (no GPU) versions.   Most users do not need the OpenPose C++/Python API  but can simply use the OpenPose Demo:  - **OpenPose Demo**: To easily process images/video/webcam and display/save the results. See [doc/demo_overview.md](doc/demo_overview.md). E.g.  run OpenPose in a video with: ``` #: Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi ```  - **Calibration toolbox**: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See [doc/modules/calibration_module.md](doc/modules/calibration_module.md).  - **OpenPose C++ API**: If you want to read a specific input  and/or add your custom post-processing function  and/or implement your own display/saving  check the C++ API tutorial on [examples/tutorial_api_cpp/](examples/tutorial_api_cpp/) and [doc/library_introduction.md](doc/library_introduction.md). You can create your custom code on [examples/user_code/](examples/user_code/) and quickly compile it with CMake when compiling the whole OpenPose project. Quickly **add your custom code**: See [examples/user_code/README.md](examples/user_code/README.md) for further details.  - **OpenPose Python API**: Analogously to the C++ API  find the tutorial for the Python API on [examples/tutorial_api_python/](examples/tutorial_api_python/).  - **Adding an extra module**: Check [doc/library_add_new_module.md](./doc/library_add_new_module.md).  - **Standalone face or hand detector**:     - **Face** keypoint detection **without body** keypoint detection: If you want to speed it up (but also reduce amount of detected faces)  check the OpenCV-face-detector approach in [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     - **Use your own face/hand detector**: You can use the hand and/or face keypoint detectors with your own face or hand detectors  rather than using the body detector. E.g.  useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     """;General;https://github.com/theyellowdiary/openpose
"""We provide several samples of bash scripts to ease operations on a Ray cluster:  - `env.sh`: Declare the configuration of Ray cluster. - `update_ray_codes.sh`: Replace with our slightly modified [RLlib](ray/rllib) and [Tune](ray/tune). - `exec_commands.sh`: Make some directories and (optionally) install ray on each node. - `ray_start.sh`: Start a Ray cluster. - `ray_stop.sh`: Stop a Ray cluster. - `upload_codes.sh`: Upload the training code to each node of a Ray cluster.   Note that here we use a latent variable instead of a latent distribution.   """;General;https://github.com/llan-ml/tesp
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/coronazap/bert_client
"""PATH/class1 <br/>  PATH/class2 <br/>   Code for evaulate FID score came from https://github.com/bioinf-jku/TTUR   """;General;https://github.com/rosinality/sagan-pytorch
"""args.gpu: make sure you use the following gpu when running the code:  Neural Network Models |  args.gpu   """;General;https://github.com/ShiyuLiang/odin-pytorch
"""1. The full problems dataset can be downloaded and pre-processed can be generated through:  ```   sh get_data.sh ```  To tweak for new directories:  2. The root directories can be set for the coding problems in `yads_data_loader.py` :  ```   rootdir1 = './description2code_current/codeforces_delete'   rootdir2 = './description2code_current/hackerearth/problems_college'   rootdir3 = './description2code_current/hackerearth/problems_normal' ```  3. Then tweak these parameters per needed experiment:  ``` questions_count = 3000 answers_count = 50 max_len_words = 800 regular_desc = True ``` 4. to run the model:  ```  python dmn_train.py ```   """;Natural Language Processing;https://github.com/ethancaballero/neural-engineers-first-attempt
"""The code is tested under a Linux desktop with a single GTX-1080 Ti GPU.   can be downloaded via  bash download_dataset.sh   cd DVBPR   cd GAN   cd PM   A quick way to use our model is using pretrained models which can be acquired via:   ``` bash download_pretrained_models.sh  ```  With pretrained models  you can see the AUC results of DVBPR  and run GAN and PM code to generate images.   """;Computer Vision;https://github.com/kang205/DVBPR
"""FastText embeds words by adding word's n-grams to the word embedding and then normalizes by total token count i.e. <b>fastText(word)<sub></sub> = (v<sub>word</sub> + &Sigma;<sub>g &isin; ngrams(word)</sub>v<sub>g</sub>) / (1 + |ngrams(word)|)</b>. However if the word is not present in the dictionary (OOV) only n-grams are used i.e. <b>(&Sigma;<sub>g &isin; ngrams(word)</sub>v<sub>g</sub>) / |ngrams(word)|</b>. For purpose of studying OOV words this asymmetry between vocabulary and out of vocabulary words is removed by only utilizing word's n-grams regardless if the word is OOV or not.  In order to study contrast between common english words e.g. ""apple"" and noise-words (usually some parsing artifacts or unusual tokens with very specific meaning) e.g. ""wales-2708"" or ""G705"" [MIT 10K Common words dataset is used](https://www.mit.edu/~ecprice/wordlist.10000).  Entire code for this post in available in [this repository in file ""main.py""](https://github.com/vackosar/fasttext-vector-norms-and-oov-words/blob/master/main.py). FastText model used is [5-gram English 2M ""cc.en.300.bin""](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz).    """;Natural Language Processing;https://github.com/vackosar/fasttext-vector-norms-and-oov-words
"""Requirements: Since my google credits are expired I have decided to build this project on Google Colab. The project will use keras with tensorflow as backend. We don't need to install libraries on Colab and can start straight away.    """;Computer Vision;https://github.com/ymittal23/PlayWithCifar
"""Clone this repository. ``` git clone https://github.com/shachoi/HANet.git cd HANet ``` Install following packages. ``` conda create --name hanet python=3.6 conda activate hanet conda install -y pytorch=1.4.0 torchvision=0.5.0 cudatoolkit=10.1 -c pytorch conda install scipy==1.4.1 conda install tqdm==4.46.0 conda install scikit-image==0.16.2 pip install tensorboardX==2.0 pip install thop ```   For Cityscapes dataset  download ""leftImg8bit_trainvaltest.zip"" and ""gtFine_trainvaltest.zip"" from https://www.cityscapes-dataset.com/downloads/<br>   You should modify the path in ""<path_to_hanet>/config.py"" according to your Cityscapes dataset path.   According to the specification of your gpu system  you may modify the training script.   Otherwise  you can train HANet (based on ResNet-101) using only finely annotated training set with following command.   To run the script ""train_r101_os8_hanet.sh""  two Titan RTX GPUs (2 X 24GB GPU Memory) are required.   """;Computer Vision;https://github.com/shachoi/HANet
"""In this competition  you’ll detect wheat heads from outdoor images of wheat plants  including wheat datasets from around the globe. Using worldwide data  you will focus on a generalized solution to estimate the number and size of wheat heads. To better gauge the performance for unseen genotypes  environments  and observational conditions  the training dataset covers multiple regions. You will use more than 3 000 images from Europe (France  UK  Switzerland) and North America (Canada). The test data includes about 1 000 images from Australia  Japan  and China.   These examples have wheat heads detected  ![alt text](https://user-images.githubusercontent.com/45472148/98257653-8a4eec80-1fb2-11eb-97d7-a01a0b07031f.png)  These examples don't have any wheat heads  ![alt text](https://user-images.githubusercontent.com/45472148/98257663-8d49dd00-1fb2-11eb-86d5-2fc98e5546a3.png)   Darknet is an open source neural network framework written in C and CUDA. It is fast  easy to install  and supports CPU and GPU computation.  > Paper: https://arxiv.org/abs/2004.10934  > Source code: https://github.com/AlexeyAB/darknet  The result of YOLO v4 is impressive compared to other models:  ![alt text](https://user-images.githubusercontent.com/45472148/98257679-8fac3700-1fb2-11eb-84a1-961fe21e7467.png)   """;Computer Vision;https://github.com/leduckhai/Global-Wheat-Detection-Kaggle
"""Source: https://github.com/bplank/2019-ma-notebook   """;General;https://github.com/bplank/teaching-dl4nlp
"""```bash $ pip install big-sleep ```   You will be able to have the GAN dream up images using natural language with a one-line command in the terminal.   You can now train more than one phrase using the delimiter ""\""   ```bash $ dream ""a pyramid made of ice"" ```  Images will be saved to wherever the command is invoked   """;General;https://github.com/notperquisites/bigsleep
"""    !python model_inspect.py --runmode=infer \       --model_name=efficientdet-d0   --ckpt_path=efficientdet-d0 \       --hparams=voc_config.yaml  \       --input_image=img.png --output_image_dir=/tmp/  You should check more details of runmode which is written in caption-4.       !python model_inspect.py --runmode=infer \       --model_name=efficientdet-d0   --ckpt_path=efficientdet-d0 \       --hparams=voc_config.yaml  \       --input_image=img.png --output_image_dir=/tmp/  You should check more details of runmode which is written in caption-4.        Quick install dependencies: pip install -r requirements.txt   Run the following command line to export models:   Then you will get:   prediction output)  use the following command:   use the following command:   Lastly  if you only have one image and just want to run a quick test  you can also run the following command (it is slow because it needs to construct the graph from scratch):   !mkdir tfrecord   You can also run eval on test-dev set with the following command:   !mkdir tfrecord   Download efficientdet coco checkpoint.  !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/efficientdet-d0.tar.gz  !tar xf efficientdet-d0.tar.gz   Install horovod.   Download efficientdet coco checkpoint.  !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/efficientdet-d0.tar.gz  !tar xf efficientdet-d0.tar.gz   Install latest TensorFlow for both GCE VM and Cloud.       !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/data/video480p.mov -O input.mov            """;Computer Vision;https://github.com/lvweiwolf/efficientdet
"""``` $ cd dcgan/ $ python3 dcgan.py ```   ``` $ cd vgan/ $ python3 vgan.py ``` """;Computer Vision;https://github.com/Byte7/GANs
"""You can install it as a package as follows. If you are testing on a local environment (see above)  make sure it is active.  ```bash #: inside the repo's root directory python -m pip install . ```  In order to use the same models as in the official version  download the converted Pytorch weights in the TF/Keras `.h5` file format for the model version you want to use from [here](https://drive.google.com/drive/folders/1OMzJNxsx-D5lyLgrQokLvbpvrZ5rM9rW?usp=sharing).  You can use one of the pre-built loading methods from the `models.default` package to instantiate one of the four versions that are equivalent to the ones provided by the original implementation.  ```python from detr_tensorflow.models.default import build_detr_resnet50 detr = build_detr_resnet50(num_classes=91) #: 91 classes for the COCO dataset detr.build() detr.load_weights(""detr-r50-e632da11.h5"") ```  Or directly instantiate the `models.DETR` class to create your own custom combination of backbone CNN  transformer architecture  and positional encoding scheme. Please  check the files `models/default.py` and `models/detr.py` for more details.  The `detr_tensorflow.utils.preprocess_image` function is designed to perform all the preprocessing required before running the model  including data normalization  resizing following the scheme used for training  and generating the image masks. It is completely implemented using only Tensorflow operations  so you can use it in combination with the `map` functionality from `tf.data.Dataset`.  Finally  to get the final detections  call the model on your data with the `post_processing` flag. This way  it returns softmax scores instead of the pre-activation logits  and also discards the `no-object` dimension from the output. It doesn't discard low scored detections tough  so as to give more flexibility in how to use the detections  but the output from DETR is simple enough that this isn't hard to do.  ```python from detr_tensorflow.utils import preprocess_image  absolute2relative  inp_image  mask = preprocess_image(image) inp_image = tf.expand_dims(inp_image  axis=0) mask = tf.expand_dims(mask  axis=0)  outputs = detr((inp_image  mask)  post_process=True) labels  scores  boxes = [outputs[k][0].numpy() for k in ['labels'  'scores'  'boxes']]  keep = scores > 0.7 labels = labels[keep] scores = scores[keep] boxes = boxes[keep] boxes = absolute2relative(boxes  (image.shape[1]  image.shape[0])).numpy() ```  (so much easier than anchor decoding + Non Max Suppression)    Short demo script that summarizes the above instructions.  ```bash python demo.py ```   """;Computer Vision;https://github.com/Leonardo-Blanger/detr_tensorflow
"""This repository use [*Cifar10*](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. When you run the training script  the dataset will be automatically downloaded.   You can run CBAM_block or SE_block added models in the below list by adding one argument --attention_module=cbam_block or --attention_module=se_block when you train a model.   You can simply run a model by executing following scripts.  - sh train_ResNext.sh  - sh train_inception_resnet_v2.sh  - sh train_inception_v4.sh   """;Computer Vision;https://github.com/kobiso/CBAM-tensorflow
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have installed: MSVS 2015/2017  CUDA 10  cuDNN 7.x  OpenCV 3.x. Then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio 2017 Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Build with the Powershell script build.ps1. If you want to use Visual Studio  you will find a custom solution created for you by CMake after the build containing all the appropriate config flags for your system.  If you have MSVS 2015  CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/khaled2ahmed/k2a
"""JTPA みんなでやろうDL オンライン勉強会 https://github.com/JTPA/learn_DL_together  Nocnoc: https://nocnoc.ooo/event/A1B6CDAC-637F-4455-9582-D086AC289268   Code: https://github.com/manashmandal/DeadSimpleSpeechRecognizer   Code: https://github.com/mikesmales/Udacity-ML-Capstone   (https://github.com/JTPA/learn_DL_together のREADME.mdの中にリンクあり）   https://www.youtube.com/watch?v=Uumd2zOOz60 <br/>       Code: https://github.com/manashmandal/DeadSimpleSpeechRecognizer <br/>   https://qiita.com/tmtakashi_dist/items/eecb705ea48260db0b62 <br/>   https://www.youtube.com/watch?v=9-ixexpjN-8 <br/>   https://qiita.com/mshinoda88/items/9770ee671ea27f2c81a9 <br/>   """;Computer Vision;https://github.com/komeiharada/learn_DL_together
"""1. Download ffhq-dataset from [here.](https://github.com/NVlabs/ffhq-dataset)  2. Put images1024x1024 in ffhq_dataset and thumbnails128x128 in ffhq_dataset128.  like this  ``` ... │ ├── ffhq_dataset │     ├── 00000.png │     ├── 00001.png │     ├── ... │     └── 69999.png ├── ffhq_dataset128 │     ├── 00000.png │     ├── 00001.png │     ├── ... │     └── 69999.png  ├── main.py ├── model.py ... ```  3. Train StyleGAN.  ``` python main.py ```  How long does it take to train using RTX 2070  ``` 64x64     1d00h 128x128   2d00h 256x256   3d18h 512x512   6d13h(estimated) 1024x1024 unknown ```  4. After training  inference can be performed.  to draw uncurated images  ``` python pred.py -m uc ```  <img src = 'examples/uc_ffhq.png' width=1280>  to draw truncation trick images  ``` python pred.py -m tt ```  <img src = 'examples/tt_ffhq.png' width=1280>  to draw style mixing images  ``` python pred.py -m sm ```  <img src = 'examples/sm_ffhq.png' width=1280>   """;Computer Vision;https://github.com/itsuki8914/stylegan-TensorFlow
"""The same notebook can be found <a href= 'https://www.kaggle.com/daenys2000/cyclegan'>here</a>.   """;Computer Vision;https://github.com/NitishaS-812k/Monet-CycleGAN
"""Just run `sudo sh run.sh`  which will:  * Download the latest English wikipedia dump * Extract and clean texts from the downloaded wikipedia dump * Pre-process the wikipedia corpus * Train word2vec model on the processed corpus to produce word embedding results  Details for each step will be discussed as below:   """;Natural Language Processing;https://github.com/jind11/word2vec-on-wikipedia
"""Install the dependencies using Conda: ``` conda create --name pytorch_pointnet --file spec-file.txt ```   shapenet: The dataset can be downloaded    """;Computer Vision;https://github.com/romaintha/pytorch_pointnet
"""For example  the above gif depicts multiple customers filling the self-checkout line with what seems to be just one employee.  spotti will recognize that at least one more employee is needed to efficiently uphold the multiple tasks of a self-checkout employee. Once this has been recognized  then spotti will be configured to notify any personnel who is available to fill in the position.    """;Computer Vision;https://github.com/cmf3673/spotti
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`  2. Download VOC2007 and VOC2012 dataset. By default  we assume the data is stored in `$HOME/data/`   ```Shell   #: Download the data.   cd $HOME/data   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar   #: Extract the data.   tar -xvf VOCtrainval_11-May-2012.tar   tar -xvf VOCtrainval_06-Nov-2007.tar   tar -xvf VOCtest_06-Nov-2007.tar   ```  3. Create the LMDB file.   ```Shell   cd $CAFFE_ROOT   #: Create the trainval.txt  test.txt  and test_name_size.txt in data/VOC0712/   ./data/VOC0712/create_list.sh   #: You can modify the parameters in create_data.sh if needed.   #: It will create lmdb files for trainval and test with encoded original image:   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb   #: and make soft links at examples/VOC0712/   ./data/VOC0712/create_data.sh   ```   1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`   ```Shell   git clone https://github.com/weiliu89/caffe.git   cd caffe   git checkout ssd   ```  2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.   ```Shell   #: Modify Makefile.config according to your Caffe installation.   cp Makefile.config.example Makefile.config   make -j8   #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.   make py   make test -j8   #: (Optional)   make runtest -j8   ```   COCO<sup>[1]</sup>: SSD300*  SSD512*  07+12+COCO: SSD300*  SSD512*  07++12+COCO: SSD300*  SSD512*  COCO models:   """;Computer Vision;https://github.com/leejang/two_stream_ssd_caffe
"""On WN18RR  run:   We start by learning rules from the UWCSE knowledge graph  a small graph in the academia domain. In `data/UWCSE` folder you can find the inputs GPFL requires for learning: - `data/<train/test/valid>.txt`: triple files for training  test and validation. - `data/<annotated_train/test/valid>.txt`: as GPFL runs on Neo4j database  we use indexing to optimize data querying. These files contain annotated training  test  validation triples with Neo4j ids. - `data/databases`: contains the Neo4j database. It can also be conveniently used for EDA with [Cypher](https://neo4j.com/developer/cypher-query-language/) and its [Data Science ecosystem](https://neo4j.com/graph-data-science-library/). - `config.json`: the GPFL configuration file.  Now we give an introduction on some options in a GPFL configuration file: - `home`: home directory of your data. - `out`: output directory. - `ins_depth`: max length of instantiated rules. - `car_depth`: max length of closed abstract rules. - `conf`: confidence threshold. - `support`: support (number of correct predictions) threshold. - `head_coverage`: head coverage threshold. - `saturation`: template saturation threshold. - `batch_size`: size of batch over which the saturation is evaluated. - `thread_number`: number of running threads. Please note as each thread is responsible for specializing a template or grounding a rule  employing large number of threads might cause out of memeory issue.  To learn rules for UWCSE  run: ``` gradle run --args=""-c data/UWCSE/config.json -r"" ``` where option `-c` specifies the location of the GPFL configuration file  and `-r` executes the chain of rule learning  application and evaluation for link prediction.    Once the program finishes  results will be saved at folder `data/UWCSE/ins3-car3`. Now navigate to the result folder  file `rules.txt` records all learned rules. To get the top rules  run following command to sort rules by quality: ``` gradle run --args=""-or data/UWCSE/ins3-car3"" ``` In the sorted `rules.txt` file  each line has values: ``` Type  Rule                                                 Conf     HC       VP       Supp  BG CAR   ADVISED_BY(X Y) <- PUBLISHES(X V1)  PUBLISHES(Y V1)  0.09333  0.31343  0.03015  21    220 ``` where `conf` is the confidence  `HC` head coverage  `VP` validation precision  `supp` support  and `BG` body grounding (total predictions).  To check the quality/type/length distribution of the learned rules  run: ``` gradle run --args=""-c data/UWCSE/config.json -ra"" ```  To find explanations about predicted and existing facts (test triples) in terms of rules  check `verifications.txt` file in the result folder  where an entry looks like this: ``` Head Query: person211	PUBLISHES	title88 Top Answer: 1	person415	PUBLISHES	title88 BAR	PUBLISHES(person415 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 title12)	0.40426 BAR	PUBLISHES(person415 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 V2)  PUBLISHES(person211 V2)	0.40299 BAR	PUBLISHES(person415 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 title182)	0.39583  Top Answer: 2	person211	PUBLISHES	title88 BAR	PUBLISHES(person211 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 V2)  PUBLISHES(person284 V2)	0.35294 BAR	PUBLISHES(person211 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 title259)	0.325 BAR	PUBLISHES(person211 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 title241)	0.325  Top Answer: 3	person240	PUBLISHES	title88 BAR	PUBLISHES(person240 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 V2)  PUBLISHES(person161 V2)	0.2459 BAR	PUBLISHES(person240 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 title268)	0.2381 BAR	PUBLISHES(person240 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 V2)  PUBLISHES(person415 V2)	0.17647  Correct Answer: 2	person211	PUBLISHES	title88 BAR	PUBLISHES(person211 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 V2)  PUBLISHES(person284 V2)	0.35294 BAR	PUBLISHES(person211 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 title259)	0.325 BAR	PUBLISHES(person211 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 title241)	0.325 ``` `Head Query: person211	PUBLISHES	title88` means that GPFL corrupts the known fact `person211	PUBLISHES	title88` into a head query `?	PUBLISHES	title88`  and asks the learned rules to suggest candidates to replace `?`. If `person211` is proposed in the answer set  it is considered as a correct answer. In this example  the correct answer ranks 2 as in: ``` Top Answer: 2	person211	PUBLISHES	title88 BAR	PUBLISHES(person211 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 V2)  PUBLISHES(person284 V2)	0.35294 BAR	PUBLISHES(person211 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 title259)	0.325 BAR	PUBLISHES(person211 Y) <- PUBLISHES(V1 Y)  PUBLISHES(V1 title241)	0.325 ``` where the following rules are top rules that suggest candidate `person211`. Therefore  these rules can be used to explain in a data-driven way why `person211` publishes paper `title88`.   To find detailed evaluation results  please refer to the `eval_log.txt` file in the result folder.   """;General;https://github.com/irokin/GPFL
"""* Dataset      We follow the instructions [here](https://github.com/ChenRocks/cnn-dailymail) for preprocessing the dataset. Meanwhile  we conduct data cleaning by removing duplicates (i.e.  both content and title of 2 articles are the same) and cleaning some invalid characters (e.g.  URLs  image comments  javascript strings  etc.). After that  all data files ```train```  ```val```  ```test``` and vocabulary file ```vocab_cnt.pkl``` are located in a specified data directory  e.g. ```./bytecup/finished_files/```.  * Pretrain word embeddings ``` python3 train_word2vec.py --data=./bytecup/finished_files --path=./bytecup/models/word2vec ``` * Make the pseudo-labels ``` python3 make_extraction_labels.py --data=./bytecup/finished_files ``` * Train abstractor and extractor ``` python3 train_abstractor.py --data=./bytecup/finished_files --path=./bytecup/models/abstractor --w2v=./bytecup/models/word2vec/word2vec.300d.332k.bin python3 train_extractor.py --data=./bytecup/finished_files --path=./bytecup/models/extractor --w2v=./bytecup/models/word2vec/word2vec.300d.332k.bin ``` * Train the RL guided model ``` python3 train_full_rl.py --data=./bytecup/finished_files --path=./bytecup/models/save --abs_dir=./bytecup/models/abstractor --ext_dir=./bytecup/models/extractor ``` * Decode process ``` python3 decode_full_model.py --data=./bytecup/finished_files --path=./bytecup/output --model_dir=./bytecup/models/save --[val/test]  ``` * Convert decoded results for submission ``` python3 commit_data.py --decode_dir=./bytecup/output --result_dir=./bytecup/result ```   """;Sequential;https://github.com/iwangjian/ByteCup2018
"""./dist_classification.sh $NUM_GPUS -c $CONFIG_FILE /path/to/dataset   ./dist_classification.sh 8 -c configs/classification/convmlp_s_imagenet.yml /path/to/ImageNet  ./dist_classification.sh 8 -c configs/classification/convmlp_m_imagenet.yml /path/to/ImageNet  ./dist_classification.sh 8 -c configs/classification/convmlp_l_imagenet.yml /path/to/ImageNet   example  if you want to run ConvMLP with CIFAR10 you should have `dataset:   ./dist_detection.sh $CONFIG_FILE $NUM_GPUS /path/to/dataset   ./dist_detection.sh configs/detection/retinanet_convmlp_s_fpn_1x_coco.py 8 /path/to/COCO  ./dist_detection.sh configs/detection/retinanet_convmlp_m_fpn_1x_coco.py 8 /path/to/COCO  ./dist_detection.sh configs/detection/retinanet_convmlp_l_fpn_1x_coco.py 8 /path/to/COCO   ./dist_segmentation.sh $CONFIG_FILE $NUM_GPUS /path/to/dataset   It can be observed that representations learned by ConvMLP involve    download the pretrained backbone if you pass the correct config files.   Our base model is in pure PyTorch and Torchvision. No extra packages are required. Please refer to [PyTorch's Getting Started](https://pytorch.org/get-started/locally/) page for detailed instructions.  You can start off with `src.convmlp`  which contains the three variants: `convmlp_s`  `convmlp_m`  `convmlp_l`: ```python3 from src.convmlp import convmlp_l  convmlp_s  model = convmlp_l(pretrained=True  progress=True) model_sm = convmlp_s(num_classes=10) ```   [mmdetection](https://github.com/open-mmlab/mmdetection) is recommended for training Mask R-CNN and required for the training script provided in this repository (same as above).  You can use our training configurations provided in `configs/detection`:  ```shell ./dist_detection.sh configs/detection/maskrcnn_convmlp_s_fpn_1x_coco.py 8 /path/to/COCO ./dist_detection.sh configs/detection/maskrcnn_convmlp_m_fpn_1x_coco.py 8 /path/to/COCO ./dist_detection.sh configs/detection/maskrcnn_convmlp_l_fpn_1x_coco.py 8 /path/to/COCO ```   <table style=""width:100%"">     <thead>         <tr>             <td><b>Dataset</b></td>              <td><b>Model</b></td>              <td><b>Backbone</b></td>              <td><b> """;Computer Vision;https://github.com/SHI-Labs/Convolutional-MLPs
"""Run   ```python train.py```  to train a model on the Cora dataset.   """;Graphs;https://github.com/gcucurull/jax-gat
"""This framework was created in order to help compare learned and variational approaches to CT reconstruction in a systematic way. The implementation is based on python libraries **odl** and **pyTorch**. The list of implemented algorithms include:  * FBP (Filtered back-projection)  * TV (Total Variation) * ADR (Adversarial Regularizer): https://arxiv.org/abs/1805.11572 * LG (Learned gradient descent): https://arxiv.org/abs/1704.04058 * LPD (Learned primal dual): https://arxiv.org/abs/1707.06474 * FL (Fully learned): https://nature.com/articles/nature25988.pdf * FBP+U (FBP with a U-Net denoiser): https://arxiv.org/abs/1505.04597  In order to add your own algorithms to the list  create a new file in the **Algorithms** folder in the form *name*.py and use BaseAlg.py as the template.  """;Computer Vision;https://github.com/Zakobian/CT_framework_
"""To get started  we propose to download a toy English-German dataset for machine translation containing 10k tokenized sentences:  ```bash wget https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz tar xf toy-ende.tar.gz cd toy-ende ```  The data consists of parallel source (`src`) and target (`tgt`) data containing one sentence per line with tokens separated by a space:  * `src-train.txt` * `tgt-train.txt` * `src-val.txt` * `tgt-val.txt`  Validation files are used to evaluate the convergence of the training. It usually contains no more than 5k sentences.  ```text $ head -n 3 toy-ende/src-train.txt It is not acceptable that   with the help of the national bureaucracies   Parliament &apos;s legislative prerogative should be made null and void by means of implementing provisions whose content   purpose and extent are not laid down in advance . Federal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness   Group Fitness   Postural Gym   Stretching and Pilates; from 2004   he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching   Pilates and Postural Gym . &quot; Two soldiers came up to me and told me that if I refuse to sleep with them   they will kill me . They beat me and ripped my clothes . ```  We need to build a **YAML configuration file** to specify the data that will be used:  ```yaml #: toy_en_de.yaml  #:#: Where the samples will be written save_data: toy-ende/run/example #:#: Where the vocab(s) will be written src_vocab: toy-ende/run/example.vocab.src tgt_vocab: toy-ende/run/example.vocab.tgt #: Prevent overwriting existing files in the folder overwrite: False  #: Corpus opts: data:     corpus_1:         path_src: toy-ende/src-train.txt         path_tgt: toy-ende/tgt-train.txt     valid:         path_src: toy-ende/src-val.txt         path_tgt: toy-ende/tgt-val.txt ...  ```  From this configuration  we can build the vocab(s) that will be necessary to train the model:  ```bash onmt_build_vocab -config toy_en_de.yaml -n_sample 10000 ```  **Notes**: - `-n_sample` is required here -- it represents the number of lines sampled from each corpus to build the vocab. - This configuration is the simplest possible  without any tokenization or other *transforms*. See [other example configurations](https://github.com/OpenNMT/OpenNMT-py/tree/master/config) for more complex pipelines.   OpenNMT-py requires:  - Python >= 3.6 - PyTorch == 1.6.0  Install `OpenNMT-py` from `pip`: ```bash pip install OpenNMT-py ```  or from the sources: ```bash git clone https://github.com/OpenNMT/OpenNMT-py.git cd OpenNMT-py pip install -e . ```  Note: if you encounter a `MemoryError` during installation  try to use `pip` with `--no-cache-dir`.  *(Optional)* Some advanced features (e.g. working pretrained models or specific transforms) require extra packages  you can install them with:  ```bash pip install -r requirements.opt.txt ```   Given sufficient CPU resources according to GPU computing power  most of the transforms should not slow the training down. (Note: for now  one producer process per GPU is spawned -- meaning you would ideally need 2N CPU threads for N GPUs).   - the vocabulary path(s) that will be used: can be that generated by onmt_build_vocab;   : Train on a single GPU   Then you can simply run:   """;Sequential;https://github.com/butsugiri/shape
"""- To replicate the semi-supervised results  run the following script ```sh sh semi.sh ``` - To replicate the full-supervised results  run the following script ```sh sh full.sh ``` - To replicate the inductive results of PPI  run the following script ```sh sh ppi.sh ```  """;Graphs;https://github.com/chennnM/GCNII
"""``` #: checkout the code and install all dependencies git clone https://github.com/andrijazz/pruning cd pruning bash init_env.sh <PROJECT_NAME> <PATH_TO_YOUR_DATA_STORAGE> pipenv install  #: activate venv pipenv shell  #: train model python run.py --mode train  #: test models and plot results to w&b python run.py --mode test  ```   """;General;https://github.com/andrijazz/pruning
"""The project provides a web app which allows users to post found hats as well as report their finds. <br> <br> The system automatically matches images and/or textual descriptions  notifying the owners of the lost items. There is the option to register their hats to be automatically matched if they are found. There is an element of gamification by means of awarding productive users with experience points as well as a very simple feed for viewing  bumping and reacting to posts about lost and found items. The project can also be modified to suit current market demand  for example handling face masks in addition to hats.   """;General;https://github.com/mvxxx/mimuw-hats
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/StoneGH/bert
"""To avoid any conflict with your existing Python setup  and to keep this project self-contained  it is suggested to work in a virtual environment with [`virtualenv`](http://docs.python-guide.org/en/latest/dev/virtualenvs/). To install `virtualenv`: ``` pip install --upgrade virtualenv ``` Create a virtual environment  activate it and install the requirements in [`requirements.txt`](requirements.txt). ``` virtualenv venv source venv/bin/activate pip install -r requirements.txt ```   You can use the [`main.py`](main.py) script in order to run reinforcement learning experiments with MAML. This script was tested with Python 3.5. Note that some environments may also work with Python 2.7 (all experiments besides MuJoCo-based environments). ``` python main.py --env-name HalfCheetahDir-v1 --num-workers 8 --fast-lr 0.1 --max-kl 0.01 --fast-batch-size 20 --meta-batch-size 40 --num-layers 2 --hidden-size 100 --num-batches 1000 --gamma 0.99 --tau 1.0 --cg-damping 1e-5 --ls-max-steps 15 --output-folder maml-halfcheetah-dir --device cuda ```   """;General;https://github.com/dragen1860/MAML-Pytorch-RL
"""Follow these instructions to install the VGGFace from the paper (https://arxiv.org/pdf/1703.07332.pdf):   $ tar xvzf vgg_face_caffe.tar.gz  $ sudo apt install caffe-cuda  $ pip install mmdnn   If you have a problem with pickle  delete your numpy and reinstall numpy with version 1.16.1   cv2 (opencv-python)   - modify paths in params folder to reflect your path - preprocess.py: preprocess our data for faster inference and lighter dataset - train.py: initialize and train the network or continue training from trained network - embedder_inference.py: (Requires trained model) Run the embedder on videos or images of a person and get embedding vector in tar file  - fine_tuning_trainng.py: (Requires trained model and embedding vector) finetune a trained model - webcam_inference.py: (Requires trained model and embedding vector) run the model using person from embedding vector and webcam input  just inference - video_inference.py: just like webcam_inference but on a video  change the path of the video at the start of the file    """;Computer Vision;https://github.com/vincent-thevenin/Realistic-Neural-Talking-Head-Models
"""Our requirements are in requirements.txt  to install run pip install -r requirements.txt in a new virtual environment.   """;Reinforcement Learning;https://github.com/colindbrown/columbia-deep-learning-project
"""The project provides a web app which allows users to post found hats as well as report their finds. <br> <br> The system automatically matches images and/or textual descriptions  notifying the owners of the lost items. There is the option to register their hats to be automatically matched if they are found. There is an element of gamification by means of awarding productive users with experience points as well as a very simple feed for viewing  bumping and reacting to posts about lost and found items. The project can also be modified to suit current market demand  for example handling face masks in addition to hats.   """;Computer Vision;https://github.com/mvxxx/mimuw-hats
"""4、tfplot 0.2.0 (optional)            5、tensorflow-gpu 1.13   2、(Recommend in this repo) Or you can choose to use a better backbone (resnet_v1d)  refer to gluon2TF.     cd $PATH_ROOT/libs/box_utils/cython_utils  python setup.py build_ext --inplace (or make)  cd $PATH_ROOT/libs/box_utils/   cd $PATH_ROOT/eval_devkit  sudo apt-get install swig  swig -c++ -python polyiou.i   2、Make tfrecord      3、Multi-gpu train  cd $PATH_ROOT/tools   cd $PATH_ROOT/tools   cd $PATH_ROOT/eval_devkit   cd $PATH_ROOT/output/summary   """;Computer Vision;https://github.com/SJTU-Thinklab-Det/OHDet_Tensorflow
"""[Architecture]: Frame-Recurrent Video Super-Resolution  Sajjadi et al. (https://arxiv.org/pdf/1512.02134.pdf) [Dataset]: It was obtained from 30 video taken from Youtube by me.  ![Model](preview/dataset.png)  Implementation of [FRVSR](https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/paper-MIFDB16.pdf) arhitecture with some tweaks such as:  * adding new PP Loss (references (https://arxiv.org/pdf/1811.09393.pdf)) * modifing arhitecture residual blocks by adding new spectral normalization layers (references (https://arxiv.org/pdf/1802.05957.pdf)) * adding depth to the model ( more channels and res blocks )  ![Model](preview/frvsr.png)    """;Computer Vision;https://github.com/adbobes/VideoSuperResolution
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/goldenbili/Bert_Test2
"""* python examples/ppo_gym.py --env-name Hopper-v2   """;General;https://github.com/Khrylx/PyTorch-RL
"""If you don't own a GPU remove the --cuda option  although I advise you to get one!   Please first install [Anaconda](https://anaconda.org) and create an Anaconda environment using the environment.yml file.  ``` conda env create -f environment.yml ```  After you create the environment  activate it. ``` source activate hw1 ```  Our current implementation only supports GPU so you need a GPU and need to have CUDA installed on your machine.   """;General;https://github.com/cvfx-2019/homework1-color-transfer
"""1.Pytorch 1.3.0   9.pip install resnest --pre   """;Computer Vision;https://github.com/He-jerry/DSSNet
"""<font size = 4>We have provided the instructions for the usage of this Notebook in the following YouTube link:   - [**YoutTube Video**](https://youtu.be/KOCPEzsrPzw): Walk through the pipeline including data upload  training and deploying the trained model.  **Important: Make sure that you create a copy of the following Colab Notebook on your Google Drive before running or making any changes to the notebook** ---  <font size = 4>We have provided the instructions for the usage of this Notebook in the following link:   - [**Colab Notebook**](https://colab.research.google.com/drive/1UNdMhPGFKTmp6vm5jGaLas-ethnLB_Yi?usp=sharing): Walk through the pipeline including data upload  training and deploying the trained model.   ---  """;General;https://github.com/akhadangi/EM-net
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;General;https://github.com/ZhichengHuang/Food-Project
"""In the next step you have to define the path to your dataset. This can be done in Configuration/config_general:   Preprocessing finished! You can now start to train your models!   """;Computer Vision;https://github.com/TUM-ML-Lab18/FaceSwap
"""```bash $ pip install segformer-pytorch ```   For example  MiT-B0  ```python import torch from segformer_pytorch import Segformer  model = Segformer(     dims = (32  64  160  256)       #: dimensions of each stage     heads = (1  2  5  8)            #: heads of each stage     ff_expansion = (8  8  4  4)     #: feedforward expansion factor of each stage     reduction_ratio = (8  4  2  1)  #: reduction ratio of each stage for efficient attention     num_layers = 2                  #: num layers of each stage     decoder_dim = 256               #: decoder dimension     num_classes = 4                 #: number of segmentation classes )  x = torch.randn(1  3  256  256) pred = model(x) #: (1  4  64  64)  #: output is (H/4  W/4) map of the number of segmentation classes ```  Make sure the keywords are at most a tuple of 4  as this repository is hard-coded to give the MiT 4 stages as done in the paper.   """;General;https://github.com/lucidrains/segformer-pytorch
"""Linux (arm) Linux (ppc) macOS (x86) macOS (x64) macOS (ppc)  MorphOS (ppc) WarpOS (m68k/ppc) Windows (x86) Windows (x64)   Auf Basis des C64 LISTINGs  kann die Spielmechanik ihrer PC Version von 1991 bestimmt werden.   ""Imbiss III""  Amiga Version von 1987 https://youtu.be/qTg-1BV26RM<br>  ""Imbiss III""  Amiga Version von 1987 https://youtu.be/Xo-Z71IkOe4?t=136<br>  Tom's Imbiss 1.0  Amiga 1992  https://youtu.be/7lVctKjUY9g<br>  Tom's Imbiss 1.0  Amiga 1992  Teil 2  https://youtu.be/4-wFfIBef5s<br>   """;Reinforcement Learning;https://github.com/Steinheilig/Imbiss
"""      ""pred_label"": 0  // the predicted label 0 or 1 using the threshold from above (used for integrity check)   The project consists of a `python` server with a web front-end. To get started clone the repository and run:  ```bash git submodule update --init --recursive pip install -r requirements.txt ```  After that the project is ready to run.  ```bash ./server.py input.csv expl.json ```  where `input.csv` and `expl.json` are files as described [below](#input-format) or as created with [`create_explanations.py`](#creating-explanations). Once the server is started navigate to the URL as prompted in the server output.  Run `./server.py -h` to get a list of all input arguments.   """;General;https://github.com/nyuvis/explanation_explorer
"""1. Clone the repository     ```shell    git clone https://github.com/YudeWang/UNet-Satellite-Image-Segmentation.git    ```  2. Install PyDenseCRF     You can follow the install instruction of [PyDenseCRF](https://github.com/lucasb-eyer/pydensecrf)     If you **do not have the permission of sudo**  you can download the source code by:     ```shell    git clone https://github.com/lucasb-eyer/pydensecrf.git    ```     Follow the instruction and install:     ```shell    cd pydensecrf-master    python setup.py install    ```  3. Download dataset and model     You can download 2017 CCF BDCI remote sensing challenge dataset and our pre-trained model from [here](https://drive.google.com/file/d/1FMRMe4qSI-JS6AzrO8kASO3BfHOLoUfM/view). Please unzip package in this repository folder and change the ckpt file name to **UNet_ResNet_itr100000.ckpt**(I used to call it FPN  while the structure of network is symmetrical and then rename it).    This project implement by gpu version of tensorflow1.3. Therefore a Nvidia GPU is needed.   python train.py --gpu=0   python test.py --gpu=0   """;General;https://github.com/YudeWang/UNet-Satellite-Image-Segmentation
"""To download the data  simply replace 'xxx' in your Kaggle API ID and Key. You can access this by  ``` My Account -> Create New API Token -> Access Kaggle.json file for ID and Key ```  """;Computer Vision;https://github.com/richieyoum/Pneumonia-diagnosis
"""Make sure you're in the repo's root directory.  Make sure you downloaded VGG19.   Make sure you downloaded VGG19.   Note: All evaluation scripts use ipython. But this behavior can be overridden by explicitly using python on the command line along with a save path. For example:   """;General;https://github.com/vdumoulin/discgen
"""Download the ModelNet40 dataset from <a href=""https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip"" target=""_blank"">here</a>.  To train a model to classify point clouds sampled from 3D shapes:      python train_PA.py --data_dir ModelNet40_Folder  Log files and network parameters will be saved to `log` folder in default.   Noted that the code may be not stable  if you can help please contact me.    """;Computer Vision;https://github.com/liruihui/PointAugment
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   GPUs: K80 ($0.14/hr)  T4 ($0.11/hr)  V100 ($0.74/hr) CUDA with Nvidia Apex FP16/32     Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) < highly recommended!! * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Google Colab Notebook](https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb) with quick training  inference and testing examples * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart)  * [A TensorRT Implementation of YOLOv3 and YOLOv4](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp)    """;Computer Vision;https://github.com/Smallflyfly/yolov3_ultralytics
"""Implementation of the algorithms described in ""Ulrike von Luxburg  A Tutorial on Spectral Clustering"" (https://arxiv.org/abs/0711.0189)  as part of the course ""Algoritmos para Data Science e Machine Learning"" from the M.Sc. in Computer Science program at PUC-Rio.  """;General;https://github.com/BSAraujo/Spectral-Clustering
"""WaveGAN can now be trained on datasets of arbitrary audio files (previously required preprocessing). You can use any folder containing audio  but here are a few example datasets to help you get started:   """;Audio;https://github.com/chrisdonahue/wavegan
"""[![Watch the video](figures/video_figure.png)](https://www.youtube.com/watch?v=a_OeT8MXzWI&feature=youtu.be)   ![](figures/cnn_imagenet_new.png)  ![](figures/imagenet_80_acc.png)   ```python """""" OFA Specialized Networks. Example: net  image_size = ofa_specialized('flops@595M_top1@80.0_finetune@75'  pretrained=True) """"""  from model_zoo import ofa_specialized net  image_size = ofa_specialized(net_id  pretrained=True) ``` If the above scripts failed to download  you download it manually from [Google Drive](https://drive.google.com/drive/folders/1ez-t_DAHDet2fqe9TZUTJmvrU-AwofAt?usp=sharing) and put them under $HOME/.torch/ofa_specialized/.   ```python """""" OFA Networks.     Example: ofa_network = ofa_net('ofa_mbv3_d234_e346_k357_w1.0'  pretrained=True) """"""  from model_zoo import ofa_net ofa_network = ofa_net(net_id  pretrained=True)      #: Randomly sample sub-networks from OFA network ofa_network.sample_active_subnet() random_subnet = ofa_network.get_active_subnet(preserve_weight=True)      #: Manually set the sub-network ofa_network.set_active_subnet(ks=7  e=6  d=4) manual_subnet = ofa_network.get_active_subnet(preserve_weight=True) ``` If the above scripts failed to download  you download it manually from [Google Drive](https://drive.google.com/drive/folders/10leLmIiMtaRu4J46KwrBaMydvQt0qFuI?usp=sharing) and put them under $HOME/.torch/ofa_nets/.   """;Computer Vision;https://github.com/seulkiyeom/once-for-all
"""sbatch ./scripts/swav_800ep_pretrain.sh   Install detr and prepare COCO dataset following these instructions.   """;Computer Vision;https://github.com/ananyahjha93/swav
"""**Windows portable version**: Simply download and use the latest version from the [Releases](https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases) section.  Otherwise  check [doc/installation.md](doc/installation.md) for instructions on how to build OpenPose from source.     OS: Ubuntu (14  16)  Windows (8  10)  Mac OSX  Nvidia TX2.   CUDA (Nvidia GPU)  OpenCL (AMD GPU)  and CPU-only (no GPU) versions.   Most users do not need the OpenPose C++/Python API  but can simply use the OpenPose Demo:  - **OpenPose Demo**: To easily process images/video/webcam and display/save the results. See [doc/demo_overview.md](doc/demo_overview.md). E.g.  run OpenPose in a video with: ``` #: Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi ```  - **Calibration toolbox**: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See [doc/modules/calibration_module.md](doc/modules/calibration_module.md).  - **OpenPose C++ API**: If you want to read a specific input  and/or add your custom post-processing function  and/or implement your own display/saving  check the C++ API tutorial on [examples/tutorial_api_cpp/](examples/tutorial_api_cpp/) and [doc/library_introduction.md](doc/library_introduction.md). You can create your custom code on [examples/user_code/](examples/user_code/) and quickly compile it with CMake when compiling the whole OpenPose project. Quickly **add your custom code**: See [examples/user_code/README.md](examples/user_code/README.md) for further details.  - **OpenPose Python API**: Analogously to the C++ API  find the tutorial for the Python API on [examples/tutorial_api_python/](examples/tutorial_api_python/).  - **Adding an extra module**: Check [doc/library_add_new_module.md](./doc/library_add_new_module.md).  - **Standalone face or hand detector**:     - **Face** keypoint detection **without body** keypoint detection: If you want to speed it up (but also reduce amount of detected faces)  check the OpenCV-face-detector approach in [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     - **Use your own face/hand detector**: You can use the hand and/or face keypoint detectors with your own face or hand detectors  rather than using the body detector. E.g.  useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     """;General;https://github.com/lncarter/Openpose
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   This is a Github template repo you can use to create your own copy of the forked StyleGAN2 sample from NVLabs.   There are a few prerequisites you will need to have in place:   4. Create a github connected project by opening the Projects tab  pressing the ""LINK A GITHUB REPO"" button  and following the instructions on the next page.  5. You next need to authorize Gradient to access your github account if you have not done so already.   """;Computer Vision;https://github.com/hanehein921/StyleGAN2
""" First create lmdb datasets:  > python prepare_data.py --out LMDB_PATH --n_worker N_WORKER --size SIZE1 SIZE2 SIZE3 ... DATASET_PATH  This will convert images to jpeg and pre-resizes it. This implementation does not use progressive growing  but you can create multiple resolution datasets using size arguments with comma separated lists  for the cases that you want to try another resolutions later.  Then you can train model in distributed settings  > python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train.py --batch BATCH_SIZE LMDB_PATH  train.py supports Weights & Biases logging. If you want to use it  add --wandb arguments to the script.   """;General;https://github.com/alekseynp/stylegan2-pytorch
"""    from tensorflow.examples.tutorials.mnist import input_data     input_data.read_data_sets('my/directory')   """;General;https://github.com/arnavdodiedo/DenseNet-MNIST
"""To run the code on cpu  change the context from mx.gpu() to mx.cpu() at Line 110   To run the code  simply run  ``` python train.py ```  """;Computer Vision;https://github.com/jainshobhit/Variational-Autoencoder
"""We used `TensorFlow 0.11` and `Python 2`. `Sklearn` is also used.  The two datasets can be loaded by running `python download_datasets.py` in the `data/` folder.  To preprocess the second dataset (opportunity challenge dataset)  the `signal` submodule of scipy is needed  as well as `pandas`.    """;Natural Language Processing;https://github.com/Liut2016/stackedResBiLSTM
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/ohomburg/stylegan
"""It should be straightforward to use SAM in your training pipeline. Just keep in mind that the training will run twice as slow  because SAM needs two forward-backward passes to estime the ""sharpness-aware"" gradient. If you're using gradient clipping  make sure to change only the magnitude of gradients  not their direction.  ```python from sam import SAM ...  model = YourModel() base_optimizer = torch.optim.SGD  #: define an optimizer for the ""sharpness-aware"" update optimizer = SAM(model.parameters()  base_optimizer  lr=0.1  momentum=0.9) ...  for input  output in data:    #: first forward-backward pass   loss = loss_function(output  model(input))  #: use this loss for any training statistics   loss.backward()   optimizer.first_step(zero_grad=True)      #: second forward-backward pass   loss_function(output  model(input)).backward()  #: make sure to do a full forward pass   optimizer.second_step(zero_grad=True) ... ```  <br>  **Alternative usage with a single closure-based `step` function**. This alternative offers similar API to native PyTorch optimizers like LBFGS (kindly suggested by [@rmcavoy](https://github.com/rmcavoy)):  ```python from sam import SAM ...  model = YourModel() base_optimizer = torch.optim.SGD  #: define an optimizer for the ""sharpness-aware"" update optimizer = SAM(model.parameters()  base_optimizer  lr=0.1  momentum=0.9) ...  for input  output in data:   def closure():     loss = loss_function(output  model(input))     loss.backward()     return loss    loss = loss_function(output  model(input))   loss.backward()   optimizer.step(closure)   optimizer.zero_grad() ... ```   """;General;https://github.com/davda54/sam
"""Download mnist_test_seq.npy from here.   python train.py --config_path configs/moving_mnist/mnist_wgan_svd_zdim-100_no-beta-all_init-uniform-all.yml --gpu 0  python train.py --config_path configs/ucf101/ucf101_wgan_svd_zdim-100_no-beta.yml --gpu 0  python train.py --config_path configs/moving_mnist/mnist_wgan_clip_zdim-100_no-beta-all_init-uniform-all.yml --gpu 0  python train.py --config_path configs/ucf101/ucf101_wgan_clip_zdim-100_no-beta.yml --gpu 0  python train.py --config_path configs/ucf101/ucf101_vanilla_zdim-100_no-beta.yml --gpu 0   """;General;https://github.com/pfnet-research/tgan
"""you can see how i choose hyperparameters below   * Jupyter notebook: [link](https://nbviewer.jupyter.org/github/simonjisu/NMT/blob/master/Neural_Machine_Translation_Tutorial.ipynb) * Preparing for demo   For 'HELP' please insert argument behind `main.py -h`. or you can just run   ``` $ cd model $ sh runtrain.sh ```   """;General;https://github.com/simonjisu/NMT
"""1. Download and extract the [LJ Speech dataset](https://keithito.com/LJ-Speech-Dataset/) 2. Clone this repo: `git clone https://github.com/NVIDIA/tacotron2.git` 3. CD into this repo: `cd tacotron2` 4. Initialize submodule: `git submodule init; git submodule update` 5. Update .wav paths: `sed -i -- 's DUMMY ljs_dataset_folder/wavs g' filelists/*.txt`     - Alternatively  set `load_mel_from_disk=True` in `hparams.py` and update mel-spectrogram paths  6. Install [PyTorch 1.0] 7. Install [Apex] 8. Install python requirements or build docker image      - Install python requirements: `pip install -r requirements.txt`   1. Download our published [Tacotron 2] model 2. Download our published [WaveGlow] model 3. `jupyter notebook --ip=127.0.0.1 --port=31337` 4. Load inference.ipynb   N.b.  When performing Mel-Spectrogram to Audio synthesis  make sure Tacotron 2 and the Mel decoder were trained on the same mel-spectrogram representation.     """;Audio;https://github.com/NVIDIA/tacotron2
"""Refer to the following code on github:</br>   """;Reinforcement Learning;https://github.com/Brook1711/RIS_components
"""Start by cloning this repo:  ``` git clone https://github.com/delmalih/MIAS-mammography-obj-detection ```   conda create --name faster-r-cnn  conda activate faster-r-cnn  conda install ipython pip   pip install -r requirements.txt  cd ..  Then  run these commands (ignore if you have already done the FCOS installation) :   : install pytorch  conda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=9.0 -c pytorch   : install pycocotools  cd $INSTALL_DIR  git clone https://github.com/cocodataset/cocoapi.git  cd cocoapi/PythonAPI  python setup.py build_ext install  : install cityscapesScripts  cd $INSTALL_DIR  git clone https://github.com/mcordts/cityscapesScripts.git  cd cityscapesScripts/  python setup.py build_ext install  : install apex  cd $INSTALL_DIR  git clone https://github.com/NVIDIA/apex.git  cd apex  python setup.py install --cuda_ext --cpp_ext  : install PyTorch Detection  cd $INSTALL_DIR  git clone https://github.com/facebookresearch/maskrcnn-benchmark.git  cd maskrcnn-benchmark  python setup.py build develop  cd $INSTALL_DIR   conda create --name retinanet python=3.6  conda activate retinanet  conda install ipython pip   pip install -r requirements.txt  cd ..  pip install tensorflow-gpu==1.9  pip install keras==2.2.5  Then  run these commands :   : clone keras-retinanet repo  git clone https://github.com/fizyr/keras-retinanet  cd keras-retinanet  pip install .   conda create --name fcos  conda activate fcos  conda install ipython pip   pip install -r requirements.txt  cd ..  Then  follow these instructions   For example  to generate 10x augmented COCO annotations  run this command :                                       --output ../mias-db/COCO \   Go to the faster-r-cnn directory: cd faster-r-cnn  Change conda env: conda deactivate &amp;&amp; conda activate faster-r-cnn   cd retinanet  conda deactivate &amp;&amp; conda activate retinanet                   coco &lt;Path to the COCO dataset&gt;   tensorboard --logdir &lt;Path to the tensorboard directory&gt;   Follow these instructions   cd fcos  conda deactivate &amp;&amp; conda activate fcos   cd faster-r-cnn  conda deactivate &amp;&amp; conda activate faster-r-cnn   cd retinanet  conda deactivate &amp;&amp; conda activate retinanet                       coco &lt;Path to the COCO dataset&gt;  cd fcos  conda deactivate &amp;&amp; conda activate fcos   """;Computer Vision;https://github.com/moallafatma/Breast_Cancer_Detection_Classification
"""[x] Train Enet with Mish Encoder on CamVid   """;Computer Vision;https://github.com/soumik12345/Enet
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/cyctrung/DPnet
"""To work locally on your own laptop or workstation  use the Conda package managment system to create a work environment with the required software. After installing miniconda (above)  follow these steps to setup the work environment and run the   Create the environment...  conda env create --prefix ./env --file environment-gpu.yml  ...then activate the environment...  conda activate ./env   """;Computer Vision;https://github.com/haaz77/keras-tutorials
"""__*To find out on how to use  please take a look at `UNet_OctConv_keras.ipynb`*__    __idea came from :__   [Accurate Retinal Vessel Segmentation viaOctave Convolution Neural Network](https://arxiv.org/pdf/1906.12193.pdf)   __UNet Network :__   [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf)  """;Computer Vision;https://github.com/koshian2/OctConv-TFKeras
"""| sh              | 0.56         | 0.77         | 0.81          |   |     bs     |     sh     |           0.88          |            0.52           |   |     hr     |     sh     |           0.78          |            0.56           |   |     sr     |     sh     |           0.73          |            0.54           |   """;Natural Language Processing;https://github.com/babylonhealth/fastText_multilingual
"""This project is implemented in [PyTorch](http://www.pytorch.org). It is recommended to create an Anaconda environment for the project and all associated package dependencies: ``` conda create -n cca_visdial python=3.6 pip source activate cca_visdial conda install pytorch torchvision cuda80 -c pytorch bash install_deps.sh ```  The project was built using Python 3.6  PyTorch 1.0 and CUDA 8.0. Check [PyTorch](http://www.pytorch.org) for other versions and operating systems.   To run the CCA algorithm on the Visual Dialogue v0.9 dataset with default settings  use the following command:   """;General;https://github.com/danielamassiceti/CCA-visualdialogue
"""* This is an assignment of [Deep Learning basic class](https://deeplearning.jp/lectures/dlb2018/) arranged a little.  * Training Fashion-MNIST by ResNet on Google Colaboratory with TensorFlow 2.0 Alpha. * Data is augmented by ImageDataGenerator of Keras.   """;Computer Vision;https://github.com/shoji9x9/Fashion-MNIST-By-ResNet
"""DeepLab is a state-of-art deep learning system for semantic image segmentation built on top of [Caffe](http://caffe.berkeleyvision.org).  It combines (1) *atrous convolution* to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks  (2) *atrous spatial pyramid pooling* to robustly segment objects at multiple scales with filters at multiple sampling rates and effective fields-of-views  and (3) densely connected conditional random fields (CRF) as post processing.  This distribution provides a publicly available implementation for the key model ingredients reported in our latest [arXiv paper](http://arxiv.org/abs/1606.00915). It also contains implementations for **all** methods reported in all our previous papers.  Please consult and consider citing the following papers:      @article{CP2016Deeplab        title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets  Atrous Convolution  and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        journal={arXiv:1606.00915}        year={2016}     }      @inproceedings{CY2016Attention        title={Attention to Scale: Scale-aware Semantic Image Segmentation}        author={Liang-Chieh Chen and Yi Yang and Jiang Wang and Wei Xu and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{CB2016Semantic        title={Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform}        author={Liang-Chieh Chen and Jonathan T Barron and George Papandreou and Kevin Murphy and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{PC2015Weak        title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation}        author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille}        booktitle={ICCV}        year={2015}     }      @inproceedings{CP2015Semantic        title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        booktitle={ICLR}        year={2015}     }   Note that if you use the densecrf implementation  please consult and cite the following paper:      @inproceedings{KrahenbuhlK11        title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}        author={Philipp Kr{\""{a}}henb{\""{u}}hl and Vladlen Koltun}        booktitle={NIPS}        year={2011}     }   1. The scripts we used for our experiments can be downloaded from this [link](https://ucla.box.com/s/4grlj8yoodv95936uybukjh5m0tdzvrf):     1. run_pascal.sh: the script for training/testing on the PASCAL VOC 2012 dataset. __Note__ You also need to download sub.sed script.     2. run_densecrf.sh and run_densecrf_grid_search.sh: the scripts we used for post-processing the DCNN computed results by DenseCRF. 2. The image list files used in our experiments can be downloaded from this [link](https://ucla.box.com/s/rd9z2xvwsfpksi7mi08i2xqrj7ab4keb):     * The zip file stores the list files for the PASCAL VOC 2012 dataset. 3. To use the mat_read_layer and mat_write_layer  please download and install [matio](http://sourceforge.net/projects/matio/files/matio/1.5.2/).   """;Computer Vision;https://github.com/liarba/caffe_dev
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.  git clone https://github.com/affinelayer/pix2pix-tensorflow.git  cd pix2pix-tensorflow   ```sh #: clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git cd pix2pix-tensorflow #: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir facades_train \   --max_epochs 200 \   --input_dir facades/train \   --which_direction BtoA #: test the model python pix2pix.py \   --mode test \   --output_dir facades_test \   --input_dir facades/val \   --checkpoint facades_train ```  The test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.  If you have Docker installed  you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:  ```sh #: train the model python tools/dockrun.py python pix2pix.py \       --mode train \       --output_dir facades_train \       --max_epochs 200 \       --input_dir facades/train \       --which_direction BtoA #: test the model python tools/dockrun.py python pix2pix.py \       --mode test \       --output_dir facades_test \       --input_dir facades/val \       --checkpoint facades_train ```   <img src=""docs/combine.png"" width=""900px""/>  ```sh #: Resize source images python tools/process.py \   --input_dir photos/original \   --operation resize \   --output_dir photos/resized #: Create images with blank centers python tools/process.py \   --input_dir photos/resized \   --operation blank \   --output_dir photos/blank #: Combine resized images with blanked images python tools/process.py \   --input_dir photos/resized \   --b_dir photos/blank \   --operation combine \   --output_dir photos/combined #: Split into train/val set python tools/split.py \   --dir photos/combined ```  The folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.   """;General;https://github.com/saviaga/faceswap
"""lr - learning rate can be scalar or function  in second case relative step size is using.  beta1  beta2 - is also can be scalar or functions  in first case algorithm works as AMSGrad. Setting beta1 to zero is turning off moments updates.  non_constant_decay - boolean  has effect if betas are scalars. If True using functions for betas (from section 7.1)  enable_factorization - boolean. Factorization works on 2D weights.  clipping_threshold - scalar. Threshold value for update clipping (from section 6)  """;General;https://github.com/DeadAt0m/adafactor-pytorch
"""👯 Clone this repo:  $ git clone https://github.com/saimj7/People-Counting-in-Real-Time.git   """;General;https://github.com/jaykshirsagar05/CrowdCounting
"""Retraining The Pre-Trained Models ```sh py -m scripts.retrain --bottleneck_dir=tf_files/bottlenecks --how_many_training_steps=500 --model_dir=tf_files/models/ --summaries_dir=tf_files/training_summaries/mobilenet_0.50_224 --output_graph=tf_files/retrained_graph.pb --output_labels=tf_files/retrained_labels.txt --architecture=mobilenet_0.50_224 --image_dir=tf_files/... ```    1. Clone the repo ```sh git clone https://github.com/ZyphonGT/TFLite-Frontier-Project-UPH ``` 2. Open Android Studio 3. Choose `Open Existing Project` 4. Open the project file `Project_File/android/tflite` 5. After Android Studio finishes loading  click on `Sync Project with Gradle Files` 6. Run the app   To get a local copy up and running follow these simple example steps.   For development purposes  it is also possible to test the trained model using image input (.jpeg  .jpg  .gif) via Command Prompt  1. Open your Command Prompt and Navigate to the project folder 2. Enter the following command ```sh py -m scripts.label_image --graph=tf_files/retrained_graph.pb  --image=PATH_TO_YOUR_TEST_IMAGE ```   """;Computer Vision;https://github.com/ZyphonGT/TFLite-Frontier-Project-UPH
"""You have to compile your model with focal loss. Sample: ``` model_prn.compile(optimizer=optimizer  loss=[focal_loss(alpha=.25  gamma=2)]) ```   """;Computer Vision;https://github.com/mkocabas/focal-loss-keras
"""```bash $ pip install local-attention ```   ```python import torch from local_attention import LocalAttention  q = torch.randn(8  2048  64) k = torch.randn(8  2048  64) v = torch.randn(8  2048  64)  attn = LocalAttention(     dim = 64                 #: dimension of each head (you need to pass this in for relative positional encoding)     window_size = 512        #: window size. 512 is optimal  but 256 or 128 yields good enough results     causal = True            #: auto-regressive or not     look_backward = 1        #: each window looks at the window before     look_forward = 0         #: for non-auto-regressive case  will default to 1  so each window looks at the window before and after it     dropout = 0.1            #: post-attention dropout     exact_windowsize = False #: if this is set to true  in the causal setting  each query will see at maximum the number of keys equal to the window size )  mask = torch.ones(1  2048).bool() out = attn(q  k  v  input_mask = mask) #: (1  8  2048  64) ```  This library also allows for local attention in the setting of shared query/key space. The normalization of the keys  as well as the masking of tokens to itself  will be taken care of.  ```python import torch from local_attention import LocalAttention  qk = torch.randn(8  2048  64) v  = torch.randn(8  2048  64)  attn = LocalAttention(     dim = 64      window_size = 512      shared_qk = True      causal = True )  mask = torch.ones(1  2048).bool() out = attn(qk  qk  v  input_mask = mask) #: (1  8  2048  64) ```  If you wish for the module to automagically pad your query / key / values as well as the mask  simply set the `autopad` keyword to `True`  ```python import torch from local_attention import LocalAttention  q = torch.randn(8  2057  64) k = torch.randn(8  2057  64) v = torch.randn(8  2057  64)  attn = LocalAttention(     window_size = 512      causal = True      autopad = True      #: auto pads both inputs and mask  then truncates output appropriately )  mask = torch.ones(1  2057).bool() out = attn(q  k  v  input_mask = mask) #: (1  8  2057  64) ```   """;Natural Language Processing;https://github.com/lucidrains/local-attention
"""音声生成は、音楽制作や映画のSE音で実用的であると考えられています。   映画などの制作に携わっている音響監督の方達は、作品内で使用するSE音を選ぶ時、多数ある音の中からその場面に合う一つを見つけなければなりません。とても面倒臭い作業です。   そこで、音声生成があるとSE音を探したい場面の情報をインプットしただけで適した音を生成してくれるとその作業が楽になるのではないかと考えられます。   <img width=""625"" alt=""how_to_use_voice_synthesis"" src=""https://user-images.githubusercontent.com/39772824/71435632-22423f80-272d-11ea-985f-6a55735da5d9.png"">   従来の音声生成には自己回帰トレーニングによるニューラルネットワークモデルがあげられるが、これは出力が出るたびにフィードバックをしなければならないので、とても時間がかかる方法です。   画像生成で使われているGANを音声生成で使用するには、スペクトログラムに変換して画像として扱うと簡単になると考えられます。    この論文では、2種類のGANの提案をしています。  --- 1つ目はSpecGANと呼ばれるもので、これは入力のオーディオデータをスペクトログラムに直して扱うモデルです。   <img width=""776"" alt=""SpecGAN"" src=""https://user-images.githubusercontent.com/39772824/71434596-962e1900-2728-11ea-9a69-93b3c72d03e9.png"">    --- 2つ目はWaveGANと呼ばれるもので、これは画像生成に使われているDCGANを音声生成に対応するように作り替えたものです。入力データを別の形に変換せずにそのまま使えるのが特徴です。   <img width=""555"" alt=""WaveGAN"" src=""https://user-images.githubusercontent.com/39772824/71434590-91696500-2728-11ea-9958-3d52cec1892f.png"">     <img src=""https://latex.codecogs.com/gif.latex?|D|_{train}"">   """;Audio;https://github.com/mahotani/ADVERSARIAL-AUDIO-SYNTHESIS
"""**Windows portable version**: Simply download and use the latest version from the [Releases](https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases) section.  Otherwise  check [doc/installation.md](doc/installation.md) for instructions on how to build OpenPose from source.     OS: Ubuntu (14  16)  Windows (8  10)  Mac OSX  Nvidia TX2.   CUDA (Nvidia GPU)  OpenCL (AMD GPU)  and CPU versions.   Most users do not need the OpenPose C++/Python API  but can simply use the OpenPose Demo:  - **OpenPose Demo**: To easily process images/video/webcam and display/save the results. See [doc/demo_overview.md](doc/demo_overview.md). E.g.  run OpenPose in a video with: ``` #: Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi ```  - **Calibration toolbox**: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See [doc/modules/calibration_module.md](doc/modules/calibration_module.md).  - **OpenPose C++ API**: If you want to read a specific input  and/or add your custom post-processing function  and/or implement your own display/saving  check the C++ API tutorial on [examples/tutorial_api_cpp/](examples/tutorial_api_cpp/) and [doc/library_introduction.md](doc/library_introduction.md). You can create your custom code on [examples/user_code/](examples/user_code/) and quickly compile it with CMake when compiling the whole OpenPose project. Quickly **add your custom code**: See [examples/user_code/README.md](examples/user_code/README.md) for further details.  - **OpenPose Python API**: Analogously to the C++ API  find the tutorial for the Python API on [examples/tutorial_api_python/](examples/tutorial_api_python/).  - **Adding an extra module**: Check [doc/library_add_new_module.md](./doc/library_add_new_module.md).  - **Standalone face or hand detector**:     - **Face** keypoint detection **without body** keypoint detection: If you want to speed it up (but also reduce amount of detected faces)  check the OpenCV-face-detector approach in [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     - **Use your own face/hand detector**: You can use the hand and/or face keypoint detectors with your own face or hand detectors  rather than using the body detector. E.g.  useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     """;General;https://github.com/zyxcambridge/openpose_all
"""**Note : Use Python 3**  The input images and target masks should be in the `data/cells/scans` and `data/cells/labels` folders respectively.   ---  Original paper by Olaf Ronneberger  Philipp Fischer  Thomas Brox: [https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)  ![network architecture](https://i.imgur.com/jeDVpqF.png)  """;Computer Vision;https://github.com/KeyG518/Unet_Cell_segmentation
"""We provide pretrained checkpoints run.zip. Extract the file to the root folder. You should be able to produce samples like the following using this checkpoint.   """;Computer Vision;https://github.com/Lornatang/PyTorch-NCSN
"""official : https://github.com/clovaai/CutMix-PyTorch   """;Computer Vision;https://github.com/airplane2230/keras_cutmix
"""Github with code for Shakespeare example: https://github.com/branavg/Text-Generation/blob/master/Text_Generation.ipynb   Download Magenta: https://magenta.tensorflow.org   """;General;https://github.com/joebluems/Soul_Of_AI
"""```python import torch from torch import nn from repmlp-pytorch import RepMLP  N=4 #:batch size C=512 #:input dim O=1024 #:output dim H=14 #:image height W=14 #:image width h=7 #:patch height w=7 #:patch width fc1_fc2_reduction=1 #:reduction ratio fc3_groups=8 #: groups repconv_kernels=[1 3 5 7] #:kernel list repmlp=RepMLP(C H W h w fc1_fc2_reduction fc3_groups repconv_kernels=repconv_kernels) x=torch.randn(N C H W) y=repmlp(x) ```   ![](./repvgg.png)   ```python  import torch from torch import nn from repvgg-pytorch import RepBlock  input=torch.randn(50 512 49 49) repblock=RepBlock(512 512) repblock.eval() out=repblock(input) repblock._switch_to_deploy() out2=repblock(input) print('difference between vgg and repvgg') print(((out2-out)**2).sum())  ```     """;Computer Vision;https://github.com/xmu-xiaoma666/RepMLP-pytorch
"""Python 3.8   """;General;https://github.com/sumitkutty/Anti-Spoof-Face-Recognition
"""Samples:   **reference speaker A:** [S0913(./data/S0913/BAC009S0913W0351.wav)](https://drive.google.com/file/d/14zU1mI8QtoBwb8cHkNdZiPmXI6Mj6pVW/view?usp=sharing)  **reference speaker B:** [GaoXiaoSong(./data/gaoxiaosong/gaoxiaosong_1.wav)](https://drive.google.com/file/d/1s0ip6JwnWmYoWFcEQBwVIIdHJSqPThR3/view?usp=sharing)    **speaker A's speech changes to speaker B's voice:** [Converted from S0913 to GaoXiaoSong (./converted_sound/S0913/BAC009S0913W0351.wav)](https://drive.google.com/file/d/1S4vSNGM-T0RTo_aclxRgIPkUJ7NEqmjU/view?usp=sharing)  ------   """;General;https://github.com/jackaduma/CycleGAN-VC2
"""<ul> <p> Docker </p> </ul> Vous pouvez directement créer un environnement stable via docker Depuis le répertoire principal  executer la commande  ``` docker build -t cyclegan . docker run -it --name cycleganC cyclegan ``` <hr> <ul> <p> Déploiement cloud </p> </ul>  Vous pouvez suivre cette procedure pour déployer une machine virtuel sur Google Cloud ``` Créer une instance 'Deep Learning VM ' 8vCPU  30 Gb RAM  GPU NVIDIA P100(ou autre)  avec le framework TensorFlow Enterprise 2.1 (CUDA 10.1). Cocher la case 'Install NVIDIA GPU driver automatically on first startup?' Copier votre clef publique dans la VM  afin de pouvoir y acceder en SSH. Copier le contenu du projet ""AIF_CycleGan"" via un scp -r ou via un gitclone. Mettre à jour les prérequis avec mise_a_jour.sh (optionnel sur une instance Deep Learning VM). Lancer python3 main.py ou un tunneling SSH pour utiliser le notebook. ``` Tunneling SSH ``` Depuis la VM:  jupyter notebook --no-browser --port=8080 Depuis l'ordinateur local ssh -N -L 8080:localhost:8080 <IDuser>@<ipVM> ```   Installer les prérequis issu du fichier requirements.txt   <li>mise_a_jour.sh: Script d'installation des requirements.txt</li>   ![Example cRAG](imgs/CycleGanExample.jpg ""Example cRAG"")    """;General;https://github.com/PDEUXA/AIF_CYCLEGAN
"""$ mvn install -Papp   $ cd app   The memory-mapped FastText may only be used from one thread  because it is not thread safe  (it keeps internal state like the mapped file positions).   To allow multithreaded use  every FastText instance must be cloned before being used in another thread.    """;Natural Language Processing;https://github.com/linkfluence/fastText4j
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/lehoanganh298/BERT-Question-Answering
""" - Most words are symbols for an extra-linguistic entity - a word is a signifier that maps to a signified (idea/thing) - Approx. 13m words in English language   - There is probably some N-dimensional space (such that N << 13m) that is sufficient to encode all semantics of our language - Most simple word vector - one-hot encoding   - Denotational semantics - the concept of representing an idea as a symbol - a word or one-hot vector - sparse  cannot capture similarity - localist encoding  Evaluation  - Intrinsic - evaluation on a specific  intermediate task   - Fast to compute   - Aids with understanding of the system   - Needs to be correlated with real task to provide a good measure of usefulness   - Word analogies - popular intrinsic evaluation method for word vectors     - Semantic - e.g. King/Man | Queen/Woman     - Syntactic - e.g. big/biggest | fast/fastest - Extrinsic - evaluation on a real task   - Slow   - May not be clear whether the problem with low performance is related to a particular subsystem  other subsystems  or interactions between subsystems   - If a subsystem is replaced and performance improves  the change is likely to be good   """;Natural Language Processing;https://github.com/jeremycz/word-vectors
"""See data/faster_rcnn_inception_resnet_v2_atrous_text.config for example configuration The parameter: second_stage_localization_loss_weight_oriented is the weight for the oriented bounding box prediction.   Changing the network configuration setting is easy. For example  to change the different aspect ratios of the anchors used  simply changing the grid_anchor_generator in the configuration file.   You do not need to use blaze build to build the code. Simply run the code from the root directory for fast experiment.   """;Computer Vision;https://github.com/dafanghe/Tensorflow_SceneText_Oriented_Box_Predictor
"""We used a subset of Imagenet dataset ILSVRC2016_CLS-LOC.tar.gz for training our models. The subset can be found in <code>/subset.txt</code>    """;General;https://github.com/twhui/SRGAN-PyTorch
"""So we talked about what transformers are  and what they can do for you (among other things). <br/> Let's get this thing running! Follow the next steps:  1. `git clone https://github.com/gordicaleksa/pytorch-original-transformer` 2. Open Anaconda console and navigate into project directory `cd path_to_repo` 3. Run `conda env create` from project directory (this will create a brand new conda environment). 4. Run `activate pytorch-transformer` (for running scripts from your console or set the interpreter in your IDE)  That's it! It should work out-of-the-box executing environment.yml file which deals with dependencies. <br/> It may take a while as I'm automatically downloading SpaCy's statistical models for English and German.  -----  PyTorch pip package will come bundled with some version of CUDA/cuDNN with it  but it is highly recommended that you install a system-wide CUDA beforehand  mostly because of the GPU drivers.  I also recommend using Miniconda installer as a way to get conda on your system. Follow through points 1 and 2 of [this setup](https://github.com/Petlja/PSIML/blob/master/docs/MachineSetup.md) and use the most up-to-date versions of Miniconda and CUDA/cuDNN for your system.   Hardware requirements   You probably heard of transformers one way or another. GPT-3 and BERT to name a few well known ones :unicorn:. The main idea   Just do pip uninstall pywin32 and then either pip install pywin32 or conda install pywin32 should fix it!   () Note: after you train your model it'll get dumped into models/binaries see what it's name is and specify it via   You can see here 3 runs  the 2 lower ones used PyTorch default initialization (one used mean for KL divergence   Just run tensorboard --logdir=runs from your Anaconda console and you can track your metrics during the training.   The repo already has everything it needs  these are just the bonus points. I've tested everything  from environment setup  to automatic model download  etc.   You just need to link the Python environment you created in the [setup](#setup) section.   """;General;https://github.com/gordicaleksa/pytorch-original-transformer
"""PyTorch book   """;Natural Language Processing;https://github.com/TEAMLAB-Lecture/deep_nlp_101
"""We recommend Anaconda as Python package management system. Please refer to `pytorch.org <https://pytorch.org/>`_ for the detail of PyTorch (``torch``) installation. The following is the corresponding ``torchvision`` versions and supported Python versions.  +--------------------------+--------------------------+---------------------------------+ | ``torch``                | ``torchvision``          | ``python``                      | +==========================+==========================+=================================+ | ``main`` / ``nightly``   | ``main`` / ``nightly``   | ``>=3.6``  ``<=3.9``            | +--------------------------+--------------------------+---------------------------------+ | ``1.10.0``               | ``0.11.1``               | ``>=3.6``  ``<=3.9``            | +--------------------------+--------------------------+---------------------------------+ | ``1.9.1``                | ``0.10.1``               | ``>=3.6``  ``<=3.9``            | +--------------------------+--------------------------+---------------------------------+ | ``1.9.0``                | ``0.10.0``               | ``>=3.6``  ``<=3.9``            | +--------------------------+--------------------------+---------------------------------+ | ``1.8.2``                | ``0.9.2``                | ``>=3.6``  ``<=3.9``            | +--------------------------+--------------------------+---------------------------------+ | ``1.8.1``                | ``0.9.1``                | ``>=3.6``  ``<=3.9``            | +--------------------------+--------------------------+---------------------------------+ | ``1.8.0``                | ``0.9.0``                | ``>=3.6``  ``<=3.9``            | +--------------------------+--------------------------+---------------------------------+ | ``1.7.1``                | ``0.8.2``                | ``>=3.6``  ``<=3.9``            | +--------------------------+--------------------------+---------------------------------+ | ``1.7.0``                | ``0.8.1``                | ``>=3.6``  ``<=3.8``            | +--------------------------+--------------------------+---------------------------------+ | ``1.7.0``                | ``0.8.0``                | ``>=3.6``  ``<=3.8``            | +--------------------------+--------------------------+---------------------------------+ | ``1.6.0``                | ``0.7.0``                | ``>=3.6``  ``<=3.8``            | +--------------------------+--------------------------+---------------------------------+ | ``1.5.1``                | ``0.6.1``                | ``>=3.5``  ``<=3.8``            | +--------------------------+--------------------------+---------------------------------+ | ``1.5.0``                | ``0.6.0``                | ``>=3.5``  ``<=3.8``            | +--------------------------+--------------------------+---------------------------------+ | ``1.4.0``                | ``0.5.0``                | ``==2.7``  ``>=3.5``  ``<=3.8`` | +--------------------------+--------------------------+---------------------------------+ | ``1.3.1``                | ``0.4.2``                | ``==2.7``  ``>=3.5``  ``<=3.7`` | +--------------------------+--------------------------+---------------------------------+ | ``1.3.0``                | ``0.4.1``                | ``==2.7``  ``>=3.5``  ``<=3.7`` | +--------------------------+--------------------------+---------------------------------+ | ``1.2.0``                | ``0.4.0``                | ``==2.7``  ``>=3.5``  ``<=3.7`` | +--------------------------+--------------------------+---------------------------------+ | ``1.1.0``                | ``0.3.0``                | ``==2.7``  ``>=3.5``  ``<=3.7`` | +--------------------------+--------------------------+---------------------------------+ | ``<=1.0.1``              | ``0.2.2``                | ``==2.7``  ``>=3.5``  ``<=3.7`` | +--------------------------+--------------------------+---------------------------------+  Anaconda:  .. code:: bash      conda install torchvision -c pytorch  pip:  .. code:: bash      pip install torchvision  From source:  .. code:: bash      python setup.py install     # or  for OSX     # MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install   In case building TorchVision from source fails  install the nightly version of PyTorch following the linked guide on the  `contributing page <https://github.com/pytorch/vision/blob/main/CONTRIBUTING.md#development-installation>`_ and retry the install.  By default  GPU support is built if CUDA is found and ``torch.cuda.is_available()`` is true. It's possible to force building GPU support by setting ``FORCE_CUDA=1`` environment variable  which is useful when building a docker image.   libpng_ - can be installed via conda :code:conda install libpng or any of the package managers for debian-based and RHEL-based Linux distributions.  libjpeg - can be installed via conda :code:conda install jpeg or any of the package managers for debian-based and RHEL-based Linux distributions. libjpeg-turbo can be used as well.   .. _Pillow : https://python-pillow.org/  .. _Pillow-SIMD : https://github.com/uploadcare/pillow-simd  .. _accimage: https://github.com/pytorch/accimage   .. _pyav : https://github.com/PyAV-Org/PyAV    conda install -c conda-forge ffmpeg   python setup.py install   Installation From source:   mkdir build  cd build  # Add -DWITH_CUDA=on support for the CUDA if needed   make install   find_package(TorchVision REQUIRED)   so make sure that it is also available to cmake via the CMAKE_PREFIX_PATH.   In order to get the torchvision operators registered with torch (eg. for the JIT)  all you need to do is to ensure that you   """;Computer Vision;https://github.com/pytorch/vision
"""The plankton dataset that we use contains 712 491 images of plankton spread __unevenly__ accross 65 different species. These are in turn divided into train  validation and test sets with the following ratios: <ul> <li>Train: &emsp;&emsp; &emsp;&nbsp; 699 491 images</li> <li>Validation:&emsp;&nbsp; 6500 images</li> <li>Test: &emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp; 6500 images</li> </ul>  In short we used the *train*  *validate* and *test* folders inside the *data-65* folder from the dataset given to us for this assignment. <br>  Here is an overview of what the different species look like:   ```python class_names = ['Acantharea'  'Acartiidae'  'Actinopterygii'  'Annelida'  'Bivalvia__Mollusca'  'Brachyura'                'bubble'  'Calanidae'  'Calanoida'  'calyptopsis'  'Candaciidae'  'Cavoliniidae'  'Centropagidae'                'Chaetognatha'  'Copilia'  'Corycaeidae'  'Coscinodiscus'  'Creseidae'  'cyphonaute'  'cypris'                'Decapoda'  'Doliolida'  'egg__Actinopterygii'  'egg__Cavolinia_inflexa'  'Eucalanidae'  'Euchaetidae'                'eudoxie__Diphyidae'  'Evadne'  'Foraminifera'  'Fritillariidae'  'gonophore__Diphyidae'  'Haloptilus'                'Harpacticoida'  'Hyperiidea'  'larvae__Crustacea'  'Limacidae'  'Limacinidae'  'Luciferidae'  'megalopa'                'multiple__Copepoda'  'nauplii__Cirripedia'  'nauplii__Crustacea'  'nectophore__Diphyidae'  'nectophore__Physonectae'                 'Neoceratium'  'Noctiluca'  'Obelia'  'Oikopleuridae'  'Oithonidae'  'Oncaeidae'  'Ophiuroidea'  'Ostracoda'  'Penilia'                'Phaeodaria'  'Podon'  'Pontellidae'  'Rhincalanidae'  'Salpida'  'Sapphirinidae'  'scale'  'seaweed'  'tail__Appendicularia'                'tail__Chaetognatha'  'Temoridae'  'zoea__Decapoda'] link_list = ['https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/{}.jpg'.format(cn) for cn in class_names] html_list = [""<table>""] for i in range(8):     html_list.append(""<tr>"")     for j in range(8):         html_list.append(""<td><center>{}</center><img src='{}'></td>"".format(class_names[i*8+j]  link_list[i*8+j]))     html_list.append(""</tr>"") html_list.append(""</table>"")  display(HTML(''.join(html_list)))    ```   <table><tr><td><center>Acantharea</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Acantharea.jpg'></td><td><center>Acartiidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Acartiidae.jpg'></td><td><center>Actinopterygii</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Actinopterygii.jpg'></td><td><center>Annelida</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Annelida.jpg'></td><td><center>Bivalvia__Mollusca</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Bivalvia__Mollusca.jpg'></td><td><center>Brachyura</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Brachyura.jpg'></td><td><center>bubble</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/bubble.jpg'></td><td><center>Calanidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Calanidae.jpg'></td></tr><tr><td><center>Calanoida</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Calanoida.jpg'></td><td><center>calyptopsis</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/calyptopsis.jpg'></td><td><center>Candaciidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Candaciidae.jpg'></td><td><center>Cavoliniidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Cavoliniidae.jpg'></td><td><center>Centropagidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Centropagidae.jpg'></td><td><center>Chaetognatha</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Chaetognatha.jpg'></td><td><center>Copilia</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Copilia.jpg'></td><td><center>Corycaeidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Corycaeidae.jpg'></td></tr><tr><td><center>Coscinodiscus</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Coscinodiscus.jpg'></td><td><center>Creseidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Creseidae.jpg'></td><td><center>cyphonaute</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/cyphonaute.jpg'></td><td><center>cypris</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/cypris.jpg'></td><td><center>Decapoda</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Decapoda.jpg'></td><td><center>Doliolida</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Doliolida.jpg'></td><td><center>egg__Actinopterygii</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/egg__Actinopterygii.jpg'></td><td><center>egg__Cavolinia_inflexa</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/egg__Cavolinia_inflexa.jpg'></td></tr><tr><td><center>Eucalanidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Eucalanidae.jpg'></td><td><center>Euchaetidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Euchaetidae.jpg'></td><td><center>eudoxie__Diphyidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/eudoxie__Diphyidae.jpg'></td><td><center>Evadne</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Evadne.jpg'></td><td><center>Foraminifera</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Foraminifera.jpg'></td><td><center>Fritillariidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Fritillariidae.jpg'></td><td><center>gonophore__Diphyidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/gonophore__Diphyidae.jpg'></td><td><center>Haloptilus</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Haloptilus.jpg'></td></tr><tr><td><center>Harpacticoida</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Harpacticoida.jpg'></td><td><center>Hyperiidea</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Hyperiidea.jpg'></td><td><center>larvae__Crustacea</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/larvae__Crustacea.jpg'></td><td><center>Limacidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Limacidae.jpg'></td><td><center>Limacinidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Limacinidae.jpg'></td><td><center>Luciferidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Luciferidae.jpg'></td><td><center>megalopa</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/megalopa.jpg'></td><td><center>multiple__Copepoda</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/multiple__Copepoda.jpg'></td></tr><tr><td><center>nauplii__Cirripedia</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/nauplii__Cirripedia.jpg'></td><td><center>nauplii__Crustacea</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/nauplii__Crustacea.jpg'></td><td><center>nectophore__Diphyidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/nectophore__Diphyidae.jpg'></td><td><center>nectophore__Physonectae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/nectophore__Physonectae.jpg'></td><td><center>Neoceratium</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Neoceratium.jpg'></td><td><center>Noctiluca</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Noctiluca.jpg'></td><td><center>Obelia</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Obelia.jpg'></td><td><center>Oikopleuridae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Oikopleuridae.jpg'></td></tr><tr><td><center>Oithonidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Oithonidae.jpg'></td><td><center>Oncaeidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Oncaeidae.jpg'></td><td><center>Ophiuroidea</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Ophiuroidea.jpg'></td><td><center>Ostracoda</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Ostracoda.jpg'></td><td><center>Penilia</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Penilia.jpg'></td><td><center>Phaeodaria</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Phaeodaria.jpg'></td><td><center>Podon</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Podon.jpg'></td><td><center>Pontellidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Pontellidae.jpg'></td></tr><tr><td><center>Rhincalanidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Rhincalanidae.jpg'></td><td><center>Salpida</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Salpida.jpg'></td><td><center>Sapphirinidae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Sapphirinidae.jpg'></td><td><center>scale</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/scale.jpg'></td><td><center>seaweed</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/seaweed.jpg'></td><td><center>tail__Appendicularia</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/tail__Appendicularia.jpg'></td><td><center>tail__Chaetognatha</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/tail__Chaetognatha.jpg'></td><td><center>Temoridae</center><img src='https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/species/Temoridae.jpg'></td></tr></table>    ```python Image(url= ""https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/test_set_scores.png"") ```     <img src=""https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/test_set_scores.png""/>    <br> The model managed 91.78% accuracy on the test set which is only a 0.5% decrease from what we observed from the validation data during training.   Here are a few magic cells to view our models history via tensorboard interface. The logs folders can be dowloaded from 'https://github.com/JakobKallestad/InceptionV3-on-plankton-images'.   : !pip install tensorboard   <br>   - In the beginning the model only went over __less than 1%__ of the entire training set per epoch and only the top layers were trainable. This is why there are so many epochs in the graph.      - The first peak was when we decided to __unfreeze more layers__ than just the top layers of inception and made them trainable which increased accuracy from about 8% to about 15%.      - The second peak was when we realized that it helped to __increase the batch size and steps_per epoch__ so that the model  over about 20% of the entire training set per epoch.      - The last __giant peak__ happened as soon as we __unfroze all the layers__ of inception for training.      At this point we decided to start from scratch in order to try and recreate the very quick increase in accuracy that the model achieved by the end  but achieve this in less epochs.       ```python #: Initial experiments with a maximum of 89% accuracy: Image(url= ""https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/original_acc.png"") ```     <img src=""https://raw.githubusercontent.com/JakobKallestad/InceptionV3-on-plankton-images/master/images/plankton/original_acc.png""/>    Red = validation data  Blue = training data <br> <br>   """;Computer Vision;https://github.com/JakobKallestad/InceptionV3-on-plankton-images
"""``` $ cd CycleGAN/ $ python CycleGAN_model.py ``` An example of the generated adversarial examples is as follows:  <img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D1.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D2.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D3.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D4.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D5.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D6.jpg"" width=""290""/>    """;General;https://github.com/Our4514/CAGFUZZ
"""```bash $ pip install halonet-pytorch ```   ```python import torch from halonet_pytorch import HaloAttention  attn = HaloAttention(     dim = 512          #: dimension of feature map     block_size = 8     #: neighborhood block size (feature map must be divisible by this)     halo_size = 4      #: halo size (block receptive field)     dim_head = 64      #: dimension of each head     heads = 4          #: number of attention heads ).cuda()  fmap = torch.randn(1  512  32  32).cuda() attn(fmap) #: (1  512  32  32) ```   """;Computer Vision;https://github.com/lucidrains/halonet-pytorch
"""* First  download a dataset  e.g. apple2orange  ```bash $ bash download_dataset.sh apple2orange ```  * Write the dataset to tfrecords  ```bash $ python3 build_data.py ```  Check `$ python3 build_data.py --help` for more details.   Python 3.6.0   My pretrained models are available at https://github.com/vanhuyz/CycleGAN-TensorFlow/releases   """;General;https://github.com/vanhuyz/CycleGAN-TensorFlow
"""**You can see trained agent in action [here](https://www.youtube.com/watch?v=kldATbEf1zE)**  A reward of +0.1 is provided for holding the ball at each step. The maximum reward is 40 points.    The state space has 33 dimensions and contains the position  rotation  velocity  and angular velocities of the arm.  The action has 4 dimension in range -1 to 1. And describe the tourge to each part of arm.  The task is episodic  and in order to solve the environment  your agent must get an average *score of +30 over 100* consecutive episodes.   This project  describes the reinforcement learning to resolve the continues control problem.  The problem is describe with continues state space and continues action space. The goal is to hold the ball with moving arm :)  I used TD3 algorithm which is extension of Deep Deterministic Gradient Policy method. I include my private extension of local exploration. more details in Report.pdf and https://arxiv.org/pdf/1802.09477.pdf  The enviroment comes from Unity  please read the Unity Environment  before making a copy and trying yourself!   The project was tested on 3.6 python and requires the following packages to be installed: - numpy 1.16.4 - torch 1.1.0 - matplotlib 3.1.0 - unityagent 0.4.0   To try it yourself and see how wise you agent can be :)  you'll need to download a new Unity environment.  You need only select the environment that matches your operating system:  Linux: download here   Windows (32-bit): download here   Clone the repository  install the Unity Enviroment and start with ExperienceManager.py (update UNITY_ENVIROMENT before run)   """;Reinforcement Learning;https://github.com/pkasala/ContinuesControl
""" you need download pretrained chinese bert model  1. Download the Bert pretrained model from [s3](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin)  2. Download the Bert config file from [s3](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json)  3. Download the Bert vocab file from [s3](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt)  4. modify `bert-base-chinese-pytorch_model.bin` to `pytorch_model.bin`   `bert-base-chinese-config.json` to `config.json`  `bert-base-chinese-vocab.txt` to `vocab.txt` 5. place `model`  `config` and `vocab` file into  the `/pybert/pretrain/bert/base-uncased` directory. 2. `pip install pytorch-transformers` from [github](https://github.com/huggingface/pytorch-transformers). 4. Prepare [BaiduNet](https://pan.baidu.com/s/1Gn0rHHhrod6ed8LDTJ-rtA){password:ruxu}  you can modify the `io.bert_processor.py` to adapt your data. 5. Modify configuration information in `pybert/config/base.py`(the path of data ...). 6. Run `python run_bert.py --do_data` to preprocess data. 7. Run `python run_bert.py --do_train --save_best` to fine tuning bert model. 8. Run `run_bert.py --do_test --do_lower_case` to predict new data.   """;Natural Language Processing;https://github.com/lonePatient/BERT-chinese-text-classification-pytorch
"""Pose estimation find the keypoints belong to the people in the image. There are two methods exist for pose estimation.  * **Bottom-Up** first finds the keypoints and associates them into different people in the image. (Generally faster and lower accuracy) * **Top-Down** first detect people in the image and estimate the keypoints. (Generally computationally intensive but better accuracy)  This repo will only include top-down pose estimation models.     <summary><strong>COCO-test with 60.9 Detector AP</strong> (click to expand)</summary>   HRNet-w32 | [download][hrnetw32]  HRNet-w48 | [download][hrnetw48]   Download a YOLOv5m trained on CrowdHuman dataset from here. (The weights are from deepakcrk/yolov5-crowdhuman.)   Run the following command.   """;Computer Vision;https://github.com/sithu31296/pose-estimation
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   %<i></i> | ultralytics/yolov3 OR-NMS 5:52@416 (`pycocotools`) | darknet     ``` bash  git clone https://github.com/ultralytics/yolov3   git clone https://github.com/cocodataset/cocoapi && cd cocoapi/PythonAPI && make && cd ../.. && cp -r cocoapi/PythonAPI/pycocotools yolov3  cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/AndrewZhou924/YOLOv3_pytorch
"""Make sure your stable Rust and Cargo are ready  and follow cargo build to compile project. It's recommended to check out tch-rs' README to download pre-built libtorch binaries to speed up build process.   * CIFAR-10: Download binary version from [CIFAR site](https://www.cs.toronto.edu/~kriz/cifar.html)  and run:  ```sh cargo run -- --dataset-name cifar-10 --dataset-dir /path/to/cifar-10-dir ```  * MNIST: Download and unpack all gzips to a directory from [MNIST site](http://yann.lecun.com/exdb/mnist/)  and run:  ```sh cargo run -- --dataset-name mnist --dataset-dir /path/to/mnist-dir ```   Here is example usage. It's suggested to visit our source code to understand details.  ```rust // model init let mut vs = VarStore::new(Device::Cuda(0)); let root = vs.root(); let model = MobileNetV3::new(     &root / ""mobilenetv3""      input_channel      n_classes      dropout      width_mult   // usually 1.0     Mode::Large  )?; let opt = Adam::default().build(&vs  learning_rate)?;  // training let logits = model.forward_t(&images  true); let loss = prediction_logits.cross_entropy_for_logits(&labels); opt.backward_step(&loss); ```   """;Computer Vision;https://github.com/jerry73204/mobilenet-v3-rs
"""GPU:    """;General;https://github.com/tsc2017/Inception-Score
"""Download Pretrained Models   bash scripts/pretrain.sh &lt;gpu&gt; --moco --nce-k 16384   bash scripts/generate.sh 0 saved/Pretrain_moco_True_dgl_gin_layer_5_lr_0.005_decay_1e-05_bsz_32_hid_64_samples_2000_nce_t_0.07_nce_k_16384_rw_hops_256_restart_prob_0.8_aug_1st_ft_False_deg_16_pos_32_momentum_0.999/current.pth usa_airport kdd imdb-binary   bash scripts/finetune.sh &lt;load_path&gt; &lt;gpu&gt; usa_airport   bash scripts/generate.sh &lt;gpu&gt; &lt;load_path&gt; imdb-binary imdb-multi collab rdt-b rdt-5k   bash scripts/finetune.sh &lt;load_path&gt; &lt;gpu&gt; imdb-binary   Run GCC:  bash scripts/generate.sh &lt;gpu&gt; &lt;load_path&gt; kdd icdm sigir cikm sigmod icde   <!--  """;General;https://github.com/Kennard123661/gcc
"""    This program performs segmentation of the left vertricle  myocardium  right ventricle     and backgound of Cardiovascular Magnetic Resonance Images  with use of a convolutional neural network based on the well-known U-Net     architecture  as described by [https://arxiv.org/pdf/1505.04597.pdf](Ronneberger et al.) For each patient  both a 3D end systolic       image and a 3D end diastolic image with its corresponding ground truth segmentation of the left ventricle  myocardium and right         ventricle is available.           The available code first divides the patients data into a training set and a test set. The training data is then loaded from the        stored location and subsequently preprocessed. Preprocessing steps include resampling the image to the same voxel spacing  removal of outliers  normalization  cropping and one-hot encoding of the labels. Before training  the trainingset is subdivided again for training and validation of the model.          For training  a network based on the U-Net architecture is used and implemented with keras. For training  many different variables       can be tweaked  which are described in some detail below. After training  the network is evaluated using the test dataset. This data is loaded and preprocessed in the same way as the training dataset and propagated through the network to obtain pixel-wise predictions for each class. These predictions are probabilities and are thresholded to obtain a binary segmentation.           The binary segmentations are then evaluated by computing the (multiclass) softdice coefficient and the Hausdorff distance between the obtained segmentations and the ground truth segmentations. The softdice coefficients and Hausdorff distances are computed for each image for each individual class and the multiclass softdice for all the classes together. These results are all automatically saved in a text file. Furthermore  the obtained segmentations as an overlay with the original images  the training log and corresponding plots and the model summary are also saved automatically.         Lastly  from the segmentations of the left ventricular cavity during the end systole and end diastole  the ejection fraction is calculated. This value is  alongside the ejection fraction computed from the ground truth segmentations  stored in the same text file with results.           - glob2 0.6     - numpy 1.15.4     - matplotlib 3.0.1     - keras 2.2.4     - SimpleITK 1.2.0     - scipy 1.1.0   """;Computer Vision;https://github.com/jellevankerk/Team-Challenge
"""1.cityscapes    Register and download the dataset from the official [website](https://www.cityscapes-dataset.com/). Then decompress them into the `datasets/cityscapes` directory:   ``` $ mv /path/to/leftImg8bit_trainvaltest.zip datasets/cityscapes $ mv /path/to/gtFine_trainvaltest.zip datasets/cityscapes $ cd datasets/cityscapes $ unzip leftImg8bit_trainvaltest.zip $ unzip gtFine_trainvaltest.zip ```  2.cocostuff     Download `train2017.zip`  `val2017.zip` and `stuffthingmaps_trainval2017.zip` split from official [website](https://cocodataset.org/#download). Then do as following: ``` $ unzip train2017.zip $ unzip val2017.zip $ mv train2017/ /path/to/BiSeNet/datasets/coco/images $ mv val2017/ /path/to/BiSeNet/datasets/coco/images  $ unzip stuffthingmaps_trainval2017.zip $ mv train2017/ /path/to/BiSeNet/datasets/coco/labels $ mv val2017/ /path/to/BiSeNet/datasets/coco/labels  $ cd /path/to/BiSeNet $ python tools/gen_coco_annos.py ```  3.custom dataset    If you want to train on your own dataset  you should generate annotation files first with the format like this:  ``` munster_000002_000019_leftImg8bit.png munster_000002_000019_gtFine_labelIds.png frankfurt_000001_079206_leftImg8bit.png frankfurt_000001_079206_gtFine_labelIds.png ... ``` Each line is a pair of training sample and ground truth image path  which are separated by a single comma ` `.    Then you need to change the field of `im_root` and `train/val_im_anns` in the configuration files. If you found what shows in `cityscapes_cv2.py` is not clear  you can also see `coco.py`.    nvidia Tesla T4 gpu  driver 450.51.05   pytorch 1.8.1  I used the following command to train the models:   With a pretrained weight  you can run inference on an single image like this:   ``` $ python tools/demo.py --config configs/bisenetv2_city.py --weight-path /path/to/your/weights.pth --img-path ./example.png ```  This would run inference on the image and save the result image to `./res.jpg`.    Or you can run inference on a video like this:   ``` $ python tools/demo_video.py --config configs/bisenetv2_coco.py --weight-path res/model_final.pth --input ./video.mp4 --output res.mp4 ``` This would generate segmentation file as `res.mp4`. If you want to read from camera  you can set `--input camera_id` rather than `input ./video.mp4`.       """;Computer Vision;https://github.com/CoinCheung/BiSeNet
"""The experimental environment is a modified version of Waterworld based on [[https://github.com/sisl/MADRL][MADRL]].    =python==3.6.1= (recommend using the anaconda/miniconda)  if you need to render the environments  =opencv= is required  Install [[https://github.com/sisl/MADRL][MADRL]].  Replace the =madrl_environments/pursuit= directory with the one in this repo.   if scene rendering is enabled  recommend to install =opencv= through [[https://github.com/conda-forge/opencv-feedstock][conda-forge]].   """;Reinforcement Learning;https://github.com/xuehy/pytorch-maddpg
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/AlexeyAB/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN >= v7  CUDA >= 7.5   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 10.0) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 10.0"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 0 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox:      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/deepakHonakeri05/darknet
"""You need to:  Download/acquire the datsets   Dataset preparation instructions can be found here.   sh run_cityscapes_experiments.sh <run> <split_rng_seed>   sh run_cityscapes_experiments.sh 02 23456  sh run_cityscapes_experiments.sh 03 34567  sh run_cityscapes_experiments.sh 04 45678  sh run_cityscapes_experiments.sh 05 56789   sh run_pascal_aug_experiments.sh <n_supervised> <n_supervised_txt>   sh run_pascal_aug_deeplab3plus_experiments.sh <n_supervised> <n_supervised_txt>   sh run_isic2017_experiments.sh <run> <split_rng_seed>   sh run_isic2017_experiments.sh 02 23456  sh run_isic2017_experiments.sh 07 78901  sh run_isic2017_experiments.sh 08 89012  sh run_isic2017_experiments.sh 09 90123   Note that running the second notebook requires that you generate some data files using the   You can re-create the toy 2D experiments by running the run_toy2d_experiments.sh shell script:   sh run_toy2d_experiments.sh <run>   """;Computer Vision;https://github.com/Britefury/cutmix-semisup-seg
"""**Requirements/Dependencies**  - Linux or macOS - Python ≥ 3.6 - PyTorch ≥ 1.3 - [torchvision](https://github.com/pytorch/vision/) that matches the PyTorch installation. 	You can install them together at [pytorch.org](https://pytorch.org) to make sure of this.   Please ensure that your version of CUDA is also compatible when installing. - OpenCV `pip install opencv-python` - PyRealSense `pip install pyrealsense2` - Pycocotools: `pip install cython; pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'` - ROS Kinetic/Melodic - [Intel RealSense ROS wrapper](https://github.com/IntelRealSense/realsense-ros) - [Catkin](www.ros.org/wiki/catkin#Installing_catkin) - gcc & g++ ≥ 4.9  **Installation**  For the installation of Detectron2 and its dependencies  please refer to the [official Detectron2 GitHub](https://github.com/facebookresearch/detectron2)  **After Installation**  * Copy and paste ros_colour_node.py  sort.py and ros_numpy from this directory into your new Detectron2 directory. * [Create a catkin workspace](http://wiki.ros.org/catkin/Tutorials/create_a_workspace) and move your directory in the source folder * Ensure ros_colour_node.py is executable. Type `chmod +x ~/catkin_ws/src/ROS-label-node/ros_colour_node.py` * To perform instance segmentation straight from a D435 camera attached to a USB port:   * Type `roslaunch realsense2_camera rs_d400_and_t265.launch`   * In a new terminal window  type `rosrun ROS-label-node ros_colour_node.py`   * If there are any complications  make sure the topic this node is subscribed to has the same name. Type `rostopic list` to see current     topics published * If implementing this node with the OctoMap library:   * Type `roslaunch octomap_server octomap_mapping.launch` (Please ensure that the file rs_d400_and_t265.launch file is in the launch folder)   * In a new terminal window  type `rosrun ROS-label-node ros_colour_node.py`  * To find the published label mask  in a new terminal type `rostopic echo /label_mask`   """;Computer Vision;https://github.com/SfTI-Robotics/ROS-label-node
"""```python import torch from crossvit import CrossViT  img = torch.ones([1  3  224  224])      model = CrossViT(image_size = 224  channels = 3  num_classes = 100) out = model(img)  print(""Shape of out :""  out.shape)      #: [B  num_classes]   ```   """;General;https://github.com/rishikksh20/CrossViT-pytorch
"""* For Caffe model [Caffemodel](http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel) * For Mobilenet [Mobilenet](https://drive.google.com/open?id=0B3gersZ2cHIxRm5PMWRoTkdHdHc)   add models to models folder   """;Computer Vision;https://github.com/RashadGarayev/OpencvDnnmodule
"""Following the instructions in install  you could compile them by yourself. If you install tensorflow by pip  one potential error can be some source files of tensorflow set the wrong relative path of cuda.h  you just need to manually change them according to your cuda path.   cd standard_training/   cd standard_training/   cd progressive_training/   More video comparison  see the following youtube links:   """;Computer Vision;https://github.com/musikisomorphie/swd
""" - MLPMixer  <img width=""500"" alt=""스크린샷 2021-05-10 오후 10 13 36"" src=""https://user-images.githubusercontent.com/22078438/117664703-0c77d200-b1dd-11eb-9dcd-498c829520a7.png"">  - ResMLP  <img width=""500"" alt=""스크린샷 2021-05-10 오후 10 13 51"" src=""https://user-images.githubusercontent.com/22078438/117664706-0da8ff00-b1dd-11eb-9541-308e76680810.png"">  - gMLP  <img width=""500"" alt=""스크린샷 2021-05-10 오후 10 13 51"" src=""https://user-images.githubusercontent.com/22078438/120160982-d48b0a00-c231-11eb-8f13-39c4f3de3cf2.png"">    """;General;https://github.com/leaderj1001/Bag-of-MLP
"""make a local clone  make changes on the local copy   push to your GitHub account: git push origin   """;Computer Vision;https://github.com/samuelmat19/GLOW-tf2
"""Mixup is a generic and straightforward data augmentation principle. In essence  mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so  mixup regularizes the neural network to favor simple linear behavior in-between training examples.  This repository contains the implementation used for the results in our paper (https://arxiv.org/abs/1710.09412).   * A computer running macOS or Linux * For training new models  you'll also need a NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl) * Python version 3.6 * A [PyTorch installation](http://pytorch.org/)   """;Computer Vision;https://github.com/facebookresearch/mixup-cifar10
"""1. Encoder or contracting Path   3. Decoder or expansive path   - git clone this repo - download dataset from above link - check for requirements. - make sure all requirements are installed - open Detecting Brain Tumor Using MRI Scan.ipynb file - rerun all cells.  """;Computer Vision;https://github.com/Aryavir07/Detecting-Brain-Tumor-Using-Deep-Learning
"""Libraries you'll need to run the project:  {``tqdm``}  Clone the repo using  ```sh git clone https://github.com/valentinmace/noisy-text.git ```   Do not hesitate to contact me if you need some help  need a feature or see some bug   I've implemented the 3 noise functions described in the paper:  1. Delete words with given probability (default is 0.1) 2. Replace words by a filler token with given probability (default is 0.1) 3. Swap words up to a certain range (default range is 3)  The default parameters are to reproduce [Edunov et al. (2018)](https://arxiv.org/abs/1808.09381) experiments but you can play with them and maybe find better values  Example of simple usage ```sh python add_noise.py data/example --progress ``` Example of complete usage ```sh python add_noise.py data/example --delete_probability 0.9 --replace_probability 0.9  --filler_token 'MASK' --permutation_range 3 ```  **Important Note**  If you are using a subword tool such as SentencePiece after adding noise to your corpus  notice that your replacement token (which is ``'BLANK'`` by default) might be segmented into somthing like ``'▁B LAN K'``  I recommend to make a pass on your corpus to correct it: (adapt it to your token and segmentation) ```sh sed -i 's/▁B LAN K/▁BLANK/g' yourtextfile ```   """;Natural Language Processing;https://github.com/valentinmace/noisy-text
"""* **CrowdHuman** The CrowdHuman dataset can be downloaded from their [official webpage](https://www.crowdhuman.org). After downloading  you should prepare the data in the following structure: ``` crowdhuman    |——————images    |        └——————train    |        └——————val    └——————labels_with_ids    |         └——————train(empty)    |         └——————val(empty)    └------annotation_train.odgt    └------annotation_val.odgt ``` If you want to pretrain on CrowdHuman (we train Re-ID on CrowdHuman)  you can change the paths in src/gen_labels_crowd_id.py and run: ``` cd src python gen_labels_crowd_id.py ``` If you want to add CrowdHuman to the MIX dataset (we do not train Re-ID on CrowdHuman)  you can change the paths in src/gen_labels_crowd_det.py and run: ``` cd src python gen_labels_crowd_det.py ``` * **MIX** We use the same training data as [JDE](https://github.com/Zhongdao/Towards-Realtime-MOT) in this part and we call it ""MIX"". Please refer to their [DATA ZOO](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md) to download and prepare all the training data including Caltech Pedestrian  CityPersons  CUHK-SYSU  PRW  ETHZ  MOT17 and MOT16.  * **2DMOT15 and MOT20**  [2DMOT15](https://motchallenge.net/data/2D_MOT_2015/) and [MOT20](https://motchallenge.net/data/MOT20/) can be downloaded from the official webpage of MOT challenge. After downloading  you should prepare the data in the following structure: ``` MOT15    |——————images    |        └——————train    |        └——————test    └——————labels_with_ids             └——————train(empty) MOT20    |——————images    |        └——————train    |        └——————test    └——————labels_with_ids             └——————train(empty) ``` Then  you can change the seq_root and label_root in src/gen_labels_15.py and src/gen_labels_20.py and run: ``` cd src python gen_labels_15.py python gen_labels_20.py ``` to generate the labels of 2DMOT15 and MOT20. The seqinfo.ini files of 2DMOT15 can be downloaded here [[Google]](https://drive.google.com/open?id=1kJYySZy7wyETH4fKMzgJrYUrTfxKlN1w)  [[Baidu] code:8o0w](https://pan.baidu.com/s/1zb5tBW7-YTzWOXpd9IzS0g).   * Clone this repo  and we'll call the directory that you cloned as ${FAIRMOT_ROOT} * Install dependencies. We use python 3.8 and pytorch == 1.9.1 ``` conda create -n FairMOT conda activate FairMOT pip install torch==1.9.1+cu102 -f https://download.pytorch.org/whl/torch_stable.html pip install torchvision==0.10.1+cu102 -f https://download.pytorch.org/whl/torch_stable.html cd ${FAIRMOT_ROOT} pip install cython pip install -r requirements.txt ``` * We use [DCNv2_pytorch_1.9](https://github.com/lbin/DCNv2/tree/pytorch_1.9) in our backbone network (pytorch_1.9 branch). Previous versions can be found in [DCNv2](https://github.com/CharlesShang/DCNv2). ``` #: use yourself pytorch version git clone -b pytorch_1.9 git@github.com:lbin/DCNv2.git cd DCNv2 ./make.sh ``` * In order to run the code for demos  you also need to install [ffmpeg](https://www.ffmpeg.org/).   <img src=""assets/MOT15.gif"" width=""400""/>   <img src=""assets/MOT16.gif"" width=""400""/> <img src=""assets/MOT17.gif"" width=""400""/>   <img src=""assets/MOT20.gif"" width=""400""/>    | Dataset    |  MOTA | IDF1 | IDS | MT | ML | FPS | |--------------|-----------|--------|-------|----------|----------|--------| |2DMOT15  | 60.6 | 64.7 |  591 | 47.6% | 11.0% | 30.5 | |MOT16       | 74.9 | 72.8 | 1074 | 44.7% | 15.9% | 25.9 | |MOT17       | 73.7 | 72.3 | 3303 | 43.2% | 17.3% | 25.9 | |MOT20       | 61.8 | 67.3 | 5243 | 68.8% | 7.6% | 13.2 |   All of the results are obtained on the [MOT challenge](https://motchallenge.net) evaluation server under the “private detector” protocol. We rank first among all the trackers on 2DMOT15  MOT16  MOT17 and  MOT20. The tracking speed of the entire system can reach up to **30 FPS**.   sh experiments/crowdhuman_dla34.sh  sh experiments/mix_ft_ch_dla34.sh   sh experiments/mix_dla34.sh   sh experiments/mot17_dla34.sh   sh experiments/mot15_ft_mix_dla34.sh   sh experiments/crowdhuman_dla34.sh  sh experiments/mix_ft_ch_dla34.sh  sh experiments/mot20_ft_mix_dla34.sh   sh experiments/mix_mot17_half_dla34.sh  sh experiments/mix_mot17_half_hrnet18.sh  sh experiments/mix_mot17_half_res34.sh  sh experiments/mix_mot17_half_res34fpn.sh  sh experiments/mix_mot17_half_res50.sh   sh experiments/all_yolov5s.sh   The default settings run tracking on the validation dataset from 2DMOT15. Using the baseline model  you can run:  cd src   cd src   cd src   To run tracking using the light version of FairMOT (68.5 MOTA on the test of MOT17)  you can run:  cd src   cd src   You can train FairMOT on custom dataset by following several steps bellow:   <img src=""assets/MOT15.gif"" width=""400""/>   <img src=""assets/MOT16.gif"" width=""400""/> <img src=""assets/MOT17.gif"" width=""400""/>   <img src=""assets/MOT20.gif"" width=""400""/>    You can input a raw video and get the demo video by running src/demo.py and get the mp4 format of the demo video: ``` cd src python demo.py mot --load_model ../models/fairmot_dla34.pth --conf_thres 0.4 ``` You can change --input-video and --output-root to get the demos of your own videos. --conf_thres can be set from 0.3 to 0.7 depending on your own videos.   """;Computer Vision;https://github.com/nemonameless/fairmot
"""• https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263   • https://www.kaggle.com/residentmario/seq-to-seq-rnn-models-attention-teacherforcing/notebook   • https://machinelearningmastery.com/develop-word-embeddings-python-gensim/   • https://github.com/fxsjy/jieba  • Source Chinese Embedding: https://github.com/Kyubyong/wordvectors   """;General;https://github.com/IpastorSan/seq2seq-with-attention-OCR-translation
"""- Python 3.8 - Pytorch 1.6.0 - opencv-python 4.4.0.42   Within ./Landmark-Driven-Facial-Expression-Recognition directory  run following command:   bash      bash train.sh   **Clone this repository:** ```bash git clone https://github.com/RainbowRui/Landmark-Driven-Facial-Expression-Recognition.git cd Landmark-Driven-Facial-Expression-Recognition ``` **Install dependencies using Anaconda:**  ```bash conda create -n fer python=3.8 source activate fer pip install opencv-python==4.4.0.42 pillow==7.2.0 conda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=10.2 -c pytorch ```  Note: the version of 'cudatoolkit' must match the running version of your machine.   """;General;https://github.com/RainbowRui/Landmark-Driven-Facial-Expression-Recognition
"""First we need a tool to encode a surface weather field   (2m air temperature anomaly  mean-sea-level-pressure  and precipitation rate)   as an image. [Script](./weather2image//make.3var.plot.R)  Then we need a set of pairs of such images - a source image  and a target image from 6-hours later. Each pair should be separated by at least 5 days  so they are independent states. [Script](./weather2image//make.training.batch.R)  Then we need to take a training set (400) of those pairs of images and pack them into the 512x256 side-by-side format used by pix2pix (source in the left half  and target in the right half). [Script](./weather2image/make_p2p_training_images.R)  Alternatively  you can get the set of training and test images I used from [Dropbox](https://www.dropbox.com/s/0knxkll2btjjnyl/weather2weather_imgs.tar).  Then train a model on this set for 200 epochs - with a fast GPU this should take about 1 hour  but  CPU-only  it takes a bit over 24 hours on my 4-core iMac. (It took about 2 hours on one gpu-node of [Isambard](http://gw4.ac.uk/isambard/)).  ```sh python weather2weather.py \   --mode train \   --output_dir $SCRATCH/weather2weather/model_train \   --max_epochs 200 \   --input_dir $SCRATCH/weather2weather/p2p_format_images_for_training \   --which_direction AtoB ``` Now make some more pairs of images (100) to test the model on - same format as the training set  but must be different weather states (times). [Script](./weather2image/make_p2p_validation_images.R)  Use the trained model to make predictions from the validation set sources and compare those predictions to the validation set targets.  ```sh python weather2weather.py \   --mode test \   --output_dir $SCRATCH/weather2weather/model_test \   --input_dir $SCRATCH/weather2weather/p2p_format_images_for_validation \   --checkpoint $SCRATCH/weather2weather/model_train ```  The test run will output an HTML file at `$SCRATCH/weather2weather/model_test/index.html` that shows input/output/target image sets. This is good for a first glance  but those images are in a packed analysis form. So we need a tool to convert the packed image pairs to a clearer image format: [Script](./weather2image/replot.p2p.image.R). This shows target weather (top left)  model output weather (top right)  target pressure increment (bottom left)  and model output pressure increment (bottom right).  To postprocess all the test cases run: ```sh ./weather2image/replot_all_validation.R \   --input.dir=$SCRATCH/weather2weather/model_test/images \   --output.dir=$SCRATCH/weather2weather/model_test/postprocessed ```  This will produce an HTML file at  `$SCRATCH/weather2weather/model_test/index.html` showing results of all the test cases.  This clearly does have skill at 6-hour weather forecasts - it gets the semi-diurnal oscillation  and some of the extratropical structure. The final step is to use the model on it's own output - by making repeated 6-hour forecasts we can make a forecast as far into the future as we like. [This is less successful](https://vimeo.com/275778137).   """;Computer Vision;https://github.com/philip-brohan/weather2weather
"""conda env create -f conda-env/requirements.yml   """;Computer Vision;https://github.com/mweiss17/SEVN-data
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/zanmange/darknet
"""All you need to use centermask2 is [detectron2](https://github.com/facebookresearch/detectron2). It's easy!     you just install [detectron2](https://github.com/facebookresearch/detectron2) following [INSTALL.md](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md).    Prepare for coco dataset following [this instruction](https://github.com/facebookresearch/detectron2/tree/master/datasets).   cd centermask2   one should execute:  cd centermask2   cd centermask2   """;Computer Vision;https://github.com/mahdi-darvish/centermask
"""  """;Natural Language Processing;https://github.com/zhangbo2008/best_pytorch_transformer
"""We provide pretrained checkpoints run.zip. Extract the file to the root folder. You should be able to produce samples like the following using this checkpoint.   """;Computer Vision;https://github.com/voxmenthe/ncsn_1
"""All exported names from the Distributions.jl package are reexported from Bijectors.   julia> using Bijectors   julia> x == z   julia> using Bijectors   julia> b⁻¹ = inv(b)   julia> b⁻¹(y)   julia> Bijectors.dimension(b)   julia> id_y = (b ∘ b⁻¹)   julia> id_y(y) ≈ y   julia> id_x = inv(id_y)   julia> id_x(x) ≈ x   julia> using Bijectors: TransformedDistribution   julia> td isa UnivariateDistribution   julia> logpdf(td  y)   julia> logabsdetjac(b⁻¹  y)   julia> logabsdetjac(b  x)   julia> y = rand(td)              #: ∈ ℝ   julia> b = PlanarLayer(2)   That's it. Now we can sample from it using rand and compute the logpdf  like any other Distribution.   julia> b = sb ∘ PlanarLayer(2)   julia> y = rand(td)   julia> 0 < y[1]   julia> 0 ≤ y[2] ≤ 1   julia> using Tracker   julia> Tracker.grad(b.w)   julia> using Flux  julia> @Flux.treelike Composed  julia> @Flux.treelike TransformedDistribution  julia> @Flux.treelike PlanarLayer   There's mainly two ways you can implement your own Bijector  and which way you choose mainly depends on the following question: are you bothered enough to manually implement logabsdetjac? If the answer is ""Yup!""  then you subtype from Bijector  if ""Naaaah"" then you subtype ADBijector.   julia> b = Logit(0.0  1.0)   julia> b(0.6)   julia> inv(b)(y)   julia> logabsdetjac(b  0.6)   As you can see it's a very contrived example  but you get the idea.   julia> b_ad = ADLogit(0.0  1.0)   julia> logabsdetjac(b_ad  0.6)   julia> y = b_ad(0.6)   julia> inv(b_ad)(y)   julia> logabsdetjac(inv(b_ad)  y)   julia> b = Logit(0.0  1.0)   julia> logabsdetjac(b  0.6)   julia> logabsdetjac(b_ad  0.6) ≈ logabsdetjac(b  0.6)   julia> Bijectors.setadbackend(:reverse_diff)   julia> b_ad = ADLogit(0.0  1.0)   julia> logabsdetjac(b_ad  0.6)   The following are the bijectors available:   Other than the `logpdf_with_trans` methods  the package also provides a more composable interface through the `Bijector` types. Consider for example the one from above with `Beta(2  2)`.  ```julia julia> using Random; Random.seed!(42);  julia> using Bijectors; using Bijectors: Logit  julia> dist = Beta(2  2) Beta{Float64}(α=2.0  β=2.0)  julia> x = rand(dist) 0.36888689965963756  julia> b = bijector(dist) #: bijection (0  1) → ℝ Logit{Float64}(0.0  1.0)  julia> y = b(x) -0.5369949942509267 ```  In this case we see that `bijector(d::Distribution)` returns the corresponding constrained-to-unconstrained bijection for `Beta`  which indeed is a `Logit` with `a = 0.0` and `b = 1.0`. The resulting `Logit <: Bijector` has a method `(b::Logit)(x)` defined  allowing us to call it just like any other function. Comparing with the above example  `b(x) == link(dist  x)`. Just to convince ourselves:  ```julia julia> b(x) == link(dist  x) true ```   But the real utility of `TransformedDistribution` becomes more apparent when using `transformed(dist  b)` for any bijector `b`. To get the transformed distribution corresponding to the `Beta(2  2)`  we called `transformed(dist)` before. This is simply an alias for `transformed(dist  bijector(dist))`. Remember `bijector(dist)` returns the constrained-to-constrained bijector for that particular `Distribution`. But we can of course construct a `TransformedDistribution` using different bijectors with the same `dist`. This is particularly useful in something called _Automatic Differentiation Variational Inference (ADVI)_.[2] An important part of ADVI is to approximate a constrained distribution  e.g. `Beta`  as follows: 1. Sample `x` from a `Normal` with parameters `μ` and `σ`  i.e. `x ~ Normal(μ  σ)`. 2. Transform `x` to `y` s.t. `y ∈ support(Beta)`  with the transform being a differentiable bijection with a differentiable inverse (a ""bijector"")  This then defines a probability density with same _support_ as `Beta`! Of course  it's unlikely that it will be the same density  but it's an _approximation_. Creating such a distribution becomes trivial with `Bijector` and `TransformedDistribution`:  ```julia julia> dist = Beta(2  2) Beta{Float64}(α=2.0  β=2.0)  julia> b = bijector(dist)              #: (0  1) → ℝ Logit{Float64}(0.0  1.0)  julia> b⁻¹ = inv(b)                    #: ℝ → (0  1) Inversed{Logit{Float64} 0}(Logit{Float64}(0.0  1.0))  julia> td = transformed(Normal()  b⁻¹) #: x ∼ 𝓝(0  1) then b(x) ∈ (0  1) TransformedDistribution{Normal{Float64} Inversed{Logit{Float64} 0} Univariate}( dist: Normal{Float64}(μ=0.0  σ=1.0) transform: Inversed{Logit{Float64} 0}(Logit{Float64}(0.0  1.0)) )   julia> x = rand(td)                    #: ∈ (0  1) 0.538956748141868 ```  It's worth noting that `support(Beta)` is the _closed_ interval `[0  1]`  while the constrained-to-unconstrained bijection  `Logit` in this case  is only well-defined as a map `(0  1) → ℝ` for the _open_ interval `(0  1)`. This is of course not an implementation detail. `ℝ` is itself open  thus no continuous bijection exists from a _closed_ interval to `ℝ`. But since the boundaries of a closed interval has what's known as measure zero  this doesn't end up affecting the resulting density with support on the entire real line. In practice  this means that  ```julia td = transformed(Beta())  inv(td.transform)(rand(td)) ```  will never result in `0` or `1` though any sample arbitrarily close to either `0` or `1` is possible. _Disclaimer: numerical accuracy is limited  so you might still see `0` and `1` if you're lucky._   We can also do _multivariate_ ADVI using the `Stacked` bijector. `Stacked` gives us a way to combine univariate and/or multivariate bijectors into a singe multivariate bijector. Say you have a vector `x` of length 2 and you want to transform the first entry using `Exp` and the second entry using `Log`. `Stacked` gives you an easy and efficient way of representing such a bijector.  ```julia julia> Random.seed!(42);  julia> using Bijectors: Exp  Log  SimplexBijector  julia> #: Original distributions        dists = (            Beta()             InverseGamma()             Dirichlet(2  3)        );  julia> #: Construct the corresponding ranges        ranges = [];  julia> idx = 1;  julia> for i = 1:length(dists)            d = dists[i]            push!(ranges  idx:idx + length(d) - 1)             global idx            idx += length(d)        end;  julia> ranges 3-element Array{Any 1}:  1:1  2:2  3:4  julia> #: Base distribution; mean-field normal        num_params = ranges[end][end] 4  julia> d = MvNormal(zeros(num_params)  ones(num_params)) DiagNormal( dim: 4 μ: [0.0  0.0  0.0  0.0] Σ: [1.0 0.0 0.0 0.0; 0.0 1.0 0.0 0.0; 0.0 0.0 1.0 0.0; 0.0 0.0 0.0 1.0] )   julia> #: Construct the transform        bs = bijector.(dists)     #: constrained-to-unconstrained bijectors for dists (Logit{Float64}(0.0  1.0)  Log{0}()  SimplexBijector{Val{true}}())  julia> ibs = inv.(bs)            #: invert  so we get unconstrained-to-constrained (Inversed{Logit{Float64} 0}(Logit{Float64}(0.0  1.0))  Exp{0}()  Inversed{SimplexBijector{Val{true}} 1}(SimplexBijector{Val{true}}()))  julia> sb = Stacked(ibs  ranges) #: => Stacked <: Bijector Stacked{Tuple{Inversed{Logit{Float64} 0} Exp{0} Inversed{SimplexBijector{Val{true}} 1}} 3}((Inversed{Logit{Float64} 0}(Logit{Float64}(0.0  1.0))  Exp{0}()  Inversed{SimplexBijector{Val{true}} 1}(SimplexBijector{Val{true}}()))  (1:1  2:2  3:4))  julia> #: Mean-field normal with unconstrained-to-constrained stacked bijector        td = transformed(d  sb);  julia> y = rand(td) 4-element Array{Float64 1}:  0.36446726136766217  0.6412195576273355   0.5067884173521743   0.4932115826478257   julia> 0.0 ≤ y[1] ≤ 1.0   #: => true true  julia> 0.0 < y[2]         #: => true true  julia> sum(y[3:4]) ≈ 1.0  #: => true true ```   """;General;https://github.com/UnofficialJuliaMirrorSnapshots/Bijectors.jl-76274a88-744f-5084-9051-94815aaf08c4
"""- Clone the repository in your GitHub account [Clone with HTTPS](https://github.com/harpreetsodhi/ChangeMyPet_Deep_Learning_Model.git) - To run the code  please download the pretrained pytorch weights first. [Pretrained Weights](https://github.com/ivclab/BigGAN-Generator-Pretrained-Pytorch/releases/tag/v0.0.0) ```shell     biggan256-release.pt    #: download this for generating 256*256 images ``` - Upload the biggan256-release.pt file to your google drive. - Open Main.ipynb file in Google Colab or your Jupyter Notebook and run it. Comments are added to the file as needed.   """;General;https://github.com/harpreetsodhi/ChangeMyPet_Deep_Learning_Model
"""To read Motivation and Goal behind this research go to this [Github Repository](https://github.com/ieee8023/covid-chestxray-dataset/blob/master/README.md).  ``` $python3 detect.py --help usage: detect.py [-h] -m MODEL -i IMAGE  optional arguments:   -h  --help            show this help message and exit   -m MODEL  --model MODEL                         path to model   -i IMAGE  --image IMAGE                         path to input image  ``` Example ``` python3 detect.py -m model/trained_model/covid-19_large.h5 -i model/samples/normal.jpeg  ```    """;General;https://github.com/Thehunk1206/Covid-19-covidcnn
"""ULMFiT can be peretrained on relativly small datasets - 100 million tokens are sufficient to get state-of-the art classification results (compared to Transformer models as BERT  which need huge amounts of training data). The easiest way is to pretrain a language model on Wikipedia.  The code for the preperation steps is heavily inspired by / copied from the **fast.ai NLP-course**: https://github.com/fastai/course-nlp/blob/master/nlputils.py  I built a docker container and script  that automates the following steps: 1) Download Wikipedia XML-dump 2) Extract the text from the dump 3) Sample 160.000 documents with a minimum length of 1800 characters (results in 100m-120m tokens) both parameters can be changed - see the usage below  The whole process will take some time depending on the download speed and your hardware. For the 'dewiki' the preperation took about 45 min.  Run the following commands in the current directory ``` #: build the wikiextractor docker file docker build -t wikiextractor ./we  #: run the docker container for a specific language #: docker run -v $(pwd)/data:/data -it wikiextractor -l <language-code>  #: for German language-code de run: docker run -v $(pwd)/data:/data -it wikiextractor -l de ... sucessfully prepared dewiki - /data/dewiki/docs/sampled  number of docs 160000/160000 with 110699119 words / tokens!  #: To change the number of sampled documents or the minimum length see usage: preprocess.py [-h] -l LANG [-n NUMBER_DOCS] [-m MIN_DOC_LENGTH] [--mirror MIRROR] [--cleanup]  #: To cleanup indermediate files (wikiextractor and all splitted documents) run the following command.  #: The Wikipedia-XML-Dump and the sampled docs will not be deleted! docker run -v $(pwd)/data:/data -it wikiextractor -l <language-code> --cleanup ```   - https://github.com/fastai/fastai   Install packages  pip install -r requirements.txt   Dataset https://github.com/benjaminvdb/DBRD   - https://github.com/laboroai/Laboro-BERT-Japanese  - https://github.com/yoheikikuta/bert-japanese     Dataset: https://github.com/e9t/nsmc   I've written a small library around this repo  to easily use the pretrained models. You don't have to bother with model  vocab and tokenizer files and paths - the following functions will take care of that.   Tutorial:  [fastai_ulmfit_pretrained_usage.ipynb](https://github.com/floleuerer/fastai_ulmfit/blob/main/fastai_ulmfit_pretrained_usage.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/floleuerer/fastai_ulmfit/blob/main/fastai_ulmfit_pretrained_usage.ipynb)   **Installation** ```` pip install fastai-ulmfit ````  **Usage**  ``` #: import from fastai_ulmfit.pretrained import *  url = 'http://bit.ly/ulmfit-dewiki'  #: get tokenizer - if pretrained=True  the SentencePiece Model used for language model pretraining will be used. Default: False  tok = tokenizer_from_pretrained(url  pretrained=False)  #: get language model learner for fine-tuning learn = language_model_from_pretrained(dls  url=url  drop_mult=0.5).to_fp16()  #: save fine-tuned model for classification path = learn.save_lm('tmp/test_lm')  #: get text classifier learner from fine-tuned model learn = text_classifier_from_lm(dls  path=path  metrics=[accuracy]).to_fp16() ````   Notebook: `5_ulmfit_inference.ipynb`   """;Sequential;https://github.com/floleuerer/fastai_ulmfit
"""1. Prepare Cityscapes evaluation scripts.  ``` bash scripts/download_cityscapescripts.sh ``` 2. Eval ``` bash scripts/eval.sh ```   1. Download Cityscapes data (gtFine_trainvaltest.zip  leftImg8bit_trainvaltest.zip). Extract them into 'data/cityscape/'.  The folder structure would then look as shown below:  ``` data/cityscape/ ├── leftImg8bit/ │   ├── train/ │   ├── val/ │   └── test/ ├── gtFine/ │   ├── train/ │   ├── val/ │   └── test/ └── imglists/     ├── train.lst     ├── val.lst     └── test.lst ```   2. Download Resnet-50 pretrained model. ``` bash scripts/download_res50.sh  ```  3. Build MXNet with ROIAlign operator.  ``` cp rcnn/CXX_OP/* incubator-mxnet/src/operator/ ```  To build MXNet from source  please refer to the [tutorial](https://mxnet.incubator.apache.org/get_started/build_from_source.html).  4. Build related cython code.  ``` make ```  5. Kick off training  ``` bash scripts/train_alternate.sh ```   1. Download model  available at [Dropbox](https://www.dropbox.com/s/zidcbbt7apwg3z6/final-0000.params?dl=0)/[BaiduYun](https://pan.baidu.com/s/1o8n4VMU)  and place it in the model folder.  2. Make sure that you have the cityscapes data in 'data/cityscapes' folder. ``` bash scripts/demo.sh ```   """;Computer Vision;https://github.com/TuSimple/mx-maskrcnn
"""NOTE: We do NOT generate the whole LSTM/Bi-LSTM architecture using Pytorch. Instead  we just use           self.name = name     The principle says: ""Everyone must be able to run everything by one click!"". So you see pretty much everything in one   the previously processed sequence. It can be seen in the following Python script:   Output:  i m glad i invited you  EOS   """;Natural Language Processing;https://github.com/astorfi/sequence-to-sequence-from-scratch
"""Doing Spation_Deform on GPU instead of CPU  greatly saving CPU resources.   Users can fetch coordinates from CUDA and do cubic interpolation at CPU by scipy.map_coordinates(order = 3)   cd cuda_backend  cmake -D CUDA_TOOLKIT_ROOT_DIR=/path/to/cuda .  make -j8       id_gpu: choose the number of GPU   """;Computer Vision;https://github.com/qsyao/cuda_spatial_deform
"""Make sure these packages are installed: - PyTorch 0.4 - Torchvision  - NumPy  Packages can be installed via PyPi package repository or Anaconda.   Notice that you will need to download the dataset mentioned before to proceed for training part. You will also need to train network to use it.  Please follow the Jupyter Notebook file for more information.  Important notice: This GitHub repository includes files for education purposes only. These files should not use for a commercial usage.  """;Computer Vision;https://github.com/iamkucuk/DCGAN-Face-Generation
"""Please run pip install . in order to ensure you got all dependencies needed  To start up the project: python -m train.py   All hyper-paramters are in:  config.py   It includes PLAY_ONLY argument which decides whether to start Agent with pre-trained weights or spend a few hours and train it from scratch :)   More details on the project can be found in:   [Report](/Report.md)     You don’t have to build the environment yourself the prebuilt one included in the project will work fine - please note it’s only compatible with Unity-ML 0.4.0b NOT the current newest version. I don’t have access to the source of the environment as it was prebuilt by Udacity.   """;Reinforcement Learning;https://github.com/jsztompka/DuelDQN
"""Download  the flower datasets from below kaggle link.   """;General;https://github.com/prasadji/Flower-Classifaction-with-Fine-Tuned-Mobilenet
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   I hope you can see this. If you follow the instructions I sent on Slack  this should get the job done. The trained model is already saved in the /models/ directory on hutchentoot  so you don't need to mess with that. If anything throws an error/needs clarifying  just send me a message!    Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/pgtinsley/insightface_oge
"""Version: Tensorflow 1.5   The above methods can be defined in the run.sh bash script with the proper parameters set. One may also can add cuda visible devices to run on gpu.   """;General;https://github.com/zoli333/Weight-Normalization
"""| Installation   Disambiguation: if you are looking for Haiku the operating system then   ""impure"" modules into pure functions that can be used with jax.jit    Because JAX installation is different depending on your CUDA version  Haiku does   to install JAX with the relevant accelerator support.  Then  install Haiku using pip:  $ pip install git+https://github.com/deepmind/dm-haiku  Alternatively  you can install via PyPI:  $ pip install -U dm-haiku  Our examples rely on additional libraries (e.g. bsuite). You can install the full set of additional requirements using pip:  $ pip install -r examples/requirements.txt   becomes my_linear). Modules can have named parameters that are accessed     #: Update parameters using SGD or Adam or ...   """;General;https://github.com/deepmind/dm-haiku
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.  git clone https://github.com/affinelayer/pix2pix-tensorflow.git  cd pix2pix-tensorflow   ```sh #: clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git cd pix2pix-tensorflow #: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir facades_train \   --max_epochs 200 \   --input_dir facades/train \   --which_direction BtoA #: test the model python pix2pix.py \   --mode test \   --output_dir facades_test \   --input_dir facades/val \   --checkpoint facades_train ```  The test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.  If you have Docker installed  you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:  ```sh #: train the model python tools/dockrun.py python pix2pix.py \       --mode train \       --output_dir facades_train \       --max_epochs 200 \       --input_dir facades/train \       --which_direction BtoA #: test the model python tools/dockrun.py python pix2pix.py \       --mode test \       --output_dir facades_test \       --input_dir facades/val \       --checkpoint facades_train ```   <img src=""docs/combine.png"" width=""900px""/>  ```sh #: Resize source images python tools/process.py \   --input_dir photos/original \   --operation resize \   --output_dir photos/resized #: Create images with blank centers python tools/process.py \   --input_dir photos/resized \   --operation blank \   --output_dir photos/blank #: Combine resized images with blanked images python tools/process.py \   --input_dir photos/resized \   --b_dir photos/blank \   --operation combine \   --output_dir photos/combined #: Split into train/val set python tools/split.py \   --dir photos/combined ```  The folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.   """;General;https://github.com/rdcarter1994/gitgan
"""cd Handwritten-Character-Recognition<br/>   cd Handwritten-Character-Recognition<br/>   """;General;https://github.com/Shantanu48114860/Handwritten-Character-Recognition
"""The warmup factor `w(t)` depends on the warmup period  which must manually be specified  for `LinearWarmup` and `ExponentialWarmup`.   Make sure you have Python 3.6+ and PyTorch 1.1+. Then  run the following command:  ``` python setup.py install ```  or  ``` pip install -U pytorch_warmup ```   For PyTorch 1.4 or above  use an LR scheduler as the following:   """;General;https://github.com/Tony-Y/pytorch_warmup
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/lippman1125/maskrcnn_benchmark_mobilenetv2
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/ZhichengHuang/Food-Project
"""- Revised the first four modules of Intro to DL with PyTorch as I need to help on PyTorch Beast Weak.   Working on the Thank you Initiative for udacity-facebook. Would love to see everyone's participation. ^_^ Here: https://secureprivataischolar.slack.com/archives/CJSCX4WAZ/p1566660348242000   """;General;https://github.com/rupaai/60DaysOfUdacity
"""How can we use the causal knowledge obtained by causal discovery to improve machine learning? Are causal discovery methods beneficial for machine learning tasks?  * **Causal mechanism transfer** <sup>[1](#references)</sup>   Domain adaptation among systems sharing the same causal mechanism   can be performed by estimating the structural equations (reduced-form equations; estimated by nonlinear ICA).    <div align=""center"">   <img src=""./docs_src/figs/schema_problem_setup.png"" alt=""Problem setup schema"" width=""50%""/>   </div>    * Theoretically well-grounded domain adaptation (generalization error bound without the partial-distribution-shift assumption).   * Intuitively accessible transfer assumption: if the data come from the same causal mechanism  information can be transferred.   * Method to directly leverage the estimated structural equations: via data augmentation.   ```bash $ pip install git+https://github.com/takeshi-teshima/few-shot-domain-adaptation-by-causal-mechanism-transfer  $ pip install -r experiments/icml2020/requirements.txt  #: To reproduce the experiments of our ICML2020 paper: $ pip install -r experiments/icml2020/requirements.txt ```  OR clone this repository and run ```bash $ pip install . ``` and the package will be installed under the name of `causal-da` (the module name will be `causal_da`).    In case you encounter any issues regarding the requirements     a full list of the installed packages in a working environment can be found at requirements-full.txt (the output of $ pip freeze).   [Documentation](https://takeshi-teshima.github.io/few-shot-domain-adaptation-by-causal-mechanism-transfer/index.html)   """;General;https://github.com/takeshi-teshima/few-shot-domain-adaptation-by-causal-mechanism-transfer
"""see notebook in nbviewer at: https://nbviewer.jupyter.org/github/g-eklund/bert_tutorial/blob/master/sentence_embedding_from_start_to_bert.ipynb   """;Natural Language Processing;https://github.com/g-eklund/bert_tutorial
"""You can find more detailed installation guides from the Fairseq repo: https://github.com/pytorch/fairseq  **1. Fairseq Installation**  Reference: [Fairseq](https://github.com/pytorch/fairseq) * [PyTorch](http://pytorch.org/) version >= 1.4.0 * Python version >= 3.6 * Currently  I-BERT only supports training on GPU  ```bash git clone https://github.com/kssteven418/I-BERT.git cd I-BERT pip install --editable ./ ```  **2. Download pre-trained RoBERTa models**  Reference: [Fairseq RoBERTa](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md)  Download pretrained RoBERTa models from the links and unzip them. * RoBERTa-Base: [roberta.base.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz) * RoBERTa-Large: [roberta.large.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz) ```bash #: In I-BERT (root) directory mkdir models && cd models wget {link} tar -xvf roberta.{base|large}.tar.gz ```   **3. Download GLUE datasets**  Reference: [Fairseq Finetuning on GLUE](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md)  First  download the data from the [GLUE website](https://gluebenchmark.com/tasks). Make sure to download the dataset in I-BERT (root) directory. ```bash #: In I-BERT (root) directory wget https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py python download_glue_data.py --data_dir glue_data --tasks all ```  Then  preprocess the data.   ```bash #: In I-BERT (root) directory ./examples/roberta/preprocess_GLUE_tasks.sh glue_data {task_name} ``` `task_name` can be one of the following: `{ALL  QQP  MNLI  QNLI  MRPC  RTE  STS-B  SST-2  CoLA}` . `ALL` will preprocess all the tasks. If the command is run propely  preprocessed datasets will be stored in `I-BERT/{task_name}-bin`  Now  you have the models and the datasets ready  so you are ready to run I-BERT!    Github Link: https://github.com/huggingface/transformers/tree/master/src/transformers/models/ibert   If you already have finetuned models  you can skip this part.   git fetch  git checkout -t origin/ibert-base   Then  run the script:   By default  models are trained according to the task-specific hyperparameters specified in [Fairseq Finetuning on GLUE](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md). However  you can also specify the hyperparameters with the options (use the option-h` for more details).   git checkout ibert   """;Natural Language Processing;https://github.com/kssteven418/I-BERT
"""Extract the place names into csv using your favourite xml parser or use the code below or run bash extract.sh if on a mac or linux system   The column header shows the seed for that column. If the header contains the empty string """" it means the model must produce the first character in the place name. Note that the model is casing aware.  |   """"   |    """"    |    """"    |    """"    | Vestr    | Østre -------- | -------- | -------- | -------- | -------- | -------- Fortenvika | Storfjellet | Hestberget | Salponøyvågen | Vestre Kjollen | Østren Koløya | Gravdådalen | Sørhaugen | Tømmelibrua | Vestrane | Østre Varde Sneveaflua | Sørneebotn | Flaten | Hårheim | Vestre Grønnholmen | Østre Kvernhaugen Vagemyrhaugen | Orrerdalen | Vifjellskjærberget | Stormoen | Vestre Ganegrunnanturveg | Østre Sag Steina | Medagen | Har-buholmen | Risa | Vestre nortelveien | Østrendgurd Osen | Husvatnet | Nybakken | Nysnø | Vestre Haugen | Østre Øvrengetan Borgita | Svartbakken | Heithaugen | Skáiuhelelen | Vestre Støle | Østredalskjær Øvre høgda | Mábbetn  bua | Storengard | Steindalsheia | Vestre Fryvassbruneset | Østre Løkstad Merkskardet | Skrud | Stordre Lodgegjer tøm | Storoialva | Vestre Hifjell | Østre Veslen Gvapeskádjávri | Beiseberget | Austre Reaneset | Haugen | Vestre Sørestøya | Østredal   """;Natural Language Processing;https://github.com/paulskeie/stadnamn
"""release of this package.) Requires python >= 2.7  cython  numpy  scipy.   """;General;https://github.com/dhruvdcoder/sparse-structured-attention
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/witwickey/alexeyDarknet
"""Intel Movidius NCS - https://developer.movidius.com/    """;Computer Vision;https://github.com/ac-optimus/ESDC_IntelCup2018
"""```bash #: create a new environment: python3 -m venv env               #: Create a virtual environment source env/bin/activate           #: Activate virtual environment  #: step 1: install COCO API: #: Note: COCO API requires numpy to install. Ensure that you have numpy installed. #: e.g. pip install numpy pip install 'git+https://github.com/cocodataset/cocoapi.git#:subdirectory=PythonAPI'  #: step 2: install Fashionpedia API via pip pip install fashionpedia   #: step 3: test. You can test that you have correctly installed the fashionpedia api #:       by running the following command inside the repo. python test.py  #: do your work ...  deactivate  #: exit ```   Clone the repo first and then do the following steps inside the repo:   python3 -m venv env               #: Create a virtual environment  source env/bin/activate           #: Activate virtual environment  : step 1: install COCO API:  : Note: COCO API requires numpy to install. Ensure that you have numpy installed.  : e.g. pip install numpy  pip install 'git+https://github.com/cocodataset/cocoapi.git#:subdirectory=PythonAPI'  : step 2: install required packages  pip install -r requirements.txt  : step 3: test. You can test that you have correctly installed the fashionpedia api  :       by running the following command inside the repo.   For the task of instance segmentation with attribute localization  we present a strong baseline model named Attribute-Mask R-CNN that is built upon [Mask R-CNN](<https://arxiv.org/abs/1703.06870>) for Fashionpedia. Check out our [predictior demo](https://github.com/KMnP/fashionpedia-api/blob/master/baseline_predictor_demo.ipynb) and [paper](<https://arxiv.org/abs/2004.12276>) for more details.   ![baseline](images/baseline.png)     """;Computer Vision;https://github.com/KMnP/fashionpedia-api
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/Satan012/BERT
"""- python 3.7   """;General;https://github.com/mdaniluk/language-detector
"""<a href='g3doc/installation.md'>Installation.</a><br>   To get help with issues you may encounter while using the DeepLab Tensorflow implementation  create a new question on [StackOverflow](https://stackoverflow.com/) with the tag ""tensorflow"".  Please report bugs (i.e.  broken code  not usage questions) to the tensorflow/models GitHub [issue tracker](https://github.com/tensorflow/models/issues)  prefixing the issue name with ""deeplab"".   """;General;https://github.com/Vignesh-95/cnn-semantic-segmentation-satellite-images
"""Build DMLab package with Bazel– https://github.com/deepmind/lab/blob/master/docs/users/build.md  Install the python module for DMLab– https://github.com/deepmind/lab/tree/master/python/pip_package   To run without a GPU  use the flag “--disable_cuda”.   All experiments use a slightly revised version of IMPALA from torchbeast   """;Reinforcement Learning;https://github.com/jerrodparker20/adaptive-transformers-in-rl
"""2. A config.json file.  You can define your own  or use one of the provided configs in the configs directory.   where &lt;gpu-id&gt; is the index of the GPU to train on.  This option can be ommitted to run the training on the CPU.   """;Computer Vision;https://github.com/albanie/collaborative-experts
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/Sushma07/dancedarknet
"""When *training with the backbone of [IBN-ResNet-50](https://arxiv.org/abs/1807.09441)*  you need to download the [ImageNet](http://www.image-net.org/) pre-trained model from this [link](https://drive.google.com/drive/folders/1thS2B8UOSBi_cJX6zRy6YYRwz_nVFI_S) and save it under the path of `logs/pretrained/`. ```shell mkdir logs && cd logs mkdir pretrained ``` The file tree should be ``` MMT/logs └── pretrained     └── resnet50_ibn_a.pth.tar ```   ```shell cd examples && mkdir data ``` Download the raw datasets [DukeMTMC-reID](https://arxiv.org/abs/1609.01775)  [Market-1501](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf)  [MSMT17](https://arxiv.org/abs/1711.08565)  and then unzip them under the directory like ``` MMT/examples/data ├── dukemtmc │   └── DukeMTMC-reID ├── market1501 │   └── Market-1501-v15.09.15 └── msmt17     └── MSMT17_V1 ```   ```shell git clone https://github.com/yxgeee/MMT.git cd MMT python setup.py install ```   sh scripts/pretrain.sh dukemtmc market1501 resnet50 1  sh scripts/pretrain.sh dukemtmc market1501 resnet50 2   sh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet50 500   Note that you could add --rr-gpu in the training scripts for faster clustering but requiring more GPU memory.   sh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet50   sh scripts/test.sh market1501 resnet50 logs/dukemtmcTOmarket1501/resnet50-MMT-500/model_best.pth.tar   sh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 500  sh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 700  sh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 900   sh scripts/train_baseline_dbscan.sh dukemtmc market1501 resnet50    Transferring from [DukeMTMC-reID](https://arxiv.org/abs/1609.01775) to [Market-1501](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf) on the backbone of [ResNet-50](https://arxiv.org/abs/1512.03385)  *i.e. Duke-to-Market (ResNet-50)*.   **Duke-to-Market (IBN-ResNet-50)** ```shell #: pre-training on the source domain sh scripts/pretrain.sh dukemtmc market1501 resnet_ibn50a 1 sh scripts/pretrain.sh dukemtmc market1501 resnet_ibn50a 2 #: end-to-end training with MMT-500 sh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet_ibn50a 500 #: or MMT-700 sh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet_ibn50a 700 #: or MMT-DBSCAN sh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet_ibn50a  #: testing the best model sh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-500/model_best.pth.tar sh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-700/model_best.pth.tar sh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-DBSCAN/model_best.pth.tar ``` **Duke-to-MSMT (ResNet-50)** ```shell #: pre-training on the source domain sh scripts/pretrain.sh dukemtmc msmt17 resnet50 1 sh scripts/pretrain.sh dukemtmc msmt17 resnet50 2 #: end-to-end training with MMT-500 sh scripts/train_mmt_kmeans.sh dukemtmc msmt17 resnet50 500 #: or MMT-1000 sh scripts/train_mmt_kmeans.sh dukemtmc msmt17 resnet50 1000 #: or MMT-DBSCAN sh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet50  #: testing the best model sh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-500/model_best.pth.tar sh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-1000/model_best.pth.tar sh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-DBSCAN/model_best.pth.tar ```   """;General;https://github.com/yxgeee/MMT
"""To generate a CSV for submission  please execute the following commands.:  .. code-block::     python scripts/prepare_submission.py --gpu 0 \        ./results/detector/model_700.npz \        ./results/classifier-finetune/model_100.npz    Python version:   matplotlib (3.1.1)  japanize-matplotlib (1.0.4)   Set environment variable:   cd <path to this repo>   python scripts/prepare_pseudo_labels.py --gpu 0 \   python scripts/finetune_classifier.py --gpu 0 \   """;Computer Vision;https://github.com/t-hanya/kuzushiji-recognition
"""cd lib/                       --dataset coco --net mobilenetv1_224_100 \                       --cuda                       --dataset coco --net mobilenetv1_224_100 \   """;General;https://github.com/wangvation/torch-mobilenet
"""We expect the directory structure to be the following: ``` path/to/data/ 	xxx.jpg 	xxx.txt 	123.jpg 	123.txt ``` As in each jpg has a corresponding txt file in the format of  ``` classIndex CenterX CenterY Width Height ``` for each line.     """;Computer Vision;https://github.com/clive819/Modified-DETR
"""``` streamlit run demo.py ```   ``` streamlit run demo_cgan.py ``` """;Computer Vision;https://github.com/JojiJoseph/DCGAN
"""Run sample.py on your model  with the --sample_npz argument  then run inception_tf13 to calculate the actual TensorFlow IS. Note that you will need to have TensorFlow 1.3 or earlier installed  as TF1.4+ breaks the original IS code.     We have not tried the apex SyncBN as my school's servers are on ancient NVIDIA drivers that don't support it--apex would probably be a good place to start.    You will need:  - [PyTorch](https://PyTorch.org/)  version 1.0.1 - tqdm  numpy  scipy  and h5py - The ImageNet training set  First  you may optionally prepare a pre-processed HDF5 version of your target dataset for faster I/O. Following this (or not)  you'll need the Inception moments needed to calculate FID. These can both be done by modifying and running  ```sh sh scripts/utils/prepare_data.sh ```  Which by default assumes your ImageNet training set is downloaded into the root folder `data` in this directory  and will prepare the cached HDF5 at 128x128 pixel resolution.  In the scripts folder  there are multiple bash scripts which will train BigGANs with different batch sizes. This code assumes you do not have access to a full TPU pod  and accordingly spoofs mega-batches by using gradient accumulation (averaging grads over multiple minibatches  and only taking an optimizer step after N accumulations). By default  the `launch_BigGAN_bs256x8.sh` script trains a full-sized BigGAN model with a batch size of 256 and 8 gradient accumulations  for a total batch size of 2048. On 8xV100 with full-precision training (no Tensor cores)  this script takes 15 days to train to 150k iterations.  You will first need to figure out the maximum batch size your setup can support. The pre-trained models provided here were trained on 8xV100 (16GB VRAM each) which can support slightly more than the BS256 used by default. Once you've determined this  you should modify the script so that the batch size times the number of gradient accumulations is equal to your desired total batch size (BigGAN defaults to 2048).  Note also that this script uses the `--load_in_mem` arg  which loads the entire (~64GB) I128.hdf5 file into RAM for faster data loading. If you don't have enough RAM to support this (probably 96GB+)  remove this argument.    """;Computer Vision;https://github.com/ajbrock/BigGAN-PyTorch
"""The code expects the ImageNet validation dataset to be available in TFRecord format in the `data/validation` directory. To provision the data  we have provided a script (`setup/get_imagenet.py`) that downloads  processes  and saves the entire ImageNet dataset in the required format. This script can be run from the `setup` directory in the following manner:  ```bash python get_imagenet.py --local_scratch_dir=""/path/to/jpeg-defense/data"" ```   Downloading the entire dataset from the ImageNet website using this script may be very slow. Optionally  we recommend downloading the  [`ILSVRC2012_img_train.tar`](http://academictorrents.com/details/a306397ccf9c2ead27155983c254227c0fd938e2)  and [`ILSVRC2012_img_val.tar`](http://academictorrents.com/details/5d6d0df7ed81efd49ca99ea4737e0ae5e3a5f2e5) using [Academic Torrents](http://academictorrents.com/)  and placing these files into the `data/raw_data` directory. Then  you can run the following command to skip downloading the dataset and proceed with converting the data into TFRecord format:  ```bash python get_imagenet.py \   --local_scratch_dir=""/path/to/jpeg-defense/data"" \   --provision_only=True ```   This repository uses attacks from the [CleverHans](https://github.com/tensorflow/cleverhans) library  and the models are adapted from [tf-slim](https://github.com/tensorflow/models/tree/master/research/slim). We also use [Sacred](https://github.com/IDSIA/sacred) to keep track of the experiments. All dependencies for this repository can be found in `requirements.txt`. To install these dependencies  run the following command from the `jpeg-defense` directory: ```bash pip install -r requirements.txt ```   To clone this repository using git  simply run the following command:  git clone https://github.com/poloclub/jpeg-defense.git  |  Name                 | Affiliation                     |   The **`main.py`** script in the `shield` package can be used to perform all the experiments using the `--perform=attack|defend|evaluate` flags.  - *attack* - Attacks the specified model with the specified method and its parameters (see `shield/opts.py`).  ```bash python main.py with \                                  perform=attack \   model=resnet_50_v2 \   attack=fgsm \   attack_options=""{'eps': 16}"" ```  - *defend* - Defends the specified attacked images with the specified defense and its parameters (see `shield/opts.py`). The defense uses the attack parameters only to determine which images are loaded for preprocessing  as these parameters are not used by the preprocessing itself.  ```bash python main.py with \                                  perform=defend \   model=resnet_50_v2 \   attack=fgsm \   attack_options=""{'eps': 16}"" \   defense=jpeg \   defense_options=""{'quality': 80}"" ```  - evaluate - Evaluates the specified model with the specified attacked/defended version of the images.  ```bash python main.py with \                                  perform=evaluate \   model=resnet_50_v2 \   attack=fgsm \   attack_options=""{'eps': 16}"" ```    [![YouTube video demo](readme/shield-demo-youtube-thumbnail.jpg)](https://youtu.be/W119nXS4xGE)    """;General;https://github.com/poloclub/jpeg-defense
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/stevenzhou2017/darknet_AlexeyAB
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/xuzhezhaozhao/bert_reading
"""Before you proceed  pip install -r ./requirements.txt   """;General;https://github.com/alexandra-chron/ntua-slp-wassa-iest2018
"""To install the latest version of DGL-KE run:  ``` sudo pip3 install dgl sudo pip3 install dglke ```  Train a `transE` model on `FB15k` dataset by running the following command:  ``` DGLBACKEND=pytorch dglke_train --model_name TransE_l2 --dataset FB15k --batch_size 1000 \ --neg_sample_size 200 --hidden_dim 400 --gamma 19.9 --lr 0.25 --max_step 500 --log_interval 100 \ --batch_size_eval 16 -adv --regularization_coef 1.00E-09 --test --num_thread 1 --num_proc 8 ```  This command will download the `FB15k` dataset  train the `transE` model and save the trained embeddings into the file.   """;General;https://github.com/awslabs/dgl-ke
"""Train the UNet on    **Note : Use Python 3**  """;Computer Vision;https://github.com/zyning/signalSeparation
"""ChainerRL is tested with 3.6. For other requirements  see [requirements.txt](requirements.txt).  ChainerRL can be installed via PyPI: ``` pip install chainerrl ```  It can also be installed from the source code: ``` python setup.py install ```  Refer to [Installation](http://chainerrl.readthedocs.io/en/latest/install.html) for more information on installation.    You can try [ChainerRL Quickstart Guide](examples/quickstart/quickstart.ipynb) first  or check the [examples](examples) ready for Atari 2600 and Open AI Gym.  For more information  you can refer to [ChainerRL's documentation](http://chainerrl.readthedocs.io/en/latest/index.html).   """;General;https://github.com/chainer/chainerrl
"""<div align = 'center'> <img src=""https://raw.githubusercontent.com/lizeng614/pytorch-neural-style/master/img_data/img/shanghai.jpg"" height=""372px""> </div>  <div align = 'center'> <img src=""https://raw.githubusercontent.com/lizeng614/pytorch-neural-style/master/img_data/output/s_ms02.jpg"" 372px> <img src=""https://raw.githubusercontent.com/lizeng614/pytorch-neural-style/master/img_data/style/display/the_scream.jpg"" height=""372px""> </div>  <div align = 'center'> <img src=""https://raw.githubusercontent.com/lizeng614/pytorch-neural-style/master/img_data/output/s_s.jpg"" height=""372px""> <img src=""https://raw.githubusercontent.com/lizeng614/pytorch-neural-style/master/img_data/style/display/stary_night.jpg"" height=""372px""> </div>  <div align = 'center'> <img src=""https://raw.githubusercontent.com/lizeng614/pytorch-neural-style/master/img_data/output/s_lm_01.jpg"" height=""372px""> <img src=""https://raw.githubusercontent.com/lizeng614/pytorch-neural-style/master/img_data/style/display/la_muse.jpg"" height=""372px""> </div>  <div align = 'center'> <img src=""https://raw.githubusercontent.com/lizeng614/pytorch-neural-style/master/img_data/output/s_w01.jpg"" height=""372px""> <img src=""https://raw.githubusercontent.com/lizeng614/pytorch-neural-style/master/img_data/style/display/wave_1.jpg"" height=""372px""> </div>   Basic usage: ``` python neural_style.py --style <image.jpg> --content <image.jpg> ``` use ```--standard-train True```to perform a suggested training process  which is 500 steps Adam of learning rate 0.1 fellowed by another 500 steps Adam of learning rate 0.01  You can run the Ipython Notebook Version. This is convenient if you want play with some parameters  you can even try different layer of SqueezeNet.This implementation only used 7 layers from it's 12 defined   """;Computer Vision;https://github.com/lizeng614/SqueezeNet-Neural-Style-Pytorch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   [![Codacy Badge](https://api.codacy.com/project/badge/Grade/4df57b0d996344cc94ca0d3e78c93211)](https://app.codacy.com/app/h4ste/oscar?utm_source=github.com&utm_medium=referral&utm_content=h4ste/oscar&utm_campaign=Badge_Grade_Settings)  This is a fork of the original (Google's) BERT implementation.   * Add Multi-GPU support with Horovod  This [blog](https://lambdalabs.com/blog/bert-multi-gpu-implementation-using-tensorflow-and-horovod-with-code/) explains all the changes we made to the original implementation.  __Install__ Please first [install Horovod](https://github.com/uber/horovod#install)  __Run__ See the commands in each section to run BERT with Multi-GPUs:  * [Sentence (and sentence-pair) classification tasks](#sentencepair)  * [SQuAD 1.1](#squad1.1)  * [SQuAD 2.0](#squad2.0)  * [Pre-training](#pretraining)      PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).   files (at least one per GPU). Assuming you have split your input dataset        you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/h4ste/oscar
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/keiyamashita111/aaproject
"""This version of unpool_with_argmax runs on the CPU not GPU so is a little slower.  Tensorflow does not provide a CPU version of max_pool_with_argmax  so if you don't have a GPU you can't run this.   """;General;https://github.com/mshunshin/SegNetCMR
"""GPU utilization is now around 85+%            spectral_norm_update_ops = tf.get_collection(SPECTRAL_NORM_UPDATE_OPS)     ...              ...          """;General;https://github.com/minhnhat93/tf-SNDCGAN
"""To practice what you've learned  a good idea would be to spend an hour on 3 of the following (3-hours total  you could through them all if you want) and then write a blog post about what you've learned.  * For an overview of the different problems within NLP and how to solve them read through:    * [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)   * [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e) * Go through [MIT's Recurrent Neural Networks lecture](https://youtu.be/SEnXr6v2ifU). This will be one of the greatest additions to what's happening behind the RNN model's you've been building. * Read through the [word embeddings page on the TensorFlow website](https://www.tensorflow.org/tutorials/text/word_embeddings). Embeddings are such a large part of NLP. We've covered them throughout this notebook but extra practice would be well worth it. A good exercise would be to write out all the code in the guide in a new notebook.  * For more on RNN's in TensorFlow  read and reproduce [the TensorFlow RNN guide](https://www.tensorflow.org/guide/keras/rnn). We've covered many of the concepts in this guide  but it's worth writing the code again for yourself. * Text data doesn't always come in a nice package like the data we've downloaded. So if you're after more on preparing different text sources for being with your TensorFlow deep learning models  it's worth checking out the following:   * [TensorFlow text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text).   * [Reading text files with Python](https://realpython.com/read-write-files-python/) by Real Python. * This notebook has focused on writing NLP code. For a mathematically rich overview of how NLP with Deep Learning happens  read [Standford's Natural Language Processing with Deep Learning lecture notes Part 1](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).     * For an even deeper dive  you could even do the whole [CS224n](http://web.stanford.edu/class/cs224n/) (Natural Language Processing with Deep Learning) course.  * Great blog posts to read:   * Andrei Karpathy's [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) dives into generating Shakespeare text with RNNs.   * [Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) by Mauro Di Pietro. An overview of different techniques for turning text into numbers and then classifying it.   * [What are word embeddings?](https://machinelearningmastery.com/what-are-word-embeddings/) by Machine Learning Mastery. * Other topics worth looking into:   * [Attention mechanisms](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/). These are a foundational component of the transformer architecture and also often add improvments to deep NLP models.   * [Transformer architectures](http://jalammar.github.io/illustrated-transformer/). This model architecture has recently taken the NLP world by storm  achieving state of the art on many benchmarks. However  it does take a little more processing to get off the ground  the [HuggingFace Models (formerly HuggingFace Transformers) library](https://huggingface.co/models/) is probably your best quick start.  ---   1. Rebuild  compile and train `model_1`  `model_2` and `model_5` using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) instead of the Functional API. 2. Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data? 3. Try fine-tuning the TF Hub Universal Sentence Encoder model by setting `training=True` when instantiating it as a Keras layer.  ``` #: We can use this encoding layer in place of our text_vectorizer and embedding layer sentence_encoder_layer = hub.KerasLayer(""https://tfhub.dev/google/universal-sentence-encoder/4""                                          input_shape=[]                                          dtype=tf.string                                          trainable=True) #: turn training on to fine-tune the TensorFlow Hub model ``` 4. Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the `sample_submission.csv` file from Kaggle (see the Files tab in Colab for what the `sample_submission.csv` file looks like). Once you've done this  [make a submission to the Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/data)  how did your model perform? 5. Combine the ensemble predictions using the majority vote (mode)  how does this perform compare to averaging the prediction probabilities of each model? 6. Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels.   Note: You can get all of the notebook code created during the videos in the video_notebooks directory.   It is taught with the following mantra:   If you're training for longer  you probably want to reduce the learning rate as you go... the closer you get to the bottom of the hill  the smaller steps you want to take. Imagine it like finding a coin at the bottom of your couch. In the beginning your arm movements are going to be large and the closer you get  the smaller your movements become.   Jordan Kern  watching these will take you from 0 to 1 with time series problems:    If you'd like some extra materials to go through to further your skills with TensorFlow and deep learning in general or to prepare more for the exam  I'd highly recommend the following:   12 May 2021 - all videos for 09 have now been released on Udemy & ZTM!!! enjoy build SkimLit 📄🔥   To prevent the course from being 100+ hours (deep learning is a broad field)  various external resources for different sections are recommended to puruse under your own discrestion.  You can find solutions to the exercises in [`extras/solutions/`](https://github.com/mrdbourke/tensorflow-deep-learning/tree/main/extras/solutions)  there's a notebook per set of exercises (one for 00  01  02... etc). Thank you to [Ashik Shafi](https://github.com/ashikshafi08) for all of the efforts creating these.  ---   1. Create a vector  scalar  matrix and tensor with values of your choosing using `tf.constant()`. 2. Find the shape  rank and size of the tensors you created in 1. 3. Create two tensors containing random values between 0 and 1 with shape `[5  300]`. 4. Multiply the two tensors you created in 3 using matrix multiplication. 5. Multiply the two tensors you created in 3 using dot product. 6. Create a tensor with random values between 0 and 1 with shape `[224  224  3]`. 7. Find the min and max values of the tensor you created in 6 along the first axis. 8. Created a tensor with random values of shape `[1  224  224  3]` then squeeze it to change the shape to `[224  224  3]`. 9. Create a tensor with shape `[10]` using your own choice of values  then find the index which has the maximum value. 10. One-hot encode the tensor you created in 9.   1. Create your own regression dataset (or make the one we created in ""Create data to view and fit"" bigger) and build fit a model to it. 2. Try building a neural network with 4 Dense layers and fitting it to your own regression dataset  how does it perform? 3. Try and improve the results we got on the insurance dataset  some things you might want to try include:   * Building a larger model (how does one with 4 dense layers go?).   * Increasing the number of units in each layer.   * Lookup the documentation of [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) and find out what the first parameter is  what happens if you increase it by 10x?   * What happens if you train for longer (say 300 epochs instead of 200)?  4. Import the [Boston pricing dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing/load_data) from TensorFlow [`tf.keras.datasets`](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) and model it.   1. Play with neural networks in the [TensorFlow Playground](https://playground.tensorflow.org/) for 10-minutes. Especially try different values of the learning  what happens when you decrease it? What happens when you increase it? 2. Replicate the model pictured in the [TensorFlow Playground diagram](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=6 6 6 6 6&seed=0.51287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&discretize_hide=true&regularizationRate_hide=true&percTrainData_hide=true&dataset_hide=true&problem_hide=true&noise_hide=true&batchSize_hide=true) below using TensorFlow code. Compile it using the Adam optimizer  binary crossentropy loss and accuracy metric. Once it's compiled check a summary of the model. ![tensorflow playground example neural network](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-tensorflow-playground-replication-exercise.png) *Try this network out for yourself on the [TensorFlow Playground website](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=6 6 6 6 6&seed=0.51287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&discretize_hide=true&regularizationRate_hide=true&percTrainData_hide=true&dataset_hide=true&problem_hide=true&noise_hide=true&batchSize_hide=true). Hint: there are 5 hidden layers but the output layer isn't pictured  you'll have to decide what the output layer should be based on the input data.* 3. Create a classification dataset using Scikit-Learn's [`make_moons()`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function  visualize it and then build a model to fit it at over 85% accuracy. 4. Train a model to get 88%+ accuracy on the fashion MNIST test set. Plot a confusion matrix to see the results after. 5. Recreate [TensorFlow's](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function) in your own code. Make sure it can accept a tensor and return that tensor after having the softmax function applied to it. 6. Create a function (or write code) to visualize multiple image predictions for the fashion MNIST at the same time. Plot at least three different images and their prediciton labels at the same time. Hint: see the [classifcation tutorial in the TensorFlow documentation](https://www.tensorflow.org/tutorials/keras/classification) for ideas. 7. Make a function to show an image of a certain class of the fashion MNIST dataset and make a prediction on it. For example  plot 3 images of the `T-shirt` class with their predictions.   1. Spend 20-minutes reading and interacting with the [CNN explainer website](https://poloclub.github.io/cnn-explainer/).   * What are the key terms? e.g. explain convolution in your own words  pooling in your own words 2. Play around with the ""understanding hyperparameters"" section in the [CNN explainer](https://poloclub.github.io/cnn-explainer/) website for 10-minutes.   * What is the kernel size?   * What is the stride?    * How could you adjust each of these in TensorFlow code? 3. Take 10 photos of two different things and build your own CNN image classifier using the techniques we've built here. 4. Find an ideal learning rate for a simple convolutional neural network model on your the 10 class dataset.   1. Build and fit a model using the same data we have here but with the MobileNetV2 architecture feature extraction ([`mobilenet_v2_100_224/feature_vector`](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4)) from TensorFlow Hub  how does it perform compared to our other models? 2. Name 3 different image classification models on TensorFlow Hub that we haven't used. 3. Build a model to classify images of two different things you've taken photos of.   * You can use any feature extraction layer from TensorFlow Hub you like for this.   * You should aim to have at least 10 images of each class  for example to build a fridge versus oven classifier  you'll want 10 images of fridges and 10 images of ovens. 4. What is the current best performing model on ImageNet?   * Hint: you might want to check [sotabench.com](https://www.sotabench.com) for this.   1. Use feature-extraction to train a transfer learning model on 10% of the Food Vision data for 10 epochs using [`tf.keras.applications.EfficientNetB0`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB0) as the base model. Use the [`ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) callback to save the weights to file. 2. Fine-tune the last 20 layers of the base model you trained in 2 for another 10 epochs. How did it go? 3. Fine-tune the last 30 layers of the base model you trained in 2 for another 10 epochs. How did it go? 4. Write a function to visualize an image from any dataset (train or test file) and any class (e.g. ""steak""  ""pizza""... etc)  visualize it and make a prediction on it using a trained model.   1. Take 3 of your own photos of food and use the trained model to make predictions on them  share your predictions with the other students in Discord and show off your Food Vision model 🍔👁. 2. Train a feature-extraction transfer learning model for 10 epochs on the same data and compare its performance versus a model which used feature extraction for 5 epochs and fine-tuning for 5 epochs (like we've used in this notebook). Which method is better? 3. Recreate the first model (the feature extraction model) with [`mixed_precision`](https://www.tensorflow.org/guide/mixed_precision) turned on.    * Does it make the model train faster?    * Does it effect the accuracy or performance of our model?    * What's the advatanges of using `mixed_precision` training?   **Note:** The chief exercise for Milestone Project 1 is to finish the ""TODO"" sections in the [Milestone Project 1 Template notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/TEMPLATE_07_food_vision_milestone_project_1.ipynb). After doing so  move onto the following.  1. Use the same evaluation techniques on the large-scale Food Vision model as you did in the previous notebook ([Transfer Learning Part 3: Scaling up](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/06_transfer_learning_in_tensorflow_part_3_scaling_up.ipynb)). More specifically  it would be good to see:   * A confusion matrix between all of the model's predictions and true labels.   * A graph showing the f1-scores of each class.   * A visualization of the model making predictions on various images and comparing the predictions to the ground truth.     * For example  plot a sample image from the test dataset and have the title of the plot show the prediction  the prediction probability and the ground truth label.  2. Take 3 of your own photos of food and use the Food Vision model to make predictions on them. How does it go? Share your images/predictions with the other students. 3. Retrain the model (feature extraction and fine-tuning) we trained in this notebook  except this time use [`EfficientNetB4`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB4) as the base model instead of `EfficientNetB0`. Do you notice an improvement in performance? Does it take longer to train? Are there any tradeoffs to consider? 4. Name one important benefit of mixed precision training  how does this benefit take place?   1. Rebuild  compile and train `model_1`  `model_2` and `model_5` using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) instead of the Functional API. 2. Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data? 3. Try fine-tuning the TF Hub Universal Sentence Encoder model by setting `training=True` when instantiating it as a Keras layer.  ``` #: We can use this encoding layer in place of our text_vectorizer and embedding layer sentence_encoder_layer = hub.KerasLayer(""https://tfhub.dev/google/universal-sentence-encoder/4""                                          input_shape=[]                                          dtype=tf.string                                          trainable=True) #: turn training on to fine-tune the TensorFlow Hub model ``` 4. Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the `sample_submission.csv` file from Kaggle (see the Files tab in Colab for what the `sample_submission.csv` file looks like). Once you've done this  [make a submission to the Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/data)  how did your model perform? 5. Combine the ensemble predictions using the majority vote (mode)  how does this perform compare to averaging the prediction probabilities of each model? 6. Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels.   1. Train `model_5` on all of the data in the training dataset for as many epochs until it stops improving. Since this might take a while  you might want to use:   * [`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to save the model's best weights only.   * [`tf.keras.callbacks.EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) to stop the model from training once the validation loss has stopped improving for ~3 epochs. 2. Checkout the [Keras guide on using pretrained GloVe embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/). Can you get this working with one of our models?   * Hint: You'll want to incorporate it with a custom token [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.   * It's up to you whether or not you fine-tune the GloVe embeddings or leave them frozen. 3. Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained  embedding for the [TensorFlow Hub BERT PubMed expert](https://tfhub.dev/google/experts/bert/pubmed/2) (a language model pretrained on PubMed texts) pretrained embedding. Does this effect results?   * Note: Using the BERT PubMed expert pretrained embedding requires an extra preprocessing step for sequences (as detailed in the [TensorFlow Hub guide](https://tfhub.dev/google/experts/bert/pubmed/2)).   * Does the BERT model beat the results mentioned in this paper? https://arxiv.org/pdf/1710.06071.pdf  4. What happens if you were to merge our `line_number` and `total_lines` features for each sequence? For example  created a `X_of_Y` feature instead? Does this effect model performance?   * Another example: `line_number=1` and `total_lines=11` turns into `line_of_X=1_of_11`. 5. Write a function (or series of functions) to take a sample abstract string  preprocess it (in the same way our model has been trained)  make a prediction on each sequence in the abstract and return the abstract in the format:   * `PREDICTED_LABEL`: `SEQUENCE`   * `PREDICTED_LABEL`: `SEQUENCE`   * `PREDICTED_LABEL`: `SEQUENCE`   * `PREDICTED_LABEL`: `SEQUENCE`   * ...     * You can find your own unstrcutured RCT abstract from PubMed or try this one from: [*Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection*](https://pubmed.ncbi.nlm.nih.gov/22244707/).   1. Does scaling the data help for univariate/multivariate data? (e.g. getting all of the values between 0 & 1)    * Try doing this for a univariate model (e.g. `model_1`) and a multivariate model (e.g. `model_6`) and see if it effects model training or evaluation results. 2. Get the most up to date data on Bitcoin  train a model & see how it goes (our data goes up to May 18 2021).   * You can download the Bitcoin historical data for free from [coindesk.com/price/bitcoin](https://www.coindesk.com/price/bitcoin) and clicking ""Export Data"" -> ""CSV"". 3. For most of our models we used `WINDOW_SIZE=7`  but is there a better window size?   * Setup a series of experiments to find whether or not there's a better window size.   * For example  you might train 10 different models with `HORIZON=1` but with window sizes ranging from 2-12. 4. Create a windowed dataset just like the ones we used for `model_1` using [`tf.keras.preprocessing.timeseries_dataset_from_array()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array) and retrain `model_1` using the recreated dataset. 5. For our multivariate modelling experiment  we added the Bitcoin block reward size as an extra feature to make our time series multivariate.    * Are there any other features you think you could add?    * If so  try it out  how do these affect the model? 6. Make prediction intervals for future forecasts. To do so  one way would be to train an ensemble model on all of the data  make future forecasts with it and calculate the prediction intervals of the ensemble just like we did for `model_8`. 7. For future predictions  try to make a prediction  retrain a model on the predictions  make a prediction  retrain a model  make a prediction  retrain a model  make a prediction (retrain a model each time a new prediction is made). Plot the results  how do they look compared to the future predictions where a model wasn't retrained for every forecast (`model_9`)? 8. Throughout this notebook  we've only tried algorithms we've handcrafted ourselves. But it's worth seeing how a purpose built forecasting algorithm goes.    * Try out one of the extra algorithms listed in the modelling experiments part such as:     * [Facebook's Kats library](https://github.com/facebookresearch/Kats) - there are many models in here  remember the machine learning practioner's motto: experiment  experiment  experiment.     * [LinkedIn's Greykite library](https://github.com/linkedin/greykite)   **Preparing your brain** 1. Read through the [TensorFlow Developer Certificate Candidate Handbook](https://www.tensorflow.org/extras/cert/TF_Certificate_Candidate_Handbook.pdf). 2. Go through the Skills checklist section of the TensorFlow Developer Certification Candidate Handbook and create a notebook which covers all of the skills required  write code for each of these (this notebook can be used as a point of reference during the exam).  ![mapping the TensorFlow Developer handbook to code in a notebook](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/11-map-the-skills-checklist-to-a-notebook.png) *Example of mapping the Skills checklist section of the TensorFlow Developer Certification Candidate handbook to a notebook.*  **Prearing your computer** 1. Go through the [PyCharm quick start](https://www.jetbrains.com/pycharm/learning-center/) tutorials to make sure you're familiar with PyCharm (the exam uses PyCharm  you can download the free version). 2. Read through and follow the suggested steps in the [setting up for the TensorFlow Developer Certificate Exam guide](https://www.tensorflow.org/extras/cert/Setting_Up_TF_Developer_Certificate_Exam.pdf). 3. After going through (2)  go into PyCharm and make sure you can train a model in TensorFlow. The model and dataset in the example `image_classification_test.py` [script on GitHub](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_classification_test.py) should be enough. If you can train and save the model in under 5-10 minutes  your computer will be powerful enough to train the models in the exam.     - Make sure you've got experience running models locally in PyCharm before taking the exam. Google Colab (what we used through the course) is a little different to PyCharm.  ![before taking the TensorFlow Developer certification exam  make sure you can run TensorFlow code in PyCharm on your local machine](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/11-getting-example-script-to-run-in-pycharm.png) *Before taking the exam make sure you can run TensorFlow code on your local machine in PyCharm. If the [example `image_class_test.py` script](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_classification_test.py) can run completely in under 5-10 minutes on your local machine  your local machine can handle the exam (if not  you can use Google Colab to train  save and download models to submit for the exam).*   * [Neural Networks and Deep Learning Book](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen - If the Zero to Mastery TensorFlow for Deep Learning course is top down  this book is bottom up. A fantastic resource to sandwich your knowledge.  * [Deeplearning.AI specializations](https://www.deeplearning.ai) - This course focuses on code-first  the deeplearning.ai specializations will teach you what's going on behind the code. * [Hands-on Machine Learning with Scikit-Learn  Keras and TensorFlow Book](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) (especially the 2nd half) - Many of the materials in this course were inspired by and guided by the pages of this beautiful text book. * [Full Stack Deep Learning](https://fullstackdeeplearning.com) - Learn how to turn your models into machine learning-powered applications. * [Made with ML MLOps materials](https://madewithml.com/#mlops) - Similar to Full Stack Deep Learning but comprised into many small lessons around all the pieces of the puzzle (data collection  labelling  deployment and more) required to build a full-stack machine learning-powered application. * [fast.ai Curriculum](https://www.fast.ai) - One of the best (and free) AI/deep learning courses online. Enough said. * [""How does a beginner data scientist like me gain experience?""](https://www.mrdbourke.com/how-can-a-beginner-data-scientist-like-me-gain-experience/) by Daniel Bourke - Read this on how to get experience for a job after studying online/at unveristy (start the job before you have it).   """;General;https://github.com/joelweber97/Python3_TF_Certificate
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/Talgin/facerec
"""​        https://zhuanlan.zhihu.com/p/31426458       ​        https://zhuanlan.zhihu.com/p/49897496   ​           https://zhuanlan.zhihu.com/p/80594704   2、链接：https://pan.baidu.com/s/1W9CPgou46-F_AdSc7iZAOA 提取码：nbeq   """;Computer Vision;https://github.com/buaazyz/battery_detect
"""As this vovnet-detectron2 is implemented as a [extension form](https://github.com/youngwanLEE/detectron2/tree/vovnet/projects/VoVNet) (detectron2/projects) upon detectron2  you just install [detectron2](https://github.com/facebookresearch/detectron2) following [INSTALL.md](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md).  Prepare for coco dataset following [this instruction](https://github.com/facebookresearch/detectron2/tree/master/datasets).   We train all models using V100 8GPUs.   one should execute:   """;Computer Vision;https://github.com/suvasis/birdnet2cs231n
"""Now  if we can classify the most common mistakes that people make     which will take away attention from the road (like appreciating   Get the pose estimate.  Get angles between the different skeletons.   """;Computer Vision;https://github.com/AkashGanesan/PedestrianAttention
"""```python from models import octave_resnet50  model = octave_resnet50(num_classes=10) ```  """;Computer Vision;https://github.com/matsuren/OctaveConv.pytorch
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/bhfs9999/maskrcnn_isic
"""[BERT](https://github.com/google-research/bert) is a pre-trained language model proposed by Google AI in 2018. It has achieved excellent results in many tasks in the NLP field  and it is also a turning point in the NLP field.  academic paper which describes BERT in detail and provides full results on a number of tasks can be found here:https://arxiv.org/abs/1810.04805.  Although after bert  a number of excellent models that have swept the NLP field  such as [RoBERTa](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md)  [XLNet](https://github.com/zihangdai/xlnet)  etc.  have also been improved on the basis of BERT.  BERT-Classifier is a general text classifier that is simple and easy to use. It has been improved on the basis of BERT and supports three paragraphs of sentences as input for prediction. The low-level API was used to reconstruct the overall pipline  which effectively solved the problem of weak flexibility of the tensorflow estimator. Optimize the training process  effectively reduce the model initialization time  solve the problem of repeatedly reloading the calculation graph during the estimator training process  and add a wealth of monitoring indicators during training  including (precision  recall  AUC  ROC curve  Confusion Matrix  F1 score  learning rate  loss  etc.)  which can effectively monitor model changes during training.  BERT-Classifier takes full advantage of the Python multi-process mechanism  multi-core speeds up the data preprocessing process  and the data preprocessing speed is more than 10 times faster than the original bert run_classifier (the specific speed increase is related to the number of CPU cores  frequency  and memory size).  Optimized the model checkpoint saving mechanism  which can save TOP N checkpoints according to different indicators  and adds the checkpoint average function  which can fuse model parameters generated in multiple different stages to further enhance model robustness.  It also supports packaging the trained models into services for use by downstream tasks.   Note that the Bert-Classifier MUST be running on Python >= 3.5 with Tensorflow == 1.14.0. the Bert-Classifier does not support Tensorflow 2.0!   You can fine-tune the model with the following command:   All experiments are based on  BERT-Base   the GPU uses GTX 1080 (8G)  and Tnesorflow version is 1.14.0.  | max_seq_len | 1 GPU | 2 GPU | 4 GPU |   | max_seq_len | 1 GPU | 2 GPU | 4 GPU |   | max_seq_len | 1 GPU | 2 GPU | 4 GPU |   | max_seq_len | 1 GPU | 2 GPU | 4 GPU |   You only need three lines of code to use the model for inference tasks  as shown below:  ```python from BertClassifier import BertClassifier model = BertClassifier(data_processor                          num_labels                          bert_config_file                         max_seq_length                          vocab_file                          tensorboard_dir                          init_checkpoint                          keep_checkpoint_max                          use_GPU                          label_smoothing                          cycle) model.predict(file_path='./data/test.tsv'  predict_batch_size=128  output_dir='./predict') #: Or single sample inference #: prob = model.predict(input_example=input_example) ```  In inference mode  the model allows two types as inputs:  When predicting a single sample  for example  in some streaming task scenarios  you may need to predict only a single sample. In this case  you need to construct an InputExample instance of the input features and pass it to the model.predict function. The function returns the probability distribution of the predicted result.  In batch sample prediction  you can directly pass in the file path. The model will first parse the file according to the MyProcessor defined above and perform batch inference. The result will be saved in the directory specified by output_dir.   """;Natural Language Processing;https://github.com/guoyaohua/BERT-Classifier
"""![Combined](https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/cam_gb_dog.jpg?raw=true)   pip install grad-cam   : You can also use it within a with statement  to make sure it is freed    To reduce noise in the CAMs  and make it fit better on the objects  two smoothing methods are supported:  - `aug_smooth=True`    Test time augmentation: increases the run time by x6.    Applies a combination of horizontal flips  and mutiplying the image   by [1.0  1.1  0.9].    This has the effect of better centering the CAM around the objects.   - `eigen_smooth=True`    First principle component of `activations*weights`    This has the effect of removing a lot of noise.   |AblationCAM | aug smooth | eigen smooth | aug+eigen smooth| |------------|------------|--------------|--------------------| ![](./examples/nosmooth.jpg) | ![](./examples/augsmooth.jpg) | ![](./examples/eigensmooth.jpg) | ![](./examples/eigenaug.jpg) |   ----------   Usage: `python cam.py --image-path <path_to_image> --method <method>`  To use with CUDA: `python cam.py --image-path <path_to_image> --use-cuda`  ----------  You can choose between:  `GradCAM`   `ScoreCAM`  `GradCAMPlusPlus`  `AblationCAM`  `XGradCAM`   `LayerCAM` and `EigenCAM`.  Some methods like ScoreCAM and AblationCAM require a large number of forward passes  and have a batched implementation.  You can control the batch size with `cam.batch_size = `  ----------   """;Computer Vision;https://github.com/jacobgil/pytorch-grad-cam
"""- You'll need to install the pretrained generator model for the COCO dataset into `checkpoints/coco_pretrained/`. Instructions for this can be found on the `nvlabs/spade` repo.  - Make sure you need to install all the Python requirements using `pip3 install -r requirements.txt`. Once you do this  you should be able to run the server using `python3 server.py`. It will run it on `0.0.0.0` on port 80. Unfortunately these are hardcoded into the server and right now you cannot pass CLI arguments to the server to specify the port and host  as the PyTorch stuff also reads from the command line (will fix this soon).   This is the example of installation in my env. Please modify the details of installation process for your own env.    ``` conda create -n nvidia_spade python=3.5.6 conda activate nvidia_spade conda install git  cd C:\ mkdir GithubClone cd GithubClone ``` ``` git clone https://github.com/NVlabs/SPADE cd SPADE conda install -c pytorch pytorch pip install -r requirements.txt ```  ``` cd backend mkdir checkpoints cd checkpoints ``` then  copy checkpoints.tar.gz in checkpoints and extract it inside the directory. ``` tar xvf checkpoints.tar.gz cd ../ ```  test the script on terminal in backend directory (This process's purpose is not visualizing on Browser)  ``` python test.py --name coco_pretrained --dataset_mode coco --dataroot C:\GithubClone\SPADE\datasets\coco_stuff ```   cd backend   """;General;https://github.com/llDataSciencell/SmartSketchNvidiaSpadeForWindows
"""Installation from the source. Python's virtual or Conda environments are recommended.  ```bash git clone https://github.com/luomancs/retriever_reader_for_okvqa.git cd retriever_reader_for_okvqa pip install -r requirements.txt ```  Visual-DPR is tested on Python 3.7 and PyTorch 1.7.1.    """;Natural Language Processing;https://github.com/luomancs/retriever_reader_for_okvqa
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='GeForce RTX 2080 Ti'  total_memory=11019MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/Mersive-Technologies/yolov3
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   For R-FCN/Faster R-CNN\: 1. Please download COCO and VOC 2007+2012 datasets  and make sure it looks like this:  	``` 	./data/coco/ 	./data/VOCdevkit/VOC2007/ 	./data/VOCdevkit/VOC2012/ 	```  2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  For DeepLab\: 1. Please download Cityscapes and VOC 2012 datasets and make sure it looks like this:  	``` 	./data/cityscapes/ 	./data/VOCdevkit/VOC2012/ 	``` 2. Please download argumented VOC 2012 annotations/image lists  and put the argumented annotations and the argumented train/val lists into:  	``` 	./data/VOCdevkit/VOC2012/SegmentationClass/ 	./data/VOCdevkit/VOC2012/ImageSets/Main/ 	```      Respectively.     2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  1. Clone the Deformable ConvNets repository  and we'll call the directory that you cloned Deformable-ConvNets as ${DCN_ROOT}. ``` git clone https://github.com/msracver/Deformable-ConvNets.git ```  2. For Windows users  run ``cmd .\init.bat``. For Linux user  run `sh ./init.sh`. The scripts will build cython module automatically and create some folders.  3. Install MXNet: 	 	**Note: The MXNet's Custom Op cannot execute parallelly using multi-gpus after this [PR](https://github.com/apache/incubator-mxnet/pull/6928). We strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.5).**  	***Quick start***  	3.1 Install MXNet and all dependencies by  	``` 	pip install -r requirements.txt 	``` 	If there is no other error message  MXNet should be installed successfully.  	 	***Build from source (alternative way)***  	3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by 	``` 	git clone --recursive https://github.com/dmlc/mxnet.git 	git checkout 998378a 	git submodule update 	#: if it's the first time to checkout  just use: git submodule update --init --recursive 	``` 	3.3 Compile MXNet 	``` 	cd ${MXNET_ROOT} 	make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1 	``` 	3.4 Install the MXNet Python binding by 	 	***Note: If you will actively switch between different versions of MXNet  please follow 3.5 instead of 3.4*** 	``` 	cd python 	sudo python setup.py install 	``` 	3.5 For advanced users  you may put your Python packge into `./external/mxnet/$(YOUR_MXNET_PACKAGE)`  and modify `MXNET_VERSION` in `./experiments/rfcn/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`. Thus you can switch among different versions of MXNet quickly.  4. For Deeplab  we use the argumented VOC 2012 dataset. The argumented annotations are provided by [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html) dataset. For convenience  we provide the converted PNG annotations and the lists of train/val images  please download them from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMRhVImMI1jRrsxDg).   We provide trained deformable convnet models  including the deformable R-FCN & Faster R-CNN models trained on COCO trainval  and the deformable DeepLab model trained on CityScapes train.  1. To use the demo with our pre-trained deformable models  please download manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMSjehIcCgAhvEAHw) or [BaiduYun](https://pan.baidu.com/s/1dFlPFED)  and put it under folder `model/`.  	Make sure it looks like this: 	``` 	./model/rfcn_dcn_coco-0000.params 	./model/rfcn_coco-0000.params 	./model/fpn_dcn_coco-0000.params 	./model/fpn_coco-0000.params 	./model/rcnn_dcn_coco-0000.params 	./model/rcnn_coco-0000.params 	./model/deeplab_dcn_cityscapes-0000.params 	./model/deeplab_cityscapes-0000.params 	./model/deform_conv-0000.params 	./model/deform_psroi-0000.params 	``` 2. To run the R-FCN demo  run 	``` 	python ./rfcn/demo.py 	``` 	By default it will run Deformable R-FCN and gives several prediction results  to run R-FCN  use 	``` 	python ./rfcn/demo.py --rfcn_only 	``` 3. To run the DeepLab demo  run 	``` 	python ./deeplab/demo.py 	``` 	By default it will run Deformable Deeplab and gives several prediction results  to run DeepLab  use 	``` 	python ./deeplab/demo.py --deeplab_only 	``` 4. To visualize the offset of deformable convolution and deformable psroipooling  run 	``` 	python ./rfcn/deform_conv_demo.py 	python ./rfcn/deform_psroi_demo.py 	```    1. All of our experiment settings (GPU #  dataset  etc.) are kept in yaml config files at folder `./experiments/rfcn/cfgs`  `./experiments/faster_rcnn/cfgs` and `./experiments/deeplab/cfgs/`. 2. Eight config files have been provided so far  namely  R-FCN for COCO/VOC  Deformable R-FCN for COCO/VOC  Faster R-CNN(2fc) for COCO/VOC  Deformable Faster R-CNN(2fc) for COCO/VOC  Deeplab for Cityscapes/VOC and Deformable Deeplab for Cityscapes/VOC  respectively. We use 8 and 4 GPUs to train models on COCO and on VOC for R-FCN  respectively. For deeplab  we use 4 GPUs for all experiments.  3. To perform experiments  run the python scripts with the corresponding config file as input. For example  to train and test deformable convnets on COCO with ResNet-v1-101  use the following command     ```     python experiments\rfcn\rfcn_end2end_train_test.py --cfg experiments\rfcn\cfgs\resnet_v1_101_coco_trainval_rfcn_dcn_end2end_ohem.yaml     ```     A cache folder would be created automatically to save the model and the log under `output/rfcn_dcn_coco/`. 4. Please find more details in config files and in our code.   """;Computer Vision;https://github.com/qilei123/DEEPLAB_4_RETINA
"""1. Archive your training data and upload it to an S3 bucket 2. Provision your EC2 instance (I used an Ubuntu AMI) 3. Log into your EC2 instance via SSH 4. Install the aws CLI client and configure it:  ```bash sudo snap install aws-cli --classic aws configure ```  You will then have to enter your AWS access keys  which you can retrieve from the management console under AWS Management Console > Profile > My Security Credentials > Access Keys  Then  run these commands  or maybe put them in a shell script and execute that:  ```bash mkdir data curl -O https://bootstrap.pypa.io/get-pip.py sudo apt-get install python3-distutils python3 get-pip.py pip3 install stylegan2_pytorch export PATH=$PATH:/home/ubuntu/.local/bin aws s3 sync s3://<Your bucket name> ~/data cd data tar -xf ../train.tar.gz ```  Now you should be able to train by simplying calling `stylegan2_pytorch [args]`.  Notes:  * If you have a lot of training data  you may need to provision extra block storage via EBS. * Also  you may need to spread your data across multiple archives. * You should run this on a `screen` window so it won't terminate once you log out of the SSH session.   You will need a machine with a GPU and CUDA installed. Then pip install the package like this  ```bash $ pip install stylegan2_pytorch ```  If you are using a windows machine  the following commands reportedly works.  ```bash $ conda install pytorch torchvision -c python $ pip install stylegan2_pytorch ```   仍然在修改过程中。当前不是稳定版本，需预先安装PaddlePaddle GPU develop版本。   在完成PaddlePaddle的安装后，在本仓库的本地克隆路径下运行pip install .完成安装。   Thanks to <a href=""https://github.com/GetsEclectic"">GetsEclectic</a>  you can now calculate the FID score periodically! Again  made super simple with one extra argument  as shown below.   ```bash $ stylegan2_pytorch --data /path/to/images ```  That's it. Sample images will be saved to `results/default` and models will be saved periodically to `models/default`.   You can specify the name of your project with  ```bash $ stylegan2_pytorch --data /path/to/images --name my-project-name ```  You can also specify the location where intermediate results and model checkpoints should be stored with  ```bash $ stylegan2_pytorch --data /path/to/images --name my-project-name --results_dir /path/to/results/dir --models_dir /path/to/models/dir ```  You can increase the network capacity (which defaults to `16`) to improve generation results  at the cost of more memory.  ```bash $ stylegan2_pytorch --data /path/to/images --network-capacity 256 ```  By default  if the training gets cut off  it will automatically resume from the last checkpointed file. If you want to restart with new settings  just add a `new` flag  ```bash $ stylegan2_pytorch --new --data /path/to/images --name my-project-name --image-size 512 --batch-size 1 --gradient-accumulate-every 16 --network-capacity 10 ```  Once you have finished training  you can generate images from your latest checkpoint like so.  ```bash $ stylegan2_pytorch  --generate ```  To generate a video of a interpolation through two random points in latent space.  ```bash $ stylegan2_pytorch --generate-interpolation --interpolation-num-steps 100 ```  To save each individual frame of the interpolation  ```bash $ stylegan2_pytorch --generate-interpolation --save-frames ```  If a previous checkpoint contained a better generator  (which often happens as generators start degrading towards the end of training)  you can load from a previous checkpoint with another flag  ```bash $ stylegan2_pytorch --generate --load-from {checkpoint number} ```  A technique used in both StyleGAN and BigGAN is truncating the latent values so that their values fall close to the mean. The small the truncation value  the better the samples will appear at the cost of sample variety. You can control this with the `--trunc-psi`  where values typically fall between `0.5` and `1`. It is set at `0.75` as default  ```bash $ stylegan2_pytorch --generate --trunc-psi 0.5 ```   """;Computer Vision;https://github.com/HighCWu/stylegan2-paddle
"""The Neural Network used for this work is a Odenet neural network: https://arxiv.org/abs/1806.07366 In particular I used a Res-Ode that's mean a miniblock of resnet before the ODE block.     - Python 3.6   - Matplotlib   As you can see the dataset is composed by several data taken from web. (This practice has also been widely used by various authors in the realization of the same papers about Covid with deep NN).  Before to train your model you have to set path for dataset. It is advisable to upload images only once and store them in an array (in our case you can find array in ""data"" folder). In my case I've stored data in npy_32 (where 32 means size of images uploaded).   To train run:  ```sh $ python odenet_big.py ```   """;General;https://github.com/LuigiRussoDev/Covid19Detection
"""como o NumPy.   vejamos a implementação feita em NumPy:   * NumPy: imediatamente executa o cálculo e gera o resultado      ""name"": ""stdout""      ""name"": ""stdout""      ""name"": ""stdout""      ""name"": ""stdout""      ""name"": ""stdout""      ""name"": ""stdout""      ""name"": ""stdout""    Note que isso funciona independente se o formato é estaticamente especificado ou     ""name"": ""stdout""      ""name"": ""stdout""    b = tf.Variable(1  name=""b"")     ""name"": ""stdout""    b = tf.Variable(1  name=""b"")     ""name"": ""stdout""    b = tf.Variable(1  name=""b"")     ""name"": ""stdout""      ""name"": ""stdout""      ""name"": ""stdout""      ""name"": ""stdout""      ""name"": ""stdout""    Python ops   Python ops:   Python ops permite converter uma função Python normal em uma operação em   Note que Python não permite a sobrecarga das   Avaliar ""a"" retornará o valor 3 como esperado. Note que criou-se 3 tensores    Note que o tensor c não tem um valor determinístico. Esse valor pode ser 3 ou 7   Note que a ordem   Isso não está somente ficando feio  mas também ineficiente. Note que estamos   ReLU em TensorFlowem python:   Note que essa é uma implementação bastante ineficiente  e é utilizável  somente para prototipagem  uma vez que código Python não é paralelizável e não  irá rodar na GPU. Uma vez verificada a ideia  você definitivamente irá querer   Na prática utilizamos operações em python   #: Run the python op.   A mesma operação pode ser feita de maneira simples em uma GPU:   separar os dados e usar uma GPU separada para precessar cada metade:   entrada e saída estejam em batchs. Note que nós também adicionamos um alcance     with tf.variable_scope(name  default_name=""merge""):             name=""proj_%d"" % i      with tf.variable_scope(name  default_name=""batch_normalization""):   """;Computer Vision;https://github.com/gmouzella/Efective_TensorFlow
"""If you don't own a GPU remove the --cuda option  although I advise you to get one!   Please first install [Anaconda](https://anaconda.org) and create an Anaconda environment using the environment.yml file.  ``` conda env create -f environment.yml ```  After you create the environment  activate it. ``` source activate hw1 ```  Our current implementation only supports GPU so you need a GPU and need to have CUDA installed on your machine.   """;General;https://github.com/sweet-rytsar/cvfve_hw1
"""This is a GAN demo for creating anime character faces from random noise.   """;Computer Vision;https://github.com/RikoLi/gan-acgface
"""(Only PhD students) Lecture (PhD students will be required to prepare a lecture)   You will need at least a single GPU. Google's Colab seems to be the best option   """;Computer Vision;https://github.com/gmum/dl-mo-2021
"""First download the code by git clone this repo: ```bash git clone https://github.com/galprz/brain-tumor-segemntation ``` Then use conda to install dependencies and setup the environment  ```bash conda end update -f environment.yml conda activate brain-tumor-segmentation ```  We tested those models` performance with the dice metric on the brain_tumor_dataset (https://figshare.com/articles/brain_tumor_dataset/1512427).   """;Computer Vision;https://github.com/galprz/brain-tumor-segmentation
"""``` import os import keras.backend as K  from data import DATA_SET_DIR from elmo.lm_generator import LMDataGenerator from elmo.model import ELMo  parameters = {     'multi_processing': False      'n_threads': 4      'cuDNN': True if len(K.tensorflow_backend._get_available_gpus()) else False      'train_dataset': 'wikitext-2/wiki.train.tokens'      'valid_dataset': 'wikitext-2/wiki.valid.tokens'      'test_dataset': 'wikitext-2/wiki.test.tokens'      'vocab': 'wikitext-2/wiki.vocab'      'vocab_size': 28914      'num_sampled': 1000      'charset_size': 262      'sentence_maxlen': 100      'token_maxlen': 50      'token_encoding': 'word'      'epochs': 10      'patience': 2      'batch_size': 1      'clip_value': 5      'cell_clip': 5      'proj_clip': 5      'lr': 0.2      'shuffle': True      'n_lstm_layers': 2      'n_highway_layers': 2      'cnn_filters': [[1  32]                      [2  32]                      [3  64]                      [4  128]                      [5  256]                      [6  512]                      [7  512]                     ]      'lstm_units_size': 400      'hidden_units_size': 200      'char_embedding_size': 16      'dropout_rate': 0.1      'word_dropout_rate': 0.05      'weight_tying': True  }  #: Set-up Generators train_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['train_dataset'])                                    os.path.join(DATA_SET_DIR  parameters['vocab'])                                    sentence_maxlen=parameters['sentence_maxlen']                                    token_maxlen=parameters['token_maxlen']                                    batch_size=parameters['batch_size']                                    shuffle=parameters['shuffle']                                    token_encoding=parameters['token_encoding'])  val_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['valid_dataset'])                                  os.path.join(DATA_SET_DIR  parameters['vocab'])                                  sentence_maxlen=parameters['sentence_maxlen']                                  token_maxlen=parameters['token_maxlen']                                  batch_size=parameters['batch_size']                                  shuffle=parameters['shuffle']                                  token_encoding=parameters['token_encoding'])  test_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['test_dataset'])                                  os.path.join(DATA_SET_DIR  parameters['vocab'])                                  sentence_maxlen=parameters['sentence_maxlen']                                  token_maxlen=parameters['token_maxlen']                                  batch_size=parameters['batch_size']                                  shuffle=parameters['shuffle']                                  token_encoding=parameters['token_encoding'])  #: Compile ELMo elmo_model = ELMo(parameters) elmo_model.compile_elmo(print_summary=True)  #: Train ELMo elmo_model.train(train_data=train_generator  valid_data=val_generator)  #: Persist ELMo Bidirectional Language Model in disk elmo_model.save(sampled_softmax=False)  #: Evaluate Bidirectional Language Model elmo_model.evaluate(test_generator)  #: Build ELMo meta-model to deploy for production and persist in disk elmo_model.wrap_multi_elmo_encoder(print_summary=True  save=True)  #: Load ELMo encoder elmo_model.load_elmo_encoder()  #: Get ELMo embeddings to feed as inputs for downstream tasks elmo_embeddings = elmo_model.get_outputs(test_generator  output_type='word'  state='mean')  #: BUILD & TRAIN NEW KERAS MODEL FOR DOWNSTREAM TASK (E.G.  TEXT CLASSIFICATION)  ```   """;Natural Language Processing;https://github.com/iliaschalkidis/ELMo-keras
"""To run deep learning examples  you will need TensorFlow and Keras  plus a number of Python packages. To run robot experiments  you'll need a simulator (Gazebo or PyBullet)  and ROS Indigo or Kinetic. Other versions of ROS may work but have not been tested. If you want to stick to the toy examples  you do not need to use this as a ROS package.   PyBullet Block Stacking download tar.gz   1. Installation Guide   3.2 MARCC instructions: learning models using JHU's MARCC cluster   setup: contains setup scripts   """;General;https://github.com/jhu-lcsr/costar_plan
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/TSLNIHAOGIT/bert
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/sguo35/yolov3
"""You will need to pip install advertorch   Transformer model with Fixup (instead of layer normalization) is available. To run the experiments  you will need to download and install the fairseq library (the provided code was tested on an earlier version: https://github.com/pytorch/fairseq/tree/5d00e8eea2644611f397d05c6c8f15083388b8b4). You can then copy the files into corresponding folders.   """;Computer Vision;https://github.com/AngusG/bn-advex-zhang-fixup
"""You can install `aft_pt` via `pip`:  ```bash pip install aft_pt ```   <img src=""https://github.com/ShenDezhou/aft-pytorch/blob/master/pic.png"" width=650>   You can import the **AFT-Full** or **AFT-Simple** layer (as described in the paper) from the package like so:   """;General;https://github.com/ShenDezhou/aft-pytorch
"""1. Git clone this repository.  $git clone https://github.com/jacquelinelala/GFN.git  $cd GFN   (If you don't have access to MATLAB  we offer a validation dataset for testing. You can download it from GoogleDrive or Pan Baidu.)  folder = 'your_downloads_directory/GOPRO_Large'; #: You should replace the your_downloads_directory by your GOPRO_Large's directory.   You should accomplish the first two steps in Test on LR-GOPRO Validation before the following steps.   """;General;https://github.com/jacquelinelala/GFN
"""Install from [pip](https://pypi.org/project/pytorch-fid/):  ``` pip install pytorch-fid ```  Requirements: - python3 - pytorch - torchvision - pillow - numpy - scipy   See https://github.com/bioinf-jku/TTUR for the original implementation using Tensorflow.   To compute the FID score between two datasets  where images of each dataset are contained in an individual folder: ``` python -m pytorch_fid path/to/dataset1 path/to/dataset2 ```  To run the evaluation on GPU  use the flag `--device cuda:N`  where `N` is the index of the GPU to use.    """;General;https://github.com/mseitzer/pytorch-fid
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/cto300/stylegan2
"""9. git clone this codebase `synergyDRL` 10. ```cd synergyDRL``` 11. ```conda env create -f updated_env.yml``` 12.  ```conda activate synergy_analysis``` 13. ```cd ..``` 14. ```pip install -e synergyDRL``` 15. ```cd synergyDRL```    6. Install [Conda](https://www.anaconda.com/distribution/) with python version 3. **Please choose 'yes' for conda init option at the end of installation.** 7. source ~/.bashrc 8. conda install lockfile (required in experiments)   1. On the home directory ~  create folder .mujoco  2. [Download](https://www.roboti.us/index.html) and unzip MuJoCo 1.50 from the MuJoCo website. We assume that the MuJoCo files are extracted to the default location (`~/.mujoco/mjpro150`). 3. Copy your MuJoCo license key (mjkey.txt) to ~/.mujoco/mjpro150/bin 4. Add the following line with the appropriate username to .bashrc   export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/username/.mujoco/mjpro150/bin  5. source ~/.bashrc   The environment should be ready to run experiments.   To deactivate and remove the conda environment:  conda deactivate  conda remove --name synergy_analysis --all   """;Reinforcement Learning;https://github.com/JiazhengChai/synergyDRL
"""- The Encoder weights are saved after each train using the `train.py` file. - We train a classifier with one hidden layer on top of the features spaces of the freezed Encoder in `classifier.py` file. - The classifier reach 85% accuracy on Mnist.    """;General;https://github.com/Medabid1/CPC
"""Simplest (dialog text is required)   """;Sequential;https://github.com/CongBao/ChatBot
"""https://dreamtolearn.com/ryan/cognitivewingman/18/en  Character Cartridges - Embodied Identity  We are entering a magical convergence phase in media and technology. The next dozen years through 2030 are going to be very interesting as we begin to understand how to develop character and identity for sensemaking systems  leveraging AI.  Multiple technologies – including in mobile  AR  and deep learning - are rapidly converging to enable organizations to compose AI-powered systems only dreamt of in Sci-Fi novels and Hollywood movies.    These sensemaking (and empathetic) systems will quickly enable assistants who can play roles that include a “Cognitive Wingman” – similar to the automated intelligence seen in media: JARVIS (Iron Man); KITT (Knight Rider); HAL (Space Odyssey); Samantha (Her); TARS (Interstellar).      These systems will: · Talk and listen · Have identity · Have relationships · Are situationally aware · Reason  Understand and Learn · Understand context and remember things · Can hold state for multiple ‘conversation turns’ · Behave in a manner that simulates emotional intelligence     With readily available technology - can build alpha versions of these systems today.  Mind you many POC's quite crappy – but it’s a start - and demonstrates feasibility.  And with widely available ML tools and techniques  and our human tendency to improve on things – good stuff will happen soon.     A key component for the creation of Digital Humans (for applications extending well beyond gaming) is a sense of identity and character.   Empathetic systems that embody cognitive elements need personality.   The best rendered face and eyes  is still just a collection of high resolution pixels – until we add voice  emotion  identity and soul.    https://blogs.unity3d.com/2020/05/12/announcing-ml-agents-unity-package-v1-0/   May 8  2020 https://openai.com/blog/emergent-tool-use/ In terms of observing autonomous or semi autonomous agents  Our character cartridge infused agents could be observed in a similar environment but instead of hide and seek for goal seeking there’s something more around curiosity or stress or cortisol  (thanks for reminder lucy)_   Jake update video - APril 28 https://www.youtube.com/watch?v=0VyLE_fG_7o&feature=youtu.be Explosions and also first cut of the room of observation / monitoring room  Jake update  bideo  march  31sat  - Orb tests  switch throw  Regina and John/Jake https://youtu.be/IBpVZaGsWrI Avatars ""animated""  after switch tthrrow (check  out 1:45 'big stretch')      Test video Feb 5  - Three Toasters loading - BLue5 Red1 Yellow1 https://www.youtube.com/watch?v=UzgKLKAutXM  Jake Test Video - Feb 4 2020 A short demo of some concepts to discuss in call  https://drive.google.com/open?id=13J85ERDewAGbgYWj1rneY7uiung7H61D  Prototype - Cartridges for Personality https://drive.google.com/open?id=1fhQMKUT-NH52kjPctRO0ZeSbR8H_ZoIj  Pad Thai Data driven avatars  https://drive.google.com/file/d/0B3WOmvm7uBq-RXZ1dlRYZWg5cmM/view  Bubbleman Avatar https://youtu.be/fOfFrGsNwHo   As you begin to build your virtual lab  you’ve decided a few things:   •   VR User (you!) enter your virtual lab ready to create several Characters and    •   You DECIDE which one you want to ANIMATE  by pointing the switch (or pushing a button) to select character in focus.   •	Media  Entertainment  Gaming & Fantasy - Sophisticated VR allows users to make-believe.  Multiplayer  massive communities  realistic  exciting  and immersive.  Value drivers of modern cinema   plus player immersion inside plots – which will include adult entertainment.  Bend physics  time & space in a Holodeck and ‘virtual worlds’; Media and Entertainment (AR VR XR) – populating characters in virtual environments;  •	Decision Support Help humans summon and engage data to help with decisions.   Consumer: high-value feature rich options (home decorating  automobile) helps buyers compare & understand (see) options.  ERP / Strategic:  Executives with data-on-demand  verbal command & control BI ERP integrations. Shared visualizations •	Cognitive Extenders - Help executives and innovators reduce cognitive load and extend cognitive range.  Better reasoning  recall  decision support  social navigation & connection making. Context aware information augmentation.  Instant context-aware data recall and visualization for decision support.  Collaboration catalyst.  LEARNING & COGNITIVE EXTENDERS -> Helping create connections and amplify abilities – Loci; COMPREHEND & CLARIFY CONTENT -> Helping to surface signal and distill information •	Cognitive Wingman - Jarvis  KITT  HAL.  Sensemaking systems understand context  to help. Use cases include autism  eldercare  Alzheimer’s & PTSD.  Cognitive Wingman is a human assistive AI/ADA buddy embedded inside AR headset – microphones & camera enable sensemaking & AR projection & audio to guide - or to guard.  Can also be used as moment-recall by therapists and caregivers. •	Expertise Projection - Amplify and project scarce expertise.  Highly skilled medical specialists projecting expertise 2000 miles away to nurse practitioners who touch patients.   Industrial – leverage expert engineers at distance to help low skilled workers repair or deploy complex assets.  Hands free.  Information overlay. •	Knowledge Map & Recall - Dark Data / Data Exhaust. Enterprises are drowning in data.  Knowledge & expertise fuels continuing innovation and digital transformation. Workers retiring  taking key knowledge. AR enables knowledge capture & recall across time/space. Verbal command/control  visual delivery. Leverage spatial memory. Neural Prosthetics. •	Unified Communications - The final destination for UC?  As close to being present  without actually being present.  Project remote attendee into an empty seat at a board meeting 3000 miles away.  Re-watch 2 year old meetings.  Look into the eyes & face of job applicant. AR for UC3.0 enables human communications at distance •	Infrastructure - AR enables engineers to see into  and project onto  complex and/or aging infrastructure assets to make best use of data  in field  real time.  Touches Digital Twin;  Decision Support;  Knowledge Mapping and recall  to enable AR equipped user ; Digital Twin / Industrial - Digital Twin is virtual/digital representation of a physical entity or system  living model that evolves over time  includes structured and unstructured data.  IOT  Edge appliances and predictive analytics.   •	Neural Adaptive - (Speculative) NLU powered context gathering / sensemaking.  Emotion and eye tracking. Neural network & deep learning powered systems to recognize patterns from biometric signals (EEG/FMRI). Education optimization. AR content & agents serving as baseline reference for neural adaptive AR systems. •	Education - AR opens up new ways for children and adults to interact with  and consume and retain knowledge  in the most efficient and effective way – for each person.  Customization;  interactivity; flexibility and leverage spatial and visual components of AR for learners most benefiting from methods. Key Elements of Solution:  1. Augments known educational best practices (learning outcomes  learning pathways) with insights gained from ML/DL analysis combined with traditional data science // 2. Leverages dynamic segmentation and clustering (cohorts  archetypes) // 3. Uses deep learning to surface key features in natural language and knowledge set / ontology // 4. Uses Machine Learning (e.g. Random Forest) to surface key features in learning path // 5. Applies Natural Language Understanding for signal extraction from students & teachers // 6. Interacts – Provides natural language and visual interactions to exchange information  including but not limited to Augmented Reality  Virtual Reality and Digital Humans. // 7. Learns.  Evolves with the content  learners and teachers to improve over time    """;Natural Language Processing;https://github.com/rustyoldrake/Character-Cartridges-Embodied-Identity
"""I will describe the layout of the dataset. The CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes  with 6000 images per class. There are 50000 training images and 10000 test images.  The dataset is divided into five training batches and one test batch  each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order  but some training batches may contain more images from one class than another. Between them  the training batches contain exactly 5000 images from each class. The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. ""Automobile"" includes sedans  SUVs  things of that sort. ""Truck"" includes only big trucks. Neither includes pickup trucks. The archive contains the files data_batch_1  data_batch_2  ...  data_batch_5  as well as test_batch. For each batch files: 	data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 color image. The first 1024 entries contain the red channel values  the next 1024 the green  and the final 1024 the blue. The image is stored in row-major order  so that the first 32 entries of the array are the red channel values of the first row of the image. 	labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data. The dataset contains another file  called batches.meta. It too contains a Python dictionary object. It has the following entries: 	label_names -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example  label_names[0] == ""airplane""  label_names[1] == ""automobile""  etc.  """;General;https://github.com/Xinyi6/CIFAR10-CNN-by-Keras
"""1. Create your virtual enviroment     - `pip install virtualenv`     - `virtualenv venv`     - `source venv/bin/activate`  2. Install dependencies     - `pip3 install pyqtgraph` (for fast plotting)     - `pip3 install pyqt5` (for fast plotting)     - `pip3 install numpy`     - `pip3 install gym` (open ai gym)     - Ensure you have cuda 9: https://www.tensorflow.org/install/gpu     - `pip3 install tensorflow-gpu==1.9.0` (cuda version 9.0)   3. Install mgym     - A local copy has been included in this repo      - https://github.com/cjm715/mgym     - `cd mgym`     - `pip install -e .`     - `cd ..`     - It should be installed  4. Install maddpg     - A local copy has been included in this repo     - https://github.com/openai/maddpg     - `cd maddpg`     - `pip install -e .`     - `cd ..`     - It should be installed    5. Test our installs:     - I have included a few test scripts that have minimal example.     - Run each test script and see if you get any loading errors     - `python3 test_mgym.py`     - `python3 test_maddpg.py`     - `python3 test_tensorflow.py`    """;Reinforcement Learning;https://github.com/goldbattle/snakes_mal
"""To get started quickly  we provide a colab link above. Otherwise  you can clone this repo and follow the below instructions.    conda env create -f environment.yml  conda activate autoint   cd experiment_scripts   To run AutoInt for neural rendering  first set up the conda environment with  conda env create -f environment.yml  conda activate autoint   Finally  use the provided config files in the experiment_scripts/configs folder to train on these datasets. For example  to train on a NeRF Blender dataset  run the following   """;Computer Vision;https://github.com/computational-imaging/automatic-integration
"""1. Initialize the submodules using ``` git submodule init git submodule update ``` 2. Compile with `make` 3. Prepare a training corpus. As a preprocessing  we replaced all whitespaces with '␣' (U+2423 Open Box Unicode Character) 4. Train embeddings (See `train.sh`) 5. Load the learned compositional n-gram embeddings and Use it (See `scne.py`)   """;Natural Language Processing;https://github.com/kdrl/SCNE
"""NEMATODE is a light-weight neural machine translation toolkit built around the [transformer](https://arxiv.org/pdf/1706.03762.pdf) model. As the name suggests  it was originally derived from the [Nematus](https://github.com/EdinburghNLP/nematus) toolkit and eventually deviated from Nematus into a stand-alone project  by adopting the transformer model and a custom data serving pipeline. Many of its components (most notably the transformer implementation) were subsequently merged into Nematus.   On one Nvidia GeForce GTX Titan X (Pascal) GPU with CUDA 9.0  our transformer-BASE implementation achieves the following training speeds:   | --summary_dir PATH | directory for saving summaries (default: same as --save_to) |   | --valid_source_dataset PATH | source validation corpus (default: None) |   To train a transformer model  modify the provided example training script - `example_training_script.sh` - as required.   """;Natural Language Processing;https://github.com/demelin/nematode
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/Shraddha2013/darknetyolo3
"""The package provides the following Modules:   The following modules and criterions can be used to implement the REINFORCE algorithm :   However  dpnn can be used without dp (for e.g. you can use it with optim)    clone = mlp:clone()   <a name='nn.Serial'></a>   <a name='nn.Inception'></a>   <a name='nn.ReverseTable'></a>   <a name='nn.Clip'></a>   <a name='nn.SpatialUniformCrop'></a>   <a name='nn.SpatialGlimpse'></a>   <a name='nn.SpatialRegionDropout'></a>   <a name='nn.FireModule'></a>   <a name='nn.SpatialFeatNormalization'></a>   <a name='nn.SpatialBinaryConvolution'></a>   <a name='nn.SimpleColorTransform'></a>   <a name='nn.PCAColorTransform'></a>   <a name='nn.Kmeans'></a>         bestKm = km:clone()   <a name='nn.ModuleCriterion'></a>   <a name='nn.NCEModule'></a>   be computed. The NCEModule can do this by switching on the following :   Furthermore  to simulate Linear + LogSoftMax instead  one need only add the following to the above:   <a name='nn.NCECriterion'></a>   <a name='nn.Reinforce'></a>   The criterion will normally be responsible for the following formula :   <a name='nn.ReinforceBernoulli'></a>   <a name='nn.ReinforceNormal'></a>   <a name='nn.ReinforceGamma'></a>   <a name='nn.ReinforceCategorical'></a>   (ref. A) for a Categorical (i.e. Multinomial with one sample) probability distribution.   <a name='nn.VRClassReward'></a>   <a name='nn.BinaryClassReward'></a>   <a name='nn.BLR'></a>   <a name='nn.SpatialBLR'></a>   [Sagar Waghmare](https://github.com/sagarwaghmare69) wrote a nice [tutorial](tutorials/ladder.md) on how to use dpnn with nngraph to reproduce the [Lateral Connections in Denoising Autoencoders Support Supervised Learning](http://arxiv.org/pdf/1504.08215.pdf).  A brief (1 hours) overview of Torch7  which includes some details about __dpnn__  is available via this [NVIDIA GTC Webinar video](http://on-demand.gputechconf.com/gtc/2015/webinar/torch7-applied-deep-learning-for-vision-natural-language.mp4). In any case  this presentation gives a nice overview of Logistic Regression  Multi-Layer Perceptrons  Convolutional Neural Networks and Recurrent Neural Networks using Torch7.    <a name='nn.Module'></a>  """;Computer Vision;https://github.com/Element-Research/dpnn
"""Example: ./train --cuda   Example: ./test --cuda   See more under the _output_ directory  __High resolution / Low resolution / Recovered High Resolution__  ![Original doggy](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/41.png) <img src=""https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/41.png"" alt=""Low res doggy"" width=""96"" height=""96""> ![Generated doggy](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/41.png)  ![Original woman](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/38.png) <img src=""https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/38.png"" alt=""Low res woman"" width=""96"" height=""96""> ![Generated woman](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/38.png)  ![Original hair](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/127.png) <img src=""https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/127.png"" alt=""Low res hair"" width=""96"" height=""96""> ![Generated hair](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/127.png)  ![Original sand](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/72.png) <img src=""https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/72.png"" alt=""Low res sand"" width=""96"" height=""96""> ![Generated sand](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/72.png)  """;Computer Vision;https://github.com/fengye-lu/PyTorch-SRGAN
"""This demo is a Swift program that uses a pre-trained Inception model (http://arxiv.org/abs/1512.00567).  It can load the prebuilt model into a Perfect TensorFlow Session object  like this:  ``` swift let g = try TF.Graph() let def = try TF.GraphDef(serializedData: model) try g.import(definition: def) ```  Accompanied by this model  a known object name list also would be loaded into memory if success.  ``` swift try fTag.open(.read) let lines = try fTag.readString() tags = lines.utf8.split(separator: 10).map { String(describing: $0) } // the tags should be looks like this if success: // tags = [""dummy""  ""kit fox""  ""English setter""  ""Siberian husky"" ...] ```  Once received a picture from client  it will decode the picture (in jpeg format) and normalize it into a specific form:  ``` swift   public func constructAndExecuteGraphToNormalizeImage(imageBytes: Data) throws -> TF.Tensor {     let H:Int32 = 224     let W:Int32 = 224     let mean:Float = 117     let scale:Float = 1     let input = try g.constant(name: ""input2""  value: imageBytes)     let batch = try g.constant( name: ""make_batch""  value: Int32(0))     let scale_v = try g.constant(name: ""scale""  value: scale)     let mean_v = try g.constant(name: ""mean""  value: mean)     let size = try g.constantArray(name: ""size""  value: [H W])     let jpeg = try g.decodeJpeg(content: input  channels: 3)     let cast = try g.cast(value: jpeg  dtype: TF.DataType.dtFloat)     let images = try g.expandDims(input: cast  dim: batch)     let resizes = try g.resizeBilinear(images: images  size: size)     let subbed = try g.sub(x: resizes  y: mean_v)     let output = try g.div(x: subbed  y: scale_v)     let s = try g.runner().fetch(TF.Operation(output)).run()     guard s.count > 0 else { throw TF.Panic.INVALID }     return s[0]   }//end normalize ```  Then you can run a TensorFlow session from this picture input:  ``` swift let result = try g.runner().feed(""input""  tensor: image).fetch(""output"").run() ```  The result is actually a possibility array which matches the known object name list  i.e.  each object in this name list will have a possibility prediction in the corresponding array slot. So checking the max possibility throughout the array may get the most possible object that the input image could be:  ``` swift public func match(image: Data) throws -> (Int  Int) {     let normalized = try constructAndExecuteGraphToNormalizeImage(imageBytes: image)     let possibilities = try executeInceptionGraph(image: normalized)     guard let m = possibilities.max()  let i = possibilities.index(of: m) else {       throw TF.Panic.INVALID     }//end guard     return (i  Int(m * 100))   } ```  The final step is translating the result object index into the tag name of the object and sending it back to the client:  ``` swift let tag = tags[result.0]       let p = result.1       response.setHeader(.contentType  value: ""text/json"")         .appendBody(string: ""{\""value\"": \""Is it a \(tag)? (Possibility: \(p)%)\""}"")         .completed() ```   <p align=""center"">     <a href=""http://perfect.org/get-involved.html"" target=""_blank"">         <img src=""http://perfect.org/assets/github/perfect_github_2_0_0.jpg"" alt=""Get Involed with Perfect!"" width=""854"" />     </a> </p>  <p align=""center"">     <a href=""https://github.com/PerfectlySoft/Perfect"" target=""_blank"">         <img src=""http://www.perfect.org/github/Perfect_GH_button_1_Star.jpg"" alt=""Star Perfect On Github"" />     </a>       <a href=""http://stackoverflow.com/questions/tagged/perfect"" target=""_blank"">         <img src=""http://www.perfect.org/github/perfect_gh_button_2_SO.jpg"" alt=""Stack Overflow"" />     </a>       <a href=""https://twitter.com/perfectlysoft"" target=""_blank"">         <img src=""http://www.perfect.org/github/Perfect_GH_button_3_twit.jpg"" alt=""Follow Perfect on Twitter"" />     </a>       <a href=""http://perfect.ly"" target=""_blank"">         <img src=""http://www.perfect.org/github/Perfect_GH_button_4_slack.jpg"" alt=""Join the Perfect Slack"" />     </a> </p>  <p align=""center"">     <a href=""https://developer.apple.com/swift/"" target=""_blank"">         <img src=""https://img.shields.io/badge/Swift-5.2-orange.svg?style=flat"" alt=""Swift 5.2"">     </a>     <a href=""https://developer.apple.com/swift/"" target=""_blank"">         <img src=""https://img.shields.io/badge/Platforms-OS%20X%20%7C%20Linux%20-lightgray.svg?style=flat"" alt=""Platforms OS X | Linux"">     </a>     <a href=""http://perfect.org/licensing.html"" target=""_blank"">         <img src=""https://img.shields.io/badge/License-Apache-lightgrey.svg?style=flat"" alt=""License Apache"">     </a>     <a href=""http://twitter.com/PerfectlySoft"" target=""_blank"">         <img src=""https://img.shields.io/badge/Twitter-@PerfectlySoft-blue.svg?style=flat"" alt=""PerfectlySoft Twitter"">     </a>     <a href=""http://perfect.ly"" target=""_blank"">         <img src=""http://perfect.ly/badge.svg"" alt=""Slack Status"">     </a> </p>  Perfect TensorFlow Server Example of Computer Vision  This repository demonstrates how to apply Perfect TensorFlow  a Swift library of Machine Learning  into a Web Service. If you are not familiar with Perfect  please try [Perfect Template Server](https://github.com/PerfectlySoft/PerfectTemplate.git) first.   """;Computer Vision;https://github.com/PerfectExamples/Perfect-TensorFlow-Demo-Vision
"""![Alt desc](http://colorizer.pythonanywhere.com/static/images/example.jpg)   """;General;https://github.com/piyushverma001/Colorizer
"""---    """;Computer Vision;https://github.com/J-woooo/Mask_Remover
"""To install  simply run:  pip install positional-encodings   ```python3 import torch from positional_encodings import PositionalEncoding1D  PositionalEncoding2D  PositionalEncoding3D  p_enc_1d = PositionalEncoding1D(10) x = torch.zeros((1 6 10)) print(p_enc_1d(x).shape) #: (1  6  10)  p_enc_2d = PositionalEncoding2D(8) y = torch.zeros((1 6 2 8)) print(p_enc_2d(y).shape) #: (1  6  2  8)  p_enc_3d = PositionalEncoding3D(11) z = torch.zeros((1 5 6 4 11)) print(p_enc_3d(z).shape) #: (1  5  6  4  11) ```  And for tensors of the form `(batchsize  ch  x)`  etc:  ```python3 import torch from positional_encodings import PositionalEncodingPermute1D  PositionalEncodingPermute2D  PositionalEncodingPermute3D  p_enc_1d = PositionalEncodingPermute1D(10) x = torch.zeros((1 10 6)) print(p_enc_1d(x).shape) #: (1  10  6)  p_enc_2d = PositionalEncodingPermute2D(8) y = torch.zeros((1 8 6 2)) print(p_enc_2d(y).shape) #: (1  8  6  2)  p_enc_3d = PositionalEncodingPermute3D(11) z = torch.zeros((1 11 5 6 4)) print(p_enc_3d(z).shape) #: (1  11  5  6  4) ```   """;Natural Language Processing;https://github.com/tatp22/multidim-positional-encoding
"""Usage instructions found here: [user manual page](USAGE.md).   """;General;https://github.com/NeuralVFX/pix2pix
"""* [5]https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a   visualise image that can maximise one class score   """;Computer Vision;https://github.com/RuoyuGuo/Visualising-Image-Classification-Models-and-Saliency-Maps
"""First install [PyTorch and Torchvision](https://pytorch.org/get-started/locally).   Then clone this repository and run one of the following commands: ```bash #: If you just want to use Lightnet pip install brambox   #: Optional (needed for training) pip install .  #: If you want to develop Lightnet pip install -r develop.txt ``` > This project is python 3.6 and higher so on some systems you might want to use 'pip3.6' instead of 'pip'   [Click Here](https://eavise.gitlab.io/lightnet) for the API documentation and guides on how to use this library.   The _examples_ folder contains code snippets to train and test networks with lightnet. For examples on how to implement your own networks  you can take a look at the files in _lightnet/models_. >If you are using a different version than the latest  >you can generate the documentation yourself by running `make clean html` in the _docs_ folder. >This does require some dependencies  like Sphinx. >The easiest way to install them is by using the __-r develop.txt__ option when installing lightnet.   """;Computer Vision;https://github.com/eavise-kul/lightnet
"""Use python &gt;= 3.6 and install the requirement with  pip install -r requirements.txt   """;Reinforcement Learning;https://github.com/JohannesAck/tf2multiagentrl
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;Computer Vision;https://github.com/DaVran369/face
"""    brew install boost cmake zlib           sudo apt install cmake g++ libboost-dev libboost-program-options-dev libboost-filesystem-dev opencl-headers ocl-icd-libopencl1 ocl-icd-opencl-dev zlib1g-dev       You need a PC with a GPU  i.e. a discrete graphics card made by NVIDIA or AMD    Follow the instructions below to compile the leelaz and autogtp binaries in   contributing instructions below.  Contributing will start when you run autogtp.   If you are on macOS  Leela Zero is available through Homebrew  the de facto standard  package manager. You can install it with:  brew install leela-zero  If you are on Unix  you have to compile the program yourself. Follow   git clone https://github.com/leela-zero/leela-zero  cd leela-zero  git submodule update --init --recursive   cmake --build .   git clone https://github.com/leela-zero/leela-zero  cd leela-zero  git submodule update --init --recursive   cmake --build .   git clone https://github.com/leela-zero/leela-zero  cd leela-zero  git submodule update --init --recursive  cd msvc   to the Visual Studio version you have.  For Windows  you can use a release package  see ""I want to help"".   This requires a working installation of TensorFlow 1.4 or later:   [ ] Improve GPU batching in the search.   [ ] CUDA specific version using cuDNN or cuBLAS.            mkdir build && cd build                mkdir build && cd build                ./autogtp/autogtp    Leela Zero is not meant to be used directly. You need a graphical interface for it  which will interface with Leela Zero through the GTP protocol.  The engine supports the [GTP protocol  version 2](https://www.lysator.liu.se/~gunnar/gtp/gtp2-spec-draft2/gtp2-spec.html).  [Lizzie](https://github.com/featurecat/lizzie/releases) is a client specifically for Leela Zero which shows live search probilities  a win rate graph  and has an automatic game analysis mode. Has binaries for Windows  Mac  and Linux.  [Sabaki](http://sabaki.yichuanshen.de/) is a very nice looking GUI with GTP 2 capability.  [LeelaSabaki](https://github.com/SabakiHQ/LeelaSabaki) is modified to show variations and winning statistics in the game tree  as well as a heatmap on the game board.  [GoReviewPartner](https://github.com/pnprog/goreviewpartner) is a tool for automated review and analysis of games using bots (saved as .rsgf files)  Leela Zero is supported.  A lot of go software can interface to an engine via GTP  so look around.  Add the --gtp commandline option on the engine command line to enable Leela Zero's GTP support. You will need a weights file  specify that with the -w option.  All required commands are supported  as well as the tournament subset  and ""loadsgf"". The full set can be seen with ""list_commands"". The time control can be specified over GTP via the time\_settings command. The kgs-time\_settings extension is also supported. These have to be supplied by the GTP 2 interface  not via the command line!   At the end of the game  you can send Leela Zero a ""dump\_training"" command  followed by the winner of the game (either ""white"" or ""black"") and a filename  e.g:      dump_training white train.txt  This will save (append) the training data to disk  in the format described below  and compressed with gzip.  Training data is reset on a new game.   """;Reinforcement Learning;https://github.com/leela-zero/leela-zero
"""1. Clone repository in the working directory    ```   cd /<path_to_working_dir>   git clone https://github.com/opencv/openvino_training_extensions.git   ```  2. Install prerequisites    ```   sudo apt-get install libturbojpeg python3-tk python3-pip virtualenv   ```   1. install nvidia driver which support at least cuda 10.0 2. download & install :snake: [anaconda](https://www.anaconda.com/products/individual ""anaconda"") ; set anaconda PATH (may check 'set Path as default' on Windows installer)  ``` conda create -n tf_gpu python=3.6 conda activate tf_gpu	 conda install -c anaconda tensorflow-gpu=1.15 conda install -c conda-forge opencv=3.4 conda install -c anaconda cython conda install -c conda-forge imgaug git clone https://github.com/czero69/acomoeye-NN.git ```   :cd: Download Acomo-14 dataset.   :small_orange_diamond: To export openvino model (you must install openvino environment  see below):   cd ./model_acomo_basic   """;Natural Language Processing;https://github.com/czero69/acomoeye-NN
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/oorrppp2/darknet-pose
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/meizi1114/bert
"""- numpy  - opencv-pyhton (version 4.0)   download the pretrained model from insight-face <a href=""https://github.com/AIInAi/tf-insightface/tree/master/pretrained"">here</a> and store it in the /pretrained directory  run in order:  - train_init.py - train_new_face.py (avec comme argument le nom de la personne que vous voulez identifier) - face_recognition_.py    """;General;https://github.com/gregoiredervaux/face_recognition
"""An example diagram of our Cluster-NMS  where X denotes IoU matrix which is calculated by `X=jaccard(boxes boxes).triu_(diagonal=1) > nms_thresh` after sorted by score descending. (Here use 0 1 for visualization.)  <img src=""cluster-nms01.png"" width=""1150px""/> <img src=""cluster-nms02.png"" width=""1150px""/>  The inputs of NMS are `boxes` with size [n 4] and `scores` with size [80 n]. (take coco as example)  There are two ways for NMS. One is that all classes have the same number of boxes. First  we use top k=200 to select the top 200 detections for every class. Then `boxes` will be [80 m 4]  where m<=200. Do Cluster-NMS and keep the boxes with `scores>0.01`. Finally  return top 100 boxes across all classes.  The other approach is that different classes have different numbers of boxes. First  we use a score threshold (e.g. 0.01) to filter out most low score detection boxes. It results in the number of remaining boxes in different classes may be different. Then put all the boxes together and sorted by score descending. (Note that the same box may appear more than once  because its scores of multiple classes are greater than the threshold 0.01.) Adding offset for all the `boxes` according to their class labels. (use `torch.arange(0 80)`.) For example  since the coordinates (x1 y1 x2 y2) of all the boxes are on interval (0 1). By adding offset  if a box belongs to class 61  its coordinates will on interval (60 61). After that  the IoU of boxes belonging to different classes will be 0. (because they are treated as different clusters.) Do Cluster-NMS and return top 100 boxes across all classes. (For this method  please refer to another our repository https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/detection/detection.py)   In order to use YOLACT++  make sure you compile the DCNv2 code.   - Clone this repository and enter it:    ```Shell    git clone https://github.com/Zzh-tju/CIoU.git    cd yolact    ```  - Set up the environment using one of the following methods:    - Using [Anaconda](https://www.anaconda.com/distribution/)      - Run `conda env create -f environment.yml`    - Manually with pip      - Set up a Python3 environment (e.g.  using virtenv).      - Install [Pytorch](http://pytorch.org/) 1.0.1 (or higher) and TorchVision.      - Install some other packages:        ```Shell        #: Cython needs to be installed before pycocotools        pip install cython        pip install opencv-python pillow pycocotools matplotlib         ```  - If you'd like to train YOLACT  download the COCO dataset and the 2014/2017 annotations. Note that this script will take a while and dump 21gb of files into `./data/coco`.    ```Shell    sh data/scripts/COCO.sh    ```  - If you'd like to evaluate YOLACT on `test-dev`  download `test-dev` with this script.    ```Shell    sh data/scripts/COCO_test.sh    ```  - If you want to use YOLACT++  compile deformable convolutional layers (from [DCNv2](https://github.com/CharlesShang/DCNv2/tree/pytorch_1.0)).    Make sure you have the latest CUDA toolkit installed from [NVidia's Website](https://developer.nvidia.com/cuda-toolkit).    ```Shell    cd external/DCNv2    python setup.py build develop    ```   YOLOv3-pytorch https://github.com/Zzh-tju/ultralytics-YOLOv3-Cluster-NMS   SSD-pytorch https://github.com/Zzh-tju/DIoU-SSD-pytorch   YOLO v3 https://github.com/Zzh-tju/DIoU-darknet  SSD https://github.com/Zzh-tju/DIoU-SSD-pytorch  Faster R-CNN https://github.com/Zzh-tju/DIoU-pytorch-detectron   : To output a coco json file for test-dev  make sure you have test-dev downloaded from above and go   By default  we train on COCO. Make sure to download the entire dataset using the commands above.   Our paper is accepted by **IEEE Transactions on Cybernetics (TCYB)**.   An example diagram of our Cluster-NMS  where X denotes IoU matrix which is calculated by `X=jaccard(boxes boxes).triu_(diagonal=1) > nms_thresh` after sorted by score descending. (Here use 0 1 for visualization.)  <img src=""cluster-nms01.png"" width=""1150px""/> <img src=""cluster-nms02.png"" width=""1150px""/>  The inputs of NMS are `boxes` with size [n 4] and `scores` with size [80 n]. (take coco as example)  There are two ways for NMS. One is that all classes have the same number of boxes. First  we use top k=200 to select the top 200 detections for every class. Then `boxes` will be [80 m 4]  where m<=200. Do Cluster-NMS and keep the boxes with `scores>0.01`. Finally  return top 100 boxes across all classes.  The other approach is that different classes have different numbers of boxes. First  we use a score threshold (e.g. 0.01) to filter out most low score detection boxes. It results in the number of remaining boxes in different classes may be different. Then put all the boxes together and sorted by score descending. (Note that the same box may appear more than once  because its scores of multiple classes are greater than the threshold 0.01.) Adding offset for all the `boxes` according to their class labels. (use `torch.arange(0 80)`.) For example  since the coordinates (x1 y1 x2 y2) of all the boxes are on interval (0 1). By adding offset  if a box belongs to class 61  its coordinates will on interval (60 61). After that  the IoU of boxes belonging to different classes will be 0. (because they are treated as different clusters.) Do Cluster-NMS and return top 100 boxes across all classes. (For this method  please refer to another our repository https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/detection/detection.py)   """;Computer Vision;https://github.com/Zzh-tju/CIoU
"""If you are testing this on your own machine I would recommend you do the setup in a virtual environment  as not to affect the rest of your files.  In Python3 you can set up a virtual environment with  ```bash python3 -m venv /path/to/new/virtual/environment ```  Or by installing virtualenv with pip by doing ```bash pip3 install virtualenv ``` Then creating the environment with ```bash virtualenv venv ``` and finally activating it with ```bash source venv/bin/activate ```  You must have Python3  Install the requirements with: ```bash pip3 install -r requirements.txt ```   You can do this with   The data for SQuAD can be downloaded with the following links and should be saved in a $SQUAD_DIR directory.   """;Natural Language Processing;https://github.com/askaydevs/distillbert-qa
"""To run a VAE model with 3 layers of stochastic units  each connected by a two-layer MLP:  *VAE: X->MLP->Z1->MLP->Z2->MLP->Z3->MLP->Z2->MLP->Z1->MLP->Xrecon*  ``` python run_models.py \ 	-lr 0.00020 \ 	-modeltype VAE \ 	-batch_size 256 \ 	-dataset mnistresample \ 	-mlp_layers 2 \ 	-latent_sizes 64 32 16 \ 	-hidden_sizes 512 256 128 \ 	-nonlin_dec leaky_rectify \ 	-nonlin_enc leaky_rectify \ 	-only_mu_up True \ 	-eq_samples 1 \ 	-iw_samples 1 \ 	-ramp_n_samples True \ 	-batch_norm True \ 	-batch_norm_output False \ 	-temp_start 0.00 -temp_epochs 200 \ 	-num_epochs 2000 -eval_epochs 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 \ 	-ladder_share_params False \ 	-only_mu_up True \ 	-lv_eps_z 1e-5 \ 	-lv_eps_out 1e-5 \ 	-outfolder results/mnistresample_VAE ```  Corresponding ladderVAE model  ``` python run_models.py \ 	-lr 0.00020 \ 	-modeltype ladderVAE \ 	-batch_size 256 \ 	-dataset mnistresample \ 	-mlp_layers 2 \ 	-latent_sizes 64 32 16 \ 	-hidden_sizes 512 256 128 \ 	-nonlin_dec leaky_rectify \ 	-nonlin_enc leaky_rectify \ 	-only_mu_up True \ 	-eq_samples 1 \ 	-iw_samples 1 \ 	-ramp_n_samples True \ 	-batch_norm True \ 	-batch_norm_output False \ 	-temp_start 0.00 -temp_epochs 200 \ 	-num_epochs 2000 -eval_epochs 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 \ 	-ladder_share_params False \ 	-only_mu_up True \ 	-lv_eps_z 1e-5 \ 	-lv_eps_out 1e-5 \ 	-outfolder results/mnistresample_ladderVAE ```  """;Computer Vision;https://github.com/casperkaae/LVAE
"""Run ```pip3 install git+https://github.com/dbaumgarten/FToDTF.git```   The programm is now installed system-wide. You can now import the package ftodtf in python3 and run the cli-command ```fasttext <optional args>```   """;Natural Language Processing;https://github.com/dbaumgarten/FToDTF
"""All the models behind the following demos are trained on the datasets mentioned in [our paper](https://arxiv.org/pdf/2011.11961.pdf).    """;Computer Vision;https://github.com/ZHKKKe/MODNet
"""- Download [FFHQ](https://github.com/NVlabs/ffhq-dataset) and put the images to `../datasets/FFHQ/imgs1024` - Download parsing masks (`512x512`) [HERE](https://drive.google.com/file/d/1eQwO8hKcaluyCnxuZAp0eJVOdgMi30uA/view?usp=sharing) generated by the pretrained FPN and put them to `../datasets/FFHQ/masks512`.  *Note: you may change `../datasets/FFHQ` to your own path. But images and masks must be stored under `your_own_path/imgs1024` and `your_own_path/masks512` respectively.*   - Ubuntu 18.04 - CUDA 10.1   - Clone this repository     ```     git clone https://github.com/chaofengc/PSFR-GAN.git     cd PSFR-GAN     ``` - Python 3.7  install required packages by `pip3 install -r requirements.txt`     - You can use --gpus to specify how many GPUs to use  &lt;=0 means running on CPU. The program will use GPU with the most available memory. Set CUDA_VISIBLE_DEVICE to specify the GPU if you do not want automatic GPU selection.   - It needs at least 8GB memory to train with batch_size=1.  You may also train your own FPN and generate masks for the HQ images by yourself with the following steps:    Train FPN with the following commmand   Generate parsing masks with your own FPN using the following command:   """;Computer Vision;https://github.com/chaofengc/PSFRGAN
"""Wrap modules which you want to use for mixup using MixupModule   """;General;https://github.com/shivamsaboo17/ManifoldMixup
"""<a name=""1""/>   <a name=""2""/>   Extension 是否可以把不同source的gradient投射到某一个axis，而不是直接zero。    <a name=""3""/>   <a name=""4""/>   <a name=""5""/>   <a name=""6""/>  <a name=""8""/>   <a name=""9""/>   <a name=""11""/>   <a name=""13""/>  <a name=""0""/>   """;General;https://github.com/changliu816/CV-paper-review
"""The goal of **semantic segmentation** is to identify objects  like cars and dogs  in an image by labelling the corresponding groups of pixels according to their classes. For an introduction  see <a href=""https://nanonets.com/blog/semantic-image-segmentation-2020/"">this article</a>. As an example  below is an image and its labelled pixels.  | <img src=""assets/rider.jpg"" alt=""biker"" width=400> | <img src=""assets/rider_label.png"" alt=""true label"" width=400> | |:---:|:---:| | Image | True label |  A **fully convolutional network (FCN)** is an artificial neural network that performs semantic segmentation.  The bottom layers of a FCN are those of a convolutional neural network (CNN)  usually taken from a pre-trained network like VGGNet or GoogLeNet. The purpose of these layers is to perform classification on subregions of the image. The top layers of a FCN are **transposed convolution/deconvolution** layers  which upsample the results of the classification to the resolution of the original image. This gives us a label for each pixel. When upsampling  we can also utilize the intermediate layers of the CNN to improve the accuracy of the segmentation. For an introduction  see <a href=""https://nanonets.com/blog/how-to-do-semantic-segmentation-using-deep-learning/"">this article</a>.  The <a href=""http://host.robots.ox.ac.uk/pascal/VOC/"">Pascal VOC project</a> is a dataset containing images whose pixels have been labeled according to 20 classes (excluding the background)  which include aeroplanes  cars  and people. We will be performing semantic segmentation according to this dataset.   """;Computer Vision;https://github.com/kevinddchen/Keras-FCN
"""```bash git clone git@github.com:philipperemy/keras-tcn.git cd keras-tcn virtualenv -p python3.6 venv source venv/bin/activate pip install -r requirements.txt #: change to tensorflow if you dont have a gpu. pip install . --upgrade #: install it as a package. ```  Note: Only compatible with Python 3 at the moment. Should be almost compatible with python 2.   pip install keras-tcn   Installation (Python 3)   """;Sequential;https://github.com/ZTianle/keras-tcn-solar
"""``` python setup.py ```   """;General;https://github.com/metataro/DirectFeedbackAlignment
"""We provide both the Windows and LINUX versions. To compile the souce codes  some external packages are required  which are used to generate random numbers for the edge-sampling algorithm in the LINE model. For Windows version  the BOOST package is used and can be downloaded at http://www.boost.org/; for LINUX  the GSL package is used and can be downloaded at http://www.gnu.org/software/gsl/   To run the script  users first need to compile the evaluation codes by running make.sh in the folder ""evaluate"". Afterwards  we can run train_youtube.bat or train_youtube.sh to run the whole pipeline.   """;Graphs;https://github.com/tangjianpku/LINE
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`  2. Download VOC2007 and VOC2012 dataset. By default  we assume the data is stored in `$HOME/data/`   ```Shell   #: Download the data.   cd $HOME/data   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar   #: Extract the data.   tar -xvf VOCtrainval_11-May-2012.tar   tar -xvf VOCtrainval_06-Nov-2007.tar   tar -xvf VOCtest_06-Nov-2007.tar   ```  3. Create the LMDB file.   ```Shell   cd $CAFFE_ROOT   #: Create the trainval.txt  test.txt  and test_name_size.txt in data/VOC0712/   ./data/VOC0712/create_list.sh   #: You can modify the parameters in create_data.sh if needed.   #: It will create lmdb files for trainval and test with encoded original image:   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb   #: and make soft links at examples/VOC0712/   ./data/VOC0712/create_data.sh   ```   1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`   ```Shell   git clone https://github.com/weiliu89/caffe.git   cd caffe   git checkout ssd   ```  2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.   ```Shell   #: Modify Makefile.config according to your Caffe installation.   cp Makefile.config.example Makefile.config   make -j8   #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.   make py   make test -j8   #: (Optional)   make runtest -j8   ```   COCO<sup>[1]</sup>: SSD300*  SSD512*  07+12+COCO: SSD300*  SSD512*  07++12+COCO: SSD300*  SSD512*  COCO models:   """;Computer Vision;https://github.com/jack16888/caffessd
"""``` #: Example of using tf.nn.conv2d conv = tf.nn.conv2d(x  w  strides  padding) relu = tf.nn.relu(conv + bias)  #: conv2d_cosnorm conv = conv2d_cosnorm(x  w  strides  padding) relu = tf.nn.relu(conv) #: No bias needed ```  ``` #: Example of using Keras from keras.layers import Conv2d model.add(Conv2D(64  (3  3)))  #: conv2d_cosnorm from sim_layer import Norm_Conv2d as Conv2D model.add(Conv2D(64  (3  3))) ```   """;General;https://github.com/iwyoo/tf_conv_cosnorm
"""For the experiment  we will use Pommerman environment. This has relatively strict constraints on environment settings and simple to deploy algorithms.   """;Reinforcement Learning;https://github.com/tegg89/magnet
"""This work builds on the foundations laid by Irwan Bello in ""LambdaNetworks: Modeling long-range interactions without attention""  [(Bello  2021)](https://arxiv.org/abs/2102.08602). Bello proposes a method where long-range interactions are modeled by layers which  transform contexts into linear functions called lambdas  in order to avoid the use of attention maps. The great advantage  of lambda layers is that they require much less compute than self-attention mechanisms according to the original paper  by Bello. This is fantastic  because it does not only provide results faster  but also saves money and has a more  favorable carbon footprint! However  Bello still uses 32 [TPUv3s](https://cloud.google.com/tpu) and the  200 GB sized ImageNet classification dataset. Therefore  we started this reproducibility project wondering: Could lambda  layers be scaled to mainstream computers while keeping its attractive properties?  In 2021 the world did not only have to deal with the [COVID-19 epidemic](https://www.who.int/emergencies/diseases/novel-coronavirus-2019)  but was struck by [chip shortages](https://www.cnbc.com/2021/02/10/whats-causing-the-chip-shortage-affecting-ps5-cars-and-more.html)  as well due to increase in consumer electronics for working at home  shut down factories in China and the rising prices of crypto-currencies.  This has decreased supply to record lows and prices to record highs. Resulting in a situation  whereby researchers  academics   and students (who are all usually on a budget) are no longer able to quickly build a cluster out of COTS (commercial off-the-shelf)  GPUs resulting in having to deal with older  less  and less efficient hardware.  No official code was released at the time of starting the project mid-March. Therefore  in order to answer the  aforementioned question  it is up to us to reproduce the paper by Bello as accurately as possible while trying to scale  it down such that it can be run on an average consumer computer.     To start using the code you can download the required Python libraries stored within requirements.txt. For that purpose    pip install -r requirements.txt   !git clone https://github.com/joigalcar3/LambdaNetworks    then run the following lines:   """;General;https://github.com/joigalcar3/LambdaNetworks
"""    brew install boost cmake           sudo apt install libboost-dev libboost-program-options-dev libboost-filesystem-dev opencl-headers ocl-icd-libopencl1 ocl-icd-opencl-dev zlib1g-dev       You need a PC with a GPU  i.e. a discrete graphics card made by NVIDIA or AMD    Follow the instructions below to compile the leelaz binary  then go into   git clone https://github.com/gcp/leela-zero  cd leela-zero  git submodule update --init --recursive  git clone https://github.com/gcp/leela-zero  cd leela-zero  git submodule update --init --recursive  git clone https://github.com/gcp/leela-zero  cd leela-zero  git submodule update --init --recursive  cd msvc   to the Visual Studio version you have.   This requires a working installation of TensorFlow 1.4 or later:   [ ] Implement GPU batching.   [ ] CUDA specific version using cuDNN or cuBLAS.            mkdir build && cd build     cmake ..     cmake --build .     ./tests     curl -O https://zero.sjeng.org/best-network     ./leelaz --weights best-network            mkdir build && cd build     cmake ..     cmake --build .     ./tests     curl -O https://zero.sjeng.org/best-network     ./leelaz --weights best-network        The engine supports the [GTP protocol  version 2](https://www.lysator.liu.se/~gunnar/gtp/gtp2-spec-draft2/gtp2-spec.html).  Leela Zero is not meant to be used directly. You need a graphical interface for it  which will interface with Leela Zero through the GTP protocol.  [Lizzie](https://github.com/featurecat/lizzie/releases) is a client specifically for Leela Zero which shows live search probilities  a win rate graph  and has an automatic game analysis mode. Has binaries for Windows  Mac  and Linux.  [Sabaki](http://sabaki.yichuanshen.de/) is a very nice looking GUI with GTP 2 capability.  [LeelaSabaki](https://github.com/SabakiHQ/LeelaSabaki) is modified to show variations and winning statistics in the game tree  as well as a heatmap on the game board.  A lot of go software can interface to an engine via GTP  so look around.  Add the --gtp commandline option on the engine command line to enable Leela Zero's GTP support. You will need a weights file  specify that with the -w option.  All required commands are supported  as well as the tournament subset  and ""loadsgf"". The full set can be seen with ""list_commands"". The time control can be specified over GTP via the time\_settings command. The kgs-time\_settings extension is also supported. These have to be supplied by the GTP 2 interface  not via the command line!   At the end of the game  you can send Leela Zero a ""dump\_training"" command  followed by the winner of the game (either ""white"" or ""black"") and a filename  e.g:      dump_training white train.txt  This will save (append) the training data to disk  in the format described below  and compressed with gzip.  Training data is reset on a new game.   """;Reinforcement Learning;https://github.com/zhoujianxing123/minigo_v17
"""` [x  fval  exitflag  output] = fmin_adam(fun  x0 <  stepSize  beta1  beta2  epsilon  nEpochSize  options>)`  `fmin_adam` is an implementation of the Adam optimisation algorithm (gradient descent with Adaptive learning rates individually on each parameter  with Momentum) from Kingma and Ba [[1]]. Adam is designed to work on stochastic gradient descent problems; i.e. when only small batches of data are used to estimate the gradient on each iteration  or when stochastic dropout regularisation is used [[2]].   ###Simple regression problem with gradients  Set up a simple linear regression problem: ![$$$y = x\cdot\phi_1 + \phi_2 + \zeta$$$](https://latex.codecogs.com/svg.latex?%5Cinline%20y%20%3D%20x%5Ccdot%5Cphi_1%20&plus;%20%5Cphi_2%20&plus;%20%5Czeta)  where ![$$$\zeta \sim N(0  0.1)$$$](https://latex.codecogs.com/svg.latex?%5Cinline%20%5Czeta%20%5Csim%20N%280%2C%200.1%29). We'll take ![$$$\phi = \left[3  2\right]$$$](https://latex.codecogs.com/svg.latex?%5Cinline%20%5Cphi%20%3D%20%5Cleft%5B3%2C%202%5Cright%5D) for this example. Let's draw some samples from this problem:  ```matlab nDataSetSize = 1000; vfInput = rand(1  nDataSetSize); phiTrue = [3 2]; fhProblem = @(phi  vfInput) vfInput .* phi(1) + phi(2); vfResp = fhProblem(phiTrue  vfInput) + randn(1  nDataSetSize) * .1; plot(vfInput  vfResp  '.'); hold; ```  <img src=""images/regression_scatter.png"" />  Now we define a cost function to minimise  which returns analytical gradients:  ```matlab function [fMSE  vfGrad] = LinearRegressionMSEGradients(phi  vfInput  vfResp)    % - Compute mean-squared error using the current parameter estimate    vfRespHat = vfInput .* phi(1) + phi(2);    vfDiff = vfRespHat - vfResp;    fMSE = mean(vfDiff.^2) / 2;        % - Compute the gradient of MSE for each parameter    vfGrad(1) = mean(vfDiff .* vfInput);    vfGrad(2) = mean(vfDiff); end ```  Initial parameters `phi0` are Normally distributed. Call the `fmin_adam` optimiser with a learning rate of 0.01.  ```matlab phi0 = randn(2  1); phiHat = fmin_adam(@(phi)LinearRegressionMSEGradients(phi  vfInput  vfResp)  phi0  0.01) plot(vfInput  fhProblem(phiHat  vfInput)  '.'); ````  Output:       Iteration   Func-count         f(x)   Improvement    Step-size     ----------   ----------   ----------   ----------   ----------           2130         4262       0.0051        5e-07      0.00013     ----------   ----------   ----------   ----------   ----------      Finished optimization.        Reason: Function improvement [5e-07] less than TolFun [1e-06].      phiHat =         2.9498         2.0273  <img src=""images/regression_fit.png"" />  ###Linear regression with minibatches  Set up a simple linear regression problem  as above.  ```matlab nDataSetSize = 1000; vfInput = rand(1  nDataSetSize); phiTrue = [3 2]; fhProblem = @(phi  vfInput) vfInput .* phi(1) + phi(2); vfResp = fhProblem(phiTrue  vfInput) + randn(1  nDataSetSize) * .1; ```  Configure minibatches. Minibatches contain random sets of indices into the data.  ```matlab nBatchSize = 50; nNumBatches = 100; mnBatches = randi(nDataSetSize  nBatchSize  nNumBatches); cvnBatches = mat2cell(mnBatches  nBatchSize  ones(1  nNumBatches)); figure; hold; cellfun(@(b)plot(vfInput(b)  vfResp(b)  '.')  cvnBatches); ``` <img src=""images/regression_minibatches.png"" />         Define the function to minimise; in this case  the mean-square error over the regression problem. The iteration index `nIter` defines which mini-batch to evaluate the problem over.  ```matlab fhBatchInput = @(nIter) vfInput(cvnBatches{mod(nIter  nNumBatches-1)+1}); fhBatchResp = @(nIter) vfResp(cvnBatches{mod(nIter  nNumBatches-1)+1}); fhCost = @(phi  nIter) mean((fhProblem(phi  fhBatchInput(nIter)) - fhBatchResp(nIter)).^2); ``` Turn off analytical gradients for the `adam` optimiser  and ensure that we permit sufficient function calls.  ```matlab sOpt = optimset('fmin_adam'); sOpt.GradObj = 'off'; sOpt.MaxFunEvals = 1e4; ```  Call the `fmin_adam` optimiser with a learning rate of `0.1`. Initial parameters are Normally distributed.  ```matlab phi0 = randn(2  1); phiHat = fmin_adam(fhCost  phi0  0.1  []  []  []  []  sOpt) ``` The output of the optimisation process (which will differ over random data and random initialisations):      Iteration   Func-count         f(x)   Improvement    Step-size     ----------   ----------   ----------   ----------   ----------            711         2848          0.3       0.0027      3.8e-06     ----------   ----------   ----------   ----------   ----------      Finished optimization.        Reason: Step size [3.8e-06] less than TolX [1e-05].      phiHat =         2.8949         1.9826       """;General;https://github.com/DylanMuir/fmin_adam
"""First  install extra packages by: pip3 install -r requirements.txt   """;Natural Language Processing;https://github.com/benywon/Chinese-GPT-2
"""Once invoked  calls def cli_main in https://github.com/pytorch/fairseq/blob/master/fairseq_cli/train.py   The main while loop involed by fairseq-train -- https://github.com/pytorch/fairseq/blob/master/fairseq_cli/train.py#L117   Issue for fine-tuning with limited resources; lots of useful tips -- https://github.com/pytorch/fairseq/issues/1413  Issue for fine-tuning with different vocab sizes -- https://github.com/pytorch/fairseq/issues/2120  Issue discussing the confusion on MAX_TOKENS in Bart for Summarization (the README is broken) -- https://github.com/pytorch/fairseq/issues/1685  Issue regarding training time  resources for BARTBase  -- https://github.com/pytorch/fairseq/issues/1651   """;Sequential;https://github.com/priyamtejaswin/multistep-retrieve-summarize
"""The Progressive GAN code repository contains a command-line tool for recreating bit-exact replicas of the datasets that we used in the paper. The tool also provides various utilities for operating on the datasets:  ``` usage: dataset_tool.py [-h] <command> ...      display             Display images in dataset.     extract             Extract images from dataset.     compare             Compare two datasets.     create_mnist        Create dataset for MNIST.     create_mnistrgb     Create dataset for MNIST-RGB.     create_cifar10      Create dataset for CIFAR-10.     create_cifar100     Create dataset for CIFAR-100.     create_svhn         Create dataset for SVHN.     create_lsun         Create dataset for single LSUN category.     create_celeba       Create dataset for CelebA.     create_celebahq     Create dataset for CelebA-HQ.     create_from_images  Create dataset from a directory full of images.     create_from_hdf5    Create dataset from legacy HDF5 archive.  Type ""dataset_tool.py <command> -h"" for more information. ```  The datasets are represented by directories containing the same image data in several resolutions to enable efficient streaming. There is a separate `*.tfrecords` file for each resolution  and if the dataset contains labels  they are stored in a separate file as well:  ``` > python dataset_tool.py create_cifar10 datasets/cifar10 ~/downloads/cifar10 > ls -la datasets/cifar10 drwxr-xr-x  2 user user         7 Feb 21 10:07 . drwxrwxr-x 10 user user        62 Apr  3 15:10 .. -rw-r--r--  1 user user   4900000 Feb 19 13:17 cifar10-r02.tfrecords -rw-r--r--  1 user user  12350000 Feb 19 13:17 cifar10-r03.tfrecords -rw-r--r--  1 user user  41150000 Feb 19 13:17 cifar10-r04.tfrecords -rw-r--r--  1 user user 156350000 Feb 19 13:17 cifar10-r05.tfrecords -rw-r--r--  1 user user   2000080 Feb 19 13:17 cifar10-rxx.labels ```  The ```create_*``` commands take the standard version of a given dataset as input and produce the corresponding `*.tfrecords` files as output. Additionally  the ```create_celebahq``` command requires a set of data files representing deltas with respect to the original CelebA dataset. These deltas (27.6GB) can be downloaded from [`datasets/celeba-hq-deltas`](https://drive.google.com/open?id=0B4qLcYyJmiz0TXY1NG02bzZVRGs).  **Note about module versions**: Some of the dataset commands require specific versions of Python modules and system libraries (e.g. pillow  libjpeg)  and they will give an error if the versions do not match. Please heed the error messages – there is **no way** to get the commands to work other than installing these specific versions.   | Feature                           | TensorFlow version                            | Original Theano version   |   Pull the Progressive GAN code repository and add it to your PYTHONPATH environment variable.  Install the required Python packages with pip install -r requirements-pip.txt   Download karras2018iclr-celebahq-1024x1024.pkl from networks/tensorflow-version and place it in the same directory as the script.   """;Computer Vision;https://github.com/thenhz/progressive_growing_of_gan
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/bgtripp/stylegan2glacier
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/RenXiangyuan/tf_bert
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;General;https://github.com/zy0851/FB-m-RCNN
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/Shraddha2013/darknett
"""https://gist.github.com/danijar/c7ec9a30052127c7a1ad169eeb83f159 (ref)   Ran on Ubuntu with Geforce GTX 1070 GPU   """;General;https://github.com/rickyHong/tfRecord-Caltech256-repl2
"""Install PyTorch 0.4   The codebase is now PyTorch 0.4 compatible for most use cases (a big shoutout to https://github.com/shawntan for a fairly comprehensive PR https://github.com/salesforce/awd-lstm-lm/pull/43). Mild readjustments to hyperparameters may be necessary to obtain quoted performance. If you desire exact reproducibility (or wish to run on PyTorch 0.3 or lower)  we suggest using an older commit of this repository. We are still working on pointer  finetune and generate functionalities.   For data setup  run ./getdata.sh.   PyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed.   """;Natural Language Processing;https://github.com/arvieFrydenlund/awd-lstm-lm
"""This is a pytorch implementation for the Visformer models. This project is based on the training code in [Deit](https://github.com/facebookresearch/deit) and the tools in [timm](https://github.com/rwightman/pytorch-image-models).   The layout of Imagenet data: ```bash /path/to/imagenet/   train/     class1/       img1.jpeg     class2/       img2.jpeg   val/     class1/       img1.jpeg     class2/       img2.jpeg ```  Clone the repository: ```bash git clone https://github.com/danczs/Visformer.git ``` Install pytorch  timm and einops: ```bash pip install -r requirements.txt ```  """;Computer Vision;https://github.com/danczs/Visformer
"""Hedwig is designed for Python 3.6 and [PyTorch](https://pytorch.org/) 0.4. PyTorch recommends [Anaconda](https://www.anaconda.com/distribution/) for managing your environment. We'd recommend creating a custom environment as follows:  ``` $ conda create --name castor python=3.6 $ source activate castor ```  And installing PyTorch as follows:  ``` $ conda install pytorch=0.4.1 cuda92 -c pytorch ```  Other Python packages we use can be installed via pip:  ``` $ pip install -r requirements.txt ```  Code depends on data from NLTK (e.g.  stopwords) so you'll have to download them.  Run the Python interpreter and type the commands:  ```python >>> import nltk >>> nltk.download() ```   $ git clone https://github.com/castorini/hedwig.git  $ git clone https://git.uwaterloo.ca/jimmylin/hedwig-data.git   cd hedwig-data/embeddings/word2vec    """;Sequential;https://github.com/castorini/hedwig
"""pytorch 0.4.1   pip install visdom   python visdom   The pretrained model refer pretrained-models.pytorch you can download it.   For training  default NVIDIA GPU.   This is the code for our papers:  - [Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression](https://arxiv.org/abs/1911.08287)  - [Enhancing Geometric Factors into Model Learning and Inference for Object Detection and Instance Segmentation](https://arxiv.org/abs/2005.03572)  ``` @Inproceedings{zheng2020distance    author    = {Zhaohui Zheng  Ping Wang  Wei Liu  Jinze Li  Rongguang Ye  Dongwei Ren}    title     = {Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression}    booktitle = {The AAAI Conference on Artificial Intelligence (AAAI)}     year      = {2020}  }  @Article{zheng2020ciou    author= {Zhaohui Zheng  Ping Wang  Dongwei Ren  Wei Liu  Rongguang Ye  Qinghua Hu  Wangmeng Zuo}    title={Enhancing Geometric Factors in Model Learning and Inference for Object Detection and Instance Segmentation}    journal={arXiv:2005.03572}    year={2020} } ```   """;Computer Vision;https://github.com/Artcs1/ssd-piou
"""``` git clone https://github.com/arkel23/PyTorch-Pretrained-ViT.git cd PyTorch-Pretrained-ViT pip install -e . python download_convert_models.py #: can modify to download different models  by default it downloads all 5 ViTs pretrained on ImageNet21k ```   ``` from pytorch_pretrained_vit import ViT  ViTConfigExtended  PRETRAINED_CONFIGS  model_name = 'B_16' def_config = PRETRAINED_CONFIGS['{}'.format(model_name)]['config'] configuration = ViTConfigExtended(**def_config) model = ViT(configuration  name=model_name  pretrained=True  load_repr_layer=False  ret_attn_scores=False) ```   """;General;https://github.com/arkel23/PyTorch-Pretrained-ViT
"""Download the datasets:  ```sh python3 -m domainbed.scripts.download \        --data_dir=./domainbed/data ```  Train a model:  ```sh python3 -m domainbed.scripts.train\        --data_dir=./domainbed/data/MNIST/\        --algorithm IGA\        --dataset ColoredMNIST\        --test_env 2 ```  Launch a sweep:  ```sh python -m domainbed.scripts.sweep launch\        --data_dir=/my/datasets/path\        --output_dir=/my/sweep/output/path\        --command_launcher MyLauncher ```  Here  `MyLauncher` is your cluster's command launcher  as implemented in `command_launchers.py`. At the time of writing  the entire sweep trains tens of thousands of models (all algorithms x all datasets x 3 independent trials x 20 random hyper-parameter choices). You can pass arguments to make the sweep smaller:  ```sh python -m domainbed.scripts.sweep launch\        --data_dir=/my/datasets/path\        --output_dir=/my/sweep/output/path\        --command_launcher MyLauncher\        --algorithms ERM DANN\        --datasets RotatedMNIST VLCS\        --n_hparams 5\        --n_trials 1 ```  After all jobs have either succeeded or failed  you can delete the data from failed jobs with ``python -m domainbed.scripts.sweep delete_incomplete`` and then re-launch them by running ``python -m domainbed.scripts.sweep launch`` again. Specify the same command-line arguments in all calls to `sweep` as you did the first time; this is how the sweep script knows which jobs were launched originally.  To view the results of your sweep:  ````sh python -m domainbed.scripts.collect_results\        --input_dir=/my/sweep/output/path ````   """;General;https://github.com/facebookresearch/DomainBed
"""    processor = DDPMixSolver(cfg_path=""your own config path"")    * run train scriptsshell script   """;Computer Vision;https://github.com/liangheming/sparse_rcnnv1
"""This repo contains tutorials covering image classification using PyTorch 1.7  torchvision 0.8  matplotlib 3.3 and scikit-learn 0.24  with Python 3.8.   To install PyTorch  see installation instructions on the [PyTorch website](pytorch.org).  The instructions to install PyTorch should also detail how to install torchvision but can also be installed via:  ``` bash pip install torchvision ```   * 1 - [Multilayer Perceptron](https://github.com/bentrevett/pytorch-image-classification/blob/master/1_mlp.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/1_mlp.ipynb)      This tutorial provides an introduction to PyTorch and TorchVision. We'll learn how to: load datasets  augment data  define a multilayer perceptron (MLP)  train a model  view the outputs of our model  visualize the model's representations  and view the weights of the model. The experiments will be carried out on the MNIST dataset - a set of 28x28 handwritten grayscale digits.  * 2 - [LeNet](https://github.com/bentrevett/pytorch-image-classification/blob/master/2_lenet.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/2_lenet.ipynb)      In this tutorial we'll implement the classic [LeNet](http://yann.lecun.com/exdb/lenet/) architecture. We'll look into convolutional neural networks and how convolutional layers and subsampling (aka pooling) layers work.  * 3 - [AlexNet](https://github.com/bentrevett/pytorch-image-classification/blob/master/3_alexnet.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/3_alexnet.ipynb)      In this tutorial we will implement [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)  the convolutional neural network architecture that helped start the current interest in deep learning. We will move on to the CIFAR10 dataset - 32x32 color images in ten classes. We show: how to define architectures using `nn.Sequential`  how to initialize the parameters of your neural network  and how to use the learning rate finder to determine a good initial learning rate.  * 4 - [VGG](https://github.com/bentrevett/pytorch-image-classification/blob/master/4_vgg.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/4_vgg.ipynb)      This tutorial will cover implementing the [VGG](https://arxiv.org/abs/1409.1556) model. However  instead of training the model from scratch we will instead load a VGG model pre-trained on the [ImageNet](http://www.image-net.org/challenges/LSVRC/) dataset and show how to perform transfer learning to adapt its weights to the CIFAR10 dataset using a technique called discriminative fine-tuning. We'll also explain how adaptive pooling layers and batch normalization works.  * 5 - [ResNet](https://github.com/bentrevett/pytorch-image-classification/blob/master/5_resnet.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/5_resnet.ipynb)      In this tutorial we will be implementing the [ResNet](https://arxiv.org/abs/1512.03385) model. We'll show how to load your own dataset  using the [CUB200](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) dataset as an example  and also how to use learning rate schedulers which dynamically alter the learning rate of your model whilst training. Specifially  we'll use the one cycle policy introduced in [this](https://arxiv.org/abs/1803.09820) paper and is now starting to be commonly used for training computer vision models.   """;Computer Vision;https://github.com/bentrevett/pytorch-image-classification
"""The code was developed on Python 3.6.4  with pytorch 0.4.0 (CUDA V8.0  CuDNN 6.0) and all experiments were run on a GeForce GTX 1080 core.   """;Sequential;https://github.com/tdmeeste/SparseSeqModels
"""Detection and tracking was carried using the OpenMMLab frameworks for each task. In this section  we give a summary on how to setup the frameworks for each task   Install MMDetection using the Getting Started   and then you need to create the COCO annotations using the convert_drone_vs_bird_to_coco.py script.   """;Computer Vision;https://github.com/KostadinovShalon/UAVDetectionTrackingBenchmark
"""If you are running experiments on MLQA  it is important that you clearly state your experimental settings.    As mentioned in the paper  some instances that cannot be answered are generated by our annotation procedure. We will release these as a separate resource shortly here.    """;Natural Language Processing;https://github.com/facebookresearch/MLQA
"""    --run-name reproduced-miniimagenet \       --use-cuda \       --run-name mini-imagenet-mtm-spsa-track \       --use-cuda \       --run-name reproduced-tieredimagenet \       --use-cuda \       --run-name tiered-imagenet-mtm-spsa \       --use-cuda \       --run-name reproduced-fc100 \       --use-cuda \       --run-name fc100-mtm-spsa-coarse \       --use-cuda \       --use-cuda \       --use-cuda \       --use-cuda \   To reproduce the results on benchmarks described in our article  use the following scripts. To vary types of the experiments  change the parameters of the scripts responsible for benchmark dataset  shot and way (e.g. miniImageNet 1-shot 5-way or CIFAR-FS 5-shot 2-way).   """;General;https://github.com/andrewbo29/mtm-meta-learning-sa
"""```bash pip install -r requirements.txt ```  If you want to use GPU  follow [these instructions](https://www.tensorflow.org/install/) to install with pip3.  Make sure Keras is using Tensorflow and you have Python 3.6.3+. Depending on your environment  you may have to run python3/pip3 instead of python/pip.   Basic Usage ------------  For training model  execute `Self-Play`  `Trainer` and `Evaluator`.  **Note**: Make sure you are running the scripts from the top-level directory of this repo  i.e. `python src/chess_zero/run.py opt`  not `python run.py opt`.   Self-Play --------  ```bash python src/chess_zero/run.py self ```  When executed  Self-Play will start using BestModel. If the BestModel does not exist  new random model will be created and become BestModel.   GPU Memory   """;Reinforcement Learning;https://github.com/Zeta36/chess-alpha-zero
"""We use [conda](https://www.anaconda.com/) to manage Python environments. To create an environment that already fulfills all package requirements of this repository  simply execute  ```console $ conda env create -f environment.yml $ conda activate hypercl_env ```   Some toy regression problems can be explored in the folder [toy_example](toy_example). Please refer to the corresponding [documentation](toy_example/README.md). Example run:  ```console $ python3 -m toy_example.train --no_cuda ```   """;Computer Vision;https://github.com/chrhenning/hypercl
"""1. Simply initial AWD-LSTM  it's a standard [`LayerRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerRNNCell). ``` from weight_drop_lstm import WeightDropLSTMCell  lstm_cell = WeightDropLSTMCell(     num_units=CELL_NUM  weight_drop_kr=WEIGHT_DP_KR       use_vd=True  input_size=INPUT_SIZE) ``` Arguments are define as follows: > `num_units`: the number of cell in LSTM layer. [ints]\ > `weight_drop_kr`: the number of steps that fast weights go forward. [int]\ > `use_vd`: If true  using variational dropout on weight drop-connect  standard dropout otherwise. [bool]\ > `input_size`: If `use_vd=True`  input_size (dimension of last channel) should be provided. [int]  The remaining keyword arguments is exactly the same as [`tf.nn.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell).   Noted that  if the weight_drop_kr is not provided or provided with 1.0  `WeightDropLSTMCell` is reducted as `LSTMCell`.  2. Insert update operation of dropout kernel to the place you want.  ``` #: By simply sess.run in each training step sess.run(lstm_cell.get_vd_update_op())  #: Or use control_dependencies vd_update_ops = lstm_cell.get_vd_update_op()  with tf.control_dependencies(vd_update_ops):     tf.train.AdamOptimizer(learning_rate).minimize(loss) ```  You can also add `get_vd_update_op()` to [`GraphKeys.UPDATE_OPS`](https://www.tensorflow.org/api_docs/python/tf/GraphKeys) when calling `WeightDropLSTMCell`.  Noted that  if you use [`control_dependencies`](https://www.tensorflow.org/api_docs/python/tf/control_dependencies)  please be careful for the order of execution.\ The variational dropout kernel should not be update before the optimizer step.    """;General;https://github.com/Janus-Shiau/awd-lstm-tensorflow
"""For an introduction on how to use this model  take a look at the [WaveNet demo notebook](https://github.com/vincentherrmann/pytorch-wavenet/blob/master/WaveNet_demo.ipynb).  You can find audio clips generated by a simple trained model in the [generated samples directory](https://github.com/vincentherrmann/pytorch-wavenet/tree/master/generated_samples)   """;Sequential;https://github.com/vincentherrmann/pytorch-wavenet
"""Model-definition is a deep learning application for fault detection in photovoltaic plants. In this repository you will find trained detection models that point out where the panel faults are by using radiometric thermal infrared pictures. In [Web-API](https://github.com/RentadroneCL/Web-API) contains a performant  production-ready reference implementation of this repository.  ![Data Flow](MLDataFlow.svg)   `python predict_ssd.py -c config.json -i /path/to/image/or/video -o /path/output/result` or `python predict_yolo.py -c config.json -i /path/to/image/or/video -o /path/output/result`  It carries out detection on the image and write the image with detected bounding boxes to the same folder.   View folder Train&Test_A/ and Train&Test_S/  example of panel anns and soiling fault anns.  Organize the dataset into 4 folders:  + train_image_folder <= the folder that contains the train images.  + train_annot_folder <= the folder that contains the train annotations in VOC format.  + valid_image_folder <= the folder that contains the validation images.  + valid_annot_folder <= the folder that contains the validation annotations in VOC format.  There is a one-to-one correspondence by file name between images and annotations. For create own data set use LabelImg code from : [https://github.com/tzutalin/labelImg](https://github.com/tzutalin/labelImg)   In the root project execute the following command to install all dependencies project   pip install -r requirements.txt   You need install Jupyter notebook to see the code example. You can find the installation documentation for the Jupyter platform  on ReadTheDocs or in github page here.  For a local installation  make sure you have pip installed and run:   pip install notebook   Before sending your pull requests  make sure you followed this list.   In ['Example_Prediction'](Code_Example/Example_prediction.ipynb) this is the example of how to implement an already trained model  it can be modified to change the model you have to use and the image in which you want to detect faults.  In ['Example Prediction AllInOne'](Code_Example/Example%20Detection%20AllInOne.ipynb) this is the example of how implement all trained model  you can use this code for predict a folder of images and have a output image with detection boxes.  In ['Example_Prediction_Orthophoto'](Code_Example/Example_prediction_Ortofoto.ipynb) this is the example of how implement all trained model  you can use this code for predict a Orthophot and have a output image with detection boxes.    `python train_ssd.py -c config.json -o /path/to/result`  or `python train_ssd.py -c config.json -o /path/to/result`  By the end of this process  the code will write the weights of the best model to file best_weights.h5 (or whatever name specified in the setting ""saved_weights_name"" in the config.json file). The training process stops when the loss on the validation set is not improved in 20 consecutive epoches.   """;Computer Vision;https://github.com/RentadroneCL/Photovoltaic_Fault_Detector
"""* Install the required dependencies:  ```javascript  pip install -r requirements.txt  ```  * [modify_data.py](https://github.com/AKASH2907/bird-species-classification/blob/master/modify_data.py) - This code is used to rename the files. <br /> For example: 10 - Type of Data Augmentation 01- Class of Bird 01 - Image Number - 100101.jpg (Image Name)  * [data_augmentation.py](https://github.com/AKASH2907/bird-species-classification/blob/master/data_augmentation/data_augmentation.py) - Various types of data augmentation used to counter the challenge of large scale variation in illumination scale  etc. and class imbalance.  * [create_validation.py](https://github.com/AKASH2907/bird-species-classification/blob/master/create_validation.py) - Used to create Validation data randomly from the augmented training data. * [gen_train_data_test_data.py](https://github.com/AKASH2907/bird-species-classification/blob/master/gen_train_data_test_data.py) - Generates X_train  Y_train  X_validation  Y_validation  X_test  Y_test * [inception_v3_finetune.py](https://github.com/AKASH2907/bird-species-classification/blob/master/inception_v3_finetune.py) - Multi-stage Training on Mask R-CNN crops generated and then on data augmented original images. * [inception_resnet_v2_finetune.py](https://github.com/AKASH2907/bird-species-classification/blob/master/inception_resnet_v2_finetune.py) - Multi-stage Training on Mask R-CNN crops generated and then on data augmented original images resized to 416x416. * [mask_rcnn/rcnn_crops.py](https://github.com/AKASH2907/bird-species-classification/blob/master/mask_rcnn/rcnn_crops.py) - Localizes bird in the images  crops and then save them for multi-stage learning. * [mask_rcnn/test_images.py](https://github.com/AKASH2907/bird-species-classification/blob/master/mask_rcnn/test_images.py) -  End-to-end model of classifying bird specie using Mask R-CNN and **ensembling** of Inception V3 and Inception ResNet V2. * [evaluate.py](https://github.com/AKASH2907/bird-species-classification/blob/master/evaluate.py) - Calculation of class-averaged precision  recall and F1-scores.   """;Computer Vision;https://github.com/AKASH2907/bird_species_classification
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/Walter-B/bert-20-classes
"""- To install  `cd` into the root directory and type `pip install -e .`  - To interactively view moving to landmark scenario (see others in ./scenarios/): `bin/interactive.py --scenario simple.py`  - Known dependencies: Python (3.5.4)  OpenAI gym (0.10.5)  numpy (1.14.5)  - To use the environments  look at the code for importing them in `make_env.py`.   """;Reinforcement Learning;https://github.com/karishnu/tf-agents-multi-particle-envs
"""cd btgym  git pull  pip install --upgrade -e .   """;General;https://github.com/Kismuz/crypto_spread_test
"""1.Download datasets   - [LSUN](http://lsun.cs.princeton.edu/2016/)   - [HWDB1.0 (Handwritten Chinese characters)](http://www.nlpr.ia.ac.cn/databases/handwriting/Download.html)  2.Convert data  ```   $ python convert_lsun.py --source_dir $SOURCE_PATH --target_dir $TARGET_PATH   $ python convert_icdar.py --source_dir $SOURCE_PATH --target_dir $TARGET_PATH ```  3.Train model  ```   $ python train_lsun.py --data_path $DATA_PATH   $ python train_chn.py --data_dir $DATA_DIR ```   """;Computer Vision;https://github.com/xudonmao/LSGAN
"""code at terminal.   """;Computer Vision;https://github.com/tatsuyaokunaga/Simpsons_Detection
"""Deployment requires building the TVM runtime code on the target embedded device (that will be used solely for running a trained and compiled model). The following instructions are taken from [this TVM tutorial](https://docs.tvm.ai/tutorials/cross_compilation_and_rpc.html#build-tvm-runtime-on-device) and have been tested on a **TX2 with CUDA-8.0 and LLVM-4.0 installed**.  First  clone the TVM repo and modify config file: ```bash git clone --recursive https://github.com/dmlc/tvm cd tvm git reset --hard ab4946c8b80da510a5a518dca066d8159473345f git submodule update --init cp cmake/config.cmake . ``` Make the following edits to the `config.cmake` file: ```cmake set(USE_CUDA OFF) -> set(USE_CUDA [path_to_cuda]) #: e.g. /usr/local/cuda-8.0/ set(USE_LLVM OFF) -> set(USE_LLVM [path_to_llvm-config]) #: e.g. /usr/lib/llvm-4.0/bin/llvm-config ```  Then build the runtime: ```bash make runtime -j2 ``` Finally  update the `PYTHONPATH` environment variable: ```bash export PYTHONPATH=$PYTHONPATH:~/tvm/python ```    bash     cd ..   On the TX2  power consumption on the main VDD_IN rail can be measured by running the following command:   """;General;https://github.com/dwofk/fast-depth
"""here. Note that    """;Natural Language Processing;https://github.com/deepmind/xquad
"""Clone the repository:  `git clone --recursive https://github.com/navganti/SIVO.git`  or  `git clone --recursive git@github.com:navganti/SIVO.git`  Ensure you use the recursive flag to initialize the submodule.   This implementation has been tested with Ubuntu 16.04.   OpenCV is used to manipulate images and features. Download and install instructions can be found here. Required version > OpenCV 3.2.  Eigen3 is used for linear algebra  specifically matrix and tensor manipulation. Required by g2o and Bayesian SegNet. Download and install instructions can be found at: http://eigen.tuxfamily.org. Required version > 3.2.0 (for Tensors).  Build caffe-segnet-cudnn7. Navigate to dependencies/caffe-segnet-cudnn7  and follow the instructions listed in the README  specifically the CMake installation steps. This process is a little involved - please follow the instructions carefully.  Ensure all other prerequisites are installed.   chmod +x build.sh   To use SIVO with the KITTI dataset  perform the following   The program can be run with the following:  ```bash ./bin/SIVO config/Vocabulary/ORBvoc.txt config/CONFIGURATION_FILE config/bayesian_segnet/PATH_TO_PROTOTXT config/bayesian_segnet/PATH_TO_CAFFEMODEL PATH_TO_DATASET_FOLDER/dataset/sequences/SEQUENCE_NUMBER ``` The parameters `CONFIGURATION_FILE`  `PATH_TO_PROTOTXT`  `PATH_TO_CAFFEMODEL`  and `PATH_TO_DATASET_FOLDER` must be modified.   <a href=""https://www.youtube.com/embed/ufvPS5wJAx0"" target=""_blank""><img src=""http://img.youtube.com/vi/ufvPS5wJAx0/0.jpg"" alt=""ORB-SLAM2"" width=""240"" height=""180"" border=""10"" /></a> <a href=""https://www.youtube.com/embed/T-9PYCKhDLM"" target=""_blank""><img src=""http://img.youtube.com/vi/T-9PYCKhDLM/0.jpg"" alt=""ORB-SLAM2"" width=""240"" height=""180"" border=""10"" /></a> <a href=""https://www.youtube.com/embed/kPwy8yA4CKM"" target=""_blank""><img src=""http://img.youtube.com/vi/kPwy8yA4CKM/0.jpg"" alt=""ORB-SLAM2"" width=""240"" height=""180"" border=""10"" /></a>   """;Computer Vision;https://github.com/navganti/SIVO
"""Python 3   """;Audio;https://github.com/pascalbakker/WaveNet-Implementation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/DeokO/bert-excercise-ongoing
"""[x] multi-gpu support   You also can use your image data by following steps.   If you didn't have the data  you can prepare it by following steps:  - [FFHQ 128×128](https://github.com/NVlabs/ffhq-dataset) | [FFHQ 512×512](https://www.kaggle.com/arnaud58/flickrfaceshq-dataset-ffhq) - [CelebaHQ 256×256](https://www.kaggle.com/badasstechie/celebahq-resized-256x256) | [CelebaMask-HQ 1024×1024](https://drive.google.com/file/d/1badu11NqxGf6qM3PTTooQDJvQbejgbTv/view)  Download the dataset and prepare it in **LMDB** or **PNG** format using script.  ```python #: Resize to get 16×16 LR_IMGS and 128×128 HR_IMGS  then prepare 128×128 Fake SR_IMGS by bicubic interpolation python prepare.py  --path [dataset root]  --out [output root] --size 16 128 -l ```  then you need to change the datasets config to your data path and image resolution:   ```json ""datasets"": {     ""train"": {         ""dataroot"": ""dataset/ffhq_16_128""  // [output root] in prepare.py script         ""l_resolution"": 16  // low resolution need to super_resolution         ""r_resolution"": 128  // high resolution         ""datatype"": ""lmdb""  //lmdb or img  path of img files     }      ""val"": {         ""dataroot"": ""dataset/celebahq_16_128""  // [output root] in prepare.py script     } }  ```   """;Computer Vision;https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement
"""**GCNet** is initially described in [arxiv](https://arxiv.org/abs/1904.11492). Via absorbing advantages of Non-Local Networks (NLNet) and Squeeze-Excitation Networks (SENet)   GCNet provides a simple  fast and effective approach for global context modeling  which generally outperforms both NLNet and SENet on major benchmarks for various recognition tasks.   a. Install PyTorch 1.1 and torchvision following the [official instructions](https://pytorch.org/).  b. Install latest apex with CUDA and C++ extensions following this [instructions](https://github.com/NVIDIA/apex#quick-start).  The [Sync BN](https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm) implemented by apex is required.  c. Clone the GCNet repository.   ```bash  git clone https://github.com/xvjiarui/GCNet.git  ```  d. Compile cuda extensions.  ```bash cd GCNet pip install cython  #: or ""conda install cython"" if you prefer conda ./compile.sh  #: or ""PYTHON=python3 ./compile.sh"" if you use system python3 without virtual environments ```  e. Install GCNet version mmdetection (other dependencies will be installed automatically).  ```bash python(3) setup.py install  #: add --user if you want to install it locally #: or ""pip install ."" ```  Note: You need to run the last step each time you pull updates from github.  Or you can run `python(3) setup.py develop` or `pip install -e .` to install mmdetection if you want to make modifications to it frequently.  Please refer to mmdetection install [instruction](https://github.com/open-mmlab/mmdetection/blob/master/INSTALL.md) for more details.   Python 3.6.7  PyTorch 1.1.0  CUDA 9.0   """;Computer Vision;https://github.com/zhusiling/GCNet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/junhahyung/bert_finetune
"""You can build the repo through the following commands: ``` $ git clone https://github.com/alexandrosstergiou/SoftPool.git $ cd SoftPool-master/pytorch $ make install --- (optional) --- $ make test ```    ImageNet weight can be downloaded from the following links:   You can load any of the 1D  2D or 3D variants after the installation with:  ```python import softpool_cuda from SoftPool import soft_pool1d  SoftPool1d from SoftPool import soft_pool2d  SoftPool2d from SoftPool import soft_pool3d  SoftPool3d ```  + `soft_poolxd`: Is a functional interface for SoftPool. + `SoftPoolxd`: Is the class-based version which created an object that can be referenced later in the code.   """;Computer Vision;https://github.com/alexandrosstergiou/SoftPool
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/BIYTC/mobilenet_maskrcnn
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/Shinya-Kouda/kgc
"""Convolutional Neural Networks (CNNs) and transformers are two mainstream model architectures currently dominating computer vision and natural language processing. The authors of the paper  however  empirically show that neither convolution nor self-attenion are necessary; in fact  muti-layered perceptrons (MLPs) can also serve as a strong baseline. The authors present MLP-Mixer  an all-MLP mode architecture  that contains two types of layers: a token-mixing layer and a channel-mixing layer. Each of the layers ""mix"" per-location and per-feature information in the input. MLP-Mixer performs comparably to other state-of-the-art models  such as [ViT](https://arxiv.org/abs/2010.11929) or [EfficientNet](https://arxiv.org/abs/1905.11946).   git clone https://github.com/jaketae/mlp-mixer.git  Navigate to the cloned directory. You can start using the model via   """;Computer Vision;https://github.com/jaketae/mlp-mixer
"""Update 13/04/2021: Converted DDPG to Tensorflow 2.   Soft Actor-Critic (SAC) (TF2  PyTorch)   """;Reinforcement Learning;https://github.com/arnomoonens/yarll
"""Run with following:   """;Graphs;https://github.com/KimMeen/GCN
"""4、tfplot 0.2.0 (optional)            5、tensorflow-gpu 1.13   2、(Recommend in this repo) Or you can choose to use a better backbone  refer to gluon2TF.     !pip install tensorflow==1.13.2 #:If Tensorflow.contrib not available   2、Make tfrecord      3、Multi-gpu train  cd $PATH_ROOT/tools   cd $PATH_ROOT/output/summary   """;Computer Vision;https://github.com/NovasMax/R3Det-Refined-Single-Stage-Detector-with-Feature-Refinement-for-RO
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/larsoncs/face_detect
"""    cd src       cd src   """;General;https://github.com/zzhuolun/IRL
"""```bash $ bash download.sh CelebA or $ bash download.sh LSUN ```    $ git clone https://github.com/heykeetae/Self-Attention-GAN.git  $ cd Self-Attention-GAN   $ cd samples/sagan_celeb   $ cd samples/sagan_lsun   """;General;https://github.com/jugatsingh/self_attention_video
"""You can tune the following parameters:   """;General;https://github.com/b-etienne/Seq2seq-PyTorch
"""Our code is implemented in Pytorch 0.4.0 and Python >=3.5. To setup  proceed as follows :  To install Pytorch head over to ```https://pytorch.org/``` or install using miniconda or anaconda package by running  ```conda install -c soumith pytorch ```.  Clone this repository :  ``` git clone https://www.github.com/kdexd/ntm-pytorch ```  The other python libraries that you'll need to run the code : ``` pip install numpy  pip install tensorboard_logger pip install matplotlib pip install tqdm pip install Pillow ```   pip install tensorboard_logger   """;Sequential;https://github.com/vlgiitr/ntm-pytorch
"""`pip install -r requirements.txt`  cd datasets   bash preprocess.sh $IMAGENET_TRAIN_DIR $PREPROCESSED_DATA_DIR   Consecutive category morphing movies: - (5x5 panels 128px images) https://www.youtube.com/watch?v=q3yy5Fxs7Lc   - (10x10 panels 128px images) https://www.youtube.com/watch?v=83D_3WXpPjQ   (If you want to use pretrained models for the image generation  please download the model from [link](https://drive.google.com/drive/folders/1xZoL48uFOCnTxNGdknEYqE5YX0ZyoUej?usp=sharing) and set the `snapshot` argument to the path to the downloaded pretrained model file (.npz).)   """;Computer Vision;https://github.com/pfnet-research/sngan_projection
"""Summarization model for short texts based on pure [transformer model](https://arxiv.org/abs/1706.03762) with [bpe encoding](http://www.aclweb.org/anthology/P16-1162).   $ python train.py --cuda --pretrain_emb   """;General;https://github.com/gooppe/transformer-summarization
"""For training: `python vae.py --train`  For generating new samples: `python vae.py --generate`  For latent space traversal: `python vae.py --traverse`   """;General;https://github.com/adityabingi/Beta-VAE
"""```sh pip install -r requirements.txt ```  ---   res50 = ResNet50(input_shape = img_size    x = GlobalAveragePooling2D()(res50.output)   """;Computer Vision;https://github.com/sek788432/Microsoft-Cats-and-Dogs-Image-Classification
"""For stable version: ```bash pip3 install --user fluence ```  For development version (recommended): ```bash git clone https://github.com/prajjwal1/fluence cd fluence python3 setup.py install --user ```   """;General;https://github.com/prajjwal1/fluence
"""You should create a folder 'LibriSpeech' with the following folders :   The split folder should contain the extracted Librispeech datasets that can be downloaded here.   Pre-trained models are available [here](https://imperialcollegelondon.app.box.com/s/hus5093xaq3errmrxnly0zwsubjlo9d8) (all configurations considered in [(Beckmann et al.  2019)](https://arxiv.org/pdf/1910.09909.pdf)). In the [examples folder](https://github.com/bepierre/SpeechVGG/tree/master/examples) of this repository we show you how to apply a pre-trained speechVGG in speaker recognition and speech/music/noise classification  as introduced in [(Beckmann et al.  2019)](https://arxiv.org/pdf/1910.09909.pdf).   """;Computer Vision;https://github.com/bepierre/SpeechVGG
"""Download the above files. Then  install Node.js and npm and pip if they have yet to be installed.    pip install virtualenv  Go to the directory where the files are installed.  Run the following commands to download the required modules and packages  and to view the web app.  npm install &amp;&amp; npm run first-build  In the future  to view the web app activate the virtual environment if it hasn't been done using   npm run view  For those who are interested to modify any .ts files  run the following command to view the changes made.  npm run build-and-view  If the virtual environment has not already been activated   bash    cd image_classification   Finally  to train the neural network  run the following command   Close the terminal and re-open the terminal.   """;Computer Vision;https://github.com/pohlinwei/AComPianist
""" Requires R version 3.2.0 and higher.  ```r install.packages('devtools') #: Install devtools package if necessary library(devtools) devtools::install_github('makeyourownmaker/mixup') ```    Factors should be one-hot encoded.    * pytorch from facebookresearch    * mxnet from unsky    Create additional training data for toy dataset: ```r library(mixup)  #: Use builtin mtcars dataset with mtcars$am (automatic/manual) as binary target data(mtcars) str(mtcars) summary(mtcars[  -9]) summary(mtcars$am)  #: Strictly speaking this is 'input mixup' (see Details section below) set.seed(42) mtcars.mix <- mixup(mtcars[  -9]  mtcars$am) summary(mtcars.mix$x) summary(mtcars.mix$y)  #: Further info ?mixup ```    """;Computer Vision;https://github.com/makeyourownmaker/mixup
"""```bash $ pip install x-transformers ```   Alternatively  if you would like to use entmax15  you can also do so with one setting as shown below.   Update: It may be that ALiBi enforces a strong local attention across the heads  and may hinder it from attending at distances greater than 1k. To avoid any issues with global message passing  I've decided to introduce another hyperparameter alibi_num_heads  so one can specify less heads for the ALiBi bias   Full encoder / decoder  ```python import torch from x_transformers import XTransformer  model = XTransformer(     dim = 512      enc_num_tokens = 256      enc_depth = 6      enc_heads = 8      enc_max_seq_len = 1024      dec_num_tokens = 256      dec_depth = 6      dec_heads = 8      dec_max_seq_len = 1024      tie_token_emb = True      #: tie embeddings of encoder and decoder )  src = torch.randint(0  256  (1  1024)) src_mask = torch.ones_like(src).bool() tgt = torch.randint(0  256  (1  1024)) tgt_mask = torch.ones_like(tgt).bool()  loss = model(src  tgt  src_mask = src_mask  tgt_mask = tgt_mask) #: (1  1024  512) loss.backward() ```  Decoder-only (GPT-like)  ```python import torch from x_transformers import TransformerWrapper  Decoder  model = TransformerWrapper(     num_tokens = 20000      max_seq_len = 1024      attn_layers = Decoder(         dim = 512          depth = 12          heads = 8     ) ).cuda()  x = torch.randint(0  256  (1  1024)).cuda()  model(x) #: (1  1024  20000) ```  GPT3 would be approximately the following (but you wouldn't be able to run it anyways)  ```python  gpt3 = TransformerWrapper(     num_tokens = 50000      max_seq_len = 2048      attn_layers = Decoder(         dim = 12288          depth = 96          heads = 96          attn_dim_head = 128     ) ).cuda() ```  Encoder-only (BERT-like)  ```python import torch from x_transformers import TransformerWrapper  Encoder  model = TransformerWrapper(     num_tokens = 20000      max_seq_len = 1024      attn_layers = Encoder(         dim = 512          depth = 12          heads = 8     ) ).cuda()  x = torch.randint(0  256  (1  1024)).cuda() mask = torch.ones_like(x).bool()  model(x  mask = mask) #: (1  1024  20000) ```  State of the art image classification  ```python import torch from x_transformers import ViTransformerWrapper  Encoder  model = ViTransformerWrapper(     image_size = 256      patch_size = 32      num_classes = 1000      attn_layers = Encoder(         dim = 512          depth = 6          heads = 8      ) )  img = torch.randn(1  3  256  256) model(img) #: (1  1000) ```  Image -> caption  ```python import torch from x_transformers import ViTransformerWrapper  TransformerWrapper  Encoder  Decoder  encoder = ViTransformerWrapper(     image_size = 256      patch_size = 32      attn_layers = Encoder(         dim = 512          depth = 6          heads = 8     ) )  decoder = TransformerWrapper(     num_tokens = 20000      max_seq_len = 1024      attn_layers = Decoder(         dim = 512          depth = 6          heads = 8          cross_attend = True     ) )  img = torch.randn(1  3  256  256) caption = torch.randint(0  20000  (1  1024))  encoded = encoder(img  return_embeddings = True) decoder(caption  context = encoded) #: (1  1024  20000) ```   """;General;https://github.com/lucidrains/x-transformers
"""```bash git clone git@github.com:philipperemy/keras-tcn.git cd keras-tcn virtualenv -p python3.6 venv source venv/bin/activate pip install -r requirements.txt #: change to tensorflow if you dont have a gpu. pip install . --upgrade #: install it as a package. ```  Note: Only compatible with Python 3 at the moment. Should be almost compatible with python 2.   pip install keras-tcn   Installation (Python 3)   """;Audio;https://github.com/zhong110020/keras-tcn
"""[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.   Getting the source code   $ wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip   $ cd fastText-0.1.0  $ make   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ mkdir build &amp;&amp; cd build &amp;&amp; cmake ..  $ make &amp;&amp; make install   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ pip install .   You can also quantize a supervised model to reduce its memory usage with the following command:   You can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.  There is also the master branch that contains all of our most recent work  but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.   This library has two main use cases: word representation learning and text classification. These were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).   """;Natural Language Processing;https://github.com/ericxsun/fastText
"""To install  use `pip`:  `pip install git+https://github.com/tmabraham/UPIT.git`  The package uses torch 1.7.1  torchvision 0.8.2  and fastai 2.3.0 (and its dependencies). It also requires nbdev 1.1.13 if you would like to add features to the package. Finally  for creating a web app model interface  gradio 1.1.6 is used.   Training a CycleGAN model is easy with UPIT! Given the paths of the images from the two domains `trainA_path` and `trainB_path`  you can do the following:  ```python #:cuda from upit.data.unpaired import * from upit.models.cyclegan import * from upit.train.cyclegan import * dls = get_dls(trainA_path  trainB_path) cycle_gan = CycleGAN(3 3 64) learn = cycle_learner(dls  cycle_gan opt_func=partial(Adam mom=0.5 sqr_mom=0.999)) learn.fit_flat_lin(100 100 2e-4) ```  The GANILLA model is only a different generator model architecture (that's meant to strike a better balance between style and content)  so the same `cycle_learner` class can be used.  ```python #:cuda from upit.models.ganilla import * ganilla = GANILLA(3 3 64) learn = cycle_learner(dls  ganilla opt_func=partial(Adam mom=0.5 sqr_mom=0.999)) learn.fit_flat_lin(100 100 2e-4) ```  Finally  we provide separate functions/classes for `DualGAN` model and training:  ```python #:cuda from upit.models.dualgan import * from upit.train.dualgan import * dual_gan = DualGAN(3 64 3) learn = dual_learner(dls  dual_gan  opt_func=RMSProp) learn.fit_flat_lin(100 100 2e-4) ```  Additionally  we provide metrics for quantitative evaluation of the models  as well as experiment tracking with Weights and Biases. Check the [documentation](https://tmabraham.github.io/UPIT) for more information!   """;General;https://github.com/tmabraham/UPIT
"""Named entity recognition (NER) is a central component in natural language processing tasks. Identifying named entities is a key part in systems e.g. for question answering or entity linking. Traditionally  NER systems are built using conditional random fields (CRFs). Recent systems are using neural network architectures like bidirectional LSTM with a CRF-layer ontop and pre-trained word embeddings ([Ma and Hovy  2016](http://aclweb.org/anthology/P16-1101); [Lample et al.  2016a](http://aclweb.org/anthology/N16-1030); [Reimers and Gurevych  2017](http://aclweb.org/anthology/D17-1035); [Lin et al.  2017](http://aclweb.org/anthology/W17-4421)).  Pre-trained word embeddings have been shown to be of great use for downstream NLP tasks ([Mikolov et al.  2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf); [Pennington et al.  2014](https://www.aclweb.org/anthology/D14-1162)). Many recently proposed approaches go beyond these pre-trained embeddings. Recent works have proposed methods that produce different representations for the same word depending on its contextual usage ([Peters et al.  2017](http://aclweb.org/anthology/P17-1161) [2018a](https://aclweb.org/anthology/N18-1202); [Akbik et al.  2018](https://www.aclweb.org/anthology/C18-1139); [Devlin et al.  2018](https://arxiv.org/abs/1810.04805)). These methods have shown to be very powerful in the fields of named entity recognition  coreference resolution  part-of-speech tagging and question answering  especially in combination with classic word embeddings.  Our paper is based on the work of [Riedl and Padó (2018)](http://aclweb.org/anthology/P18-2020). They showed how to build a model for German named entity recognition (NER) that performs at the state of the art for both contemporary and historical texts. Labeled historical texts for German named entity recognition are a low-resource domain. In order to achieve robust state-of-the-art results for historical texts they used transfer-learning with labeled data from other high-resource domains like CoNLL-2003 ([Tjong Kim Sang and De Meulder  2003](http://aclweb.org/anthology/W03-0419)) or GermEval ([Benikova et al.  2014](http://www.lrec-conf.org/proceedings/lrec2014/pdf/276_Paper.pdf)). They showed that using Bi-LSTM with a CRF as the top layer and word embeddings outperforms CRFs with hand-coded features in a big-data situation.  We build up upon their work and use the same low-resource datasets for Historic German. Furthermore  we show how to achieve new state-of-the-art results for Historic German named entity recognition by using only unlabeled data via pre-trained language models and word embeddings. We also introduce a novel language model pre-training objective  that uses only contemporary texts for training to achieve comparable state-of-the-art results on historical texts.   preprocessing steps like tokenization are needed. We use 1/500 of the complete   With the --number argument you should define a unique id for your experiment.   This sections shows how to use one of our trained models with Flair in order to perform NER on a sentence.  Then you can use the following code to perform NER:  ```python from flair.data import Sentence from flair.models import SequenceTagger  #: Noisy OCR :) sentence = Sentence(""April Martin Ansclm   K. Gefan - gen-Auffehers Georg Sausgruber ."")  tagger: SequenceTagger = SequenceTagger.load(""dbmdz/flair-historic-ner-onb"") tagger.predict(sentence)  sentence.to_tagged_string() ```  This outputs:  ```python 'April Martin <B-PER> Ansclm <E-PER>   K. Gefan - gen-Auffehers Georg <B-PER> Sausgruber <E-PER> .' ```   """;Natural Language Processing;https://github.com/dbmdz/historic-ner
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/tvinith/bert
"""Python 3.7   """;Computer Vision;https://github.com/CREVIOS/cscongress
"""For 进阶者（Like You）：   For 大神（May Be Like You）：   Patches Are All You Need?---ICLR2022 (Under Review)   ```python from model.attention.ExternalAttention import ExternalAttention import torch  input=torch.randn(50 49 512) ea = ExternalAttention(d_model=512 S=8) output=ea(input) print(output.shape) ```  ***    ```python from model.attention.SelfAttention import ScaledDotProductAttention import torch  input=torch.randn(50 49 512) sa = ScaledDotProductAttention(d_model=512  d_k=512  d_v=512  h=8) output=sa(input input input) print(output.shape) ```  ***   ```python from model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention import torch  input=torch.randn(50 49 512) ssa = SimplifiedScaledDotProductAttention(d_model=512  h=8) output=ssa(input input input) print(output.shape)  ```  ***   ```python from model.attention.SEAttention import SEAttention import torch  input=torch.randn(50 512 7 7) se = SEAttention(channel=512 reduction=8) output=se(input) print(output.shape)  ```  ***   ```python from model.attention.SKAttention import SKAttention import torch  input=torch.randn(50 512 7 7) se = SKAttention(channel=512 reduction=8) output=se(input) print(output.shape)  ``` ***   ```python from model.attention.CBAM import CBAMBlock import torch  input=torch.randn(50 512 7 7) kernel_size=input.shape[2] cbam = CBAMBlock(channel=512 reduction=16 kernel_size=kernel_size) output=cbam(input) print(output.shape)  ```  ***   ```python from model.attention.BAM import BAMBlock import torch  input=torch.randn(50 512 7 7) bam = BAMBlock(channel=512 reduction=16 dia_val=2) output=bam(input) print(output.shape)  ```  ***   ```python from model.attention.ECAAttention import ECAAttention import torch  input=torch.randn(50 512 7 7) eca = ECAAttention(kernel_size=3) output=eca(input) print(output.shape)  ```  ***   ```python from model.attention.DANet import DAModule import torch  input=torch.randn(50 512 7 7) danet=DAModule(d_model=512 kernel_size=3 H=7 W=7) print(danet(input).shape)  ```  ***   ```python from model.attention.PSA import PSA import torch  input=torch.randn(50 512 7 7) psa = PSA(channel=512 reduction=8) output=psa(input) print(output.shape)  ```  ***    ```python  from model.attention.EMSA import EMSA import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 64 512) emsa = EMSA(d_model=512  d_k=512  d_v=512  h=8 H=8 W=8 ratio=2 apply_transform=True) output=emsa(input input input) print(output.shape)      ```  ***    ```python  from model.attention.ShuffleAttention import ShuffleAttention import torch from torch import nn from torch.nn import functional as F   input=torch.randn(50 512 7 7) se = ShuffleAttention(channel=512 G=8) output=se(input) print(output.shape)       ```   ***    ```python from model.attention.MUSEAttention import MUSEAttention import torch from torch import nn from torch.nn import functional as F   input=torch.randn(50 49 512) sa = MUSEAttention(d_model=512  d_k=512  d_v=512  h=8) output=sa(input input input) print(output.shape)  ```  ***    ```python from model.attention.SGE import SpatialGroupEnhance import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 512 7 7) sge = SpatialGroupEnhance(groups=8) output=sge(input) print(output.shape)  ```  ***    ```python from model.attention.A2Atttention import DoubleAttention import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 512 7 7) a2 = DoubleAttention(512 128 128 True) output=a2(input) print(output.shape)  ```     ```python from model.attention.AFT import AFT_FULL import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 49 512) aft_full = AFT_FULL(d_model=512  n=49) output=aft_full(input) print(output.shape)  ```        ```python from model.attention.OutlookAttention import OutlookAttention import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 28 28 512) outlook = OutlookAttention(dim=512) output=outlook(input) print(output.shape)  ```   ***        ```python  from model.attention.ViP import WeightedPermuteMLP import torch from torch import nn from torch.nn import functional as F  input=torch.randn(64 8 8 512) seg_dim=8 vip=WeightedPermuteMLP(512 seg_dim) out=vip(input) print(out.shape)  ```   ***       ```python  from model.attention.CoAtNet import CoAtNet import torch from torch import nn from torch.nn import functional as F  input=torch.randn(1 3 224 224) mbconv=CoAtNet(in_ch=3 image_size=224) out=mbconv(input) print(out.shape)  ```   ***        ```python  from model.attention.HaloAttention import HaloAttention import torch from torch import nn from torch.nn import functional as F  input=torch.randn(1 512 8 8) halo = HaloAttention(dim=512      block_size=2      halo_size=1 ) output=halo(input) print(output.shape)  ```   ***   ```python  from model.attention.PolarizedSelfAttention import ParallelPolarizedSelfAttention SequentialPolarizedSelfAttention import torch from torch import nn from torch.nn import functional as F  input=torch.randn(1 512 7 7) psa = SequentialPolarizedSelfAttention(channel=512) output=psa(input) print(output.shape)   ```   ***    ```python  from model.attention.CoTAttention import CoTAttention import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 512 7 7) cot = CoTAttention(dim=512 kernel_size=3) output=cot(input) print(output.shape)    ```  ***    ```python  from model.attention.ResidualAttention import ResidualAttention import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 512 7 7) resatt = ResidualAttention(channel=512 num_class=1000 la=0.2) output=resatt(input) print(output.shape)    ```  ***     ```python from model.attention.S2Attention import S2Attention import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 512 7 7) s2att = S2Attention(channels=512) output=s2att(input) print(output.shape)  ```  ***     ```python from model.attention.gfnet import GFNet import torch from torch import nn from torch.nn import functional as F  x = torch.randn(1  3  224  224) gfnet = GFNet(embed_dim=384  img_size=224  patch_size=16  num_classes=1000) out = gfnet(x) print(out.shape)  ```  ***    ```python from model.attention.TripletAttention import TripletAttention import torch from torch import nn from torch.nn import functional as F input=torch.randn(50 512 7 7) triplet = TripletAttention() output=triplet(input) print(output.shape) ```   ***    ```python from model.attention.CoordAttention import CoordAtt import torch from torch import nn from torch.nn import functional as F  inp=torch.rand([2  96  56  56]) inp_dim  oup_dim = 96  96 reduction=32  coord_attention = CoordAtt(inp_dim  oup_dim  reduction=reduction) output=coord_attention(inp) print(output.shape) ```  ***    ```python from model.attention.MobileViTAttention import MobileViTAttention import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     m=MobileViTAttention()     input=torch.randn(1 3 49 49)     output=m(input)     print(output.shape)  #:output:(1 3 49 49)      ```  ***    ```python from model.attention.ParNetAttention import * import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     input=torch.randn(50 512 7 7)     pna = ParNetAttention(channel=512)     output=pna(input)     print(output.shape) #:50 512 7 7      ```  ***    ```python from model.attention.UFOAttention import * import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     input=torch.randn(50 49 512)     ufo = UFOAttention(d_model=512  d_k=512  d_v=512  h=8)     output=ufo(input input input)     print(output.shape) #:[50  49  512]      ```  ***    ```python  from model.backbone.resnet import ResNet50 ResNet101 ResNet152 import torch if __name__ == '__main__':     input=torch.randn(50 3 224 224)     resnet50=ResNet50(1000)     #: resnet101=ResNet101(1000)     #: resnet152=ResNet152(1000)     out=resnet50(input)     print(out.shape)  ```    ```python  from model.backbone.resnext import ResNeXt50 ResNeXt101 ResNeXt152 import torch  if __name__ == '__main__':     input=torch.randn(50 3 224 224)     resnext50=ResNeXt50(1000)     #: resnext101=ResNeXt101(1000)     #: resnext152=ResNeXt152(1000)     out=resnext50(input)     print(out.shape)   ```     ```python  from model.backbone.MobileViT import * import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     input=torch.randn(1 3 224 224)      #:#:#: mobilevit_xxs     mvit_xxs=mobilevit_xxs()     out=mvit_xxs(input)     print(out.shape)      #:#:#: mobilevit_xs     mvit_xs=mobilevit_xs()     out=mvit_xs(input)     print(out.shape)       #:#:#: mobilevit_s     mvit_s=mobilevit_s()     out=mvit_s(input)     print(out.shape)  ```       ```python  from model.backbone.ConvMixer import * import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     x=torch.randn(1 3 224 224)     convmixer=ConvMixer(dim=512 depth=12)     out=convmixer(x)     print(out.shape)  #:[1  1000]   ```         ```python from model.mlp.repmlp import RepMLP import torch from torch import nn  N=4 #:batch size C=512 #:input dim O=1024 #:output dim H=14 #:image height W=14 #:image width h=7 #:patch height w=7 #:patch width fc1_fc2_reduction=1 #:reduction ratio fc3_groups=8 #: groups repconv_kernels=[1 3 5 7] #:kernel list repmlp=RepMLP(C O H W h w fc1_fc2_reduction fc3_groups repconv_kernels=repconv_kernels) x=torch.randn(N C H W) repmlp.eval() for module in repmlp.modules():     if isinstance(module  nn.BatchNorm2d) or isinstance(module  nn.BatchNorm1d):         nn.init.uniform_(module.running_mean  0  0.1)         nn.init.uniform_(module.running_var  0  0.1)         nn.init.uniform_(module.weight  0  0.1)         nn.init.uniform_(module.bias  0  0.1)  #:training result out=repmlp(x) #:inference result repmlp.switch_to_deploy() deployout = repmlp(x)  print(((deployout-out)**2).sum()) ```   ```python from model.mlp.mlp_mixer import MlpMixer import torch mlp_mixer=MlpMixer(num_classes=1000 num_blocks=10 patch_size=10 tokens_hidden_dim=32 channels_hidden_dim=1024 tokens_mlp_dim=16 channels_mlp_dim=1024) input=torch.randn(50 3 40 40) output=mlp_mixer(input) print(output.shape) ```  ***   ```python from model.mlp.resmlp import ResMLP import torch  input=torch.randn(50 3 14 14) resmlp=ResMLP(dim=128 image_size=14 patch_size=7 class_num=1000) out=resmlp(input) print(out.shape) #:the last dimention is class_num ```  ***   ```python from model.mlp.g_mlp import gMLP import torch  num_tokens=10000 bs=50 len_sen=49 num_layers=6 input=torch.randint(num_tokens (bs len_sen)) #:bs len_sen gmlp = gMLP(num_tokens=num_tokens len_sen=len_sen dim=512 d_ff=1024) output=gmlp(input) print(output.shape) ```  ***   ```python from model.mlp.sMLP_block import sMLPBlock import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     input=torch.randn(50 3 224 224)     smlp=sMLPBlock(h=224 w=224)     out=smlp(input)     print(out.shape) ```    ```python  from model.rep.repvgg import RepBlock import torch   input=torch.randn(50 512 49 49) repblock=RepBlock(512 512) repblock.eval() out=repblock(input) repblock._switch_to_deploy() out2=repblock(input) print('difference between vgg and repvgg') print(((out2-out)**2).sum()) ```    ***   ```python from model.rep.acnet import ACNet import torch from torch import nn  input=torch.randn(50 512 49 49) acnet=ACNet(512 512) acnet.eval() out=acnet(input) acnet._switch_to_deploy() out2=acnet(input) print('difference:') print(((out2-out)**2).sum())  ```    ***   ```python from model.conv.DepthwiseSeparableConvolution import DepthwiseSeparableConvolution import torch from torch import nn from torch.nn import functional as F  input=torch.randn(1 3 224 224) dsconv=DepthwiseSeparableConvolution(3 64) out=dsconv(input) print(out.shape) ```  ***    ```python from model.conv.MBConv import MBConvBlock import torch from torch import nn from torch.nn import functional as F  input=torch.randn(1 3 224 224) mbconv=MBConvBlock(ksize=3 input_filters=3 output_filters=512 image_size=224) out=mbconv(input) print(out.shape)   ```  ***    ```python from model.conv.Involution import Involution import torch from torch import nn from torch.nn import functional as F  input=torch.randn(1 4 64 64) involution=Involution(kernel_size=3 in_channel=4 stride=2) out=involution(input) print(out.shape) ```  ***    ```python from model.conv.DynamicConv import * import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     input=torch.randn(2 32 64 64)     m=DynamicConv(in_planes=32 out_planes=64 kernel_size=3 stride=1 padding=1 bias=False)     out=m(input)     print(out.shape) #: 2 32 64 64  ```  ***    ```python from model.conv.CondConv import * import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     input=torch.randn(2 32 64 64)     m=CondConv(in_planes=32 out_planes=64 kernel_size=3 stride=1 padding=1 bias=False)     out=m(input)     print(out.shape)  ```  ***  """;Computer Vision;https://github.com/xmu-xiaoma666/External-Attention-pytorch
"""https://github.com/dvschultz/ml-art-colabs/blob/master/deepdream.ipynb <br>   https://github.com/dvschultz/ai/blob/master/SinGAN.ipynb <br>   https://github.com/dvschultz/ai/blob/master/neural_style_tf.ipynb <br>   https://github.com/dvschultz/ai/blob/master/StyleGAN2.ipynb <br>   Karras T.  Aittala  M.  Hellsten  J.  Laine  S.  Lehtinen  J.  Aila  T. (2020): Met- Faces. Version 1. <br>  https://github.com/NVlabs/metfaces-dataset <br>   https://www.kaggle.com/greg115/abstract-art/version/1 <br>   1. Analyze your own images using the [classification script](https://github.com/bennyqp/artificial-inspiration/blob/main/ai_image_classification.ipynb) and create the corresponding CSV file or download the original ""Creative Portrait Dataset"" for Unity and the corresponding CSV file [here](https://drive.google.com/file/d/1l8oa6ncwP0rItGJ3a2RVeEg1e5dkOgmf/view?usp=sharing). 1. Clone this repository and replace the file ""artificial-inspiration/Unity VR Dataset Explorer/Assets/Resources/img2vec.csv"" with your generated img2vec.csv or with the downloaded file.  1. Replace the folder ""artificial-inspiration/Unity VR Dataset Explorer/Assets/Resources/images/"" with your generated image folder or the one you downloaded. Important: The folder MUST be named ""images"" and the CSV file ""img2vec.cvs"" 1. Open the folder ""artificial-inspiration/Unity VR Dataset Explorer"" with Unity 2019.4.15f1 and open the scene ""vrDataExplorer"". When Unity asks you if you want to enable the backends because of the new input system  click no!  1. If you want to access your selected images online later  upload the content in the folder ""artificial-inspiration/selected images web app"" to a server. Then add the link to the file ""artificial-inspiration/selected images web app/images/uploadImages.php"" in Unity under ""Images Upload URL"" in the script ""Upload Images"". 1. If you don't use VR  activate ""Start in Explore Mode"" in the ""Constructor"" script. You can view the images and apply filters in the editor. Most of the functions are unfortunately not available. 1. If you use VR  you can use all the features. You can find them all in Unity and use most of them during the VR experience to explore your dataset and find the most exciting images.  1. Pretty much all the settings parameters are in the scripts on the ""GlobalScripts"" GameObject. Here you can play around and change the settings to try out different things.  1. Get inspired and develop new ideas ;-)   [You can also download the final Oculus build with the given sample data as an .apk file for your Oculus Quest here.](https://drive.google.com/file/d/1eiHNsIFS2pggfxFwTzurIDvFk4qxgDGs/view?usp=sharing) You can then run it using Sidequest  for example. However  it is recommended to run the application via Unity using Oculus Link  as it requires quite a bit of performance and can lag when run as a standalone.  <br><br>  The overriding goal is that the creativity of the user in relation to the subject matter is stimulated by this process and thus novel creative results can be developed.    www.artificial-inspiration.com <br><br><br><br> ![artificial inspiration images](https://github.com/bennyqp/artificial-inspiration/blob/main/Demo%20Images/artificial_inspiration_img06.jpg) ![artificial inspiration images](https://github.com/bennyqp/artificial-inspiration/blob/main/Demo%20Images/artificial_inspiration_img07.jpg) ![artificial inspiration images](https://github.com/bennyqp/artificial-inspiration/blob/main/Demo%20Images/artificial_inspiration_img08.jpg) ![artificial inspiration images](https://github.com/bennyqp/artificial-inspiration/blob/main/Demo%20Images/artificial_inspiration_img09.jpg)  <br><br>  """;Computer Vision;https://github.com/bennyqp/artificial-inspiration
"""- Training:   Each of the three models has its own training code. To train each model  just run: > python train_srcnn.py <br/> > python train_srresnet.py <br/> > python train_srgan.py <br/> - Testing:   The three models have the same evaluation process. To test each model run: > python evaluate.py -m MODEL #The model youwant to test here we support ['srcnn' 'srresnet' 'srgan']     """;Computer Vision;https://github.com/maiyuxiaoge/srgan
""" **loss** MeanSquaredError<br /> **metrics** MeanAbsoluteError<br /> **optimizer** Adam with learning rate scheduler<br /> **monitors** val_loss<br /> **loss_weights** [0.2  0.8] (When multiple heads i.e. Inception_v3)    ```shell pip install smartflow ```  ```shell git clone https://github.com/ThanasisMattas/smartflow.git ```   | Requirements           |   | matplotlib >= 3.3.2    |   | pandas >= 1.1.3        |   | scipy >= 1.5.3         |   On-devise  using the [normalization_layer()].    <img src=""media/input-pred-gt_visualizations/it_00364.png"" width=800>   """;Computer Vision;https://github.com/ThanasisMattas/smartflow
"""<a href='g3doc/installation.md'>Installation.</a><br>   To get help with issues you may encounter while using the DeepLab Tensorflow implementation  create a new question on [StackOverflow](https://stackoverflow.com/) with the tag ""tensorflow"".  Please report bugs (i.e.  broken code  not usage questions) to the tensorflow/models GitHub [issue tracker](https://github.com/tensorflow/models/issues)  prefixing the issue name with ""deeplab"".   """;Computer Vision;https://github.com/EdwinAlegria/object_semantic_deeplabv3
"""git clone git@github.com:cybertronai/pytorch-lamb.git  cd pytorch-lamb  pip install -e .   """;General;https://github.com/Smerity/pytorch-lamb
"""This repository is a PyTorch implementation for semantic segmentation / scene parsing. The code is easy to use for training and testing on various datasets. The codebase mainly uses ResNet50/101/152 as backbone and can be easily adapted to other basic classification structures. Implemented networks including [PSPNet](https://hszhao.github.io/projects/pspnet) and [PSANet](https://hszhao.github.io/projects/psanet)  which ranked 1st places in [ImageNet Scene Parsing Challenge 2016 @ECCV16](http://image-net.org/challenges/LSVRC/2016/results)  [LSUN Semantic Segmentation Challenge 2017 @CVPR17](https://blog.mapillary.com/product/2017/06/13/lsun-challenge.html) and [WAD Drivable Area Segmentation Challenge 2018 @CVPR18](https://bdd-data.berkeley.edu/wad-2018.html). Sample experimented datasets are [ADE20K](http://sceneparsing.csail.mit.edu)  [PASCAL VOC 2012](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6) and [Cityscapes](https://www.cityscapes-dataset.com).  <img src=""./figure/pspnet.png"" width=""900""/>   1. Highlight:     - Fast multiprocessing training ([nn.parallel.DistributedDataParallel](https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html)) with official [nn.SyncBatchNorm](https://pytorch.org/docs/master/nn.html#torch.nn.SyncBatchNorm).    - Better reimplementation results with well designed code structures.    - All initialization models  trained models and predictions are [available](https://drive.google.com/open?id=15wx9vOM0euyizq-M1uINgN0_wjVRf9J3).  2. Requirement:     - Hardware: 4-8 GPUs (better with >=11G GPU memory)    - Software: PyTorch>=1.1.0  Python3  [tensorboardX](https://github.com/lanpa/tensorboardX)    3. Clone the repository:     ```shell    git clone https://github.com/hszhao/semseg.git    ```  4. Train:     - Download related datasets and symlink the paths to them as follows (you can alternatively modify the relevant paths specified in folder `config`):       ```      cd semseg      mkdir -p dataset      ln -s /path_to_ade20k_dataset dataset/ade20k      ```     - Download ImageNet pre-trained [models]((https://drive.google.com/open?id=15wx9vOM0euyizq-M1uINgN0_wjVRf9J3)) and put them under folder `initmodel` for weight initialization. Remember to use the right dataset format detailed in [FAQ.md](./FAQ.md).     - Specify the gpu used in config then do training:       ```shell      sh tool/train.sh ade20k pspnet50      ```    - If you are using [SLURM](https://slurm.schedmd.com/documentation.html) for nodes manager  uncomment lines in train.sh and then do training:       ```shell      sbatch tool/train.sh ade20k pspnet50      ```  5. Test:     - Download trained segmentation models and put them under folder specified in config or modify the specified paths.     - For full testing (get listed performance):       ```shell      sh tool/test.sh ade20k pspnet50      ```     - **Quick demo** on one image:       ```shell      PYTHONPATH=./ python tool/demo.py --config=config/ade20k/ade20k_pspnet50.yaml --image=figure/demo/ADE_val_00001515.jpg TEST.scales '[1.0]'      ```  6. Visualization: [tensorboardX](https://github.com/lanpa/tensorboardX) incorporated for better visualization.     ```shell    tensorboard --logdir=exp/ade20k    ```  7. Other:     - Resources: GoogleDrive [LINK](https://drive.google.com/open?id=15wx9vOM0euyizq-M1uINgN0_wjVRf9J3) contains shared models  visual predictions and data lists.    - Models: ImageNet pre-trained models and trained segmentation models can be accessed. Note that our ImageNet pretrained models are slightly different from original [ResNet](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py) implementation in the beginning part.    - Predictions: Visual predictions of several models can be accessed.    - Datasets: attributes (`names` and `colors`) are in folder `dataset` and some sample lists can be accessed.    - Some FAQs: [FAQ.md](./FAQ.md).    - Former video predictions: high accuracy -- [PSPNet](https://youtu.be/rB1BmBOkKTw)  [PSANet](https://youtu.be/l5xu1DI6pDk); high efficiency -- [ICNet](https://youtu.be/qWl9idsCuLQ).   """;Computer Vision;https://github.com/hszhao/semseg
"""git clone https://github.com/amyllykoski/CycleGAN  cd CycleGAN  Use your own dataset (trainA  trainB  testA  testB) as outlined in https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix  Make sure you have already installed the necessary Python packages like so:  pip install -r requirements.txt   jupyter notebook .   """;Computer Vision;https://github.com/amyllykoski/CycleGAN
"""```bash $ pip install big-sleep ```   You will be able to have the GAN dream up images using natural language with a one-line command in the terminal.   You can now train more than one phrase using the delimiter ""\""   ```bash $ dream ""a pyramid made of ice"" ```  Images will be saved to wherever the command is invoked   """;Computer Vision;https://github.com/luqui/big-sleep
"""I will describe the layout of the dataset. The CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes  with 6000 images per class. There are 50000 training images and 10000 test images.  The dataset is divided into five training batches and one test batch  each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order  but some training batches may contain more images from one class than another. Between them  the training batches contain exactly 5000 images from each class. The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. ""Automobile"" includes sedans  SUVs  things of that sort. ""Truck"" includes only big trucks. Neither includes pickup trucks. The archive contains the files data_batch_1  data_batch_2  ...  data_batch_5  as well as test_batch. For each batch files: 	data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 color image. The first 1024 entries contain the red channel values  the next 1024 the green  and the final 1024 the blue. The image is stored in row-major order  so that the first 32 entries of the array are the red channel values of the first row of the image. 	labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data. The dataset contains another file  called batches.meta. It too contains a Python dictionary object. It has the following entries: 	label_names -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example  label_names[0] == ""airplane""  label_names[1] == ""automobile""  etc.  """;Computer Vision;https://github.com/Xinyi6/CIFAR10-CNN-by-Keras
"""Appropriate instructions to run the code are available on this Colab notebook. Please feel free to create your own copy and play with it.   """;Graphs;https://github.com/yashkotadia/GatedGCN-PATTERN
"""I would recommend to use upsampling by default  unless you know that your problem requires high spatial resolution. Still  you can easily experiment with both by just changing the up_mode parameter.   """;Computer Vision;https://github.com/jiye-ML/unet_pytorch
"""**nb** can be installed from PIP  remember the name is `nbnb`:  ``` sudo pip3 install nbnb ```     2020.09.14: We release a primary version of 0.04  which you can build a simple YoloV5 with nb easily!     pip install nbnb   Here is an example of using NB to build YoloV5!   **updates**: We have another YoloV5-ASFF version added in example!  ```python import torch from torch import nn from nb.torch.blocks.bottleneck_blocks import SimBottleneckCSP from nb.torch.blocks.trans_blocks import Focus from nb.torch.blocks.head_blocks import SPP from nb.torch.blocks.conv_blocks import ConvBase from nb.torch.utils import device  class YoloV5(nn.Module):      def __init__(self  num_cls=80  ch=3  anchors=None):         super(YoloV5  self).__init__()         assert anchors != None  'anchor must be provided'          #: divid by         cd = 2         wd = 3          self.focus = Focus(ch  64//cd)         self.conv1 = ConvBase(64//cd  128//cd  3  2)         self.csp1 = SimBottleneckCSP(128//cd  128//cd  n=3//wd)         self.conv2 = ConvBase(128//cd  256//cd  3  2)         self.csp2 = SimBottleneckCSP(256//cd  256//cd  n=9//wd)         self.conv3 = ConvBase(256//cd  512//cd  3  2)         self.csp3 = SimBottleneckCSP(512//cd  512//cd  n=9//wd)         self.conv4 = ConvBase(512//cd  1024//cd  3  2)         self.spp = SPP(1024//cd  1024//cd)         self.csp4 = SimBottleneckCSP(1024//cd  1024//cd  n=3//wd  shortcut=False)          #: PANet         self.conv5 = ConvBase(1024//cd  512//cd)         self.up1 = nn.Upsample(scale_factor=2)         self.csp5 = SimBottleneckCSP(1024//cd  512//cd  n=3//wd  shortcut=False)          self.conv6 = ConvBase(512//cd  256//cd)         self.up2 = nn.Upsample(scale_factor=2)         self.csp6 = SimBottleneckCSP(512//cd  256//cd  n=3//wd  shortcut=False)          self.conv7 = ConvBase(256//cd  256//cd  3  2)         self.csp7 = SimBottleneckCSP(512//cd  512//cd  n=3//wd  shortcut=False)          self.conv8 = ConvBase(512//cd  512//cd  3  2)         self.csp8 = SimBottleneckCSP(512//cd  1024//cd  n=3//wd  shortcut=False)      def _build_backbone(self  x):         x = self.focus(x)         x = self.conv1(x)         x = self.csp1(x)         x_p3 = self.conv2(x)  #: P3         x = self.csp2(x_p3)         x_p4 = self.conv3(x)  #: P4         x = self.csp3(x_p4)         x_p5 = self.conv4(x)  #: P5         x = self.spp(x_p5)         x = self.csp4(x)         return x_p3  x_p4  x_p5  x      def _build_head(self  p3  p4  p5  feas):         h_p5 = self.conv5(feas)  #: head P5         x = self.up1(h_p5)         x_concat = torch.cat([x  p4]  dim=1)         x = self.csp5(x_concat)          h_p4 = self.conv6(x)  #: head P4         x = self.up2(h_p4)         x_concat = torch.cat([x  p3]  dim=1)         x_small = self.csp6(x_concat)          x = self.conv7(x_small)         x_concat = torch.cat([x  h_p4]  dim=1)         x_medium = self.csp7(x_concat)          x = self.conv8(x_medium)         x_concat = torch.cat([x  h_p5]  dim=1)         x_large = self.csp8(x)         return x_small  x_medium  x_large      def forward(self  x):         p3  p4  p5  feas = self._build_backbone(x)         xs  xm  xl = self._build_head(p3  p4  p5  feas)         return xs  xm  xl ```  A simple example to build a layer of conv:  ```python from nb.torch.base.conv_block import ConvBase a = ConvBase(128  256  3  1  2  norm_cfg=dict(type=""BN"")  act_cfg=dict(type=""Hardswish"")) ``` Be note that  the reason for us using `cfg` to specific norm and activation is for users dynamically switch their configuration of model in yaml format rather than hard code it.  A simple example of using GhostNet:  ```python from nb.torch.backbones.ghostnet import GhostNet  m = GhostNet(num_classes=8)  #: if you want FPN output m = GhostNet(fpn_levels=[4  5  6]) ```  A simple example of using MobilenetV3:  ```python from nb.torch.backbones.mobilenetv3_new import MobilenetV3_Small ```       """;General;https://github.com/jinfagang/nb
"""PATH/class1 <br/>  PATH/class2 <br/>   Code for evaulate FID score came from https://github.com/bioinf-jku/TTUR   """;Computer Vision;https://github.com/rosinality/sagan-pytorch
"""- Dont sample from a Uniform distribution  ![cube](images/cube.png ""Cube"")  - Sample from a gaussian distribution  ![sphere](images/sphere.png ""Sphere"")  - When doing interpolations  do the interpolation via a great circle  rather than a straight line from point A to point B - Tom White's [Sampling Generative Networks](https://arxiv.org/abs/1609.04468) ref code https://github.com/dribnet/plat has more details    - Label Smoothing  i.e. if you have two target labels: Real=1 and Fake=0  then for each incoming sample  if it is real  then replace the label with a random number between 0.7 and 1.2  and if it is a fake sample  replace it with 0.0 and 0.3 (for example).   - Salimans et. al. 2016 - make the labels the noisy for the discriminator: occasionally flip the labels when training the discriminator   - Experience Replay   - Keep a replay buffer of past generations and occassionally show them   - Keep checkpoints from the past of G and D and occassionaly swap them out for a few iterations - All stability tricks that work for deep deterministic policy gradients - See Pfau & Vinyals (2016)   - optim.Adam rules!   - See Radford et. al. 2015 - Use SGD for discriminator and ADAM for generator   - if you have labels available  training the discriminator to also classify the samples: auxillary GANs   - Provide noise in the form of dropout (50%). - Apply on several layers of our generator at both training and test time - https://arxiv.org/pdf/1611.07004v1.pdf    """;General;https://github.com/jaingaurav3/GAN-Hacks
"""pip install -r requirements.txt   """;Computer Vision;https://github.com/tanyanair/segmentation_uncertainty
"""Chrome extension   """;General;https://github.com/gfursin/browser-extension-for-reproducible-research
"""Follow the instructions below to get our project running on your local machine.  1. Clone the repository and make sure you have prerequisites below to run the code. 2. Run `python src/main.py --help` to see the various options available to specify. 3. To train the model  run the command `python src/main.py ...` along with the flags. For example  to run on the maps (map-to-satellite) dataset  you may run  ```bash python src/main.py --mode train --data_root '../datasets/maps' --num_epochs 100 --data_invert ```  4. All the outputs will be saved to `src/output/[timestamp]` where `[timestamp]` is the time of start of training.   """;Computer Vision;https://github.com/vamsi3/pix2pix
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.  git clone https://github.com/affinelayer/pix2pix-tensorflow.git  cd pix2pix-tensorflow   ```sh #: clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git cd pix2pix-tensorflow #: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir facades_train \   --max_epochs 200 \   --input_dir facades/train \   --which_direction BtoA #: test the model python pix2pix.py \   --mode test \   --output_dir facades_test \   --input_dir facades/val \   --checkpoint facades_train ```  The test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.  If you have Docker installed  you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:  ```sh #: train the model python tools/dockrun.py python pix2pix.py \       --mode train \       --output_dir facades_train \       --max_epochs 200 \       --input_dir facades/train \       --which_direction BtoA #: test the model python tools/dockrun.py python pix2pix.py \       --mode test \       --output_dir facades_test \       --input_dir facades/val \       --checkpoint facades_train ```   <img src=""docs/combine.png"" width=""900px""/>  ```sh #: Resize source images python tools/process.py \   --input_dir photos/original \   --operation resize \   --output_dir photos/resized #: Create images with blank centers python tools/process.py \   --input_dir photos/resized \   --operation blank \   --output_dir photos/blank #: Combine resized images with blanked images python tools/process.py \   --input_dir photos/resized \   --b_dir photos/blank \   --operation combine \   --output_dir photos/combined #: Split into train/val set python tools/split.py \   --dir photos/combined ```  The folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.   """;General;https://github.com/utunga/pix2pix-tensorflow
"""Dataset is about 30GB and can be downloaded from aws as described [here](doc/datasets.md). After downloading dataset install dependencies and preprocess dataset:  1. Follow instructions at https://github.com/matterport/Mask_RCNN to install Mask-RCNN model libraries from github   2. Install project dependencies:     ```bash    pip3 install -r requirements.txt    ```  3. Download pre-trained COCO weights (`mask_rcnn_coco.h5`) from the [releases page](https://github.com/matterport/Mask_RCNN/releases). We use them to do transfer learning.   4. Copy `mask_rcnn_coco.h5` into `mask_rcnn` folder.  5. Setup environment variable DATASET_ROOT and make sure it exists.   6. Run pre-processing of the dataset. Preprocessing splits every tile into set of smaller images and also splits original Test dataset into Dev and Test datasets.     ```bash    python3 build_dataset.py    ```   """;General;https://github.com/asyrovprog/cs230project
"""**MacBERT** is an improved BERT with novel **M**LM **a**s **c**orrection pre-training task  which mitigates the discrepancy of pre-training and fine-tuning.  Instead of masking with [MASK] token  which never appears in the ﬁne-tuning stage  **we propose to use similar words for the masking purpose**. A similar word is obtained by using [Synonyms toolkit (Wang and Hu  2017)](https://github.com/chatopera/Synonyms)  which is based on word2vec (Mikolov et al.  2013) similarity calculations. If an N-gram is selected to mask  we will ﬁnd similar words individually. In rare cases  when there is no similar word  we will degrade to use random word replacement.  Here is an example of our pre-training task. |  | Example       | | -------------- | ----------------- | | **Original Sentence**  | we use a language model to predict the probability of the next word. | |  **MLM** | we use a language [M] to [M] ##di ##ct the pro [M] ##bility of the next word . | | **Whole word masking**   | we use a language [M] to [M] [M] [M] the [M] [M] [M] of the next word . | | **N-gram masking** | we use a [M] [M] to [M] [M] [M] the [M] [M] [M] [M] [M] next word . | | **MLM as correction** | we use a text system to ca ##lc ##ulate the po ##si ##bility of the next word . |  Except for the new pre-training task  we also incorporate the following techniques.  - Whole Word Masking (WWM) - N-gram masking - Sentence-Order Prediction (SOP)  **Note that our MacBERT can be directly replaced with the original BERT as there is no differences in the main neural architecture.**  For more technical details  please check our paper: [Revisiting Pre-trained Models for Chinese Natural Language Processing](https://arxiv.org/abs/2004.13922)    | Section | Description | |-|-| | [Introduction](#Introduction) | Introduction to MacBERT | | [Download](#Download) | Download links for MacBERT | | [Quick Load](#Quick-Load) | Learn how to quickly load our models through [🤗Transformers](https://github.com/huggingface/transformers) | | [Results](#Results) | Results on several Chinese NLP datasets | | [FAQ](#FAQ) | Frequently Asked Questions | | [Citation](#Citation) | Citation |    If you need these models in PyTorch/TensorFlow2    2) Download from https://huggingface.co/hfl   """;Natural Language Processing;https://github.com/ymcui/MacBERT
"""refrence that i followed: https://machinelearningmastery.com/cyclegan-tutorial-with-keras/   CycleGAN github: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix   if you are egyptian or someone who can't open medium website you can use this extension:   drive: https://drive.google.com/drive/folders/1jNj-ao5Ybb5HxuKZ3ZFKmmn04Sx2aOsv?usp=sharing   """;Computer Vision;https://github.com/NDCHIRO/Cartoonizer
""": Get the extended-vocab probability distribution   python version：3.6  pytorch version：1.3.1   """;General;https://github.com/BugOMan/summary_generator
"""For running the majority of the provided Meta-RL environments  the Mujoco physics engine as well as a  corresponding python wrapper are required. For setting up [Mujoco](http://www.mujoco.org/) and [mujoco-py](https://github.com/openai/mujoco-py)   please follow the instructions [here](https://github.com/openai/mujoco-py).     ``` pip install -r requirements.txt ```   Ensure that you have a working MPI implementation ([see here](https://mpi4py.readthedocs.io/en/stable/install.html) for more instructions).   For Ubuntu you can install MPI through the package manager:  ``` sudo apt-get install libopenmpi-dev ```   The provided code can be either run in A) docker container provided by us or B) using python on your local machine. The latter requires multiple installation steps in order to setup dependencies.   pip install --upgrade virtualenv   """;General;https://github.com/Zhiwei-Z/prompzzw
"""<p align=""center"">   <img src=""https://github.com/raunak-sood2003/Automated-Lung-Segmentation/blob/master/Images/lung_mask_pred_unet15.png"" /> </p>  """;Computer Vision;https://github.com/raunak-sood2003/Automated-Lung-Segmentation
"""**EmoCause** is a dataset of annotated emotion cause words in emotional situations from the [EmpatheticDialogues](https://aclanthology.org/P19-1534.pdf) valid and test set. The goal is to recognize emotion cause words in sentences by training only on sentence-level emotion labels without word-level labels (*i.e.  weakly-supervised emotion cause recognition*). **EmoCause** is based on the fact that humans do not recognize the cause of emotions with supervised learning on word-level cause labels. Thus  we do not provide a training set.  ![figure](images/many_emocause.png)  You can download the **EmoCause** eval set [[here]](https://drive.google.com/file/d/1LR4B47Fna_l63G1X4DZtuttG-GrinnaY/view?usp=sharing).<br> Note  the dataset will be downloaded automatically when you run the experiment command below.    Our code is built on the [ParlAI](https://parl.ai/) framework. We recommend you create a conda environment as follows  ```bash conda env create -f environment.yml ```  and activate it with  ```bash conda activate focused-empathy python -m spacy download en ```   """;Natural Language Processing;https://github.com/skywalker023/focused-empathy
"""A few images of Duke campus landmarks painted in the style of various Renaissance painters. ![alt text](https://github.com/avellal14/AI-for-Art/blob/master/AI_for_Art_1.png) ![alt text](https://github.com/avellal14/AI-for-Art/blob/master/AI_for_Art_2.png) ![alt text](https://github.com/avellal14/AI-for-Art/blob/master/AI_for_Art_3.png) ![alt text](https://github.com/avellal14/AI-for-Art/blob/master/AI_for_Art_4.png) ![alt text](https://github.com/avellal14/AI-for-Art/blob/master/AI_for_Art_5.png) ![alt text](https://github.com/avellal14/AI-for-Art/blob/master/AI_for_Art_6.png) ![alt text](https://github.com/avellal14/AI-for-Art/blob/master/AI_for_Art_7.png)  """;General;https://github.com/avellal14/AI-for-Art
"""For 进阶者（Like You）：   For 大神（May Be Like You）：   Patches Are All You Need?---ICLR2022 (Under Review)   ```python from model.attention.ExternalAttention import ExternalAttention import torch  input=torch.randn(50 49 512) ea = ExternalAttention(d_model=512 S=8) output=ea(input) print(output.shape) ```  ***    ```python from model.attention.SelfAttention import ScaledDotProductAttention import torch  input=torch.randn(50 49 512) sa = ScaledDotProductAttention(d_model=512  d_k=512  d_v=512  h=8) output=sa(input input input) print(output.shape) ```  ***   ```python from model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention import torch  input=torch.randn(50 49 512) ssa = SimplifiedScaledDotProductAttention(d_model=512  h=8) output=ssa(input input input) print(output.shape)  ```  ***   ```python from model.attention.SEAttention import SEAttention import torch  input=torch.randn(50 512 7 7) se = SEAttention(channel=512 reduction=8) output=se(input) print(output.shape)  ```  ***   ```python from model.attention.SKAttention import SKAttention import torch  input=torch.randn(50 512 7 7) se = SKAttention(channel=512 reduction=8) output=se(input) print(output.shape)  ``` ***   ```python from model.attention.CBAM import CBAMBlock import torch  input=torch.randn(50 512 7 7) kernel_size=input.shape[2] cbam = CBAMBlock(channel=512 reduction=16 kernel_size=kernel_size) output=cbam(input) print(output.shape)  ```  ***   ```python from model.attention.BAM import BAMBlock import torch  input=torch.randn(50 512 7 7) bam = BAMBlock(channel=512 reduction=16 dia_val=2) output=bam(input) print(output.shape)  ```  ***   ```python from model.attention.ECAAttention import ECAAttention import torch  input=torch.randn(50 512 7 7) eca = ECAAttention(kernel_size=3) output=eca(input) print(output.shape)  ```  ***   ```python from model.attention.DANet import DAModule import torch  input=torch.randn(50 512 7 7) danet=DAModule(d_model=512 kernel_size=3 H=7 W=7) print(danet(input).shape)  ```  ***   ```python from model.attention.PSA import PSA import torch  input=torch.randn(50 512 7 7) psa = PSA(channel=512 reduction=8) output=psa(input) print(output.shape)  ```  ***    ```python  from model.attention.EMSA import EMSA import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 64 512) emsa = EMSA(d_model=512  d_k=512  d_v=512  h=8 H=8 W=8 ratio=2 apply_transform=True) output=emsa(input input input) print(output.shape)      ```  ***    ```python  from model.attention.ShuffleAttention import ShuffleAttention import torch from torch import nn from torch.nn import functional as F   input=torch.randn(50 512 7 7) se = ShuffleAttention(channel=512 G=8) output=se(input) print(output.shape)       ```   ***    ```python from model.attention.MUSEAttention import MUSEAttention import torch from torch import nn from torch.nn import functional as F   input=torch.randn(50 49 512) sa = MUSEAttention(d_model=512  d_k=512  d_v=512  h=8) output=sa(input input input) print(output.shape)  ```  ***    ```python from model.attention.SGE import SpatialGroupEnhance import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 512 7 7) sge = SpatialGroupEnhance(groups=8) output=sge(input) print(output.shape)  ```  ***    ```python from model.attention.A2Atttention import DoubleAttention import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 512 7 7) a2 = DoubleAttention(512 128 128 True) output=a2(input) print(output.shape)  ```     ```python from model.attention.AFT import AFT_FULL import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 49 512) aft_full = AFT_FULL(d_model=512  n=49) output=aft_full(input) print(output.shape)  ```        ```python from model.attention.OutlookAttention import OutlookAttention import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 28 28 512) outlook = OutlookAttention(dim=512) output=outlook(input) print(output.shape)  ```   ***        ```python  from model.attention.ViP import WeightedPermuteMLP import torch from torch import nn from torch.nn import functional as F  input=torch.randn(64 8 8 512) seg_dim=8 vip=WeightedPermuteMLP(512 seg_dim) out=vip(input) print(out.shape)  ```   ***       ```python  from model.attention.CoAtNet import CoAtNet import torch from torch import nn from torch.nn import functional as F  input=torch.randn(1 3 224 224) mbconv=CoAtNet(in_ch=3 image_size=224) out=mbconv(input) print(out.shape)  ```   ***        ```python  from model.attention.HaloAttention import HaloAttention import torch from torch import nn from torch.nn import functional as F  input=torch.randn(1 512 8 8) halo = HaloAttention(dim=512      block_size=2      halo_size=1 ) output=halo(input) print(output.shape)  ```   ***   ```python  from model.attention.PolarizedSelfAttention import ParallelPolarizedSelfAttention SequentialPolarizedSelfAttention import torch from torch import nn from torch.nn import functional as F  input=torch.randn(1 512 7 7) psa = SequentialPolarizedSelfAttention(channel=512) output=psa(input) print(output.shape)   ```   ***    ```python  from model.attention.CoTAttention import CoTAttention import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 512 7 7) cot = CoTAttention(dim=512 kernel_size=3) output=cot(input) print(output.shape)    ```  ***    ```python  from model.attention.ResidualAttention import ResidualAttention import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 512 7 7) resatt = ResidualAttention(channel=512 num_class=1000 la=0.2) output=resatt(input) print(output.shape)    ```  ***     ```python from model.attention.S2Attention import S2Attention import torch from torch import nn from torch.nn import functional as F  input=torch.randn(50 512 7 7) s2att = S2Attention(channels=512) output=s2att(input) print(output.shape)  ```  ***     ```python from model.attention.gfnet import GFNet import torch from torch import nn from torch.nn import functional as F  x = torch.randn(1  3  224  224) gfnet = GFNet(embed_dim=384  img_size=224  patch_size=16  num_classes=1000) out = gfnet(x) print(out.shape)  ```  ***    ```python from model.attention.TripletAttention import TripletAttention import torch from torch import nn from torch.nn import functional as F input=torch.randn(50 512 7 7) triplet = TripletAttention() output=triplet(input) print(output.shape) ```   ***    ```python from model.attention.CoordAttention import CoordAtt import torch from torch import nn from torch.nn import functional as F  inp=torch.rand([2  96  56  56]) inp_dim  oup_dim = 96  96 reduction=32  coord_attention = CoordAtt(inp_dim  oup_dim  reduction=reduction) output=coord_attention(inp) print(output.shape) ```  ***    ```python from model.attention.MobileViTAttention import MobileViTAttention import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     m=MobileViTAttention()     input=torch.randn(1 3 49 49)     output=m(input)     print(output.shape)  #:output:(1 3 49 49)      ```  ***    ```python from model.attention.ParNetAttention import * import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     input=torch.randn(50 512 7 7)     pna = ParNetAttention(channel=512)     output=pna(input)     print(output.shape) #:50 512 7 7      ```  ***    ```python from model.attention.UFOAttention import * import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     input=torch.randn(50 49 512)     ufo = UFOAttention(d_model=512  d_k=512  d_v=512  h=8)     output=ufo(input input input)     print(output.shape) #:[50  49  512]      ```  ***    ```python  from model.backbone.resnet import ResNet50 ResNet101 ResNet152 import torch if __name__ == '__main__':     input=torch.randn(50 3 224 224)     resnet50=ResNet50(1000)     #: resnet101=ResNet101(1000)     #: resnet152=ResNet152(1000)     out=resnet50(input)     print(out.shape)  ```    ```python  from model.backbone.resnext import ResNeXt50 ResNeXt101 ResNeXt152 import torch  if __name__ == '__main__':     input=torch.randn(50 3 224 224)     resnext50=ResNeXt50(1000)     #: resnext101=ResNeXt101(1000)     #: resnext152=ResNeXt152(1000)     out=resnext50(input)     print(out.shape)   ```     ```python  from model.backbone.MobileViT import * import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     input=torch.randn(1 3 224 224)      #:#:#: mobilevit_xxs     mvit_xxs=mobilevit_xxs()     out=mvit_xxs(input)     print(out.shape)      #:#:#: mobilevit_xs     mvit_xs=mobilevit_xs()     out=mvit_xs(input)     print(out.shape)       #:#:#: mobilevit_s     mvit_s=mobilevit_s()     out=mvit_s(input)     print(out.shape)  ```       ```python  from model.backbone.ConvMixer import * import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     x=torch.randn(1 3 224 224)     convmixer=ConvMixer(dim=512 depth=12)     out=convmixer(x)     print(out.shape)  #:[1  1000]   ```         ```python from model.mlp.repmlp import RepMLP import torch from torch import nn  N=4 #:batch size C=512 #:input dim O=1024 #:output dim H=14 #:image height W=14 #:image width h=7 #:patch height w=7 #:patch width fc1_fc2_reduction=1 #:reduction ratio fc3_groups=8 #: groups repconv_kernels=[1 3 5 7] #:kernel list repmlp=RepMLP(C O H W h w fc1_fc2_reduction fc3_groups repconv_kernels=repconv_kernels) x=torch.randn(N C H W) repmlp.eval() for module in repmlp.modules():     if isinstance(module  nn.BatchNorm2d) or isinstance(module  nn.BatchNorm1d):         nn.init.uniform_(module.running_mean  0  0.1)         nn.init.uniform_(module.running_var  0  0.1)         nn.init.uniform_(module.weight  0  0.1)         nn.init.uniform_(module.bias  0  0.1)  #:training result out=repmlp(x) #:inference result repmlp.switch_to_deploy() deployout = repmlp(x)  print(((deployout-out)**2).sum()) ```   ```python from model.mlp.mlp_mixer import MlpMixer import torch mlp_mixer=MlpMixer(num_classes=1000 num_blocks=10 patch_size=10 tokens_hidden_dim=32 channels_hidden_dim=1024 tokens_mlp_dim=16 channels_mlp_dim=1024) input=torch.randn(50 3 40 40) output=mlp_mixer(input) print(output.shape) ```  ***   ```python from model.mlp.resmlp import ResMLP import torch  input=torch.randn(50 3 14 14) resmlp=ResMLP(dim=128 image_size=14 patch_size=7 class_num=1000) out=resmlp(input) print(out.shape) #:the last dimention is class_num ```  ***   ```python from model.mlp.g_mlp import gMLP import torch  num_tokens=10000 bs=50 len_sen=49 num_layers=6 input=torch.randint(num_tokens (bs len_sen)) #:bs len_sen gmlp = gMLP(num_tokens=num_tokens len_sen=len_sen dim=512 d_ff=1024) output=gmlp(input) print(output.shape) ```  ***   ```python from model.mlp.sMLP_block import sMLPBlock import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     input=torch.randn(50 3 224 224)     smlp=sMLPBlock(h=224 w=224)     out=smlp(input)     print(out.shape) ```    ```python  from model.rep.repvgg import RepBlock import torch   input=torch.randn(50 512 49 49) repblock=RepBlock(512 512) repblock.eval() out=repblock(input) repblock._switch_to_deploy() out2=repblock(input) print('difference between vgg and repvgg') print(((out2-out)**2).sum()) ```    ***   ```python from model.rep.acnet import ACNet import torch from torch import nn  input=torch.randn(50 512 49 49) acnet=ACNet(512 512) acnet.eval() out=acnet(input) acnet._switch_to_deploy() out2=acnet(input) print('difference:') print(((out2-out)**2).sum())  ```    ***   ```python from model.conv.DepthwiseSeparableConvolution import DepthwiseSeparableConvolution import torch from torch import nn from torch.nn import functional as F  input=torch.randn(1 3 224 224) dsconv=DepthwiseSeparableConvolution(3 64) out=dsconv(input) print(out.shape) ```  ***    ```python from model.conv.MBConv import MBConvBlock import torch from torch import nn from torch.nn import functional as F  input=torch.randn(1 3 224 224) mbconv=MBConvBlock(ksize=3 input_filters=3 output_filters=512 image_size=224) out=mbconv(input) print(out.shape)   ```  ***    ```python from model.conv.Involution import Involution import torch from torch import nn from torch.nn import functional as F  input=torch.randn(1 4 64 64) involution=Involution(kernel_size=3 in_channel=4 stride=2) out=involution(input) print(out.shape) ```  ***    ```python from model.conv.DynamicConv import * import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     input=torch.randn(2 32 64 64)     m=DynamicConv(in_planes=32 out_planes=64 kernel_size=3 stride=1 padding=1 bias=False)     out=m(input)     print(out.shape) #: 2 32 64 64  ```  ***    ```python from model.conv.CondConv import * import torch from torch import nn from torch.nn import functional as F  if __name__ == '__main__':     input=torch.randn(2 32 64 64)     m=CondConv(in_planes=32 out_planes=64 kernel_size=3 stride=1 padding=1 bias=False)     out=m(input)     print(out.shape)  ```  ***  """;General;https://github.com/xmu-xiaoma666/External-Attention-pytorch
"""Recent advancement in deep learning has become one of the most powerful tools to solve the image classification and segmentation problem.Deep learning model learn the filter that helps in extraction and learning of the important feature form the images. These feature helps to find differences as well as similarities amongst the image. Deep learning models require large dataset to learn the complex data representation.In the paper[1] the authors have used DCNN model to find the green cover in the cities using Cityscapes dataset. Cityscapes dataset has 2975 images and mask of green cover of different cities around the world which was used as the training data and 500 image with masks were used as testing dataset. The images were google street view images. The DCNN model has an IOU of 61.2 percent. In this approach I used state of the art unet model and mobile net v2 model. Unet gave an IOU of 74.5 percent and mobile net v2 model gave an IOU of 64.3 percent which were better and lighter model than previously used DCNN model. The model were even tested on different machine type with different configuration to check their performance.   """;General;https://github.com/anant1203/Applying-Deep-Learning-for-Large-scale-Quantification-of-Urban-Tree-Cover
"""  1. Clone this repository:   ```   git clone https://github.com/salinasJJ/BBpose.git    ```   2. Create a virtual environment (using Pipenv or Conda for example).    3. Install the project onto your system:   ```   pip install -e BBpose   ```                                                                  4. Install dependencies:   ```   pip install -r BBpose/bbpose/requirements.txt   ```   5. Make script executable:    ```   chmod +x BBpose/bbpose/ingestion/scripts/data.sh           ```           --version 0 \   Note #2: CHECK VERSION NUMBER! Users should get into the practice of always checking which version number is being set in order to avoid overwriting old models.   """;Computer Vision;https://github.com/salinasJJ/BBpose
"""- `python3 train.py <exp_name>` - Config file must be provided at `./configs/<exp_name>.py` - experiment logs saved in `./EXP_LOGS/log_<exp_name>.txt`   """;General;https://github.com/GaParmar/WGAN-GP
"""The repository has the following structure:   Instead  a dropout with a probability p=0.2 (20%) seems to get similar metric performance.   """;Computer Vision;https://github.com/imagesegmentation2020/indoor
""".. image:: https://img.shields.io/circleci/build/github/silvandeleemput/memcnn/master.svg         .. image:: https://readthedocs.org/projects/memcnn/badge/?version=latest         .. image:: https://img.shields.io/pypi/v/memcnn.svg          :alt: PyPI - Latest release          :target: https://pypi.python.org/pypi/memcnn  .. image:: https://img.shields.io/conda/vn/silvandeleemput/memcnn?label=anaconda          :alt: Conda - Latest release           :target: https://pypi.python.org/pypi/memcnn  .. image:: https://img.shields.io/pypi/pyversions/memcnn.svg                :alt: PyPI - Python version          :target: https://pypi.python.org/pypi/memcnn   Installation: https://memcnn.readthedocs.io/en/latest/installation.html   | revnet-38  |             2:17       |    2:09                  |       2:20           |              2:16    |   Creating an AdditiveCoupling with memory savings ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  .. code:: python      import torch     import torch.nn as nn     import memcnn       # define a new torch Module with a sequence of operations: Relu o BatchNorm2d o Conv2d     class ExampleOperation(nn.Module):         def __init__(self  channels):             super(ExampleOperation  self).__init__()             self.seq = nn.Sequential(                                         nn.Conv2d(in_channels=channels  out_channels=channels                                                    kernel_size=(3  3)  padding=1)                                          nn.BatchNorm2d(num_features=channels)                                          nn.ReLU(inplace=True)                                     )          def forward(self  x):             return self.seq(x)       # generate some random input data (batch_size  num_channels  y_elements  x_elements)     X = torch.rand(2  10  8  8)      # application of the operation(s) the normal way     model_normal = ExampleOperation(channels=10)     model_normal.eval()      Y = model_normal(X)      # turn the ExampleOperation invertible using an additive coupling     invertible_module = memcnn.AdditiveCoupling(         Fm=ExampleOperation(channels=10 // 2)          Gm=ExampleOperation(channels=10 // 2)     )      # test that it is actually a valid invertible module (has a valid inverse method)     assert memcnn.is_invertible_module(invertible_module  test_input_shape=X.shape)      # wrap our invertible_module using the InvertibleModuleWrapper and benefit from memory savings during training     invertible_module_wrapper = memcnn.InvertibleModuleWrapper(fn=invertible_module  keep_input=True  keep_input_inverse=True)      # by default the module is set to training  the following sets this to evaluation     # note that this is required to pass input tensors to the model with requires_grad=False (inference only)     invertible_module_wrapper.eval()      # test that the wrapped module is also a valid invertible module     assert memcnn.is_invertible_module(invertible_module_wrapper  test_input_shape=X.shape)      # compute the forward pass using the wrapper     Y2 = invertible_module_wrapper.forward(X)      # the input (X) can be approximated (X2) by applying the inverse method of the wrapper on Y2     X2 = invertible_module_wrapper.inverse(Y2)      # test that the input and approximation are similar     assert torch.allclose(X  X2  atol=1e-06)   """;Computer Vision;https://github.com/silvandeleemput/memcnn
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/DaVran369/face
"""| Name           | Type  | Default                       | Description                         | | -------------- | ----- | ----------------------------- | ----------------------------------- | | data_dir       | str   | ""modelnet40_normal_resampled"" | train & test data dir               | | num_point      | int   | 1024                          | sample number of points             | | batch_size     | int   | 32                            | batch size in training              | | num_category   | int   | 40                            | ModelNet10/40                       | | learning_rate  | float | 1e-3                          | learning rate in training           | | max_epochs     | int   | 200                           | max epochs in training              | | num_workers    | int   | 32                            | number of workers in dataloader     | | log_batch_num  | int   | 50                            | log info per log_batch_num          | | model_path     | str   | ""pointnet.pdparams""           | save/load model in training/testing | | lr_decay_step  | int   | 20                            | step_size in StepDecay              | | lr_decay_gamma | float | 0.7                           | gamma in StepDecay                  |   This project reproduces PointNet based on paddlepaddle framework.  PointNet provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  **Paper:** [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/pdf/1612.00593.pdf)  **Competition Page:** [PaddlePaddle AI Studio](https://aistudio.baidu.com/aistudio/competition/detail/106)  **PointNet Architecture:** ![arch](arch.png)  **Other Version Implementation:**  - [TensorFlow (Official)](https://github.com/charlesq34/pointnet) - [PyTorch](https://github.com/yanx27/Pointnet_Pointnet2_pytorch)  **Acceptance condition**  - Classification Accuracy 89.2 on ModelNet40 Dataset   Download [alignment ModelNet](https://shapenet.cs.stanford.edu/media/modelnet40_normal_resampled.zip) and save in `modelnet40_normal_resampled/`. The same dataset as the PyTorch version implementation.  ``` wget https://shapenet.cs.stanford.edu/media/modelnet40_normal_resampled.zip unzip modelnet40_normal_resampled.zip ```   | PointNet (PyTorch)      | 90.6     |   Hardware: GPU/CPU   ├── requirements.txt   | Framework version | Paddle 2.1.2                                                          |  | Support hardware  | GPU/CPU                                                               |  | Download link     | pointnet.pdparams                              |   """;Computer Vision;https://github.com/Phimos/Paddle-PointNet
"""This repository contains the code for the blog post: [Using Microsoft AI to Build a Lung-Disease Prediction Model using Chest X-Ray Images](https://blogs.technet.microsoft.com/machinelearning/2018/03/07/using-microsoft-ai-to-build-a-lung-disease-prediction-model-using-chest-x-ray-images/)  by Xiaoyong Zhu  George Iordanescu  Ilia Karmanov  data scientists from Microsoft  and Mazen Zawaideh  radiologist resident from University of Washington Medical Center.  In this repostory  we provide you the Keras code (`001-003 Jupyter Notebooks under AzureChestXRay_AMLWB\Code\02_Model`) and PyTorch code (`AzureChestXRay_AMLWB\Code\02_Model060_Train_pyTorch`). You should be able to run the code from scratch and get the below result using Azure Machine Learning platform or run it using your own GPU machine.   If you are using Azure Machine Learning as the training platform  all the dependencies should be installed. However  if you are trying out in your own environment  you should also install [keras-contrib](https://github.com/keras-team/keras-contrib) repository to run Keras code.  If you are trying out the lung detection algorithm  you need to install a few other additional libraries. Please refer to the `README.md` file under folder `AzureChestXRay\AzureChestXRay_AMLWB\Code\src\finding_lungs` for more details.   """;Computer Vision;https://github.com/Azure/AzureChestXRay
"""```python device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') model = ResNetUNet(n_class=6) model = model.to(device)  #: check keras-like model summary using torchsummary from torchsummary import summary summary(model  input_size=(3  224  224)) ```      ----------------------------------------------------------------             Layer (type)               Output Shape         Param #     ================================================================                 Conv2d-1         [-1  64  224  224]           1 792                   ReLU-2         [-1  64  224  224]               0                 Conv2d-3         [-1  64  224  224]          36 928                   ReLU-4         [-1  64  224  224]               0                 Conv2d-5         [-1  64  112  112]           9 408            BatchNorm2d-6         [-1  64  112  112]             128                   ReLU-7         [-1  64  112  112]               0              MaxPool2d-8           [-1  64  56  56]               0                 Conv2d-9           [-1  64  56  56]           4 096           BatchNorm2d-10           [-1  64  56  56]             128                  ReLU-11           [-1  64  56  56]               0                Conv2d-12           [-1  64  56  56]          36 864           BatchNorm2d-13           [-1  64  56  56]             128                  ReLU-14           [-1  64  56  56]               0                Conv2d-15          [-1  256  56  56]          16 384           BatchNorm2d-16          [-1  256  56  56]             512                Conv2d-17          [-1  256  56  56]          16 384           BatchNorm2d-18          [-1  256  56  56]             512                  ReLU-19          [-1  256  56  56]               0            Bottleneck-20          [-1  256  56  56]               0                Conv2d-21           [-1  64  56  56]          16 384           BatchNorm2d-22           [-1  64  56  56]             128                  ReLU-23           [-1  64  56  56]               0                Conv2d-24           [-1  64  56  56]          36 864           BatchNorm2d-25           [-1  64  56  56]             128                  ReLU-26           [-1  64  56  56]               0                Conv2d-27          [-1  256  56  56]          16 384           BatchNorm2d-28          [-1  256  56  56]             512                  ReLU-29          [-1  256  56  56]               0            Bottleneck-30          [-1  256  56  56]               0                Conv2d-31           [-1  64  56  56]          16 384           BatchNorm2d-32           [-1  64  56  56]             128                  ReLU-33           [-1  64  56  56]               0                Conv2d-34           [-1  64  56  56]          36 864           BatchNorm2d-35           [-1  64  56  56]             128                  ReLU-36           [-1  64  56  56]               0                Conv2d-37          [-1  256  56  56]          16 384           BatchNorm2d-38          [-1  256  56  56]             512                  ReLU-39          [-1  256  56  56]               0            Bottleneck-40          [-1  256  56  56]               0                Conv2d-41          [-1  128  56  56]          32 768           BatchNorm2d-42          [-1  128  56  56]             256                  ReLU-43          [-1  128  56  56]               0                Conv2d-44          [-1  128  28  28]         147 456           BatchNorm2d-45          [-1  128  28  28]             256                  ReLU-46          [-1  128  28  28]               0                Conv2d-47          [-1  512  28  28]          65 536           BatchNorm2d-48          [-1  512  28  28]           1 024                Conv2d-49          [-1  512  28  28]         131 072           BatchNorm2d-50          [-1  512  28  28]           1 024                  ReLU-51          [-1  512  28  28]               0            Bottleneck-52          [-1  512  28  28]               0                Conv2d-53          [-1  128  28  28]          65 536           BatchNorm2d-54          [-1  128  28  28]             256                  ReLU-55          [-1  128  28  28]               0                Conv2d-56          [-1  128  28  28]         147 456           BatchNorm2d-57          [-1  128  28  28]             256                  ReLU-58          [-1  128  28  28]               0                Conv2d-59          [-1  512  28  28]          65 536           BatchNorm2d-60          [-1  512  28  28]           1 024                  ReLU-61          [-1  512  28  28]               0            Bottleneck-62          [-1  512  28  28]               0                Conv2d-63          [-1  128  28  28]          65 536           BatchNorm2d-64          [-1  128  28  28]             256                  ReLU-65          [-1  128  28  28]               0                Conv2d-66          [-1  128  28  28]         147 456           BatchNorm2d-67          [-1  128  28  28]             256                  ReLU-68          [-1  128  28  28]               0                Conv2d-69          [-1  512  28  28]          65 536           BatchNorm2d-70          [-1  512  28  28]           1 024                  ReLU-71          [-1  512  28  28]               0            Bottleneck-72          [-1  512  28  28]               0                Conv2d-73          [-1  128  28  28]          65 536           BatchNorm2d-74          [-1  128  28  28]             256                  ReLU-75          [-1  128  28  28]               0                Conv2d-76          [-1  128  28  28]         147 456           BatchNorm2d-77          [-1  128  28  28]             256                  ReLU-78          [-1  128  28  28]               0                Conv2d-79          [-1  512  28  28]          65 536           BatchNorm2d-80          [-1  512  28  28]           1 024                  ReLU-81          [-1  512  28  28]               0            Bottleneck-82          [-1  512  28  28]               0                Conv2d-83          [-1  256  28  28]         131 072           BatchNorm2d-84          [-1  256  28  28]             512                  ReLU-85          [-1  256  28  28]               0                Conv2d-86          [-1  256  14  14]         589 824           BatchNorm2d-87          [-1  256  14  14]             512                  ReLU-88          [-1  256  14  14]               0                Conv2d-89         [-1  1024  14  14]         262 144           BatchNorm2d-90         [-1  1024  14  14]           2 048                Conv2d-91         [-1  1024  14  14]         524 288           BatchNorm2d-92         [-1  1024  14  14]           2 048                  ReLU-93         [-1  1024  14  14]               0            Bottleneck-94         [-1  1024  14  14]               0                Conv2d-95          [-1  256  14  14]         262 144           BatchNorm2d-96          [-1  256  14  14]             512                  ReLU-97          [-1  256  14  14]               0                Conv2d-98          [-1  256  14  14]         589 824           BatchNorm2d-99          [-1  256  14  14]             512                 ReLU-100          [-1  256  14  14]               0               Conv2d-101         [-1  1024  14  14]         262 144          BatchNorm2d-102         [-1  1024  14  14]           2 048                 ReLU-103         [-1  1024  14  14]               0           Bottleneck-104         [-1  1024  14  14]               0               Conv2d-105          [-1  256  14  14]         262 144          BatchNorm2d-106          [-1  256  14  14]             512                 ReLU-107          [-1  256  14  14]               0               Conv2d-108          [-1  256  14  14]         589 824          BatchNorm2d-109          [-1  256  14  14]             512                 ReLU-110          [-1  256  14  14]               0               Conv2d-111         [-1  1024  14  14]         262 144          BatchNorm2d-112         [-1  1024  14  14]           2 048                 ReLU-113         [-1  1024  14  14]               0           Bottleneck-114         [-1  1024  14  14]               0               Conv2d-115          [-1  256  14  14]         262 144          BatchNorm2d-116          [-1  256  14  14]             512                 ReLU-117          [-1  256  14  14]               0               Conv2d-118          [-1  256  14  14]         589 824          BatchNorm2d-119          [-1  256  14  14]             512                 ReLU-120          [-1  256  14  14]               0               Conv2d-121         [-1  1024  14  14]         262 144          BatchNorm2d-122         [-1  1024  14  14]           2 048                 ReLU-123         [-1  1024  14  14]               0           Bottleneck-124         [-1  1024  14  14]               0               Conv2d-125          [-1  256  14  14]         262 144          BatchNorm2d-126          [-1  256  14  14]             512                 ReLU-127          [-1  256  14  14]               0               Conv2d-128          [-1  256  14  14]         589 824          BatchNorm2d-129          [-1  256  14  14]             512                 ReLU-130          [-1  256  14  14]               0               Conv2d-131         [-1  1024  14  14]         262 144          BatchNorm2d-132         [-1  1024  14  14]           2 048                 ReLU-133         [-1  1024  14  14]               0           Bottleneck-134         [-1  1024  14  14]               0               Conv2d-135          [-1  256  14  14]         262 144          BatchNorm2d-136          [-1  256  14  14]             512                 ReLU-137          [-1  256  14  14]               0               Conv2d-138          [-1  256  14  14]         589 824          BatchNorm2d-139          [-1  256  14  14]             512                 ReLU-140          [-1  256  14  14]               0               Conv2d-141         [-1  1024  14  14]         262 144          BatchNorm2d-142         [-1  1024  14  14]           2 048                 ReLU-143         [-1  1024  14  14]               0           Bottleneck-144         [-1  1024  14  14]               0               Conv2d-145          [-1  512  14  14]         524 288          BatchNorm2d-146          [-1  512  14  14]           1 024                 ReLU-147          [-1  512  14  14]               0               Conv2d-148            [-1  512  7  7]       2 359 296          BatchNorm2d-149            [-1  512  7  7]           1 024                 ReLU-150            [-1  512  7  7]               0               Conv2d-151           [-1  2048  7  7]       1 048 576          BatchNorm2d-152           [-1  2048  7  7]           4 096               Conv2d-153           [-1  2048  7  7]       2 097 152          BatchNorm2d-154           [-1  2048  7  7]           4 096                 ReLU-155           [-1  2048  7  7]               0           Bottleneck-156           [-1  2048  7  7]               0               Conv2d-157            [-1  512  7  7]       1 048 576          BatchNorm2d-158            [-1  512  7  7]           1 024                 ReLU-159            [-1  512  7  7]               0               Conv2d-160            [-1  512  7  7]       2 359 296          BatchNorm2d-161            [-1  512  7  7]           1 024                 ReLU-162            [-1  512  7  7]               0               Conv2d-163           [-1  2048  7  7]       1 048 576          BatchNorm2d-164           [-1  2048  7  7]           4 096                 ReLU-165           [-1  2048  7  7]               0           Bottleneck-166           [-1  2048  7  7]               0               Conv2d-167            [-1  512  7  7]       1 048 576          BatchNorm2d-168            [-1  512  7  7]           1 024                 ReLU-169            [-1  512  7  7]               0               Conv2d-170            [-1  512  7  7]       2 359 296          BatchNorm2d-171            [-1  512  7  7]           1 024                 ReLU-172            [-1  512  7  7]               0               Conv2d-173           [-1  2048  7  7]       1 048 576          BatchNorm2d-174           [-1  2048  7  7]           4 096                 ReLU-175           [-1  2048  7  7]               0           Bottleneck-176           [-1  2048  7  7]               0               Conv2d-177           [-1  1024  7  7]       2 098 176                 ReLU-178           [-1  1024  7  7]               0             Upsample-179         [-1  1024  14  14]               0               Conv2d-180          [-1  512  14  14]         524 800                 ReLU-181          [-1  512  14  14]               0               Conv2d-182          [-1  512  14  14]       7 078 400                 ReLU-183          [-1  512  14  14]               0             Upsample-184          [-1  512  28  28]               0               Conv2d-185          [-1  512  28  28]         262 656                 ReLU-186          [-1  512  28  28]               0               Conv2d-187          [-1  512  28  28]       4 719 104                 ReLU-188          [-1  512  28  28]               0             Upsample-189          [-1  512  56  56]               0               Conv2d-190          [-1  256  56  56]          65 792                 ReLU-191          [-1  256  56  56]               0               Conv2d-192          [-1  256  56  56]       1 769 728                 ReLU-193          [-1  256  56  56]               0             Upsample-194        [-1  256  112  112]               0               Conv2d-195         [-1  64  112  112]           4 160                 ReLU-196         [-1  64  112  112]               0               Conv2d-197        [-1  128  112  112]         368 768                 ReLU-198        [-1  128  112  112]               0             Upsample-199        [-1  128  224  224]               0               Conv2d-200         [-1  64  224  224]         110 656                 ReLU-201         [-1  64  224  224]               0               Conv2d-202          [-1  6  224  224]             390     ================================================================     Total params: 40 549 382     Trainable params: 40 549 382     Non-trainable params: 0     ----------------------------------------------------------------    ```python from torch.utils.data import Dataset  DataLoader from torchvision import transforms  datasets  models  class SimDataset(Dataset):     def __init__(self  count  transform=None):         self.input_images  self.target_masks = simulation.generate_random_data(192  192  count=count)         self.transform = transform      def __len__(self):         return len(self.input_images)      def __getitem__(self  idx):         image = self.input_images[idx]         mask = self.target_masks[idx]         if self.transform:             image = self.transform(image)          return [image  mask]  #: use the same transformations for train/val in this example trans = transforms.Compose([     transforms.ToTensor()      transforms.Normalize([0.485  0.456  0.406]  [0.229  0.224  0.225]) #: imagenet ])  train_set = SimDataset(2000  transform = trans) val_set = SimDataset(200  transform = trans)  image_datasets = {     'train': train_set  'val': val_set }  batch_size = 25  dataloaders = {     'train': DataLoader(train_set  batch_size=batch_size  shuffle=True  num_workers=0)      'val': DataLoader(val_set  batch_size=batch_size  shuffle=True  num_workers=0) } ```   First clone the repository and cd into the project directory.       inp = inp.numpy().transpose((1  2  0))   ```python import math  model.eval()   #: Set model to the evaluation mode  #: Create another simulation dataset for test test_dataset = SimDataset(3  transform = trans) test_loader = DataLoader(test_dataset  batch_size=3  shuffle=False  num_workers=0)  #: Get the first batch inputs  labels = next(iter(test_loader)) inputs = inputs.to(device) labels = labels.to(device)  #: Predict pred = model(inputs) #: The loss functions include the sigmoid function. pred = F.sigmoid(pred) pred = pred.data.cpu().numpy() print(pred.shape)  #: Change channel-order and make 3 channels for matplot input_images_rgb = [reverse_transform(x) for x in inputs.cpu()]  #: Map each channel (i.e. class) to each color target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()] pred_rgb = [helper.masks_to_colorimg(x) for x in pred]  helper.plot_side_by_side([input_images_rgb  target_masks_rgb  pred_rgb]) ```      (3  6  192  192)   """;Computer Vision;https://github.com/usuyama/pytorch-unet
"""ImageNet needs to be manually downloaded following the instructions here.   """;General;https://github.com/NivNayman/XNAS
"""`python3 vamperouge.py {adresseIP} {port}`  `pytorch` and `numpy` are required.   """;Reinforcement Learning;https://github.com/langorou/vamperouge
"""First step is having a dataset Dataset that can directly work - 	* maps 	* facade 	* night2day 	* edge2shoes 	* edge2handbages  After downloading the dataset  You need to put dataset in dataset folder and set the dataset name in main.py   You can change epoch  batch size  print frequency(for image generation)  image size and learning rate in mani.py as well  after that you go and run main.py  ``` python3 main.py ```  """;Computer Vision;https://github.com/gagan16/Cyclegan-tensorflow
"""Grab imagenet  [do standard pre-processing](https://github.com/soumith/imagenet-multiGPU.torch#data-processing) and use `--data-dir=${DATA_DIR}`. **Note:** This SimCLR implementation expects two pytorch `imagefolder` locations: `train` and `test` as opposed to `val` in the preprocessor above.   **NOTE0**: this will not produce SOTA results  but is good for debugging. The authors use a batch size of 4096+ for SOTA.     **NOTE1**: Setup your github ssh tokens; if you get an authentication issue from the git clone this is most likely it.   ``` bash > git clone --recursive git+ssh://git@github.com/jramapuram/SimCLR.git #: DATADIR is the location of imagenet or anything that works with imagefolder. > ./docker/run.sh ""python main.py --data-dir=$DATADIR \                                     --batch-size=64 \                                     --num-replicas=1 \                                     --epochs=100"" 0  #: add --debug-step to do a single minibatch ``` The bash script `docker/run.sh` pulls the appropriate docker container.   If you want to setup your own environment use:   - `environment.yml` (conda) in **addition** to   - `requirements.txt` (pip)      or just take a look at the Dockerfile in `docker/Dockerfile`.   Setup stuff according to the [slurm bash script](./slurm/run.sh). Then:  ``` bash > cd slurm && sbatch run.sh ```      1. Start each replica worker pointing to the master using `--distributed-master=`.   2. Set the total number of replicas appropriately using `--num-replicas=`.   3. Set each node to have a unique `--distributed-rank=` ranging from `[0  num_replicas)`.   3. Ensure network connectivity between workers. You will get NCCL errors if there are resolution problems here.   4. Profit.    For example  with a 2 node setup run the following on the master node: ```bash python main.py \      --epochs=100 \      --data-dir=<YOUR_DATA_DIR> \      --batch-size=128 \                   #: divides into 64 per node      --convert-to-sync-bn \      --visdom-url=http://MY_VISDOM_URL \  #: optional  not providing uses tensorboard      --visdom-port=8097 \                 #: optional  not providing uses tensorboard      --num-replicas=2 \                   #: specifies total available nodes  2 in this example           --distributed-master=127.0.0.1 \      --distributed-port=29301 \      --distributed-rank=0 \               #: rank-0 is the master      --uid=simclrv00_0 ```  and the following on the child node:  ```bash export MASTER=<IP_ADDR_OF_MASTER_ABOVE> python main.py \      --epochs=100 \      --data-dir=<YOUR_DATA_DIR> \      --batch-size=128 \                   #: divides into 64 per node      --convert-to-sync-bn \      --visdom-url=http://MY_VISDOM_URL \  #: optional  not providing uses tensorboard      --visdom-port=8097 \                 #: optional  not providing uses tensorboard      --num-replicas=2 \                   #: specifies total available nodes  2 in this example      --distributed-master=$MASTER \      --distributed-port=29301 \      --distributed-rank=1 \               #: rank-1 is this child  increment for extra nodes      --uid=simclrv00_0 ```    """;General;https://github.com/jramapuram/SimCLR
"""Follow the instruction [TensorFlow-Slim Models](https://github.com/tensorflow/models/tree/master/slim).   """;Computer Vision;https://github.com/pudae/tensorflow-densenet
"""To date  most open access public smart meter datasets are still at 30-minute or hourly temporal resolution. While this level of granularity could be sufficient for billing or deriving aggregated generation or consumption patterns  it may not fully capture the weather transients or consumption spikes. One potential solution is to synthetically interpolate high resolution data from commonly accessible lower resolution data  for this work  the SRGAN model is used for this purpose.  """;Computer Vision;https://github.com/tomtrac/SRGAN_power_data_generation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/mayurnewase/Quora-Bert
"""Code for the labs of the [Deep Learning course](https://uvadlc.github.io/) offered in MSc. in Artificial Intelligence at the University of Amsterdam.   """;General;https://github.com/KrishnaTarun/Deep-Learning-Lab
"""MLP Mixer is based on multi layer perceptron it does not use modern days CNN   It has two kinds of multi layer preceptrons one is directly applied to image patches   which are created original image then we transpose the layer and apply MLP layer across patches In the extreme case  Multi layer perceptron architecture can be seen as a very special CNN  which uses 1×1 convolutions for channel mixing  and single-channel depth-wise convolutions of a full receptive field and parameter sharing for token mixing. However  the converse is not true as typical CNNs are not special cases of Mixer. Furthermore  a convolution is more complex than the plain matrix multiplication in MLPs as it requires an additional costly reduction to matrix multiplication and/or specialized implementation. If you want to see training of cifar10 dataset using above architecture you can refer [here](https://github.com/imad08/MLP-Mixer/blob/main/MLP.ipynb)   ```bash $ python3 main.py  ``` NOTE: on Colab Notebook use following command: ```python !git clone link-to-repo %run main.py  ```   """;Computer Vision;https://github.com/imad08/MLP-Mixer
"""**Windows portable version**: Simply download and use the latest version from the [Releases](https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases) section.  Otherwise  check [doc/installation.md](doc/installation.md) for instructions on how to build OpenPose from source.     OS: Ubuntu (14  16)  Windows (8  10)  Mac OSX  Nvidia TX2.   CUDA (Nvidia GPU)  OpenCL (AMD GPU)  and CPU-only (no GPU) versions.   Most users do not need the OpenPose C++/Python API  but can simply use the OpenPose Demo:  - **OpenPose Demo**: To easily process images/video/webcam and display/save the results. See [doc/demo_overview.md](doc/demo_overview.md). E.g.  run OpenPose in a video with: ``` #: Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi ```  - **Calibration toolbox**: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See [doc/modules/calibration_module.md](doc/modules/calibration_module.md).  - **OpenPose C++ API**: If you want to read a specific input  and/or add your custom post-processing function  and/or implement your own display/saving  check the C++ API tutorial on [examples/tutorial_api_cpp/](examples/tutorial_api_cpp/) and [doc/library_introduction.md](doc/library_introduction.md). You can create your custom code on [examples/user_code/](examples/user_code/) and quickly compile it with CMake when compiling the whole OpenPose project. Quickly **add your custom code**: See [examples/user_code/README.md](examples/user_code/README.md) for further details.  - **OpenPose Python API**: Analogously to the C++ API  find the tutorial for the Python API on [examples/tutorial_api_python/](examples/tutorial_api_python/).  - **Adding an extra module**: Check [doc/library_add_new_module.md](./doc/library_add_new_module.md).  - **Standalone face or hand detector**:     - **Face** keypoint detection **without body** keypoint detection: If you want to speed it up (but also reduce amount of detected faces)  check the OpenCV-face-detector approach in [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     - **Use your own face/hand detector**: You can use the hand and/or face keypoint detectors with your own face or hand detectors  rather than using the body detector. E.g.  useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     """;General;https://github.com/KarinaKorsgaard/Bartender-AI-OpenPose-Osc-Sender
"""    prefix_allowed_tokens_fn=lambda batch_id  sent: trie.get(sent.tolist())    For a full review of (m)GENRE API see: * [examples for GENRE](https://github.com/facebookresearch/GENRE/blob/main/examples_genre) on how to use GENRE for both pytorch fairseq and huggingface transformers; * [examples for mGENRE](https://github.com/facebookresearch/GENRE/blob/main/examples_mgenre) on how to use mGENRE.   """;Sequential;https://github.com/facebookresearch/GENRE
"""**Windows portable version**: Simply download and use the latest version from the [Releases](https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases) section.  Otherwise  check [doc/installation.md](doc/installation.md) for instructions on how to build OpenPose from source.     OS: Ubuntu (14  16)  Windows (8  10)  Mac OSX  Nvidia TX2.   CUDA (Nvidia GPU)  OpenCL (AMD GPU)  and CPU-only (no GPU) versions.   Most users do not need the OpenPose C++/Python API  but can simply use the OpenPose Demo:  - **OpenPose Demo**: To easily process images/video/webcam and display/save the results. See [doc/demo_overview.md](doc/demo_overview.md). E.g.  run OpenPose in a video with: ``` #: Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi ```  - **Calibration toolbox**: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See [doc/modules/calibration_module.md](doc/modules/calibration_module.md).  - **OpenPose C++ API**: If you want to read a specific input  and/or add your custom post-processing function  and/or implement your own display/saving  check the C++ API tutorial on [examples/tutorial_api_cpp/](examples/tutorial_api_cpp/) and [doc/library_introduction.md](doc/library_introduction.md). You can create your custom code on [examples/user_code/](examples/user_code/) and quickly compile it with CMake when compiling the whole OpenPose project. Quickly **add your custom code**: See [examples/user_code/README.md](examples/user_code/README.md) for further details.  - **OpenPose Python API**: Analogously to the C++ API  find the tutorial for the Python API on [examples/tutorial_api_python/](examples/tutorial_api_python/).  - **Adding an extra module**: Check [doc/library_add_new_module.md](./doc/library_add_new_module.md).  - **Standalone face or hand detector**:     - **Face** keypoint detection **without body** keypoint detection: If you want to speed it up (but also reduce amount of detected faces)  check the OpenCV-face-detector approach in [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     - **Use your own face/hand detector**: You can use the hand and/or face keypoint detectors with your own face or hand detectors  rather than using the body detector. E.g.  useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     """;General;https://github.com/kevin51jiang/htv-iv
"""Base GitHub Repo:https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch   : with a coco-pretrained  you can even freeze the backbone and train heads only   """;Computer Vision;https://github.com/DataXujing/EfficientDet_pytorch
"""1. [Install Webots R2021a](https://www.cyberbotics.com/) 2. Install Python versions 3.8     * Follow the Using Python guide provided by Webots 3. Install deepbots 0.1.3.dev2 through pip running the following command:\ <code>pip install -i https://test.pypi.org/simple/ deepbots</code> 4. Install PyTorch via pip   * Find more examples in [deepworlds](https://github.com/aidudezzz/deepworlds). * [Reach a Target via PPOAgent with Panda](https://github.com/KelvinYang0320/deepbots-panda/tree/Panda-deepbots-0.1.2) """;Reinforcement Learning;https://github.com/KelvinYang0320/deepbots-panda
"""Running YOLO on the CPU is doable but very slow. The GPU-accelerated version of TensorFlow 1.x is much faster. On a portable  scaled down Turing-class GPU (GTX 1660 Ti) with 6 GB RAM we get up to 20 fps from the neural network alone - which is then reduced to half by the code after output parsing  display  etc (surely there's a lot of optimizations yet to be done - e.g. replace loops with vector operations).   Depending on your OS  you need to pick the serial port (COM3  /dev/ttyS0) that the Maestro is using  and inject commands into it via the maestro.py library.   """;Computer Vision;https://github.com/FlorinAndrei/TensorAim
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   For R-FCN/Faster R-CNN\: 1. Please download COCO and VOC 2007+2012 datasets  and make sure it looks like this:  	``` 	./data/coco/ 	./data/VOCdevkit/VOC2007/ 	./data/VOCdevkit/VOC2012/ 	```  2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  For DeepLab\: 1. Please download Cityscapes and VOC 2012 datasets and make sure it looks like this:  	``` 	./data/cityscapes/ 	./data/VOCdevkit/VOC2012/ 	``` 2. Please download argumented VOC 2012 annotations/image lists  and put the argumented annotations and the argumented train/val lists into:  	``` 	./data/VOCdevkit/VOC2012/SegmentationClass/ 	./data/VOCdevkit/VOC2012/ImageSets/Main/ 	```      Respectively.     2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  1. Clone the Deformable ConvNets repository  and we'll call the directory that you cloned Deformable-ConvNets as ${DCN_ROOT}. ``` git clone https://github.com/msracver/Deformable-ConvNets.git ```  2. For Windows users  run ``cmd .\init.bat``. For Linux user  run `sh ./init.sh`. The scripts will build cython module automatically and create some folders.  3. Install MXNet: 	 	**Note: The MXNet's Custom Op cannot execute parallelly using multi-gpus after this [PR](https://github.com/apache/incubator-mxnet/pull/6928). We strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.5).**  	***Quick start***  	3.1 Install MXNet and all dependencies by  	``` 	pip install -r requirements.txt 	``` 	If there is no other error message  MXNet should be installed successfully.  	 	***Build from source (alternative way)***  	3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by 	``` 	git clone --recursive https://github.com/dmlc/mxnet.git 	git checkout 998378a 	git submodule update 	#: if it's the first time to checkout  just use: git submodule update --init --recursive 	``` 	3.3 Compile MXNet 	``` 	cd ${MXNET_ROOT} 	make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1 	``` 	3.4 Install the MXNet Python binding by 	 	***Note: If you will actively switch between different versions of MXNet  please follow 3.5 instead of 3.4*** 	``` 	cd python 	sudo python setup.py install 	``` 	3.5 For advanced users  you may put your Python packge into `./external/mxnet/$(YOUR_MXNET_PACKAGE)`  and modify `MXNET_VERSION` in `./experiments/rfcn/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`. Thus you can switch among different versions of MXNet quickly.  4. For Deeplab  we use the argumented VOC 2012 dataset. The argumented annotations are provided by [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html) dataset. For convenience  we provide the converted PNG annotations and the lists of train/val images  please download them from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMRhVImMI1jRrsxDg).   We provide trained deformable convnet models  including the deformable R-FCN & Faster R-CNN models trained on COCO trainval  and the deformable DeepLab model trained on CityScapes train.  1. To use the demo with our pre-trained deformable models  please download manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMSjehIcCgAhvEAHw) or [BaiduYun](https://pan.baidu.com/s/1dFlPFED)  and put it under folder `model/`.  	Make sure it looks like this: 	``` 	./model/rfcn_dcn_coco-0000.params 	./model/rfcn_coco-0000.params 	./model/fpn_dcn_coco-0000.params 	./model/fpn_coco-0000.params 	./model/rcnn_dcn_coco-0000.params 	./model/rcnn_coco-0000.params 	./model/deeplab_dcn_cityscapes-0000.params 	./model/deeplab_cityscapes-0000.params 	./model/deform_conv-0000.params 	./model/deform_psroi-0000.params 	``` 2. To run the R-FCN demo  run 	``` 	python ./rfcn/demo.py 	``` 	By default it will run Deformable R-FCN and gives several prediction results  to run R-FCN  use 	``` 	python ./rfcn/demo.py --rfcn_only 	``` 3. To run the DeepLab demo  run 	``` 	python ./deeplab/demo.py 	``` 	By default it will run Deformable Deeplab and gives several prediction results  to run DeepLab  use 	``` 	python ./deeplab/demo.py --deeplab_only 	``` 4. To visualize the offset of deformable convolution and deformable psroipooling  run 	``` 	python ./rfcn/deform_conv_demo.py 	python ./rfcn/deform_psroi_demo.py 	```    1. All of our experiment settings (GPU #  dataset  etc.) are kept in yaml config files at folder `./experiments/rfcn/cfgs`  `./experiments/faster_rcnn/cfgs` and `./experiments/deeplab/cfgs/`. 2. Eight config files have been provided so far  namely  R-FCN for COCO/VOC  Deformable R-FCN for COCO/VOC  Faster R-CNN(2fc) for COCO/VOC  Deformable Faster R-CNN(2fc) for COCO/VOC  Deeplab for Cityscapes/VOC and Deformable Deeplab for Cityscapes/VOC  respectively. We use 8 and 4 GPUs to train models on COCO and on VOC for R-FCN  respectively. For deeplab  we use 4 GPUs for all experiments.  3. To perform experiments  run the python scripts with the corresponding config file as input. For example  to train and test deformable convnets on COCO with ResNet-v1-101  use the following command     ```     python experiments\rfcn\rfcn_end2end_train_test.py --cfg experiments\rfcn\cfgs\resnet_v1_101_coco_trainval_rfcn_dcn_end2end_ohem.yaml     ```     A cache folder would be created automatically to save the model and the log under `output/rfcn_dcn_coco/`. 4. Please find more details in config files and in our code.   """;Computer Vision;https://github.com/zengzhaoyang/Weak_Detection
"""make a local clone  make changes on the local copy   push to your GitHub account: git push origin   """;General;https://github.com/samuelmat19/GLOW-tf2
"""For example  one can increase the speaking rate by 20 % and decrease the pitch by 20 % by   Alternately  you can align the corpus by yourself.    """;Audio;https://github.com/keonlee9420/FastPitchFormant
"""In our experiment environment (cudnn v5.1  CUDA 7.5  one TITAN X GPU)  the code runs with speed 5iters/s when batch size is set to be 64. The hyperparameters are identical to the original [torch implementation] (https://github.com/liuzhuang13/DenseNet).   """;Computer Vision;https://github.com/YixuanLi/densenet-tensorflow
"""**Windows portable version**: Simply download and use the latest version from the [Releases](https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases) section.  Otherwise  check [doc/installation.md](doc/installation.md) for instructions on how to build OpenPose from source.     OS: Ubuntu (14  16)  Windows (8  10)  Mac OSX  Nvidia TX2.   CUDA (Nvidia GPU)  OpenCL (AMD GPU)  and CPU-only (no GPU) versions.   Most users do not need the OpenPose C++/Python API  but can simply use the OpenPose Demo:  - **OpenPose Demo**: To easily process images/video/webcam and display/save the results. See [doc/demo_overview.md](doc/demo_overview.md). E.g.  run OpenPose in a video with: ``` #: Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi ```  - **Calibration toolbox**: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See [doc/modules/calibration_module.md](doc/modules/calibration_module.md).  - **OpenPose C++ API**: If you want to read a specific input  and/or add your custom post-processing function  and/or implement your own display/saving  check the C++ API tutorial on [examples/tutorial_api_cpp/](examples/tutorial_api_cpp/) and [doc/library_introduction.md](doc/library_introduction.md). You can create your custom code on [examples/user_code/](examples/user_code/) and quickly compile it with CMake when compiling the whole OpenPose project. Quickly **add your custom code**: See [examples/user_code/README.md](examples/user_code/README.md) for further details.  - **OpenPose Python API**: Analogously to the C++ API  find the tutorial for the Python API on [examples/tutorial_api_python/](examples/tutorial_api_python/).  - **Adding an extra module**: Check [doc/library_add_new_module.md](./doc/library_add_new_module.md).  - **Standalone face or hand detector**:     - **Face** keypoint detection **without body** keypoint detection: If you want to speed it up (but also reduce amount of detected faces)  check the OpenCV-face-detector approach in [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     - **Use your own face/hand detector**: You can use the hand and/or face keypoint detectors with your own face or hand detectors  rather than using the body detector. E.g.  useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     """;General;https://github.com/mohammadreyaz/openposered
"""  ```   pip install ulangel   ```   * Dataloader: Here we use the pytorch dataloader  to get dataset item in the way defined by the sampler.     language_model.modules     Github: https://github.com/salesforce/awd-lstm-lm   """;Natural Language Processing;https://github.com/uchange/ulangel
"""Make sure that you have **Python 3.6+** and **pip** installed. We recommend installing the stable version of `rlcard` with `pip`:  ``` pip3 install rlcard ``` The default installation will only include the card environments. To use PyTorch implementation of the training algorithms  run ``` pip3 install rlcard[torch] ``` If you are in China and the above command is too slow  you can use the mirror provided by Tsinghua University: ``` pip3 install rlcard -i https://pypi.tuna.tsinghua.edu.cn/simple ``` Alternatively  you can clone the latest version with (if you are in China and Github is slow  you can use the mirror in [Gitee](https://gitee.com/daochenzha/rlcard)): ``` git clone https://github.com/datamllab/rlcard.git ``` or only clone one branch to make it faster: ``` git clone -b master --single-branch --depth=1 https://github.com/datamllab/rlcard.git ``` Then install with ``` cd rlcard pip3 install -e . pip3 install -e .[torch] ```  We also provide [**conda** installation method](https://anaconda.org/toubun/rlcard):  ``` conda install -c toubun rlcard ```  Conda installation only provides the card environments  you need to manually install Pytorch on your demands.   You can use the the following interface to make an environment. You may optionally specify some configurations with a dictionary.  *   env = rlcard.make(env_id  config={}): Make an environment. env_id is a string of a environment; config is a dictionary that specifies some environment configurations  which are as follows.   A **short example** is as below.  ```python import rlcard from rlcard.agents import RandomAgent  env = rlcard.make('blackjack') env.set_agents([RandomAgent(num_actions=env.num_actions)])  print(env.num_actions) #: 2 print(env.num_players) #: 1 print(env.state_shape) #: [[2]] print(env.action_shape) #: [None]  trajectories  payoffs = env.run() ```  RLCard can be flexibly connected to various algorithms. See the following examples:  *   [Playing with random agents](docs/toy-examples.md#playing-with-random-agents) *   [Deep-Q learning on Blackjack](docs/toy-examples.md#deep-q-learning-on-blackjack) *   [Training CFR (chance sampling) on Leduc Hold'em](docs/toy-examples.md#training-cfr-on-leduc-holdem) *   [Having fun with pretrained Leduc model](docs/toy-examples.md#having-fun-with-pretrained-leduc-model) *   [Training DMC on Dou Dizhu](docs/toy-examples.md#training-dmc-on-dou-dizhu) *   [Evaluating Agents](docs/toy-examples.md#evaluating-agents)   Run `examples/human/leduc_holdem_human.py` to play with the pre-trained Leduc Hold'em model. Leduc Hold'em is a simplified version of Texas Hold'em. Rules can be found [here](docs/games.md#leduc-holdem).  ``` >> Leduc Hold'em pre-trained model  >> Start a new game! >> Agent 1 chooses raise  =============== Community Card =============== ┌─────────┐ │░░░░░░░░░│ │░░░░░░░░░│ │░░░░░░░░░│ │░░░░░░░░░│ │░░░░░░░░░│ │░░░░░░░░░│ │░░░░░░░░░│ └─────────┘ ===============   Your Hand    =============== ┌─────────┐ │J        │ │         │ │         │ │    ♥    │ │         │ │         │ │        J│ └─────────┘ ===============     Chips      =============== Yours:   + Agent 1: +++ =========== Actions You Can Choose =========== 0: call  1: raise  2: fold  >> You choose action (integer): ``` We also provide a GUI for easy debugging. Please check [here](https://github.com/datamllab/rlcard-showdown/). Some demos:  ![doudizhu-replay](https://github.com/datamllab/rlcard-showdown/blob/master/docs/imgs/doudizhu-replay.png?raw=true) ![leduc-replay](https://github.com/datamllab/rlcard-showdown/blob/master/docs/imgs/leduc-replay.png?raw=true)   """;Reinforcement Learning;https://github.com/datamllab/rlcard
"""We trained on several datasets  including CIFAR10  CelebA64  LSUN Church 64 and CelebA HQ 256.  For large datasets  we store the data in LMDB datasets for I/O efficiency. Check [here](https://github.com/NVlabs/NVAE#set-up-file-paths-and-data) for information regarding dataset preparation.   We use the following commands on each dataset for training VAEBM. Note that you need to train the NVAE on corresponding dataset before running the training command here.   We train VAEBM on CIFAR-10 using one 32-GB V100 GPU.    We train VAEBM on CelebA 64 using one 32-GB V100 GPU.    We train VAEBM on LSUN Church 64 using one 32-GB V100 GPU.    """;Computer Vision;https://github.com/NVlabs/VAEBM
"""Run sample.py on your model  with the --sample_npz argument  then run inception_tf13 to calculate the actual TensorFlow IS. Note that you will need to have TensorFlow 1.3 or earlier installed  as TF1.4+ breaks the original IS code.     We have not tried the apex SyncBN as my school's servers are on ancient NVIDIA drivers that don't support it--apex would probably be a good place to start.    You will need:  - [PyTorch](https://PyTorch.org/)  version 1.0.1 - tqdm  numpy  scipy  and h5py - The ImageNet training set  First  you may optionally prepare a pre-processed HDF5 version of your target dataset for faster I/O. Following this (or not)  you'll need the Inception moments needed to calculate FID. These can both be done by modifying and running  ```sh sh scripts/utils/prepare_data.sh ```  Which by default assumes your ImageNet training set is downloaded into the root folder `data` in this directory  and will prepare the cached HDF5 at 128x128 pixel resolution.  In the scripts folder  there are multiple bash scripts which will train BigGANs with different batch sizes. This code assumes you do not have access to a full TPU pod  and accordingly spoofs mega-batches by using gradient accumulation (averaging grads over multiple minibatches  and only taking an optimizer step after N accumulations). By default  the `launch_BigGAN_bs256x8.sh` script trains a full-sized BigGAN model with a batch size of 256 and 8 gradient accumulations  for a total batch size of 2048. On 8xV100 with full-precision training (no Tensor cores)  this script takes 15 days to train to 150k iterations.  You will first need to figure out the maximum batch size your setup can support. The pre-trained models provided here were trained on 8xV100 (16GB VRAM each) which can support slightly more than the BS256 used by default. Once you've determined this  you should modify the script so that the batch size times the number of gradient accumulations is equal to your desired total batch size (BigGAN defaults to 2048).  Note also that this script uses the `--load_in_mem` arg  which loads the entire (~64GB) I128.hdf5 file into RAM for faster data loading. If you don't have enough RAM to support this (probably 96GB+)  remove this argument.    """;General;https://github.com/ajbrock/BigGAN-PyTorch
"""Create a conda environment with pytorch cython and scikit-learn : ``` conda create --name kbc_env python=3.7 source activate kbc_env conda install --file requirements.txt -c pytorch ```  Then install the kbc package to this environment ``` python setup.py install ```   mkdir src_data  cd src_data   python -m kbc.process_datasets   CKG: https://tinyurl.com/yxps3ca2  CKGE: https://tinyurl.com/y42ktou3  CKGE-KW: https://tinyurl.com/y4xshws3   """;Graphs;https://github.com/twktheainur/kbc
"""Run the following to install:  ```sh pip install fast-transformer ```   To install fast-transformer  along with tools you need to develop and test  run the following in your virtualenv:  git clone https://github.com/Rishit-dagli/Fast-Transformer.git  : or clone your own fork  cd fast-transformer  pip install -e .[dev]   ```python import tensorflow as tf from fast_transformer import FastTransformer  mask = tf.ones([1  4096]  dtype=tf.bool) model = FastTransformer(     num_tokens = 20000      dim = 512      depth = 2      max_seq_len = 4096      absolute_pos_emb = True  #: Absolute positional embeddings     mask = mask ) x = tf.experimental.numpy.random.randint(0  20000  (1  4096))  logits = model(x) #: (1  4096  20000) ```   """;Natural Language Processing;https://github.com/Rishit-dagli/Fast-Transformer
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.  git clone https://github.com/affinelayer/pix2pix-tensorflow.git  cd pix2pix-tensorflow   ```sh #: clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git cd pix2pix-tensorflow #: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir facades_train \   --max_epochs 200 \   --input_dir facades/train \   --which_direction BtoA #: test the model python pix2pix.py \   --mode test \   --output_dir facades_test \   --input_dir facades/val \   --checkpoint facades_train ```  The test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.  If you have Docker installed  you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:  ```sh #: train the model python tools/dockrun.py python pix2pix.py \       --mode train \       --output_dir facades_train \       --max_epochs 200 \       --input_dir facades/train \       --which_direction BtoA #: test the model python tools/dockrun.py python pix2pix.py \       --mode test \       --output_dir facades_test \       --input_dir facades/val \       --checkpoint facades_train ```   <img src=""docs/combine.png"" width=""900px""/>  ```sh #: Resize source images python tools/process.py \   --input_dir photos/original \   --operation resize \   --output_dir photos/resized #: Create images with blank centers python tools/process.py \   --input_dir photos/resized \   --operation blank \   --output_dir photos/blank #: Combine resized images with blanked images python tools/process.py \   --input_dir photos/resized \   --b_dir photos/blank \   --operation combine \   --output_dir photos/combined #: Split into train/val set python tools/split.py \   --dir photos/combined ```  The folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.   """;General;https://github.com/JobQiu/hackrice
"""- Install the required packages     - `pip install -r requirements.txt`  - Download data     - https://drive.google.com/file/d/1Na7e2yJy1Oix8-HcKQS97u1VZodpZ-OZ/view?usp=sharing     - Train and Test on electricity dataset     - `python ./main.py --exp_name electricity --conf_file_path ./conf/electricity.yaml`          Plot prediction on Test set     - `python ./main.py --exp_name electricity --conf_file_path ./conf/electricity.yaml --inference=True`             https://pytorch.org/tutorials/beginner/transformer_tutorial.html   """;General;https://github.com/stevinc/Transformer_Timeseries
"""- EfficientNet B0: A convolutional neural network finetuned from the EfficientNet B0 architecture (model📜 ) | You will need to install efficientnet for keras through this repo   The network can correctly classify some indicative examples from real world events such as - The Tesla Model X crash into a roadside barrier (2018) -> *traffic_incident* with probability 0.78 <img src=""./Figure/Tesla.jpg"" width=""512"">  - The Hawaii Volcano Eruption (2018) -> *fire* with probability 0.9116652 <img src=""./Figure/Hawaii Vulcano.jpg"" width=""512"">  The network can also be ported on andoid and itegraded with UAV applications to process the video feed locally.  <img src=""./Figure/Android_App.jpg"" height=""512"">   """;General;https://github.com/ckyrkou/EmergencyNet
"""In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   To build the tf-record and pre-train the model  download the OpenWebText corpus (12G) and setup your data directory in build_data.sh and pretrain.sh. Then run  bash build_data.sh   bash pretrain.sh   bash finetune.sh   """;Sequential;https://github.com/yyht/Conv_Bert
"""  python     #: Then update estimator    cd examples/correlated_gaussians/   We provide code to showcase the two functionalities we just talked about   A naive differentiation of the previously defined loss_regularized term will probably fail to yield the expected result. A very probable reason for that is that I(X;Z) will most likely have gradients whose scale are much larger than those from the loss_gan. In order to maintain a certain balance in gradients' scale  on must use adaptive gradient clipping proposed in [1]. This simple trick consists in scaling the MI term such that its gradient norm doesn't exceed the one from the original loss. Concretely  one must instead minimize:  ``` scale = min( ||G(loss)||  ||G(I(X;Z))|| ) / ||G(I(X;Z))|| loss_regularized = loss_gan - beta * scale * I(X;Z) ``` where G(.) defines the gradient operator with respect to specified parameters.  We provide a simple example in 2D referred to as ""25 gaussians experiments"" where the target distribution is:  <img src=""https://github.com/mboudiaf/Mutual-Information-Variational-Bounds/blob/master/screens/gan_target.png"" width=""250"">  The simple GAN will produce  with the provided generator and discriminator architecture distributions like:  <img src=""https://github.com/mboudiaf/Mutual-Information-Variational-Bounds/blob/master/screens/gan _noreg.png"" width=""250"">  While the above plot clearly exposes a mode collapse  one can easily reduce this mode collapse by adding a MI regularization:  <img src=""https://github.com/mboudiaf/Mutual-Information-Variational-Bounds/blob/master/screens/gan_mine.png"" width=""250"">  To see the code for this example  first go the example directory ``` cd examples/gan/ ``` Then  run some tests to make sure the code doesn't yield any bug: ``` python3 run_test.py ``` Finally  to run the experiments  you can check all the available options in ""demo_gaussian.py""  and loop over any parameters by modifying the header of run_exp.py. Then simply run: ``` python3 run_exp.py ```    """;General;https://github.com/mboudiaf/Mutual-Information-Variational-Bounds
"""`pip install -r requitements.txt`       python main.py train path/to/coco path/to/wikiart [OPTIONS]   - COCO: https://cocodataset.org/#download - WikiArt: https://www.kaggle.com/c/painter-by-numbers/data   """;Computer Vision;https://github.com/aadhithya/AdaIN-pytorch
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/HongSic/DarknetAI
"""``` pip install pytriplet ```  or if you want to get the latest updates:  ```shell git clone https://github.com/wangcongcong123/ttt.git cd ttt pip install -e . ```  * make sure `transformers>=3.1.0`. If not  install via `pip install transformers -U`   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wangcongcong123/ttt/blob/master/ttt_notebook.ipynb)   The following demonstrates the example of fine-tuning T5-small for sst2 ([example_t5.py](example_t5.py)).  ![](ttt_demo.png) <!--  * This can be scaled to t5-large  t5-large  or even 3B if a 8-cores TPU is available. For GPUs  this fine-tuning is tested on T5-large in a server of 4 12-GB GTX-1080s even with per_device_train_batch_size=2  leading to a out-of-memory exception (OOM). -->  ```python from ttt import iid_denoise_text text=""ttt is short for a package for fine-tuning 🤗 Transformers with TPUs  written in Tensorflow2.0"" #: here the text is split by space to tokens  you can use huggingface's T5Tokenizer to tokenize as well. original  source  target=iid_denoise_text(text.split()  span_length=3  corrupt_ratio=0.25)  #: original: ['ttt'  'is'  'short'  'for'  'a'  'package'  'for'  'fine-tuning'  '🤗'  'Transformers'  'with'  'TPUs '  'written'  'in'  'Tensorflow2.0'] #: source: ['ttt'  '<extra_id_0>'  'a'  'package'  'for'  'fine-tuning'  '🤗'  'Transformers'  'with'  '<extra_id_1>'  '<extra_id_2>'] #: target: ['<extra_id_0>'  'is'  'short'  'for'  '<extra_id_1>'  'TPUs '  'written'  'in'  'Tensorflow2.0'] ```   <a name=""wmt_en_ro_t5""></a> **Fine-tuning**: No boilerplate codes changed (the same as [example_t5](example_t5.py)) except for the following args: ```python3 #: any one from MODELS_SUPPORT (check:ttt/args.py) args.model_select = ""t5-small"" #: the path to the translation dataset  each line represents an example in jsonl format like: {""target"": ""...""  ""source"" ""...""} #: it will download automatically for the frist time from: https://s3.amazonaws.com/datasets.huggingface.co/translation/wmt_en_ro.tar.gz args.data_path = ""data/wmt_en_ro"" #: any one from TASKS_SUPPORT (check:ttt/args.py) args.task = ""translation"" args.max_src_length=128 args.max_tgt_length=128 args.source_field_name=""source"" args.target_field_name=""target"" args.eval_on=""bleu"" #:this refers to sacrebleu as used in T5 paper ```  ** On a TPUv3-8  the bleu score achieved by t5-base is 27.9 (very close to 28 as reported in [the T5 paper](https://arxiv.org/abs/1910.10683))  the fine-tuning args are [here](https://ucdcs-student.ucd.ie/~cwang/ttt/models/en2ro_t5_base/args.json) and training log is [here](https://ucdcs-student.ucd.ie/~cwang/ttt/models/en2ro_t5_base/train.log).    ```python3 from ttt import *  if __name__ == '__main__':     args = get_args()     #: check what args are available     logger.info(f""args: {json.dumps(args.__dict__  indent=2)}"")     #:#:#:#:#:#:#:#:#:#:#:#:#:#:#: customize args     #: args.use_gpu = True     args.use_tpu = True     args.do_train = True     args.use_tb = True     #: any one from MODELS_SUPPORT (check:ttt/args.py)     args.model_select = ""bert-base-uncased""     #: select a dataset following jsonl format  where text filed name is ""text"" and label field name is ""label""     args.data_path = ""data/glue/sst2""     #: any one from TASKS_SUPPORT (check:ttt/args.py)     args.task = ""single-label-cls""     args.log_steps = 400     #: any one from LR_SCHEDULER_SUPPORT (check:ttt/args.py)     args.scheduler=""warmuplinear""     #: set do_eval = False if your data does not contain a validation set. In that case  patience  and early_stop will be invalid     args.do_eval = True     args.tpu_address = ""x.x.x.x"" #: replace with yours     #:#:#:#:#:#:#:#:#:#:#:#:#:#:#: end customize args     #: to have a sanity check for the args     sanity_check(args)     #: seed everything  make deterministic     set_seed(args.seed)     tokenizer = get_tokenizer(args)     inputs = get_inputs(tokenizer  args)     model  _ = create_model(args  logger  get_model)     #: start training  here we keras high-level API     training_history = model.fit(         inputs[""x_train""]          inputs[""y_train""]          epochs=args.num_epochs_train          verbose=2          batch_size=args.per_device_train_batch_size*args.num_replicas_in_sync          callbacks=get_callbacks(args  inputs  logger  get_evaluator)      ) ```  So far the package has included the following supports for `args.model_select`  `args.task` and `args.scheduler` ([args.py](ttt/args.py)).  ```python3 #: these have been tested and work fine. more can be added to this list to test MODELS_SUPPORT = [""distilbert-base-cased"" ""bert-base-uncased""  ""bert-large-uncased""  ""google/electra-base-discriminator""                    ""google/electra-large-discriminator""  ""albert-base-v2""  ""roberta-base""                    ""t5-small"" ""t5-base""] #: if using t5 models  the tasks has to be t2t* ones TASKS_SUPPORT = [""single-label-cls""  ""t2t""] #: in the future  more schedulers will be added  such as warmupconstant  warmupcosine  etc. LR_SCHEDULER_SUPPORT = [""warmuplinear""  ""warmupconstant""  ""constant""] ```   """;Natural Language Processing;https://github.com/wangcongcong123/ttt
""" You can download the dataset via the link below.<br><br> <a href=""https://github.com/OlafenwaMoses/Traffic-Net/releases/tag/1.0"" >https://github.com/OlafenwaMoses/Traffic-Net/releases/tag/1.0</a>  <br><br>     <b><a href=""https://github.com/OlafenwaMoses/Traffic-Net/releases/download/1.0/trafficnet_resnet_model_ex-055_acc-0.913750.h5"" >https://github.com/OlafenwaMoses/Traffic-Net/releases/download/1.0/trafficnet_resnet_model_ex-055_acc-0.913750.h5</a></b><br>   Running the experiment or prediction requires that you have Tensorflow  and Keras  OpenCV and ImageAI installed. You can install this dependencies via the commands below.  <br><span><b>- Tensorflow 1.4.0 (and later versions)  </b>      <a href=""https://www.tensorflow.org/install/install_windows"" style=""text-decoration: none;"" > Install</a></span> or install via pip <pre> pip3 install --upgrade tensorflow </pre>   <span><b>- OpenCV  </b>        <a href=""https://pypi.python.org/pypi/opencv-python"" style=""text-decoration: none;"" >Install</a></span> or install via pip <pre> pip3 install opencv-python </pre>   <span><b>- Keras 2.x  </b>     <a href=""https://keras.io/#installation"" style=""text-decoration: none;"" >Install</a></span> or install via pip <pre> pip3 install keras </pre>       <span>      <pre>pip3 install imageai </pre></span> <br><br> <br>   """;Computer Vision;https://github.com/kishorkuttan/traffic_net
"""Manufacturing is becoming automated on a broad scale. The technology enables manufacturers to affordably boost their throughput  improve quality and become nimbler as they respond to customer demands. Automation is a revolution in manufacturing quality control. It allows the companies to set certain bars or criteria for the products being manufactured. Then it also aids in real-time tracking of the manufacturing process through machine vision cameras and/or recordings.   The core deliverable for this project is building deep learning image classification models which can automate the process of inspection for casting defects. I have produced a rest endpoint which can accept a cast image and subsequently run a tuned model to classify if the cast is acceptable or not.    As part of this project  I have built the computer vision models in 3 different ways addressing different personas  because not all companies will have a resolute data science team.   1.	Using Keras Tensorflow model (convolution2d) what a trained team of data scientists would do. 2.	Using Azure Machine Learning Designer (designer) which enables AI engineers and Data scientists use a drag and drop prebuilt model DenseNet (densenet).  3.	Using Azure custom vision (custom-vision) which democratizes the process of building the computer vision model with little to no training.   The work that will be subsequently done as part of this paper will have at the very least embody the following principles (ai/responsible-ai  n.d.):   •	Fair - AI must maximize efficiencies without destroying dignity and guard against bias.   •	Accountable - AI must have algorithmic accountability.   •	Transparent - AI systems must be transparent and understandable.   •	Ethical - AI must assist humanity and be designed for intelligent privacy.     """;Computer Vision;https://github.com/RajdeepBiswas/Manufacturing-Quality-Inspection
"""To install  simply run:  pip3 install qrnn   """;Sequential;https://github.com/JonathanRaiman/tensorflow_qrnn
"""chmod +x ./example/train_harmorization_s2ad.sh && ./example/train_harmorization_s2ad.sh   chmod +x ./example/train_harmorization_s2asc.sh && ./example/train_harmorization_s2asc.sh   chmod +x ./example/train_harmorization_wo_mask.sh && ./example/train_harmorization_wo_mask.sh   clone this repo.   run the notebook   Just visit our [google colab notebook](https://colab.research.google.com/drive/1UTjyi0J1F2mjc9rf9ZbFUOL2_kkZmdlQ?usp=sharing).    """;General;https://github.com/vinthony/s2am
"""git clone git@github.com:MustafaMustafa/cosmoGAN.git  cd cosmoGAN/networks   cd ../   cd cosmoGAN/networks   """;Computer Vision;https://github.com/MustafaMustafa/cosmoGAN
"""```bash git clone git@github.com:philipperemy/keras-tcn.git cd keras-tcn virtualenv -p python3.6 venv source venv/bin/activate pip install -r requirements.txt #: change to tensorflow if you dont have a gpu. pip install . --upgrade #: install it as a package. ```  Note: Only compatible with Python 3 at the moment. Should be almost compatible with python 2.   pip install keras-tcn   Installation (Python 3)   pip install tox   """;Audio;https://github.com/anandharaju/Basic_TCN
"""3. Setup the environment  3.1. PyTorch environment  3.2. TensorFlow environment   4.1. Check the environment   Install conda   cd $REPO_DIR  conda create -n rtdl python=3.8.8  conda activate rtdl  conda install pytorch==1.7.1 torchvision==0.8.2 cudatoolkit=10.1.243 numpy=1.19.2 -c pytorch -y  conda install cudnn=7.6.5 -c anaconda -y  pip install -r requirements.txt  conda install -c conda-forge nodejs -y  jupyter labextension install @jupyter-widgets/jupyterlab-manager  : if the following commands do not succeed  update conda  conda env config vars set PYTHONPATH=${PYTHONPATH}:${REPO_DIR}  conda env config vars set PROJECT_DIR=${REPO_DIR}   conda env config vars set CUDA_HOME=${CONDA_PREFIX}  conda env config vars set CUDA_ROOT=${CONDA_PREFIX}  conda deactivate  conda activate rtdl   This environment is needed only for experimenting with TabNet. For all other cases use the PyTorch environment.  The instructions are the same as for the PyTorch environment (including installation of PyTorch!)  but:   - right before pip install -r requirements.txt do the following:    - pip install tensorflow-gpu==1.14   1. Download the data: wget https://www.dropbox.com/s/o53umyg6mn3zhxy/rtdl_data.tar.gz?dl=1 -O rtdl_data.tar.gz   4. Unpack the archive: tar -xvf rtdl_data.tar.gz  Before we start  let's check that the environment is configured successfully. The following  commands should train one MLP on the California Housing dataset:  mkdir draft   : you can choose any other name instead of ""reproduced.toml""; it is better to keep this   python -c ""   p = Path('output/california_housing/mlp/tuning/reproduced.toml')   mkdir -p output/california_housing/mlp/tuned_reproduced   python -c ""   *This section only provides specific commands with few comments. After completing the tutorial  we recommend checking the next section for better understanding of how to work with the repository. It will also help to better understand the tutorial.*  In this tutorial  we will reproduce the results for MLP on the California Housing dataset. We will cover: - tuning - evaluation - ensembling - comparing models with each other  Note that the chances to get **exactly** the same results are rather low  however  they should not differ much from ours. Before running anything  go to the root of the repository and explicitly set `CUDA_VISIBLE_DEVICES` (if you plan to use GPU): ```bash cd $PROJECT_DIR export CUDA_VISIBLE_DEVICES=0 ```   """;General;https://github.com/yandex-research/rtdl
"""This version supports cudnn v2 acceleration. @TimoSaemann has a branch supporting a more recent version of Caffe (Dec 2016) with cudnn v5.1:   In solver.prototxt set a path for snapshot_prefix. Then in a terminal run   If you would just like to try out a pretrained example model  then you can find the model used in the [SegNet webdemo](http://mi.eng.cam.ac.uk/projects/segnet/) and a script to run a live webcam demo here: https://github.com/alexgkendall/SegNet-Tutorial  For a more detailed introduction to this software please see the tutorial here: http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html   """;Computer Vision;https://github.com/alexgkendall/caffe-segnet
"""- Clone this repo: ``` git clone https://github.com/ruiliu-ai/DivCo.git ```  cd DivCo/DivCo-DCGAN   cd DivCo/DivCo-BicycleGAN   You can download the datasets from the DRIT [Github Project]. <br>   cd DivCo/DivCo-DRIT   Download datasets for each task into the dataset folder ``` mkdir datasets ```  **DivCo-DCGAN** <br> ``` python test.py --dataroot ./datasets/Cifar10 --resume ./models/DivCo-DCGAN/00199.pth ``` **DivCo-BicycleGAN** <br> ``` python test.py --dataroot ./datasets/facades --checkpoints_dir ./models/DivCo-BicycleGAN/facades --epoch 400 ``` ``` python test.py --dataroot ./datasets/maps --checkpoints_dir ./models/DivCo-BicycleGAN/maps --epoch 400 ``` **DivCo-DRIT** <br> ``` python test.py --dataroot ./datasets/yosemite --resume ./models/DivCo-DRIT/yosemite/01199.pth --concat 1 ``` ``` python test.py --dataroot ./datasets/cat2dog --resume ./models/DivCo-DRIT/cat2dog/01199.pth --concat 0 ```   """;Computer Vision;https://github.com/ruiliu-ai/DivCo
"""Simply run counter.py or tracker-counter.py.  counter.py is less volatile than tracker-counter.py  but the latter is able to track the location of your hand.    """;Computer Vision;https://github.com/spencerkraisler/Finger-Counter
"""Accurately ranking the vast number of candidate detections is crucial for dense object detectors to achieve high performance. In this work  we propose to learn IoU-aware classification scores (**IACS**) that simultaneously represent the object presence confidence and localization accuracy  to produce a more accurate ranking of detections in dense object detectors. In particular  we design a new loss function  named **Varifocal Loss (VFL)**  for training a dense object detector to predict the IACS  and a new efficient star-shaped bounding box feature representation (the features at nine yellow sampling points) for estimating the IACS and refining coarse bounding boxes. Combining these two new components and a bounding box refinement branch  we build a new IoU-aware dense object detector based on the FCOS+ATSS architecture  what we call **VarifocalNet** or **VFNet** for short. Extensive experiments on MS COCO benchmark show that our VFNet consistently surpasses the strong baseline by ~2.0 AP with different backbones. Our best model VFNet-X-1200 with Res2Net-101-DCN reaches a single-model single-scale AP of **55.1** on COCO `test-dev`  achieving the state-of-the-art performance among various object detectors.  <div align=""center"">   <img src=""VFNet.png"" width=""600px"" />   <p>Learning to Predict the IoU-aware Classification Score.</p> </div>   - This VarifocalNet implementation is based on [MMDetection](https://github.com/open-mmlab/mmdetection). Therefore the installation is the same as original MMDetection.  - Please check [get_started.md](docs/get_started.md) for installation. Note that you should change the version of PyTorch and CUDA to yours when installing **mmcv** in `step 3` and clone this repo instead of MMdetection in `step 4`.  - If you run into problems with `pycocotools`  please install it by:    ```   pip install ""git+https://github.com/open-mmlab/cocoapi.git#:subdirectory=pycocotools""   ```   2020.10.29 This repo has been refactored so that users can pull the latest updates from the upstream official MMDetection repo. The previous one can be found in the old branch.   Assuming you have put the COCO dataset into data/coco/ and have downloaded the models into the checkpoints/  you can now evaluate the models on the COCO val2017 split:   The following command line will train vfnet_r50_fpn_1x_coco on 8 GPUs:   Once the installation is done  you can follow the steps below to run a quick demo.  - Download the [model](https://drive.google.com/file/d/1aF3Fi5rYeMqSC3Ndo4VEqjPXk4fcOfjt/view?usp=sharing) and put it into one folder under the root directory of this project  say  `checkpoints/`. - Go to the root directory of this project in terminal and activate the corresponding virtual environment. - Run    ```   python demo/image_demo.py demo/demo.jpg configs/vfnet/vfnet_r50_fpn_1x_coco.py checkpoints/vfnet_r50_1x_41.6.pth   ```    and you should see an image with detections.   Please see [exist_data_model.md](docs/1_exist_data_model.md) for the basic usage of MMDetection. They also provide [colab tutorial](demo/MMDet_Tutorial.ipynb) for beginners.  For troubleshooting  please refer to [faq.md](docs/faq.md)   """;Computer Vision;https://github.com/hyz-xmaster/VarifocalNet
"""```bash cd ${DynamicRCNN_ROOT} mkdir data mkdir output ```  Prepare data and pretrained models: - [COCO dataset](http://cocodataset.org/#download) - [ImageNet Pretrained Models from Detectron](https://github.com/facebookresearch/Detectron/blob/master/MODEL_ZOO.md#imagenet-pretrained-models)  Then organize them as follows:  ``` DynamicRCNN ├── dynamic_rcnn ├── models ├── output ├── data │   ├── basemodels/R-50.pkl │   ├── coco │   │   ├── annotations │   │   ├── train2017(2014) │   │   ├── val2017(2014) ```   git clone https://github.com/hkzhang95/DynamicRCNN.git  Please make sure your CUDA is successfully installed and be added to the PATH. I only test CUDA-9.0 for my experiments.  cd ${DynamicRCNN_ROOT}  python setup.py build develop   cd models/zhanghongkai/dynamic_rcnn/coco/dynamic_rcnn_r50_fpn_1x   cd models/zhanghongkai/dynamic_rcnn/coco/dynamic_rcnn_r50_fpn_1x   realpath log | xargs mkdir   """;General;https://github.com/hkzhang95/DynamicRCNN
"""requirements : Keras  tensorflow  numpy  PIL  cv2  <span class=""figcaption_hack"">Source :   """;Computer Vision;https://github.com/CVxTz/face_age_gender
"""Cloning the repo  ```shell $ git clone http://github.com/xuyuwei/resnet-tf $ cd resnet-tf ```  Setting up the virtualenv  installing TensorFlow (OS X) ```shell $ virtualenv venv $ source venv/bin/activate (venv)$ pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl ```  If you don't have virtualenv installed  run `pip install virtualenv`. Also  the cifar-10 data for python can be found at: https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz. Place the data in the main directory.  Start Training: ```shell (venv)$ python main.py  ```  This starts the training for ResNet-20  saving the progress after training every 512 images. To train a net of different depth  comment the line in `main.py` ``` net = models.resnet(X  20) ``` and uncomment the line initializing the appropriate model.   """;Computer Vision;https://github.com/t0930198/backup
"""python3 soft_actor_critic train --env-name MountainCarContinuous-v0 --learning-rate 0.001   - pytorch-soft-actor-critic    """;Reinforcement Learning;https://github.com/thomashirtz/soft-actor-critic
"""Download the 3D KITTI detection dataset from [here](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d).  The downloaded data includes:  - Velodyne point clouds _**(29 GB)**_: input data to the YOLO3D model - Training labels of object data set _**(5 MB)**_: input label to the YOLO3D model - Camera calibration matrices of object data set _**(16 MB)**_: for visualization of predictions - Left color images of object data set _**(12 GB)**_: for visualization of predictions  Please make sure that you construct the source code & dataset directories structure as below.   cd src/data_process   To reproduce the results  you can run the bash shell script   cd logs/&lt;saved_fn&gt;/tensorboard/   │   ├── test.sh   │   └── train.sh   ![demo](./docs/demo.gif)  - **Inputs**: Bird-eye-view _(BEV)_ maps that are encoded by _**height  intensity and density**_ of 3D LiDAR point clouds. - **The input size**: _608 x 608 x 3_ - **Outputs**: **7 degrees of freedom** _(7-DOF)_ of objects: `(cx  cy  cz  l  w  h  θ)`    - `cx  cy  cz`: The center coordinates.    - `l  w  h`: length  width  height of the bounding box.    - `θ`: The heading angle in radians of the bounding box. - **Objects**: Cars  Pedestrians  Cyclists.   |   |Backbone   | Detector   | |---|---|---| |**BoF**   |[x] Dropblock <br> [x] Random rescale  rotation (global) <br> [x] Mosaic/Cutout augmentation|[x] Cross mini-Batch Normalization <br>[x] Dropblock <br> [x] Random training shapes <br>   | |**BoS**   |[x] Mish activation <br> [x] Cross-stage partial connections (CSP) <br> [x] Multi-input weighted residual connections (MiWRC)   |[x] Mish activation <br> [x] SPP-block <br> [x] SAM-block <br> [x] PAN path-aggregation block <br>|    ``` python train.py --help  usage: train.py [-h] [--seed SEED] [--saved_fn FN] [--root-dir PATH]                 [-a ARCH] [--cfgfile PATH] [--pretrained_path PATH]                 [--use_giou_loss] [--img_size IMG_SIZE]                 [--hflip_prob HFLIP_PROB] [--cutout_prob CUTOUT_PROB]                 [--cutout_nholes CUTOUT_NHOLES] [--cutout_ratio CUTOUT_RATIO]                 [--cutout_fill_value CUTOUT_FILL_VALUE]                 [--multiscale_training] [--mosaic] [--random-padding]                 [--no-val] [--num_samples NUM_SAMPLES]                 [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE]                 [--print_freq N] [--tensorboard_freq N] [--checkpoint_freq N]                 [--start_epoch N] [--num_epochs N] [--lr_type LR_TYPE]                 [--lr LR] [--minimum_lr MIN_LR] [--momentum M] [-wd WD]                 [--optimizer_type OPTIMIZER] [--burn_in N]                 [--steps [STEPS [STEPS ...]]] [--world-size N] [--rank N]                 [--dist-url DIST_URL] [--dist-backend DIST_BACKEND]                 [--gpu_idx GPU_IDX] [--no_cuda]                 [--multiprocessing-distributed] [--evaluate]                 [--resume_path PATH] [--conf-thresh CONF_THRESH]                 [--nms-thresh NMS_THRESH] [--iou-thresh IOU_THRESH]  The Implementation of YOLO3D-YOLOv4 using PyTorch  optional arguments:   -h  --help            show this help message and exit   --seed SEED           re-produce the results with seed random   --saved_fn FN         The name using for saving logs  models ...   --root-dir PATH    The ROOT working directory   -a ARCH  --arch ARCH  The name of the model architecture   --cfgfile PATH        The path for cfgfile (only for darknet)   --pretrained_path PATH                         the path of the pretrained checkpoint   --use_giou_loss       If true  use GIoU loss during training. If false  use                         MSE loss for training   --img_size IMG_SIZE   the size of input image   --hflip_prob HFLIP_PROB                         The probability of horizontal flip   --cutout_prob CUTOUT_PROB                         The probability of cutout augmentation   --cutout_nholes CUTOUT_NHOLES                         The number of cutout area   --cutout_ratio CUTOUT_RATIO                         The max ratio of the cutout area   --cutout_fill_value CUTOUT_FILL_VALUE                         The fill value in the cut out area  default 0. (black)   --multiscale_training                         If true  use scaling data for training   --mosaic              If true  compose training samples as mosaics   --random-padding      If true  random padding if using mosaic augmentation   --no-val              If true  dont evaluate the model on the val set   --num_samples NUM_SAMPLES                         Take a subset of the dataset to run and debug   --num_workers NUM_WORKERS                         Number of threads for loading data   --batch_size BATCH_SIZE                         mini-batch size (default: 4)  this is the totalbatch                         size of all GPUs on the current node when usingData                         Parallel or Distributed Data Parallel   --print_freq N        print frequency (default: 50)   --tensorboard_freq N  frequency of saving tensorboard (default: 20)   --checkpoint_freq N   frequency of saving checkpoints (default: 2)   --start_epoch N       the starting epoch   --num_epochs N        number of total epochs to run   --lr_type LR_TYPE     the type of learning rate scheduler (cosin or                         multi_step)   --lr LR               initial learning rate   --minimum_lr MIN_LR   minimum learning rate during training   --momentum M          momentum   -wd WD  --weight_decay WD                         weight decay (default: 1e-6)   --optimizer_type OPTIMIZER                         the type of optimizer  it can be sgd or adam   --burn_in N           number of burn in step   --steps [STEPS [STEPS ...]]                         number of burn in step   --world-size N        number of nodes for distributed training   --rank N              node rank for distributed training   --dist-url DIST_URL   url used to set up distributed training   --dist-backend DIST_BACKEND                         distributed backend   --gpu_idx GPU_IDX     GPU index to use.   --no_cuda             If true  cuda is not used.   --multiprocessing-distributed                         Use multi-processing distributed training to launch N                         processes per node  which has N GPUs. This is the                         fastest way to use PyTorch for either single node or                         multi node data parallel training   --evaluate            only evaluate the model  not training   --resume_path PATH    the path of the resumed checkpoint   --conf-thresh CONF_THRESH                         for evaluation - the threshold for class conf   --nms-thresh NMS_THRESH                         for evaluation - the threshold for nms   --iou-thresh IOU_THRESH                         for evaluation - the threshold for IoU ```  [python-image]: https://img.shields.io/badge/Python-3.6-ff69b4.svg [python-url]: https://www.python.org/ [pytorch-image]: https://img.shields.io/badge/PyTorch-1.5-2BAF2B.svg [pytorch-url]: https://pytorch.org/  """;Computer Vision;https://github.com/maudzung/YOLO3D-YOLOv4-PyTorch
"""``` git clone --depth=1 https://github.com/luke-97/BayesByBackprop.git pip install -r requirements.txt ```   ``` virtualenv --python=python3.6 bbb source bbb/bin/activate ```   I would suggest to use a seperate virtual environment for this purpose so that it doesnot conflict.   """;Computer Vision;https://github.com/chandu-97/BayesByBackprop
""" * 2. only using tensorflow version 1.x .   --traindir : you can set your image path   """;General;https://github.com/behnoudshafizadeh/ESPCN-SuperResolution-Tensorflow
"""```$ pip install self-attention-cv```   It would be nice to pre-install pytorch in your environment  in case you don't have a GPU. To run the tests from the terminal  ```$ pytest``` you may need to run ``` export PYTHONPATH=$PATHONPATH:`pwd` ``` before.    """;Computer Vision;https://github.com/The-AI-Summer/self-attention-cv
"""Use the fastai (https://www.fast.ai/)framework to implement transfer learning for text classification.  The language model was trained on a corpus named Wikitext-103.(https://openreview.net/pdf?id=Byj72udxe)  If you want to know some detailed information  please refer to Jeremy Howard’s paper.(https://arxiv.org/abs/1801.06146v5)  fasiai Github https://github.com/fastai/fastai  （Note: It is highly recommended to use the conda method to install.）  The tutorial below the official github course folder is fastai version is 0.7. I am using the address of the corresponding tutorial for 1.0.0 should be:  https://github.com/fastai/course-v3  video:  https://www.usfca.edu/data-institute/certificates/deep-learning-part-two """;Natural Language Processing;https://github.com/SkullFang/ULMFIT_NLP_Classification
"""For calculating the Q Values we used a Neural Network  rather than just showing the current state to the network  the next 4 possible states(left  right  up  down) were also shown. This intuition was inspired from Monte Carlo Tree Search estimation where game is played till the end to determine the Q-Values. Instead of using a normal Q Network  a Double Q Network was used one for predicting Q values and other for predicting actions. This is done to try and reduce the large overestimations of action values which result form a positive bias introduced in Q Learning. For data preprocessing log2 normalisation  training was done using the Bellman's Equation. The policy used was Epsilon greedy  to allow exploration the value of epsilon was annealed down by 5%.    This project requires: - Python (Python 3.6) - Numpy - TensorFlow - Keras - Gym (OpenAI Gym) - Matplotlib  Once everything is installed open the DQN_Agent_2048.ipynb file on a jupyter notebook and run the cells.   """;Reinforcement Learning;https://github.com/dsgiitr/rl_2048
"""  --use-tfrecord        train using tfrecord dataset     --device {CPU GPU TPU}                           Savedmodel path   You can start training by running script like below ```sh $ python -m scripts.train \ 	--dataset-path ""data/*.txt"" \ 	--batch-size 2048 --dev-batch-size 2048 \ 	--epoch 90 --steps-per-epoch 250 --auto-encoding \ 	--learning-rate 2e-4 \ 	--device gpu \ 	--tensorboard-update-freq 50 --model-name TransformerSeq2Seq --model-config-path resources/configs/transformer.yml ```   You can start training by running script like below ```sh $ python -m scripts.evaluate \     --model-path ~/Downloads/output/models/model-50epoch-nanloss_0.3870acc.ckpt \     --model-config-path ~/Downloads/output/model_config.yml  \     --dataset-path test.txt \     --auto-encoding \     --beam-size 2 \     --disable-mixed-precision  [skip some messy logs...] [2020-12-20 01:42:28 308] RNN `implementation=2` is not supported when `recurrent_dropout` is set. Using `implementation=1`. DEBUG:tensorflow:RNN `implementation=2` is not supported when `recurrent_dropout` is set. Using `implementation=1`. [2020-12-20 01:42:28 311] RNN `implementation=2` is not supported when `recurrent_dropout` is set. Using `implementation=1`. DEBUG:tensorflow:RNN `implementation=2` is not supported when `recurrent_dropout` is set. Using `implementation=1`. [2020-12-20 01:42:28 315] RNN `implementation=2` is not supported when `recurrent_dropout` is set. Using `implementation=1`. [2020-12-20 01:42:30 963] Loaded weights of model Perplexity: 17.618855794270832  BLEU: 0.07615733809469007: : 1it [00:04   4.38s/it] [2020-12-20 01:42:35 347] Finished evalaution! [2020-12-20 01:42:35 348] Perplexity: 17.618855794270832  BLEU: 0.07615733809469007 ``` Results is ppl and BLEU.   You can start training by running script like below ```sh $ python -m scripts.inference \     --dataset-path test.txt \     --model-path ~/Downloads/output/models/model-50epoch-nanloss_0.3870acc.ckpt \     --output-path out.txt \     --save-pair  [skip some messy logs...] [2020-12-20 01:52:27 856] Loaded weights of model [2020-12-20 01:52:27 857] Start Inference [2020-12-20 01:52:35 629] Ended Inference  Start to save... [2020-12-20 01:52:35 631] Saved (original sentence decoded sentence) pairs to out.txt ```   You can test your trained model interactively. If you want to finish  just enter to put empty input. ```sh $ python -m scripts.interactive \     --model-name TransformerSeq2Seq \     --model-path ~/model-28epoch-0.1396loss_0.9812acc.ckpt \     --model-config-path resources/configs/transformer.yml  [2021-04-30 00:27:00 037] Loaded weights of model Please Input Text: 너 이름이 뭐야? 2021-04-30 00:27:23.437004: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10 2021-04-30 00:27:23.705647: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7 Output: 너 이름이 뭐야?  Perplexity: 1.0628 Please Input Text: 근데 어쩌라는 걸까 Output: 근데 어쩌라는 걸까  Perplexity: 1.0594 Please Input Text:  헤헤헤헤 Output: 헤헤헤헤  Perplexity: 1.4151 Please Input Text: ```   You can simply convert model checkpoint to savedmodel format. ```sh $ python -m scripts.convert_to_savedmodel \     --model-name RNNSeq2SeqWithAttention \     --model-config-path ~/Downloads/output/model_config.yml \     --model-weight-path ~/Downloads/output/models/model-50epoch-nanloss_0.3870acc.ckpt \     --output-path seq2seq-model/1  [skip some messy logs...] Instructions for updating: This property should not be used in TensorFlow 2.0  as updates are applied automatically. INFO:tensorflow:Assets written to: seq2seq-model/1/assets [2020-12-20 01:58:49 424] Assets written to: seq2seq-model/1/assets [2020-12-20 01:58:51 285] Saved model to seq2seq-model/1 ``` If you make savedmodel by using this script  tokenize is included in savedmodel so you can use generate sequence without tokenizer or vocab.   ```sh $ docker run -v `pwd`/seq2seq-model:/models/seq2seq -e MODEL_NAME=seq2seq -p 8501:8501 -dt tensorflow/serving ``` You can open tensorflow serving server.  ```sh $ curl -XPOST localhost:8501/v1/models/seq2seq:predict -d '{""inputs"":[""안녕하세요""  ""나는 오늘 밥을 먹었다""  ""아니 지금 뭐라고요?  그게 대체 무슨 말이에요!!""]}' {     ""outputs"": {         ""perplexity"": [             1.00468457              1.06678605              1.04327798         ]          ""sentences"": [             ""안녕하세요""              ""나는 오늘 밥을 먹었다""              ""아니 지금 뭐라고요?  그게 대체 무슨 말이에요!!""         ]     } } ``` - By default  signature function is greedy search. Like above example  you can send texts then receice ppl and gernerated texts.  ```sh $ curl -XPOST localhost:8501/v1/models/seq2seq:predict -d '{""inputs"":{""texts"":[""반갑습니다""  ""학교가기 싫다""]  ""beam_size"":3}  ""signature_name"":""beam_search""}' {     ""outputs"": {         ""sentences"": [             [                 ""반갑습니다""                  ""반갑습니다""                  ""반갑습니다""             ]              [                 ""학교가기 싫다""                  ""학교가기놔""                  ""학교가기 챙겨""             ]         ]          ""perplexity"": [             [                 1.0299294                  1.0299294                  1.0299294             ]              [                 1.22807097                  1.54527545                  1.56684875             ]         ]     } } ``` - If you want to inference by beam searching  set `signature_name` as beam_search and request with beam_size. - Response also contains beam size number of texts per an example.   """;General;https://github.com/cosmoquester/seq2seq
"""Python >= 3.6   """;Computer Vision;https://github.com/anked10/CNNs_tensorflow2_InceptionV3-maderas
"""First of all  install the MXNet 2 release such as MXNet 2 Alpha. You may use the following commands:  ```bash #: Install the version with CUDA 10.2 python3 -m pip install -U --pre ""mxnet-cu102>=2.0.0a""  #: Install the version with CUDA 11 python3 -m pip install -U --pre ""mxnet-cu110>=2.0.0a""  #: Install the cpu-only version python3 -m pip install -U --pre ""mxnet>=2.0.0a"" ```   To install GluonNLP  use  ```bash python3 -m pip install -U -e .  #: Also  you may install all the extra requirements via python3 -m pip install -U -e .""[extras]"" ```  If you find that you do not have the permission  you can also install to the user folder:  ```bash python3 -m pip install -U -e . --user ```  For Windows users  we recommend to use the [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/about).    : Also  you can use python -m to access the toolkits   You can use Docker to launch a JupyterLab development environment with GluonNLP installed.  ``` #: GPU Instance docker pull gluonai/gluon-nlp:gpu-latest docker run --gpus all --rm -it -p 8888:8888 -p 8787:8787 -p 8786:8786 --shm-size=2g gluonai/gluon-nlp:gpu-latest  #: CPU Instance docker pull gluonai/gluon-nlp:cpu-latest docker run --rm -it -p 8888:8888 -p 8787:8787 -p 8786:8786 --shm-size=2g gluonai/gluon-nlp:cpu-latest ```   For more details  you can refer to the guidance in [tools/docker](tools/docker).  """;General;https://github.com/dmlc/gluon-nlp
"""The sketch dataset can be downloaded as follow:  ./download_data.sh sketch   wget https://dl.fbaipublicfiles.com/moco/moco_checkpoints/moco_v2_800ep/moco_v2_800ep_pretrain.pth.tar   python src/main.py --exp-name vmtc-repr --cuda --run-id sketch-real vmtc_repr --ss-path moco_v2_800ep_pretrain.pth.tar   To create the MNIST->SVHN grids  simply run the following command:   To create the Sketches->Reals grid  simply run the following command:   """;General;https://github.com/lavoiems/Cats-UDT
""" I have been using this product for a few years now and it is the best thing on the market to keep your teeth white. It does not taste bad at all like some of these other products do. The only problem with this product is that you need to use it every day or else they will start coming back in after about 2 weeks. But if you do that  then it's worth it. You can also buy them from Amazon but shipping takes forever. So just make sure you order enough so you don't run out.     I have been using this product for a few years now and it is the best thing on the market to keep your teeth white. It does not taste bad at all like some of these other products do. The only problem with this product is that you need to use it every day or else they will start coming back in after about 2 weeks. But if you do that  then it's worth it. You can also buy them from Amazon but shipping takes forever. So just make sure you order enough so you don't run out.    Questions Q: What is the capital of India?  A: mumbai.   Q: Who was a British politician who served as Prime Minister from 1922 to 1924?   A: edward viibert   Q: The name of which city in New South Wales has been used for many years by the Australian National Football team?   A: sydney   Q: Which American actor starred with his wife and daughter on the television series 'Family Affair'?   A: james coburn   Q: In what year did the first edition of this book appear?   A: 1962   Q: How long does it take to make one pound of sausage?   Questions Q: What is the capital of India?   A: mumbai.   Q: Who was a British politician who served as Prime Minister from 1922 to 1924?   A: edward viibert   Q: The name of which city in New South Wales has been used for many years by the Australian National Football team?   A: sydney   Q: Which American actor starred with his wife and daughter on the television series 'Family Affair'?   A: james coburn   Q: In what year did the first edition of this book appear?   A: 1962   Q: How long does it take to make one pound of sausage?   """;Natural Language Processing;https://github.com/JunnYu/paddle_ctrl
"""Instructions for acquiring PTB and WT2 can be found here. While CIFAR-10 can be automatically downloaded by torchvision  ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions here.   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/IsmaelCesar/darts
"""English | [简体中文](README_CN.md)  [![Documentation](https://readthedocs.org/projects/mmpose/badge/?version=latest)](https://mmpose.readthedocs.io/en/latest/?badge=latest) [![actions](https://github.com/open-mmlab/mmpose/workflows/build/badge.svg)](https://github.com/open-mmlab/mmpose/actions) [![codecov](https://codecov.io/gh/open-mmlab/mmpose/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmpose) [![PyPI](https://img.shields.io/pypi/v/mmpose)](https://pypi.org/project/mmpose/) [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/master/LICENSE) [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues) [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)  MMPose is an open-source toolbox for pose estimation based on PyTorch. It is a part of the [OpenMMLab project](https://github.com/open-mmlab).  The master branch works with **PyTorch 1.5+**.  https://user-images.githubusercontent.com/15977946/124654387-0fd3c500-ded1-11eb-84f6-24eeddbf4d91.mp4   Please refer to [data_preparation.md](docs/en/data_preparation.md) for a general knowledge of data preparation.   Please refer to [install.md](docs/en/install.md) for installation.   - [x] [COCO-WholeBody](https://mmpose.readthedocs.io/en/latest/papers/datasets.html#coco-wholebody-eccv-2020) \[[homepage](https://github.com/jin-s13/COCO-WholeBody/)\] (ECCV'2020)   Please see [getting_started.md](docs/en/getting_started.md) for the basic usage of MMPose. There are also tutorials:  - [learn about configs](docs/en/tutorials/0_config.md) - [finetune model](docs/en/tutorials/1_finetune.md) - [add new dataset](docs/en/tutorials/2_new_dataset.md) - [customize data pipelines](docs/en/tutorials/3_data_pipeline.md) - [add new modules](docs/en/tutorials/4_new_modules.md) - [export a model to ONNX](docs/en/tutorials/5_export_model.md) - [customize runtime settings](docs/en/tutorials/6_customize_runtime.md)   """;General;https://github.com/open-mmlab/mmpose
"""<br>The most likely classification is acceptable for the picture -> both get a 1<br>   |     Image    | Current accepted results (based on gt)                | TOP-5 results from NASNetLarge                                                             | |:------------:|-------------------------------------------------------|--------------------------------------------------------------------------------------------| | ![](https://drive.google.com/uc?export=view&id=14J8Lir-uKsqtujJF7GbJHduqPBLA_2dU)0AKZCRZA.png <br>| train  Train  industry  railway  Railway  sky  sunset | 'freight_car'  'electric_locomotive'  'passenger_car'  'trailer_truck'  'steam_locomotive' |    **NasNet Large** - [x] Brute force approach: finished (@Alex)    * Code https://github.com/asad-62/IVP-DNN/blob/main/full-res-nas-net_alex_291220.py    * Results https://github.com/asad-62/IVP-DNN/blob/main/results_29-12_nasnet-large.csv  - ~~[ ] PCA approach: doing until 03.01.2021 (@Asad  @Abhinav  @Muhammad)~~  **EfficientNetB7** - [X] Brute force approach: finished (@Alex)   * Code https://github.com/asad-62/IVP-DNN/blob/main/efficient-net_alex_291220.py    * Results https://github.com/asad-62/IVP-DNN/blob/main/results_29-12_effnetB7.csv - ~~[ ] PCA approach: doing until 03.01.2021 (@Asad  @Abhinav  @Muhammad)~~  **DenseNet 201** - [X] Brute force approach: finished (@Alex)   * Code https://github.com/asad-62/IVP-DNN/blob/main/dense-net_alex_291220.py    * Results https://github.com/asad-62/IVP-DNN/blob/main/results_29-12_densenet.csv  - ~~[ ] PCA approach: doing until 03.01.2021 (@Asad  @Abhinav  @Muhammad)~~    """;General;https://github.com/asad-62/IVP-DNN
"""You can try it at:   """;Computer Vision;https://github.com/javiergarciamolina/selfie-background-removal
"""Fully Convolutional Networks (FCNs) are a natural extension of CNNs to tackle per pixel prediction problems such as semantic image segmentation. FCNs add upsampling layers to standard CNNs to recover the spatial resolution of the input at the output layer. In  order to compensate for the resolution loss induced by pooling layers  FCNs introduce skip connections between their downsampling  and upsampling paths. Skip connections help the upsampling path recover fine-grained information from the downsampling layers.  One evolution of CNNs are [Residual Networks](https://arxiv.org/abs/1512.03385) (ResNets). ResNets are designed to ease the training of  very deep networks by introducing a residual block that sums the non-linear transformation of the input and its identity mapping.  The identity mapping is implemented by means of a shortcut connection. ResNets can be extended to work as FCNs. ResNets incorporate  shortcut paths to FCNs and increase the number of connections within a network. This additional shortcut paths improve the segmentation  accuracy and also help the network to converge faster.  Recently another CNN architecture called [DenseNet](https://arxiv.org/abs/1608.06993) has been introduced. DenseNets are built from  *dense blocks* and pooling operations  where each dense block is an iterative concatenation of previous feature maps. This architecture  can be seen as an extension of ResNets  which performs iterative summation of previous feature maps. The result of this modification  is that DenseNets are more efficient in there parameter usage.  The [https://arxiv.org/abs/1611.09326](https://arxiv.org/abs/1611.09326) paper extends DenseNets to work as FCNs by adding an upsampling  path to recover the full input resolution.    Clone Github repo with CamVid data  git clone https://github.com/mostafaizz/camvid.git       --train-path ./camvid-preprocessed/camvid-384x480-train.tfrecords \       --train-path ./camvid-preprocessed/camvid-384x480-train.tfrecords \   Here are the color encodings for the labels:  ![""LabelsColorKey""](images/LabelsColorKey.jpg?raw=true ""LabelsColorKey"")  The following examples show the original image  the true label map and the predicted label map:  ![""camvid-segmentation-1""](images/camvid-segmentation-1.png?raw=true ""camvid-segmentation-1"")  ![""camvid-segmentation-1""](images/camvid-segmentation-2.png?raw=true ""camvid-segmentation-2"")  ![""camvid-segmentation-3""](images/camvid-segmentation-3.png?raw=true ""camvid-segmentation-3"")  ![""camvid-segmentation-4""](images/camvid-segmentation-4.png?raw=true ""camvid-segmentation-4"")  ![""camvid-segmentation-5""](images/camvid-segmentation-5.png?raw=true ""camvid-segmentation-5"")   """;General;https://github.com/asprenger/keras_fc_densenet
"""該論文提出了一個通用的reconstruction network，希望能夠對輸入的任意style進行transfer，而不需要重新train model；換句話說，就是希望能夠使用任意的reference image來進行style transfer，擺脫傳統的style transfer對於style和content loss需要通過對layer的嘗試參數，來得到一個和style較爲匹配的表述纔能有較好的效果，且針對不同的style這一步驟需要重新training這樣的缺點。 該論文提出了Whitening & Coloring transform layer (WCT layer)，它的實作觀念在於，對於任何一種style image(reference image)，要能夠使content表現出style的風格，只需在feature map上分布表徵一致。 首先，將feature map減去平均值，然後乘上對自己的協方差矩陣的逆矩陣，來進行whitening的動作，以利將feature map拉到一個白話的分布空間。然後透過對reference image取得feature map的coloring協方差矩陣的方式，將其乘以content image whitening後的結果，並加上平均值，就可以將content image whitening後的feature map空間轉移到reference image圖片上平均分布；最後，透過Stylization Weight Control 的公式：  <a href=""https://www.codecogs.com/eqnedit.php?latex=\widehat{f_{cs}}&space;=&space;\alpha&space;\widehat{f_{cs}}&space;&plus;&space;(1&space;-&space;\alpha)\widehat{f_c}"" target=""_blank""><img src=""https://latex.codecogs.com/gif.latex?\widehat{f_{cs}}&space;=&space;\alpha&space;\widehat{f_{cs}}&space;&plus;&space;(1&space;-&space;\alpha)\widehat{f_c}"" title=""\widehat{f_{cs}} = \alpha \widehat{f_{cs}} + (1 - \alpha)\widehat{f_c}"" /></a>  就可以完成將reference image整合input image的動作。   該論文透過卷積神經網路，將圖片的內容及風格分開並重建，提供一個style transfer的做法。  該論文利用簡單的統計分析，將一張圖的顏色特徵轉移到另外一張圖上，其中，色彩校正的部分主要是藉由選擇合適的source image，並將其特徵應用到target image上來實現。   Cycle gan跟傳統的gan做圖像轉換的方式不同，它不需要配對的數據集(paired image data set)；利用兩個generator、discrimnator和轉換的一致性(consistency)，cycle gan只需要不同風格的unpaired image data set即可運作。   |![](https://i.imgur.com/RrztFYY.jpg)|![](https://i.imgur.com/APtBT2Q.jpg)|![](https://i.imgur.com/j4ttZKo.jpg)| | ----------------- | --------------- | --------------- |   ![](https://i.imgur.com/felJKFp.jpg)  ![](https://i.imgur.com/FkezUlt.png)     ![](https://i.imgur.com/0n5p0oR.png)    """;General;https://github.com/eugene08976/hw1
"""Includes cifar10 training example. Achieves ~86% accuracy using Resnet18 model.  ![cifar10_convergence](images/convergence.png?raw=true ""Convergence on cifar10"")  Note that ResNet18 as implemented doesn't really seem appropriate for CIFAR-10 as the last two residual stages end up  as all 1x1 convolutions from downsampling (stride). This is worse for deeper versions. A smaller  modified ResNet-like  architecture achieves ~92% accuracy (see [gist](https://gist.github.com/JefferyRPrice/c1ecc3d67068c8d9b3120475baba1d7e)).  """;General;https://github.com/raghakot/keras-resnet
"""Manual: https://github.com/AlexeyAB/darknet/wiki   Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/  Pytorch - Scaled-YOLOv4: https://github.com/WongKinYiu/ScaledYOLOv4  TensorFlow: pip install yolov4 YOLOv4 on TensorFlow 2.0 / TFlite / Andriod: https://github.com/hunglc007/tensorflow-yolov4-tflite   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example   PyTorch > ONNX:    TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   Deepstream 5.0 / TensorRT for YOLOv4 https://github.com/NVIDIA-AI-IOT/yolov4_deepstream or https://github.com/marcoslucianops/DeepStream-Yolo   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like CUDA  cudnn  ZED and build against those. It will also create a shared object library file to use darknet for code development.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   You also need to specify for which graphics card the code is generated. This is done by setting ARCH=. If you use a never version than CUDA 11 you further need to edit line 20 from Makefile and remove -gencode arch=compute_30 code=sm_30 \ as Kepler GPU support was dropped in CUDA 11. You can also drop the general ARCH= and just uncomment ARCH= for your graphics card.   * MSVS: https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community  * Cmake GUI: Windows win64-x64 Installerhttps://cmake.org/download/   find the executable file darknet.exe in the output path to the binaries you specified  This is the recommended approach to build Darknet on Windows.  Install Visual Studio 2017 or 2019. In case you need to download it  please go here: Visual Studio Community  Install CUDA (at least v10.0) enabling VS Integration during installation.   PS Code\&gt;              git clone https://github.com/microsoft/vcpkg  PS Code\&gt;              cd vcpkg   PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base cuda cudnn]:x64-windows for a quicker install of dependencies  PS Code\vcpkg&gt;         cd ..  PS Code\&gt;              git clone https://github.com/AlexeyAB/darknet  PS Code\&gt;              cd darknet   Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   in Python: https://github.com/tzutalin/labelImg  in Python: https://github.com/Cartucho/OpenLabeling   in JavaScript: https://github.com/opencv/cvat   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux   * using `build.sh` or   * build `darknet` using `cmake` or   * set `LIBSO=1` in the `Makefile` and do `make` * on Windows   * using `build.ps1` or   * build `darknet` using `cmake` or   * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs:  * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h   * Python examples using the C API:     * https://github.com/AlexeyAB/darknet/blob/master/darknet.py     * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py  * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp   * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp  ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`      * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)  `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42)  ```cpp struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false);         std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/shanky1947/YOLOv3-Darknet-Custom-Object-Detection
"""This method aims at helping computer vision practitioners faced with an overfit problem. The idea is to replace  in a 3-branch ResNet  the standard summation of residual branches by a stochastic affine combination. The largest tested model improves on the best single shot published result on CIFAR-10 by reaching 2.72% test error.  ![shake-shake](https://s3.eu-central-1.amazonaws.com/github-xg/architecture3.png)  Figure 1: **Left:** Forward training pass. **Center:** Backward training pass. **Right:** At test time.   This repository contains the code for the paper Decoupled Weight Decay Regularization (old title: Fixing Weight Decay Regularization in Adam) by Ilya Loshchilov and Frank Hutter  ICLR 2019 [arXiv](https://arxiv.org/abs/1711.05101).   The code represents a tiny modification of the source code provided for the Shake-Shake regularization by Xavier Gastaldi [arXiv](https://arxiv.org/abs/1705.07485). Since the usage of both is very similar  the introduction and description of the original Shake-Shake code is given below. Please consider to  *first* run the Shake-Shake code and then our code.   Find below a few examples to train a 26 2x96d ""Shake-Shake-Image"" ResNet on CIFAR-10 with 1 GPU. To run on 4 GPUs  set `CUDA_VISIBLE_DEVICES=0 1 2 3` and `-nGPU 4`. For test purposes you may reduce `-nEpochs` from 1500 to e.g. 150 and set `-widenFactor` to 4 to use a smaller network.  To run on ImageNet32x32  set `-dataset` to imagenet32 and reduce `-nEpochs` to 150. You may consider to use `-weightDecay=0.05` for CIFAR-10.   Importantly  please copy with replacement `adam.lua` and `sgd.lua` from `UPDATETORCHFILES` to `YOURTORCHFOLDER/install/share/lua/5.1/optim/`  To run AdamW for `nEpochs=1500` epochs without restarts with initial learning rate `LR=0.001`  normalized weight decay `weightDecay=0.025`     ``` CUDA_VISIBLE_DEVICES=0 th main.lua -algorithmType ADAMW -nEpochs 1500 -Te 1500 -Tmult 2 -widenFactor 6 -LR 0.001 -weightDecay 0.025 -dataset cifar10 -nGPU 1 -depth 26 -irun 1 -batchSize 128 -momentum 0.9 -shareGradInput false -optnet true -netType shakeshake -forwardShake true -backwardShake true -shakeImage true -lrShape cosine -LRdec true ```  To run AdamW for `nEpochs=1500` epochs with restarts  where the first restart will happen after `Te=100` epochs and the second restart after 200 more epochs because `100*Tmult=200`.   ``` CUDA_VISIBLE_DEVICES=0 th main.lua -algorithmType ADAMW -nEpochs 1500 -Te 100 -Tmult 2 -widenFactor 6 -LR 0.001 -weightDecay 0.025 -dataset cifar10 -nGPU 1 -depth 26 -irun 1 -batchSize 128 -momentum 0.9 -shareGradInput false -optnet true -netType shakeshake -forwardShake true -backwardShake true -shakeImage true -lrShape cosine -LRdec true ```  To run SGDW for `nEpochs=150` epochs without restarts with initial learning rate `LR=0.05`  normalized weight decay `weightDecay=0.025`     ``` CUDA_VISIBLE_DEVICES=0 th main.lua -algorithmType SGDW -nEpochs 1500 -Te 1500 -Tmult 2 -widenFactor 6 -LR 0.05 -weightDecay 0.025 -dataset cifar10 -nGPU 1 -depth 26 -irun 1 -batchSize 128 -momentum 0.9 -shareGradInput false -optnet true -netType shakeshake -forwardShake true -backwardShake true -shakeImage true -lrShape cosine -LRdec true ```  To run SGDW for `nEpochs=150` epochs with restarts  where the first restart will happen after `Te=100` epochs and the second restart after 200 more epochs because `100*Tmult=200`.   ``` CUDA_VISIBLE_DEVICES=0 th main.lua -algorithmType SGDW -nEpochs 1500 -Te 100 -Tmult 2 -widenFactor 6 -LR 0.001 -weightDecay 0.025 -dataset cifar10 -nGPU 1 -depth 26 -irun 1 -batchSize 128 -momentum 0.9 -shareGradInput false -optnet true -netType shakeshake -forwardShake true -backwardShake true -shakeImage true -lrShape cosine -LRdec true ```  Acknowledgments: We thank Patryk Chrabaszcz for creating functions dealing with ImageNet32x32 dataset.    0. Install [fb.resnet.torch] (https://github.com/facebook/fb.resnet.torch)  [optnet](https://github.com/fmassa/optimize-net) and [lua-stdlib](https://github.com/lua-stdlib/lua-stdlib). 1. Download Shake-Shake ``` git clone https://github.com/xgastaldi/shake-shake.git ``` 2. Copy the elements in the shake-shake folder and paste them in the fb.resnet.torch folder. This will overwrite 5 files (*main.lua*  *train.lua*  *opts.lua*  *checkpoints.lua* and *models/init.lua*) and add 3 new files (*models/shakeshake.lua*  *models/shakeshakeblock.lua* and *models/mulconstantslices.lua*). 3. You can train a 26 2x32d ""Shake-Shake-Image"" ResNet on CIFAR-10+ using  ``` th main.lua -dataset cifar10 -nGPU 1 -batchSize 128 -depth 26 -shareGradInput false -optnet true -nEpochs 1800 -netType shakeshake -lrShape cosine -widenFactor 2 -LR 0.2 -forwardShake true -backwardShake true -shakeImage true ```   You can train a 26 2x96d ""Shake-Shake-Image"" ResNet on 2 GPUs using  ``` CUDA_VISIBLE_DEVICES=0 1 th main.lua -dataset cifar10 -nGPU 2 -batchSize 128 -depth 26 -shareGradInput false -optnet true -nEpochs 1800 -netType shakeshake -lrShape cosine -widenFactor 6 -LR 0.2 -forwardShake true -backwardShake true -shakeImage true ```  A widenFactor of 2 corresponds to 32d  4 to 64d  etc..   """;General;https://github.com/loshchil/AdamW-and-SGDW
"""make new directory called tmux_tmp copy ncurses.tar.gz and tmx-2.5.tar.gz to tmux_tmp directory copy install_tmux.sh to server and run ./install_tmux.sh  setup directories  clone rbi and setup conda environment  ```bash mkdir -p ~/data/rbi/results mkdir -p ~/data/rbi/logs mkdir -p ~/projects cd ~/projects git clone https://github.com/eladsar/rbi.git cd ~/projects/rbi conda env create -f install/environment.yml source activate torch1 pip install atari-py ```   copy anaconda file to server and run: sh Anaconda3-2018.12-Linux-x86_64.sh   cd install   and from the server terminal run   Use the identifier name and the resume parameter to choose the required run.   """;Reinforcement Learning;https://github.com/eladsar/rbi
"""To get started  download the pre-trained [VGG 16 weights](http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz). Move the file `vgg_16.ckpt` to `tf-ssd-vgg/models/vgg_16_imagenet`.  To train the neural network  download the below and move them to `tf-ssd-vgg/data/raw/voc2007` and `tf-ssd-vgg/data/raw/voc2012`. - [Pascal VOC 2007 trainval](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar) - [Pascal VOC 2007 test](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar) - [Pascal VOC 2012 trainval](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar)   """;Computer Vision;https://github.com/PranavEranki/SSD-Algorithm
"""This is a proposition for the Capstone project for the EPFL Extension School Applied Machine Learning program. The objective is to train a neural net for custom class object detection and run inference at the edge by: - building a custom data set and annotate it; - train a network using data augmentation techniques and transfer learning with fine-tuning of the last layers; - (if possible) running inference at the edge on a device with limited computing power.  I will thoroughly document each phase of the project and draw conclusions on the best techniques to use.       os.mkdir(imdir)   (old name : TFOD_latest_short.mp4  changed to gopro_footage_edited.mp4) -->   I used Paperspace which is a cloud platform that provides Linux virtual machines with GPU computing power.   - each with a Quadro P4000 GPU   - tensorflow-gpu (1.12.0)   -- source   (source : https://ngrok.com/docs)   weights : download from http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_kitti_2018_01_28.tar.gz   Source :   from : https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config   """;Computer Vision;https://github.com/petrum01/Capstone_project_object_detection
"""Epoch 1: <p align=""center""> <img src="".//MNIST_GAN_FC/samples/fake_images-1.png"" width=""300""/> </p> Epoch 5: <p align=""center""> <img src="".//MNIST_GAN_FC/samples/fake_images-5.png"" width=""300""/> </p> Epoch 10: <p align=""center""> <img src="".//MNIST_GAN_FC/samples/fake_images-10.png"" width=""300""/> </p> Epoch 20: <p align=""center""> <img src="".//MNIST_GAN_FC/samples/fake_images-20.png"" width=""300""/> </p> Epoch 50: <p align=""center""> <img src="".//MNIST_GAN_FC/samples/fake_images-50.png"" width=""300""/> </p> Epoch 100: <p align=""center""> <img src="".//MNIST_GAN_FC/samples/fake_images-100.png"" width=""300""/> </p>   Epoch 1: <p align=""center""> <img src="".//MNIST_GAN_CNN/samples_mode_collapse/fake_images-1.png"" width=""300""/> </p> Epoch 5: <p align=""center""> <img src="".//MNIST_GAN_CNN/samples_mode_collapse/fake_images-5.png"" width=""300""/> </p> Epoch 10: <p align=""center""> <img src="".//MNIST_GAN_CNN/samples_mode_collapse/fake_images-10.png"" width=""300""/> </p> Epoch 20: <p align=""center""> <img src="".//MNIST_GAN_CNN/samples_mode_collapse/fake_images-20.png"" width=""300""/> </p> Epoch 50: <p align=""center""> <img src="".//MNIST_GAN_CNN/samples_mode_collapse/fake_images-50.png"" width=""300""/> </p>   References:  - https://gist.github.com/t-ae/732f78671643de97bbe2c46519972491  - https://arxiv.org/abs/1606.03498  - https://mc.ai/gan-ways-to-improve-gan-performance/  Epoch 1: <p align=""center""> <img src="".//MNIST_GAN_CNN/samples/fake_images-1.png"" width=""300""/> </p> Epoch 5: <p align=""center""> <img src="".//MNIST_GAN_CNN/samples/fake_images-5.png"" width=""300""/> </p> Epoch 10: <p align=""center""> <img src="".//MNIST_GAN_CNN/samples/fake_images-15.png"" width=""300""/> </p> Epoch 15: <p align=""center""> <img src="".//MNIST_GAN_CNN/samples/fake_images-25.png"" width=""300""/> </p> Epoch 20: <p align=""center""> <img src="".//MNIST_GAN_CNN/samples/fake_images-30.png"" width=""300""/> </p>   """;General;https://github.com/nanwei1/MNIST_GAN
"""Dependencies:        <img width=40% src=""https://github.com/reiniscimurs/TD3_Separate_Action/blob/master/Gifs/Cheetah40.gif"">       <img width=40% src=""https://github.com/reiniscimurs/TD3_Separate_Action/blob/master/Gifs/Cheetah50.gif"">       <img width=40% src=""https://github.com/reiniscimurs/TD3_Separate_Action/blob/master/Gifs/Cheetah100.gif"">   """;General;https://github.com/reiniscimurs/TD3_Separate_Action
"""In `data/custom/train.txt` and `data/custom/val.txt`  add paths to images that will be used as train and validation data respectively.   ```bash git clone https://github.com/Lornatang/YOLOv4-PyTorch.git cd YOLOv4-PyTorch/ pip install -r requirements.txt ```   Clone and install requirements   Download PascalVoc2007  Download COCO2014  Download COCO2017   To train on COCO2014/COCO2017 run:    To train on VOC07+12 run:   To train on COCO2014/COCO2017 run:    To train on VOC07+12 run:   Webcam:  --source 0  HTTP stream:  --source https://v.qq.com/x/page/x30366izba3.html   cd configs/   bash create_model.sh your-dataset-num-classes   To train on the custom dataset run:   """;Computer Vision;https://github.com/Lornatang/YOLOv4-PyTorch
"""If you'd rather checkout and build the model locally you can follow the run locally steps below.  You can deploy the model-serving microservice on Red Hat OpenShift by following the instructions for the OpenShift web console or the OpenShift Container Platform CLI in this tutorial  specifying codait/max-question-answering as the image name.   On your Kubernetes cluster  run the following commands:   Clone this repository locally. In a terminal  run the following command:  $ git clone https://github.com/IBM/MAX-Question-Answering.git   $ cd MAX-Question-Answering    The API server automatically generates an interactive Swagger documentation page. Go to `http://localhost:5000` to load it. From there you can explore the API and also create test requests.  Use the `model/predict` endpoint to upload a test json file (you can use/alter the files from the `samples` folder) and get answers to the questions from the API.  Sample input: ```json {   ""paragraphs"": [     {       ""context"": ""John lives in Brussels and works for the EU""        ""questions"": [         ""Where does John Live?""          ""What does John do?""          ""What is his name?""       ]     }      {       ""context"": ""Jane lives in Paris and works for the UN""        ""questions"": [         ""Where does Jane Live?""          ""What does Jane do?""       ]     }   ] } ```  ![Example of getting answers from the API](docs/swagger-screenshot.png)  You can also test it on the command line  for example:  ```shell $ curl -X POST ""http://localhost:5000/model/predict"" -H ""accept: application/json"" -H ""Content-Type: application/json"" -d ""{\""paragraphs\"": [{ \""context\"": \""John lives in Brussels and works for the EU\""  \""questions\"": [\""Where does John Live?\"" \""What does John do?\"" \""What is his name?\"" ]} { \""context\"": \""Jane lives in Paris and works for the UN\""  \""questions\"": [\""Where does Jane Live?\"" \""What does Jane do?\"" ]}]}"" ```  You should see a JSON response like that below:  ```json {   ""status"": ""ok""    ""predictions"": [     [       ""Brussels""        ""works for the EU""        ""John""     ]      [       ""Paris""        ""works for the UN""     ]   ] } ```   """;Natural Language Processing;https://github.com/kiranlvs93/MAX-Question-Answering
"""  <img align=""left"" alt=""Tensorflow 2"" width=""40px"" src=""https://www.vectorlogo.zone/logos/numpy/numpy-icon.svg""/>   """;Computer Vision;https://github.com/sagnik1511/Transfer-Learning-with-Python
"""Install hyperopt https://github.com/hyperopt/hyperopt   Agents tested using CartPole env.   - Install dependancies imported ([my tf2 conda env as reference](https://github.com/anita-hu/TF2-RL/blob/master/mytf2env.txt)) - Each file contains example code that runs training on CartPole env - Training: `python3 TF2_DDPG_LSTM.py` - Tensorboard: `tensorboard --logdir=DDPG/logs`   | DQN Basic  time step = 4  500 reward | DQN LSTM  time step = 4  500 reward | | --- | --- | | <img src=""DQN/gifs/test_render_basic_time_step4_reward500.gif"" height=""200""> | <img src=""DQN/gifs/test_render_lstm_time_step4_reward500.gif"" height=""200""> |  | DDPG Basic  500 reward | DDPG LSTM  time step = 5  500 reward | | --- | --- | | <img src=""DDPG/gifs/test_render_basic_reward500.gif"" height=""200""> | <img src=""DDPG/gifs/test_render_lstm_time_step5_reward500.gif"" height=""200""> |  | AE-DDPG Basic  500 reward | PPO Basic  500 reward | | --- | -- | | <img src=""AE-DDPG/gifs/test_render_basic_reward500.gif"" height=""200""> | <img src=""PPO/gifs/test_render_basic_reward500.gif"" height=""200""> |  """;Reinforcement Learning;https://github.com/anita-hu/TF2-RL
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   Install <a href=""https://www.tensorflow.org/get_started/os_setup"" target=""_blank"">TensorFlow</a>. You may also need to install h5py. The code has been tested with Python 2.7  TensorFlow 1.0.1  CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04.  If you are using PyTorch  you can find a third-party pytorch implementation <a href=""https://github.com/fxia22/pointnet.pytorch"" target=""_blank"">here</a>.  To install h5py for Python: ```bash sudo apt-get install libhdf5-dev sudo pip install h5py ```   Created by <a href=""http://charlesrqi.com"" target=""_blank"">Charles R. Qi</a>  <a href=""http://ai.stanford.edu/~haosu/"" target=""_blank"">Hao Su</a>  <a href=""http://cs.stanford.edu/~kaichun/"" target=""_blank"">Kaichun Mo</a>  <a href=""http://geometry.stanford.edu/member/guibas/"" target=""_blank"">Leonidas J. Guibas</a> from Stanford University.  ![prediction example](https://github.com/charlesq34/pointnet/blob/master/doc/teaser.png)   cd part_seg  sh download_data.sh   To train a model to classify point clouds sampled from 3D shapes:      python train.py  Log files and network parameters will be saved to `log` folder in default. Point clouds of <a href=""http://modelnet.cs.princeton.edu/"" target=""_blank"">ModelNet40</a> models in HDF5 files will be automatically downloaded (416MB) to the data folder. Each point cloud contains 2048 points uniformly sampled from a shape surface. Each cloud is zero-mean and normalized into an unit sphere. There are also text files in `data/modelnet40_ply_hdf5_2048` specifying the ids of shapes in h5 files.  To see HELP for the training script:      python train.py -h  We can use TensorBoard to view the network architecture and monitor the training progress.      tensorboard --logdir log  After the above training  we can evaluate the model and output some visualizations of the error cases.      python evaluate.py --visu  Point clouds that are wrongly classified will be saved to `dump` folder in default. We visualize the point cloud by rendering it into three-view images.  If you'd like to prepare your own data  you can refer to some helper functions in `utils/data_prep_util.py` for saving and loading HDF5 files.   * <a href=""http://stanford.edu/~rqi/pointnet2/"" target=""_blank"">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a> by Qi et al. (NIPS 2017) A hierarchical feature learning framework on point clouds. The PointNet++ architecture applies PointNet recursively on a nested partitioning of the input point set. It also proposes novel layers for point clouds with non-uniform densities. * <a href=""http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w13/Engelmann_Exploring_Spatial_Context_ICCV_2017_paper.pdf"" target=""_blank"">Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</a> by Engelmann et al. (ICCV 2017 workshop). This work extends PointNet for large-scale scene segmentation. * <a href=""https://arxiv.org/abs/1710.04954"" target=""_blank"">PCPNET: Learning Local Shape Properties from Raw Point Clouds</a> by Guerrero et al. (arXiv). The work adapts PointNet for local geometric properties (e.g. normal and curvature) estimation in noisy point clouds. * <a href=""https://arxiv.org/abs/1711.06396"" target=""_blank"">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a> by Zhou et al. from Apple (arXiv) This work studies 3D object detection using LiDAR point clouds. It splits space into voxels  use PointNet to learn local voxel features and then use 3D CNN for region proposal  object classification and 3D bounding box estimation. * <a href=""https://arxiv.org/abs/1711.08488"" target=""_blank"">Frustum PointNets for 3D Object Detection from RGB-D Data</a> by Qi et al. (arXiv) A novel framework for 3D object detection with RGB-D data. The method proposed has achieved first place on KITTI 3D object detection benchmark on all categories (last checked on 11/30/2017).  """;Computer Vision;https://github.com/Lw510107/pointnet-2018.6.27-
"""git clone https://github.com/hiyouga/SAGAN-PyTorch.git  cd SAGAN-PyTorch   """;Computer Vision;https://github.com/hiyouga/SAGAN-PyTorch
"""For example  this command train a RotatE model on FB15k dataset with GPU 0.    --cuda \   CUDA_VISIBLE_DEVICES=$GPU_DEVICE python -u $CODE_PATH/run.py --do_test --cuda -init $SAVE   bash run.sh train RotatE FB15k 0 0 1024 256 1000 24.0 1.0 0.0001 200000 16 -de   """;Graphs;https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) < highly recommended!! * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Google Colab Notebook](https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb) with quick training  inference and testing examples * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart)  * [A TensorRT Implementation of YOLOv3-SPP](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp)    """;Computer Vision;https://github.com/hwillard98/htmlify-yolo-training
"""Please install the following requirements: * Python 3.6 * TensorFlow 2.0 * Numpy * Pandas * Scikit-learn * Gensim * NLTK <br><br>  The submission script which was executed as a Kaggle kernel is `submission_script.py` file. It Is also available  [here](https://www.kaggle.com/milanp/quora-insincere-questions-late-submission-script/code?scriptVersionId=23933427) as a public kernel. <br><br> If you want to make experiments with different models and hyperparameters  please use `main.py` and `build_model.py` files. The model can be tweaked in `build_model.py` file  whereas 5-folded cross validated experiment is executed in `main.py`. Before running  please update the appropriate paths to train and test datasets (lines 37 and 38)  as well as paths to embedding files (lines 40-43) in `main.py`. Both training data and pretrained embeddings are available on the competition's [official website](https://www.kaggle.com/c/quora-insincere-questions-classification/). You can also change some hyperparameters in `hparams` dictionary in lines 97-120 of `main.py`.  <br><br> Settings from every experiment will be saved to a separate folder  which will be printed out at the end. Both `main.py` and `build_model.py` files are modified and belong to a small competition framework which is described in detail [here](https://github.com/mpavlovic/toxic-comments-classification).   """;General;https://github.com/mpavlovic/insincere-questions-classifier
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`  2. Download VOC2007 and VOC2012 dataset. By default  we assume the data is stored in `$HOME/data/`   ```Shell   #: Download the data.   cd $HOME/data   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar   #: Extract the data.   tar -xvf VOCtrainval_11-May-2012.tar   tar -xvf VOCtrainval_06-Nov-2007.tar   tar -xvf VOCtest_06-Nov-2007.tar   ```  3. Create the LMDB file.   ```Shell   cd $CAFFE_ROOT   #: Create the trainval.txt  test.txt  and test_name_size.txt in data/VOC0712/   ./data/VOC0712/create_list.sh   #: You can modify the parameters in create_data.sh if needed.   #: It will create lmdb files for trainval and test with encoded original image:   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb   #: and make soft links at examples/VOC0712/   ./data/VOC0712/create_data.sh   ```   1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`   ```Shell   git clone https://github.com/weiliu89/caffe.git   cd caffe   git checkout ssd   ```  2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.   ```Shell   #: Modify Makefile.config according to your Caffe installation.   cp Makefile.config.example Makefile.config   make -j8   #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.   make py   make test -j8   #: (Optional)   make runtest -j8   ```   For convenience  please follow the VOC dataset format to make the new dataset. Click [here](https://drive.google.com/open?id=11nA6c_NUgV4TyuXK1roLW27K2gMDqFMZ) to download the MELON dataset I made for this repo.  ``` cd ~/data/VOCdevkit ``` ``` mkdir MELON ```  Put all training/test images in `MELON/JPEGImages`  Put all xml-format labels in `MELON/Annotations`  Add all the training/val samples in `MELON/ImageSets/Main/trainval.txt`  Add all the test samples in `MELON/ImageSets/Main/test.txt`  The final directory structure is like this:  ``` VOCdevkit ├── MELON │   ├── Annotations │   ├── ImageSets │   │   └── Main │   └── JPEGImages ├── VOC2007 │   ├── Annotations │   ├── ImageSets │   │   ├── Layout │   │   ├── Main │   │   └── Segmentation │   ├── JPEGImages │   ├── SegmentationClass │   └── SegmentationObject └── VOC2012     ├── Annotations     ├── ImageSets     │   ├── Action     │   ├── Layout     │   ├── Main     │   └── Segmentation     ├── JPEGImages     ├── SegmentationClass     └── SegmentationObject ```   Following the original instructions to compile SSD. Make sure that you can run it successfully.   First cd to the SSD root directory. Then    COCO<sup>[1]</sup>: SSD300*  SSD512*  07+12+COCO: SSD300*  SSD512*  07++12+COCO: SSD300*  SSD512*  COCO models:   """;Computer Vision;https://github.com/Coldmooon/SSD-on-Custom-Dataset
"""|pytorch-handbook|11.9k|pytorch handbook是一本开源的书籍，目标是帮助那些希望和使用PyTorch进行深度学习开发和研究的朋友快速入门，其中包含的Pytorch教程全部通过测试保证可以成功运行|   |nltk|9k|NLTK Source|   |pretrained-models.pytorch|6.9k|Pretrained ConvNets for pytorch: NASNet  ResNeXt  ResNet  InceptionV4  InceptionResnetV2  Xception  DPN  etc.|   |tensorboardX|6.4k|tensorboard for pytorch (and chainer  mxnet  numpy  ...)|   |keras-js|4.7k|Run Keras models in the browser  with GPU support using WebGL|   |practical-pytorch|4.1k|DEPRECATED and not maintained - see official repo at https://github.com/pytorch/tutorials|   |pytorch-cifar|3k|95.16% on CIFAR10 with PyTorch|   |tensorflow-windows-wheel|2.9k|Tensorflow prebuilt binary for Windows|   |pytorch-doc-zh|2.4k|Pytorch 中文文档|   |pytorch-beginner|2.1k|pytorch tutorial for beginners|  |tangent|2.1k|Source-to-Source Debuggable Derivatives in Pure Python|   |tensorflow-build-archived|2k|TensorFlow binaries supporting AVX  FMA  SSE|   |segmentation_models.pytorch|1.9k|Segmentation models with pretrained backbones. PyTorch.|   |pytorch-seq2seq|1.8k|Tutorials on implementing a few sequence-to-sequence (seq2seq) models with PyTorch and TorchText.|   |PyTorch-Encoding|1.5k|A PyTorch CV Toolkit|   |MobileNet|1.4k|MobileNet build with Tensorflow|   |faster_rcnn_pytorch|1.4k|Faster RCNN with PyTorch|   |yolo2-pytorch|1.3k|YOLOv2 in PyTorch|   |Awesome-pytorch-list-CNVersion|1.1k|Awesome-pytorch-list 翻译工作进行中......|   |pytorch-classification|1k|Classification with PyTorch.|   """;Sequential;https://github.com/aymericdamien/TopDeepLearning
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;Computer Vision;https://github.com/anookeen/yolo
"""```python3 X = load_data() #: This is assumed to be a 2-dimensional numpy array  where rows represent data samples. ``` Basic DISCERN instance (numpy-based): ```python3 from DISCERN import DISCERN  di = DISCERN() di.fit(X)  clustering_labels = di.labels_ cluster_centers = di.cluster_centers_ sse_loss = di.inertia_ ``` Fix the number of clusters to a specific number (only use DISCERN to initialize K-Means) ```python3 di = DISCERN(n_clusters=K) ``` Use Spherical K-Means ```python3 di = DISCERN(metric='cosine') ``` Specify an upper bound for the number of clusters ```python3 di = DISCERN(max_n_clusters=1000) ```    """;General;https://github.com/alihassanijr/DISCERN
"""![Result_8a](./Images/result_8a.jpg)      ![Gif](./Images/TWDNE_interpolator.gif)      ----  Execute or include file named `SG2_main.pu`. Then execute following instruction:   > `model = GAN()` Create style GAN2 object   > `model.train(restart=False)` Train model > `model.predict(24)` Generate fake image with 4 rows and 6 cols   > `model.predict(6)` Generate fake image with 2 rows and 3 cols   > `model.predict(2)` Generate fake image with 1 rows and 2 cols   > `model.save_weights()` Save model   > `mdoel.load_weights()` Load model    ----     """;Computer Vision;https://github.com/RyanWu2233/Style_GAN2_TWDNE
"""Install PyTorch 0.4   The codebase is now PyTorch 0.4 compatible for most use cases (a big shoutout to https://github.com/shawntan for a fairly comprehensive PR https://github.com/salesforce/awd-lstm-lm/pull/43). Mild readjustments to hyperparameters may be necessary to obtain quoted performance. If you desire exact reproducibility (or wish to run on PyTorch 0.3 or lower)  we suggest using an older commit of this repository. We are still working on pointer  finetune and generate functionalities.   For data setup  run ./getdata.sh.   PyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed.   """;General;https://github.com/soyoung97/awd-lstm-gru
"""Install this package using `pip install vit-keras`  You can use the model out-of-the-box with ImageNet 2012 classes using something like the following. The weights will be downloaded automatically.  ```python from vit_keras import vit  utils  image_size = 384 classes = utils.get_imagenet_classes() model = vit.vit_b16(     image_size=image_size      activation='sigmoid'      pretrained=True      include_top=True      pretrained_top=True ) url = 'https://upload.wikimedia.org/wikipedia/commons/d/d7/Granny_smith_and_cross_section.jpg' image = utils.read(url  image_size) X = vit.preprocess_inputs(image).reshape(1  image_size  image_size  3) y = model.predict(X) print(classes[y[0].argmax()]) #: Granny smith ```  You can fine-tune using a model loaded as follows.  ```python image_size = 224 model = vit.vit_l32(     image_size=image_size      activation='sigmoid'      pretrained=True      include_top=True      pretrained_top=False      classes=200 ) #: Train this model on your data as desired. ```   """;Computer Vision;https://github.com/faustomorales/vit-keras
"""1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   Requirements: hardware  Basic installation   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   To train and test a Faster R-CNN detector using the **alternating optimization** algorithm from our NIPS 2015 paper  use `experiments/scripts/faster_rcnn_alt_opt.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  (""alt opt"" refers to the alternating optimization training algorithm described in the NIPS paper.)  To train and test a Faster R-CNN detector using the **approximate joint training** method  use `experiments/scripts/faster_rcnn_end2end.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  This method trains the RPN module jointly with the Fast R-CNN network  rather than alternating between training the two. It results in faster (~ 1.5x speedup) training times and similar detection accuracy. See these [slides](https://www.dropbox.com/s/xtr4yd4i5e0vw8g/iccv15_tutorial_training_rbg.pdf?dl=0) for more details.  Artifacts generated by the scripts in `tools` are written in this directory.  Trained Fast R-CNN networks are saved under:  ``` output/<experiment directory>/<dataset name>/ ```  Test outputs are saved under:  ``` output/<experiment directory>/<dataset name>/<network snapshot name>/ ```  """;Computer Vision;https://github.com/rickyHong/py-faster-rcnn-repl-cudnn5-support
"""Original Source Code - https://github.com/salesforce/awd-lstm-lm   """;Sequential;https://github.com/vganesh46/awd-lstm-pytorch-implementation
"""-   Acquire the data  e.g. as a snapshot called `256x256.zip` in [my data repository][data-repository]  -   Run [`StyleGAN2_ADA_training.ipynb`][colab-notebook-training] to train a StyleGAN2-ADA model from scratch. [![Open In Colab][colab-badge]][colab-notebook-training] -   Run [`StyleGAN2_ADA_image_sampling.ipynb`][colab-notebook-sampling] to generate images with a trained StyleGAN2-ADA model  [![Open In Colab][colab-badge]][colab-notebook-sampling] -   To automatically resume training from the latest checkpoint  you will have to use [my fork][stylegan2-ada-fork] of StyleGAN2-ADA.   """;Computer Vision;https://github.com/woctezuma/steam-stylegan2-ada
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/RuiLiFeng/noise
"""release of this package.) Requires python >= 2.7  cython  numpy  scipy.   """;General;https://github.com/weiwang2330/sparse-structured-attention
"""ParlAI currently requires Python3 and [Pytorch](https://pytorch.org) 1.1 or newer. Dependencies of the core modules are listed in `requirement.txt`. Some models included (in `parlai/agents`) have additional requirements.  Run the following commands to clone the repository and install ParlAI:  ```bash git clone https://github.com/facebookresearch/ParlAI.git ~/ParlAI cd ~/ParlAI; python setup.py develop ```  This will link the cloned directory to your site-packages.  This is the recommended installation procedure  as it provides ready access to the examples and allows you to modify anything you might need. This is especially useful if you if you want to submit another task to the repository.  All needed data will be downloaded to `~/ParlAI/data`  and any non-data files if requested will be downloaded to `~/ParlAI/downloads`. If you need to clear out the space used by these files  you can safely delete these directories and any files needed will be downloaded again.   A large set of scripts can be found in `parlai/scripts`. Here are a few of them. Note: If any of these examples fail  check the [requirements section](#requirements) to see if you have missed something.  Display 10 random examples from the SQuAD task ```bash python -m parlai.scripts.display_data -t squad ```  Evaluate an IR baseline model on the validation set of the Personachat task: ```bash python -m parlai.scripts.eval_model -m ir_baseline -t personachat -dt valid ```  Train a single layer transformer on PersonaChat (requires pytorch and torchtext). Detail: embedding size 300  4 attention heads   2 epochs using batchsize 64  word vectors are initialized with fasttext and the other elements of the batch are used as negative during training. ```bash python -m parlai.scripts.train_model -t personachat -m transformer/ranker -mf /tmp/model_tr6 --n-layers 1 --embedding-size 300 --ffn-size 600 --n-heads 4 --num-epochs 2 -veps 0.25 -bs 64 -lr 0.001 --dropout 0.1 --embedding-type fasttext_cc --candidates batch ```     """;Natural Language Processing;https://github.com/joe-prog/https-github.com-facebookresearch-ParlAI
"""1. Download the environment from one of the links below.  You need only select the environment that matches your operating system:     - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip)     - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip)     - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip)     - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip)          (_For Windows users_) Check out [this link](https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64) if you need help with determining if your computer is running a 32-bit version or 64-bit version of the Windows operating system.      (_For AWS_) If you'd like to train the agent on AWS (and have not [enabled a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md))  then please use [this link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux_NoVis.zip) to obtain the ""headless"" version of the environment.  You will **not** be able to watch the agent without enabling a virtual screen  but you will be able to train the agent.  (_To watch the agent  you should follow the instructions to [enable a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md)  and then download the environment for the **Linux** operating system above._)  2. See file `requirements.txt` for python dependencies.    """;Reinforcement Learning;https://github.com/bonniesjli/MADDPG_Tennis
"""cd \models:   cd \blockselector:   """;General;https://github.com/ShaharLutatiPersonal/hyperhypernetworks
"""Hardware at the core: Nvidia GPU   CUDA 7  CUDA 8   """;Computer Vision;https://github.com/jiandai/mlTst
"""If you don't own a GPU remove the --cuda option  although I advise you to get one!   Please first install [Anaconda](https://anaconda.org) and create an Anaconda environment using the environment.yml file.  ``` conda env create -f environment.yml ```  After you create the environment  activate it. ``` source activate hw1 ```  Our current implementation only supports GPU so you need a GPU and need to have CUDA installed on your machine.   """;Computer Vision;https://github.com/cvfx-2019/homework1-color-transfer
"""1. Git clone this repository.  $git clone https://github.com/jacquelinelala/GFN.git  $cd GFN   (If you don't have access to MATLAB  we offer a validation dataset for testing. You can download it from GoogleDrive or Pan Baidu.)  folder = 'your_downloads_directory/GOPRO_Large'; #: You should replace the your_downloads_directory by your GOPRO_Large's directory.   You should accomplish the first two steps in Test on LR-GOPRO Validation before the following steps.   """;Computer Vision;https://github.com/jacquelinelala/GFN
"""    1. Install the dependencies      2. Download the pretrained models here: https://drive.google.com/file/d/0Bx4sNrhhaBr3TDRMMUN3aGtHZzg/view?usp=sharing              Then extract those files into models      3. Run main.py   ![GIF Demo](https://media.giphy.com/media/l378mx3j8ZsWlOuze/giphy.gif)  Live demo: https://www.youtube.com/watch?v=6CeCheBN0Mg    @Author: David Vu   """;Computer Vision;https://github.com/yang-neu/FaceRec
"""```bash $ pip install se3-transformer-pytorch ```   <b>If you had been using any version of SE3 Transformers prior to version 0.6.0  please update. A huge bug has been uncovered by <a href=""https://github.com/MattMcPartlon"">@MattMcPartlon</a>  if you were not using the adjacency sparse neighbors settings and relying on nearest neighbors functionality </b>   You can also have the network automatically derive for you the Nth-degree neighbors with one extra keyword num_adj_degrees. If you would like the system to differentiate between the degree of the neighbors as edge information  further pass in a non-zero adj_dim.   You can use SE3 Transformers autoregressively with just one extra flag   Or you can try deleting the cache directory  which should exist at   You can also designate your own directory where you want the caches to be stored  in the case that the default directory may have permission issues   ```python import torch from se3_transformer_pytorch import SE3Transformer  model = SE3Transformer(     dim = 512      heads = 8      depth = 6      dim_head = 64      num_degrees = 4      valid_radius = 10 )  feats = torch.randn(1  1024  512) coors = torch.randn(1  1024  3) mask  = torch.ones(1  1024).bool()  out = model(feats  coors  mask) #: (1  1024  512) ```  Potential example usage in Alphafold2  as outlined <a href=""https://fabianfuchsml.github.io/alphafold2/"">here</a>  ```python import torch from se3_transformer_pytorch import SE3Transformer  model = SE3Transformer(     dim = 64      depth = 2      input_degrees = 1      num_degrees = 2      output_degrees = 2      reduce_dim_out = True      differentiable_coors = True )  atom_feats = torch.randn(2  32  64) coors = torch.randn(2  32  3) mask  = torch.ones(2  32).bool()  refined_coors = coors + model(atom_feats  coors  mask  return_type = 1) #: (2  32  3) ```  You can also let the base transformer class take care of embedding the type 0 features being passed in. Assuming they are atoms  ```python import torch from se3_transformer_pytorch import SE3Transformer  model = SE3Transformer(     num_tokens = 28        #: 28 unique atoms     dim = 64      depth = 2      input_degrees = 1      num_degrees = 2      output_degrees = 2      reduce_dim_out = True )  atoms = torch.randint(0  28  (2  32)) coors = torch.randn(2  32  3) mask  = torch.ones(2  32).bool()  refined_coors = coors + model(atoms  coors  mask  return_type = 1) #: (2  32  3) ```  If you think the net could further benefit from positional encoding  you can featurize your positions in space and pass it in as follows.  ```python import torch from se3_transformer_pytorch import SE3Transformer  model = SE3Transformer(     dim = 64      depth = 2      input_degrees = 2      num_degrees = 2      output_degrees = 2      reduce_dim_out = True  #: reduce out the final dimension )  atom_feats  = torch.randn(2  32  64  1) #: b x n x d x type0 coors_feats = torch.randn(2  32  64  3) #: b x n x d x type1  #: atom features are type 0  predicted coordinates are type 1 features = {'0': atom_feats  '1': coors_feats} coors = torch.randn(2  32  3) mask  = torch.ones(2  32).bool()  refined_coors = coors + model(features  coors  mask  return_type = 1) #: (2  32  3) - equivariant to input type 1 features and coordinates ```   First install `sidechainnet`  ```bash $ pip install sidechainnet ```  Then run the protein backbone denoising task  ```bash $ python denoise.py ```   """;Computer Vision;https://github.com/lucidrains/se3-transformer-pytorch
"""| Parameter | Default | | --------- | ------- | | Config    | None    |   ***  This project is based on the pixel2style2pixel (pSp). pSp framework generates a series of style vectors based on a novel encoder network  which is fed into a pre-trained style generator to form an extended W + potential space. The encoder can directly reconstruct real input images.   4 Environment   |       Pytorch                                           |                           Paddle                             |   Baidu driver：https://pan.baidu.com/s/1G-Ffs8-y93R0ZlD9mEU6Eg password：m3nb   | Version     | Paddle 2.1.2     |   """;Computer Vision;https://github.com/771979972/Paddle_pSp
"""| Installation     name=""NAM_GALLUP""                                version=f'fold_{fold + 1}')   ```bash $ python main.py -h usage: Neural Additive Models [-h] [--num_epochs NUM_EPOCHS]                               [--learning_rate LEARNING_RATE]                               [--batch_size BATCH_SIZE] --data_path DATA_PATH                               --features_columns FEATURES_COLUMNS                               [FEATURES_COLUMNS ...] --targets_column                               TARGETS_COLUMN [TARGETS_COLUMN ...]                               [--weights_column WEIGHTS_COLUMN]                               [--experiment_name EXPERIMENT_NAME]                               [--regression REGRESSION] [--logdir LOGDIR]                               [--wandb WANDB]                               [--hidden_sizes HIDDEN_SIZES [HIDDEN_SIZES ...]]                               [--activation {exu relu}] [--dropout DROPOUT]                               [--feature_dropout FEATURE_DROPOUT]                               [--decay_rate DECAY_RATE]                               [--l2_regularization L2_REGULARIZATION]                               [--output_regularization OUTPUT_REGULARIZATION]                               [--dataset_name DATASET_NAME] [--seed SEED]                               [--num_basis_functions NUM_BASIS_FUNCTIONS]                               [--units_multiplier UNITS_MULTIPLIER]                               [--shuffle SHUFFLE] [--cross_val CROSS_VAL]                               [--num_folds NUM_FOLDS]                               [--num_splits NUM_SPLITS] [--fold_num FOLD_NUM]                               [--num_models NUM_MODELS]                               [--early_stopping_patience EARLY_STOPPING_PATIENCE]                               [--use_dnn USE_DNN] [--save_top_k SAVE_TOP_K]  optional arguments:   -h  --help            show this help message and exit   --num_epochs NUM_EPOCHS                         The number of epochs to run training for.   --learning_rate LEARNING_RATE                         Hyperparameter: learning rate.   --batch_size BATCH_SIZE                         Hyperparameter: batch size.   --data_path DATA_PATH                         The path for the training data   --features_columns FEATURES_COLUMNS [FEATURES_COLUMNS ...]                         Name of the feature columns in the dataset   --targets_column TARGETS_COLUMN [TARGETS_COLUMN ...]                         Name of the target column in the dataset   --weights_column WEIGHTS_COLUMN                         Name of the weights column in the dataset   --experiment_name EXPERIMENT_NAME                         The name for the experiment   --regression REGRESSION                         Boolean flag indicating whether we are solving a                         regression task or a classification task.   --logdir LOGDIR       Path to dir where to store summaries.   --wandb WANDB         Using wandb for experiments tracking and logging   --hidden_sizes HIDDEN_SIZES [HIDDEN_SIZES ...]                         Feature Neural Net hidden sizes   --activation {exu relu}                         Activation function to used in the hidden layer.                         Possible options: (1) relu  (2) exu   --dropout DROPOUT     Hyperparameter: Dropout rate   --feature_dropout FEATURE_DROPOUT                         Hyperparameter: Prob. with which features are dropped   --decay_rate DECAY_RATE                         Hyperparameter: Optimizer decay rate   --l2_regularization L2_REGULARIZATION                         Hyperparameter: l2 weight decay   --output_regularization OUTPUT_REGULARIZATION                         Hyperparameter: feature reg   --dataset_name DATASET_NAME                         Name of the dataset to load for training.   --seed SEED           seed for torch   --num_basis_functions NUM_BASIS_FUNCTIONS                         Number of basis functions to use in a FeatureNN for a                         real-valued feature.   --units_multiplier UNITS_MULTIPLIER                         Number of basis functions for a categorical feature   --shuffle SHUFFLE     Shuffle the training data   --cross_val CROSS_VAL                         Boolean flag indicating whether to perform cross                         validation or not.   --num_folds NUM_FOLDS                         Number of N folds   --num_splits NUM_SPLITS                         Number of data splits to use   --fold_num FOLD_NUM   Index of the fold to be used   --num_models NUM_MODELS                         the number of models to train.   --early_stopping_patience EARLY_STOPPING_PATIENCE                         Early stopping epochs   --use_dnn USE_DNN     Deep NN baseline.   --save_top_k SAVE_TOP_K                         Indicates the maximum number of recent checkpoint                         files to keep. ```    """;General;https://github.com/AmrMKayid/nam
"""First of all  install the MXNet 2 release such as MXNet 2 Alpha. You may use the following commands:  ```bash #: Install the version with CUDA 10.2 python3 -m pip install -U --pre ""mxnet-cu102>=2.0.0a""  #: Install the version with CUDA 11 python3 -m pip install -U --pre ""mxnet-cu110>=2.0.0a""  #: Install the cpu-only version python3 -m pip install -U --pre ""mxnet>=2.0.0a"" ```   To install GluonNLP  use  ```bash python3 -m pip install -U -e .  #: Also  you may install all the extra requirements via python3 -m pip install -U -e .""[extras]"" ```  If you find that you do not have the permission  you can also install to the user folder:  ```bash python3 -m pip install -U -e . --user ```  For Windows users  we recommend to use the [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/about).    : Also  you can use python -m to access the toolkits   You can use Docker to launch a JupyterLab development environment with GluonNLP installed.  ``` #: GPU Instance docker pull gluonai/gluon-nlp:gpu-latest docker run --gpus all --rm -it -p 8888:8888 -p 8787:8787 -p 8786:8786 --shm-size=2g gluonai/gluon-nlp:gpu-latest  #: CPU Instance docker pull gluonai/gluon-nlp:cpu-latest docker run --rm -it -p 8888:8888 -p 8787:8787 -p 8786:8786 --shm-size=2g gluonai/gluon-nlp:cpu-latest ```   For more details  you can refer to the guidance in [tools/docker](tools/docker).  """;Sequential;https://github.com/dmlc/gluon-nlp
""" Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   Create tfrecord  clone this repo  then   * Clone this repo   !git clone https://github.com/skyflynil/stylegan2.git  %cd stylegan2   create your dataset for train  !mkdir dataset   * https://github.com/NVlabs/stylegan2  * https://github.com/akanimax/msg-stylegan-tf   """;Computer Vision;https://github.com/xx88xx/stylegan2ky
"""Clone the master branch of the respository using git clone -b master --single-branch https://github.com/richzhang/colorization.git   The provided scripts run representation learning tests. Note that the scripts run on release models. Modify scripts accordingly if you want to test your own trained model.   We also include demo usage as an iPython notebook  under [`./demo/colorization_demo_v2.ipynb`](https://github.com/richzhang/colorization/blob/master/demo/colorization_demo_v2.ipynb). This IPython Notebook demonstrates how to use our colorization network to colorize a grayscale image. To run this  after cloning the directory  `cd` into the `demo` directory  run `ipython notebook` and open `colorization_demo_v2.ipynb` in your web browser.   The following contains instructions for training a colorization network from scratch. After cloning the repository  from the root directory:  (1) Run `./train/fetch_init_model.sh`. This will load model `./models/init_v2.caffemodel`. This model was obtained using the k-means initialization implemented in [Kraehenbuehl et al  ICLR 2016](https://github.com/philkr/magic_init).  (2) Run `./train/fetch_caffe.sh`. This will load a modified Caffe into directory `./caffe-colorization`. For guidelines and help with installation of Caffe  consult the [installation guide](http://caffe.berkeleyvision.org/) and [Caffe users group](https://groups.google.com/forum/#!forum/caffe-users).  (3) Add the `./resources/` directory (as an absolute path) to your system environment variable $PYTHONPATH. This directory contains custom Python layers.  (4) Modify paths in data layers `./models/colorization_train_val_v2.prototxt` to locate where ImageNet LMDB files are on your machine. These should be BGR images  non-mean centered  in [0 255].  (5) Run `./train/train_model.sh [GPU_ID]`  where `[GPU_ID]` is the gpu you choose to specify. Notes about training:  (a) Training completes around 450k iterations. Training is done on mirrored and randomly cropped 176x176 resolution images  with mini-batch size 40.  (b) Snapshots every 1000 iterations will be saved in `./train/models/colornet_iter_[ITERNUMBER].caffemodel` and `./train/models/colornet_iter_[ITERNUMBER].solverstate`.  (c) If training is interupted  resume training by running `./train/train_resume.sh ./train/models/colornet_iter_[ITERNUMBER].solverstate [GPU_ID]`  where `[ITERNUMBER]` is the last snapshotted model.  (d) Check validation loss by running `./val_model.sh ./train/models/colornet_iter_[ITERNUMBER].caffemodel [GPU_ID] 1000`  where [ITERNUMBER] is the model you would like to validate. This runs the first 10k imagenet validation images at full 256x256 resolution through the model. Validation loss on `colorization_release_v2.caffemodel` is 7715.  (e) Check model outputs by running the IPython notebook demo. Replace the release model with your snapshotted model.  (f) To download reference pre-trained model  run `./models/fetch_release_models.sh`. This will load reference model `./models/colorization_release_v2.caffemodel`. This model used to generate results in the [ECCV 2016 camera ready](arxiv.org/pdf/1603.08511.pdf).  For completeness  this will also load model `./models/colorization_release_v2_norebal.caffemodel`  which is was trained without class rebalancing. This model will provide duller but ""safer"" colorizations. This will also load model `./models/colorization_release_v1.caffemodel`  which was used to generate the results in the [arXiv v1](arxiv.org/pdf/1603.08511v1.pdf) paper.   """;General;https://github.com/SharmaAjay19/VideoImageColorization
"""To install from pip:   pip install xtcocotools   | COCO [6]         | 200K   | 17    |   ✔️  |    ✔️     |          |    *     |    ✔️     |          |          | 250K  |   | COCO-WholeBody   | 200K   | 133   |   ✔️  |   ✔️      |    ✔️     |    ✔️     |    ✔️     |    ✔️     |    ✔️     | 250K  |   1. COCO-WholeBody dataset is **ONLY** for research and non-commercial use.  2. The annotations of COCO-WholeBody dataset belong to [SenseTime Research](https://www.sensetime.com)  and are licensed under a [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by-nc/4.0/legalcode).  3. We do not own the copyright of the images. Use of the images must abide by the [Flickr Terms of Use](https://www.flickr.com/creativecommons/). The users of the images accept full responsibility for the use of the dataset  including but not limited to the use of any copies of copyrighted images that they may create from the dataset.   """;Computer Vision;https://github.com/jin-s13/COCO-WholeBody
"""If you'd rather checkout and build the model locally you can follow the run locally steps below.  You can deploy the model-serving microservice on Red Hat OpenShift by following the instructions for the OpenShift web console or the OpenShift Container Platform CLI in this tutorial  specifying codait/max-question-answering as the image name.   On your Kubernetes cluster  run the following commands:   Clone this repository locally. In a terminal  run the following command:  $ git clone https://github.com/IBM/MAX-Question-Answering.git   $ cd MAX-Question-Answering   The API server automatically generates an interactive Swagger documentation page. Go to `http://localhost:5000` to load it. From there you can explore the API and also create test requests.  Use the `model/predict` endpoint to upload a test json file (you can use/alter the files from the `samples` folder) and get answers to the questions from the API.  Sample input: ```json {   ""paragraphs"": [     {       ""context"": ""John lives in Brussels and works for the EU""        ""questions"": [         ""Where does John Live?""          ""What does John do?""          ""What is his name?""       ]     }      {       ""context"": ""Jane lives in Paris and works for the UN""        ""questions"": [         ""Where does Jane Live?""          ""What does Jane do?""       ]     }   ] } ```  ![Example of getting answers from the API](docs/swagger-screenshot.png)  You can also test it on the command line  for example:  ```shell $ curl -X POST ""http://localhost:5000/model/predict"" -H ""accept: application/json"" -H ""Content-Type: application/json"" -d ""{\""paragraphs\"": [{ \""context\"": \""John lives in Brussels and works for the EU\""  \""questions\"": [\""Where does John Live?\"" \""What does John do?\"" \""What is his name?\"" ]} { \""context\"": \""Jane lives in Paris and works for the UN\""  \""questions\"": [\""Where does Jane Live?\"" \""What does Jane do?\"" ]}]}"" ```  You should see a JSON response like that below:  ```json {   ""status"": ""ok""    ""predictions"": [     [       ""Brussels""        ""works for the EU""        ""John""     ]      [       ""Paris""        ""works for the UN""     ]   ] } ```   """;Natural Language Processing;https://github.com/IBM/MAX-Question-Answering
"""code  : https://github.com/ifzhang/FairMOT   Our demo code can do Action recognition  Tracking  and Re-identification. Recognizable actions can be classified as walk  carry cargo  telephone  and stand.  **Our full code will be released on December 29th.**   <div align=""center"">   <img src=""tracking_action_trcognition.gif"" width=""800px""/> </div>   """;Computer Vision;https://github.com/seominseok0429/vidoe_tracking-Reidentification-Action_recognition_Demo
"""At this time  you need to download the pretrained models manually.  Since this project is based on pytorch  so you need the pytorch version of the BERT.    pytorch-pretrained-bert has changed name to pytorch-transformers. here is the github:   """;Natural Language Processing;https://github.com/darr/nerbert
"""git clone https://github.com/mkavim/finetune_bert.git  cd finetune_bert  pip install .   https://github.com/bojone/bert4keras     https://github.com/CyberZHG/keras-bert     https://github.com/huggingface/transformers     https://github.com/amir-abdi/keras_to_tensorflow      """;Natural Language Processing;https://github.com/mkavim/finetune_bert
"""  Ensure that [python>=3.6](https://www.python.org/)   [torch>=1.6.0](https://pytorch.org/)  torchvision>=0.7.0 is installed .    ```bash    $ git clone https://github.com/benihime91/pytorch_retinanet.git    $ cd pytorch_retinanet    $ pip install -r requirements.txt    ```    Note: for `pytorch-lightning` versions >= 1.0.0 t training will fail .      set dataset.root_dir = {path to the coco dataset}      Note:    df = pd.read_csv(path)   - <a href=""https://colab.research.google.com/github/benihime91/pytorch_retinanet/blob/master/demo.ipynb"" target=""_parent""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/></a>  [demo.ipynb](https://github.com/benihime91/pytorch_retinanet/blob/master/demo.ipynb)      Clone the Github Repo    ```bash    $ git clone https://github.com/benihime91/pytorch_retinanet.git    ```     For easy training pipeline  we recommend using **`pytorch-lightning`** for **training** and **testing**.              First of all open the **`hparams.yaml`** file and modify it according to need. Instructions to     modeify the same are present inside the file.              Create a python script inside the `retinanet repo`. Name it whatever you want and then insert the     following lines:    ```python     from omegaconf import OmegaConf  DictConfig     import pytorch_lightning as pl     from pytorch_lightning import Trainer      from model import RetinaNetModel          #: load in the hparams yaml file     hparams = OmegaConf.load(""hparams.yaml"")      #: instantiate lightning module     model = RetinaNetModel(hparams)          #: Instantiate Trainer     trainer = Trainer()     #: start train     trainer.fit(model)     #: to test model using COCO API     trainer.test(model)    ```   """;Computer Vision;https://github.com/benihime91/pytorch_retinanet
"""You should prepare lmdb dataset   """;Computer Vision;https://github.com/avakong/stylegan-test
"""You can install this package from PyPI:  ```sh pip install mlpmixer-flax ```  Or directly from GitHub:  ```sh pip install --upgrade git+https://github.com/SauravMaheshkar/MLP-Mixer.git ```   conda env create --name &lt;env-name&gt; sauravmaheshkar/mlpmixer  conda activate &lt;env-name&gt;   ```python from mlpmixer_flax.config import mixer_b16_config from mlpmixer_flax.dataloader import get_dataset_info from mlpmixer_flax.models import MlpMixer  dataset = ""cifar10"" num_classes = get_dataset_info(dataset  ""train"")[""num_classes""] model = MlpMixer(num_classes=num_classes  **mixer_b16_config) ```  The easiest way to get started would be to try out the [FineTuning Example Notebook](https://github.com/SauravMaheshkar/MLP-Mixer/blob/main/examples/FineTuning_Example.ipynb).   """;Computer Vision;https://github.com/SauravMaheshkar/MLP-Mixer
"""One of the most important applications of seismic reflection is the hydrocarbon exploration which is closely related to salt deposits analysis. This problem is very important even nowadays due to it’s non-linear nature. Taking into account the recent developments in deep learning networks [TGS-NOPEC Geophysical Company](https://www.tgs.com/) hosted the Kaggle [competition](https://www.kaggle.com/c/tgs-salt-identification-challenge) for salt deposits segmentation problem in seismic image data. In this paper  we demonstrate the great performance of several novel deep learning techniques merged into a single neural network. Using a [U-Net](https://arxiv.org/abs/1505.04597) with [ResNeXt-50](https://arxiv.org/abs/1611.05431) encoder pretrained on [ImageNet](http://www.image-net.org/) as our base architecture  we implemented [Spatial-Channel Squeeze & Excitation](https://arxiv.org/abs/1803.02579)  [Lovasz loss](https://github.com/bermanmaxim/LovaszSoftmax)  analog of [CoordConv](https://eng.uber.com/coordconv/) and [Hypercolumn](https://arxiv.org/abs/1411.5752) methods.  This architecture was a part of the [solutiuon](https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/69274) (27th out of 3234 teams top 1%) in the [TGS Salt Identification Challenge](https://www.kaggle.com/c/tgs-salt-identification-challenge).   """;Computer Vision;https://github.com/K-Mike/Automatic-salt-deposits-segmentation
"""[ ] Create one self-contained notebook   """;General;https://github.com/jackbandy/deep_learning_ulmfit
"""동빈나 https://www.youtube.com/watch?v=FPcdxHCxH_o   """;Computer Vision;https://github.com/seongilp/DFE604-2020F-FinalProject
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/ahhan02/darknet-alex
"""Once you have a working version of Moses  edit the MOSES_PATH variable inside the PBSMT/run.sh script to indicate the location of Moses directory. Then  simply run:  cd PBSMT   Install tools   Download pretrained word embeddings   """;Natural Language Processing;https://github.com/facebookresearch/UnsupervisedMT
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   3. Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (**Note:** To disable Loss-Window use flag `-dont_show`. If you are using CPU  try `darknet_no_gpu.exe` instead of `darknet.exe`.)   Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/tops666/volo-v4-
"""This repository contains the original modified residual network which modified the classical resnet ""Deep Residual Learning for Image Recognition"" (http://arxiv.org/abs/1512.03385). Original residual block was modified to improve model performance.    """;Computer Vision;https://github.com/xinkuansong/modified-resnet-acc-0.9638-10.7M-parameters
""" `params['tune_postfix'] = '_tune_guf'` a postfix to add for checkpoint saving and output files.  `params['gradual_unfreezing'] = True` use gradual unfreezing while tuning.  ###d- Use a checkpoint for Testing  """;Natural Language Processing;https://github.com/amagooda/SummaRuNNer_coattention
"""This is a PyTorch implementation of [Data-Driven Neuron Allocation for Scale Aggregation Networks](https://arxiv.org/pdf/1904.09460.pdf).(CVPR2019) with pretrained models.    GPU time   from pytorch.scalenet import *     <img src=""https://github.com/Eli-YiLi/ScaleNet/blob/master/figures/coco.png"" width=""300"">   """;Computer Vision;https://github.com/Eli-YiLi/ScaleNet
"""repo on GitHub.   my script that numerically checks PyTorch gradients.  My convergence issues were due to a critical PyTorch bug   this repository  make sure your PyTorch version   I haven't tested this as thoroughly  you should make sure   I also tried training a net with ADAM and found that it didn't converge as well with the default hyper-parameters compared to SGD with a reasonable learning rate schedule.  ![](images/adam-loss-error.png)   I think there are ways to improve the memory utilization in this code as in the [the official space-efficient Torch implementation](https://github.com/gaohuang/DenseNet_lite). I also would be interested in multi-GPU support.   """;Computer Vision;https://github.com/bamos/densenet.pytorch
"""For doing this we used a library created by an other competitor  that can be found in the read_test_files folder   python        """;Computer Vision;https://github.com/Stamatis-Ilias/PLAsTiCC-Astronomical-Classification
""" ```bash git clone https://github.com/JCBrouwer/maua-stylegan2 cd maua-stylegan2 pip install -r requirements.txt ```  Alternatively  check out this [Colab Notebook](https://colab.research.google.com/drive/1Ig1EXfmBC01qik11Q32P0ZffFtNipiBR)   or (in e.g. a jupyter notebook):   """;Computer Vision;https://github.com/JCBrouwer/maua-stylegan2
"""Current Version : 2.1.0.0   Exculte ``` python ***.py --help ``` to get help when you use ***.py    1.Download pretrain model([buriburisuri model](https://drive.google.com/drive/folders/1HTxjhPnSqVpMkZS732pu3KOSJXZy8waf?usp=sharing)) and extract to 'release' directory  2.Execute <pre><code> python demo.py -input_path <wave_file path> </code></pre> to transform a speech wave file to the English sentence. The result will be printed on the console.   For example  try the following command. <pre><code> python demo.py -input_path=data/demo.wav -ckpt_dir=release/buriburisuri </code></pre>  The result will be as follows: <pre><code> please scool stella </code></pre>  The ground truth is as follows: <pre><code> PLEASE SCOOL STELLA </code></pre>  As mentioned earlier  there is no language model  so there are some cases where capital letters  punctuations  and words are misspelled.   """;Audio;https://github.com/kingstarcraft/speech-to-text-wavenet2
"""- シビックテックに新しく興味をもってくれた人に、各地ブリゲードの特徴をわかりやすく伝えたい。 - 既にシビックテック活動をしている人にも、他の地域のブリゲードの特徴がわかるようにしたい。 ![ブリゲードマッピングのきっかけ](img/brigade_mapping_trigger.png) - そこで、各地のブリゲードの性格や得意分野がわかるような「俯瞰図」「得意分野マップ」みたいなものを作れば、どのブリゲードと相性が良いかわかるようになるのでは?   - GitHub Pages     - [https://siramatu.github.io/brigade-visualizer/](https://siramatu.github.io/brigade-visualizer/) - Embedding Projector     - [https://projector.tensorflow.org/?config=https://raw.githubusercontent.com/siramatu/brigade-visualizer/master/embedding_projector_config.json](https://projector.tensorflow.org/?config=https://raw.githubusercontent.com/siramatu/brigade-visualizer/master/embedding_projector_config.json)   ```bash #: レポジトリをクローンする git clone https://github.com/siramatu/brigade-visualizer.git cd brigade-visualizer ```  """;Natural Language Processing;https://github.com/siramatu/brigade-visualizer
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/Shraddha2013/mydarknetyolo
"""If you prefer to construct the conda environment manually  please follow the commands speficiend in `manual_installation.txt`   conda env create -f conda_requirements.txt  This will create the conda environment ckconv with the correct dependencies.  The same conda environment can be created with pip by running:  conda create -n ckconv python=3.7  conda activate ckconv  conda install pytorch==1.7.0 torchvision==0.8.1 torchaudio=0.7.0 cudatoolkit=10.1 -c pytorch  pip install -r requirements.txt   """;Computer Vision;https://github.com/dwromero/ckconv
"""* Clone the repository * Create a directory for your experiments  logs and model weights: `mkdir output` * Download GloVE word vectors: https://nlp.stanford.edu/projects/glove/ * Modify the `config.py` file to set up the paths where your GloVE  SquAD and models will be located * Create a Python virtual environment  source to it: `mkvirualenv qa-env ; workon qa-env` if you use virtualenvwrapper * Install the dependencies: `pip install -r requirements.txt ; python -m spacy download en` * Run `python make_dataset.py` to download SquAD dataset and pre-process the data * Run `python train.py` to train the model with hyper-parameters found in `config.py` * Run `python test.py` to test the model EM and F1 scores on Dev examples * Play with `eval.py` to answer your own questions! :)   ├── requirements.txt   &lt;- Required Python libraries to build the project   PyTorch pretrained BERT: https://github.com/huggingface/pytorch-pretrained-BERT   """;Natural Language Processing;https://github.com/ElizaLo/Question-Answering-based-on-SQuAD
"""Add SE-like models. (done)      check model files under the fig/nn floder.     ```python from lib.nn.OCtaveResnet import resnet50 from lib.nn.res2net import se_resnet50 from lib.nn.AdaptiveConvResnet import PixelAwareResnet50  DataSetAwareResnet50  model = resnet50().cuda() model = se_resnet50().cuda() model = PixelAwareResnet50().cuda() model = DataSetAwareResnet50().cuda()  ```  """;Computer Vision;https://github.com/lxtGH/OctaveConv_pytorch
"""    1. Install the dependencies      2. Download the pretrained models here: https://drive.google.com/file/d/0Bx4sNrhhaBr3TDRMMUN3aGtHZzg/view?usp=sharing              Then extract those files into models      3. Run run_script.py   """;Computer Vision;https://github.com/Mariya1285/Codeathon
"""| provenance | https://github.com/onnx/models/tree/master/models/image_classification/mobilenet |    """;General;https://github.com/modelhub-ai/mobilenet
"""> * The main results on these UCI datasets are summarized below. > * ARM-Net achieves overall best performance. > * More results and technical details can be found [here](https://github.com/nusdbsystem/ARM-Net/tree/uci#main-results-evaluated-on-first-36121-datasets-updating). >   | Model |  Rank(Best_Cnt)  | abalone|  acute-inflammation|  acute-nephritis|  adult|  annealing|  arrhythmia|  audiology-std|  balance-scale|  balloons|  bank|  blood|  breast-cancer|  breast-cancer-wisc|  breast-cancer-wisc-diag|  breast-cancer-wisc-prog|  breast-tissue|  car|  cardiotocography-10clases|  cardiotocography-3clases|  chess-krvk|  chess-krvkp|  congressional-voting|  conn-bench-sonar-mines-rocks|  conn-bench-vowel-deterding|  connect-4|  contrac|  credit-approval|  cylinder-bands|  dermatology|  echocardiogram|  ecoli|  energy-y1|  energy-y2|  fertility|  flags|  glass| |:-----------:|:-----------:|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------| | `n_samples` | - | 4177|  120|  120|  48842|  898|  452|  196|  625|  16|  4521|  748|  286|  699|  569|  198|  106|  1728|  2126|  2126|  28056|  3196|  435|  208|  990|  67557|  1473|  690|  512|  366|  131|  336|  768|  768|  100|  194|  214| | `n_features` | - | 9|  7|  7|  15|  32|  263|  60|  5|  5|  17|  5|  10|  10|  31|  34|  10|  7|  22|  22|  7|  37|  17|  61|  12|  43|  10|  16|  36|  35|  11|  8|  9|  9|  10|  29|  10| | LR | 6-th (0/36) | 0.6293/0.0080|  0.9833/0.0211|  0.9533/0.0552|  0.8423/0.0008|  0.1280/0.0172|  0.5442/0.0184|  0.7040/0.0480|  0.8718/0.0310|  0.7250/0.0935|  0.8904/0.0023|  0.7610/0.0043|  0.6923/0.0171|  0.9490/0.0090|  0.9641/0.0103|  0.6626/0.0656|  0.5283/0.1371|  0.8032/0.0052|  0.7595/0.0118|  0.8798/0.0120|  0.2743/0.0009|  0.9438/0.0035|  0.5705/0.0328|  0.7385/0.0186|  0.7121/0.0088|  0.7547/0.0004|  0.4829/0.0383|  0.8557/0.0119|  0.6305/0.0647|  0.9399/0.0313|  0.7600/0.0605|  0.7988/0.0510|  0.8391/0.0123|  0.8448/0.0297|  0.5800/0.1066|  0.4206/0.0365|  0.5290/0.0281| | FM | 5-th (3/36) | 0.6329/0.0067|  0.9767/0.0389|  0.8700/0.0945|  0.8443/0.0005|  0.1960/0.1493|  0.5283/0.0211|  0.4880/0.0588|  `0.9224/0.0087`|  0.5750/0.1275|  0.8882/0.0028|  0.7647/0.0000|  0.6909/0.0604|  0.9599/0.0048|  `0.9697/0.0048`|  0.6626/0.0849|  0.5094/0.0818|  0.8882/0.0097|  0.7616/0.0161|  0.8903/0.0172|  0.3127/0.0035|  0.9796/0.0038|  0.5705/0.0306|  `0.9502/0.0087`|  0.9502/0.0087|  0.8264/0.0005|  0.4524/0.0140|  0.8638/0.0093|  0.7016/0.0250|  0.9202/0.0350|  0.7846/0.0600|  0.7595/0.0680|  0.8823/0.0086|  0.8604/0.0283|  0.7720/0.0688|  0.3423/0.0200|  0.5907/0.0361| | DNN | 4-th (6/36) |0.6560/0.0051|  0.9900/0.0200|  0.9500/0.0316|  0.8519/0.0015|  0.4420/0.2346|  0.6442/0.0114|  0.6880/0.0466|  0.8987/0.0048|  0.5500/0.2318|  0.8900/0.0035|  0.7583/0.0050|  0.7147/0.0082|  0.9633/0.0033|  0.9648/0.0107|  0.7091/0.0475|  0.5849/0.0396|  0.9442/0.0034|  0.7797/0.0121|  0.9178/0.0031|  0.6842/0.0147|  0.9775/0.0032|  0.5834/0.0147|  0.7481/0.0377|  `0.9745/0.0063`|  0.8501/0.0023|  0.5084/0.0158|  0.8417/0.0187|  `0.7359/0.0386`|  `0.9639/0.0101`|  0.7846/0.0337|  `0.8524/0.0166`|  0.8688/0.0107|  `0.8865/0.0094`|  0.8320/0.0722|  `0.4969/0.0272`|  0.5850/0.0316| | SNN | 3rd (6/36) |0.6457/0.0043|  0.9567/0.0389|  0.9000/0.0548|  0.8489/0.0009|  0.2280/0.2671|  0.5841/0.0410|  `0.7200/0.0253`|  0.9058/0.0240|  0.7250/0.1225|  0.8885/0.0019|  0.8885/0.0019|  0.7105/0.0105|  `0.9656/0.0041`|  0.9690/0.0112|  0.6727/0.0903|  `0.6000/0.0690`|  `0.9632/0.0066`|  `0.8008/0.0125`|  0.9029/0.0086|  0.6796/0.0141|  0.9726/0.0061|  0.5779/0.0209|  0.7135/0.0300|  0.9693/0.0100|  0.8491/0.0013|  0.5106/0.0098|  `0.8719/0.0121`|  0.7000/0.0163|  0.9388/0.0269|  0.7877/0.0439|  0.8179/0.035|  0.8714/0.0142|  0.8854/0.0154|  0.7600/0.1180|  0.4804/0.0231|  0.5738/0.0602| | Perceiver-IO | 2nd (6/36) |0.6381/0.0143|  `1.0000/0.0000`|  0.9367/0.0531|  0.8521/0.0011|  `0.7600/0.0000`|  0.5602/0.0053|  0.0080/0.0160|  0.8821/0.0166|  `0.7750/0.0500`|  0.8850/0.0000|  0.7620/0.0000|  0.7063/0.0088|  0.9352/0.0313|  0.9556/0.0142|  `0.7596/0.0118`|  0.3208/0.0597|  0.9326/0.0120|  0.5325/0.0861|  0.7817/0.0035|  0.6834/0.0151|  0.8106/0.0895|  `0.6129/0.0000`|  0.5635/0.0817|  0.6732/0.0521|  0.7538/0.0000|  0.4457/0.0122|  0.7745/0.1075|  0.6133/0.0078|  0.4295/0.0754|  0.7662/0.0834|  0.6440/0.0239|  0.8417/0.0295|  0.8807/0.0325|  `0.8560/0.0480`|  0.3010/0.0247|  0.4093/0.0415| | `ARM-Net` | `1st (15/36)` |`0.6603/0.0034`|  0.9767/0.0389|  `0.9600/0.0800`|  `0.8562/0.0011`|  0.1500/0.1131|  `0.6487/0.0214`|  0.5520/0.0299|  0.9135/0.0070|  0.7500/0.0791|  `0.8922/0.0012`|  `0.8922/0.0012`|  `0.7203/0.0193`|  0.9530/0.0118|  0.9521/0.0186|  0.6828/0.0485|  0.5170/0.0638|  0.9463/0.0086|  0.7868/0.0054|  `0.9146/0.0051`|  `0.6982/0.0109`|  `0.9826/0.0040`|  0.5760/0.0193|  0.7712/0.0335|  0.9675/0.0115|  `0.8672/0.0028`|  `0.5228/0.0119`|  0.8620/0.0187|  0.7133/0.0305|  0.9497/0.0181|  `0.8338/0.0406`|  0.8214/0.0279|  `0.8844/0.0048`|  0.8750/0.0304|  0.8240/0.0528|  0.4330/0.0526|  `0.6150/0.0232`|    > * The main results on these large benchmark datasets are summarized below. > * ARM-Net achieves the overall best performance. > * More results and technical details can be found in the [paper](https://dl.acm.org/doi/10.1145/3448016.3457321). > * Note that all the results are reported with a *fixed embedding size* of **10** for a fair comparison  and higher AUC can be obtained by increasing the embedding size.   ```sh E.g.  with a larger embedding size of 100  ARM-Net (single head  without ensemble with a DNN) can obtain 0.9817 AUC on Frappe with only 10 exponential neurons.  CUDA_VISIBLE_DEVICES=0 python train.py --model armnet_1h --nemb 100 --h  10 --alpha 1.7 --lr 0.001 --exp_name frappe_armnet_1h_nemb --repeat 5  AUC and Model Size of this ARM-Net of different embedding sizes are compared below.  ``` | Embedding Size | 10  | 20  | 30  | 40  |  50  | 60  | 70  | 80  | 90  |  **100**   | 110  | 120  | |:--------------:|:---:|:---:|:---:|:---:|:----:|:---:|:---:|:---:|:---:|:----------:|:---:|:---:| |      AUC       | 0.9777  | 0.9779  | 0.9801  | 0.9803  | 0.9798  | 0.9807  | 0.9808  | 0.9810  | 0.9810  | **0.9817** | 0.9811  | 0.9805  | |   Model Size   | 177K  | 262K  | 348K  | 434K  | 520K  | 606K  | 692K  | 779K  | 866K  |  **953K**  | 1.04M |  1.13M  |    <img src=""https://user-images.githubusercontent.com/14588544/139670215-77544a4b-5bec-4ede-9b58-1ac1a24ff4cd.png"" width=""660"" />    """;General;https://github.com/nusdbsystem/ARM-Net
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`  2. Download VOC2007 and VOC2012 dataset. By default  we assume the data is stored in `$HOME/data/`   ```Shell   #: Download the data.   cd $HOME/data   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar   #: Extract the data.   tar -xvf VOCtrainval_11-May-2012.tar   tar -xvf VOCtrainval_06-Nov-2007.tar   tar -xvf VOCtest_06-Nov-2007.tar   ```  3. Create the LMDB file.   ```Shell   cd $CAFFE_ROOT   #: Create the trainval.txt  test.txt  and test_name_size.txt in data/VOC0712/   ./data/VOC0712/create_list.sh   #: You can modify the parameters in create_data.sh if needed.   #: It will create lmdb files for trainval and test with encoded original image:   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb   #: and make soft links at examples/VOC0712/   ./data/VOC0712/create_data.sh   ```   1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`   ```Shell   git clone https://github.com/weiliu89/caffe.git   cd caffe   git checkout ssd   ```  2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.   ```Shell   #: Modify Makefile.config according to your Caffe installation.   cp Makefile.config.example Makefile.config   make -j8   #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.   make py   make test -j8   #: (Optional)   make runtest -j8   ```   COCO<sup>[1]</sup>: SSD300*  SSD512*  07+12+COCO: SSD300*  SSD512*  07++12+COCO: SSD300*  SSD512*  COCO models:   """;Computer Vision;https://github.com/CV-deeplearning/caffe
"""```bash make ```  git clone https://github.com/sigmorphon2020/task0-data.git   """;General;https://github.com/AssafSinger94/sigmorphon-2020-inflection
"""``` git clone https://github.com/Tony-Y/cgnn.git CGNN_HOME=`pwd`/cgnn ```   You can create a configuration file (config.json) using the one-hot encoding as follows:   The user guide in [this GitHub Pages site](https://Tony-Y.github.io/cgnn/) provides the complete explanation of the CGNN architectures  and the description of program options. Usage examples are contained in the directory `cgnn/examples`.   """;Graphs;https://github.com/Tony-Y/cgnn
"""Code used: Tensorflow 1.9 for GPU along with the requisite CUDA and CuDNN libraries on a Win10 machine with Ryzen 1600     """;Computer Vision;https://github.com/varunvprabhu/simple_pose_hourglass
"""3.  The sub-version use cudnn is <a href=""https://github.com/TanDongXu/CUDA-MCDNN"">tdx</a>     You can compile the code on windows or linux.      windows: X:/Program Files (x86) /NVIDIA Corporation/CUDA Samples/v6.5/common/inc (For include file ""helper_cuda""); X:/Program Files/opencv/vs2010/install/include (Depend on situation)   windows: X:/Program Files/opencv/vs2010/install/x86/cv10/lib (Depend on situation)     GPU compute   CMake for Linux  sudo apt-get install cmake libopencv-dev   mkdir build    cd build     make -j16    cd ../mnist/    sh get_mnist.sh     sh get_cifar10.sh    cd ../    ./build/CUDA-CNN 1 1     Install vs2010.  Download and install <a href=""http://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.0.0-beta/""> opencv-2.4</a> or other higher versions  Download and install <a href=""https://developer.nvidia.com/cuda-downloads""> cuda-5.0</a> or other higher versions   Install opencv and cuda  Start the nsight from cuda   Project->Proerties for add-> Build-> Settings->CUDA->Device linker mode: separate compilation      Project->Proerties for add-> Build-> Settings->CUDA->Generate GPU code 2.0  Project->Proerties for add-> Build-> Settings->Tool Settings->NVCC Compiler->includes: +/usr/local/cuda/samples/common/inc/; + opencv sdk include path ;      1. <a href=""https://github.com/zhxfl/CUDA-CNN/blob/master/Config/Cifar10Config.txt"">CIFAR10</a>   2. <a href=""https://github.com/zhxfl/CUDA-CNN/blob/master/Config/MnistConfig.txt"">MNIST</a>      """;Computer Vision;https://github.com/zhxfl/CUDA-CNN
"""You can easily test the output masks on your images via the CLI.   **Note : Use Python 3**  """;Computer Vision;https://github.com/HanYong201308/Pytorch-UNet2
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/ivychill/insightface
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/shaikhzhas/bert
"""Architecture of our complementary segmentation network  the optimal CompNet. The dense blocks (DB)  corresponding to the gray bars  are used in each encoder and decoder. The triple (x y z) in each dense block indicates that it has x convolutional layers with a kernel size 3×3; each layer has y filters  except for the last one that has z filters. SO: segmentation output for the brain mask; CO: complementary segmentation output for the non-brain mask; RO: reconstruction output for the input image. These three outputs produced by the Sigmoid function are the final predictions; while all other Sigmoids produce intermediate outputs  except for the green one that is the concatenation of the summation from each intermediate layers. Best viewed in color.  *ROI and CO branches -  We take the downsampling branch of a U-Net as it is  however we split the upsampling branch into two halves  one to obtain the Region of Interest and the other for Complementary aka non region of interest. Losses here are negative dice for ROI and positive dice for Non-ROI region.*  *Reconstruction Branch -  Next we merge these two ROI and non ROI outputs using ""Summation"" operation and then pass it into another U-Net  This U-Net is the reconstruction branch. The input is the summed image from previous step and the output is the ""original"" image that we start with. The loss of reconstruction branch is MSE.*  ``` The code in this repository provides only the stand alone code for this architecture. You may implement it as is  or convert it into modular structure if you so wish. The dataset of OASIS can obtained from the link above and the preprocessiong steps involved are mentioned in the paper.  You have to provide the inputs. ```   email me - rd31879@uga.edu for any questions !! Am happy to discuss    Python - Python-2    Numpy - Numpy  Sklearn - Scipy/Sklearn/Scikit-learn  CUDA - CUDA-8  CUDNN - CUDNN-5 You have to register to get access to CUDNN   """;Computer Vision;https://github.com/raun1/MICCAI2018---Complementary_Segmentation_Network-Raw-Code
"""Update: 07.11.2020.   """;General;https://github.com/scrayish/ML_NLP
"""```bash $ bash download.sh CelebA or $ bash download.sh LSUN ```    $ git clone https://github.com/heykeetae/Self-Attention-GAN.git  $ cd Self-Attention-GAN   $ cd samples/sagan_celeb   $ cd samples/sagan_lsun   """;Computer Vision;https://github.com/yanqi1811/self-attention
"""- Install PyTorch and dependencies from http://pytorch.org - Install Torch vision from the source. ```bash git clone https://github.com/pytorch/vision cd vision python setup.py install ``` - Install python libraries [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate). ```bash pip install visdom pip install dominate ``` - Clone this repo: ```bash git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix cd pytorch-CycleGAN-and-pix2pix ```   <a href=""https://github.com/yunjey/mnist-svhn-transfer"">[Minimal PyTorch]</a> (by yunjey)   <a href=""https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN"">[Mxnet]</a> (by Ldpe2G)    <a href=""https://github.com/taey16/pix2pixBEGAN.pytorch"">[Pytorch]</a> (by taey16)   - Test the model (`bash ./scripts/test_pix2pix.sh`):bash   More example scripts can be found at scripts directory.  You can download a pretrained model (e.g. horse2zebra) with the following script:  bash pretrained_models/download_cyclegan_model.sh horse2zebra   bash ./datasets/download_cyclegan_dataset.sh horse2zebra   ``` bash   bash pretrained_models/download_pix2pix_model.sh facades_label2photo   CPU/GPU (default --gpu_ids 0): set--gpu_ids -1 to use CPU mode; set --gpu_ids 0 1 2 for multi-GPU mode. You need a large batch size (e.g. --batchSize 32) to benefit from multiple GPUs.     bash ./datasets/download_cyclegan_dataset.sh dataset_name   bash ./datasets/download_pix2pix_dataset.sh dataset_name   """;General;https://github.com/tomgillooly/geogan
"""it's somewhat harder to install. MonoBeast requires only Python and  PyTorch (we suggest using PyTorch version 1.2 or newer).   The easiest way to build and install all of PolyBeast's dependencies   To run PolyBeast directly on Linux or MacOS  follow this guide.  Create a new Conda environment  and install PolyBeast's requirements:   $ conda create -n torchbeast python=3.7  $ conda activate torchbeast  $ pip install -r requirements.txt  Install PyTorch either from   website (select Conda).   $ git submodule update --init --recursive   $ pip install nest/  $ python setup.py install  Create a new Conda environment  and install PolyBeast's requirements:   $ conda create -n torchbeast  $ conda activate torchbeast  $ pip install -r requirements.txt  PyTorch can be installed as per its  website (select Conda).   $ git submodule update --init --recursive   $ pip install nest/  $ python setup.py install   We would love to have you contribute to TorchBeast or use it for your   MonoBeast is a pure Python + PyTorch implementation of IMPALA.  To set it up  create a new conda environment and install MonoBeast's requirements:  ```bash $ conda create -n torchbeast $ conda activate torchbeast $ conda install pytorch -c pytorch $ pip install -r requirements.txt ```  Then run MonoBeast  e.g. on the [Pong Atari environment](https://gym.openai.com/envs/Pong-v0/):  ```shell $ python -m torchbeast.monobeast --env PongNoFrameskip-v4 ```  By default  MonoBeast uses only a few actors (each with their instance of the environment). Let's change the default settings (try this on a beefy machine!):  ```shell $ python -m torchbeast.monobeast \      --env PongNoFrameskip-v4 \      --num_actors 45 \      --total_steps 30000000 \      --learning_rate 0.0004 \      --epsilon 0.01 \      --entropy_cost 0.01 \      --batch_size 4 \      --unroll_length 80 \      --num_buffers 60 \      --num_threads 4 \      --xpid example ```  Results are logged to `~/logs/torchbeast/latest` and a checkpoint file is written to `~/logs/torchbeast/latest/model.tar`.  Once training finished  we can test performance on a few episodes:  ```shell $ python -m torchbeast.monobeast \      --env PongNoFrameskip-v4 \      --mode test \      --xpid example ```  MonoBeast is a simple  single-machine version of IMPALA. Each actor runs in a separate process with its dedicated instance of the environment and runs the PyTorch model on the CPU to create actions. The resulting rollout trajectories (environment-agent interactions) are sent to the learner. In the main process  the learner consumes these rollouts and uses them to update the model's weights.    """;Reinforcement Learning;https://github.com/facebookresearch/torchbeast
"""该系统实现了基于深度框架的语音识别中的声学模型和语言模型建模，其中声学模型包括CNN-CTC、GRU-CTC、CNN-RNN-CTC，语言模型包含[transformer](https://jalammar.github.io/illustrated-transformer/)、[CBHG](https://github.com/crownpku/Somiao-Pinyin)，数据集包含stc、primewords、Aishell、thchs30四个数据集。  本系统更整体介绍：https://blog.csdn.net/chinatelecom08/article/details/82557715  本项目现已训练一个迷你的语音识别系统，将项目下载到本地上，下载[thchs数据集](http://www.openslr.org/resources/18/data_thchs30.tgz)并解压至data，运行`test.py`，不出意外能够进行识别，结果如下：       the  0 th example.     文本结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     原文汉字： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然     识别结果： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然  若自己建立模型则需要删除现有模型，重新配置参数训练，具体实现流程参考本页最后。   |Name | train | dev | test   get source list...   make am vocab...   make lm pinyin vocab...   make lm hanzi vocab...   get source list...   make am vocab...   make lm pinyin vocab...   make lm hanzi vocab...   我的github: https://github.com/audier   """;General;https://github.com/StevenLai1994/AM_and_LM
"""Use prepare_coco_dataset.sh to download the COCO2017 dataset and create the tfrecords.   """;General;https://github.com/srihari-humbarwadi/retinanet-tensorflow2.x
"""This work is accepted at the PRIME workshop in MICCAI 2021.  > **Investigating and Quantifying the Reproducibility of Graph Neural Networks in Predictive Medicine** > > Mohammed Amine Gharsallaoui  Furkan Tornaci and Islem Rekik > > BASIRA Lab  Faculty of Computer and Informatics  Istanbul Technical University  Istanbul  Turkey > > **Abstract:** *Graph neural networks (GNNs) have gained an unprecedented attention in many domains including dysconnectivity disorder diagnosis thanks to their high performance in tackling graph classification tasks. Despite the large stream of GNNs developed recently  prior efforts invariably focus on boosting the classification accuracy while ignoring the model reproducibility and interpretability  which are vital in pinning down disorder-specific biomarkers. Although less investigated  the discriminativeness of the original input features -biomarkers  which is reflected by their learnt weights using a GNN gives informative insights about their reliability. Intuitively  the reliability of a given biomarker is emphasized if it belongs to the sets of top discriminative regions of interest (ROIs) using different models. Therefore  we define the first axis in our work as \emph{reproducibility across models}  which evaluates the commonalities between sets of top discriminative biomarkers for a pool of GNNs. This task mainly answers this question: \emph{How likely can two models be congruent in terms of their respective sets of top discriminative biomarkers?} The second axis of research in our work is to investigate \emph{reproducibility in generated connectomic datasets}. This is addressed by answering this question: \emph{how likely would the set of top discriminative biomarkers by a trained model for a ground-truth dataset be consistent with a predicted dataset by generative learning?} In this paper  we propose a reproducibility assessment framework  a method for quantifying the commonalities in the GNN-specific learnt feature maps across models  which can complement explanatory approaches of GNNs and provide new ways to assess predictive medicine via biomarkers reliability. We evaluated our framework using four multiview connectomic datasets of healthy neurologically disordered subjects with five GNN architectures and two different learning mindsets: (a) conventional training on all samples (resourceful) and (b) a few-shot training on random samples (frugal).*    Download version for your system (We used Python 3.8  on 64bit Windows 10 )  Install the platform  Create a conda environment by typing:  conda create –n env_reproducibility pip python=3.8   ![reproducibility scores](results_figure.png) The figure demonstrates an example of output for a population of 80 subjects where each subject has 2 views (each represented by 35 by 35 matrix). We computed the reproducibility scores of 5 GNN models using two training settings (cross-validation and few-shot). For each view  we display the scores using real and generated datasets.   """;Graphs;https://github.com/basiralab/Reproducible-Generative-Learning
"""We implement both the Nueral ODE and ResNet architectures for classification of electrocardiogram signals. The data is taken from the frequently used MIT-BIH ECG database  which contains over 100 000 labeled samples of ECG signals from a single heartbeat. The data and a description of the sampling procedure used can be found at https://www.physionet.org/content/mitdb/1.0.0/. We briefly visualize the data  which enables us an intuitive look at the various features and differences between each class that our neural networks will learn and distinguish.   We construct both network architectures with the help of PyTorch and the torchdiffeq library found in the original Neural ODE paper: https://arxiv.org/abs/1806.07366. The ResNet and ODENet  the implementation of the Neural ODE  are designed to be as similar as possible  so we can compare the two fairly. Both models contain identical downampling layers  1D convolutions  normalizations (Group)  activations (ReLU)  and output layers. We train both models and evaluate them on the testing set while also noting differences in speed  memory  and accuracy. Overall  both models perform comparably on the testing data. However  there is a tradeoff between speed and memory. The ResNet is faster to train while the ODENet contains fewer tunable parameters (less memory).   """;General;https://github.com/abaietto/neural_ode_classification
"""The paper results can be reproduced by running: ``` ./run_experiments.sh ``` Experiments on single environments can be run by calling: ``` python main.py --env HalfCheetah-v2 ```  Hyper-parameters can be modified with different arguments to main.py. We include an implementation of DDPG (DDPG.py)  which is not used in the paper  for easy comparison of hyper-parameters with TD3. This is not the implementation of ""Our DDPG"" as used in the paper (see OurDDPG.py).   Algorithms which TD3 compares against (PPO  TRPO  ACKTR  DDPG) can be found at [OpenAI baselines repository](https://github.com/openai/baselines).    """;Reinforcement Learning;https://github.com/sfujim/TD3
"""- Download the ImageNet validation set and move images to labeled subfolders. To do the latter  you can use [this script](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh). Make sure the folder `val` is under `data/`. - Download the `PNASNet-5_Large_331` pretrained model: ```bash cd data wget https://storage.googleapis.com/download.tensorflow.org/models/pnasnet-5_large_2017_12_13.tar.gz tar xvf pnasnet-5_large_2017_12_13.tar.gz ```   ```bash python main.py ```  The last printed line should read: ```bash Test: [50000/50000]	Prec@1 0.829	Prec@5 0.962 ```  """;General;https://github.com/chenxi116/PNASNet.TF
"""- Python version >= 3.6 - TensorFlow >= 2.3.0  Install NeurST from source: ``` git clone https://github.com/bytedance/neurst.git cd neurst/ pip3 install -e . ``` If there exists ImportError during running  manually install the required packages at that time.   """;Sequential;https://github.com/bytedance/neurst
"""U-Net is a fully convolutional neural network with an encoder-decoder structure designed for sementic image segmantation on biomedical images. [[1]](#1) It is a very effective meta-network architecture that has been adapted to incorporate other convolutional neural network architecture designs.   """;Computer Vision;https://github.com/hayashimasa/UNet-PyTorch
"""`pip install -r requitements.txt`       python main.py train path/to/coco path/to/wikiart [OPTIONS]   - COCO: https://cocodataset.org/#download - WikiArt: https://www.kaggle.com/c/painter-by-numbers/data   """;General;https://github.com/aadhithya/AdaIN-pytorch
"""These papers are classified into one of the following seven classes:   """;Graphs;https://github.com/bcsrn/gcn
"""python 아주 살짝  기본 linux 명령어 : linux.md  실습내용    윈도우 환경에서 linux command HowTo : how_to_linux_command_on_windows.md   dynamic robotics 1 : https://www.youtube.com/watch?v=_sBBaNYex3E  dynamic robotics 2 : https://www.youtube.com/watch?v=94nnAOZRg8k  cart pole : https://www.youtube.com/watch?v=XiigTGKZfks  bidirectional RNN : https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66   동영상 스타일 변환 : https://www.youtube.com/watch?v=Khuj4ASldmU   Papers with code : https://paperswithcode.com/       jupyter           cd  pwd  ls          mkdir  rm  cp           git  wget       gpu           python           pip       numpy       pandas       compile   """;Computer Vision;https://github.com/dhrim/cau_2021
"""All the parameters related to training/decoding will be stored in a yaml file. Hyperparameter tuning and massive experiment and can be managed easily this way. See [config files](config/) for the exact format and examples.   Before you start  make sure all the packages required listed above are installed correctly  See the instructions on the Preprocess wiki page for preprocessing instructions.   Note that the arguments--ckpdir=XXX --ckpt=XXX``` needs to be set correctly for the above command to run properly.   """;General;https://github.com/andi611/Mockingjay-Speech-Representation
"""|-- coco(dataset)   pip install pycocotools webcolors   """;Computer Vision;https://github.com/GuoQuanhao/EfficientDet-Paddle
"""The hyper-parameters are divided in 4 categories.    This pipeline's purpose is to train a neural network to segment NifTi files from examples.   Since the training requires example  the first step consists in producing manual segmentations of a fraction of the files. 10 to 50% of the files should be a good proportion  however this sample must be representative of the rest of the dataset. Datasets with great variability might require bigger fractions to be manually segmented.   The network is trained through a gradient back-propagation algorithm on the loss. The loss quantifies the difference between the predictions of the network and the manual segementations.   Once trained  the network can be used to automtically segment the entire dataset.  For training and inference  the volumes are sliced along the vertical axis and treated as collections of 2D images. Thus the image processing operations are 2D operations. Data augmentation is used on the training data. It consists in random modifications of the images and their corresponding GT to create more various examples.   <img src=""./media/process.png"" alt=""process schema"" width=""600""/>    Rename the *parameters_template.json* file to *parameters.json* and modify the values with the hyper-parameters you want.   See the section **Description of the hyper-parameters** below for a complete description of their functions.   A copy of the *parameters.json* file is added to the folder of the run where the model is saved.      Clone the repo:   ``` bash git clone https://github.com/neuropoly/multiclass-segmentation cd multiclass-segmentation ```  The required librairies can be easily installed with pip:  ``` bash pip install -r requirements.txt ```    > Note: To use tensorboard you must also install tensorflow with    > ``` pip install tensorflow```   You can use the --cuda option to use cuda (thus running on GPU)  and the --GPU_id argument (int) to define the id of the GPU to use (default is 0). For example :    """;Computer Vision;https://github.com/neuropoly/multiclass-segmentation
"""1.  Prepare for the running environment.       You can either use the docker image we provide  or follow the installation steps in [`OpenPCDet`](https://github.com/open-mmlab/OpenPCDet).       ```     docker pull djiajun1206/pcdet-pytorch1.5     ```  2. Prepare for the data.      Please download the official [KITTI 3D object detection](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d) dataset and organize the downloaded files as follows (the road planes could be downloaded from [[road plane]](https://drive.google.com/file/d/1d5mq0RXRnvHPVeKx6Q612z0YRO1t2wAp/view?usp=sharing)  which are optional for data augmentation in the training):       ```     Voxel-R-CNN     ├── data     │   ├── kitti     │   │   │── ImageSets     │   │   │── training     │   │   │   ├──calib & velodyne & label_2 & image_2 & (optional: planes)     │   │   │── testing     │   │   │   ├──calib & velodyne & image_2     ├── pcdet     ├── tools     ```     Generate the data infos by running the following command:     ```     python -m pcdet.datasets.kitti.kitti_dataset create_kitti_infos tools/cfgs/dataset_configs/kitti_dataset.yaml     ```  3. Setup.      ```     python setup.py develop     ```   1. Training.          The configuration file is in tools/cfgs/voxelrcnn  and the training scripts is in tools/scripts.      ```     cd tools     sh scripts/train_voxel_rcnn.sh     ```  2. Evaluation.      The configuration file is in tools/cfgs/voxelrcnn  and the training scripts is in tools/scripts.      ```     cd tools     sh scripts/eval_voxel_rcnn.sh     ```     """;Computer Vision;https://github.com/djiajunustc/Voxel-R-CNN
"""Kerascv - https://pypi.org/project/kerascv/   """;Computer Vision;https://github.com/kwantommy/SE-ResNeXt-image-classification
""" 1. Head over to [Google Colab](https://colab.research.google.com/). It is recommended that you switch to a GPU notebook as things will usually run a little faster that way. There are instructions for this on the colaboratory site. 2. Download the .ipynb file in this repository 3. Upload that file to Google Colabatory and run from there!  **Note: Google Colaboratory has time limits for their systems  so you may not be able to fully train the Mogrifier LSTM on their system without some special effort.**   Recommended you use Python `3.7`. You will need to make sure that your virtualenv setup is of the correct version of python. We will be using *PyTorch*.  Please see below for executing a virtual environment.  ```shell cd MogrifierLSTM pip3 install virtualenv #: If you didn't install it virtualenv -p $(which python3) ./venv_lstm source ./venv_lstm/bin/activate  #: Install dependencies pip3 install -r requirements.txt  #: View the notebook  #: Deactivate the virtual environment when you are done deactivate ```   To view the notebook  simply run the following command to start an ipython kernel.   : add your virtual environment to jupyter notebook  python -m ipykernel install --user --name=venv_lstm  : port is only needed if you want to work on more than one notebook  jupyter notebook --port=<your_port>   Check the python environment you are using on the top right corner.  If the name of environment doesn't match  change it to your virtual environment in ""Kernel>Change kernel"".   :replace ""run"" with whatever directory you have saved   """;Sequential;https://github.com/RMichaelSwan/MogrifierLSTM
"""FastAI + transformers - https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2   Introspective Ratinale (interpret-text github) - https://github.com/interpretml/interpret-text   """;Natural Language Processing;https://github.com/tomdyer10/fake_news
"""* base docker environment: https://hub.docker.com/r/evariste/autodl  * pre requirements ```bash $ apt update $ apt install python3-tk ```  * clone and init. the repository ```bash $ git clone https://github.com/kakaobrain/autoclint.git && cd autoclint $ #: 3rd parties libarary $ git submodule init $ git submodule update $ #: download pretrained models $ wget https://download.pytorch.org/models/resnet18-5c106cde.pth -O ./models/resnet18-5c106cde.pth $ #: download public datasets $ cd autodl && python download_public_datasets.py && cd .. ```  * run public datasets ```bash $ #: images $ python autodl/run_local_test.py -time_budget=1200 -code_dir='./' -dataset_dir='autodl/AutoDL_public_data/Munster/'; cp autodl/AutoDL_scoring_output/learning-curve-*.png ./results $ python autodl/run_local_test.py -time_budget=1200 -code_dir='./' -dataset_dir='autodl/AutoDL_public_data/Chucky/'; cp autodl/AutoDL_scoring_output/learning-curve-*.png ./results $ python autodl/run_local_test.py -time_budget=1200 -code_dir='./' -dataset_dir='autodl/AutoDL_public_data/Pedro/'; cp autodl/AutoDL_scoring_output/learning-curve-*.png ./results $ python autodl/run_local_test.py -time_budget=1200 -code_dir='./' -dataset_dir='autodl/AutoDL_public_data/Decal/'; cp autodl/AutoDL_scoring_output/learning-curve-*.png ./results $ python autodl/run_local_test.py -time_budget=1200 -code_dir='./' -dataset_dir='autodl/AutoDL_public_data/Hammer/'; cp autodl/AutoDL_scoring_output/learning-curve-*.png ./results $ #: videos $ python autodl/run_local_test.py -time_budget=1200 -code_dir='./' -dataset_dir='autodl/AutoDL_public_data/Kreatur/'; cp autodl/AutoDL_scoring_output/learning-curve-*.png ./results $ python autodl/run_local_test.py -time_budget=1200 -code_dir='./' -dataset_dir='autodl/AutoDL_public_data/Katze/'; cp autodl/AutoDL_scoring_output/learning-curve-*.png ./results $ python autodl/run_local_test.py -time_budget=1200 -code_dir='./' -dataset_dir='autodl/AutoDL_public_data/Kraut/'; cp autodl/AutoDL_scoring_output/learning-curve-*.png ./results ```  * (optional) display learning curve ```bash $ #: item2 utils to visualize learning curve $ wget https://www.iterm2.com/utilities/imgcat -O bin/imgcat; chmod 0677 bin/imgcat $ bin/imgcat ./results/learning-curve-*.png ```   experiment environment: [BrainCloud][] V1.XLARGE Type (NVIDIA Tesla V100 1GPU  14CPU  122GB)   experiment environment: CodaLab (NVIDIA Tesla P100 1GPU  4vCPU  26GB)   experiment environment: [BrainCloud][] V1.XLARGE Type (NVIDIA Tesla V100 1GPU  14CPU  122GB)   experiment environment: CodaLab (NVIDIA Tesla P100 1GPU  4vCPU  26GB)   experiment environment: CodaLab (NVIDIA Tesla P100 1GPU  4vCPU  26GB)   """;Computer Vision;https://github.com/kakaobrain/autoclint
""" - I tried this in Ubuntu and Mac.   - Reset log_directory and data_directory in actor.py  and learner.py.    You can find demo on [youtube](https://www.youtube.com/watch?v=vSkLegIUD98).    """;Reinforcement Learning;https://github.com/Lyusungwon/apex_dqn_pytorch
"""**QA_PreProcess.py\QA_PreProcess.ipynb:** Converts the raw induction tasks data set to separate ndarrays containing questions  answers  and facts with all words being in the form of GloVe pre-trained vector representations.    **DMN+.py\DMN+.ipynb:** The DMN+ model  along with training  validation and testing.    Numpy 1.13.3   """;General;https://github.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master
"""Google AI in 2017 proposed a new simple network architecture  the Transformer  based solely on attention mechanisms  dispensing with recurrence and convolutions entirely. As you see  this model architecture becomes the base stone of the following state-of-the-art pre-trained models in Natural Language Processing (NLP)  such as GPT  BERT  Transformer-XL XLnet  RoBERTa.   This repo will walk through the implementation of Transformer. Code is simple  clean and easy to understand. Some of these codes are based on The [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html).  Currently this project is working in process  as always  PRs are welcome :)   ~~~ git clone https://github.com/walkacross/transformer-pytorch.git  cd transformer-pytorch  python setup.py develop ~~~   """;General;https://github.com/walkacross/transformer-pytorch
"""- [Installation](https://github.com/Cadene/pretrained-models.pytorch#installation) - [Quick examples](https://github.com/Cadene/pretrained-models.pytorch#quick-examples) - [Few use cases](https://github.com/Cadene/pretrained-models.pytorch#few-use-cases)     - [Compute imagenet logits](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-logits)     - [Compute imagenet validation metrics](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-validation-metrics) - [Evaluation on ImageNet](https://github.com/Cadene/pretrained-models.pytorch#evaluation-on-imagenet)     - [Accuracy on valset](https://github.com/Cadene/pretrained-models.pytorch#accuracy-on-validation-set)     - [Reproducing results](https://github.com/Cadene/pretrained-models.pytorch#reproducing-results) - [Documentation](https://github.com/Cadene/pretrained-models.pytorch#documentation)     - [Available models](https://github.com/Cadene/pretrained-models.pytorch#available-models)         - [AlexNet](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [BNInception](https://github.com/Cadene/pretrained-models.pytorch#bninception)         - [CaffeResNet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet)         - [DenseNet121](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet161](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet169](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DualPathNet68](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet92](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet98](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet107](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet113](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [FBResNet152](https://github.com/Cadene/pretrained-models.pytorch#facebook-resnet)         - [InceptionResNetV2](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [InceptionV3](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [InceptionV4](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [NASNet-A-Large](https://github.com/Cadene/pretrained-models.pytorch#nasnet)         - [NASNet-A-Mobile](https://github.com/Cadene/pretrained-models.pytorch#nasnet)         - [PNASNet-5-Large](https://github.com/Cadene/pretrained-models.pytorch#pnasnet)         - [PolyNet](https://github.com/Cadene/pretrained-models.pytorch#polynet)         - [ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)         - [ResNeXt101_64x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)         - [ResNet101](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet152](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet18](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet34](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet50](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [SENet154](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet50](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet101](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet152](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNeXt50_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SqueezeNet1_0](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [SqueezeNet1_1](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG11](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG13](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG16](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG19](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG11_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG13_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG16_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG19_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [Xception](https://github.com/Cadene/pretrained-models.pytorch#xception)     - [Model API](https://github.com/Cadene/pretrained-models.pytorch#model-api)         - [model.input_size](https://github.com/Cadene/pretrained-models.pytorch#modelinput_size)         - [model.input_space](https://github.com/Cadene/pretrained-models.pytorch#modelinput_space)         - [model.input_range](https://github.com/Cadene/pretrained-models.pytorch#modelinput_range)         - [model.mean](https://github.com/Cadene/pretrained-models.pytorch#modelmean)         - [model.std](https://github.com/Cadene/pretrained-models.pytorch#modelstd)         - [model.features](https://github.com/Cadene/pretrained-models.pytorch#modelfeatures)         - [model.logits](https://github.com/Cadene/pretrained-models.pytorch#modellogits)         - [model.forward](https://github.com/Cadene/pretrained-models.pytorch#modelforward) - [Reproducing porting](https://github.com/Cadene/pretrained-models.pytorch#reproducing)     - [ResNet*](https://github.com/Cadene/pretrained-models.pytorch#hand-porting-of-resnet152)     - [ResNeXt*](https://github.com/Cadene/pretrained-models.pytorch#automatic-porting-of-resnext)     - [Inception*](https://github.com/Cadene/pretrained-models.pytorch#hand-porting-of-inceptionv4-and-inceptionresnetv2)   Results were obtained using (center cropped) images of the same size than during the training process.  Model | Version | Acc@1 | Acc@5 --- | --- | --- | --- PNASNet-5-Large | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 82.858 | 96.182 [PNASNet-5-Large](https://github.com/Cadene/pretrained-models.pytorch#pnasnet) | Our porting | 82.736 | 95.992 NASNet-A-Large | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 82.693 | 96.163 [NASNet-A-Large](https://github.com/Cadene/pretrained-models.pytorch#nasnet) | Our porting | 82.566 | 96.086 SENet154 | [Caffe](https://github.com/hujie-frank/SENet) | 81.32 | 95.53 [SENet154](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 81.304 | 95.498 PolyNet | [Caffe](https://github.com/CUHK-MMLAB/polynet) | 81.29 | 95.75 [PolyNet](https://github.com/Cadene/pretrained-models.pytorch#polynet) | Our porting | 81.002 | 95.624 InceptionResNetV2 | [Tensorflow](https://github.com/tensorflow/models/tree/master/slim) | 80.4 | 95.3 InceptionV4 | [Tensorflow](https://github.com/tensorflow/models/tree/master/slim) | 80.2 | 95.3 [SE-ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 80.236 | 95.028 SE-ResNeXt101_32x4d | [Caffe](https://github.com/hujie-frank/SENet) | 80.19 | 95.04 [InceptionResNetV2](https://github.com/Cadene/pretrained-models.pytorch#inception) | Our porting | 80.170 | 95.234 [InceptionV4](https://github.com/Cadene/pretrained-models.pytorch#inception) | Our porting | 80.062 | 94.926 [DualPathNet107_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.746 | 94.684 ResNeXt101_64x4d | [Torch7](https://github.com/facebookresearch/ResNeXt) | 79.6 | 94.7 [DualPathNet131](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.432 | 94.574 [DualPathNet92_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.400 | 94.620 [DualPathNet98](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.224 | 94.488 [SE-ResNeXt50_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 79.076 | 94.434 SE-ResNeXt50_32x4d | [Caffe](https://github.com/hujie-frank/SENet) | 79.03 | 94.46 [Xception](https://github.com/Cadene/pretrained-models.pytorch#xception) | [Keras](https://github.com/keras-team/keras/blob/master/keras/applications/xception.py) | 79.000 | 94.500 [ResNeXt101_64x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext) | Our porting | 78.956 | 94.252 [Xception](https://github.com/Cadene/pretrained-models.pytorch#xception) | Our porting | 78.888 | 94.292 ResNeXt101_32x4d | [Torch7](https://github.com/facebookresearch/ResNeXt) | 78.8 | 94.4 SE-ResNet152 | [Caffe](https://github.com/hujie-frank/SENet) | 78.66 | 94.46 [SE-ResNet152](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 78.658 | 94.374 ResNet152 | [Pytorch](https://github.com/pytorch/vision#models) | 78.428 | 94.110 [SE-ResNet101](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 78.396 | 94.258 SE-ResNet101 | [Caffe](https://github.com/hujie-frank/SENet) | 78.25 | 94.28 [ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext) | Our porting | 78.188 | 93.886 FBResNet152 | [Torch7](https://github.com/facebook/fb.resnet.torch) | 77.84 | 93.84 SE-ResNet50 | [Caffe](https://github.com/hujie-frank/SENet) | 77.63 | 93.64 [SE-ResNet50](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 77.636 | 93.752 [DenseNet161](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.560 | 93.798 [ResNet101](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.438 | 93.672 [FBResNet152](https://github.com/Cadene/pretrained-models.pytorch#facebook-resnet) | Our porting | 77.386 | 93.594 [InceptionV3](https://github.com/Cadene/pretrained-models.pytorch#inception) | [Pytorch](https://github.com/pytorch/vision#models) | 77.294 | 93.454 [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.152 | 93.548 [DualPathNet68b_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 77.034 | 93.590 [CaffeResnet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet) | [Caffe](https://github.com/KaimingHe/deep-residual-networks) | 76.400 | 92.900 [CaffeResnet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet) | Our porting | 76.200 | 92.766 [DenseNet169](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 76.026 | 92.992 [ResNet50](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 76.002 | 92.980 [DualPathNet68](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 75.868 | 92.774 [DenseNet121](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 74.646 | 92.136 [VGG19_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 74.266 | 92.066 NASNet-A-Mobile | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 74.0 | 91.6 [NASNet-A-Mobile](https://github.com/veronikayurchuk/pretrained-models.pytorch/blob/master/pretrainedmodels/models/nasnet_mobile.py) | Our porting | 74.080 | 91.740 [ResNet34](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 73.554 | 91.456 [BNInception](https://github.com/Cadene/pretrained-models.pytorch#bninception) | Our porting | 73.524 | 91.562 [VGG16_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 73.518 | 91.608 [VGG19](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 72.080 | 90.822 [VGG16](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 71.636 | 90.354 [VGG13_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 71.508 | 90.494 [VGG11_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 70.452 | 89.818 [ResNet18](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 70.142 | 89.274 [VGG13](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 69.662 | 89.264 [VGG11](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 68.970 | 88.746 [SqueezeNet1_1](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 58.250 | 80.800 [SqueezeNet1_0](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 58.108 | 80.428 [Alexnet](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 56.432 | 79.194  Notes: - the Pytorch version of ResNet152 is not a porting of the Torch7 but has been retrained by facebook. - For the PolyNet evaluation each image was resized to 378x378 without preserving the aspect ratio and then the central 331×331 patch from the resulting image was used.  Beware  the accuracy reported here is not always representative of the transferable capacity of the network on other tasks and datasets. You must try them all! :P       3. `git clone https://github.com/Cadene/pretrained-models.pytorch.git` 4. `cd pretrained-models.pytorch` 5. `python setup.py install`    3. `pip install pretrainedmodels`   1. [python3 with anaconda](https://www.continuum.io/downloads) 2. [pytorch with/out CUDA](http://pytorch.org)   <a href=""https://travis-ci.org/Cadene/pretrained-models.pytorch""><img src=""https://api.travis-ci.org/Cadene/pretrained-models.pytorch.svg?branch=master""/></a>   - 13/01/2018: pip install pretrainedmodels  pretrainedmodels.model_names  pretrainedmodels.pretrained_settings   Source: TensorFlow Slim repo   Source: TensorFlow Slim repo and Pytorch/Vision repo for inceptionv3   Source: MXNET repo of Chen Yunpeng  The porting has been made possible by Ross Wightman in his PyTorch repo.   Source: Keras repo   Source: TensorFlow Slim repo   Source: Pytorch/Vision repo   - To import `pretrainedmodels`:  ```python import pretrainedmodels ```  - To print the available pretrained models:  ```python print(pretrainedmodels.model_names) > ['fbresnet152'  'bninception'  'resnext101_32x4d'  'resnext101_64x4d'  'inceptionv4'  'inceptionresnetv2'  'alexnet'  'densenet121'  'densenet169'  'densenet201'  'densenet161'  'resnet18'  'resnet34'  'resnet50'  'resnet101'  'resnet152'  'inceptionv3'  'squeezenet1_0'  'squeezenet1_1'  'vgg11'  'vgg11_bn'  'vgg13'  'vgg13_bn'  'vgg16'  'vgg16_bn'  'vgg19_bn'  'vgg19'  'nasnetalarge'  'nasnetamobile'  'cafferesnet101'  'senet154'   'se_resnet50'  'se_resnet101'  'se_resnet152'  'se_resnext50_32x4d'  'se_resnext101_32x4d'  'cafferesnet101'  'polynet'  'pnasnet5large'] ```  - To print the available pretrained settings for a chosen model:  ```python print(pretrainedmodels.pretrained_settings['nasnetalarge']) > {'imagenet': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth'  'input_space': 'RGB'  'input_size': [3  331  331]  'input_range': [0  1]  'mean': [0.5  0.5  0.5]  'std': [0.5  0.5  0.5]  'num_classes': 1000}  'imagenet+background': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth'  'input_space': 'RGB'  'input_size': [3  331  331]  'input_range': [0  1]  'mean': [0.5  0.5  0.5]  'std': [0.5  0.5  0.5]  'num_classes': 1001}} ```  - To load a pretrained models from imagenet:  ```python model_name = 'nasnetalarge' #: could be fbresnet152 or inceptionresnetv2 model = pretrainedmodels.__dict__[model_name](num_classes=1000  pretrained='imagenet') model.eval() ```  **Note**: By default  models will be downloaded to your `$HOME/.torch` folder. You can modify this behavior using the `$TORCH_HOME` variable as follow: `export TORCH_HOME=""/local/pretrainedmodels""`  - To load an image and do a complete forward pass:  ```python import torch import pretrainedmodels.utils as utils  load_img = utils.LoadImage()  #: transformations depending on the model #: rescale  center crop  normalize  and others (ex: ToBGR  ToRange255) tf_img = utils.TransformImage(model)   path_img = 'data/cat.jpg'  input_img = load_img(path_img) input_tensor = tf_img(input_img)         #: 3x400x225 -> 3x299x299 size may differ input_tensor = input_tensor.unsqueeze(0) #: 3x299x299 -> 1x3x299x299 input = torch.autograd.Variable(input_tensor      requires_grad=False)  output_logits = model(input) #: 1x1000 ```  - To extract features (beware this API is not available for all networks):  ```python output_features = model.features(input) #: 1x14x14x2048 size may differ output_logits = model.logits(output_features) #: 1x1000 ```   """;General;https://github.com/Cadene/pretrained-models.pytorch
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/edhart151/darknet
"""Note that this could be relying on a different ""take"" than you would expect. For example  you could have answered ลูกเขย in the second example because it  is the one associated with male gender.   """;General;https://github.com/cstorm125/thai2fit
"""Convolutional neural network is a key architure to recognize and classify the image to classes. However  variation of convolutional neural networks are very many and very vary their space/time usage for learning.   In this project  I compared the state-of-art convolutional neural networks and compared them based on **Performance** and **Resources**(space/time). Through this project  it helps to decide the model to solve problem based on problem size and limitation of resources.    --------   """;Computer Vision;https://github.com/wonjaek36/sodeep_final
"""bash ./datasets/download_dataset.sh dataset_name   bash ./models/download_model.sh model_name   bash ./scripts/eval_cityscapes/download_fcn8s.sh  Then make sure ./scripts/eval_cityscapes/ is in your system's python path. If not  run the following command to add it   Now you can run the following command to evaluate your predictions:  python ./scripts/eval_cityscapes/evaluate.py --cityscapes_dir /path/to/original/cityscapes/dataset/ --result_dir /path/to/your/predictions/ --output_dir /path/to/output/directory/   Install it with: luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec   - Install torch and dependencies from https://github.com/torch/distro - Install torch packages `nngraph` and `display` ```bash luarocks install nngraph luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec ``` - Clone this repo: ```bash git clone git@github.com:phillipi/pix2pix.git cd pix2pix ``` - Download the dataset (e.g. [CMP Facades](http://cmp.felk.cvut.cz/~tylecr1/facade/)): ```bash bash ./datasets/download_dataset.sh facades ``` - Train the model ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA th train.lua ``` - (CPU only) The same training command without using a GPU or CUDNN. Setting the environment variables ```gpu=0 cudnn=0``` forces CPU only ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA gpu=0 cudnn=0 batchSize=10 save_epoch_freq=5 th train.lua ``` - (Optionally) start the display server to view results as the model trains. ( See [Display UI](#display-ui) for more details): ```bash th -ldisplay.start 8000 0.0.0.0 ```  - Finally  test the model: ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA phase=val th test.lua ``` The test results will be saved to an html file here: `./results/facades_generation/latest_net_G_val/index.html`.   """;Computer Vision;https://github.com/svikramank/pix2pix
"""You have an audio recording  and you want to know where certain classes of sounds are.  SongExplorer is trained to recognize such words by manually giving it a few examples.  It will then automatically calculate the probability  over time  of when those words occur in all of your recordings.  Applications suitable for SongExplorer include quantifying the rate or pattern of words emitted by a particular species  distinguishing a recording of one species from another  and discerning whether individuals of the same species produce different song.  Underneath the hood is a deep convolutional neural network.  The input is the raw audio stream  and the output is a set of mutually-exclusive probability waveforms corresponding to each word of interest.  Training begins by first thresholding one of your recordings in the time- and frequency-domains to find sounds that exceed the ambient noise. These sounds are then clustered into similar categories for you to manually annotate with however many word labels naturally occur.  A classifier is then trained on this corpus of ground truth  and a new recording is analyzed by it.  The words it automatically finds are then clustered as before  but this time are displayed with predicted labels.  You manually correct the mistakes  both re-labeling words that it got wrong  as well as labeling words it missed.  These new annotations are added to the ground truth  and the process of retraining the classifier and analyzing and correcting new recordings is repeated until the desired accuracy is reached.        V.time_sigma_string.value = ""4 2""     V.time_smooth_ms_string.value = ""6.4""     V.frequency_n_ms_string.value = ""25.6""     V.frequency_nw_string.value = ""4""     V.frequency_p_string.value = ""0.1 1.0""     V.frequency_smooth_ms_string.value = ""25.6""       SongExplorer can be run on all three major platforms.  The installation procedure is different on each due to various support of the technologies used. We recommend using Singularity on Linux  and Docker on Microsoft Windows and Apple Macintosh.  Training your own classifier is fastest with an Nvidia graphics processing unit (GPU).  TensorFlow  the machine learning framework from Google that SongExplorer uses  supports Ubuntu  Windows and Mac.  The catch is that Nvidia (and hence TensorFlow) currently doesn't support GPUs on Macs.  So while using a pre-trained classifier would be fine on a Mac  because inference is just as fast on the CPU  training your own would take longer.  Docker  a popular container framework which provides an easy way to deploy software across platforms  supports Linux  Windows and Mac  but only supports GPUs on Linux.  Moreover  on Windows and Mac it runs within a heavy-weight virtual machine  and on all platforms it requires administrator privileges to both install and run.  Singularity is an alternative to Docker that does not require root access. For this reason it is required in certain high-performance computing (HPC) environments.  Currently it only natively supports Linux.  There is a version for Macs which uses a light-weight virtual machine  but it is not being actively developed anymore.  You can run Singularity on Windows within a virtual environment  like Docker does  but would have to set that up yourself.  As with Docker  GPUs are only accessible on Linux.  To use SongExplorer with a GPU on Windows one must install it manually  without the convenience of a container.  We're looking for volunteers to write a Conda recipe to make this easy.   provide your manual annotations.  These manual annotations will serve to   snippets signifies your computer terminal's command line.  Square brackets   Platform-specific installation instructions can be found at  Sylabs.  SongExplorer has been tested with version 3.4 on   On Linux you'll also need to install the CUDA and CUDNN drivers from   click the Download button  or equivalently use the command line (for which you   Platform-specific installation instructions can be found at   Prompt on Windows  or the Terminal on Mac  and download the SongExplorer image       -e SONGEXPLORER_BIN -h=`hostname` -p 5006:5006 ^       -e SONGEXPLORER_BIN -h=`hostname` -p 5006:5006 \   userid.  Optionally specify the current working directory with the -w flag.   definition (e.g.  ""%HOMEPATH%/songexplorer/configuration.pysh"" on Windows  or   $ $SONGEXPLORER_BIN cp /opt/songexplorer/configuration.pysh $PWD [%CD% on Windows]   Note that ""configuration.pysh"" must be a valid Python and Bash file.  Hence  the unusual "".pysh"" extension.   SongExplorer provides the option to use a GPU or not with the train_gpu variable.   train_{cpu gpu}_{ncpu_cores ngpu_cards ngigabytes_memory} variables specify   without a GPU.   underestimate the resources required  particularly memory consumption.  To make   Use the nvidia-smi command to similarly monitor the GPU card.  The same   the GPU cores.  Use the watch command to receive repeated updates (i.e.   | NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |   | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |   | Processes:                                                       GPU Memory |  |  GPU       PID   Type   Process name                             Usage      |   Set the SONGEXPLORER environment variable plus the songexplorer alias on both   cluster also requires that the cluster be configured to permit hosting a web   train_gpu_cluster_flags=""-n 2 -gpu 'num=1' -q gpu_rtx""   printed to the terminal.  In the output above this is ""arthurb-ws2:5006""  which   ""configuration.pysh"".  Now press DoIt!.  Output into the log directory are   fields.  Now train a classifier on your annotations using the Train button.   thresholds that one can use to achieve a specified precision-recall ratio.  Use           $PWD/groundtruth-data/round2 [%CD%/... on Windows]   precision-recall ratios using the Ethogram button.  Choose one of the   ""ckpt-*"".  You'll also need to specify which "".tf"" files to threshold using the   set you specify  click on Omit One.  Select the set as described above for   Take care to choose the correct one.   The following Bash code directly calls this script to make predictions on a set   The above workflow could also easily be performed in Julia  Python  Matlab  or   One can also supply your own tensorflow code that implements a   $ git clone https://github.com/JaneliaSciComp/SongExplorer.git  $ rm -rf songexplorer/.git  $ sudo singularity build -s songexplorer.img songexplorer/containers/singularity.def   $ sudo singularity build songexplorer.sif songexplorer.img   To build an image without GPU support  comment out the section titled ""install   your shell environment.  source_path in ""configuration.pysh"" must be set   $ cd songexplorer   well as the Linux Bash interfaces.  To run them  simply execute ""runtests.sh"":   Let's walk through the steps needed to train a classifier completely from scratch.  Recordings need to be monaural 16-bit little-endian PCM-encoded WAV files. They should all be sampled at the same rate  which can be anything.  For this tutorial we supply you with *Drosophila melanogaster* data sampled at 2500 Hz.  First  let's get some data bundled with SongExplorer into your home directory.      $ $SONGEXPLORER_BIN ls -1 /opt/songexplorer/data     20161207T102314_ch1-annotated-person1.csv     20161207T102314_ch1.wav*     20190122T093303a-7-annotated-person2.csv*     20190122T093303a-7-annotated-person3.csv*     20190122T093303a-7.wav*     20190122T132554a-14-annotated-person2.csv*     20190122T132554a-14-annotated-person3.csv*     20190122T132554a-14.wav*     Antigua_20110313095210_ch26.wav     my_frozen_graph_1k_0.pb*     PS_20130625111709_ch3-annotated-person1.csv     PS_20130625111709_ch3.wav*     vgg_labels.txt*      $ mkdir -p groundtruth-data/round1      $ $SONGEXPLORER_BIN cp /opt/songexplorer/data/PS_20130625111709_ch3.wav \           $PWD/groundtruth-data/round1 [%CD%/... on Windows]   corresponding stages in `kernel_sizes`.  See [LeCun *et al* (1989; Neural Computation)](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf).  * `dilate after` specifies the first layer  starting from zero  at which to start dilating the convolutional kernels.  See [Yu and Koltun (2016; arXiv)](https://arxiv.org/pdf/1511.07122.pdf).  * `stride after` specifies the first layer  starting from zero  at which to start striding the convolutional kernels by two.  * `connection` specifies whether to use identity bypasses  which can help models with many layers converge.  See [He  Zhang  Ren  and Sun (2015; arXiv](https://arxiv.org/abs/1512.03385).  * `dropout` is the fraction of hidden units on each forward pass to omit during training.  See [Srivastava  Hinton  *et al* (2014; J. Machine Learning Res.)](http://jmlr.org/papers/v15/srivastava14a.html).  There is also:  * `weights seed` specifies whether to randomize the initial weights or not.  A value of -1 results in different values for each fold.  They are also different each time you run `x-validate`.  Any other number results in a set of initial weights that is unique to that number across all folds and repeated runs.  * `batch seed` similarly specifies whether to randomize the order in which samples are drawn from the ground-truth data set during training.  A value of -1 results in a different order for each fold and run;  any other number results in a unique order specific to that number across folds and runs.  To perform a simple grid search for the optimal value for a particular hyperparameter  first choose how many folds you want to partition your ground-truth data into using `k-fold`.  Then set the hyperparameter of interest to the first value you want to try and choose a name for the `Logs Folder` such that its prefix will be shared across all of the hyperparameter values you plan to validate.  Suffix any additional hyperparameters of interest using underscores.  (For example  to search mini-batch and keep track of kernel size and feature maps  use ""mb-64_ks129_fm64"".)  If your models is small  use `models_per_job` in ""configuration.pysh"" to train multiple folds on a GPU. Click the `X-Validate` button and then `DoIt!`.  One classifier will be trained for each fold  using it as the validation set and the remaining folds for training.  Separate files and subdirectories are created in the `Logs Folder` that are suffixed by the fold number and the letter ""k"".  Plot overlayed training curves with the `Accuracy` button  as before.  Repeat the above procedure for each of remaining hyperparameter values you want to try (e.g. ""mb-128_ks129_fm64""  ""mb-256_ks129_fm64""  etc.).  Then use the `Compare` button to create a figure of the cross-validation data over the hyperparameter values  specifying the prefix that the logs folders have in common (""mb"" in this case). Output are three files:  * ""[suffix]-compare-confusion-matrices.pdf"" contains the summed confusion matrix for each of the values tested.  * ""[suffix]-compare-overall-params-speed.pdf"" plots the accuracy  number of trainable parameters  and training time for each model.  * ""[suffix]-compare-precision-recall.pdf"" shows the final error rates for each model and wanted word.       M.init(""configuration.pysh"")     V.init(None)     C.init(None)           run([""hetero""  ""start""  str(M.local_ncpu_cores)           str(M.local_ngpu_cards)  str(M.local_ngigabytes_memory)])       """;General;https://github.com/JaneliaSciComp/SongExplorer
"""The code has only been tested and verified with Python 3.6. Assuming you have an installation of [pipenv](https://docs.pipenv.org/) for Python 3  you may clone the project  navigate to the root folder and run:  ```bash make install ```  This will most likely take care of the dependencies  unless you're using Windows.   In the `examples` folder you will find a small sample of data  downloaded from the [LJ Speech Dataset](https://keithito.com/LJ-Speech-Dataset/). The dataset originally contains about 24 hours of speech  but I selected just a few files to create a small proof of concept  since I ran the training on my laptop and training such a complex architecture on a huge dataset was not viable for me. I used 50 files for training and 6 for validation.   """;Audio;https://github.com/peustr/wavenet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/SCismycat/bert_code_view
"""You will also need aria2 installed  Install python dependencies via pip install -r requirements.txt   Note that you will need ffmpeg to be installed on your machine to run this script   """;Computer Vision;https://github.com/EmptySamurai/pytorch-reconet
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/artxtech/darknet-rnn
"""For example  the XCiT-S12/16 model can be trained using the following command     ![Detection and Instance segmentation result for an ultra high resolution image 6000x4000](https://user-images.githubusercontent.com/8495451/122474488-962c7380-cfc3-11eb-9e9c-51beda07740b.jpg) )   <!--  First  clone the repo ``` git clone https://github.com/facebookresearch/XCiT.git ```  Then  you can install the required packages including: [Pytorch](https://pytorch.org/) version 1.7.1  [torchvision](https://pytorch.org/vision/stable/index.html) version 0.8.2 and [Timm](https://github.com/rwightman/pytorch-image-models) version 0.4.8 ``` pip install -r requirements.txt ```  Download and extract the [ImageNet](https://imagenet.stanford.edu/) dataset. Afterwards  set the ```--data-path``` argument to the corresponding extracted ImageNet path.  For full details about all the available arguments  you can use ``` python main.py --help ```  For detection and segmentation downstream tasks  please check:  + COCO Object detection and Instance segmentation: [XCiT Detection](detection/)  + ADE20k Semantic segmentation: [XCiT Semantic Segmentation](semantic_segmentation/)  ---   """;General;https://github.com/facebookresearch/xcit
"""a GPU.   Omniglot dataset. Download from https://github.com/brendenlake/omniglot/tree/master/python    """;General;https://github.com/oscarknagg/few-shot
"""`python preprocessing.py --in_dir ljspeech --out_dir DATASETS/ljspeech`   """;Audio;https://github.com/ksw0306/WaveVAE
"""[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.   extremeText like fastText can be build as executable using Make (recommended) or/and CMake:  $ git clone https://github.com/mwydmuch/extremeText.git  $ cd extremeText  (optional) $ cmake .  $ make   The easiest way to get extremeText is to use pip.  $ pip install extremetext  Installing on MacOS may require setting MACOSX_DEPLOYMENT_TARGET=10.9 first:   $ pip install extremetext  The latest version of extremeText can be build from sources using pip or alternatively setuptools.  $ git clone https://github.com/mwydmuch/extremeText.git  $ cd extremeText  $ pip install .  (or) $ python setup.py install   Merge with the latest changes from fastText.   Getting the source code   $ wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip   $ cd fastText-0.1.0  $ make   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ mkdir build &amp;&amp; cd build &amp;&amp; cmake ..  $ make &amp;&amp; make install   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ pip install .   You can also quantize a supervised model to reduce its memory usage with the following command:   extremeText adds new options for fastText supervised command:  ``` $ ./extremetext supervised  New losses for multi-label classification:   -loss sigmoid          -loss plt           (Probabilistic Labels Tree)  With the following optional arguments:   General:   -l2                 L2 regularization (default = 0)   -tfidfWeights       calculate TF-IDF weights for words   -wordsWeights       read word weights from file (format: <word>:<weights>)   -weight             document weight prefix (default = __weight__; format: <weight prefix>:<document weight>)   -tag                tags prefix (default = __tag__)  tags are ignored words  that are outputed with prediction   -eosWeight          weight of EOS token (default = 1.0)   -freezeVectors      freeze pretrained word vectors for supervised learning      PLT (Probabilistic Labels Tree):   -treeType           type of PLT: complete  huffman  kmeans (default = kmeans)   -arity              arity of PLT (default = 2)   -maxLeaves          maximum number of leaves (labels) in one internal node of PLT (default = 100)   -kMeansEps          stopping criteria for k-means clustering (default = 0.001)      Ensemble:   -ensemble           size of the ensemble (default = 1)   -bagging            bagging ratio (default = 1.0) ```  extremeText also adds new commands and makes other to work in parallel: ``` $ ./extremetext predict[-prob] <model> <test-data> [<k>] [<th>] [<output>] [<thread>] $ ./extremetext get-prob <model> <input> [<th>] [<output>] [<thread>] ```   You can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.  There is also the master branch that contains all of our most recent work  but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.   This library has two main use cases: word representation learning and text classification. These were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).   """;Natural Language Processing;https://github.com/mwydmuch/extremeText
"""The validation set is real captcha image crawl from the railway booking website and labeled manually. Load the data as same as above  and X(feature(image)) put in ```vali_data```  Y(label) in ```vali_label```.   Before train the model  we have to load the data into memory.  Firstly we have to process X part: feature(our captcha image). The data we input to CNN should be numpy array type  so we use Pillow to read image and convert it to numpy array.  ```python for index in range(1  50001  1)     image = Image.open(""./data/train_set/"" + str(index) + "".jpg"") #:Load our image     nparr = np.array(image) #: Convert to numpy array     nparr = nparr / 255.0 ```  The shape of nparr is ```(60  200  3)```  it's same as the input we just designed in the model. And we plan to use 50 000 captcha image to train the model  so the input shape to CNN will be ```(50000  60  200  3)```. Use numpy.stack to merge them all:  ```python train_data = np.stack([np.array(Image.open(""./data/train_set/"" + str(index) + "".jpg""))/255.0 for index in range(1  50001  1)]) ```  Now  the shape of train_data is ```(50000  60  200  3)```。  The next is Y part  label: the answer of the training set. Because the model is multi-output(6 softmax regression classifier)  so the Y should be a list containing 6 numpy array  like this: ``` [[First digit of first image ...  First digit of last image]  [Second digit of first image ...  Second digit of last image]  [...]  [...]  [...]  [...]] ``` And every digit is present as one-hot encoding  for example 0 is ```[1  0  0  0  .... 0]```  2 is```[0  0  1  0  .... 0]```  ```python traincsv = open('./data/train_set/train.csv'  'r'  encoding = 'utf8') read_label = [toonehot(row[1]) for row in csv.reader(traincsv)] train_label = [[] for _ in range(6)] for arr in read_label:     for index in range(6):         train_label[index].append(arr[index]) train_label = [arr for arr in np.asarray(train_label)] ```   Firstly we have to observe the captcha  it's easy to find that the captcha is made up of two primary elements: - ```5 ~ 6 digits``` number and the text size is not same. Furthermore  they are being rotated  and the color is floating. - The color of background is floating  and there have some white and black interference lines  and some of them will overlay on the number.  And more...: - The angle of rotation is between about ```-55 ~ 55 degrees```  and the size is about ```25 ~ 27pt```. - We can found that one number has not only one style  so we guess that there have two fonts randomly in change. The first one obviously is ```Courier New-Bold```  and the second one is ```Times New Roman-Bold```.(You can use software such as Photoshop to cross-comparison.) - About the range of background and text color  we can through the color quantization such as k-means to get color of every background and text  and so we can calculate the color range.(I used k-means in opencv to implement.) - The color range(R/G/B) of the background is between about ```180 ~ 250```  and text is between ```10 ~ 140```. - Those interference lines form a rectangle  they have two styles: left and up sides are black  right and down sides are white  and vice versa.(you can also treat them as be rotated 180 degrees). - The number of the rectangle is between about ```30 ~ 32```  randomly distribute on captcha image  and the width and height is between about ```5 ~ 21px```. Besides  there has 40% white line will overlay on the number  and about 20% by the black line.  With these observation  we are about to generate training set! Now  let's observe where the number place on the image:  ![image](./readme_img/old/5.PNG)![image](./readme_img/old/6.PNG)![image](./readme_img/old/7.PNG)  From these images we can find that the text(number) are not regularly distributed on the image  we can guess that the text is randomly moved left or right after a rotation. There has even some text overlap together  so we can't crop the image and process only one number at a time.  Above is the captcha rule we simply observed. The implement of training set generate is in ```captcha_gen.py```  you can try to implement it in your own way.  ![image](./readme_img/old/8.jpg)  The generator finally will output 50 000 captcha image and a csv labeled answer.  ![image](./readme_img/old/9.PNG)![image](./readme_img/old/10.PNG)    It is not difficult for building a CNN model to solve a captcha  but where and how do we get a labeled training set?  ![image](./readme_img/old/2.jpeg)![image](./readme_img/old/3.jpeg)![image](./readme_img/old/4.jpeg)  We can write a program to crawl thousands of captcha image  and labeled it manually  but it's a time-consuming job! Maybe we can try to generate some captcha image by imitating it. But of course  the image we generate should be really close to the real  otherwise  the accuracy on validation set will really bad.   以6碼的為例，我們現在有75000張驗證碼圖片，我們取其中前60000張為訓練集，後15000張為驗證集來訓練。(驗證集也是一樣，只是改成取後15000張。)  ```python traincsv = open('./data/6_real_train_set/captcha_train.csv'  'r'  encoding = 'utf8') train_data = np.stack([np.array(Image.open(""./data/6_real_train_set/"" + row[0] + "".jpg""))/255.0 for row in csv.reader(traincsv)][:60000]) traincsv = open('./data/6_real_train_set/captcha_train.csv'  'r'  encoding = 'utf8') read_label = [toonehot(row[1]) for row in csv.reader(traincsv)][:60000] train_label = [[] for _ in range(6)] for arr in read_label:     for index in range(6):         train_label[index].append(arr[index]) train_label = [arr for arr in np.asarray(train_label)] ```  另外判斷是5碼or6碼的模型，則是各以5/6碼的前60000張各隨機挑選20000張=40000張當訓練集，剩下15000張各隨機挑選5000張=10000張當驗證集。   驗證集的載入方式跟訓練集相同，這邊略過。   在訓練之前我們要先將資料載入到記憶體中，前面產生訓練集和驗證集的時候，我們是將驗證碼存成一張張編號好的圖片，並用csv檔記錄下了答案。  這邊一樣以6碼的為例，首先我們先處理X的部分，也就是特徵值，這邊就是指我們的圖片。 而要輸入進CNN的資料必須是numpy array的形式，所以我們用Pillow來讀取圖片並轉為numpy格式：  ```python traincsv = open('./data/6_imitate_train_set/captcha_train.csv'  'r'  encoding = 'utf8') for row in csv.reader(traincsv):     image = Image.open(""./data/6_imitate_train_set/"" + row[0] + "".jpg"") #: 讀取圖片     nparr = np.array(image) #: 轉成np array     nparr = nparr / 255.0 ```  這時我們下```nparr.shape```，可以看到矩陣的大小是```(60  200  3)```，跟前面模型設計的Input是相同的。  而我們計劃使用50000張圖片來訓練，所以最後輸入給CNN的矩陣大小會是```(50000  60  200  3)```，這部分只要利用stack就可以把它們合併，整理成下面:  ```python train_data = np.stack([np.array(Image.open(""./data/6_imitate_train_set/"" + row[0] + "".jpg""))/255.0 for row in csv.reader(traincsv)]) ```  最後train_data的shape就會是```(50000  60  200  3)```。  接下來Y則是訓練集的標記，也就是我們訓練集的答案。  因為我們的模型是多輸出的結構(6組softmax函數分類器)，所以Y要是一個含有6個numpy array的list，大概像是這樣： ``` [[第一張第1個數字 ... 最後一張第1個數字]  [第一張第2個數字 ... 最後一張第2個數字]  [...]  [...]  [...]  [...]] ``` 而其中每個數字都是以one-hot encoding表示，例如0就是```[1  0  0  0  .... 0]```，2就是```[0  0  1  0  .... 0]```  ```python traincsv = open('./data/6_imitate_train_set/captcha_train.csv'  'r'  encoding = 'utf8') #: 讀取訓練集的標記 read_label = [toonehot(row[1]) for row in csv.reader(traincsv)] #: 將每一行的文字轉成one-hot encoding train_label = [[] for _ in range(6)] #: 各組輸出的答案要放到train_label  for arr in read_label:     for index in range(6):         train_label[index].append(arr[index]) #: 原本是[[第1字答案  ...  第6字答案] ......  [第1字答案  ...  第6字答案]]                                               #: 要轉成[[第1字答案 ...  第1字答案] ...  [第6字答案 ...  第6字答案]]才符合Y的輸入 train_label = [arr for arr in np.asarray(train_label)] #: 最後要把6個numpy array 放在一個list ```   讓我們來模仿產生一些驗證碼吧！ 首先我們要先觀察驗證碼，你可以寫一支爬蟲程式(eg.```captcha_scrawl.py```)去擷取一兩百張驗證碼回來細細比對。我們不難發現台鐵的驗證碼不外乎由兩個主要元素組成： - ```5 ~ 6碼```的數字及英文(不包含O和I)，大小似乎不一致，而且都有經過旋轉，另外顏色是浮動的。 - 背景是浮動的顏色，另外還有不少干擾的線條，看起來應該是矩形，由黑線和白線組成，且有部分會蓋到數字上面。  進一步研究會發現: - 數字的旋轉角度約在```-55 ~ 55度```間，大小約```25 ~ 27pt```。 - 字型的部分，仔細觀察會發現同一個字會有兩種不一樣的樣式，推測是有兩種字型隨機更替，其中一個很明顯是```Courier New-Bold```，另一個比對一下也不難發現即是```Times New Roman-Bold```。 - 背景和字型顏色的部分，可以用一些色彩均值化的手法快速的從數百張的驗證碼中得出每一張的背景及數字的顏色，進而我們就能算出顏色的範圍。這部分可以用OpenCV的k-means來實作，這邊就不再贅述。  背景的R/G/B範圍約是在```180 ~ 250```間，文字的部分則是```10 ~ 140```間。 - 干擾的線條是矩形，有左、上是黑線條且右、下是白線條和倒過來，共兩種樣式(也可以當作是旋轉180度)，平均大約會出現```30 ~ 32個```隨機分布在圖中，長寬都大約落在```5 ~ 21px```間。 另外，大約有4成的機會白線會蓋在數字上，黑線蓋在文字上的機率則更低。  有了這些觀察，只差一點點就可以產生訓練集了，我們現在來觀察文字都落在圖片上的甚麼位置上:  ![image](./readme_img/captcha_seperate1.png)![image](./readme_img/captcha_seperate2.png)![image](./readme_img/captcha_seperate3.png)  從這幾張圖中不難看出文字並非規則地分布在圖片上，我們可以猜測文字是旋轉後被隨機左移或右移了，甚至還會有重疊的情況，所以沒辦法用切割的方式一次處理一個文字。  以上就是我們簡單觀察到的驗證碼規則，訓練集產生的部分實作在```captcha_gen.py```中，雖然寫得有點雜亂，不過沒甚麼特別的地方，就是照著上面的規則產生，可以試著以自己的方式實作看看。  ![image](./readme_img/captcha_sample4.jpg)  ```python if __name__ == ""__main__"":     generate(50000  ""./data/56_imitate_train_set/""   ENGP=100  FIVEP=50  ENGNOLIMIT=True  filename=""train"")     generate(10240  ""./data/56_imitate_vali_set/""   ENGP=100  FIVEP=50  ENGNOLIMIT=True  filename=""vali"")     generate(50000  ""./data/5_imitate_train_set/""   ENGP=100  FIVEP=100  ENGNOLIMIT=True  filename=""train"")     generate(10240  ""./data/5_imitate_vali_set/""   ENGP=100  FIVEP=100  ENGNOLIMIT=True  filename=""vali"")     generate(50000  ""./data/6_imitate_train_set/""   ENGP=100  FIVEP=0  ENGNOLIMIT=True  filename=""train"")     generate(10240  ""./data/6_imitate_vali_set/""   ENGP=100  FIVEP=0  ENGNOLIMIT=True  filename=""vali"") ```  最後會為我們預計建立的三個CNN(2.2.1會提到)各分別產生50000筆Training data和10240筆Validate data，答案則標記在csv檔中。  ![image](./readme_img/csv.png)![image](./readme_img/generate.png)    要建立一個辨識驗證碼的CNN模型其實並非難事，難的是要如何取得標記好的訓練集呢?  ![image](./readme_img/captcha_sample1.jpg)![image](./readme_img/captcha_sample2.jpg)![image](./readme_img/captcha_sample3.jpg)  在這邊我們會嘗試兩種方法(2.與3.)：  Click here or scroll down for english version   分類驗證碼是5碼 or 6碼的模型(train_cnn_imitate_56)則達到約98.13%。   """;Computer Vision;https://github.com/tiger154/captcha-solver-custom
"""Install PyTorch 0.2   For data setup  run ./getdata.sh.   PyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed.   ``` @InProceedings{P18-1221    author = 	""Parvez  Md Rizwan 		and Chakraborty  Saikat 		and Ray  Baishakhi 		and Chang  Kai-Wei""    title = 	""Building Language Models for Text with Named Entities""    booktitle = 	""Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)""    year = 	""2018""    publisher = 	""Association for Computational Linguistics""    pages = 	""2373--2383""    location = 	""Melbourne  Australia""    url = 	""http://aclweb.org/anthology/P18-1221"" } ```    """;Sequential;https://github.com/uclanlp/NamedEntityLanguageModel
"""1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   pip install -r requirements.txt   Requirements: hardware  Basic installation   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   To train and test a Faster R-CNN detector using the **alternating optimization** algorithm from our NIPS 2015 paper  use `experiments/scripts/faster_rcnn_alt_opt.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  (""alt opt"" refers to the alternating optimization training algorithm described in the NIPS paper.)  To train and test a Faster R-CNN detector using the **approximate joint training** method  use `experiments/scripts/faster_rcnn_end2end.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  This method trains the RPN module jointly with the Fast R-CNN network  rather than alternating between training the two. It results in faster (~ 1.5x speedup) training times and similar detection accuracy. See these [slides](https://www.dropbox.com/s/xtr4yd4i5e0vw8g/iccv15_tutorial_training_rbg.pdf?dl=0) for more details.  Artifacts generated by the scripts in `tools` are written in this directory.  Trained Fast R-CNN networks are saved under:  ``` output/<experiment directory>/<dataset name>/ ```  Test outputs are saved under:  ``` output/<experiment directory>/<dataset name>/<network snapshot name>/ ```  """;Computer Vision;https://github.com/rh01/faster-rcnn
"""To use training/evaluating scripts as well as all models  you need to clone the repository and install dependencies: ``` git clone git@github.com:osmr/imgclsmob.git pip install -r requirements.txt ```   - pytorchcv for PyTorch    - PyTorch models    """;Computer Vision;https://github.com/osmr/imgclsmob
"""![Block](imgs/diagonal_illustration.png)  AdaHessian is a second order based optimizer for the neural network training based on PyTorch. The library supports the training of convolutional neural networks ([image_classification](https://github.com/amirgholami/adahessian/tree/master/image_classification)) and transformer-based models ([transformer](https://github.com/amirgholami/adahessian/tree/master/transformer)). Our TensorFlow implementation is [adahessian_tf](https://github.com/amirgholami/adahessian/tree/master/adahessian_tf).  Please see [this paper](https://arxiv.org/pdf/2006.00719.pdf) for more details on the AdaHessian algorithm.  For more details please see:  - [Video explanation of AdaHessian](https://www.youtube.com/watch?v=S87ancnZ0MM) - [AdaHessian paper](https://arxiv.org/pdf/2006.00719.pdf).   If you are interested to install the library through pip  then we recommend doing so through pytorch-optimizer package as follows:  ``` $ pip install torch_optimizer ```  ```python import torch_optimizer as optim  #: model = ... optimizer = optim.Adahessian(     m.parameters()      lr= 1.0      betas= (0.9  0.999)     eps= 1e-4      weight_decay=0.0      hessian_power=1.0  )       loss_fn(m(input)  target).backward(create_graph = True) #: create_graph=True is necessary for Hessian calculation optimizer.step() ```   Please first clone the AdaHessian library to your local system: ``` git clone https://github.com/amirgholami/adahessian.git ``` You can import the optimizer as follows:  ```python from optim_adahessian import Adahessian ... model = YourModel() optimizer = Adahessian(model.parameters()) ... for input  output in data:   optimizer.zero_grad()   loss = loss_function(output  model(input))   loss.backward(create_graph=True)  #: You need this line for Hessian backprop   optimizer.step() ... ```  Please note that the optim_adahessian is in the image_classification folder. We also have adapted the Adahessian implementation to be compatible with fairseq repo  which can be used for NLP tasks. This is the [link](https://github.com/amirgholami/adahessian/blob/master/transformer/fairseq/optim/adahessian.py) to that version  which can be found in transformer folder.   """;General;https://github.com/amirgholami/adahessian
"""A few steps are required for setting up each individual dataset.   cd ~/code/genesis   git clone https://github.com/deepmind/dsprites-dataset.git data/multi_dsprites/dsprites-dataset   pip install gsutil  cd ~/code/genesis   cd data   tar xvzf shapestacks-mjcf.tar.gz  tar xvzf shapestacks-rgb.tar.gz  cd -   The instance segmentation labels for ShapeStacks can be downloaded from here.   git clone https://github.com/deepmind/deepmind-research.git ~/code/deepmind-research   Make sure the deepmind-research is on your python path:   Create a separate environment according to `deepmind-research/sketchy/requirements.txt`shell   conda deactivate  conda create -n sketchy python=3.7  conda activate sketchy  pip install -r ~/code/deepmind-research/sketchy/requirements.txt   pip install torch==1.3.1 torchvision==0.4.2 tqdm pillow   conda deactivate   You can train Genesis-v2  Genesis  MONet and baseline VAEs on the datasets using the default hyperparameters with  e.g.:   """;General;https://github.com/applied-ai-lab/genesis
"""Navigate to the folder Sentiment-Analysis-with-BERT and run the command below:  pip install -r requirements.txt  Note that pytorch will have some issues of installing. Make sure you install pytorch separately if you have any issues.  If you find any issues/bugs with the code. I am available on email at venkyr91193@gmail.com  using the pytorch implementation with the help of the libary tranformers. (https://huggingface.co/transformers/)   """;Natural Language Processing;https://github.com/venkyr91193/Sentiment-Analysis-with-BERT
"""![](./pictures/decoder_stride_calculation.png)   """;Computer Vision;https://github.com/anandjebakumar/unet
"""Jupyter Notebook   """;Natural Language Processing;https://github.com/PaulSudarshan/Language-Classification-Using-Naive-Bayes-Algorithm
"""Please follow the instructions of py-faster-rcnn [here](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to setup VOC and COCO datasets (Part of COCO is done). The steps involve downloading data and optionally creating soft links in the ``data`` folder. Since faster RCNN does not rely on pre-computed proposals  it is safe to ignore the steps that setup proposals.  If you find it useful  the ``data/cache`` folder created on Xinlei's side is also shared [here](https://drive.google.com/drive/folders/0B1_fAEgxdnvJSmF3YUlZcHFqWTQ).   1. Clone the repository   ```Shell   git clone https://github.com/ruotianluo/pytorch-faster-rcnn.git   ```  2. Install the [Python COCO API](https://github.com/pdollar/coco). The code requires the API to access COCO dataset.   ```Shell   cd data   git clone https://github.com/pdollar/coco.git   cd coco/PythonAPI   make   cd ../../..   ```     - Due to the randomness in GPU training especially for VOC  the best numbers are reported (with 2-3 attempts) here. According to Xinlei's experience  for COCO you can almost always get a very close number (within ~0.2%) despite the randomness.      cd data/imagenet_weights     python #: open python in terminal and run the following Python code     ```Python      cd ../..     For Resnet101  you can set up like:Shell      cd data/imagenet_weights     #: download from my gdrive (link in pytorch-resnet)      cd ../..   For Mobilenet V1  you can set up like:      cd data/imagenet_weights      cd ../..     ./experiments/scripts/train_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/train_faster_rcnn.sh 1 coco res101     ./experiments/scripts/test_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/test_faster_rcnn.sh 1 coco res101   1. Download pre-trained model (only google drive works)   <!-- ```Shell   #: Resnet101 for voc pre-trained on 07+12 set   #: ./data/scripts/fetch_faster_rcnn_models.sh   ```   **Note**: if you cannot download the models through the link  or you want to try more models  you can check out the following solutions and optionally update the downloading script: -->   - ~~Another server [here](http://gs11655.sp.cs.cmu.edu/xinleic/tf-faster-rcnn/).~~   - Google drive [here](https://drive.google.com/open?id=0B7fNdx_jAqhtNE10TDZDbFRuU0E).  **(Optional)** Instead of downloading my pretrained or converted model  you can also convert from tf-faster-rcnn model. You can download the tensorflow pretrained model from [tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn/#demo-and-test-with-pre-trained-models). Then run: ```Shell python tools/convert_from_tensorflow.py --tensorflow_model resnet_model.ckpt  python tools/convert_from_tensorflow_vgg.py --tensorflow_model vgg_model.ckpt ```  This script will create a `.pth` file with the same name in the same folder as the tensorflow model.  2. Create a folder and a soft link to use the pre-trained model   ```Shell   NET=res101   TRAIN_IMDB=voc_2007_trainval+voc_2012_trainval   mkdir -p output/${NET}/${TRAIN_IMDB}   cd output/${NET}/${TRAIN_IMDB}   ln -s ../../../data/voc_2007_trainval+voc_2012_trainval ./default   cd ../../..   ```  3. Demo for testing on custom images   ```Shell   #: at repository root   GPU_ID=0   CUDA_VISIBLE_DEVICES=${GPU_ID} ./tools/demo.py   ```   **Note**: Resnet101 testing probably requires several gigabytes of memory  so if you encounter memory capacity issues  please install it with CPU support only. Refer to [Issue 25](https://github.com/endernewton/tf-faster-rcnn/issues/25).  4. Test with pre-trained Resnet101 models   ```Shell   GPU_ID=0   ./experiments/scripts/test_faster_rcnn.sh $GPU_ID pascal_voc_0712 res101   ```   **Note**: If you cannot get the reported numbers (79.8 on my side)  then probably the NMS function is compiled improperly  refer to [Issue 5](https://github.com/endernewton/tf-faster-rcnn/issues/5).   """;Computer Vision;https://github.com/raphael-cohen/capsnet_for_Faster-RCNN
"""- Download COCO 2017 datasets  modify the paths of training and evalution datasets in `config.py`.  - ```   #: Build DCN  NMS  CUDA FocalLoss.   cd build_stuff   python setup.py build develop   ```  - Download weights and put the weight files in `weights` folder.     1.1 <= PyTorch <= 1.4 (Version > 1.4 will cause a compilation error).  Python >= 3.6.    --score_voting  activate score voting during validation.   : Evaluate COCO val2017 on a specific GPU.   """;Computer Vision;https://github.com/feiyuhuahuo/PAA_minimal
"""Following are the papers:   $ git clone https://github.com/MicroprocessorX069/Comparison-of-DC-GANS-and-SA-GANS.git   """;General;https://github.com/MicroprocessorX069/Comparison-of-DC-GANS-and-SA-GANS
"""We present Sandwich Batch Normalization (SaBN)  an extremely easy improvement of Batch Normalization (BN) with only a few lines of code changes.  ![method](imgs/architect.png)  We demonstrate the prevailing effectiveness of SaBN as a drop-in replacement in four tasks: 1. **conditional image generation**  2. **neural architecture search**  3. **adversarial training**  4. **arbitrary neural style transfer**.   Check each of them for more information: 1. [GAN](https://github.com/VITA-Group/Sandwich-Batch-Normalization/blob/main/GAN) 2. [NAS](https://github.com/VITA-Group/Sandwich-Batch-Normalization/blob/main/NAS) 3. [Adv](https://github.com/VITA-Group/Sandwich-Batch-Normalization/blob/main/Adv) 4. [NST](https://github.com/VITA-Group/Sandwich-Batch-Normalization/blob/main/NST)   """;General;https://github.com/VITA-Group/Sandwich-Batch-Normalization
"""1. clone the repo and use `pip install -r requirements.txt` to setup the operating environment. 2. run following commands ```bash cd gnn python run_gcn_cora.py python run_graphsage_cora.py python run_gat_cora.py ```   """;Graphs;https://github.com/shenweichen/GraphNeuralNetwork
"""installation dependencies:     2. gym_torcs: https://github.com/ugo-nama-kun/gym_torcs   """;Reinforcement Learning;https://github.com/siyuofzhou/DDPG
"""git clone https://github.com/kinimod23/NMT_Project.git  cd ~/NMT_Project/NMT_environment/shell_scripts  Set up the NMT environment:  bash sockeye_wmt_env.sh   bash sockeye_wmt_prep.sh   cd ~/NMT_Project/NLR_pre-training/glove   git init .  git remote add -t \* -f origin http://github.com/stanfordnlp/glove  git checkout master   cd ~/NMT_Project/NMT_environment/shell_scripts  bash sockeye_wmt_create.small.embs.sh   bash sockeye_wmt_train_basel.sh   bash sockeye_wmt_train_small.prembs.sh model_wmt17_small.glove   cd ~/NMT_Project/NMT_environment/shell_scripts  bash sockeye_wmt_create.large.embs.sh   bash sockeye_wmt_train_large.prembs.sh model_wmt17_large.glove   cd ~/NMT_Project/NMT_environment/shell_scripts  bash sockeye_wmt_eval.sh model_wmt17_basel  bash sockeye_wmt_eval.sh model_wmt17_small.glove  bash sockeye_wmt_eval.sh model_wmt17_large.glove   bash sockeye_wmt_prembs.recheck.sh model_wmt17_small.glove &amp;&amp; exit  bash sockeye_wmt_prembs.recheck.sh model_wmt17_large.glove &amp;&amp; exit   mkdir ~/Desktop/recheck_embs  cd ~/Desktop/recheck_embs  wget https://raw.githubusercontent.com/kinimod23/NMT_Project/master/NMT_environment/tools/recheck_embs.sh   bash recheck_embs.sh model_wmt17_basel  bash recheck_embs.sh model_wmt17_large.glove   cd ~/NMT_Project/Signifikanztests       cd ~/NMT_Project/NMT_environment/shell_scripts     bash sockeye_wmt_prep_add.data  Train glove embeddings with previously generated additional BPE training data:      cd ~/NMT_Project/NLR_pre-training/glove      &nbsp;   Natural language Representations (NLRs) might ignore key features of distributional semantics! A new NLR model is typically evaluated across several tasks  and is considered an improvement if it achieves better accuracy than its predecessors. However  different applications rely on different aspects of word embeddings  and good performance in one application does not necessarily imply equally good performance on another.   WMT 2017 Translation Task http://data.statmt.org/wmt17/translation-task/preprocessed/de-en/  ------------------------------------------------------------------------------------------  """;Natural Language Processing;https://github.com/kinimod23/NMT_Project
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/fciannel/bert_fciannel
"""1. Download and Preprocess WMT 14 data      The WMT English to German dataset can be preprocessed using the `prepare-wmt14en2de.sh` script.     By default it will produce a dataset that was modeled after [Attention Is All You Need (Vaswani et al.  2017)](https://arxiv.org/abs/1706.03762)  but with additional news-commentary-v12 data from WMT'17.      To use only data available in WMT'14 or to replicate results obtained in the original [Convolutional Sequence to Sequence Learning (Gehring et al.  2017)](https://arxiv.org/abs/1705.03122) paper  please use the `--icml17` option.      ```bash     #: Download and prepare the data     cd examples/translation/     #: WMT'17 data:     bash prepare-wmt14en2de.sh     #: or to use WMT'14 data:     #: bash prepare-wmt14en2de.sh --icml17      cd  wmt14_en_de     #: cd wmt17_en_de      mkdir ./tmp/esc/      sed -r 's/(@@ )|(@@ ?$)//g' train.en > ./tmp/esc/train.esc.tok.en     sed -r 's/(@@ )|(@@ ?$)//g' valid.en > ./tmp/esc/valid.esc.tok.en     sed -r 's/(@@ )|(@@ ?$)//g' test.en > ./tmp/esc/test.esc.tok.en     ../mosesdecoder/scripts/tokenizer/detokenizer.perl -l en < ./tmp/esc/train.esc.tok.en > ./tmp/esc/train.esc.en     ../mosesdecoder/scripts/tokenizer/detokenizer.perl -l en < ./tmp/esc/valid.esc.tok.en > ./tmp/esc/valid.esc.en     ../mosesdecoder/scripts/tokenizer/detokenizer.perl -l en < ./tmp/esc/test.esc.tok.en > ./tmp/esc/test.esc.en      rm ./tmp/esc/train.esc.tok.en     rm ./tmp/esc/valid.esc.tok.en     rm ./tmp/esc/test.esc.tok.en      ```   2. Perform explicit sentence compression      ```      CUDA_VISIBLE_DEVICES=0 python ./scripts/generate_esc.py --esc_model_path ./pretrain/model/esc_giga/ --esc_max_len_a 0.6 --esc_max_len_b 0 --esc_min_len 5 --input_path ./examples/translation/wmt14_en_de/tmp/esc/train.esc.en --output_path ./examples/translation/wmt14_en_de/tmp/esc/train.en-esc      CUDA_VISIBLE_DEVICES=0 python ./scripts/generate_esc.py --esc_model_path ./pretrain/model/esc_giga/ --esc_max_len_a 0.6 --esc_max_len_b 0 --esc_min_len 5 --input_path ./examples/translation/wmt14_en_de/tmp/esc/valid.esc.en --output_path ./examples/translation/wmt14_en_de/tmp/esc/valid.en-esc       CUDA_VISIBLE_DEVICES=0 python ./scripts/generate_esc.py --esc_model_path ./pretrain/model/esc_giga/ --esc_max_len_a 0.6 --esc_max_len_b 0 --esc_min_len 5 --input_path ./examples/translation/wmt14_en_de/tmp/esc/test.esc.en --output_path ./examples/translation/wmt14_en_de/tmp/esc/test.en-esc      BPEROOT=subword-nmt/subword_nmt      python $BPEROOT/apply_bpe.py -c ./wmt14_en_de/code < ./wmt14_en_de/tmp/esc/train.en-esc > ./wmt14_en_de/tmp/esc/bpe.train.en-esc     python $BPEROOT/apply_bpe.py -c ./wmt14_en_de/code < ./wmt14_en_de/tmp/esc/valid.en-esc > ./wmt14_en_de/tmp/esc/bpe.valid.en-esc     python $BPEROOT/apply_bpe.py -c ./wmt14_en_de/code < ./wmt14_en_de/tmp/esc/test.en-esc > ./wmt14_en_de/tmp/esc/bpe.test.en-esc      cp ./wmt14_en_de/tmp/esc/bpe.train.en-esc ./wmt14_en_de/train.en-esc     cp ./wmt14_en_de/tmp/esc/bpe.valid.en-esc ./wmt14_en_de/valid.en-esc     cp ./wmt14_en_de/tmp/esc/bpe.test.en-esc ./wmt14_en_de/test.en-esc      ```  3. Binarize the dataset  ``` TEXT=./examples/translation/wmt14_en_de python fairseq_cli/multicontext_preprocess.py --source-lang en --target-lang de --source-context en-esc --trainpref $TEXT/train --validpref $TEXT/valid --destdir data-bin/wmt14_en_de_esc --thresholdtgt 0 --thresholdsrc 0 --joined-dictionary --workers 20 ```                   --save-interval-updates 1000 --keep-interval-updates 200 --max-update ${UPDATES} > $LOG_PATH   """;General;https://github.com/bcmi220/esc4nmt
"""**T-TA**  or **T**ransformer-based **T**ext **A**utoencoder   is a new deep bidirectional language model for unsupervised learning tasks. T-TA learns the straightforward learning objective  *language autoencoding*  which is to predict all tokens in a sentence at once using only their context. Unlike ""masked language model""  T-TA has *self-masking* mechanism in order to avoid merely copying the input to output. Unlike BERT (which is for fine-tuning the entire pre-trained model)  T-TA is especially beneficial to obtain contextual embeddings   which are fixed representations of each input token generated from the hidden layers of the trained language model.  T-TA model architecture is based on the [BERT](https://arxiv.org/abs/1810.04805) model architecture  which is mostly a standard [Transformer](https://arxiv.org/abs/1706.03762) architecture. Our code is based on [Google's BERT github](https://github.com/google-research/bert)  which includes methods for building customized vocabulary  preparing the Wikipedia dataset  etc.    We release the *pre-processed* librispeech text-only data (1.66 GB tar.gz file). In this corpus  each line is a single sentence   so we use the sentence unit (rather than the paragraph unit) for a training instance. The original data can be found in [LibriSpeech-LM](http://www.openslr.org/11/).  ```shell cd data wget http://milabfile.snu.ac.kr:16000/tta/corpus.librispeech-lower.sub-32k.tar.gz tar -xvzf corpus.librispeech-lower.sub-32k.tar.gz cd .. ``` Then  `corpus-eval.librispeech-lower.sub-32k.txt` and  `corpus-train.librispeech-lower.sub-32k.txt` will be appear in `data/` folder.   After getting the pre-processed plain text data  we make tfrecords (it takes some time for creating tfrecords of train data):  ```shell rm tfrecords/tta-librispeech-lower-sub-32k #: delete dummy (symbolic link)  python create_tfrecords.py \     --input_file data/corpus-eval.librispeech-lower.sub-32k.txt \     --vocab_file configs/vocab-lower.sub-32k.txt \     --output_file tfrecords/tta-librispeech-lower-sub-32k/eval.tfrecord \     --num_output_split 1  python create_tfrecords.py \     --input_file data/corpus-train.librispeech-lower.sub-32k.txt \     --vocab_file configs/vocab-lower.sub-32k.txt \     --output_file tfrecords/tta-librispeech-lower-sub-32k/train.tfrecord ```    cd models   cd ..   For running this code  you may need several python packages: numpy  scipy  and sklearn.   cd data   tar -xvzf Stsbenchmark.tar.gz  cd ..   ```shell git clone https://github.com/joongbo/tta.git cd tta ```    """;General;https://github.com/joongbo/tta
"""PyTorch (tested on v1.7.1)  Python 3.7+ & dependencies (requirements.txt)    pip install -r requirements.txt   * Python 3.7+ & dependencies (requirements.txt)   For now you need to install the Python dependencies specified in requirements.txt (look above)   """;General;https://github.com/lukas-blecher/LaTeX-OCR
"""I've been searching for a Tensorflow implementation of YOLOv2 for a while but the darknet version and derivatives are not really easy to understand. This one is an hopefully easier-to-understand version of Tiny YOLOv2. The weight extraction  weights structure  weight assignment  network  inference and postprocessing are made as simple as possible.  The output of this implementation on the test image ""dog.jpg"" is the following:  ![alt text](https://github.com/simo23/tinyYOLOv2/blob/master/dog_output.jpg ""YOLOv2 output"")  Just to be clear  this implementation is called ""tiny-yolo-voc"" on pjreddie's site and can be found here:  ![alt text](https://github.com/simo23/tinyYOLOv2/blob/master/pjsite.png ""YOLOv2 site"")   The code runs at ~15fps on my laptop which has a 2GB Nvidia GeForce GTX 960M GPU   - Clone the project and place it where you want - Download the binary file (~60MB) from pjreddie's site: https://pjreddie.com/media/files/yolov2-tiny-voc.weights and place it into the folder where the scripts are - Launch test.py or test_webcam.py. Change the input_img_path and the weights_path in the main if you want  now the network has ""dog.jpg"" as input_img. The code is now configured to run with weights and input image in the same folder as the script.  ```python python3 test.py ```  - If you are launching them for the first time  the weights will be extracted from the binary file and a ckpt will be created. Next time only the ckpt will be used!   I've been struggling on understanding how the binary weights file was written. I hope to save you some time by explaining how I imported the weights into a Tensorflow network:  - Download the binary file from pjreddie's site: https://pjreddie.com/media/files/yolov2-tiny-voc.weights  - Extract the weights from binary to a numpy float32 array with  weight_array = np.fromfile(weights_path  dtype='f') - Delete the first 4 numbers because they are not relevant - Define a function ( load_conv_layer ) to take a part of the array and assign it to the Tensorflow variables of the net - IMPORTANT: the weights order is [ 'biases' 'gamma' 'moving_mean' 'moving_variance' 'kernel']  - IMPORTANT: the 'biases' here refer to the beta value of the Batch Normalization. It does not refer to the biases that must be added after the conv2d because they are set all to zero! ( According to the paper by Ioffe et al. https://arxiv.org/abs/1502.03167 )  - IMPORTANT: the kernel weights are written in Caffe style which means they have shape = (out_dim  in_dim  height  width). They must be converted into Tensorflow style which has shape = (height  width  in_dim  out_dim) - IMPORTANT: in order to obtain the correct results from the weights they need to be DENORMALIZED according to Batch Normalization. It can be done in two ways: define the network with Batch Normalization and use the weights as they are OR define the net without BN ( this implementation ) and DENORMALIZE the weights. ( details are in weights_loader.py ) - In order to verify that the weights extraction is succesfull  I check the total number of params with the number of weights into the weight file. They are both 15867885 in my case.   Another key point is how the predictions tensor is made. It is a 13x13x125 tensor. To process it better:  - Convert the tensor to have shape = 13x13x5x25 = grid_cells x n_boxes_in_each_cell x n_predictions_for_each_box - The 25 predictions are: 2 coordinates and 2 shape values (x y h w)  1 Objectness score  20 Class scores - Now access to the tensor in an easy way! E.g. predictions[row  col  b  :4] will return the 2 coords and shape of the ""b"" B-Box which is in the [row col] grid cell - They must be postprocessed according to the parametrization of YOLOv2. In my implementation it is made like this:   ```python  #: Pre-defined anchors shapes! #: They are not coordinates of the boxes  they are height and width of the 5 anchors defined by YOLOv2 anchors = [1.08 1.19   3.42 4.41   6.63 11.38   9.42 5.11   16.62 10.52] image_height = image_width = 416 n_grid_cells = 13 n_b_boxes = 5  for row in range(n_grid_cells):   for col in range(n_grid_cells):     for b in range(n_b_boxes):        tx  ty  tw  th  tc = predictions[row  col  b  :5]              #: IMPORTANT: (416) / (13) = 32! The coordinates and shape values are parametrized w.r.t center of the grid cell       #: They are parameterized to be in [0 1] so easier for the network to predict and learn       #: With the iterations on every grid cell at [row col] they return to their original positions              #: The x y coordinates are: (pre-defined coordinates of the grid cell [row col] + parametrized offset)*32        center_x = (float(col) + sigmoid(tx)) * 32.0       center_y = (float(row) + sigmoid(ty)) * 32.0        #: Also the width and height must return to the original value by looking at the shape of the anchors       roi_w = np.exp(tw) * anchors[2*b + 0] * 32.0       roi_h = np.exp(th) * anchors[2*b + 1] * 32.0              #: Compute the final objectness score (confidence that there is an object in the B-Box)        final_confidence = sigmoid(tc)        class_predictions = predictions[row  col  b  5:]       class_predictions = softmax(class_predictions)        ```  YOLOv2 predicts parametrized values that must be converted to full size by multiplying them by 32! You can see other EQUIVALENT ways to do this but this one works fine. I've seen someone who  instead of multiplying by 32  divides by 13 and then multiplies by 416 which at the end equals a single multiplication by 32.    """;General;https://github.com/simo23/tinyYOLOv2
"""Python     Numpy      Pytorch     For installing all the requirements just run the requirements.txt file using the following command:  pip3 install -r requirements.txt   requirements.txt - Provides ready to install all the required libraries for running the implementation.   """;Reinforcement Learning;https://github.com/hemilpanchiwala/Dueling_Network_Architectures
"""```python from mlp_mixer import MlpMixer mlp_mixer=MlpMixer(num_classes=1000 num_blocks=10 patch_size=10 tokens_hidden_dim=32 channels_hidden_dim=1024 tokens_mlp_dim=16 channels_mlp_dim=1024) input=torch.randn(50 3 40 40) output=mlp_mixer(input) print(output.shape)  ```    """;Computer Vision;https://github.com/xmu-xiaoma666/MLP-Mixer-pytorch
"""This project is dedicated to unsupervised methods of text data transformation. The representation of text in such transformed form is inherent part of Natural Language Processing (NLP) and allows to consume unstructured data by various deep learning algorithms.  If NLP is a new term for you and sounds a bit mysterious  Yoav Goldberg in his [book](https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984) describes it as:  >_Natural language processing (NLP) is a collective term referring to automatic computational processing of human languages. This includes both algorithms that take human-produced text as input  and algorithms that produce natural looking text as outputs._  NLP embraces wide range of tasks e.g. named entity recognition  sentiment analysis  automatic summarization  natural language generation and many more. Each one is a subject for separate project and could be thoroughly studied.    Two tested methods in this project are **Unsupervised Word Segmentation into Subword Units** and **GloVe embeddings trained on our own corpus**.  I intentionally choose Polish language text to be analyzed  what I elaborate below  but be aware that the methods used are applicable to any language and text data.    If you wish to run code yourself  the easiest way is to use **Google Colab**. It provides ready to use enviroment with free GPU  python  keras and all packages already configured to run this code.  Here's how you can use it:  1. Open [https://colab.research.google.com](https://colab.research.google.com) click **Sign in** in the upper right corner  use your Google credentials to sign in. 2. Click **GITHUB** tab  paste https://github.com/ksulima/Unsupervised-method-to-NPL-Polish-language and press Enter 3. Choose the notebook you want to open  e.g. 01TextCleaning.ipynb 4. Click **File -> Save a copy in Drive...** to save your progress in Google Drive 5. If you need a GPU  click **Runtime -> Change runtime type** and select **GPU** in Hardware accelerator box 6. Download dataset from my [google drive](https://drive.google.com/drive/folders/1F41MZVPitnya9xE4goWDpw_wVHqqNxLG) or original source (described in paragraph Dataset) and upload it to your google drive. Files should be in directory according to **01TextCleaning.ipynb.**      """;Natural Language Processing;https://github.com/ksulima/Unsupervised-method-to-NPL-Polish-language
"""  bash       bash   """;General;https://github.com/Liut2016/ecg-supcontrast
"""This work uses two publicly available databases: KonIQ-10k [KonIQ-10k: An ecologically valid database for deep learning of blind image quality assessment](https://ieeexplore.ieee.org/document/8968750) by V. Hosu  H. Lin  T. Sziranyi  and D. Saupe;  and LIVE-wild [Massive online crowdsourced study of subjective and objective picture quality](https://ieeexplore.ieee.org/document/7327186) by D. Ghadiyaram  and A.C. Bovik  1) The two databases were merged  and then split to training and testing sets. Please see README in databases for details. 2) Make MOS files (note: do NOT include head line):      For database with score distribution available  the MOS file is like this (koniq format):     ```         image path  voter number of quality scale 1  voter number of quality scale 2  voter number of quality scale 3  voter number of quality scale 4  voter number of quality scale 5  MOS or Z-score         10004473376.jpg 0 0 25 73 7 3.828571429         10007357496.jpg 0 3 45 47 1 3.479166667         10007903636.jpg 1 0 20 73 2 3.78125         10009096245.jpg 0 0 21 75 13 3.926605505     ```      For database with standard deviation available  the MOS file is like this (live format):     ```         image path  standard deviation  MOS or Z-score         t1.bmp 18.3762 63.9634         t2.bmp 13.6514 25.3353         t3.bmp 18.9246 48.9366         t4.bmp 18.2414 35.8863     ```      The format of MOS file ('koniq' or 'live') and the format of MOS or Z-score ('mos' or 'z_score') should also be specified in misc/imageset_handler/get_image_scores. 3) In the train script in train/train_triq.py the folders containing training and testing images are provided. 4) Pretrained ImageNet weights can be downloaded (see README in.\pretrained_weights) and pointed to in the train script.       args['image_aug'] = True      args['gpu'] = 0   1) Clone this repository. 2) Install required Python packages. The code is developed by PyCharm in Python 3.7. The requirements.txt document is generated by PyCharm  and the code should also be run in latest versions of the packages.       args['result_folder'] = r'..\databases\experiments'      args['lr_base'] = 1e-4 / 2   """;Computer Vision;https://github.com/junyongyou/triq
""" |Backbone|Detectors|AP|AP<sub>50</sub>|AP<sub>75</sub>|AP<sub>S</sub>|AP<sub>M</sub>|AP<sub>L</sub>|Weights| |:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:| |ResNet-50 + Triplet Attention (k = 7)|Mask RCNN|**35.8**|**57.8**|**38.1**|**18**|**38.1**|**50.7**|[Google Drive](https://drive.google.com/file/d/18hFlWdziAsK-FB_GWJk3iBRrtdEJK7lf/view?usp=sharing)|    ``` usage: train_imagenet.py  [-h] [--arch ARCH] [-j N] [--epochs N] [--start-epoch N] [-b N]                           [--lr LR] [--momentum M] [--weight-decay W] [--print-freq N]                           [--resume PATH] [-e] [--pretrained] [--world-size WORLD_SIZE]                           [--rank RANK] [--dist-url DIST_URL]                           [--dist-backend DIST_BACKEND] [--seed SEED] [--gpu GPU]                           [--multiprocessing-distributed]                           DIR  PyTorch ImageNet Training  positional arguments:   DIR                   path to dataset  optional arguments:   -h  --help            show this help message and exit   --arch ARCH  -a ARCH  model architecture: alexnet | densenet121 |                         densenet161 | densenet169 | densenet201 |                         resnet101 | resnet152 | resnet18 | resnet34 |                         resnet50 | squeezenet1_0 | squeezenet1_1 | vgg11 |                         vgg11_bn | vgg13 | vgg13_bn | vgg16 | vgg16_bn | vgg19                         | vgg19_bn (default: resnet18)   -j N  --workers N     number of data loading workers (default: 4)   --epochs N            number of total epochs to run   --start-epoch N       manual epoch number (useful on restarts)   -b N  --batch-size N  mini-batch size (default: 256)  this is the total                         batch size of all GPUs on the current node when using                         Data Parallel or Distributed Data Parallel   --lr LR  --learning-rate LR                         initial learning rate   --momentum M          momentum   --weight-decay W  --wd W                         weight decay (default: 1e-4)   --print-freq N  -p N  print frequency (default: 10)   --resume PATH         path to latest checkpoint (default: none)   -e  --evaluate        evaluate model on validation set   --pretrained          use pre-trained model   --world-size WORLD_SIZE                         number of nodes for distributed training   --rank RANK           node rank for distributed training   --dist-url DIST_URL   url used to set up distributed training   --dist-backend DIST_BACKEND                         distributed backend   --seed SEED           seed for initializing training.   --gpu GPU             GPU id to use.   --multiprocessing-distributed                         Use multi-processing distributed training to launch N                         processes per node  which has N GPUs. This is the                         fastest way to use PyTorch for either single node or                         multi node data parallel training ```   """;General;https://github.com/landskape-ai/triplet-attention
"""Simply run `lumi --help`.   First  clone the repo on your machine and then install with `pip`:  ```bash git clone https://github.com/tryolabs/luminoth.git cd luminoth pip install -e . ```   Just install from PyPI:  ```bash pip install luminoth ```  Optionally  Luminoth can also install TensorFlow for you if you install it with `pip install luminoth[tf]` or `pip install luminoth[tf-gpu]`  depending on the version of TensorFlow you wish to use.   Luminoth currently supports Python 2.7 and 3.4–3.6.   If you wish to train using Google Cloud ML Engine  the optional dependencies must be installed:  pip install luminoth[gcloud]   By default summary and graph logs are saved to jobs/ under the current directory. You can use TensorBoard by running:  tensorboard --logdir path/to/jobs   There is one main command line interface which you can use with the `lumi` command. Whenever you are confused on how you are supposed to do something just type:  `lumi --help` or `lumi <subcommand> --help`  and a list of available options with descriptions will show up.   """;Computer Vision;https://github.com/tryolabs/luminoth
"""```python from lsuv import lsuv_init  model = lsuv_init(ResNet34()  train_loader  needed_std=1.0  std_tol=0.1                    max_attempts=10  do_orthonorm=True  device=device) ```   """;General;https://github.com/shunk031/LSUV.pytorch
"""- Download dataset for training. This can be any wav files with sample rate 22050Hz. (e.g. LJSpeech was used in paper) - preprocess: `python preprocess.py -c config/default.yaml -d [data's root path]` - Edit configuration `yaml` file       vocoder = vocoder.cuda()      mel = mel.cuda()   """;Audio;https://github.com/seungwonpark/melgan
"""In this project we are trying to build a system which can parse clinical discharge notes and generate information about clinical events along with temporal information about their occurrence. In  phase 1 of the project we establish a foundation for that by treating clinical events and temporal information as individual clinical entities and extracting those by performing Named Entity Recognition (NER) using fine-tuned NCBI BERT model. In phase 2 of the project  we take inspiration from the paper KG-BERT to train a model using the word embedding representations generated from phase-1 fine-tuned model. We observe that the model do not perform well on test data. Thus we analyze the nature of the relationships and the model behavior to find out possible reasons of the poor performance.  Then we decide to fine-tune NCBI BERT for relation extraction task. For that we format the input in a specific way so that the model can perform sequence classification successfully using Transformer's attention patterns and BERT special tokens. We evaluate the model on test data and see significant improvement. We dockerise the solution along with the Flask application developed in phase-1 used to visualise the tagged data.   Though the fine-tuned NCBI-BERT model perform well on test data  there exists a major challenge related to usability of the model. So far the model is tested on relevant entity pairs for which TLINKs are available. The way of finding such relevant entity pairs from the available set of entities is yet to be found.  Another area with scope of improvement is to augment data using the nature of relationships to improve the model behavior. For example  if event A is known to happen before patient admission  it means it also occurs before all other events which occurred after admission. Also the input to BERT while fine-tuning can be improved. Our experiment can be treated as a baseline for future research and development in this area.                                           Image source(https://towardsdatascience.com/)   """;Natural Language Processing;https://github.com/ManasRMohanty/DS5500-capstone
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/RuiLiFeng/invGAN
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have installed: MSVS 2015/2017  CUDA 10  cuDNN 7.x  OpenCV 3.x. Then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio 2017 Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv #:replace with opencv[cuda] in case you want to use cuda-accelerated openCV   Build with the Powershell script build.ps1. If you want to use Visual Studio  you will find a custom solution created for you by CMake after the build containing all the appropriate config flags for your system.  If you have MSVS 2015  CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/kongzhiyou/darknet-master
"""1. Edit hyperparameters in `main.py`  2. Train the model  ```bash $ python train.py ```  3. Logs will be collected in specified folder. You can use `tensorboard` to view them in a browser ```bash $ tensorboard --logdir ./logs/ ```  4. Test the model ```bash $ python test.py ```  Example  ```bash $ python test.py -pp ./models/IMPALA_RacecarBulletEnv-v0_400.pt -hd 32 -en RacecarBulletEnv-v0 -ne 10 -el 1000 -ld ./logs/ ```     """;Reinforcement Learning;https://github.com/threewisemonkeys-as/torched_impala
"""./compile.sh #Compile cython code   """;General;https://github.com/adefazio/point-saga
"""bash run.sh train ConE wn18rr 0 1 1024 50 500 10 0.5 0.001 40000 4 -de \   bash run.sh category ConE wn18rr 0 1 1024 50 500 0.1 0.5 0.001 20000 4 -de \   Knowledge Graph Data:  - `entities.dict`: a dictionary mapping entities to unique ids  - `relations.dict`: a dictionary mapping relations to unique ids  - `train.txt`: the KGE model is trained to fit this data set  - `valid.txt`: create a blank file if no validation data is available  - `test.txt`: the KGE model is evaluated on this data set  - `relation_category.txt`: a dictionary mapping relations to their type (1-1 indicates non-hierarchical  1-M indicates hyponym  M-1 indicates hypernym)  required for ConE model  - `class_test_X.txt`: Test data for ancestor-descendant prediction task  *X*=easy: 0% inferred descendant pairs  *X*=medium: 50% inferred descendant pairs  *X*=hard: 100% inferred descendant pairs  - `lca_test_X.txt`: LCA prediction under *X*-hop is evaluated on this data set   """;Graphs;https://github.com/snap-stanford/ConE
"""* Tensorflow - version 2.0+. * Keras - with tensorflow in the backend & importing layers and model. * NumPy - Mathematical computations. * PIL - Image manipulation. * Matplotlib - For the plotting of images. * tfutils - Utility for Keras and Tensorflow.   ```sh #:Installation - tfutils $ pip3 install git+https://github.com/am1tyadav/tfutils.git ```   """;Computer Vision;https://github.com/nikitakodkany/DCGAN-Implementation
"""Topological quantum error correcting codes  and in particular the surface code  currently provide the most promising path to scalable fault tolerant quantum computation. While a variety of decoders exist for such codes  recently decoders obtained via machine learning techniques have attracted attention due to both their potential flexibility  with respect to codes and noise models  and their potentially fast run times. Here  we demonstrate how reinforcement learning techniques  and in particular deepQ learning  can be utilized to solve this problem and obtain such decoders.   Now that we have specified all the required parameters we can instantiate our environment:   2) Run the command ""bash make_executable.sh"". This will allow the controller - a python script which will be run periodically to control the training process - to submit jobs via slurm.  3) Using vim or some other in-terminal editor  modify the following in Controller.py:   - simulation_script.sh   - run the command ""watch -n interval_in_seconds python Controller.py""   - We are now looking at the watch output - kill this with ctrl+c   """;Reinforcement Learning;https://github.com/R-Sweke/DeepQ-Decoding
"""- [x] CUDA version of Feature Refinement Module (FRM) as PyTorch extension   python setup.py install  sh rtools/train.sh   4. Have fun with sh rtools/train.sh and watch the model train!  sh rtools/test.sh   """;General;https://github.com/SJTU-Thinklab-Det/r3det-on-mmdetection
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   This is a Github template repo you can use to create your own copy of the forked StyleGAN2 sample from NVLabs.   There are a few prerequisites you will need to have in place:   4. Create a github connected project by opening the Projects tab  pressing the ""LINK A GITHUB REPO"" button  and following the instructions on the next page.  5. You next need to authorize Gradient to access your github account if you have not done so already.   """;General;https://github.com/thomaspool97/StyleGAN2
"""Vega is an AutoML algorithm tool chain developed by Noah's Ark Laboratory  the main features are as follows:  1. Full pipeline capailities: The AutoML capabilities cover key functions such as Hyperparameter Optimization  Data Augmentation  Network Architecture Search (NAS)  Model Compression  and Fully Train. These functions are highly decoupled and can be configured as required  construct a complete pipeline. 2. Industry-leading AutoML algorithms: Provides Noah's Ark Laboratory's self-developed **[industry-leading algorithm (Benchmark)](./docs/benchmark.md)** and **[Model Zoo](./docs/model_zoo.md)** to download the state-of-the-art (SOTA) models. 3. Fine-grained network search space: The network search space can be freely defined  and rich network architecture parameters are provided for use in the search space. The network architecture parameters and model training hyperparameters can be searched at the same time  and the search space can be applied to Pytorch  TensorFlow and MindSpore. 4. High-concurrency neural network training capability: Provides high-performance trainers to accelerate model training and evaluation. 5. Multi-Backend: PyTorch (GPU and Ascend 910)  TensorFlow (GPU and Ascend 910)  MindSpore (Ascend 910). 6. Ascend platform: Search and training on the Ascend 910 and model evaluation on the Ascend 310.   Run the following commands to install Vega and related open-source software:  ```bash pip3 install --user --upgrade noah-vega ```  If you need to install the Ascend 910 training environment  please contact us.   Run the `vega` command to run the Vega application. For example  run the following command to run the `CARS` algorithm:  ```bash vega ./examples/nas/cars/cars.yml ```  The `cars.yml` file contains definitions such as pipeline  search algorithm  search space  and training parameters. Vega provides more than 40 examples for reference: [Examples](https://github.com/huawei-noah/vega/tree/master/examples)  [Example Guide](./docs/en/user/examples.md)  and [Configuration Guide](./docs/en/user/config_reference.md).   """;General;https://github.com/huawei-noah/vega
"""NEZHA-PyTorch is the PyTorch version of NEZHA.   """;General;https://github.com/huawei-noah/Pretrained-Language-Model
"""This is the official code of [high-resolution representations for Semantic Segmentation](https://arxiv.org/abs/1904.04514).  We augment the HRNet with a very simple segmentation head shown in the figure below. We aggregate the output representations at four different resolutions  and then use a 1x1 convolutions to fuse these representations. The output representations is fed into the classifier. We evaluate our methods on three datasets  Cityscapes  PASCAL-Context and LIP.  <!-- ![](figures/seg-hrnet.png) --> <figure>   <text-align: center;>   <img src=""./figures/seg-hrnet.png"" alt=""hrnet"" title="""" width=""900"" height=""150"" /> </figcaption> </figure>  Besides  we further combine HRNet with [Object Contextual Representation](https://arxiv.org/pdf/1909.11065.pdf) and achieve higher performance on the three datasets. The code of HRNet+OCR is contained in this branch. We illustrate the overall framework of OCR in the Figure and the equivalent Transformer pipelines:  <figure>   <text-align: center;>   <img src=""./figures/OCR.PNG"" alt=""OCR"" title="""" width=""900"" height=""200"" /> </figure>     <figure>   <text-align: center;>   <img src=""./figures/SegmentationTransformerOCR.png"" alt=""Segmentation Transformer"" title="""" width=""600"" /> </figure>   You need to download the [Cityscapes](https://www.cityscapes-dataset.com/)  [LIP](http://sysu-hcp.net/lip/) and [PASCAL-Context](https://cs.stanford.edu/~roozbeh/pascal-context/) datasets.  Your directory tree should be look like this: ````bash $SEG_ROOT/data ├── cityscapes │   ├── gtFine │   │   ├── test │   │   ├── train │   │   └── val │   └── leftImg8bit │       ├── test │       ├── train │       └── val ├── lip │   ├── TrainVal_images │   │   ├── train_images │   │   └── val_images │   └── TrainVal_parsing_annotations │       ├── train_segmentations │       ├── train_segmentations_reversed │       └── val_segmentations ├── pascal_ctx │   ├── common │   ├── PythonAPI │   ├── res │   └── VOCdevkit │       └── VOC2010 ├── cocostuff │   ├── train │   │   ├── image │   │   └── label │   └── val │       ├── image │       └── label ├── ade20k │   ├── train │   │   ├── image │   │   └── label │   └── val │       ├── image │       └── label ├── list │   ├── cityscapes │   │   ├── test.lst │   │   ├── trainval.lst │   │   └── val.lst │   ├── lip │   │   ├── testvalList.txt │   │   ├── trainList.txt │   │   └── valList.txt ````   1. For LIP dataset  install PyTorch=0.4.1 following the [official instructions](https://pytorch.org/). For Cityscapes and PASCAL-Context  we use PyTorch=1.1.0. 2. `git clone https://github.com/HRNet/HRNet-Semantic-Segmentation $SEG_ROOT` 3. Install dependencies: pip install -r requirements.txt  If you want to train and evaluate our models on PASCAL-Context  you need to install [details](https://github.com/zhanghang1989/detail-api). ````bash pip install git+https://github.com/zhanghang1989/detail-api.git#:subdirectory=PythonAPI ````   The PyTroch 1.1 version ia available here.  The PyTroch 0.4.1 version is available here.   You can download the pretrained models from  https://github.com/HRNet/HRNet-Image-Classification. Slightly different  we use align_corners = True for upsampling in HRNet.   <!-- **Note** We reproduce HRNet+OCR results on COCO-Stuff dataset with PyTorch 0.4.1. -->   <!-- **Note** We reproduce HRNet+OCR results on ADE20K dataset with PyTorch 0.4.1. -->   : For PyTorch 0.4.1  : For PyTorch 1.1.0   Note that we only reproduce HRNet+OCR on LIP dataset using PyTorch 0.4.1. So we recommend to use PyTorch 0.4.1 if you want to train on LIP dataset.   """;Computer Vision;https://github.com/HRNet/HRNet-Semantic-Segmentation
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   Install <a href=""https://www.tensorflow.org/get_started/os_setup"" target=""_blank"">TensorFlow</a>. You may also need to install h5py. The code has been tested with Python 2.7  TensorFlow 1.0.1  CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04.  If you are using PyTorch  you can find a third-party pytorch implementation <a href=""https://github.com/fxia22/pointnet.pytorch"" target=""_blank"">here</a>.  To install h5py for Python: ```bash sudo apt-get install libhdf5-dev sudo pip install h5py ```   Created by <a href=""http://charlesrqi.com"" target=""_blank"">Charles R. Qi</a>  <a href=""http://ai.stanford.edu/~haosu/"" target=""_blank"">Hao Su</a>  <a href=""http://cs.stanford.edu/~kaichun/"" target=""_blank"">Kaichun Mo</a>  <a href=""http://geometry.stanford.edu/member/guibas/"" target=""_blank"">Leonidas J. Guibas</a> from Stanford University.  ![prediction example](https://github.com/charlesq34/pointnet/blob/master/doc/teaser.png)   cd part_seg  sh download_data.sh   To train a model to classify point clouds sampled from 3D shapes:      python train.py  Log files and network parameters will be saved to `log` folder in default. Point clouds of <a href=""http://modelnet.cs.princeton.edu/"" target=""_blank"">ModelNet40</a> models in HDF5 files will be automatically downloaded (416MB) to the data folder. Each point cloud contains 2048 points uniformly sampled from a shape surface. Each cloud is zero-mean and normalized into an unit sphere. There are also text files in `data/modelnet40_ply_hdf5_2048` specifying the ids of shapes in h5 files.  To see HELP for the training script:      python train.py -h  We can use TensorBoard to view the network architecture and monitor the training progress.      tensorboard --logdir log  After the above training  we can evaluate the model and output some visualizations of the error cases.      python evaluate.py --visu  Point clouds that are wrongly classified will be saved to `dump` folder in default. We visualize the point cloud by rendering it into three-view images.  If you'd like to prepare your own data  you can refer to some helper functions in `utils/data_prep_util.py` for saving and loading HDF5 files.   * <a href=""http://stanford.edu/~rqi/pointnet2/"" target=""_blank"">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a> by Qi et al. (NIPS 2017) A hierarchical feature learning framework on point clouds. The PointNet++ architecture applies PointNet recursively on a nested partitioning of the input point set. It also proposes novel layers for point clouds with non-uniform densities. * <a href=""http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w13/Engelmann_Exploring_Spatial_Context_ICCV_2017_paper.pdf"" target=""_blank"">Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</a> by Engelmann et al. (ICCV 2017 workshop). This work extends PointNet for large-scale scene segmentation. * <a href=""https://arxiv.org/abs/1710.04954"" target=""_blank"">PCPNET: Learning Local Shape Properties from Raw Point Clouds</a> by Guerrero et al. (arXiv). The work adapts PointNet for local geometric properties (e.g. normal and curvature) estimation in noisy point clouds. * <a href=""https://arxiv.org/abs/1711.06396"" target=""_blank"">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a> by Zhou et al. from Apple (arXiv) This work studies 3D object detection using LiDAR point clouds. It splits space into voxels  use PointNet to learn local voxel features and then use 3D CNN for region proposal  object classification and 3D bounding box estimation. * <a href=""https://arxiv.org/abs/1711.08488"" target=""_blank"">Frustum PointNets for 3D Object Detection from RGB-D Data</a> by Qi et al. (arXiv) A novel framework for 3D object detection with RGB-D data. The method proposed has achieved first place on KITTI 3D object detection benchmark on all categories (last checked on 11/30/2017).  """;Computer Vision;https://github.com/LONG-9621/PointNet
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   This is a Github template repo you can use to create your own copy of the forked StyleGAN2 sample from NVLabs.   There are a few prerequisites you will need to have in place:   4. Create a github connected project by opening the Projects tab  pressing the ""LINK A GITHUB REPO"" button  and following the instructions on the next page.  5. You next need to authorize Gradient to access your github account if you have not done so already.   """;General;https://github.com/sahidrahmanxx12/TEMPEK
"""Instructions for acquiring PTB and WT2 can be found here. While CIFAR-10 can be automatically downloaded by torchvision  ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions here.   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/flymin/darts
"""- Playground Environment code base: https://github.com/flowersteam/playground_env   The demo script is /src/imagine/experiments/play.py. It can be used as such:  ```python play.py```   """;General;https://github.com/flowersteam/Imagine
"""<img alt=""corrrespondence maps"" src=""/figures/PredictionExample.png"" width=""25%""/>   """;General;https://github.com/mshaikh2/AttentionHandwritingVerification
"""Python 3  jupyter or jupyterlab   """;General;https://github.com/henry32144/wgan-gp-tensorflow
"""The following images had the highest loss when evaluated as part of the test (not holdout) set during training:  ![highest loss MixNet imgs](https://www.dropbox.com/s/7nlo210srtq9xwg/mixnet_xl%20%20-%20CK%2BA%20-%2020ephighest_loss_images.png?dl=1)  * * *   `#TODO`       *Note: the above links to timm source code as the MixNet paper is already linked above*   `#TODO`   """;General;https://github.com/pszemraj/BoulderAreaDetector
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/Maz101/Bert
"""it's somewhat harder to install. MonoBeast requires only Python and  PyTorch (we suggest using PyTorch version 1.2 or newer).   The easiest way to build and install all of PolyBeast's dependencies   To run PolyBeast directly on Linux or MacOS  follow this guide.  Create a new Conda environment  and install PolyBeast's requirements:   $ conda create -n torchbeast python=3.7  $ conda activate torchbeast  $ pip install -r requirements.txt  Install PyTorch either from   website (select Conda).   $ git submodule update --init --recursive   $ pip install nest/  $ python setup.py install  Create a new Conda environment  and install PolyBeast's requirements:   $ conda create -n torchbeast  $ conda activate torchbeast  $ pip install -r requirements.txt  PyTorch can be installed as per its  website (select Conda).   $ git submodule update --init --recursive   $ pip install nest/  $ python setup.py install   We would love to have you contribute to TorchBeast or use it for your   MonoBeast is a pure Python + PyTorch implementation of IMPALA.  To set it up  create a new conda environment and install MonoBeast's requirements:  ```bash $ conda create -n torchbeast $ conda activate torchbeast $ conda install pytorch -c pytorch $ pip install -r requirements.txt ```  Then run MonoBeast  e.g. on the [Pong Atari environment](https://gym.openai.com/envs/Pong-v0/):  ```shell $ python -m torchbeast.monobeast --env PongNoFrameskip-v4 ```  By default  MonoBeast uses only a few actors (each with their instance of the environment). Let's change the default settings (try this on a beefy machine!):  ```shell $ python -m torchbeast.monobeast \      --env PongNoFrameskip-v4 \      --num_actors 45 \      --total_steps 30000000 \      --learning_rate 0.0004 \      --epsilon 0.01 \      --entropy_cost 0.01 \      --batch_size 4 \      --unroll_length 80 \      --num_buffers 60 \      --num_threads 4 \      --xpid example ```  Results are logged to `~/logs/torchbeast/latest` and a checkpoint file is written to `~/logs/torchbeast/latest/model.tar`.  Once training finished  we can test performance on a few episodes:  ```shell $ python -m torchbeast.monobeast \      --env PongNoFrameskip-v4 \      --mode test \      --xpid example ```  MonoBeast is a simple  single-machine version of IMPALA. Each actor runs in a separate process with its dedicated instance of the environment and runs the PyTorch model on the CPU to create actions. The resulting rollout trajectories (environment-agent interactions) are sent to the learner. In the main process  the learner consumes these rollouts and uses them to update the model's weights.    """;General;https://github.com/facebookresearch/torchbeast
""" First create lmdb datasets:  > python prepare_data.py --out LMDB_PATH --n_worker N_WORKER --size SIZE1 SIZE2 SIZE3 ... DATASET_PATH  This will convert images to jpeg and pre-resizes it. This implementation does not use progressive growing  but you can create multiple resolution datasets using size arguments with comma separated lists  for the cases that you want to try another resolutions later.  Then you can train model in distributed settings  > python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train.py --batch BATCH_SIZE LMDB_PATH  train.py supports Weights & Biases logging. If you want to use it  add --wandb arguments to the script.   """;Computer Vision;https://github.com/kh12lee/kh_lee
""" - Download `ILSVRC2012_img_train.tar`   `ILSVRC2012_img_val.tar`   `synset_labels.txt`  - Put in /your_path/to/data  ```  python src/dataset/imagenet_to_tfrecord.py --raw_data_dir /your_path/to/data                                             --local_scratch_dir /path/to/tfrecords  ```    ---  : Run horovod command   """;Computer Vision;https://github.com/timctho/random-wired-nn-tensorflow
"""As stated above  the data is separated into a training set and a validation set. Since all parameters are provided by the paper clearly  there is no model tuning process that depend on validation results. The validation set is used as the function of the testing set. The code below will print out the training process. Graphs of loss and accuracy through training and other Keras supported metrics could be accessed fro mthe history variable.  ```python DeepLOB_model = initiate_DeepLOB_model(lookback_timestep  feature_num  conv_filter_num  inception_num  LSTM_num  leaky_relu_alpha                            loss  optimizer  metrics)  #: definte the training stop criteria (no new max validation accuracy in 20 consecutive epochs) es = EarlyStopping(monitor='val_accuracy'  mode='max'  patience = stop_epoch_num  verbose=1) history = DeepLOB_model.fit(X_train  y_train  epochs=num_epoch  batch_size=batch_size  verbose=2  validation_data=(X_test  y_test)  callbacks = [es]) ```   ```python          """;Computer Vision;https://github.com/yuxiangalvin/DeepLOB-Model-Implementation-Project
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter. #: For single sentence inputs  don't use the delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/frankcgq105/BERTCHEN
"""1. https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py  2. https://github.com/akamaster/pytorch_resnet_cifar10.   """;General;https://github.com/seyrankhademi/ResNet_CIFAR10
"""The output is generated audio. It has taken ~2 seconds for generation on GPU GeForce GTX 1060. You can listen to it in the ipynb notebook or here.   | Text| `It was a great day` | `I love Machine Learning` | `My name is Pytorch and I live on cuda` | `I gonna take my horse to the old town road` | |-------|-------|-------|-------|-------| | Generated speech in wav | [link](http://marianpetruk.github.com/projects/text2speech/generated/itwaagrda.wav)   [alt_link](generated_audio/wav/itwaagrda.wav) | [link](http://marianpetruk.github.com/projects/text2speech/generated/ilomale.wav)   [alt_link](generated_audio/wav/ilomale.wav) | [link](http://marianpetruk.github.com/projects/text2speech/generated/mynaispyanilioncu.wav)   [alt_link](generated_audio/wav/mynaispyanilioncu.wav) | [link](http://marianpetruk.github.com/projects/text2speech/generated/igotamyhototholtoro.wav)   [alt_link](generated_audio/wav/igotamyhototholtoro.wav) | | Converted wav to mp3 | [link](generated_audio/converted_to_mp3/itwaagrda.mp3) | [link](generated_audio/converted_to_mp3/ilomale.mp3) | [link](generated_audio/converted_to_mp3/mynaispyanilioncu.mp3) | [link](generated_audio/converted_to_mp3/igotamyhototholtoro.mp3) |   """;Sequential;https://github.com/IvKosar/text2speech
"""* Matplotlib 3.3.4  The current version of CarRacing-v0 has memory bugs. To solve it  we need to download manually the newest ""car_racing.py"" script from Gym GitHub.<br/>   """;Reinforcement Learning;https://github.com/wpiszlogin/driver_critic
"""Main folder contains two notebooks: one implemented using GPU/PyTorch and the other implemented using GPU/Tensorflow.    Train/Run Model on GPU:   """;Computer Vision;https://github.com/tooth2/Artistic-Style-Transfer
"""Base on pytorch-softdtw-cuda (post) for the soft-DTW.   """;Natural Language Processing;https://github.com/keonlee9420/Parallel-Tacotron2
"""OS: Linux 16.02   CUDA: version 8.0  CUDNN: version 5.1   Use the code ```extract_mask.m``` to generate instance mask for the images in MS COCO 2017 training dataset.   We verify the generalization ability of our PSIS on instance segmentation task of MS COCO 2017. The instance segmetatnion results are shown belows:  |Training Sets | AP@0.50:0.95 | AP@0.50 | AP@0.75| AP@Small | AP@Med. | AP@Large |  AR@1 | AR@10 | AR@100 | AR@Small | AR@Med. | AR@Large  |  |--------------|:------------:|:-------:|:------:|:--------:|:-------:|:--------:|:-----:|:-----:|:------:|:--------:|:-------:|:----:| |   ori  |  35.9   | 57.7 | 38.4 |  19.2  | 39.7  |  49.7 | 30.5 | 47.3  | 49.6 |  29.7 |  53.8 |  65.8 | |  psis([model](https://drive.google.com/open?id=13kB4zvwR__O9vSGz9cvy4Br3C-mwSTGZ))  |  36.7   | 58.4 | 39.4 |  19.0  | 40.6  |  50.2 | 31.0 | 48.2  | 50.3 |  29.8 |  54.4 |  66.9 | | ori×2  |  36.6   | 58.2 | 39.2 |  18.5  |  40.3 |  50.4 | 31.0 | 47.7  | 49.7 |  29.5 |  53.5 |  66.6 | | psis×2([Coming Soon]()) |  37.1   | 58.8 | 39.9 |  19.3  |  41.2 |  50.8 | 31.1 | 47.7  | 50.4 |  30.2 |  54.5 |  67.9 |  Above results clearly show PSIS offers a new and complementary way to use instance masks for improving both detection and segmentation performance.   Here we show some examples of synthetic images generated by our IS strategy. The new (switched) instances are denoted in red boxes  and our instance-switching strategy can clearly preserve contextual coherence in the original images.  <img src=""https://github.com/Hwang64/PSIS/blob/master/img/examples.jpg"">  """;Computer Vision;https://github.com/Hwang64/PSIS
"""| pytorch                      | 0.4584 s     | 0.4929 s     |   """;Computer Vision;https://github.com/Jittor/lsgan-jittor
"""tinymind 使用说明：https://gitee.com/ai100/quiz-w7-doc   """;Computer Vision;https://github.com/ByakuyaUncia/DenseNetUncia
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/nicolasCruzW21/maskrcnn-Tracing
"""run the scripts in the ./ImageNet/experiment. Note that resnet18 experimetns are run on one GPU  and resnet-50/101 are run on 4 GPU in the scripts.    """;General;https://github.com/huangleiBuaa/IterNorm-pytorch
"""can be installed by simply calling pip install paperswithcode-client.   exists on Papers with Code. You can use the search to check if it exists  and if it doesn't  you can add a    If you cannot find your dataset on the website  you can create it with the API like this:           name=""VeryTinyImageNet""                evaluated_on=""2020-11-20""               external_source_url=""https://my.competition.com/leaderboard/entry1""               external_source_url=""https://my.competition.com/leaderboard/entry2""       evaluated_on=""2020-11-20""       external_source_url=""https://my.competition.com/leaderboard/entry1""   You can then access it by going to https://paperswithcode.com/sota/&lt;your_leaderboard_id&gt; or find it using    To install:  ```bash pip install paperswithcode-client ```  To list papers indexed on Papers with Code:  ```python  from paperswithcode import PapersWithCodeClient  client = PapersWithCodeClient() papers = client.paper_list() print(papers.results[0]) print(papers.next_page) ```  For full docs please see our [ReadTheDocs](https://paperswithcode-client.readthedocs.io/en/latest/) page.   """;General;https://github.com/paperswithcode/paperswithcode-client
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/feng-lab/nuclei
"""The n-best list is assumed to have the same format as Moses:      sentence-ID (starting from 0) ||| translation ||| scores  new scores will be appended to the end.    """;General;https://github.com/JeffreyJosanne/nematus_tf
"""Dependencies:  + Python 3.3+. + Python bindings for OpenCV. (Optional  but required by a lot of features) + TensorFlow ≥ 1.5  < 2   * TF is not not required if you only want to use `tensorpack.dataflow` alone as a data processing library   * TF2 is supported in some simple models if used in graph mode (and replace `tf` by `tf.compat.v1` when needed) ``` pip install --upgrade git+https://github.com/tensorpack/tensorpack.git #: or add `--user` to install to user's local directories ```  Please note that tensorpack is not yet stable. If you use tensorpack in your code  remember to mark the exact version of tensorpack you use as your dependencies.   We refuse toy examples. Instead of showing tiny CNNs trained on MNIST/Cifar10  we provide training scripts that reproduce well-known papers.  We refuse low-quality implementations. Unlike most open source repos which only __implement__ papers  [Tensorpack examples](examples) faithfully __reproduce__ papers  demonstrating its __flexibility__ for actual research.   """;General;https://github.com/tensorpack/tensorpack
""" - gpu_index: gpu index  default: 0   """;Computer Vision;https://github.com/ChengBinJin/DCGAN-TensorFlow
""" GhostNet: More Features from Cheap Operations. CVPR 2020. [[arXiv]](https://arxiv.org/abs/1911.11907)  By Kai Han  Yunhe Wang  Qi Tian  Jianyuan Guo  Chunjing Xu  Chang Xu.  - **Approach**  <div align=""center"">    <img src=""./fig/ghost_module.png"" width=""720""> </div>  - **Performance**  GhostNet beats other SOTA lightweight CNNs such as **MobileNetV3** and **FBNet**.  <div align=""center"">    <img src=""./fig/flops_latency.png"" width=""720""> </div>    Usage example: ``` import torch from ghost_net import ghost_net  model = ghost_net(width_mult=1.0) input = torch.randn(32 3 224 224) y = model(input) print(y) ```   """;Computer Vision;https://github.com/iamhankai/ghostnet.pytorch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/MaZhiyuanBUAA/bert-tf1.4.0
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/chalothon/BERT_Practice
"""[2] https://github.com/tensorflow/tensorflow/tree/r1.11/tensorflow/contrib/eager/python/examples/resnet50   """;Computer Vision;https://github.com/Baichenjia/Resnet
"""You can find an introduction to single-image super-resolution in [this article](https://krasserm.github.io/2019/09/04/super-resolution/).  It also demonstrates how EDSR and WDSR models can be fine-tuned with SRGAN (see also [this section](#srgan-for-fine-tuning-edsr-and-wdsr-models)).   Create a new [conda](https://conda.io) environment with      conda env create -f environment.yml      and activate it with      conda activate sisr   weights-wdsr-b-32-x4.tar.gz    weights-srgan.tar.gz    : Train the GAN with 200 000 steps.  gan_trainer.train(train_ds  steps=200000)   gan_trainer.train(train_ds  steps=200000)   gan_trainer.train(train_ds  steps=200000)   Examples in this section require following pre-trained weights for running (see also example notebooks):     """;Computer Vision;https://github.com/krasserm/super-resolution
"""Training of this magnitude definitely needed some beefed up hardware and since I'm a console guy (PS4)  I resorted to the EC instances Amazon provides (https://aws.amazon.com/ec2/instance-types/). Udacity's Amazon credits came in handy!  At first  I tried the g2.xlarge instance that Udacity's project on Traffic sign classifier had suggested (did that on my laptop back then) but the memory or the compute capability was nowhere near sufficient  since TF apparently drops to CPU and RAM after detecting that there isn't sufficient capacity on the GPU.  In the end  p2.xlarge EC2 instance were what I trained my network on. There was ~10GB GPU memory utilization and ~92% GPU at peak. My network trained pretty well on this setup.  NOTE: I faced a lot of issues when getting setup on the remote instance due to issues with certain libraries being out of date and anaconda not having those updates. Luckily Amazon released its latest (v6 at time) deep learning Ubuntu AMI which worked just fine out of the box. So if you are using EC2  make sure to test sample code and library imports in python first to make sure the platform is ready for your code.      """;General;https://github.com/Rohed/ml-1
"""``` git clone https://github.com/yoshitomo-matsubara/torchdistill.git cd torchdistill/ pip3 install -e . #: or use pipenv pipenv install ""-e ."" ```   ``` pip3 install torchdistill #: or use pipenv pipenv install torchdistill ```   - Python 3.6 >= - pipenv (optional)   If you find models on PyTorch Hub or GitHub repositories supporting PyTorch Hub        name: 'resnest50d'   Executable code can be found in [examples/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/) such as - [Image classification](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/image_classification.py): ImageNet (ILSVRC 2012)  CIFAR-10  CIFAR-100  etc - [Object detection](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/object_detection.py): COCO 2017  etc - [Semantic segmentation](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/semantic_segmentation.py): COCO 2017  PASCAL VOC  etc - [Text classification](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/hf_transformers/text_classification.py): GLUE  etc  For CIFAR-10 and CIFAR-100  some models are reimplemented and available as pretrained models in ***torchdistill***.  More details can be found [here](https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.1.1).    Some Transformer models fine-tuned by ***torchdistill*** for GLUE tasks are available at [Hugging Face Model Hub](https://huggingface.co/yoshitomo-matsubara).  Sample GLUE benchmark results and details can be found [here](https://github.com/yoshitomo-matsubara/torchdistill/tree/master/examples/hf_transformers#sample-benchmark-results-and-fine-tuned-models).   The following examples are available in [demo/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/demo/).  Note that the examples are for Google Colab users. Usually  [examples/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/) would be a better reference  if you have your own GPU(s).   """;Natural Language Processing;https://github.com/yoshitomo-matsubara/torchdistill
"""To extract and run the network in Code::Blocks <br/> $ mkdir *MyDir* <br/> $ cd *MyDir* <br/> $ wget https://github.com/Qengineering/YoloV3-ncnn-Jetson-Nano/archive/refs/heads/main.zip <br/> $ unzip -j master.zip <br/> Remove master.zip  LICENSE and README.md as they are no longer needed. <br/>  $ rm master.zip <br/> $ rm LICENSE <br/> $ rm README.md <br/> <br/> Your *MyDir* folder must now look like this: <br/>  parking.jpg <br/> busstop.jpg <br/> YoloV3.cpb <br/> yoloV3.cpp <br/> mobilenetv2_yolov3.bin <br/> mobilenetv2_yolov3.param <br/><br/>  ------------   """;Computer Vision;https://github.com/Qengineering/YoloV3-ncnn-Jetson-Nano
"""**Faster** R-CNN is an object detection framework based on deep convolutional networks  which includes a Region Proposal Network (RPN) and an Object Detection Network. Both networks are trained for sharing convolutional layers for fast testing.  **This repo contains a Python implementation of Faster-RCNN originally developed in Matlab. This code works with models trained using Matlab version of Faster-RCNN which is main difference between this and py-faster-rcnn.**  This code was developed for internal use in one of my projects at the end of 2015. I decided to publish it as is.   Use provided Dockerfile to build container with all required dependencies.   Download MATLAB Faster-RCNN models:   If the automatic ""fetch_data"" fails  you may manually download resouces from:   """;Computer Vision;https://github.com/smichalowski/faster_rcnn
"""Python 3 dependencies:  * Tensorflow 1.15 * matplotlib * numpy * imageio *  configargparse  The LLFF data loader requires ImageMagick.  We provide a conda environment setup file including all of the above dependencies. Create the conda environment `nerf` by running: ``` conda env create -f environment.yml ```  You will also need the [LLFF code](http://github.com/fyusion/llff) (and COLMAP) set up to compute poses if you want to run on your own real data.   conda env create -f environment.yml  conda activate nerf  bash download_example_data.sh   bash download_example_data.sh   bash download_example_weights.sh   pip install trimesh pyrender PyMCubes   """;Computer Vision;https://github.com/bmild/nerf
"""tinymind 使用说明：https://gitee.com/ai100/quiz-w7-doc   """;Computer Vision;https://github.com/0492wzl/tensorflow_slim_densenet
"""installed. Clone the repository in your home directory:  cd ~  git clone https://github.com/MostafaDehghani/bAbI-T2T.git  mkdir ~/babi_data   cd ~/babi_data/tmp   Then simply run the following commands to generate   train with one of these models  you can use this command:   you can run the following commands.    we analyzed both the attention distributions   """;Natural Language Processing;https://github.com/MostafaDehghani/bAbI-T2T
"""CPN 2D detections for Human3.6 M datasets are provided by [VideoPose3D](https://github.com/facebookresearch/VideoPose3D) by Pavllo etal.  which can be downloaded by: ```bash cd data wget https://dl.fbaipublicfiles.com/video-pose-3d/data_2d_h36m_cpn_ft_h36m_dbb.npz wget https://dl.fbaipublicfiles.com/video-pose-3d/data_2d_h36m_detectron_ft_h36m.npz cd .. ``` 3D labels and ground truth can be downloaded and put in data/ folder [3d gt labels](https://drive.google.com/file/d/1P7W3ldx2lxaYJJYcf3RG4Y9PsD4EJ6b0/view?usp=sharing)   To train on Human3.6M with 3-frame  run:   """;Computer Vision;https://github.com/vanoracai/Exploiting-Spatial-temporal-Relationships-for-3D-Pose-Estimation-via-Graph-Convolutional-Networks
"""1. python notebook file which can be run in colaboratory or jupyter notebook.    """;Computer Vision;https://github.com/Singh-sid930/YOLO_pytorch
"""![Schematic Illustration MAS](https://github.com/SAP-samples/acl2019-commonsense/blob/master/img/mas_illustration.png) The recently introduced [BERT (Deep Bidirectional Transformers for Language Understanding)](https://github.com/google-research/bert) [1] model exhibits strong performance on several language understanding benchmarks. In this work  we describe a simple re-implementation of BERT for commonsense reasoning. We show that the attentions produced by BERT can be directly utilized for tasks such as the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). Our proposed attention-guided commonsense reasoning method is conceptually simple yet empirically powerful. Experimental analysis on multiple datasets demonstrates that our proposed system performs remarkably well on all cases while outperforming the previously reported state of the art by a margin. While results suggest that BERT seems to implicitly learn to establish complex relationships between entities  solving commonsense reasoning tasks might require more than unsupervised models learned from huge text corpora. The sample code provided within this repository allows to replicate the results reported in the paper for PDP and WSC.  1. Install BertViz by cloning the repository and getting dependencies: ``` git clone https://github.com/jessevig/bertviz.git cd bertviz pip install -r requirements.txt cd .. ```  2. To replicate the results proceed to step 3). If you want to run the stand-alone version  you can just use [MAS.py](https://github.com/SAP-samples/acl2019-commonsense-reasoning/blob/master/MAS.py). Usage is showcased in the Jupyter Notebook example [MAS_Example.ipynb](https://github.com/SAP-samples/acl2019-commonsense-reasoning/blob/master/MAS_Example.ipynb).  3. Add BertViz path to Python path: ```   export PYTHONPATH=$PYTHONPATH:/home/ubuntu/bertviz/ ``` alternatively  you can add the statement to commonsense.py after importing of sys  e.g. ``` sys.path.append(""/home/ubuntu/bertviz/"") ```  4. Clone this repository and install dependencies: ``` git clone https://github.com/SAP/acl2019-commonsense-reasoning cd acl2019-commonsense-reasoning pip install -r requirements.txt ```  5. Create 'data' sub-directory and download files for PDP and WSC challenges: ``` mkdir data wget https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/PDPChallenge2016.xml wget https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WSCollection.xml cd .. ``` 6. Run the scripts from the paper  For replicating the results on WSC: ``` python commonsense.py --data_dir=~/acl2019-commonsense-reasoning/data/ --bert_mode=bert-base-uncased --do_lower_case --task_name=MNLI ```  For replicating the results on PDP: ``` python commonsense.py --data_dir=~/acl2019-commonsense-reasoning/data/ --bert_mode=bert-base-uncased --do_lower_case --task_name=pdp ```  For more information on the individual functions  please refer to their doc strings.   """;Natural Language Processing;https://github.com/SAP-samples/acl2019-commonsense
"""**Monolingual data (MLM)**: Follow the same procedure as in [I.1](https://github.com/facebookresearch/XLM#1-preparing-the-data)  and download multiple monolingual corpora  such as the Wikipedias.  Note that we provide a [tokenizer script](https://github.com/facebookresearch/XLM/blob/master/tools/tokenize.sh):  ``` lg=en cat my_file.$lg | ./tools/tokenize.sh $lg > my_tokenized_file.$lg & ```  **Parallel data (TLM)**: We provide download scripts for some language pairs in the *get-data-para.sh* script. ``` #: Download and tokenize parallel data in 'data/wiki/para/en-zh.{en zh}.{train valid test}' ./get-data-para.sh en-zh & ```  For other language pairs  look at the [OPUS collection](http://opus.nlpl.eu/)  and modify the get-data-para.sh script [here)(https://github.com/facebookresearch/XLM/blob/master/get-data-para.sh#L179-L180) to add your own language pair.  Now create you training set for the BPE vocabulary  for instance by taking 100M sentences from each monolingua corpora. ``` #: build the training set for BPE tokenization (50k codes) OUTPATH=data/processed/XLM_en_zh/50k mkdir -p $OUTPATH shuf -r -n 10000000 data/wiki/train.en >> $OUTPATH/bpe.train shuf -r -n 10000000 data/wiki/train.zh >> $OUTPATH/bpe.train ``` And learn the 50k BPE code as in the previous section on the bpe.train file. Apply BPE tokenization on the monolingual and parallel corpora  and binarize everything using *preprocess.py*:  ``` pair=en-zh  for lg in $(echo $pair | sed -e 's/\-/ /g'); do   for split in train valid test; do     $FASTBPE applybpe $OUTPATH/$pair.$lg.$split data/wiki/para/$pair.$lg.$split $OUTPATH/codes     python preprocess.py $OUTPATH/vocab $OUTPATH/$pair.$lg.$split   done done ```   First  get the monolingual data (English Wikipedia  the [TBC corpus](https://yknzhu.wixsite.com/mbweb) is not hosted anymore). ``` #: Download and tokenize Wikipedia data in 'data/wiki/en.{train valid test}' #: Note: the tokenization includes lower-casing and accent-removal ./get-data-wiki.sh en ```  [Install fastBPE](https://github.com/facebookresearch/XLM/tree/master/tools#fastbpe) and **learn BPE** vocabulary (with 30 000 codes here): ``` OUTPATH=data/processed/XLM_en/30k  #: path where processed files will be stored FASTBPE=tools/fastBPE/fast  #: path to the fastBPE tool  #: create output path mkdir -p $OUTPATH  #: learn bpe codes on the training set (or only use a subset of it) $FASTBPE learnbpe 30000 data/wiki/txt/en.train > $OUTPATH/codes ```  Now **apply BPE** tokenization to train/valid/test files: ``` $FASTBPE applybpe $OUTPATH/train.en data/wiki/txt/en.train $OUTPATH/codes & $FASTBPE applybpe $OUTPATH/valid.en data/wiki/txt/en.valid $OUTPATH/codes & $FASTBPE applybpe $OUTPATH/test.en data/wiki/txt/en.test $OUTPATH/codes & ```  and get the post-BPE vocabulary: ``` cat $OUTPATH/train.en | $FASTBPE getvocab - > $OUTPATH/vocab & ```  **Binarize the data** to limit the size of the data we load in memory: ``` #: This will create three files: $OUTPATH/{train valid test}.en.pth #: After that we're all set python preprocess.py $OUTPATH/vocab $OUTPATH/train.en & python preprocess.py $OUTPATH/vocab $OUTPATH/valid.en & python preprocess.py $OUTPATH/vocab $OUTPATH/test.en & ```   You can now use the pretrained model for cross-lingual classification. To download a model trained with the command above on the MLM-TLM objective  run:   Before running the scripts below  make sure you download the tokenizers from the [tools/](https://github.com/facebookresearch/XLM/tree/master/tools) directory.   """;Natural Language Processing;https://github.com/deterministic-algorithms-lab/Large-XLM
"""- Software: Ubuntu 16.04.3 LTS  CUDA>=8.0  Python>=3.5  PyTorch>=0.4.0  - Dependencies: numpy  scipy  opencv  yacs  tqdm   chmod +x download_ADE20K.sh   For example  you can start with our provided configurations:    This library can be installed via pip to easily integrate with another codebase  pip install git+https://github.com/CSAILVision/semantic-segmentation-pytorch.git@master   1. Here is a simple demo to do inference on a single image: ```bash chmod +x demo_test.sh ./demo_test.sh ``` This script downloads a trained model (ResNet50dilated + PPM_deepsup) and a test image  runs the test script  and saves predicted segmentation (.png) to the working directory.  2. To test on an image or a folder of images (```$PATH_IMG```)  you can simply do the following: ``` python3 -u test.py --imgs $PATH_IMG --gpu $GPU --cfg $CFG ```   """;Computer Vision;https://github.com/CSAILVision/semantic-segmentation-pytorch
"""This project is tested under the following environment setting.   - Cuda: 8.0  Cudnn: v5.1 or v7.03  - Python: 2.7.14(setup with Miniconda2)   Lasagne(version 0.2.dev1)   """;General;https://github.com/zhenxuan00/triple-gan
"""An unofficial implementation of the Set Transformer framework in Tensorflow/Keras based on the paper: [Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks](https://arxiv.org/abs/1810.00825).  The official pytorch repository is available at: https://github.com/juho-lee/set_transformer.   The set transformer framework can be installed by executing the following command in this repository:  ```bash pip install . ``` """;General;https://github.com/DLii-Research/tf-set-transformer
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/cyrilzakka/stylegan2-tpu
"""OpenCV [https://opencv.org/]<br/>   Tesseract [https://github.com/tesseract-ocr/] <br/>   1. You can start conversation with our bot on [https://vk.com/club174379976]  or add him into a conversation. <br/> 2. Next  grab images from `~/test_images` and sent to the bot 3. ??? 4. Enjoy!  """;General;https://github.com/Greeser/holyvk
"""These steps will allow you to be able to run SAI and play with SAI:  - for Windows  see [here](/docs/RUN-WINDOWS.md) - for Linux  see [here](/docs/RUN-LINUX.md)   With SAI you can:   ideally a GPU (a discrete graphics card made by NVIDIA or AMD    or you can run the program without a GPU (CPU-only)  but performance   Note that you can use either your own physical hardware  or run SAI   SAI can run on Windows  MacOS  and Linux.    see the autogtp instructions:  for Windows: here  for Linux: here   Note that these are mostly borrowed from Leela Zero's github  so   If you want to help  you can use your computing device in the  collective effort to make SAI stronger.  This will make SAI play against itself (selfplay games) and other versions  of itself (match games) on the [SAI server](http://sai.unich.it/)   to train and get stronger.  Any help is greatly appreciated!   """;Reinforcement Learning;https://github.com/sai-dev/sai
"""The following are example detections.   """;General;https://github.com/DrMMZ/RetinaNet
"""python: 3.7  Pytorch: 1.2.0   """;General;https://github.com/csyanbin/MAML-Pytorch-Multi-GPUs
"""Download weights   Download train dataset   """;General;https://github.com/Lornatang/SRGAN-PyTorch
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/Gryffindor112358/Arcface
"""Set the following ENV variable:   $ARNOLD_OUTPUT: Output directory  Set the following ENV variable:   $ARNOLD_OUTPUT: Output directory   bash scripts/run.sh ./apps/mobilenet/mobilenet_v2_mnas.yml   bash scripts/run_non_distributed_no_copy.sh ./apps/mobilenet/mobilenet_v2_mnas.yml   See requirements.txt  The code is developed using python 3. NVIDIA GPUs are needed. The code is developed and tested using 4 servers with 32 NVIDIA V100 GPU cards. Other platforms or GPU cards are not fully tested.   """;Computer Vision;https://github.com/meijieru/yet_another_mobilenet_series
"""Causion: To install the library  please git clone the repository instead of downloading the zip file  since source files inside the folder ./pytorch/prroi_pool/src/ and tensorflow/prroi_pool/src/kernels/external are symbol-linked. Downloading the repository as a zip file will break these symbolic links. Also  there are reports indicating that Windows git versions also breaks the symbol links. See issues/58.   mkdir tensorflow/prroi_pool/build  cd tensorflow/prroi_pool/build   mkdir tensorflow/prroi_pool/build  cd tensorflow/prroi_pool/build   nmake BUILD=release   In the directory `pytorch/`  we provide a PyTorch-based implementation of PrRoI Pooling. It requires PyTorch 1.0+ and only supports CUDA (CPU mode is not implemented). Since we use PyTorch JIT for cxx/cuda code compilation  to use the module in your code  simply do:  ``` from prroi_pool import PrRoIPool2D  avg_pool = PrRoIPool2D(window_height  window_width  spatial_scale) roi_features = avg_pool(features  rois)  #: for those who want to use the ""functional""  from prroi_pool.functional import prroi_pool2d roi_features = prroi_pool2d(features  rois  window_height  window_width  spatial_scale) ```   **!!! Please first checkout to the branch pytorch0.4.**  In the directory `pytorch/`  we provide a PyTorch-based implementation of PrRoI Pooling. It requires PyTorch 0.4 and only supports CUDA (CPU mode is not implemented). To use the PrRoI Pooling module  first goto `pytorch/prroi_pool` and execute `./travis.sh` to compile the essential components (you may need `nvcc` for this step). To use the module in your code  simply do:  ``` from prroi_pool import PrRoIPool2D  avg_pool = PrRoIPool2D(window_height  window_width  spatial_scale) roi_features = avg_pool(features  rois)  #: for those who want to use the ""functional""  from prroi_pool.functional import prroi_pool2d roi_features = prroi_pool2d(features  rois  window_height  window_width  spatial_scale) ```  Here   - RoI is an `m * 5` float tensor of format `(batch_index  x0  y0  x1  y1)`  following the convention in the original Caffe implementation of RoI Pooling  although in some frameworks the batch indices are provided by an integer tensor. - `spatial_scale` is multiplied to the RoIs. For example  if your feature maps are down-sampled by a factor of 16 (w.r.t. the input image)  you should use a spatial scale of `1/16`. - The coordinates for RoI follows the [L  R) convension. That is  `(0  0  4  4)` denotes a box of size `4x4`.   In the directory `tensorflow/`  we provide a TensorFlow-based implementation of PrRoI Pooling. It tested TensorFlow 2.2 and only supports CUDA (CPU mode is not implemented). To compile the essential components  follow the instruction below  To use the PrRoI Pooling module  to compile the essential components (you may need `nvcc` for this step). To use the module in your code  simply do:  """;Computer Vision;https://github.com/vacancy/PreciseRoIPooling
"""First build the package prior to running any code: ``` python setup.py develop ```  Repository implements experiments on several datasets: * [Image classification on CIFAR10/100](cifar/) * [Image classification on STL10](stl/) * [Image classification on ImageNet](imagenet/) * [Object detection on MS COCO and PASCAL VOC](mmdetection/) * [Boundary prediction on BSDS500](bsds/)   """;Computer Vision;https://github.com/matej-ulicny/harmonic-networks
"""To run the following codes  users should have the following packages    - Parallel Computing Toolbox (optional for GPU usage)   """;Computer Vision;https://github.com/zcemycl/Matlab-GAN
"""``` python benchmark.py ``` - 로컬에서 여러 모델을 실행시 평균 추론시간을 계산 할 수 있다. - 기본적으로 CPU를 사용하며 GPU를 사용할 수 있는 환경인 경우 GPU를 사용하여 연산한다.  ``` python benchmark_with_streaming.py ``` - 스트리밍을 통해 입력받은 모델에서 객체 탐지를 수행한다. -> 개선된 모델로 추가 예정 - 스트리밍시 프레임당 추론시간을 출력한다. (평균 추론 시간을 구하고 싶다면 benchmark.py를 사용하도록 한다.) - 현재 데모 영상을 사용하고 있으며 라즈베리파이에서 수신하기 위해서는 VideoCapture를 다음과 같이 변경해야한다.  현재 ```python cap = cv.VideoCapture('https://www.freedesktop.org/software/gstreamer-sdk/data/media/sintel_trailer-480p.webm') ```  변경 ```python cap = cv.VideoCapture('udpsrc port=9777 ! application/x-rtp ! rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! appsink'  cv.CAP_GSTREAMER) ``` - 라즈베리파이와 연동시 네트워크 에뮬레이터를 통해서 4G 또는 5G의 환경을 조성해서 실행하도록한다. - 서버에서 다시 라즈베리파이로 영상을 보내는 경우는 아직 구현되어 있지 않다.    """;General;https://github.com/Video-Streaming-Pipeline/Video-Streaming-Pipeline
"""the remaining modules are mainly copied from the original DETR implementation : https://github.com/facebookresearch/detr   `git clone https://github.com/hanouticelina/deformable-DETR.git`  `cd deformable-DETR`   """;Computer Vision;https://github.com/hanouticelina/deformable-DETR
"""Compile tensorflow from source code. Following the link to install tensorflow https://www.tensorflow.org/install/install_sources  tensorflow r1.14. Later version has not been tested. One addtional step after compile tensorflow as per the instruction is to compile the //tensorflow:libtensorflow.so which will be used in the c++ player.    """;Reinforcement Learning;https://github.com/wangml999/chess_zero
"""*Note: Any MCN version is compatible   If you use GPU  set opts.idx_gpus = 1' in line 7 and opts.install.cuda_path = 'your cuda folder' in line 11)   """;Computer Vision;https://github.com/cshanjiewu/CVPR19
"""**Requirements**: Visual Studio 2013   |  Linux (CPU)   |  Windows (CPU) |   Installation instructions   Copy .\windows\CommonSettings.props.example to .\windows\CommonSettings.props  By defaults Windows build requires CUDA and cuDNN libraries.  Both can be disabled by adjusting build variables in .\windows\CommonSettings.props.   3rd party dependencies required by Caffe are automatically resolved via NuGet.  Download CUDA Toolkit 7.5 from nVidia website.  If you don't have CUDA installed  you can experiment with CPU_ONLY build.   Download cuDNN v4 or cuDNN v5 from nVidia website.  Unpack downloaded zip to %CUDA_PATH% (environment variable set by CUDA installer).   Install for all users and add Python to PATH (through installer).  Run the following commands from elevated command prompt:  conda install --yes numpy scipy matplotlib scikit-image pip  pip install protobuf   * set PythonPath environment variable to point to &lt;caffe_root&gt;\Build\x64\Release\pycaffe  or   To build Caffe Matlab wrapper set MatlabSupport to true and MatlabDir to the root of your Matlab installation in .\windows\CommonSettings.props.   """;Computer Vision;https://github.com/Code-0x00/caffe_windows
"""Python >= 3.6.1   Though this project is still WIP  all examples are verified to work.  First  install [pipenv](https://pipenv.readthedocs.io/en/latest/). E.g. you can install it via ``` bash pip install pipenv --user ```  Then  clone this repository and create a virtual environment in it. ```bash git clone https://github.com/kngwyu/Rainy.git cd Rainy pipenv --site-packages --three install ```  Now you are ready to start!  ```bash pipenv run python examples/acktr_cart_pole.py train ```  After training  you can run learned agents.  Please replace `(log-directory)` in the below command with your real log directory. It should be named like `acktr_cart_pole-190219-134651-35a14601`. ``` bash pipenv run python acktr_cart_pole.py eval (log-directory) --render ```  You can also plot training results in your log directory. This command opens an ipython shell with your log file. ``` bash pipenv run python -m rainy.ipython ``` Then you can plot training rewards via ```python log = open_log('log-directory') log.plot_reward(12 * 20  max_steps=int(4e5)  title='ACKTR cart pole') ``` ![ACKTR cart pole](./pictures/acktr-cart-pole.png)   """;General;https://github.com/alexmlamb/blocks_rl_gru_setup
"""source code: https://github.com/AlexeyAB/darknet   useful links: https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&sk=6039748846bbcf1d960c3061542591d7   """;Computer Vision;https://github.com/scienXX/roomcamera2
"""<a href=""https://media.giphy.com/media/Ri2xsHaPlKv2sbv4pz/giphy.gif""><img src=""https://media.giphy.com/media/Ri2xsHaPlKv2sbv4pz/giphy.gif""/></a>   """;General;https://github.com/UrosOgrizovic/SimpleGoogleQuickdraw
""" - Notebook: folder chứa các notebook để train    - Run các cell trong notebook   pip install -r requirements.txt. để cài đặt các packages cần thiết   """;Computer Vision;https://github.com/vieduy/Neural-Style-Transfer
"""We use the implementation done by `lucidrains <https://github.com/lucidrains/lambda-networks>`_  .. code-block:: bash     pip install lambda-networks          """;General;https://github.com/rajatsaini0294/lambda-mobilenets
"""Current Version : 0.0.0.2   find -type f -name '*.sph' | awk '{printf ""sox -t sph %s -b 16 -t wav %s\n""  $0  $0"".wav"" }' | bash   sudo apt-get install sox     Run the following command in the console to pre-process whole dataset.   CUDA_VISIBLE_DEVICES=0 1 python train.py ( <== Use only GPU 0  1 )   For example  try the following command.   Extract the following zip file to the 'asset/train/' directory.   """;Sequential;https://github.com/Shivendra-psc/speechbot
"""- Clone this repo: ```bash git clone git@github.com:yenchenlin/pix2pix-tensorflow.git cd pix2pix-tensorflow ``` - Download the dataset (script borrowed from [torch code](https://github.com/phillipi/pix2pix/blob/master/datasets/download_dataset.sh)): ```bash bash ./download_dataset.sh facades ``` - Train the model ```bash python main.py --phase train ``` - Test the model: ```bash python main.py --phase test ```   """;Computer Vision;https://github.com/yenchenlin/pix2pix-tensorflow
"""Currently we only support installation from source code using setuptools. Checkout the source code and run the following commands:      ``` pip install -e . ```   This project recommends Python 3.7 or higher. We recommend creating a new virtual environment for this project (using virtual env or conda).     ```python import torch import torch.nn as nn from conformer import Conformer  batch_size  sequence_length  dim = 3  12345  80  cuda = torch.cuda.is_available()   device = torch.device('cuda' if cuda else 'cpu')  inputs = torch.rand(batch_size  sequence_length  dim).to(device) input_lengths = torch.IntTensor([12345  12300  12000]) targets = torch.LongTensor([[1  3  3  3  3  3  4  5  6  2]                              [1  3  3  3  3  3  4  5  2  0]                              [1  3  3  3  3  3  4  2  0  0]]).to(device) target_lengths = torch.LongTensor([9  8  7])  model = nn.DataParallel(Conformer(num_classes=10  input_dim=dim                                     encoder_dim=32  num_encoder_layers=3                                     decoder_dim=32)).to(device)  #: Forward propagate outputs = model(inputs  input_lengths  targets  target_lengths)  #: Recognize input speech outputs = model.module.recognize(inputs  input_lengths) ```     """;Natural Language Processing;https://github.com/sooftware/conformer
"""The training and evaluation scripts operate on datasets stored as multi-resolution TFRecords. Each dataset is represented by a directory containing the same image data in several resolutions to enable efficient streaming. There is a separate *.tfrecords file for each resolution  and if the dataset contains labels  they are stored in a separate file as well. By default  the scripts expect to find the datasets at `datasets/<NAME>/<NAME>-<RESOLUTION>.tfrecords`. The directory can be changed by editing [config.py](./config.py):  ``` result_dir = 'results' data_dir = 'datasets' cache_dir = 'cache' ```  To obtain the FFHQ dataset (`datasets/ffhq`)  please refer to the [Flickr-Faces-HQ repository](https://github.com/NVlabs/ffhq-dataset).  To obtain the CelebA-HQ dataset (`datasets/celebahq`)  please refer to the [Progressive GAN repository](https://github.com/tkarras/progressive_growing_of_gans).  To obtain other datasets  including LSUN  please consult their corresponding project pages. The datasets can be converted to multi-resolution TFRecords using the provided [dataset_tool.py](./dataset_tool.py):  ``` > python dataset_tool.py create_lsun datasets/lsun-bedroom-full ~/lsun/bedroom_lmdb --resolution 256 > python dataset_tool.py create_lsun_wide datasets/lsun-car-512x384 ~/lsun/car_lmdb --width 512 --height 384 > python dataset_tool.py create_lsun datasets/lsun-cat-full ~/lsun/cat_lmdb --resolution 256 > python dataset_tool.py create_cifar10 datasets/cifar10 ~/cifar10 > python dataset_tool.py create_from_images datasets/custom-dataset ~/custom-images ```   Code: https://github.com/NVlabs/stylegan  FFHQ: https://github.com/NVlabs/ffhq-dataset   """;Computer Vision;https://github.com/khurram702/StyleBasedGAN
"""DDPG implementation with pytorch   (&lt;env_name&gt;)$ pip install -r path/to/requirements.txt   """;Reinforcement Learning;https://github.com/J93T/TP4-DDPG
"""```bash $ pip install vit-pytorch ```   For a Pytorch implementation with pretrained models  please see Ross Wightman's repository <a href=""https://github.com/rwightman/pytorch-image-models"">here</a>.   You can also use the handy .to_vit method on the DistillableViT instance to get back a ViT instance.   You can train this with a near SOTA self-supervised learning technique  <a href=""https://github.com/lucidrains/byol-pytorch"">BYOL</a>  with the following code.   $ pip install byol-pytorch   A pytorch-lightning script is ready for you to use at the repository link above.   $ pip install nystrom-attention   $ pip install x-transformers   ```python import torch from vit_pytorch import ViT  v = ViT(     image_size = 256      patch_size = 32      num_classes = 1000      dim = 1024      depth = 6      heads = 16      mlp_dim = 2048      dropout = 0.1      emb_dropout = 0.1 )  img = torch.randn(1  3  256  256) mask = torch.ones(1  8  8).bool() #: optional mask  designating which patch to attend to  preds = v(img  mask = mask) #: (1  1000) ```   """;Computer Vision;https://github.com/tianhai123/vit-pytorch
"""```bash pip install labml-nn ```   """;Graphs;https://github.com/labmlai/annotated_deep_learning_paper_implementations
"""Notebook based on https://github.com/lucidrains/TimeSformer-pytorch repository.    """;Computer Vision;https://github.com/davide-coccomini/TimeSformer-Video-Classification
"""First  you will need to download and setup a dataset. The easiest way is to use one of the already existing datasets on UC Berkeley's repository: ``` ./download_dataset <dataset_name> ``` Valid <dataset_name> are: apple2orange  summer2winter_yosemite  horse2zebra  monet2photo  cezanne2photo  ukiyoe2photo  vangogh2photo  maps  cityscapes  facades  iphone2dslr_flower  ae_photos  Alternatively you can build your own dataset by setting up the following directory structure:      .     ├── datasets                        |   ├── <dataset_name>          Follow the instructions in pytorch.org for your current setup   pip3 install visdom   If you don't own a GPU remove the --cuda option  although I advise you to get one!   """;General;https://github.com/ZC119/Handwritten-CycleGAN
"""Please check the [Environment](https://github.com/CSAILVision/semantic-segmentation-pytorch/blob/master/README.md#environment)  [Training](https://github.com/CSAILVision/semantic-segmentation-pytorch/blob/master/README.md#training) and [Evaluation](https://github.com/CSAILVision/semantic-segmentation-pytorch/blob/master/README.md#evaluation) subsection in the repo [Scene Parsing on MIT ADE20K](https://github.com/CSAILVision/semantic-segmentation-pytorch) for a quick start.   The code is tested under the following configurations.  Hardware: 1-8 GPUs (with at least 12G GPU memories)  Software: CUDA 9.0  Python 3.6  PyTorch 0.4.0  tensorboardX   The training script with ResNet-50-sn backbone can be found here:  ./scripts/train.sh   Use git to clone this repository:  ``` git clone https://github.com/switchablenorms/SwitchNorm_Segmentation.git ```   """;Computer Vision;https://github.com/switchablenorms/SwitchNorm_Segmentation
"""![ ](Figures/Introduction.png)  This paper proposes a Hierarchical Learned Video Compression (HLVC) method with three hierarchical quality layers and a recurrent enhancement network. As illustrated in Figure 1  the frames in layers 1  2 and 3 are compressed with the highest  medium and the lowest quality  respectively. The benefits of hierarchical quality are two-fold: First  the high quality frames  which provide high quality references  are able to improve the compression performance of other frames at the encoder side; Second  because of the high correlation among neighboring frames  at the decoder side  the low quality frames can be enhanced by making use of the advantageous information in high quality frames. The enhancement improves quality without bit-rate overhead  thus improving the rate-distortion performance. For example  the frames 3 and 8 in Figure 1  which belong to layer 3  are compressed with low quality and bit-rate. Then  our recurrent enhancement network significantly improves their quality  taking advantage of higher quality frames  e.g.  frames 0 and 5. As a result  the frames 3 and 8 reach comparable quality to frame 5 in layer 2  but consume much less bit-rate. Therefore  our HLVC approach achieves efficient video compression.   --com  the path to save the compressed/decompressed frame.   ffmpeg -pix_fmt yuv420p -s WidthxHeight -i Name.yuv -filter:v ""crop=1920:1072:0:0"" Name_crop.yuv   """;Computer Vision;https://github.com/RenYang-home/HLVC
"""The Adversarial Autoencoder behaves similarly to [Variational Autoencoders](https://arxiv.org/abs/1312.6114)  forcing the latent space of an autoencoder to follow a predefined prior. In the case of the Adversarial Autoencoder  this latent space can be defined arbitrarily and easily sampled and fed into the Discriminator in the network.  <div> <img src=""https://raw.githubusercontent.com/greentfrapp/keras-aae/master/images/aae_latent.png"" alt=""Latent space from Adversarial Autoencoder"" width=""whatever"" height=""300px"" style=""display: inline-block;""> <img src=""https://raw.githubusercontent.com/greentfrapp/keras-aae/master/images/regular_latent.png"" alt=""Latent space from regular Autoencoder"" width=""whatever"" height=""300px"" style=""display: inline-block;""> </div>  *The left image shows the latent space of an unseen MNIST test set after training with an Adversarial Autoencoder for 50 epochs  which follows a 2D Gaussian prior. Contrast this with the latent space of the regular Autoencoder trained under the same conditions  with a far more irregular latent distribution.*   """;Computer Vision;https://github.com/greentfrapp/keras-aae
"""Deep Q-Learning Network (DQN) [2] is one of the most popular deep reinforcement learning algorithms. It is an off-policy learning algorithm that is highly sample efficient. Over the years  many improvements were proposed to improve the performance of DQN. Of the many extensions available for the DQN algorithm  some popular enhancements were combined by the DeepMind team and presented as the Rainbow DQN algorithm. These imporvements were found to be mostly orthogonal  with each component contributing to various degrees.  The six add-ons to the base DQN algorithm in the Rainbow version are      1. Double Q-Learning      2. Prioritized Experience Replay      3. Dueling Networks      4. Multi-step (n-step) Learning      5. Distributional Value Learning      6. Noisy Networks   This repository contains an implementation of the rainbow DQN algorithm put forward by the DeepMind team in the paper '[Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)'. [1]   <p align=""center"" >   <img width=""160"" height=""210"" src=""media/pong.gif"">   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   <img width=""315"" height=""210"" src=""media/cartpole.gif""> </p>    git clone https://github.com/roboticist-by-day/rainbow_dqn.git  cd rainbow_dqn  To create the python environment using Anaconda package manager   conda create --name &lt;env_name&gt; --file requirements.txt  conda activate &lt;env_name&gt;   Note: The 'Pong-v0' environment requires significant memory to store the replay buffer.   For those interested in studying the contributions of various rainbow components  the code supports functionlity to perform ablation studies. However  some combinations might not have the helper functions pre-defined. To disable improvements  modify the arguments passed to the rainbow class as necessary and pass the flags that disable the respective components.   Training data is written to tensorbaord. Currently  training can be restored by providing the model state dict. The code can also be easily extended to restore the optimizer and other epoch dependent data.   """;Reinforcement Learning;https://github.com/mohith-sakthivel/rainbow_dqn
"""BERT-base and BERT-large are respectively 110M and 340M parameters models and it can be difficult to fine-tune them on a single GPU with the recommended batch size for good performance (in most case a batch size of 32).  To help with fine-tuning these models  we have included several techniques that you can activate in the fine-tuning scripts [`run_classifier.py`](./examples/run_classifier.py) and [`run_squad.py`](./examples/run_squad.py): gradient-accumulation  multi-gpu training  distributed training and 16-bits training . For more details on how to use these techniques you can read [the tips on training large batches in PyTorch](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255) that I published earlier this month.  Here is how to use these techniques in our scripts:  - **Gradient Accumulation**: Gradient accumulation can be used by supplying a integer greater than 1 to the `--gradient_accumulation_steps` argument. The batch at each step will be divided by this integer and gradient will be accumulated over `gradient_accumulation_steps` steps. - **Multi-GPU**: Multi-GPU is automatically activated when several GPUs are detected and the batches are splitted over the GPUs. - **Distributed training**: Distributed training can be activated by supplying an integer greater or equal to 0 to the `--local_rank` argument (see below). - **16-bits training**: 16-bits training  also called mixed-precision training  can reduce the memory requirement of your model on the GPU by using half-precision training  basically allowing to double the batch size. If you have a recent GPU (starting from NVIDIA Volta architecture) you should see no decrease in speed. A good introduction to Mixed precision training can be found [here](https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/) and a full documentation is [here](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html). In our scripts  this option can be activated by setting the `--fp16` flag and you can play with loss scaling using the `--loss_scale` flag (see the previously linked documentation for details on loss scaling). The loss scale can be zero in which case the scale is dynamically adjusted or a positive power of two in which case the scaling is static.  To use 16-bits training and distributed training  you need to install NVIDIA's apex extension [as detailed here](https://github.com/nvidia/apex). You will find more information regarding the internals of `apex` and how to use `apex` in [the doc and the associated repository](https://github.com/nvidia/apex). The results of the tests performed on pytorch-BERT by the NVIDIA team (and my trials at reproducing them) can be consulted in [the relevant PR of the present repository](https://github.com/huggingface/pytorch-pretrained-BERT/pull/116).  Note: To use *Distributed Training*  you will need to run one training script on each of your machines. This can be done for example by running the following command on each server (see [the above mentioned blog post]((https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)) for more details): ```bash python -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=$THIS_MACHINE_INDEX --master_addr=""192.168.1.1"" --master_port=1234 run_classifier.py (--arg1 --arg2 --arg3 and all other arguments of the run_classifier script) ``` Where `$THIS_MACHINE_INDEX` is an sequential index assigned to each of your machine (0  1  2...) and the machine with rank 0 has an IP address `192.168.1.1` and an open port `1234`.   We get the following results on the dev set of GLUE benchmark with an uncased BERT base  model. All experiments were run on a P100 GPU with a batch size of 32.  | Task | Metric | Result | |-|-|-| | CoLA | Matthew's corr. | 57.29 | | SST-2 | accuracy | 93.00 | | MRPC | F1/accuracy | 88.85/83.82 | | STS-B | Pearson/Spearman corr. | 89.70/89.37 | | QQP | accuracy/F1 | 90.72/87.41 | | MNLI | matched acc./mismatched acc.| 83.95/84.39 | | QNLI | accuracy | 89.04 | | RTE | accuracy | 61.01 | | WNLI | accuracy | 53.52 |  Some of these results are significantly different from the ones reported on the test set of GLUE benchmark on the website. For QQP and WNLI  please refer to [FAQ #12](https://gluebenchmark.com/faq) on the webite.  Before running anyone of these GLUE tasks you should download the [GLUE data](https://gluebenchmark.com/tasks) by running [this script](https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e) and unpack it to some directory `$GLUE_DIR`.  ```shell export GLUE_DIR=/path/to/glue export TASK_NAME=MRPC  python run_classifier.py \   --task_name $TASK_NAME \   --do_train \   --do_eval \   --do_lower_case \   --data_dir $GLUE_DIR/$TASK_NAME \   --bert_model bert-base-uncased \   --max_seq_length 128 \   --train_batch_size 32 \   --learning_rate 2e-5 \   --num_train_epochs 3.0 \   --output_dir /tmp/$TASK_NAME/ ```  where task name can be one of CoLA  SST-2  MRPC  STS-B  QQP  MNLI  QNLI  RTE  WNLI.  The dev set results will be present within the text file 'eval_results.txt' in the specified output_dir. In case of MNLI  since there are two separate dev sets  matched and mismatched  there will be a separate output folder called '/tmp/MNLI-MM/' in addition to '/tmp/MNLI/'.  The code has not been tested with half-precision training with apex on any GLUE task apart from MRPC  MNLI  CoLA  SST-2. The following section provides details on how to run half-precision training with MRPC. With that being said  there shouldn't be any issues in running half-precision training with the remaining GLUE tasks as well  since the data processor for each task inherits from the base class DataProcessor.   This repo was tested on Python 2.7 and 3.5+ (examples are tested only on python 3.5+) and PyTorch 0.4.1/1.0.0   | Installation | How to install the package |   PyTorch pretrained bert can be installed by pip as follows:  pip install pytorch-pretrained-bert  If you want to reproduce the original tokenization process of the OpenAI GPT paper  you will need to install ftfy (limit to version 4.4.3 if you are using Python 2) and SpaCy :  pip install spacy ftfy==4.4.3  python -m spacy download en   pip install [--editable] .  Here also  if you want to reproduce the original tokenization process of the OpenAI GPT model  you will need to install ftfy (limit to version 4.4.3 if you are using Python 2) and SpaCy :  pip install spacy ftfy==4.4.3  python -m spacy download en   A series of tests is included in the tests folder and can be run using pytest (install pytest if needed: pip install pytest).  You can run the tests with the command:   : If you have a GPU  put everything on cuda  tokens_tensor = tokens_tensor.to('cuda')  segments_tensors = segments_tensors.to('cuda')   : If you have a GPU  put everything on cuda  tokens_tensor = tokens_tensor.to('cuda')  segments_tensors = segments_tensors.to('cuda')   : If you have a GPU  put everything on cuda  tokens_tensor = tokens_tensor.to('cuda')   : If you have a GPU  put everything on cuda  tokens_tensor = tokens_tensor.to('cuda')   : get the predicted last token   : If you have a GPU  put everything on cuda  tokens_tensor_1 = tokens_tensor_1.to('cuda')  tokens_tensor_2 = tokens_tensor_2.to('cuda')   : If you have a GPU  put everything on cuda  tokens_tensor_1 = tokens_tensor_1.to('cuda')  tokens_tensor_2 = tokens_tensor_2.to('cuda')   : get the predicted last token   : If you have a GPU  put everything on cuda  tokens_tensor_1 = tokens_tensor_1.to('cuda')  tokens_tensor_2 = tokens_tensor_2.to('cuda')   : If you have a GPU  put everything on cuda  tokens_tensor_1 = tokens_tensor_1.to('cuda')  tokens_tensor_2 = tokens_tensor_2.to('cuda')   : get the predicted last token   If PRE_TRAINED_MODEL_NAME_OR_PATH is a shortcut name  the pre-trained weights will be downloaded from AWS S3 (see the links here) and stored in a cache folder to avoid future download (the cache folder can be found at ~/.pytorch_pretrained_bert/).   schedule : schedule to use for the warmup (see above).   First install apex as indicated here.   The data for SQuAD can be downloaded with the following links and should be saved in a $SQUAD_DIR directory.   This command will download a pre-processed version of the WikiText 103 dataset in which the vocabulary has been computed.   If you have a recent GPU (starting from NVIDIA Volta series)  you should try 16-bit fine-tuning (FP16).   To run this specific conversion script you will need to have TensorFlow and PyTorch installed (pip install tensorflow). The rest of the repository only requires PyTorch.   export OPENAI_GPT_CHECKPOINT_FOLDER_PATH=/path/to/openai/pretrained/numpy/weights   | Sub-section | Description | |-|-| | [Training large models: introduction  tools and examples](#Training-large-models-introduction -tools-and-examples) | How to use gradient-accumulation  multi-gpu training  distributed training  optimize on CPU and 16-bits training to train Bert models | | [Fine-tuning with BERT: running the examples](#Fine-tuning-with-BERT-running-the-examples) | Running the examples in [`./examples`](./examples/): `extract_classif.py`  `run_classifier.py`  `run_squad.py` and `run_lm_finetuning.py` | | [Fine-tuning with OpenAI GPT  Transformer-XL and GPT-2](#Fine-tuning-with-OpenAI-GPT-Transformer-XL-and-GPT-2) | Running the examples in [`./examples`](./examples/): `run_openai_gpt.py`  `run_transfo_xl.py` and `run_gpt2.py` | | [Fine-tuning BERT-large on GPUs](#Fine-tuning-BERT-large-on-GPUs) | How to fine tune `BERT large`|   BERT-base and BERT-large are respectively 110M and 340M parameters models and it can be difficult to fine-tune them on a single GPU with the recommended batch size for good performance (in most case a batch size of 32).  To help with fine-tuning these models  we have included several techniques that you can activate in the fine-tuning scripts [`run_classifier.py`](./examples/run_classifier.py) and [`run_squad.py`](./examples/run_squad.py): gradient-accumulation  multi-gpu training  distributed training and 16-bits training . For more details on how to use these techniques you can read [the tips on training large batches in PyTorch](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255) that I published earlier this month.  Here is how to use these techniques in our scripts:  - **Gradient Accumulation**: Gradient accumulation can be used by supplying a integer greater than 1 to the `--gradient_accumulation_steps` argument. The batch at each step will be divided by this integer and gradient will be accumulated over `gradient_accumulation_steps` steps. - **Multi-GPU**: Multi-GPU is automatically activated when several GPUs are detected and the batches are splitted over the GPUs. - **Distributed training**: Distributed training can be activated by supplying an integer greater or equal to 0 to the `--local_rank` argument (see below). - **16-bits training**: 16-bits training  also called mixed-precision training  can reduce the memory requirement of your model on the GPU by using half-precision training  basically allowing to double the batch size. If you have a recent GPU (starting from NVIDIA Volta architecture) you should see no decrease in speed. A good introduction to Mixed precision training can be found [here](https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/) and a full documentation is [here](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html). In our scripts  this option can be activated by setting the `--fp16` flag and you can play with loss scaling using the `--loss_scale` flag (see the previously linked documentation for details on loss scaling). The loss scale can be zero in which case the scale is dynamically adjusted or a positive power of two in which case the scaling is static.  To use 16-bits training and distributed training  you need to install NVIDIA's apex extension [as detailed here](https://github.com/nvidia/apex). You will find more information regarding the internals of `apex` and how to use `apex` in [the doc and the associated repository](https://github.com/nvidia/apex). The results of the tests performed on pytorch-BERT by the NVIDIA team (and my trials at reproducing them) can be consulted in [the relevant PR of the present repository](https://github.com/huggingface/pytorch-pretrained-BERT/pull/116).  Note: To use *Distributed Training*  you will need to run one training script on each of your machines. This can be done for example by running the following command on each server (see [the above mentioned blog post]((https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)) for more details): ```bash python -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=$THIS_MACHINE_INDEX --master_addr=""192.168.1.1"" --master_port=1234 run_classifier.py (--arg1 --arg2 --arg3 and all other arguments of the run_classifier script) ``` Where `$THIS_MACHINE_INDEX` is an sequential index assigned to each of your machine (0  1  2...) and the machine with rank 0 has an IP address `192.168.1.1` and an open port `1234`.   We showcase several fine-tuning examples based on (and extended from) [the original implementation](https://github.com/google-research/bert/):  - a *sequence-level classifier* on nine different GLUE tasks  - a *token-level classifier* on the question answering dataset SQuAD  and - a *sequence-level multiple-choice classifier* on the SWAG classification corpus. - a *BERT language model* on another target corpus   We provide three examples of scripts for OpenAI GPT  Transformer-XL and OpenAI GPT-2 based on (and extended from) the respective original implementations:  - fine-tuning OpenAI GPT on the ROCStories dataset - evaluating Transformer-XL on Wikitext 103 - unconditional and conditional generation from a pre-trained OpenAI GPT-2 model   """;Natural Language Processing;https://github.com/cedrickchee/pytorch-pretrained-BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/owainwest/uniprot_bert
"""```bash #: installation pip install -r requirements.txt #: to train python copy_task.py --train #: to evaluate python copy_task.py --eval  ```   """;Sequential;https://github.com/clemkoa/ntm
"""``` pip install -r requirements.txt CUDA_VISIBLE_DEVICES=0 python main.py --zdims 32 32 32 --downsample 1 1 1 --nonlin elu --skip --blocks-per-layer 4 --gated --freebits 0.5 --learn-top-prior --data-dep-init --seed 42 --dataset static_mnist ```  Dependencies include [boilr](https://github.com/addtt/boiler-pytorch) (a framework  for PyTorch) and [multiobject](https://github.com/addtt/multi-object-datasets) (which provides multi-object datasets with PyTorch dataloaders).     - One day I'll get around to evaluating the IW bound on all datasets with 10000 samples.     that they are all conditioned on the same samples. These correspond to one    ``` pip install -r requirements.txt CUDA_VISIBLE_DEVICES=0 python main.py --zdims 32 32 32 --downsample 1 1 1 --nonlin elu --skip --blocks-per-layer 4 --gated --freebits 0.5 --learn-top-prior --data-dep-init --seed 42 --dataset static_mnist ```  Dependencies include [boilr](https://github.com/addtt/boiler-pytorch) (a framework  for PyTorch) and [multiobject](https://github.com/addtt/multi-object-datasets) (which provides multi-object datasets with PyTorch dataloaders).     """;Computer Vision;https://github.com/addtt/ladder-vae-pytorch
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 10.0: https://developer.nvidia.com/cuda-toolkit-archive (on Linux do Post-installation Actions)  OpenCV 3.3.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.3.0/opencv-3.3.0-vc14.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 10.0) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 10.0"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * **Yolo v3** COCO - video: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * **Yolo v3** COCO - WebCam 0: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * **Yolo v3** COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * **Yolo v3 - save result to the file res.avi**: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 test.mp4 -out_filename res.avi` * **Yolo v3 Tiny** COCO - video: `darknet.exe detector demo data/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **Yolo v3 Tiny** on GPU #0: `darknet.exe detector demo data/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 0 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;General;https://github.com/ghaniskn/GCorp-Darknet
"""You can install this package from PyPI:  ```sh pip install gmlp-flax ```  Or directly from GitHub:  ```sh pip install --upgrade git+https://github.com/SauravMaheshkar/gMLP.git ```   conda env create --name &lt;env-name&gt; sauravmaheshkar/gmlp  conda activate &lt;env-name&gt;   ```python import jax from gmlp_flax import gMLP  random_key = jax.random.PRNGKey(0)  x = jax.random.randint(key=random_key  minval=0  maxval=20000  shape=(1  1000))  init_rngs = {""params"": random_key}  gMLP(num_tokens=20000  dim=512  depth=4).init(init_rngs x) ```   """;General;https://github.com/SauravMaheshkar/gMLP
"""1. The environment can be downloaded from one of the links below for all operating systems:     - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip)     - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip)     - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip)     - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip)     - _For AWS_: To train the agent on AWS (without [enabled virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md))  use [this link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux_NoVis.zip) to obtain the ""headless"" version of the environment.  The agent can **not** be watched without a virtual screen  but can be trained.  (_To watch the agent  one can follow the instructions to [enable a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md)  and then download the environment for the **Linux** operating system above._)  2. Place the downloaded file in the same directory as this GitHub repository and unzip the file.  3. Use the `requirements.txt` file to set up a python environment with all necessary packages installed.   """;Reinforcement Learning;https://github.com/EyaRhouma/collaboration-competition-MADDPG
"""See http://pytorch.org/docs/torchvision/models.html?highlight=densenet for directly using the pretrained models in PyTorch.   0. Get the CIFAR data prepared following the [Caffe's official CIFAR tutorial](http://caffe.berkeleyvision.org/gathered/examples/cifar10.html). 1. make\_densenet.py contains the code to generate the network and solver prototxt file. First change the data path in function make\_net() and preprocessing mean file in function densenet() to your own path of corresponding data file. 2. By default make\_densenet.py generates a DenseNet with Depth L=40  Growth rate k=12 and Dropout=0.2. To experiment with different settings  change the code accordingly (see the comments in the code). Example prototxt files are already included. Use ```python densenet_make.py``` to generate new prototxt files. 3. Change the caffe path in train.sh. Then use ```sh train.sh``` to train a DenseNet.   """;Computer Vision;https://github.com/liuzhuang13/DenseNetCaffe
"""    sh train.sh   """;General;https://github.com/JusperLee/UtterancePIT-Speech-Separation
"""DeBERTa (Decoding-enhanced BERT with disentangled attention) improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism  where each word is represented using two vectors that encode its content and position  respectively  and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second  an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks.   `pip install deberta`   Our fine-tuning experiments are carried on half a DGX-2 node with 8x32 V100 GPU cards  the results may vary due to different GPU models  drivers  CUDA SDK versions  using FP16 or FP32  and random seeds.    |MNLI xxlarge v2|   experiments/glue/mnli.sh xxlarge-v2|  91.7/91.9 +/-0.1|   4h|  |MNLI xlarge v2|    experiments/glue/mnli.sh xlarge-v2|   91.7/91.6 +/-0.1|   2.5h|  |MNLI xlarge|   experiments/glue/mnli.sh xlarge|  91.5/91.2 +/-0.1|   2.5h|   Docker is the recommended way to run the code as we already built every dependency into the our docker [bagai/deberta](https://hub.docker.com/r/bagai/deberta) and you can follow the [docker official site](https://docs.docker.com/engine/install/ubuntu/) to install docker on your machine.  To run with docker  make sure your system fullfil the requirements in the above list. Here are the steps to try the GLUE experiments: Pull the code  run `./run_docker.sh`    and then you can run the bash commands under `/DeBERTa/experiments/glue/`    Pull the code and run `pip3 install -r requirements.txt` in the root directory of the code  then enter `experiments/glue/` folder of the code and try the bash commands under that folder for glue experiments.   ``` Python  #: To apply DeBERTa into your existing code  you need to make two changes on your code  #: 1. change your model to consume DeBERTa as the encoder from DeBERTa import deberta import torch class MyModel(torch.nn.Module):   def __init__(self):     super().__init__()     #: Your existing model code     self.deberta = deberta.DeBERTa(pre_trained='base') #: Or 'large' 'base-mnli' 'large-mnli' 'xlarge' 'xlarge-mnli' 'xlarge-v2' 'xxlarge-v2'     #: Your existing model code     #: do inilization as before     #:      self.deberta.apply_state() #: Apply the pre-trained model of DeBERTa at the end of the constructor     #:   def forward(self  input_ids):     #: The inputs to DeBERTa forward are     #: `input_ids`: a torch.LongTensor of shape [batch_size  sequence_length] with the word token indices in the vocabulary     #: `token_type_ids`: an optional torch.LongTensor of shape [batch_size  sequence_length] with the token types indices selected in [0  1].      #:    Type 0 corresponds to a `sentence A` and type 1 corresponds to a `sentence B` token (see BERT paper for more details).     #: `attention_mask`: an optional parameter for input mask or attention mask.      #:   - If it's an input mask  then it will be torch.LongTensor of shape [batch_size  sequence_length] with indices selected in [0  1].      #:      It's a mask to be used if the input sequence length is smaller than the max input sequence length in the current batch.      #:      It's the mask that we typically use for attention when a batch has varying length sentences.     #:   - If it's an attention mask then if will be torch.LongTensor of shape [batch_size  sequence_length  sequence_length].      #:      In this case  it's a mask indicate which tokens in the sequence should be attended by other tokens in the sequence.      #: `output_all_encoded_layers`: whether to output results of all encoder layers  default  True     encoding = deberta.bert(input_ids)[-1]  #: 2. Change your tokenizer with the the tokenizer built in DeBERta from DeBERTa import deberta vocab_path  vocab_type = deberta.load_vocab(pretrained_id='base') tokenizer = deberta.tokenizers[vocab_type](vocab_path) #: We apply the same schema of special tokens as BERT  e.g. [CLS]  [SEP]  [MASK] max_seq_len = 512 tokens = tokenizer.tokenize('Examples input text of DeBERTa') #: Truncate long sequence tokens = tokens[:max_seq_len -2] #: Add special tokens to the `tokens` tokens = ['[CLS]'] + tokens + ['[SEP]'] input_ids = tokenizer.convert_tokens_to_ids(tokens) input_mask = [1]*len(input_ids) #: padding paddings = max_seq_len-len(input_ids) input_ids = input_ids + [0]*paddings input_mask = input_mask + [0]*paddings features = { 'input_ids': torch.tensor(input_ids  dtype=torch.int)  'input_mask': torch.tensor(input_mask  dtype=torch.int) }  ```   """;General;https://github.com/microsoft/DeBERTa
"""```shell go get github.com/LdDl/go-darknet ```   Example Go program is provided in the [example] directory. Please refer to the code on how to use this Go package.  Building and running program:  * Navigate to [example] folder     ```shell     cd $GOPATH/github.com/LdDl/go-darknet/example/base_example     ```  * Download dataset (sample of image  coco.names  yolov4.cfg (or v3)  yolov4.weights(or v3)).     ```shell     #:for yolo v4     ./download_data.sh     #:for yolo v3     ./download_data_v3.sh     ``` * Note: you don't need *coco.data* file anymore  because sh-script above does insert *coco.names* into 'names' field in *yolov4.cfg* file (so AlexeyAB's fork can deal with it properly)     So last rows in yolov4.cfg file will look like:     ```bash     ......     [yolo]     .....     iou_loss=ciou     nms_kind=greedynms     beta_nms=0.6      names = coco.names #: this is path to coco.names file     ```  * Also do not forget change batch and subdivisions sizes from:     ```shell     batch=64     subdivisions=8     ```     to     ```shell     batch=1     subdivisions=1     ```     It will reduce amount of VRAM used for detector test.   * Build and run program     Yolo V4:     ```shell     go build main.go && ./main --configFile=yolov4.cfg --weightsFile=yolov4.weights --imageFile=sample.jpg     ```      Output should be something like this:     ```shell     traffic light (9): 73.5039% | start point: (238 73) | end point: (251  106)     truck (7): 96.6401% | start point: (95 79) | end point: (233  287)     truck (7): 96.4774% | start point: (662 158) | end point: (800  321)     truck (7): 96.1841% | start point: (0 77) | end point: (86  333)     truck (7): 46.8695% | start point: (434 173) | end point: (559  216)     car (2): 99.7370% | start point: (512 188) | end point: (741  329)     car (2): 99.2533% | start point: (260 191) | end point: (422  322)     car (2): 99.0333% | start point: (425 201) | end point: (547  309)     car (2): 83.3919% | start point: (386 210) | end point: (437  287)     car (2): 75.8621% | start point: (73 199) | end point: (102  274)     car (2): 39.1925% | start point: (386 206) | end point: (442  240)     bicycle (1): 76.3121% | start point: (189 298) | end point: (253  402)     person (0): 97.7213% | start point: (141 129) | end point: (283  362)     ```      Yolo V3:     ```     go build main.go && ./main --configFile=yolov3.cfg --weightsFile=yolov3.weights --imageFile=sample.jpg     ```      Output should be something like this:     ```shell     truck (7): 49.5197% | start point: (0 136) | end point: (85  311)     car (2): 36.3747% | start point: (95 152) | end point: (186  283)     truck (7): 48.4384% | start point: (95 152) | end point: (186  283)     truck (7): 45.6590% | start point: (694 178) | end point: (798  310)     car (2): 76.8379% | start point: (1 145) | end point: (84  324)     truck (7): 25.5731% | start point: (107 89) | end point: (215  263)     car (2): 99.8783% | start point: (511 185) | end point: (748  328)     car (2): 99.8194% | start point: (261 189) | end point: (427  322)     car (2): 99.6408% | start point: (426 197) | end point: (539  311)     car (2): 74.5610% | start point: (692 186) | end point: (796  316)     car (2): 72.8053% | start point: (388 206) | end point: (437  276)     bicycle (1): 72.2932% | start point: (178 270) | end point: (268  406)     person (0): 97.3026% | start point: (143 135) | end point: (268  343)     ```   """;Computer Vision;https://github.com/LdDl/go-darknet
"""$ export THEANO_FLAGS=device=gpu floatX=float32   $ export THEANO_FLAGS=device=gpu floatX=float32   """;Natural Language Processing;https://github.com/nyu-dl/dl4mt-simul-trans
"""This Git is intended as a playground for experimenting with various neural network models and libraries. It contains implementations of  * mnist_mlp: A simple multilayer perceptron for MNIST implemented with keras * mnist_cnn: A simple convolutional neural network for MNIST implemented with keras * usps_cnn: A simple convolutional neural network for USPS dataset implemented with keras * variational_autoencoder: Two implementations (one in pure Theano  one in lasagne) of the model proposed in   *If cPickle import throughs error  change it to _pickle*   """;Computer Vision;https://github.com/bsivanantham/VariationalAutoEncoder
"""**Normalizing flows** (NFs) are an exciting new family of neural networks for density estimation  sampling  and likelihood inference [1]. A NF is built by combining *invertible* components with tractable Jacobian computations  and they are shown to generalize many known models  such as invertible ResNets or autoregressive architectures.  This code provides a self-contained demo in pure PyTorch of a real-world NF (RealNVP  [2]). We try to make our code as concise and simple as possible  with low dependency on external libraries.   """;Computer Vision;https://github.com/ispamm/realnvp-demo-pytorch
"""```python from hrnet import HRNet  ''' parameters from left to right:   input channels  first branch channels (hyper parameter)  output channels    notice:   Only the number of first branch channels is necessary  numbers of channels of    other branches are calculated according to the paper ''' model = HRNet(3  16  8) ```   """;Computer Vision;https://github.com/shuuchen/HRNet
""":build path to train dir   path_trained_model = os.path.abspath(trainedModel_filename)   """;Computer Vision;https://github.com/MeAmarP/Fruit-Classification
"""AS-MLP Pytorch  shift is implemented by cupy  and can only run on GPU.   ```python import jittor as jt from models_jittor import gMLPForImageClassification as gMLP_jt from models_jittor import ResMLPForImageClassification as ResMLP_jt from models_jittor import MLPMixerForImageClassification as MLPMixer_jt from models_jittor import ViP as ViP_jt from models_jittor import S2MLPv2 as S2MLPv2_jt from models_jittor import S2MLPv1_deep as S2MLPv1_deep_jt  from models_jittor import ConvMixer as ConvMixer_jt from models_jittor import convmlp_s as ConvMLP_s_jt  from models_jittor import convmlp_l as ConvMLP_l_jt  from models_jittor import convmlp_m as ConvMLP_m_jt  from models_jittor import RaftMLP as RaftMLP_jt from models_jittor import SparseMLP as SparseMLP_jt from models_jittor import HireMLP as HireMLP_jt from models_jittor import AS_MLP as AS_MLP_jt  model_jt = MLPMixer_jt(     image_size=(224 112)      patch_size=16      in_channels=3      num_classes=1000      d_model=256      depth=12  )  images = jt.randn(8  3  224  224) with jt.no_grad():     output = model_jt(images) print(output.shape) #: （8， 1000）  #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:  import torch from models_pytorch import gMLPForImageClassification as gMLP_pt from models_pytorch import ResMLPForImageClassification as ResMLP_pt from models_pytorch import MLPMixerForImageClassification as MLPMixer_pt from models_pytorch import ViP as ViP_pt from models_pytorch import S2MLPv2 as S2MLPv2_pt  from models_pytorch import S2MLPv1_deep as S2MLPv1_deep_pt from models_pytorch import ConvMixer as ConvMixer_pt  from models_pytorch import convmlp_s as ConvMLP_s_pt  from models_pytorch import convmlp_l as ConvMLP_l_pt  from models_pytorch import convmlp_m as ConvMLP_m_pt  from models_pytorch import RaftMLP as RaftMLP_pt from models_pytorch import SparseMLP as SparseMLP_pt from models_pytorch import HireMLP as HireMLP_pt from models_pytorch import GFNet as GFNet_pt from models_pytorch import CycleMLP_B2 as CycleMLP_B2_pt from models_pytorch import AS_MLP as AS_MLP_pt  model_pt = ViP_pt(     image_size=224      patch_size=16      in_channels=3      num_classes=1000      d_model=256      depth=30      segments = 16      weighted = True )  images = torch.randn(8  3  224  224)  with torch.no_grad():     output = model_pt(images) print(output.shape) #: （8， 1000）   #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: Non-square images and patch sizes #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:  model_jt = ViP_jt(     image_size=(224  112)      patch_size=(16  8)      in_channels=3      num_classes=1000      d_model=256      depth=30      segments = 16      weighted = True ) images = jt.randn(8  3  224  112) with jt.no_grad():     output = model_jt(images) print(output.shape) #: （8， 1000）  #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: 2 Stages S2MLPv2 #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: model_pt = S2MLPv2_pt(     in_channels = 3      image_size = (224 224)      patch_size = [(7 7)  (2 2)]      d_model = [192  384]      depth = [4  14]      num_classes = 1000       expansion_factor = [3  3] )  #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: ConvMLP With Pretrain Params #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: model_jt = ConvMLP_s_jt(pretrained = True  num_classes = 1000)   #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: RaftMLP #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: model_jt = RaftMLP_jt(         layers = [             {""depth"": 12              ""dim"": 768              ""patch_size"": 16              ""raft_size"": 4}         ]          gap = True     )  #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: SparseMLP #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: model_pt = SparseMLP_pt(         image_size=224          patch_size=4          in_channels=3          num_classes=1000          d_model=96          depth=[2 10 24 2]          expansion_factor = 2          patcher_norm= True     )  #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: HireMLP #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:  model_pt = HireMLP_pt(         patch_size=4          in_channels=3          num_classes=1000          d_model=[64  128  320  512]          h = [4 3 3 2]          w = [4 3 3 2]          cross_region_step = [2 2 1 1]          cross_region_interval = 2          depth=[4 6 24 3]          expansion_factor = 2          patcher_norm = True      	padding_type = 'circular'      )  #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: GFNet #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: model_pt = GFNet_pt()   #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: CycleMLP #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: model_pt = CycleMLP_B2_pt()  #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: AS-MLP #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: model_pt = AS_MLP_pt()  ```     """;Computer Vision;https://github.com/liuruiyang98/Jittor-MLP
"""I tested this project with python 3.6.9 with the following packages and versions: tensorflow==1.14.0 gast==0.2.2 (note that you have to install this specific version and overwrite the one installed by tensorflow  o/w there will be errors)  You can install all the packages I used (for running on CPU) using `pip install -r requirements-cpu.txt`  preferrablely under using a virtual environment.   If you want to use GPU  use `pip install -r requirements.txt`. Depending on your CUDA version  this may require additional setups.   First  use chmod +x *.sh to make all shell files executable.   Instance Model joins all the representations from tokens indexed with `entity_mask`  (aka token `[CLS]`  `PROT1`  and `PROT2`) and use this long output as input to the classification layers.  The `entity_mask` for the above sentence is `[0  1  4]`  where 1 and 4 are the positions of `PROT1` and `PROT2` in the sentence.  Note that if you use instance model  you would implemented the way to calculate `entity_mask` yourself. The function is `get_entity_mask()`. It takes 2 input parameters: 1. `tokens`: this is the list of tokens after tokenization. 2. `tokenizer`: this is the tokenizer used for tokenizing the sentence.  Instance model is implemented in [instance_model.py](instance_model.py).   Both the scripts mentioned below should be able to run after you changed `PROJECT_DIR`.  Refer to the [BERT repo](https://github.com/google-research/bert) for additional pretrained models  as well as [BioBert repo](https://github.com/dmis-lab/biobert) for models pre-trained with bio text.   Run [predict.sh](predict.sh) to predict. Please refer to the file itself for documentations.  `test_results.csv` will be generated under directory `TRAINED_CLASSIFIER` after prediction. It's a tsv file with 4 columns: 1. guid 2. predicted probabilities for each category (Note: multiple categories not tested) 3. real label for the sentence 4. the sentence itself   """;Natural Language Processing;https://github.com/luckynozomi/PPI_Bert
"""the include shell script launch_train_sbatch.sh is setup to all training on the Enki Cluster.   2019-02-27 13:05:03.068185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0  1  2  3   2019-02-27 13:05:04.527345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 3   2019-02-27 13:05:04.528317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y Y Y   2019-02-27 13:05:04.529605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N Y Y   2019-02-27 13:05:04.530902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   Y Y N Y   2019-02-27 13:05:04.532257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   Y Y Y N   2019-02-27 13:45:03.442171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0  1  2  3   2019-02-27 13:45:05.405293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 2 3   2019-02-27 13:45:05.405302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y N N   2019-02-27 13:45:05.405307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N N N   2019-02-27 13:45:05.405311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 2:   N N N Y   2019-02-27 13:45:05.405315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 3:   N N Y N   2019-02-27 13:40:43.708536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:    2019-02-27 13:40:43.708571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0   2019-02-27 13:40:44.094585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0   2019-02-27 13:40:44.094608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N   2019-02-27 13:40:44.094840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 7490 MB memory) -> physical GPU (device: 0  name: GeForce GTX 1080 Ti  pci bus id: 0000:d8:00.0  compute capability: 6.1)                           lmdb database to use for (Required)                           lmdb database to use for testing (Required)                           Options: ['cpu'  'gpu:#:']. Use the GPU if you have a   gradient_update_location: whether to perform gradient averaging on the CPU or GPU. See the above discussion about GPU connection topology.   """;Computer Vision;https://github.com/usnistgov/small-data-cnns
"""Hence  one should be extremely cautious when comparing MAML with its competitors as is evident from the discussion above.   Python 3.6.8 (or any Python 3 distribution)  PyTorch 1.3.1 (or any PyTorch > 1.0)   python train.py --config=configs/convnet4/mini-imagenet/train_reproduce.yaml --gpu=0 1   * MAML-Pytorch https://github.com/dragen1860/MAML-Pytorch  * HowToTrainYourMAMLPytorch https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch   """;General;https://github.com/fmu2/PyTorch-MAML
"""This package is not distributed on PyPI - you will have to install it from source:  ```bash $> git clone github.com/aaronsnoswell/jitterbug-dmc $> cd jitterbug-dmc $> pip install . ```  To test the installation:  ```bash $> cd ~ $> python >>> import jitterbug_dmc >>> jitterbug_dmc.demo() ```   following command from the 'benchmarks' directory:   enter the command   """;Computer Vision;https://github.com/RoboticsDesignLab/jitterbug
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;Computer Vision;https://github.com/trongnghia00/darknet
"""| It’s okay if you don’t like me. Not everyone has good taste.      | Sarcastic         |   I would like to reiterate that the model is far from perfect. For best performance I would suggest you use examples which do not have any conversational dependence or history. They should summarize emotions in a single sentence (2 at maximum ). Please try to give as much context as possible since phrases such as **Sure!**   **What a day!** could be interpreted as both sarcastic and otherwise depending on the context.   I will try to improve the model by training it on more examples in the future. The goal of this repo is only to implement and understand Encoders. For **practical usage using a pre-trained model such as BERT obviously yields better performance**   """;General;https://github.com/rajlm10/Chandler
"""This code applies [dynamic evaluation](https://arxiv.org/abs/1709.07432) to pretrained Transformer-XL models from this [paper](https://arxiv.org/abs/1901.02860). Our codebase is a modified version of [their codebase](https://github.com/kimiyoung/transformer-xl). We used this code to obtain state of the art results on WikiText-103 (perplexity: 16.4)  enwik8 (bits/char: 0.94)  and text8 (bits/char: 1.04).   """;Natural Language Processing;https://github.com/benkrause/dynamiceval-transformer
"""You can refer https://github.com/amdegroot/ssd.pytorch for further information.    """;Computer Vision;https://github.com/arvind1998/SSD-Object-Detection
"""![Figure 1 from paper](examples/figure1.png)  Pytorch implementation of paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929).  We provide the pretrained pytorch weights which are converted from pretrained jax/flax models. We also provide fine-tune and evaluation script.  Similar results as in [original implementation](https://github.com/google-research/vision_transformer) are achieved.    Create environment: ``` conda create --name vit --file requirements.txt conda activate vit ```   """;Computer Vision;https://github.com/asyml/vision-transformer-pytorch
"""    $ git clone https://github.com/eriklindernoren/PyTorch-GAN     $ cd PyTorch-GAN/     $ sudo pip3 install -r requirements.txt   $ cd implementations/pixelda/   ``` $ cd implementations/acgan/ $ python3 acgan.py ```  <p align=""center"">     <img src=""assets/acgan.gif"" width=""360""\> </p>   ``` $ cd implementations/aae/ $ python3 aae.py ```   _BEGAN: Boundary Equilibrium Generative Adversarial Networks_   ``` $ cd implementations/began/ $ python3 began.py ```   ``` $ cd data/ $ bash download_pix2pix_dataset.sh edges2shoes $ cd ../implementations/bicyclegan/ $ python3 bicyclegan.py ```  <p align=""center"">     <img src=""assets/bicyclegan.png"" width=""480""\> </p> <p align=""center"">     Various style translations by varying the latent code. </p>    ``` $ cd implementations/bgan/ $ python3 bgan.py ```   ``` $ cd implementations/cluster_gan/ $ python3 clustergan.py ```  <p align=""center"">     <img src=""assets/cluster_gan.gif"" width=""360""\> </p>    ``` $ cd implementations/cgan/ $ python3 cgan.py ```  <p align=""center"">     <img src=""assets/cgan.gif"" width=""360""\> </p>   ``` $ cd implementations/ccgan/ $ python3 ccgan.py ```   ``` $ cd implementations/context_encoder/ <follow steps at the top of context_encoder.py> $ python3 context_encoder.py ```  <p align=""center"">     <img src=""assets/context_encoder.png"" width=""640""\> </p> <p align=""center"">     Rows: Masked | Inpainted | Original | Masked | Inpainted | Original </p>   ``` $ cd implementations/cogan/ $ python3 cogan.py ```  <p align=""center"">     <img src=""assets/cogan.gif"" width=""360""\> </p> <p align=""center"">     Generated MNIST and MNIST-M images </p>   ``` $ cd data/ $ bash download_cyclegan_dataset.sh monet2photo $ cd ../implementations/cyclegan/ $ python3 cyclegan.py --dataset_name monet2photo ```  <p align=""center"">     <img src=""assets/cyclegan.png"" width=""900""\> </p> <p align=""center"">     Monet to photo translations. </p>   ``` $ cd implementations/dcgan/ $ python3 dcgan.py ```  <p align=""center"">     <img src=""assets/dcgan.gif"" width=""240""\> </p>   ``` $ cd data/ $ bash download_pix2pix_dataset.sh edges2shoes $ cd ../implementations/discogan/ $ python3 discogan.py --dataset_name edges2shoes ```  <p align=""center"">     <img src=""assets/discogan.png"" width=""480""\> </p> <p align=""center"">     Rows from top to bottom: (1) Real image from domain A (2) Translated image from <br>     domain A (3) Reconstructed image from domain A (4) Real image from domain B (5) <br>     Translated image from domain B (6) Reconstructed image from domain B </p>   ``` $ cd implementations/dragan/ $ python3 dragan.py ```   ``` $ cd data/ $ bash download_pix2pix_dataset.sh facades $ cd ../implementations/dualgan/ $ python3 dualgan.py --dataset_name facades ```   ``` $ cd implementations/ebgan/ $ python3 ebgan.py ```   ``` $ cd implementations/esrgan/ <follow steps at the top of esrgan.py> $ python3 esrgan.py ```  <p align=""center"">     <img src=""assets/enhanced_superresgan.png"" width=""320""\> </p> <p align=""center"">     Nearest Neighbor Upsampling | ESRGAN </p>   ``` $ cd implementations/gan/ $ python3 gan.py ```  <p align=""center"">     <img src=""assets/gan.gif"" width=""240""\> </p>   ``` $ cd implementations/infogan/ $ python3 infogan.py ```  <p align=""center"">     <img src=""assets/infogan.gif"" width=""360""\> </p> <p align=""center"">     Result of varying categorical latent variable by column. </p>  <p align=""center"">     <img src=""assets/infogan.png"" width=""360""\> </p> <p align=""center"">     Result of varying continuous latent variable by row. </p>   ``` $ cd implementations/lsgan/ $ python3 lsgan.py ```    ``` $ cd data/ $ bash download_pix2pix_dataset.sh edges2shoes $ cd ../implementations/munit/ $ python3 munit.py --dataset_name edges2shoes ```  <p align=""center"">     <img src=""assets/munit.png"" width=""480""\> </p> <p align=""center"">     Results by varying the style code. </p>   ``` $ cd data/ $ bash download_pix2pix_dataset.sh facades $ cd ../implementations/pix2pix/ $ python3 pix2pix.py --dataset_name facades ```  <p align=""center"">     <img src=""assets/pix2pix.png"" width=""480""\> </p> <p align=""center"">     Rows from top to bottom: (1) The condition for the generator (2) Generated image <br>     based of condition (3) The true corresponding image to the condition </p>   ``` $ cd implementations/relativistic_gan/ $ python3 relativistic_gan.py                 #: Relativistic Standard GAN $ python3 relativistic_gan.py --rel_avg_gan   #: Relativistic Average GAN ```   ``` $ cd implementations/sgan/ $ python3 sgan.py ```   ``` $ cd implementations/softmax_gan/ $ python3 softmax_gan.py ```   ``` $ cd implementations/stargan/ <follow steps at the top of stargan.py> $ python3 stargan.py ```  <p align=""center"">     <img src=""assets/stargan.png"" width=""640""\> </p> <p align=""center"">     Original | Black Hair | Blonde Hair | Brown Hair | Gender Flip | Aged </p>   ``` $ cd implementations/srgan/ <follow steps at the top of srgan.py> $ python3 srgan.py ```  <p align=""center"">     <img src=""assets/superresgan.png"" width=""320""\> </p> <p align=""center"">     Nearest Neighbor Upsampling | SRGAN </p>   ``` $ cd data/ $ bash download_cyclegan_dataset.sh apple2orange $ cd implementations/unit/ $ python3 unit.py --dataset_name apple2orange ```   ``` $ cd implementations/wgan/ $ python3 wgan.py ```   ``` $ cd implementations/wgan_gp/ $ python3 wgan_gp.py ```  <p align=""center"">     <img src=""assets/wgan_gp.gif"" width=""240""\> </p>   ``` $ cd implementations/wgan_div/ $ python3 wgan_div.py ```  <p align=""center"">     <img src=""assets/wgan_div.png"" width=""240""\> </p>  """;Computer Vision;https://github.com/eriklindernoren/PyTorch-GAN
"""<p float=""center"">     <img src=""figures/eca_module.jpg"" width=""1000"" alt=""Struct."">     <br>     <em>Structural comparison of SE and ECA attention mechanism.</em> </p>  Efficient Channel Attention (ECA) is a simple efficient extension of the popular Squeeze-and-Excitation Attention Mechanism  which is based on the foundation concept of Local Cross Channel Interaction (CCI). Instead of using fully-connected layers with reduction ratio bottleneck as in the case of SENets  ECANet uses an adaptive shared (across channels) 1D convolution kernel on the downsampled GAP *C* x 1 x 1 tensor. ECA is an equivalently plug and play module similar to SE attention mechanism and can be added anywhere in the blocks of a deep convolutional neural networks. Because of the shared 1D kernel  the parameter overhead and FLOPs cost added by ECA is significantly lower than that of SENets while achieving similar or superior performance owing to it's capabilities of constructing adaptive kernels. This work was accepted at the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)  2020.    ``` pip install -r requirements.txt ```  This reproduction is build on PyTorch and MMDetection. Ensure you have CUDA Toolkit > 10.1 installed. For more details regarding installation of MMDetection  please visit this [resources page](https://mmdetection.readthedocs.io/en/latest/get_started.html#installation).  If ```pip install mmcv-full``` takes a lot of time or fails  use the following line (customize the torch and cuda versions as per your requirements): ``` pip install mmcv-full==latest+torch1.7.0+cu101 -f https://download.openmmlab.com/mmcv/dist/index.html ```  Although [Echo](https://github.com/digantamisra98/Echo) can be installed via pip  the features we currently use in this project aren't available in the latest pip version. So it's advisable to rather install from source by the following commands and then clone this repository within the directory where Echo source is present and installed in your environment/local/instance:  ``` import os git clone https://github.com/digantamisra98/Echo.git os.chdir(""/path_to_Echo"") git clone https://github.com/digantamisra98/ECANet.git pip install -e ""/path_to_Echo/"" ```   Using the above linked colab notebook  you can run comparative runs for different attention mechanisms on CIFAR-10 using ResNets. You can add your own attention mechanisms by adding them in the source of Echo package.   Note: MMDetection has significantly changed since and hence this notebook would be incompatible with the latest version.   """;Computer Vision;https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET
"""pip install tensorboardX   ``` python3 main.py -h  usage: main.py [-h] [--gpus GPUS] [--cpus CPUS] [--save_dir SAVE_DIR]                [--img_num IMG_NUM] [--optim_G {adam sgd}]                [--optim_D {adam sgd}] [--loss {wgangp lsgan}]                [--start_resl START_RESL] [--end_resl END_RESL]                [--beta [BETA [BETA ...]]] [--momentum MOMENTUM]                [--decay DECAY] [--gp_lambda GP_LAMBDA]  PGGAN  optional arguments:   -h  --help            show this help message and exit   --gpus GPUS           Select GPU Numbering | 0 1 2 3 |   --cpus CPUS           The number of CPU workers   --save_dir SAVE_DIR   Directory which models will be saved in   --img_num IMG_NUM     The number of images to be used for each phase   --optim_G {adam sgd}   --optim_D {adam sgd}   --loss {wgangp lsgan}   --start_resl START_RESL   --end_resl END_RESL   --beta [BETA [BETA ...]]                         Beta for Adam optimizer   --momentum MOMENTUM   Momentum for SGD optimizer   --decay DECAY         Weight decay for optimizers   --gp_lambda GP_LAMBDA                         Lambda as a weight of Gradient Panelty in WGAN-GP loss ```   ####################################### 1. 训练数据预处理 cd ./datas python3 preDataset.py     """;Computer Vision;https://github.com/suhuijia/pggan
"""Before running the app from the commandline check with the requirements.txt file which python libraries are used in both apps. If needed   pip install the requirements.txt ahead of running the apps.   """;Sequential;https://github.com/bfopengradient/NLP_text_summarization_apps
"""All unit tests in baselines can be run using pytest runner: ``` pip install pytest pytest ```   - Clone the repo and cd into it:     ```bash     git clone https://github.com/openai/baselines.git     cd baselines     ``` - If you don't have TensorFlow installed already  install your favourite flavor of TensorFlow. In most cases  you may use     ```bash      pip install tensorflow-gpu==1.14 #: if you have a CUDA-compatible gpu and proper drivers     ```     or      ```bash     pip install tensorflow==1.14     ```     to install Tensorflow 1.14  which is the latest version of Tensorflow supported by the master branch. Refer to [TensorFlow installation guide](https://www.tensorflow.org/install/)     for more details.   - Install baselines package     ```bash     pip install -e .     ```   sudo apt-get update &amp;&amp; sudo apt-get install cmake libopenmpi-dev python3-dev zlib1g-dev  Installation of system packages on Mac requires Homebrew. With Homebrew installed  run the following:  brew install cmake openmpi  From the general python package sanity perspective  it is a good idea to use virtual environments (virtualenvs) to make sure packages from different projects do not interfere with each other. You can install virtualenv (which is itself a pip package) via  pip install virtualenv   To create a virtualenv called venv with python3  one runs   virtualenv /path/to/venv --python=python3  To activate a virtualenv:    python -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v4 --num_timesteps=2e7 --save_path=~/models/pong_20M_ppo2   python -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v4 --num_timesteps=0 --load_path=~/models/pong_20M_ppo2 --play   The directory can be changed with the --log_path command-line option.  python -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v4 --num_timesteps=2e7 --save_path=~/models/pong_20M_ppo2 --log_path=~/logs/Pong/   Another way the temp directory can be changed is through the use of the $OPENAI_LOGDIR environment variable.   For instance  to train a fully-connected network controlling MuJoCo humanoid using PPO2 for 20M timesteps ```bash python -m baselines.run --alg=ppo2 --env=Humanoid-v2 --network=mlp --num_timesteps=2e7 ``` Note that for mujoco environments fully-connected network is default  so we can omit `--network=mlp` The hyperparameters for both network and the learning algorithm can be controlled via the command line  for instance: ```bash python -m baselines.run --alg=ppo2 --env=Humanoid-v2 --network=mlp --num_timesteps=2e7 --ent_coef=0.1 --num_hidden=32 --num_layers=3 --value_network=copy ``` will set entropy coefficient to 0.1  and construct fully connected network with 3 layers with 32 hidden units in each  and create a separate network for value function estimation (so that its parameters are not shared with the policy network  but the structure is the same)  See docstrings in [common/models.py](baselines/common/models.py) for description of network parameters for each type of model  and  docstring for [baselines/ppo2/ppo2.py/learn()](baselines/ppo2/ppo2.py#L152) for the description of the ppo2 hyperparameters.    DQN with Atari is at this point a classics of benchmarks. To run the baselines implementation of DQN on Atari Pong: ``` python -m baselines.run --alg=deepq --env=PongNoFrameskip-v4 --num_timesteps=1e6 ```   """;Reinforcement Learning;https://github.com/openai/baselines
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/zsweet/BERT_zsw
""" **BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.    In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant    See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/thanhlong1997/bert_quora
"""Once in the tmux session  you can see all your windows with ctrl-b w.   environments with low latency.  Alternatively  you can run the   stderr.  If you run both the agent and the environment on nearby   (you can connect to this view via note above)   ``` conda create --name universe-starter-agent python=3.5 source activate universe-starter-agent  brew install tmux htop cmake golang libjpeg-turbo      #: On Linux use sudo apt-get install -y tmux htop cmake golang libjpeg-dev  pip install ""gym[atari]"" pip install universe pip install six pip install tensorflow conda install -y -c https://conda.binstar.org/menpo opencv3 conda install -y numpy conda install -y scipy ```   Add the following to your `.bashrc` so that you'll have the correct environment when the `train.py` script spawns new bash shells ```source activate universe-starter-agent```   """;General;https://github.com/openai/universe-starter-agent
"""This repository contains code to train and evaluate 3D Convolutional Neural Networks for semantic segmentation on medical images. The architectures developed in this framework are a combination of auto-encoder [UNet](https://arxiv.org/abs/1505.04597) with shortcut connections as in [ResNet](https://arxiv.org/abs/1512.03385)  densely connections for deep supervision as in [DensetNet](https://arxiv.org/abs/1608.06993) and Merge-And-Run mapping for attention focusing as in [MRGE](https://arxiv.org/abs/1611.07718).   Clone the repository and install the requirements ```shell $ git clone https://gitlab.com/iss_mia/cnn_segmentation/ desired_directory $ python3 -m pip install -r requirements.txt ```   Set all parameters in the [configuration file](./config/config_default.yaml). Check call arguments: ```shell $ python3 main.py -h  ```   """;General;https://github.com/lab-midas/med_segmentation
"""If you would like to quickly start using the AI explainability 360 toolkit without cloning this repository  then you can install the [aix360 pypi package](https://pypi.org/project/aix360/) as follows.   ```bash (your environment)$ pip install aix360 ```  If you follow this approach  you may need to download the notebooks in the [examples](./examples) folder separately.     Clone the latest version of this repository:  ```bash (aix360)$ git clone https://github.com/Trusted-AI/AIX360 ```  If you'd like to run the examples and tutorial notebooks  download the datasets now and place them in their respective folders as described in [aix360/data/README.md](aix360/data/README.md).  Then  navigate to the root directory of the project which contains `setup.py` file and run:  ```bash (aix360)$ pip install -e . ```   Supported Configurations:  | OS      | Python version | | ------- | -------------- | | macOS   | 3.6  | | Ubuntu  | 3.6  | | Windows | 3.6  |   with other projects on your system. A virtual environment manager is strongly  recommended to ensure dependencies may be installed safely. If you have trouble installing the toolkit  try this first.   if you are curious) and can be installed from   Then  to create a new Python 3.6 environment  run:  conda create --name aix360 python=3.6  conda activate aix360  The shell should now look like (aix360) $. To deactivate the environment  run:  (aix360)$ conda deactivate   Note: Older versions of conda may use source activate aix360 and source  deactivate (activate aix360 and deactivate on Windows).   """;General;https://github.com/Trusted-AI/AIX360
"""path = r""/Users/edvardhulten/real_nvp_2d/""  #: change to your own path (unless your name is Edvard Hultén too)   """;Computer Vision;https://github.com/e-hulten/real-nvp-2d
"""bash ./download_datasets.sh apple2orange  Please ensure that you have the following directory tree structure in your repository.       bash ./scripts/train_base.sh apple2orange   Check the folder name in checkpoints directory (e.g.  apple2orange).       bash ./scripts/test_base.sh apple2orange 2018_10_16_14_49_55   In recent experiments  we found that  spectral normaliation (SN) can help stabilize the training stage. So we add SN in this implementation. You may need to update your pytorch to 0.4.1 to support SN  or use an old version without SN.   """;Computer Vision;https://github.com/Xiaoming-Yu/SingleGAN
"""Instructions for acquiring PTB and WT2 can be found here. While CIFAR-10 can be automatically downloaded by torchvision  ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions here.   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/siddikui/Fast-DARTS
"""This implementation is based on [mmdetection](https://github.com/open-mmlab/mmdetection)(v1.0.0). Please refer to [INSTALL.md](docs/INSTALL.md) for installation and dataset preparation.   For your convenience  we provide the following trained models on COCO (more models are coming soon).   Once the installation is done  you can download the provided models and use [inference_demo.py](demo/inference_demo.py) to run a quick demo.   """;Computer Vision;https://github.com/Fei-dong/SOLO_SORT
"""Type juptyer notebook into terminal and a browser window will pop up. Click on neural style transfer1.ipynb. You can iteratively compile each block of code to see the output results.        """;Computer Vision;https://github.com/rishabh-vij/Neural_Style_transfer
"""[ImageNet](http://www.image-net.org/) is a common academic data set in machine learning for training an image recognition system. Code in this directory demonstrates how to use TensorFlow to train and evaluate a type of convolutional neural network (CNN) on this data set.      http://arxiv.org/abs/1512.00567      This network achieves 21.2% top-1 and 5.6% top-5 error for single frame evaluation with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. Below is a visualization of the model architecture.   ![image](https://github.com/r-karthik/images/blob/master/detection_of_pests/layers.png)            Steps to Follow   1. Fatkun batch Download (chrome Extension)   Python 3.5+   Install python 3.5 or above  tensorflow & Docker toolbox.   Clone the repository & navigate to the directory.      git clone https://github.com/r-karthik/Detection-of-pests.git   ![image](https://github.com/r-karthik/images/blob/master/detection_of_pests/cnn.JPG)   """;Computer Vision;https://github.com/r-karthik/Detection-of-pests
"""cd btgym  git pull  pip install --upgrade -e .   """;Computer Vision;https://github.com/Kismuz/crypto_spread_test
"""To synthesize audio:  First fetch the weights using the script provided  	bash download_weights.sh  Then pass prompts (separated by end lines) to 'test.py' through stdin. The audio appears in Tensorboard.  	python3 test.py < prompts.txt 	 	echo ""This is a test prompt for the system to say."" | python3 test.py  To train the model:  First run the data fetching script (preferably after obtaining a username and password for the Nancy corpus)  	bash download_data.sh  Then preprocess the data  	python3 preprocess.py arctic  	python3 preprocess.py nancy    Now we're ready to start training  	python3 train.py --train-set nancy (--restore optional)  To see the audio outputs created by Tacotron  open up Tensorboard.  Monitoring the attention alignments produced under the images tab in Tensorboard is by far the best way to debug your model while its training. You'll likely only see generalization to new examples if/when the attention becomes monotonic. The gif below shows the model learning an alignment using the default parameters on the Nancy dataset.  ![Attention Alignments](https://github.com/barronalex/Tacotron/raw/master/images/attention.gif)   """;Sequential;https://github.com/barronalex/Tacotron
"""All videos in the Physion test set have been manually evaluated to ensure that the behavior of the simulated physics does not feature glitches or unexpected behaviors. A small number of stimuli that contain potential physics glitches have been identified; the stimulus names can be seen [here](analysis/manual_stim_evaluation_buggy_stims.txt) or downloaded at the following link:  **Download URL**: [https://physics-benchmarking-neurips2021-dataset.s3.amazonaws.com/manual_stim_evaluation_glitchy_test_stims.txt](https://physics-benchmarking-neurips2021-dataset.s3.amazonaws.com/manual_stim_evaluation_glitchy_test_stims.txt).   Download URL: https://physics-benchmarking-neurips2021-dataset.s3.amazonaws.com/PhysionTestHDF5.tar.gz.    """;Graphs;https://github.com/cogtoolslab/physics-benchmarking-neurips2021
"""This is an implementation for the paper **""Learning Invariant Representation for Unsupervised Image Restoration"" (CVPR 2020)**  a simple and efficient framework for unsupervised image restoration  which is injected into the general domain transfer architecture. More details could be found in the original paper.    """;General;https://github.com/Wenchao-Du/LIR-for-Unsupervised-IR
"""- Install [PyTorch](http://pytorch.org/)(version v1.0 as of on March 2019) by selecting your environment on the website and running the appropriate command. - Please install cv2 and visdom form conda-forge.  - I recommend using anaconda 3.7.  - You will also need Matlab. If you have distributed computing license then it would be faster otherwise it should also be fine.  Just replace `parfor` with simple `for` in Matlab scripts. I would be happy to accept a PR for python version of this part. - Clone this repository.    * Note: We currently only support Python 3.7 with Pytorch version v1.0 on Linux system. - We currently support [UCF24](http://www.thumos.info/download.html) with [revised annotaions](https://github.com/gurkirt/corrected-UCF101-Annots) released with our [real-time online action detection paper](https://arxiv.org/pdf/1611.08563.pdf). Unlike [ROAD](https://github.com/gurkirt/realtime-action-detection) implementation  we support [JHMDB21](http://jhmdb.is.tue.mpg.de/) as well. - Similar to [ROAD](https://github.com/gurkirt/realtime-action-detection) setup  to simulate the same training and evaluation setup we provide extracted `rgb` images from videos along with optical flow images (both `brox flow` and `real-time flow`) computed for the UCF24 and JHMDB21 datasets. You can download it from my [google drive link](https://drive.google.com/drive/folders/1o0JNYZl2Wv9bi66wF_SQ4N5cxdCyHTJR?usp=sharing)) - Install opencv package for anaconda using ``conda install opencv`` - We also support [Visdom](https://github.com/facebookresearch/visdom) for visualization of loss and frame-meanAP on validation subset during training.   * To use Visdom in the browser:    ```Shell   #: First install Python server and client    conda install -c conda-forge visdom   #: Start the server (probably in a screen or tmux)   python -m visdom.server --port=8097   ```   * Then (during training) navigate to http://localhost:8097/ (see the Training section below for more details).   <a href='#installation'>Installation</a>   Similar to ROAD  we requires VGG-16 weights pretrained on UCF24 using ROAD implmentation.   If you want you can train for these weights using ROAD   OR download the above pretrained models to above directory.    You can use --fusion_type=CAT for concatnation fusion. Sum Fusion requires little less GPU memory.    For instructions on Visdom usage/installation  see the <a href='#installation'>Installation</a> section. By default  it is off.  If you don't like to use visdom then you always keep track of train using logfile which is saved under save_root directory   To compute frame-mAP you can use frameAP.m script. You will need to specify data_root  data_root.    - NMS is performed once in python then again in Matlab; one has to do that on GPU in python   Also  Feynman27 pushed a python version of the incremental_linking   """;Computer Vision;https://github.com/gurkirt/AMTNet
"""|            |                   |        | ---------- |-------------------| | **Author**       | Trần Vĩnh Toàn- Foundation 8 - VTC AI| | **Title**        | Computer Vision - EfficientNet For Fruits & Vegetables Detection App  | | **Topics**       | Ứng dụng trong computer vision  sử dụng thuật toán chính là CNN| | **Descriptions** | Input sẽ là tấm hình với các loại quả khác nhau và file labels-v2.txt chứa danh sách tên của 130 loại quả tương ứng. Dữ liệu dùng để train là dataset của 130 loại quả có kích thước (100px X 100px). Train toàn bộ dữ liệu này bằng cấu trúc mạng CNN  sử dụng model EfficientNet ( Chi tiết về model : https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet  Paper: https://arxiv.org/abs/1905.11946). khi train xong sẽ trả ra output là file trọng số ```weights```. Ta sẽ sử dụng trọng số ```weights``` đã train để predict name của các object trong hình| | **Links**        | https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet   https://github.com/rwightman/pytorch-image-models| | **Framework**    | PyTorch| | **Pretrained Models**  | sử dụng weight đã được train sẵn [https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra-a5e2fbc7.pth](https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra-a5e2fbc7.pth) [https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_es_ra-f111e99c.pth](https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_es_ra-f111e99c.pth)| | **Datasets**     |Mô hình được train với bộ dữ liệu 130 loại quả tại: https://www.kaggle.com/moltean/fruits/data| | **Level of difficulty**| Normal +  có thể train lại với tập dữ liệu khác và model khác tốc độ tùy thuộc vào CPU & GPU & data input|   This is an neural network web app visualizing the training of the network and testing accuracy ~ 99.5% accuracy. The neural network uses pretrained EfficientNet_B3 and then trained to classify images of fruits and vegetables. It is built using Pytorch framework using Python as primary language. App is built using Flask. **© Toàn Vinh**   """;Computer Vision;https://github.com/toanbkmt/EfficientnetFruitDetect
"""pip install msgpack==1.0.2  pip install msgpack-numpy==0.4.7.1  pip install spacy==2.3.5  pip install torchtext==0.4   """;Natural Language Processing;https://github.com/SeoroMin/transformer_pytorch_ver2
"""The code is based on Python 3.5  TensorFlow 1.15  and Spektral 0.1.2.  All required libraries are listed in `requirements.txt` and can be installed with  ```bash pip install -r requirements.txt ```    """;Graphs;https://github.com/FilippoMB/Spectral-Clustering-with-Graph-Neural-Networks-for-Graph-Pooling
"""Install PyTorch 0.2   For data setup  run ./getdata.sh.   PyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed.   """;General;https://github.com/jhave/RERITES-AvgWeightDescentLSTM-PoetryGeneration
"""Code adapted from https://github.com/chrischute/glow Adding conditioning layer to affine coupling layer. Tried conditioning for many domain. Applied style transfer using the property of conditional flow model. (reconstruct image giving different condition in forward and reverse procedure of Glow)    """;Computer Vision;https://github.com/5yearsKim/Conditional-Normalizing-Flow
"""First  you will need to download and setup a dataset. The easiest way is to use one of the already existing datasets on UC Berkeley's repository: ``` ./download_dataset <dataset_name> ``` Valid <dataset_name> are: apple2orange  summer2winter_yosemite  horse2zebra  monet2photo  cezanne2photo  ukiyoe2photo  vangogh2photo  maps  cityscapes  facades  iphone2dslr_flower  ae_photos  Alternatively you can build your own dataset by setting up the following directory structure:      .     ├── datasets                        |   ├── <dataset_name>          Follow the instructions in pytorch.org for your current setup   pip3 install visdom   If you don't own a GPU remove the --cuda option  although I advise you to get one!   """;Computer Vision;https://github.com/aitorzip/PyTorch-CycleGAN
"""Dowload the datasets from the UTKFace dataset. UTKFace.tar.gz can be downloaded    `python3 inference_img.py`  - Estimated result:  ![selenagomez](https://user-images.githubusercontent.com/48142689/74002820-ff997c80-49a2-11ea-815b-2c64ffc91193.jpg)   """;Computer Vision;https://github.com/buiquangmanhhp1999/age_gender_estimation
"""args.gpu: make sure you use the following gpu when running the code:  Neural Network Models |  args.gpu   """;Computer Vision;https://github.com/ericjang/odin
"""Two experiments are included in this repository  where benchmarks are from the paper [Generalized Sliced Wasserstein Distances](http://papers.nips.cc/paper/8319-generalized-sliced-wasserstein-distances) and the paper [Distributional Sliced-Wasserstein and Applications to Generative Modeling](https://arxiv.org/pdf/2002.07367.pdf)  respectively. The first one is on the task of sliced Wasserstein flow  and the second one is on generative modellings with GANs. For more details and setups  please refer to the original paper **Augmented Sliced Wasserstein Distances**.  To install the required python packages  run the following command:  pip install -r requirements.txt   The pytorch code for calculating the FID score is from https://github.com/mseitzer/pytorch-fid.   The generative modelling experiment evaluates the performances of GANs trained with different sliced-based Wasserstein metrics. To train and evaluate the model  run the following command:  ``` python main.py  --model-type ASWD --dataset CIFAR --epochs 200 --num-projection 1000 --batch-size 512 --lr 0.0005 ```   """;General;https://github.com/ShwanMario/ASWD
"""Pytorch-Lightning https://pytorch-lightning.readthedocs.io/en/latest/   """;General;https://github.com/xultaeculcis/coral-net
""" First create lmdb datasets:  > python prepare_data.py --out LMDB_PATH --n_worker N_WORKER --size SIZE1 SIZE2 SIZE3 ... DATASET_PATH  This will convert images to jpeg and pre-resizes it. This implementation does not use progressive growing  but you can create multiple resolution datasets using size arguments with comma separated lists  for the cases that you want to try another resolutions later.  Then you can train model in distributed settings  > python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train.py --batch BATCH_SIZE LMDB_PATH  train.py supports Weights & Biases logging. If you want to use it  add --wandb arguments to the script.   """;Computer Vision;https://github.com/alekseynp/stylegan2-pytorch
"""bash go_proc_doc.sh   bash process_all_keys.sh   passing --gpu -1 can run evaluation on CPUs.   """;Natural Language Processing;https://github.com/PlusLabNLP/TempGen
"""cd src/chessrl   cd src/chessrl   cd src/chessrl   """;Reinforcement Learning;https://github.com/AIRLegend/ChessRL
"""Please follow our [GitHub](https://github.com/deepmodeling/deepmd-kit) webpage to download the [latest released version](https://github.com/deepmodeling/deepmd-kit/tree/master) and [development version](https://github.com/deepmodeling/deepmd-kit/tree/devel).  DeePMD-kit offers multiple installation methods. It is recommend using easily methods like [offline packages](doc/install/easy-install.md#offline-packages)  [conda](doc/install/easy-install.md#with-conda) and [docker](doc/install/easy-install.md#with-docker).   One may manually install DeePMD-kit by following the instuctions on [installing the Python interface](doc/install/install-from-source.md#install-the-python-interface) and [installing the C++ interface](doc/install/install-from-source.md#install-the-c-interface). The C++ interface is necessary when using DeePMD-kit with LAMMPS  i-PI or GROMACS.    Download and install   Easy install  Install from source code  Install LAMMPS  Install i-PI  Install GROMACS  Building conda packages   Integrate with third-party packages   deepmd: DeePMD-kit python modules.   source/gmx: source code of Gromacs plugin.   A quick-start on using DeePMD-kit can be found as follows:  - [Prepare data with dpdata](doc/data/dpdata.md) - [Training a model](doc/train/training.md) - [Freeze a model](doc/freeze/freeze.md) - [Test a model](doc/test/test.md) - [Run MD with LAMMPS](doc/third-party/lammps.md)  A full [document](doc/train/train-input-auto.rst) on options in the training input script is available.   """;Computer Vision;https://github.com/deepmodeling/deepmd-kit
"""Two DataLoader are provided: - ImageLoader: per each iteration it loads data in image format (jpg png  ...)     - *Dataset/JigsawImageLoader.py* uses PyTorch DataLoader and iterator     - *Dataset/ImageDataLoader.py* custom implementation.  The default loader is *JigsawImageLoader.py*. *ImageDataLoader.py* is slightly faster when using single core.  The images can be preprocessed using *_produce_small_data.py_* which resize the image to 256  keeping the aspect ratio  and crops a patch of size 255x255 in the center.   The LRN layer crushes with a PyTorch version older than 0.3   """;General;https://github.com/bbrattoli/JigsawPuzzlePytorch
"""--name ffhq_aegan_wplus_decoupled \   --name ffhq_aegan_wplus_joint \   --name ffhq_gan \   --name ffhq_alae_wtied_recw=1_mlpd=4 \   """;Computer Vision;https://github.com/phymhan/stylegan2-pytorch
"""```python #:#:#: To start off let's do a basic data summary.  #: Number of training examples n_train = augmented_X_train.shape[0]  #: Number of validation examples n_validation = X_valid.shape[0]  #: Number of testing examples n_test = X_test.shape[0]  #: What's the shape of an image? image_shape = augmented_X_train[0].shape  #: How many classes are in the dataset n_classes = np.unique(augmented_y_train).shape[0]  print(""Number of training examples =""  n_train) print(""Number of validation examples =""  n_validation) print(""Number of testing examples =""  n_test) print(""Image data shape =""  image_shape) print(""Number of classes =""  n_classes) ```      Number of training examples = 155275     Number of validation examples = 4410     Number of testing examples = 12630     Image data shape = (32  32  3)     Number of classes = 43    ```python X_train = augmented_X_train y_train = augmented_y_train  ```   ```python #:#:#: To start off let's do a basic data summary.  #: Number of training examples n_train = X_train.shape[0]  #: Number of validation examples n_validation = X_valid.shape[0]  #: Number of testing examples n_test = X_test.shape[0]  #: What's the shape of an image? image_shape = X_train[0].shape  #: How many classes are in the dataset n_classes = np.unique(y_train).shape[0]  print(""Number of training examples =""  n_train) print(""Number of validation examples =""  n_validation) print(""Number of testing examples =""  n_test) print(""Image data shape =""  image_shape) print(""Number of classes =""  n_classes) ```      Number of training examples = 34799     Number of validation examples = 4410     Number of testing examples = 12630     Image data shape = (32  32  3)     Number of classes = 43    * sample_images/children.jpg --> Children crossing  Test Set --> Children crossing 100% match  * sample_images/wild_animal_crossing.jpg --> Wild animals crossing  Test Set --> Wild animals crossing 100% match  * sample_images/loose_gravel.jpg --> Yield  Test Set --> Yield no match  Test Set prediction correct but not a match on the new image * sample_images/maximum_height_allowed.jpg --> Go straight or right  Test Set -->   Test Set prediction correct but not a match on the new image * sample_images/60_speedlimit.jpg --> Speed limit (60km/h)  Test Set --> Speed limit (60km/h)100% match  The test set has an accuracy of 100% compared to the new sample image prediction accuracy of 60%. This can be attributed to overfitting on the test set. The image quality is also a determining factor on these results.     I performed the following data preprocessing steps:  * Initial test of the model performance with current dataset without twicking any parameters   * Trying to get the model to perform with an accuracy rate over 93% is quite tricky because they are so many paremeters that can be tuned and twicked. There are multiple approaches my first step for the machine learning pipeline was to run the model first and then pre-process the data.     * Image Augmentation details are explained the ""How I did Data Augmentation"" section   * Standardize the pixel values: `new_value = (old_value - 128) / 128`   * This has the effect of zero-centering the data  and making the data fall within the range -1 to 1   * Standardizing the pixel values helps gradient descent converge faster   * Dividing the pixel values by 128 is not strictly necessary for this project  because all original pixel values are on the same scale  from 0 to 255. However  this division step is computationally cheap  and it's good practice in the long term to standardize the image preprocessing requirements  over many different projects with potentially different image representations. Over the course of many projects in the future  abiding by this practice can help reduce confusion and potential frustration.    * Convert the integer class labels into one-hot encoded labels   * This is important because different traffic signs do not have integer-like relationships with one another. For example  if a stop sign is labeled 0  a speed limit sign labeled 1  and a yield sign labeled 2  an integer label would imply a stop sign is more similar to a speed limit sign than a yield sign. A neural network classifier would mathematically assume this relationship as well. Practically  there is no such relationship. By converting all labels into one-hot encoded labels  we avoid this incorrect underlying assumption.    * Randomly shuffle the data   * It is extremely important to shuffle the training data  so that we do not obtain entire minibatches of    highly correlated examples  * If the data is given in some meaningful order  this can bias the gradient and lead to poor convergence.    ```python def preprocess_data_shuffle(X  y):     """"""     Preprocess image data  and convert labels into one-hot     and shuffle the data     Arguments:         * X: Image data         * y: Labels      Returns:         * Preprocessed X_shuffled  one-hot version of y_shuffled     """"""     #: Convert from RGB to grayscale if applicable     if GRAYSCALE:         X = rgb_to_gray(X)      #: Make all image array values fall within the range -1 to 1     #: Note all values in original images are between 0 and 255  as uint8     X = X.astype('float32')     X = (X - 128.) / 128.      #: Convert the labels from numerical labels to one-hot encoded labels     y_onehot = np.zeros((y.shape[0]  NUM_CLASSES))     for i  onehot_label in enumerate(y_onehot):         onehot_label[y[i]] = 1.     y = y_onehot     #:Shuffle the data     X_shuffled  y_shuffled = shuffle(X  y)     return X_shuffled  y_shuffled ```   ```python def preprocess_data(X  y):     """"""     Preprocess image data  and convert labels into one-hot      Arguments:         * X: Image data         * y: Labels      Returns:         * Preprocessed X  one-hot version of y     """"""     #: Convert from RGB to grayscale if applicable     if GRAYSCALE:         X = rgb_to_gray(X)      #: Make all image array values fall within the range -1 to 1     #: Note all values in original images are between 0 and 255  as uint8     X = X.astype('float32')     X = (X - 128.) / 128.      #: Convert the labels from numerical labels to one-hot encoded labels     y_onehot = np.zeros((y.shape[0]  NUM_CLASSES))     for i  onehot_label in enumerate(y_onehot):         onehot_label[y[i]] = 1.     y = y_onehot          return X  y ```   ```python def next_batch(X  y  batch_size  augment_data):     """"""     Generator to generate data and labels     Each batch yielded is unique  until all data is exhausted     If all data is exhausted  the next call to this generator will throw a StopIteration      Arguments:         * X: image data  a tensor of shape (dataset_size  32  32  3)         * y: labels  a tensor of shape (dataset_size )  <-- i.e. a list         * batch_size: Size of the batch to yield         * augment_data: Boolean value  whether to augment the data (i.e. perform image transform)      Yields:         A tuple of (images  labels)  where:             * images is a tensor of shape (batch_size  32  32  3)             * labels is a tensor of shape (batch_size )     """"""          #: We know X and y are randomized from the train/validation split already      #: so just sequentially yield the batches     start_idx = 0     while start_idx < X.shape[0]:         images = X[start_idx : start_idx + batch_size]         labels = y[start_idx : start_idx + batch_size]          yield (np.array(images)  np.array(labels))          start_idx += batch_size ```   ```python def calculate_accuracy(data_gen  data_size  batch_size  accuracy  x  y  keep_prob  sess):     """"""     Helper function to calculate accuracy on a particular dataset      Arguments:         * data_gen: Generator to generate batches of data         * data_size: Total size of the data set  must be consistent with generator         * batch_size: Batch size  must be consistent with generator         * accuracy  x  y  keep_prob: Tensor objects in the neural network         * sess: TensorFlow session object containing the neural network graph      Returns:         * Float representing accuracy on the data set     """"""     num_batches = math.ceil(data_size / batch_size)     last_batch_size = data_size % batch_size      accs = []  #: accuracy for each batch      for _ in range(num_batches):         images  labels = next(data_gen)          #: Perform forward pass and calculate accuracy         #: Note we set keep_prob to 1.0  since we are performing inference         acc = sess.run(accuracy  feed_dict={x: images  y: labels  keep_prob: 1.})         accs.append(acc)      #: Calculate average accuracy of all full batches (the last batch is the only partial batch)     acc_full = np.mean(accs[:-1])      #: Calculate weighted average of accuracy accross batches     acc = (acc_full * (data_size - last_batch_size) + accs[-1] * last_batch_size) / data_size      return acc ```   ```python #: Count frequency of each label labels  counts = np.unique(y_train  return_counts=True)  #: Plot the histogram plt.rcParams[""figure.figsize""] = [15  5] axes = plt.gca() axes.set_xlim([-1 43])  plt.bar(labels  counts  tick_label=labels  width=0.8  align='center') plt.title('Class Distribution across Training Data') plt.show() ```   ![png](output_19_0.png)   The training data's class distribution is highly skewed.   ```python #: Count frequency of each label labels  counts = np.unique(y_valid  return_counts=True)  #: Plot the histogram plt.rcParams[""figure.figsize""] = [15  5] axes = plt.gca() axes.set_xlim([-1 43])  plt.bar(labels  counts  tick_label=labels  width=0.8  align='center') plt.title('Class Distribution across Validation Data') plt.show() ```   ![png](output_21_0.png)   The validation  data is also skewed  in the same way the training data is skewed. The only difference is the individual counts for each class are less.   ```python #: Count frequency of each label labels  counts = np.unique(y_test  return_counts=True)  #: Plot the histogram plt.rcParams[""figure.figsize""] = [15  5] axes = plt.gca() axes.set_xlim([-1 43])  plt.bar(labels  counts  tick_label=labels  width=0.8  align='center') plt.title('Class Distribution across Testing Data') plt.show() ```   ![png](output_23_0.png)   The test data is also skewed  in the same way the training data is skewed. The only difference is the individual counts for each class are less.   with the following differences:   Go straight or left: 11.88%   I obtained the above 5 images from a dataset of German traffic signs [German Traffic Signs](https://www.adac.de/_mmm/pdf/fi_verkehrszeichen_engl_infobr_0915_30482.pdf)   ```python #: Read sample image files  resize them  convert to numpy arrays w/ dtype=uint8 image_files  = ['sample_images_german/' + image_file for image_file in os.listdir('sample_images_german')] images = [] for image_file in image_files:     image = Image.open(image_file)     image = image.convert('RGB')     image = image.resize((IMG_SIZE  IMG_SIZE)  Image.ANTIALIAS)     image = np.array(list(image.getdata())  dtype='uint8')     image = np.reshape(image  (32  32  3))      images.append(image) images = np.array(images  dtype='uint8')  #: Visually inspect sample images for i  image in enumerate(images):     plt.subplot(1  5  i+1)     plt.imshow(image)     plt.title(image_files[i])  plt.tight_layout() plt.show() ```   ![png](output_81_0.png)    ```python #: Load signnames.csv to map label number to sign string label_map = {} with open('signnames.csv'  'r') as f:     first_line = True     for line in f:         #: Ignore first line         if first_line:             first_line = False             continue          #: Populate label_map         label_int  label_string = line.split(' ')         label_int = int(label_int)          label_map[label_int] = label_string           """;General;https://github.com/ibabbar/Traffic-Sign-Classifier
"""See more details  including system dependencies  python requirements and setups in [install.md](./docs/install.md). Please follows the instructions in [install.md](./docs/install.md) to install this firstly.  **Notice that `imags_size=512` need at least 9.8GB GPU memory.** if you are using a middle-level GPU(e.g. RTX 2060)  you should change the `image_size` to 384 or 256. The following table can be used as a reference:  | image_size | preprocess | personalize | run_imitator | recommended gpu                    | | ---------- | ---------- | ----------- | ------------ | ---------------------------------- | | 256x256    | 3.1 GB     | 4.3 GB      | 1.1 GB       | RTX 2060 / RTX 2070                | | 384x384    | 3.1 GB     | 7.9 GB      | 1.5 GB       | GTX 1080Ti / RTX 2080Ti / Titan Xp | | 512x512    | 3.1 GB     | 9.8 GB      | 2 GB         | GTX 1080Ti / RTX 2080Ti / Titan Xp | | 1024x1024  | 3.1 GB     | 20 GB       | -            | RTX Titan / P40 / V100 32G         |    [x] 12/20/2020  A precompiled version on Windows has been released! [Usage]   """;Computer Vision;https://github.com/iPERDance/iPERCore
"""sbatch ./scripts/swav_800ep_pretrain.sh   Install detr and prepare COCO dataset following these instructions.   The queue is composed of feature representations from the previous batches. [These lines](./main_swav.py#L305-L306) discard the oldest feature representations from the queue and save the newest one (i.e. from the current batch) through a round-robin mechanism. This way  the assignment problem is performed on more samples: without the queue we assign `B` examples to `num_prototypes` clusters where `B` is the total batch size while with the queue we assign `(B + queue_length)` examples to `num_prototypes` clusters. This is especially useful when working with small batches because it improves the precision of the assignment.  If you start using the queue too early or if you use a too large queue  this can considerably disturb training: this is because the queue members are too inconsistent. After introducing the queue the loss should be lower than what it was without the queue. On the following loss curve (30 first epochs of this [script](./scripts/swav_200ep_bs256_pretrain.sh)) we introduced the queue at epoch 15. We observe that it made the loss go more down. <div align=""left"">   <img width=""35%"" alt=""SwAV training loss batch_size=256 during the first 30 epochs"" src=""https://dl.fbaipublicfiles.com/deepcluster/swav_loss_bs256_30ep.png""> </div>  If when introducing the queue  the loss goes up and does not decrease afterwards you should stop your training and change the queue parameters. We recommend (i) using a smaller queue  (ii) starting the queue later in training.   """;General;https://github.com/facebookresearch/swav
"""│       ├── run_resnet101_baseline.sh   """;General;https://github.com/tarujg/domain-adapt
"""Among all the skin cancer type  melanoma is the least common skin cancer  but it is responsible for **75%** of death [SIIM-ISIC Melanoma Classification  2020](https://www.kaggle.com/c/siim-isic-melanoma-classification). Being a less common skin cancer type but is spread very quickly to other body parts if not diagnosed early. The **International Skin Imaging Collaboration (ISIC)** is facilitating skin images to reduce melanoma mortality. Melanoma can be cured if diagnosed and treated in the early stages. Digital skin lesion images can be used to make a teledermatology automated diagnosis system that can support clinical decision.  Currently  deep learning has revolutionised the future as it can solve complex problems. The motivation is to develop a solution that can help dermatologists better support their diagnostic accuracy by ensembling contextual images and patient-level information  reducing the variance of predictions from the model.   """;Computer Vision;https://github.com/Tirth27/Skin-Cancer-Classification-using-Deep-Learning
""" We implement MPNet and this pre-training toolkit based on the codebase of [fairseq](https://github.com/pytorch/fairseq). The installation is as follow:  ``` pip install --editable pretraining/ pip install pytorch_transformers==1.0.0 transformers scipy sklearn ```    """;Natural Language Processing;https://github.com/michael-wzhu/mpnet_zh
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M and VGG2 datasets  which were already packed in the MxNet binary format. The network backbones include ResNet  InceptionResNet_v2  DenseNet  DPN and MobiletNet. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss. * loss-type=0:  Softmax * loss-type=1:  SphereFace * loss-type=2:  CosineFace * loss-type=4:  ArcFace * loss-type=5:  Combined Margin * loss-type=12: TripletLoss  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu80   git clone --recursive https://github.com/deepinsight/insightface.git   """;General;https://github.com/zrui94/insight_mx
"""with the old scripts you can still run three different SAC versions    The new script combines all extensions and the add-ons can be simply added by setting the corresponding flags.  `python run.py -info sac`  **Parameter:** To see the options: `python run.py -h`  <pre> -env  Environment name  default = Pendulum-v0 -per  Adding Priorizied Experience Replay to the agent if set to 1  default = 0 -munchausen  Adding Munchausen RL to the agent if set to 1  default = 0 -dist  --distributional  Using a distributional IQN Critic network if set to 1  default = 0 -d2rl  Uses Deep Actor and Deep Critic Networks if set to 1  default = 0 -n_step  Using n-step bootstrapping  default = 1 -ere  Adding Emphasizing Recent Experience to the agent if set to 1  default = 0 -info  Information or name of the run -frames  The amount of training interactions with the environment  default is 100000 -eval_every  Number of interactions after which the evaluation runs are performed  default = 5000 -eval_runs  Number of evaluation runs performed  default = 1 -seed  Seed for the env and torch network weights  default is 0 -lr_a  Actor learning rate of adapting the network weights  default is 3e-4 -lr_c  Critic learning rate of adapting the network weights  default is 3e-4 -a  --alpha  entropy alpha value  if not choosen the value is leaned by the agent -layer_size  Number of nodes per neural network layer  default is 256 -repm  --replay_memory  Size of the Replay memory  default is 1e6 -bs  --batch_size  Batch size  default is 256 -t  --tau  Softupdate factor tau  default is 0.005 -g  --gamma  discount factor gamma  default is 0.99 --saved_model  Load a saved model to perform a test run! -w  --worker  Number of parallel worker (attention  batch-size increases proportional to worker number!)  default = 1 </pre>   """;Reinforcement Learning;https://github.com/BY571/Soft-Actor-Critic-and-Extensions
"""We implement v3 version (which is the latest version on June  2019.).   All the experiments are done on NVIDIA TESLA T4.   Note: You know the GPU/TPU won't get exactly the same results even we use fixed random seed.   The implementation is based on BERT [repository](https://github.com/google-research/bert)  which uses `AdamWeightDecayOptimizer` (appears in [`optimization.py`](https://github.com/google-research/bert/blob/master/optimization.py)) for pre-training and fine-tuning.  - Just use `LAMBOptimizer` as a regular optimizer in TensorFlow  similar to `Adam` or `AdamWeightDecayOptimizer`. - Find LAMB optimizer in `optimization.py`. - There is nothing special to tune other than initial `learning_rate`.    """;Natural Language Processing;https://github.com/liuqiangict/lamb_optimizer
"""jupyter notebook mura.ipynb.   4. pytorch dataloading :      https://pytorch.org/tutorials/beginner/data_loading_tutorial.html   7. DenseNet-169: https://pytorch.org/docs/master_modules/torchvision/models/densenet.html   """;General;https://github.com/sparvangada/capstone_project
"""source code: https://github.com/AlexeyAB/darknet   useful links: https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&sk=6039748846bbcf1d960c3061542591d7   """;Computer Vision;https://github.com/wonseok-Jang/ros_darknet
"""This is the PyTorch implementation. The MegEngine version is at https://github.com/megvii-model/DiverseBranchBlock   You may download the models reported in the paper (ResNet-18  ResNet-50  MobileNet) from Google Drive (https://drive.google.com/drive/folders/1BPuqY_ktKz8LvHjFK5abD0qy3ESp8v6H?usp=sharing) or Baidu Cloud (https://pan.baidu.com/s/1wPaQnLKyNjF_bEMNRo4z6Q  the access code is ""dbbk""). For the ease of transfer learning on other tasks  we provide both training-time and inference-time models. For ResNet-18 as an example  assume IMGNET_PATH is the path to your directory that contains the ""train"" and ""val"" directories of ImageNet  you may test the accuracy by running ``` python test.py IMGNET_PATH train ResNet-18_DBB_7101.pth -a ResNet-18 -t DBB ``` Here ""train"" indicates the training-time structure    Assume your model is like ``` class SomeModel(nn.Module):     def __init__(self  ...):         ...         self.some_conv = nn.Conv2d(...)         self.some_bn = nn.BatchNorm2d(...)         ...              def forward(self  inputs):         out = ...         out = self.some_bn(self.some_conv(out))         ... ``` For training  just use DiverseBranchBlock to replace the conv-BN. Then SomeModel will be like ``` class SomeModel(nn.Module):     def __init__(self  ...):         ...         self.some_dbb = DiverseBranchBlock(...  deploy=False)         ...              def forward(self  inputs):         out = ...         out = self.some_dbb(out)         ... ``` Train the model just like you train the other regular models. Then call **switch_to_deploy** of every DiverseBranchBlock  test  and save. ``` model = SomeModel(...) train(model) for m in train_model.modules():     if hasattr(m  'switch_to_deploy'):         m.switch_to_deploy() test(model) save(model) ```   """;Computer Vision;https://github.com/DingXiaoH/DiverseBranchBlock
"""The training and evaluation scripts operate on datasets stored as multi-resolution TFRecords. Each dataset is represented by a directory containing the same image data in several resolutions to enable efficient streaming. There is a separate *.tfrecords file for each resolution  and if the dataset contains labels  they are stored in a separate file as well. By default  the scripts expect to find the datasets at `datasets/<NAME>/<NAME>-<RESOLUTION>.tfrecords`. The directory can be changed by editing [config.py](./config.py):  ``` result_dir = 'results' data_dir = 'datasets' cache_dir = 'cache' ```  To obtain the FFHQ dataset (`datasets/ffhq`)  please refer to the [Flickr-Faces-HQ repository](https://github.com/NVlabs/ffhq-dataset).  To obtain the CelebA-HQ dataset (`datasets/celebahq`)  please refer to the [Progressive GAN repository](https://github.com/tkarras/progressive_growing_of_gans).  To obtain other datasets  including LSUN  please consult their corresponding project pages. The datasets can be converted to multi-resolution TFRecords using the provided [dataset_tool.py](./dataset_tool.py):  ``` > python dataset_tool.py create_lsun datasets/lsun-bedroom-full ~/lsun/bedroom_lmdb --resolution 256 > python dataset_tool.py create_lsun_wide datasets/lsun-car-512x384 ~/lsun/car_lmdb --width 512 --height 384 > python dataset_tool.py create_lsun datasets/lsun-cat-full ~/lsun/cat_lmdb --resolution 256 > python dataset_tool.py create_cifar10 datasets/cifar10 ~/cifar10 > python dataset_tool.py create_from_images datasets/custom-dataset ~/custom-images ```   Code: https://github.com/NVlabs/stylegan  FFHQ: https://github.com/NVlabs/ffhq-dataset   """;Computer Vision;https://github.com/isaacschaal/SG_training
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/stigma0617/maskrcnn-benchmark-vovnet
"""```bash $ pip install feedback-transformer-pytorch ```   ```python import torch from feedback_transformer_pytorch import FeedbackTransformer  model = FeedbackTransformer(     num_tokens = 20000            #: number of tokens     dim = 512                     #: dimension     depth = 6                     #: depth     seq_len = 2                   #: the sequence length of each segment or window     mem_len = 256                 #: length of the memory buffer     dim_head = 64                 #: dimension of each head     heads = 8                     #: number of heads     attn_dropout = 0.1            #: attention dropout     ff_dropout = 0.1              #: feedforward dropout ).cuda()  x = torch.randint(0  20000  (2  64)).cuda() model(x)  #: (2  64  20000) ```  If you would like to have fine control over the memory (when to detach  etc)  you can do it with some extra keyword arguments on `.forward`  ```python import torch from feedback_transformer_pytorch import FeedbackTransformer  model = FeedbackTransformer(     num_tokens = 20000      dim = 512      depth = 6      seq_len = 32      mem_len = 256 ).cuda()  x1 = torch.randint(0  20000  (2  32)).cuda() x2 = torch.randint(0  20000  (2  32)).cuda() x3 = torch.randint(0  20000  (2  32)).cuda()  out1  mem1 = model(x1  return_memory = True) out2  mem2 = model(x2  memory = mem1  return_memory = True) out3  mem3 = model(x3  memory = mem2  return_memory = True)  #: (2  32  20000) ```   """;Natural Language Processing;https://github.com/lucidrains/feedback-transformer-pytorch
"""Mesh TensorFlow (`mtf`) is a language for distributed deep learning  capable of specifying a broad class of distributed tensor computations.  The purpose of Mesh TensorFlow is to formalize and implement distribution strategies for your computation graph over your hardware/processors. For example: ""Split the batch over rows of processors and split the units in the hidden layer across columns of processors."" Mesh TensorFlow is implemented as a layer over TensorFlow.  Watch our [YouTube video](https://www.youtube.com/watch?v=HgGyWS40g-g).    To install the latest stable version  run  ```sh pip install mesh-tensorflow ```  To install the latest development version  run  ```sh pip install -e ""git+https://github.com/tensorflow/mesh.git#:egg=mesh-tensorflow"" ```  Installing `mesh-tensorflow` does not automatically install or update TensorFlow. We recommend installing it via `pip install tensorflow` or `pip install tensorflow-gpu`. See TensorFlow’s [installation instructions for details](https://www.tensorflow.org/install/). If you're using a development version of Mesh TensorFlow  you may need to use TensorFlow's nightly package (`tf-nightly`).   different from the CPU/GPU implementation.   git clone https://github.com/tensorflow/mesh.git   To illustrate  let us consider a simple model for the MNIST image-classification task.  Our network has one hidden layer with 1024 units  and an output layer with 10 units (corresponding to the 10 digit classes).  The code consists of two parts  the first describing the mathematical operations  and the second describing the devices and tensor/computation layout. For the full example  see [`examples/mnist.py`]( https://github.com/tensorflow/mesh/blob/master/examples/mnist.py). TODO(noam): verify that this code works.  ```Python #: tf_images is a tf.Tensor with shape [100  28  28] and dtype tf.float32 #: tf_labels is a tf.Tensor with shape [100] and dtype tf.int32 graph = mtf.Graph() mesh = mtf.Mesh(graph  ""my_mesh"") batch_dim = mtf.Dimension(""batch""  100) rows_dim = mtf.Dimension(""rows""  28) cols_dim = mtf.Dimension(""cols""  28) hidden_dim = mtf.Dimension(""hidden""  1024) classes_dim = mtf.Dimension(""classes""  10) images = mtf.import_tf_tensor(     mesh  tf_images  shape=[batch_dim  rows_dim  cols_dim]) labels = mtf.import_tf_tensor(mesh  tf_labels  [batch_dim]) w1 = mtf.get_variable(mesh  ""w1""  [rows_dim  cols_dim  hidden_dim]) w2 = mtf.get_variable(mesh  ""w2""  [hidden_dim  classes_dim]) #: einsum is a generalization of matrix multiplication (see numpy.einsum) hidden = mtf.relu(mtf.einsum(images  w1  output_shape=[batch_dim  hidden_dim])) logits = mtf.einsum(hidden  w2  output_shape=[batch_dim  classes_dim]) loss = mtf.reduce_mean(mtf.layers.softmax_cross_entropy_with_logits(     logits  mtf.one_hot(labels  classes_dim)  classes_dim)) w1_grad  w2_grad = mtf.gradients([loss]  [w1  w2]) update_w1_op = mtf.assign(w1  w1 - w1_grad * 0.001) update_w2_op = mtf.assign(w2  w2 - w2_grad * 0.001) ```  In the code above  we have built a Mesh TensorFlow graph  which is simply a Python structure.  We have completely defined the mathematical operations. In the code below  we specify the mesh of processors and the layout of the computation.  ```Python devices = [""gpu:0""  ""gpu:1""  ""gpu:2""  ""gpu:3""] mesh_shape = [(""all_processors""  4)] layout_rules = [(""batch""  ""all_processors"")] mesh_impl = mtf.placement_mesh_impl.PlacementMeshImpl(     mesh_shape  layout_rules  devices) lowering = mtf.Lowering(graph  {mesh:mesh_impl}) tf_update_ops = [lowering.lowered_operation(update_w1_op)                   lowering.lowered_operation(update_w2_op)] ```  The particular layout above implements data-parallelism  splitting the batch of examples evenly across all four processors.  Any Tensor with a ""batch"" dimension (e.g. `images`  `h`  `logits`  and their gradients) is split in that dimension across all processors  while any tensor without a ""batch"" dimension (e.g. the model parameters) is replicated identically on every processor.  Alternatively  for model-parallelism  we can set `layout_rules=[(""hidden""  ""all_processors"")]`.  In this case  any tensor with a ""hidden"" dimension (e.g. `hidden`  `w1`  `w2`)  is split  while any other tensor (e.g. `image`  `logits`) is fully replicated.  We can even combine data-parallelism and model-parallelism on a 2-dimensional mesh of processors.  We split the batch along one dimension of the mesh  and the units in the hidden layer along the other dimension of the mesh  as below.  In this case  the hidden layer is actually tiled between the four processors  being split in both the ""batch"" and ""hidden_units"" dimensions.  ```Python mesh_shape = [(""processor_rows""  2)  (""processor_cols""  2)] layout_rules = [(""batch""  ""processor_rows"")  (""hidden""  ""processor_cols"")] ```   Take our example `Tensor` `image_batch` with shape:  `[(""batch""  100)  (""rows""  28"")  (""cols""  28)  (""channels""  3)]`  Assume that this `Tensor` is assigned to a mesh of 8 processors with shape: `[(""processor_rows""  2)  (""processor_cols""  4)]`  * If we use an empty set of layout rules `[]`  we get no splitting.  Each   processor contains the whole `Tensor`.  * If we use the layout rules `""batch:processor_cols""`  then the `""batch""`   dimension of the `Tensor` is split across the `""processor_cols""` dimension of   the batch.  This means that each processor contains a Tensor slice with shape   `[25  28  28  3]`.  For example  processors (0  3) and (1  3) contain   identical slices - `image_batch[75:100  :  :  :]`.  * If we use the layout rules `""rows:processor_rows;cols:processor_cols""`     then the image is split in two dimensions  with each processor containing one   spatial tile with shape `[100  14  7  3]`.   For example  processor (0  1)   contains the slice `image_batch[:  0:14  7:14  :]`.  Some layout rules would lead to illegal layouts:  * `""batch:processor_rows;rows:processor_rows""` is illegal because two tensor   dimensions could not be split across the same mesh dimension.  * `""channels:processor_rows""` is illegal because the size of the tensor   dimension is not evenly divisible by the size of the mesh dimension.   TODO(trandustin ylc): update given mtf pypi package  ```sh ctpu up -name=ylc-mtf-donut -tf-version=nightly -tpu-size=v2-8 -zone=us-central1-b ```   """;Natural Language Processing;https://github.com/tensorflow/mesh
"""  - Please refer [Code for Data Generation](https://github.com/twtygqyy/pytorch-SRResNet/tree/master/data) for creating training files.   - Data augmentations including flipping  rotation  downsizing are adopted.      --cuda                Use cuda?     --cuda             use cuda?   ``` usage: demo.py [-h] [--cuda] [--model MODEL] [--image IMAGE]                [--dataset DATASET] [--scale SCALE] [--gpus GPUS]  optional arguments:   -h  --help         show this help message and exit   --cuda             use cuda?   --model MODEL      model path   --image IMAGE      image name   --dataset DATASET  dataset name   --scale SCALE      scale factor  Default: 4   --gpus GPUS        gpu ids (default: 0) ``` We convert Set5 test set images to mat format using Matlab  for simple image reading An example of usage is shown as follows: ``` python demo.py --model model/model_srresnet.pth --dataset Set5 --image butterfly_GT --scale 4 --cuda ```   """;General;https://github.com/twtygqyy/pytorch-SRResNet
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Exporting is done with --mode export.  You should specify the export directory to use with --model_dir:   ```sh #: clone this repo git clone https://github.com/BrookInternSOMA/pix2pix-tensorflow.git cd pix2pix-tensorflow #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir barcode_train \   --max_epochs 1000 \   --input_dir barcode/train \   --which_direction BtoA  #: load checkpoint   --checkpoint ./barcode_train  #: test the model python pix2pix.py \   --mode test \   --output_dir barcode_test \   --input_dir barcode/val \   --checkpoint barcode_train ```   """;Computer Vision;https://github.com/BrookInternSOMA/pix2pix-barcode
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/benjamintli/darknet-gun-detector
"""We used Python 3.7 in our experiments.    Install latest version from the master branch on Github by: ``` git clone <GITHUB-URL> cd data2text-bioleaflets pip install -r requirements.txt ```   If you are using your own data  it must be formatted as one directory with 6 files:    To see all the possible command line options  run:    $DATA_DIR/test.source \   Generations of *BioLeaflets* test dataset with different models you can find in `results/` directory.    ![image](https://user-images.githubusercontent.com/14000852/120937066-a747cb80-c70b-11eb-8155-2b3c72a69326.png)  """;Natural Language Processing;https://github.com/bayer-science-for-a-better-life/data2text-bioleaflets
"""SCRFD is an efficient high accuracy face detection approach which initially described in [Arxiv](https://arxiv.org/abs/2105.04714).  <img src=""https://github.com/nttstar/insightface-resources/blob/master/images/scrfd_evelope.jpg"" width=""400"" alt=""prcurve""/>   Please refer to [mmdetection](https://github.com/open-mmlab/mmdetection/blob/master/docs/get_started.md) for installation.     1. Install [mmcv](https://github.com/open-mmlab/mmcv). (mmcv-full==1.2.6 and 1.3.3 was tested)   2. Install build requirements and then install mmdet.        ```        pip install -r requirements/build.txt        pip install -v -e .  #: or ""python setup.py develop""        ```   """;Computer Vision;https://github.com/SajjadAemmi/SCR-Face-Detection
"""  Name | Packages    Name | Packages   Name | Packages   Github Repository : https://github.com/scikit-learn/scikit-learn  Installation   : Pip  pip install -U scikit-learn  : Conda  onda install scikit-learn   Installation    : Pip  pip install statsmodels   : Conda  conda install -c conda-forge statsmodels   Installation    : Pip  pip install pygam  : Conda  conda install -c conda-forge pygam   Installation    pip install -U skater   pip3 install --upgrade tensorflow   sudo pip install keras  pip install -U skater   conda install gxx_linux-64  pip3 install --upgrade tensorflow   sudo pip install keras  sudo pip install -U --no-deps --force-reinstall --install-option=""--rl=True"" skater==1.1.1b1  : Conda  conda install -c conda-forge Skater   Installation    : Pip  pip install pdpbox   Installation    : Pip  pip install lime   Installation   : Pip  pip install pycebox   Installation   : Pip  pip install git+git://github.com/christophM/rulefit.git   Github Repository : https://github.com/scikit-learn-contrib/skope-rules  Installation    : Pip  pip install skope-rules   Installation   : Pip  pip install alibi   Github Repository : https://github.com/kohpangwei/influence-release   Installation   : Pip  pip install shap  : Conda  conda install -c conda-forge shap   """;General;https://github.com/TooTouch/WhiteBox-Part2
"""**Swin Transformer** (the name `Swin` stands for **S**hifted **win**dow) is initially described in [arxiv](https://arxiv.org/abs/2103.14030)  which capably serves as a general-purpose backbone for computer vision. It is basically a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection.  Swin Transformer achieves strong performance on COCO object detection (`58.7 box AP` and `51.1 mask AP` on test-dev) and ADE20K semantic segmentation (`53.5 mIoU` on val)  surpassing previous models by a large margin.  ![teaser](figures/teaser.png)   a CLA and decorate the PR appropriately (e.g.  status check  comment). Simply follow the instructions   - For **Image Classification**  please see [get_started.md](get_started.md) for detailed instructions. - For **Object Detection and Instance Segmentation**  please see [Swin Transformer for Object Detection](https://github.com/SwinTransformer/Swin-Transformer-Object-Detection). - For **Semantic Segmentation**  please see [Swin Transformer for Semantic Segmentation](https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation). - For **Self-Supervised Learning**  please see [Transformer-SSL](https://github.com/SwinTransformer/Transformer-SSL). - For **Video Recognition**  please see [Video Swin Transformer](https://github.com/SwinTransformer/Video-Swin-Transformer).   ***In this pargraph  we cross link third-party repositories which use Swin and report results. You can let us know by raising an issue***   (`Note please report accuracy numbers and provide trained models in your new repository to facilitate others to get sense of correctness and model behavior`)  [08/29/2021] Swin Transformer for Image Restoration: [SwinIR](https://github.com/JingyunLiang/SwinIR)  [08/12/2021] Swin Transformer for person reID: [https://github.com/layumi/Person_reID_baseline_pytorch](https://github.com/layumi/Person_reID_baseline_pytorch)  [06/29/2021] Swin-Transformer in PaddleClas and inference based on whl package: [https://github.com/PaddlePaddle/PaddleClas](https://github.com/PaddlePaddle/PaddleClas)  [04/14/2021] Swin for RetinaNet in Detectron: https://github.com/xiaohu2015/SwinT_detectron2.  [04/16/2021] Included in a famous model zoo: https://github.com/rwightman/pytorch-image-models.  [04/20/2021] Swin-Transformer classifier inference using TorchServe: https://github.com/kamalkraj/Swin-Transformer-Serve   """;Computer Vision;https://github.com/DominickZhang/Distillation-Swin-Transformer
"""The training and evaluation scripts operate on datasets stored as multi-resolution TFRecords. Each dataset is represented by a directory containing the same image data in several resolutions to enable efficient streaming. There is a separate *.tfrecords file for each resolution  and if the dataset contains labels  they are stored in a separate file as well. By default  the scripts expect to find the datasets at `datasets/<NAME>/<NAME>-<RESOLUTION>.tfrecords`. The directory can be changed by editing [config.py](./config.py):  ``` result_dir = 'results' data_dir = 'datasets' cache_dir = 'cache' ```  To obtain the FFHQ dataset (`datasets/ffhq`)  please refer to the [Flickr-Faces-HQ repository](https://github.com/NVlabs/ffhq-dataset).  To obtain the CelebA-HQ dataset (`datasets/celebahq`)  please refer to the [Progressive GAN repository](https://github.com/tkarras/progressive_growing_of_gans).  To obtain other datasets  including LSUN  please consult their corresponding project pages. The datasets can be converted to multi-resolution TFRecords using the provided [dataset_tool.py](./dataset_tool.py):  ``` > python dataset_tool.py create_lsun datasets/lsun-bedroom-full ~/lsun/bedroom_lmdb --resolution 256 > python dataset_tool.py create_lsun_wide datasets/lsun-car-512x384 ~/lsun/car_lmdb --width 512 --height 384 > python dataset_tool.py create_lsun datasets/lsun-cat-full ~/lsun/cat_lmdb --resolution 256 > python dataset_tool.py create_cifar10 datasets/cifar10 ~/cifar10 > python dataset_tool.py create_from_images datasets/custom-dataset ~/custom-images ```   Code: https://github.com/NVlabs/stylegan  FFHQ: https://github.com/NVlabs/ffhq-dataset   """;Computer Vision;https://github.com/ashutosh1919/FaceGenerationStyleGAN
"""- parameters.py contains the parameter environment setting - Transformer package comtains the main architecture of Transformer Encoder-Decoder Model - to be continued       """;General;https://github.com/shuaishuaij/Machine-Translation
"""Install with pip: ```bash pip install pytorch_pretrained_vit ```  Or from source: ```bash git clone https://github.com/lukemelas/ViT-PyTorch cd ViT-Pytorch pip install -e . ```   Install with pip install pytorch_pretrained_vit and load a pretrained ViT with:   At the moment  you can easily:   |    Name         | Pretrained on |Finetuned on|Available? |   Below is a simple  complete example. It may also be found as a Jupyter notebook in `examples/simple` or as a [Colab Notebook]().   <!-- TODO: new Colab -->  ```python import json from PIL import Image import torch from torchvision import transforms  #: Load ViT from pytorch_pretrained_vit import ViT model = ViT('B_16_imagenet1k'  pretrained=True) model.eval()  #: Load image #: NOTE: Assumes an image `img.jpg` exists in the current directory img = transforms.Compose([     transforms.Resize((384  384))       transforms.ToTensor()      transforms.Normalize(0.5  0.5)  ])(Image.open('img.jpg')).unsqueeze(0) print(img.shape) #: torch.Size([1  3  384  384])  #: Classify with torch.no_grad():     outputs = model(img) print(outputs.shape)  #: (1  1000) ```  <!--  You can easily extract features with `model.extract_features`: ```python from efficientnet_pytorch import EfficientNet model = EfficientNet.from_pretrained('efficientnet-b0')  #: ... image preprocessing as in the classification example ... print(img.shape) #: torch.Size([1  3  384  384])  features = model.extract_features(img) print(features.shape) #: torch.Size([1  1280  7  7]) ``` -->  <!--  Exporting to ONNX for deploying to production is now simple: ```python import torch from efficientnet_pytorch import EfficientNet  model = EfficientNet.from_pretrained('efficientnet-b1') dummy_input = torch.randn(10  3  240  240)  model.set_swish(memory_efficient=False) torch.onnx.export(model  dummy_input  ""test-b1.onnx""  verbose=True) ```  [Here](https://colab.research.google.com/drive/1rOAEXeXHaA8uo3aG2YcFDHItlRJMV0VP) is a Colab example. -->    """;Natural Language Processing;https://github.com/lukemelas/PyTorch-Pretrained-ViT
"""your nodes if you want to target them by name in the restored version (when you   If you want to go deeper in the math  the one piece you are missing is the explanation   ``` > python3 rnn_train.py ``` The script **rnn_train.py** trains a language model on the complete works of William Shakespeare. You can also train on Tensorflow Python code. See comments in the file.  The file **rnn_train_stateistuple.py** implements the same model using  the state_is_tuple=True option in tf.nn.rnn_cell.MultiRNNCell (default). Training is supposedly faster (by ~10%) but handling the state as a tuple is a bit more cumbersome.  ``` > tensorboard --logdir=log ``` The training script **rnn_train.py** is set up to save training and validation data as ""Tensorboard sumaries"" in the ""log"" folder. They can be visualised with Tensorboard. In the screenshot below  you can see the RNN being trained on 6 epochs of Shakespeare. The training and valisation curves stay close together which means that overfitting is not a major issue here.  You can try to add some dropout (pkeep=0.8 for example) but it will not improve the situation much becasue it is already quite good.   ![Image](https://martin-gorner.github.io/tensorflow-rnn-shakespeare/tensorboard_screenshot.png) ``` > python3 rnn_play.py ```      The script **rnn_play.py** uses a trained checkpoint to generate a new ""Shakespeare"" play.   You can also generate new ""Tensorflow Python"" code. See comments in the file.  Checkpoint files can be downloaded from here:       [Fully trained](https://drive.google.com/file/d/0B5njS_LX6IsDc2lWTmtyanRpOHc/view?usp=sharing) on Shakespeare or Tensorflow Python source.        [Partially trained](https://drive.google.com/file/d/0B5njS_LX6IsDUlFsMkdhclNSazA/view?usp=sharing) to see how they make progress in training.  ``` > python3 -m unittest tests.py ``` Unit tests can be run with the command above.    ```          TITUS ANDRONICUS   ACT I   SCENE III	An ante-chamber. The COUNT's palace.   [Enter CLEOMENES  with the Lord SAY]   Chamberlain     Let me see your worshing in my hands.   LUCETTA     I am a sign of me  and sorrow sounds it.   [Enter CAPULET and LADY MACBETH]   What manner of mine is mad  and soon arise?   JULIA     What shall by these things were a secret fool      That still shall see me with the best and force?   Second Watchman     Ay  but we see them not at home: the strong and fair of thee      The seasons are as safe as the time will be a soul      That works out of this fearful sore of feather     To tell her with a storm of something storms     That have some men of man is now the subject.     What says the story  well say we have said to thee      That shall she not  though that the way of hearts      We have seen his service that we may be sad.   [Retains his house] ADRIANA What says my lord the Duke of Burgons of Tyre?   DOMITIUS ENOBARBUS     But  sir  you shall have such a sweet air from the state      There is not so much as you see the store      As if the base should be so foul as you.   DOMITIUS ENOY     If I do now  if you were not to seek to say      That you may be a soldier's father for the field.   [Exit]  ```  """;General;https://github.com/BenjaminGonzalez/BamCode
"""Dowload the [arXiv dataset from Kaggle](https://www.kaggle.com/Cornell-University/arxiv).  Save this in arbitrary path. Then run  ```bash python3 prep_data.py --datapath /path/to/your_json_file.json ```  This script parses your .json file then converts into .csv file  after that it does NLP techniques for abstracts. After this script  you will have ./data/df_to_model.csv file. You can use it for training from scratch.    train_df['prefix'] = ""summarize""  eval_df['prefix'] = ""summarize""   GPU Utilization   - [Adam: A Method for Stochastic Optimization (Kingma et al.  2014)](https://arxiv.org/abs/1412.6980)          [['A Stochastic Optimization Theory Method for Non-stationary Strategies'            'A Probabilistic Algorithm for Stochastic Optimization using Adaptive Moments'            'Algorithm for gradient-based optimization of stochastic functions']]            - [Language Models are Few-Shot Learners (Brown et al.  2020)](https://arxiv.org/abs/1412.6980)          [['Train-off: scaling up language models for precision data sets'            'GPT-3: A Comparable Evaluation of Task and Benchmarking'            'Testing performance of fine-tuning languages']]  Also it can recommend very useful titles for you papers. For example based on our paper called ""Data Communication Protocols In Wireless Sensor Networks""  based on this abstract:          This paper describes the concept of sensor networks which has been made viable by the convergence of micro-         electro-mechanical systems technology  wireless communications and digital electronics. First  the sensing tasks and the         potential sensor networks applications are explored  and a review of factors influencing the design of sensor networks is         provided. Then  the communication architecture for sensor networks is outlined  and the algorithms and protocols         developed for each layer in the literature are explored. Open research issues for the realization of sensor networks are         also discussed.  this titles are recommended/generated:          ['Sensor networks: The novel concepts and perspectives'           'Sensor Networks: Convergence and Applications'           'The Applied Field Theory Theory of Sensor Networks for e+ e- to Sensing Networks']]           You can find other examples in ./T5 directory (in the README.md).   <img src=""./docs/gif/demo.gif"" width=""700"" height=""400"" />  Local server for demo is created by Flask API. Just run  ``` cd demo conda activate simpletransformers python3 app.py ``` and play!   """;General;https://github.com/safakkbilici/Academic-Paper-Title-Recommendation
"""Clone DOTA_Devkit as a sub-module:  ```shell REPO_ROOT$ git submodule update --init --recursive REPO_ROOT/fcos_core/DOTA_devkit$ sudo apt-get install swig REPO_ROOT/fcos_core/DOTA_devkit$ swig -c++ -python polyiou.i REPO_ROOT/fcos_core/DOTA_devkit$ python setup.py build_ext --inplace ``` Edit the `config.json` and run:  ```shell REPO_ROOT$ python prepare.py ```   This FCOS implementation is based on [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark). Therefore the installation is the same as original maskrcnn-benchmark.  Please check [INSTALL.md](INSTALL.md) for installation instructions. You may also want to see the original [README.md](MASKRCNN_README.md) of maskrcnn-benchmark.   For users who only want to use FCOS as an object detector in their projects  they can install it by pip. To do so  run: ``` pip install torch  #: install pytorch if you do not have it pip install git+https://github.com/tianzhi0549/FCOS.git #: run this command line for a demo  fcos https://github.com/tianzhi0549/FCOS/raw/master/demo/images/COCO_val2014_000000000885.jpg ``` Please check out [here](fcos/bin/fcos) for the interface usage.   For your convenience  we provide the following trained models (more models are coming soon).   Make your directory layout like this:   """;Computer Vision;https://github.com/lijain/FCOS-change
"""[x] GPU accelerated   conda env create -f conda-cpu.yml  conda activate yolov3-tf2-cpu  : Tensorflow GPU  conda env create -f conda-gpu.yml  conda activate yolov3-tf2-gpu   pip install -r requirements.txt   sudo apt install nvidia-driver-430  : Windows/Other   See the documentation here https://github.com/zzh8829/yolov3-tf2/blob/master/docs/training_voc.md   ``` bash   You can compile all the keras fitting functionalities with gradient tape using the   if for some reason you would like to have more boxes you can use the --yolo_max_boxes flag.   """;Computer Vision;https://github.com/zzh8829/yolov3-tf2
"""pip install python-cdd  pip install -r https://raw.githubusercontent.com/offscale/cdd-python/master/requirements.txt  pip install https://api.github.com/repos/offscale/cdd-python/zipball#egg=cdd   as_numpy: Optional[bool] = None       as_numpy: Optional[bool] = None    You have to run a tool to synchronise your various formats.   Slow  manual duplication; or   usage: python -m cdd [-h] [--version]     --class-name CLASS_NAMES     --name-tpl NAME_TPL   Template for the name  e.g.  `{name}Config`.   usage: python -m cdd gen_routes [-h] --crud {CRUD CR C R U D CR CU CD CRD}     --crud {CRUD CR C R U D CR CU CD CRD}     --routes-path ROUTES_PATH                              --output-directory OUTPUT_DIRECTORY [--dry-run]                           Modules/FQN to omit. If unspecified will emit all                           Modules/FQN to emit. If unspecified will emit all     --output-directory OUTPUT_DIRECTORY  -o OUTPUT_DIRECTORY   To create a `class` from [`tf.keras.optimizers.Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam):  ```python >>> from cdd.source_transformer import to_code  >>> from cdd import emit  parse  >>> import tensorflow as tf  >>> from typing import Optional  >>> print(to_code(emit.class_(parse.class_(tf.keras.optimizers.Adam                                             merge_inner_function=""__init__"")                                class_name=""AdamConfig"")))   class AdamConfig(object):     """"""     Optimizer that implements the Adam algorithm.      Adam optimization is a stochastic gradient descent method that is based on     adaptive estimation of first-order and second-order moments.      According to     [Kingma et al.  2014](http://arxiv.org/abs/1412.6980)      the method is ""*computationally     efficient  has little memory requirement  invariant to diagonal rescaling of     gradients  and is well suited for problems that are large in terms of     data/parameters*"".       Usage:      >>> opt = tf.keras.optimizers.Adam(learning_rate=0.1)     >>> var1 = tf.Variable(10.0)     >>> loss = lambda: (var1 ** 2)/2.0       #: d(loss)/d(var1) == var1     >>> step_count = opt.minimize(loss  [var1]).numpy()     >>> #: The first step is `-learning_rate*sign(grad)`     >>> var1.numpy()     9.9      Reference:       - [Kingma et al.  2014](http://arxiv.org/abs/1412.6980)       - [Reddi et al.  2018](           https://openreview.net/pdf?id=ryQu7f-RZ) for `amsgrad`.      Notes:      The default value of 1e-7 for epsilon might not be a good default in     general. For example  when training an Inception network on ImageNet a     current good choice is 1.0 or 0.1. Note that since Adam uses the     formulation just before Section 2.1 of the Kingma and Ba paper rather than     the formulation in Algorithm 1  the ""epsilon"" referred to here is ""epsilon     hat"" in the paper.      The sparse implementation of this algorithm (used when the gradient is an     IndexedSlices object  typically because of `tf.gather` or an embedding     lookup in the forward pass) does apply momentum to variable slices even if     they were not used in the forward pass (meaning they have a gradient equal     to zero). Momentum decay (beta1) is also applied to the entire momentum     accumulator. This means that the sparse behavior is equivalent to the dense     behavior (in contrast to some momentum implementations which ignore momentum     unless a variable slice was actually used).      :cvar learning_rate: A `Tensor`  floating point value  or a schedule that is a         `tf.keras.optimizers.schedules.LearningRateSchedule`  or a callable that takes no arguments and         returns the actual value to use  The learning rate.     :cvar beta_1: A float value or a constant float tensor  or a callable that takes no arguments and         returns the actual value to use. The exponential decay rate for the 1st moment estimates.     :cvar beta_2: A float value or a constant float tensor  or a callable that takes no arguments and         returns the actual value to use  The exponential decay rate for the 2nd moment estimates.     :cvar epsilon: A small constant for numerical stability. This epsilon is ""epsilon hat"" in the         Kingma and Ba paper (in the formula just before Section 2.1)  not the epsilon in Algorithm 1 of the         paper.     :cvar amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from the paper ""On the         Convergence of Adam and beyond"".     :cvar name: Optional name for the operations created when applying gradients.     :cvar kwargs: Keyword arguments. Allowed to be one of `""clipnorm""` or `""clipvalue""`. `""clipnorm""`         (float) clips gradients by norm; `""clipvalue""` (float) clips gradients by value.""""""     learning_rate: float = 0.001     beta_1: float = 0.9     beta_2: float = 0.999     epsilon: float = 1e-07     amsgrad: bool = False     name: Optional[str] = 'Adam'     kwargs: Optional[dict] = None     _HAS_AGGREGATE_GRAD: bool = True ```   """;General;https://github.com/offscale/cdd-python
"""Install the package and make predictions on Neurofinder using a pre-trained UNet2DS model.  **Note: This assumes python3.5 and pip3.**  ``` #: Install from Github repo. Need to upgrade pip. $ pip install --upgrade --user -I pip $ pip install --user git+https://github.com/alexklibisz/deep-calcium.git  #: Download the model from Github releases. $ wget https://github.com/alexklibisz/deep-calcium/releases/download/v0.0.1-weights/unet2ds_model.hdf5   #: Download the example script and evaluate predictions on the first training dataset. #: This will download and preprocess the dataset to ~/.deep-calcium-datasets  requiring ~3.1GB of disk space. $ wget https://raw.githubusercontent.com/alexklibisz/deep-calcium/dev/examples/neurons/unet2ds_nf.py $ CUDA_VISIBLE_DEVICES=""0"" python unet2ds_nf.py evaluate neurofinder.00.00 --model unet2ds_model.hdf5 ```  You should see output similar to this:  ``` $ INFO:predict:Loaded model from unet2ds_model.hdf5. $ INFO:predict:neurofinder.00.00: prec=0.976  reca=1.000  incl=0.084  excl=0.109  comb=0.988 $ INFO:predict:Saved /home/kzh/.deep-calcium/checkpoints/neurons_unet2ds_nf/neurofinder.00.00_mp.png $ INFO:predict:Mean prec=0.976  reca=1.000  comb=0.988 $ INFO:evaluation:Evaluation without TTA. $ INFO:predict:Loaded model from unet2ds_model.hdf5. $ INFO:predict:neurofinder.00.00: prec=0.919  reca=1.000  incl=0.106  excl=0.133  comb=0.958 $ INFO:predict:Saved /home/kzh/.deep-calcium/checkpoints/neurons_unet2ds_nf/neurofinder.00.00_mp.png $ INFO:predict:Mean prec=0.919  reca=1.000  comb=0.958 ```   Install the package and make predictions on Neurofinder using a pre-trained UNet2DS model.  **Note: This assumes python3.5 and pip3.**  ``` #: Install from Github repo. Need to upgrade pip. $ pip install --upgrade --user -I pip $ pip install --user git+https://github.com/alexklibisz/deep-calcium.git  #: Download the model from Github releases. $ wget https://github.com/alexklibisz/deep-calcium/releases/download/v0.0.1-weights/unet2ds_model.hdf5   #: Download the example script and evaluate predictions on the first training dataset. #: This will download and preprocess the dataset to ~/.deep-calcium-datasets  requiring ~3.1GB of disk space. $ wget https://raw.githubusercontent.com/alexklibisz/deep-calcium/dev/examples/neurons/unet2ds_nf.py $ CUDA_VISIBLE_DEVICES=""0"" python unet2ds_nf.py evaluate neurofinder.00.00 --model unet2ds_model.hdf5 ```  You should see output similar to this:  ``` $ INFO:predict:Loaded model from unet2ds_model.hdf5. $ INFO:predict:neurofinder.00.00: prec=0.976  reca=1.000  incl=0.084  excl=0.109  comb=0.988 $ INFO:predict:Saved /home/kzh/.deep-calcium/checkpoints/neurons_unet2ds_nf/neurofinder.00.00_mp.png $ INFO:predict:Mean prec=0.976  reca=1.000  comb=0.988 $ INFO:evaluation:Evaluation without TTA. $ INFO:predict:Loaded model from unet2ds_model.hdf5. $ INFO:predict:neurofinder.00.00: prec=0.919  reca=1.000  incl=0.106  excl=0.133  comb=0.958 $ INFO:predict:Saved /home/kzh/.deep-calcium/checkpoints/neurons_unet2ds_nf/neurofinder.00.00_mp.png $ INFO:predict:Mean prec=0.919  reca=1.000  comb=0.958 ```   """;Computer Vision;https://github.com/alexklibisz/deep-calcium
"""[![PyPI Version][pypi-image]][pypi-url]   Update: You can now install pytorch-cluster via Anaconda for all major OS/PyTorch/CUDA combinations 🤗  Given that you have pytorch &gt;= 1.8.0 installed  simply run  conda install pytorch-cluster -c pyg  We alternatively provide pip wheels for all major OS/PyTorch/CUDA combinations  see here.  To install the binaries for PyTorch 1.10.0  simply run  pip install torch-cluster -f https://data.pyg.org/whl/torch-1.10.0+${CUDA}.html  where ${CUDA} should be replaced by either cpu  cu102  or cu113 depending on your PyTorch installation.   | Linux   | ✅    | ✅      | ✅      |  | Windows | ✅    | ✅      | ✅      |  | macOS   | ✅    |         |         |  To install the binaries for PyTorch 1.9.0 and 1.9.1  simply run  pip install torch-cluster -f https://data.pyg.org/whl/torch-1.9.0+${CUDA}.html  where ${CUDA} should be replaced by either cpu  cu102  or cu111 depending on your PyTorch installation.   | Linux   | ✅    | ✅      | ✅      |  | Windows | ✅    | ✅      | ✅      |  | macOS   | ✅    |         |         |  Note: Binaries of older versions are also provided for PyTorch 1.4.0  PyTorch 1.5.0  PyTorch 1.6.0  PyTorch 1.7.0/1.7.1 and PyTorch 1.8.0/1.8.1 (following the same procedure).  Ensure that at least PyTorch 1.4.0 is installed and verify that cuda/bin and cuda/include are in your $PATH and $CPATH respectively  e.g.:   pip install torch-cluster   mkdir build  cd build  : Add -DWITH_CUDA=on support for the CUDA if needed   make install   """;Graphs;https://github.com/rusty1s/pytorch_cluster
"""The Mask R-CNN algorithm was trained using a set of 800 hand-classified wells. This training set included 6032 live larvae  942 moribund larvae  2178 dead larvae  164 eggs  290 aged dead larvae  173 L2 larve  and 328 artifacts. Labelling of images was performed using the [VGG Image Annotator (VIA) version 2](https://www.robots.ox.ac.uk/~vgg/software/via/).[^1]   The data set was augmented to produce 48 000 training images and 8 000 validation images. Augmentation was performed using `misc/image_augment.py`  which applies random rotations  scaling  gamma adjustments  blurring  and flipping about the y-axis to generate new images and corresponding json images. The augmentation program also creates a new annotation `.json` file for all the transformed labels for the augmented images. Note that the `.json` annotation file must match that of the VIA output annotation to be properly adjusted during the augmentation process.    For Windows 10  the recommended method is to use the included batch file to invoke the program. The PYTHON_HOME variable should be set to the computer's path to the python executable. Once the program is started  the GUI can be used to enter all the required data. See the [Graphical User Interface section](#gui) for details.   Alternatively  one may invoke the program directly with `main.py`  which takes no command line arguments.   All variables used by the program are set in either `./config.txt` or in `./stats/analysis_config.txt`; the former contains arguments for the GUI and general program options and filepaths  while the latter contains options related to data analysis  statistical options  and filepaths related to the statistical analysis output. The [`./docs/stats_config`](./docs/stats_config.md) file describes the permitted variables and their actions for the statistical analysis and plotting options  while [`./docs/general_config`](./docs/general_config.md) describes the permitted options for the general program.    """;Computer Vision;https://github.com/jsklimavicz/Plate_Analyzer
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;General;https://github.com/zsl1996/mask
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   This is a Github template repo you can use to create your own copy of the forked StyleGAN2 sample from NVLabs.   There are a few prerequisites you will need to have in place:   4. Create a github connected project by opening the Projects tab  pressing the ""LINK A GITHUB REPO"" button  and following the instructions on the next page.  5. You next need to authorize Gradient to access your github account if you have not done so already.   """;General;https://github.com/babyboy201/StyleGAN2
"""For example  to make reconstruction figures  you can run:   You can get the pre-generated dataset from: https://drive.google.com/drive/folders/11Vz0fqHS2rXDb5pprgTjpD7S2BAJhi1P   If you experience problems  try deleting all .pth files  updating dlutils* package (pip install dlutils --upgrade) and then run download_all.py again.   In addition to installing required packages:  pip install -r requirements.txt  You will need to install DareBlopy:  pip install dareblopy   If only one GPU available  it will run on single GPU  no special care is needed.   To run the demo  you will need to have a CUDA capable GPU  PyTorch >= v1.3.1 and cuda/cuDNN drivers installed. Install the required packages:      pip install -r requirements.txt    Download pre-trained models:      python training_artifacts/download_all.py  Run the demo:      python interactive_demo.py  You can specify **yaml** config to use. Configs are located here: https://github.com/podgorskiy/ALAE/tree/master/configs. By default  it uses one for FFHQ dataset. You can change the config using `-c` parameter. To run on `celeb-hq` in 256x256 resolution  run:      python interactive_demo.py -c celeba-hq256  However  for configs other then FFHQ  you need to obtain new principal direction vectors for the attributes.   """;Computer Vision;https://github.com/shockyou1988/ALAE
"""4、tfplot 0.2.0 (optional)            5、tensorflow-gpu 1.13   2、(Recommend in this repo) Or you can choose to use a better backbone  refer to gluon2TF.     !pip install tensorflow==1.13.2 #:If Tensorflow.contrib not available   2、Make tfrecord      3、Multi-gpu train  cd $PATH_ROOT/tools   cd $PATH_ROOT/output/summary   """;General;https://github.com/NovasMax/R3Det-Refined-Single-Stage-Detector-with-Feature-Refinement-for-RO
"""This project was designed for: * Python 3.6 * TensorFlow 1.12.0  Please install requirements & project: ``` $ cd /path/to/project/ $ git clone https://github.com/filippogiruzzi/semantic_segmentation.git $ cd semantic_segmentation/ $ pip3 install -r requirements.txt $ pip3 install -e . --user --upgrade ```   Installation    The project semantic_segmentation/ has the following structure:   ``` $ cd /path/to/project/semantic_segmentation/semseg/ ```   """;Reinforcement Learning;https://github.com/filippogiruzzi/semantic_segmentation
"""pip install -U transformers   | roformer_chinese_small             | chinese_roformer_L-6_H-384_A-6.zip (download code：gy97)               |   | roformer_chinese_char_small        | chinese_roformer-char_L-6_H-384_A-6.zip (download code：a44c)          |   | roformer_chinese_sim_char_small    | chinese_roformer-sim-char_L-6_H-384_A-6.zip (download code：h68q)      |   | roformer_chinese_sim_char_ft_small | chinese_roformer-sim-char-ft_L-6_H-384_A-6.zip (download code：gty5)   |   : pytorch   pt_outputs_sentence = ""pytorch: ""   : pytorch: 今天[天气||天||心情||阳光||空气]很好，我[想||要||打算||准备||喜欢]去公园玩。   bert4keras vs pytorch   bert4keras vs pytorch   """;General;https://github.com/JunnYu/RoFormer_pytorch
"""numpy    """;Computer Vision;https://github.com/ReshmikaD/Artistic-Style-Transfer-CNN
"""GPUs: K80 ($0.14/hr)  T4 ($0.11/hr)  V100 ($0.74/hr) CUDA with Nvidia Apex FP16/32     Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [Notebook](https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb) <a href=""https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a> * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) << highly recommended * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart)  ![Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/yolov3?logo=docker) * [A TensorRT Implementation of YOLOv3 and YOLOv4](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp)     """;Computer Vision;https://github.com/Shellllllly/plateDetection
"""Python 3.6        To train a model  run the following command:      python train.py --test-image=./DSC_1177.jpg --dataset=/dataset_path --lr=0.0001      To get all train params run:          python train.py -h      To test image run:      python test.py --checkpoint=./ch/ckpt_0_4000.pth --input=./DSC_1177.jpg --output=out.png        """;Computer Vision;https://github.com/creotiv/hdrnet-pytorch
""" - Pip install  ```bash $ pip install --upgrade tf_siren ```   - Pip install (test support)  ```bash $ pip install --upgrade tf_siren[tests] ```   Copy the `tf_siren` folder to your local directory and import either `SinusodialRepresentationDense` or `SIRENModel`.  ```python from tf_siren import SinusodialRepresentationDense from tf_siren import SIRENModel  #: You can use SinusodialRepresentationDense exactly like you ordinarily use Dense layers. ip = tf.keras.layers.Input(shape=[2]) x = SinusodialRepresentationDense(32                                    activation='sine'  #: default activation function                                   w0=1.0)(ip)        #: w0 represents sin(w0 * x) where x is the input.                                    model = tf.keras.Model(inputs=ip  outputs=x)  #: Or directly use the model class to build a multi layer SIREN model = SIRENModel(units=256  final_units=3  final_activation='sigmoid'                     num_layers=5  w0=1.0  w0_initial=30.0) ```   """;General;https://github.com/zion-king/Tensorflow-Sinusodial-Representation-Networks-SIREN
"""As we did not provide you with any optional command parameters  you can only change them inside our code to match your requirement.   5/14: DEBUG: Parallel conflict on Windows (Due to the speed limit  we migrate to Linux platform)   """;General;https://github.com/sergkuzn148/stg
"""Automatic image colorization has been a popular image-to-image translation problem of significant interest for several practical application areas including restoration of aged or degraded images. This project attempts to utilize CycleGANs to colorize grayscale images back to their colorful RGB form.   """;General;https://github.com/ArkaJU/Image-Colorization-CycleGAN
"""- MXNet   """;Computer Vision;https://github.com/DeepMark/deepmark
"""The objective of this project is to learn more about conditional generative models. Having worked with GANs  it seems beneficial to study more about adding additional descriptive information with the input image to produce models that are able to distinctly represent specific subjects in the generated data. It seems to be a part of how users can select specific features or labels for the model to generate. As an early step of looking at this and taking into account the limitations of resources and time  this project will be experimenting with the vanilla variational autoencoder and a conditional variational autoencoder.    Install pytorch from here: https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/  For the Jetson TX2  Pytorch cannot be installed using the method described on the above Github page. An alternate version for the GPU must be downloaded from the NVIDIA site as indicated by the link above.    For a short version of the code. Open the terminal on the computer and cd into the folder with demo.py and run in python3 with the following command. The demo trains on the MNIST data for 10 epochs and with 6000 training samples and 1000 training samples.  ``` cd vae python3 demo.py ```  or open terminal and run  ``` bash demo.sh ```  """;Computer Vision;https://github.com/jenyliu/DLA_interview
"""This is the code repo of our TMM 2019 work titled  [""COMIC: Towards A Compact Image Captioning Model with Attention""](https://arxiv.org/abs/1903.01072).  In this paper  we tackle the problem of compactness of image captioning models which is hitherto unexplored.  We showed competitive results on both MS-COCO and InstaPIC-1.1M datasets despite having an embedding vocabularly size that is 39x-99x smaller.  <img src=""TMM.png"" height=""200"">  **Some pre-trained model checkpoints are available at  [this repo](https://github.com/jiahuei/COMIC-Pretrained-Captioning-Models).**    Updated on 25 Feb 2021: [Object Relation Transformer](https://papers.nips.cc/paper/9293-image-captioning-transforming-objects-into-words.pdf)  with Radix encoding that can achieve CIDEr score of 1.291 after SCST training. [Code at this repo](https://github.com/jiahuei/sparse-image-captioning).  Updated on 12 June 2019: Self-Critical Sequence Training (SCST)  Updated on 06 June 2019: Pre-trained model repo  Released on 03 June 2019.    Run `./src/setup.sh`. This will download the required Stanford models  and run all the dataset pre-processing scripts.   1. Download the required `tar.gz` files from Oracle:     - [JDK 8](https://www.oracle.com/technetwork/es/java/javase/downloads/jdk8-downloads-2133151.html)     - [JCE 8](https://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html) 1. Follow instructions on [this repo](https://github.com/chrishantha/install-java/tree/63997dc81aaf9184ffe715d7381fa822bd39f357).   1. Editing setup.sh   python coco_prepro.py --dataset_dir /path/to/coco/dataset   """;General;https://github.com/jiahuei/COMIC-Compact-Image-Captioning-with-Attention
"""This method is based on DINO [paper](https://arxiv.org/pdf/2104.14294.pdf). The framework can be installed using the following commands: ``` git clone https://github.com/facebookresearch/dino.git cd dino;  touch __init__.py echo -e ""import sys\nfrom os.path import dirname  join\nsys.path.insert(0  join(dirname(__file__)  '.'))"" >> __init__.py; cd ../; ```  The code was made using the commit ba9edd1 of DINO repo (please rebase if breakage).   """;General;https://github.com/valeoai/LOST
"""Korean : pip install konlpy  English : pip install nltk   pip3 install -r requirements.txt  """;Natural Language Processing;https://github.com/tree-park/kor-to-eng-translation
"""* Does not work on windows due to WSL not supporting host mode on the port   git clone https://github.com/vinayak19th/MDD_Project   If you have an NVIDA CUDA supported GPU  you can run the server for GPU runtime  docker run --runtime=nvidia -d -p 8501:8501 -p 8500:8500 --name bart vinayak1998th/bart_serve:gpu   """;Sequential;https://github.com/vinayak19th/Brevis-2.0
"""The Jupyter Notebook can be accessed from <a href=""./dubai-satellite-imagery-segmentation.ipynb"">here</a>. -->   The Jupyter Notebook can be accessed from <a href=""./dubai-satellite-imagery-segmentation.ipynb"">here</a>.   """;General;https://github.com/ayushdabra/dubai-satellite-imagery-segmentation
"""This is pytorch version of MPE  forked from the pytorch MPE.   - To install  `cd` into the root directory and type `pip install -e .`  - To interactively view moving to landmark scenario (see others in ./scenarios/): `bin/interactive.py --scenario simple.py`  - Known dependencies: OpenAI gym  numpy  - To use the environments  look at the code for importing them in `make_env.py`.   """;Reinforcement Learning;https://github.com/zoeyuchao/MPE-pytorch
"""In the paper writers used 2 GAN types which are DCGAN and BiGAN classify images. DCGAN wass used for creating happy  sad  male  female faces. BiGAN  which employed from Adversarial Feature Learning  was ran on Imagenet ILSVRC 2012 dataset to classify images.   ---   """;General;https://github.com/COMP6248-Reproducability-Challenge/CapturingHumanCategoryRepresentation
"""I used the pandas library to calculate summary statistics of the traffic signs data set:  * Number of training examples = 34799 * Number of validation examples = 4410 * Number of testing examples = 12630 * Image data shape = (32  32  3) * Number of classes = 43   Here are the results of the prediction:  | Image			        |     Prediction	        					|  |:---------------------:|:---------------------------------------------:|  | Children crossing     | Children crossing  							|  | Right-of-way          | Right-of-way									| | Priority road			| Priority road									| | Turn right ahead 		| Turn right ahead				 				| | Road work 			| Road work         							|   The model was able to correctly guess 5 of the 5 traffic signs  which gives an accuracy of 100%  which is close to 98% from the test set.   My final model results were: * training set accuracy of 99%+ * validation set accuracy of 99%+ * test set accuracy of 98%+  If a well known architecture was chosen: * Inception v4 was chosen for traffic sign classifier. * This is a very suitable architecture because it has a very high accuracy for classifier (general inception v4 can be used to classify 1000 classes)  and it is quite efficient. * It can be concluded this model works very well because all 3 data sets have very high accuracies  which means the model is not under or over fitting (balanced variance and bias).  Additional visualization of the validation accuracy is analyzed to understand what works or not.  Below is the validation set recall and precision by class.  Note that class 16 has a low recall (false negative)  meaning images from class 16 were predicted as some other clases.  In precision chart  class 41 has a low value (false positive).  It is likely that many class 16 images were misclassified as class 41.  ![alt text][image9] ![alt text][image10]  Images were pulled from classes 16 and 41 and quickly one can see that some class 16 images have red circular borders are quite faded so they could be similar to class 41 images.  Below are classes 16 (left) and 41 (right) sample images.  ![alt text][image11]   First  I normalized all training images to have float32 from 0 to 1.  I found the accuracy increases faster than -1 to 1 during network training. The matricies below simply show one random image before and after normalization.  Before normalization:  [[51 36 45 ... 80 79 73]   [47 34 38 ... 64 75 79]    [45 32 38 ... 61 68 71]    ...    [43 38 34 ... 46 42 37]    [44 36 31 ... 36 33 35]    [41 36 38 ... 52 48 50]]   After normalization:  [[0.1849315  0.08219178 0.14383562 ... 0.38356164 0.37671232 0.33561644]   [0.15753424 0.06849315 0.09589041 ... 0.2739726  0.34931508 0.37671232]    [0.14383562 0.05479452 0.09589041 ... 0.25342464 0.30136988 0.3219178 ]    ...    [0.13013698 0.09589041 0.06849315 ... 0.15068494 0.12328767 0.0890411 ]    [0.1369863  0.08219178 0.04794521 ... 0.08219178 0.06164384 0.07534247]    [0.11643836 0.08219178 0.09589041 ... 0.19178082 0.16438356 0.1780822 ]]     As mentioned before  data augmentation is applied to even out image quantity difference among classes  and to include variations of same images.  * sharpen or smoothing * random rotate image * random stretch/squeeze image  * random darken partial image * random move image  Here is an example of a traffic sign image before and after augmentation.  The image is stretched horizontally and partially darkened at the bottom.  ![alt text][image4] ![alt text][image5]  When all training images are added up  the quantity shows:  ![alt text][image6]  As a last step  the training set is shuffled to remove any order.    I used the pandas library to calculate summary statistics of the traffic signs data set:  * Number of training examples = 34799 * Number of validation examples = 4410 * Number of testing examples = 12630 * Image data shape = (32  32  3) * Number of classes = 43   You're reading it! and here is a link to my [project code](https://github.com/waynecoffee9/Traffic-Sign-Classifier/blob/master/CarND-Traffic-Sign-Classifier-Project/Traffic_Sign_Classifier-inception.ipynb) If you are unable to view it under github  use https://nbviewer.jupyter.org/ and paste the link to view.   You're reading it! and here is a link to my [project code](https://github.com/waynecoffee9/Traffic-Sign-Classifier/blob/master/CarND-Traffic-Sign-Classifier-Project/Traffic_Sign_Classifier-inception.ipynb) If you are unable to view it under github  use https://nbviewer.jupyter.org/ and paste the link to view.   First  I normalized all training images to have float32 from 0 to 1.  I found the accuracy increases faster than -1 to 1 during network training. The matricies below simply show one random image before and after normalization.  Before normalization:  [[51 36 45 ... 80 79 73]   [47 34 38 ... 64 75 79]    [45 32 38 ... 61 68 71]    ...    [43 38 34 ... 46 42 37]    [44 36 31 ... 36 33 35]    [41 36 38 ... 52 48 50]]   After normalization:  [[0.1849315  0.08219178 0.14383562 ... 0.38356164 0.37671232 0.33561644]   [0.15753424 0.06849315 0.09589041 ... 0.2739726  0.34931508 0.37671232]    [0.14383562 0.05479452 0.09589041 ... 0.25342464 0.30136988 0.3219178 ]    ...    [0.13013698 0.09589041 0.06849315 ... 0.15068494 0.12328767 0.0890411 ]    [0.1369863  0.08219178 0.04794521 ... 0.08219178 0.06164384 0.07534247]    [0.11643836 0.08219178 0.09589041 ... 0.19178082 0.16438356 0.1780822 ]]     As mentioned before  data augmentation is applied to even out image quantity difference among classes  and to include variations of same images.  * sharpen or smoothing * random rotate image * random stretch/squeeze image  * random darken partial image * random move image  Here is an example of a traffic sign image before and after augmentation.  The image is stretched horizontally and partially darkened at the bottom.  ![alt text][image4] ![alt text][image5]  When all training images are added up  the quantity shows:  ![alt text][image6]  As a last step  the training set is shuffled to remove any order.    My final model results were: * training set accuracy of 99%+ * validation set accuracy of 99%+ * test set accuracy of 98%+  If a well known architecture was chosen: * Inception v4 was chosen for traffic sign classifier. * This is a very suitable architecture because it has a very high accuracy for classifier (general inception v4 can be used to classify 1000 classes)  and it is quite efficient. * It can be concluded this model works very well because all 3 data sets have very high accuracies  which means the model is not under or over fitting (balanced variance and bias).  Additional visualization of the validation accuracy is analyzed to understand what works or not.  Below is the validation set recall and precision by class.  Note that class 16 has a low recall (false negative)  meaning images from class 16 were predicted as some other clases.  In precision chart  class 41 has a low value (false positive).  It is likely that many class 16 images were misclassified as class 41.  ![alt text][image9] ![alt text][image10]  Images were pulled from classes 16 and 41 and quickly one can see that some class 16 images have red circular borders are quite faded so they could be similar to class 41 images.  Below are classes 16 (left) and 41 (right) sample images.  ![alt text][image11]   Here are some of the visualized feature maps evaluated on the first new image (children crossing).  It seems some feature maps picked up the shape of the triangle.  Some feature maps picked up the shape of the human figures inside the triangle.  Some feature maps picked up the blue sky on the left.  ![alt text][image18] ![alt text][image19] ![alt text][image20]  """;General;https://github.com/waynecoffee9/Traffic-Sign-Classifier
"""Pre-trained models can be downloaded from here.   """;Reinforcement Learning;https://github.com/georgesung/TD3
"""Using the package is relatively straightforward:  ```python from Twins.barlow import * from Twins.transform_utils import * import torch from torchvision import models import torchvision.transforms as transforms import torchvision.datasets as dsets  #:This is just any generic model model = torchvison.some_model  #:Optional: define transformations for your specific dataset. #:Generally  it is best to use the original augmentations in the #:paper  replacing the Imagenet normalization with the normalization #:for your dataset.  transform = transforms.Compose([                 transforms.RandomResizedCrop(image_size                                              interpolation=Image.BICUBIC)                  transforms.RandomHorizontalFlip(p=0.5)                  transforms.RandomApply(                     [transforms.ColorJitter(brightness=0.4  contrast=0.4                                              saturation=0.2  hue=0.1)]                      p=0.8                 )                  transforms.RandomGrayscale(p=0.2)                  GaussianBlur(p=1.0)                  Solarization(p=0.0)                  transforms.ToTensor()                  transforms.Normalize(mean  std)             ])  #:For the transform argument for the dataset  pass in  #: Twins.transform_utils.Transform(transform_1  transform_2) #:If transforms are None  the Imagenet default is used. dataset = dsets.some_dataset(**kwargs                                transform=Transform(transform  transform))  loader = torch.utils.data.DataLoader(dataset                                          batch_size=batch_size                                          shuffle=True) #:Make the BT instance  passing the model  the latent rep layer id  #: hidden units for the projection MLP  the tradeoff factor  #: and the loss scale. learner = BarlowTwins(model  'avg_pool'  [512 1024  1024  1024]                        3.9e-3  1)  optimizer = torch.optim.Adam(learner.parameters()  lr=0.001)  #:Single training epoch for batch_idx  ((x1 x2)  _) in enumerate(loader):     loss = learner(x1  x2)     optimizer.zero_grad()     loss.backward()     optimizer.step() ``` That is basically it. Hopefully this is helpful!   """;General;https://github.com/MaxLikesMath/Barlow-Twins-Pytorch
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`  2. Download VOC2007 and VOC2012 dataset. By default  we assume the data is stored in `$HOME/data/`   ```Shell   #: Download the data.   cd $HOME/data   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar   #: Extract the data.   tar -xvf VOCtrainval_11-May-2012.tar   tar -xvf VOCtrainval_06-Nov-2007.tar   tar -xvf VOCtest_06-Nov-2007.tar   ```  3. Create the LMDB file.   ```Shell   cd $CAFFE_ROOT   #: Create the trainval.txt  test.txt  and test_name_size.txt in data/VOC0712/   ./data/VOC0712/create_list.sh   #: You can modify the parameters in create_data.sh if needed.   #: It will create lmdb files for trainval and test with encoded original image:   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb   #: and make soft links at examples/VOC0712/   ./data/VOC0712/create_data.sh   ```   1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`   ```Shell   git clone https://github.com/weiliu89/caffe.git   cd caffe   git checkout ssd   ```  2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.   ```Shell   #: Modify Makefile.config according to your Caffe installation.   cp Makefile.config.example Makefile.config   make -j8   #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.   make py   make test -j8   #: (Optional)   make runtest -j8   ```   COCO<sup>[1]</sup>: SSD300*  SSD512*  07+12+COCO: SSD300*  SSD512*  07++12+COCO: SSD300*  SSD512*  COCO models:   """;Computer Vision;https://github.com/weidi1024/caffe-ssd-correct
"""<img src=""imgs/img1.png"" width=""320"">   <img src=""imgs/img2.png"" width=""320"">  It is important to note that as a result of the adversarial loss function  approximating the original colors of the images is not the only goal of the generators. They also have to fill the images with realistic/believable colors in order to fool the discriminators (although  these tasks can be equivalent). In this regard  the models perform quite well  often creating colorful and lifelike samples.   """;General;https://github.com/karoly-hars/GAN_image_colorizing
"""This work is done in python. you will require pip for installation. Create a virtual environment and in the virtual environment install the dependencies from the requirements.txt.   ``` pip install -r requirements.txt ```   """;Computer Vision;https://github.com/darrishabh/coviprox
"""Update 13/04/2021: Converted DDPG to Tensorflow 2.   Soft Actor-Critic (SAC) (TF2  PyTorch)   """;General;https://github.com/arnomoonens/yarll
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   This is a fork of the original (Google's) BERT implementation.   * Add Multi-GPU support with Horovod  This [blog](https://lambdalabs.com/blog/bert-multi-gpu-implementation-using-tensorflow-and-horovod-with-code/) explains all the changes we made to the original implementation.  __Install__ Please first [install Horovod](https://github.com/uber/horovod#install)  __Run__ See the commands in each section to run BERT with Multi-GPUs:  * [Sentence (and sentence-pair) classification tasks](#sentencepair)  * [SQuAD 1.1](#squad1.1)  * [SQuAD 2.0](#squad2.0)  * [Pre-training](#pretraining)      PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/LeoWood/bert-horovod
"""Gradient accumulation: https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255   """;Computer Vision;https://github.com/voqtuyen/GAN-Intuition
"""To quickly get up and running the training script (DDPG + locally available csv file)  do the following:      1. Run this set of commands to create and active python virtual environment and afterwards install required dependencies     ```{shell}     > python -m venv ./env     > ./env/Scripts/activate     > pip install -r ./requirements.txt     ```      2. Install PyTorch accroding to: https://pytorch.org/get-started/locally/      3. Run the following command:     ```{shell}     > python ./main.py     ```      4. View the progress of the model in Tensorboard:     ```{shell}     > tensorboard --logdir=./.cache/tensorboard     ```   """;Reinforcement Learning;https://github.com/majercakdavid/gym-virtual-quant-trading
"""      - 'name'   - save_path : Path for saving the models.   https://github.com/reedscot/icml2016 (the authors version)   """;Computer Vision;https://github.com/snow-mn/GAN-INT-CLS
"""Code runs on a single GPU and has been tested with  Python 3.7.2   """;Computer Vision;https://github.com/fmu2/realNVP
"""Please install packages listed above first  and then  ``` git clone https://github.com/r9y9/deepvoice3_pytorch && cd deepvoice3_pytorch pip install -e "".[bin]"" ```   https://github.com/hash2430/dv3_world: DeepVoice3 with WORLD vocoder support. #166   To use pre-trained models  it's highly recommended that you are on the specific git commit noted above. i.e.   git checkout ${commit_hash}   git checkout 4357976   Suppose you build a DeepVoice3-style model using LJSpeech dataset  then you can train your model by:   Logs are dumped in ./log directory by default. You can monitor logs by tensorboard:   Now that you have data prepared  then you can train a multi-speaker version of DeepVoice3 by:   If you want to reuse learned embedding from other dataset  then you can do this instead by:   Now that you have data prepared  then you can train a multi-speaker version of DeepVoice3 by:   Notebooks supposed to be executed on https://colab.research.google.com are available:  - [DeepVoice3: Multi-speaker text-to-speech demo](https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/DeepVoice3_multi_speaker_TTS_en_demo.ipynb) - [DeepVoice3: Single-speaker text-to-speech demo](https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/DeepVoice3_single_speaker_TTS_en_demo.ipynb)   """;Sequential;https://github.com/r9y9/deepvoice3_pytorch
"""We provide a jupyter notebook example to show how to use the anycost generator for image synthesis at diverse costs: `notebooks/intro.ipynb`.  We also provide a colab version of the notebook: [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mit-han-lab/anycost-gan/blob/master/notebooks/intro_colab.ipynb). Be sure to select the GPU as the accelerator in runtime options.     or you can download the pre-computed inceptions from here and put it under assets/inceptions.   <a href=""https://youtu.be/_yEziPl9AkM?t=90""><img src='assets/figures/demo.gif' width=600></a>  Here  we can use the Anycost generator for **interactive image editing**. A full generator takes **~3s** to render an image  which is too slow for editing. While with Anycost generator  we can provide a visually similar preview at **5x faster speed**. After adjustment  we hit the ""Finalize"" button to synthesize the high-quality final output. Check [here](https://youtu.be/_yEziPl9AkM?t=90) for the full demo.     - Clone this repo:  ```bash git clone https://github.com/mit-han-lab/anycost-gan.git cd anycost-gan ```  - Install PyTorch 1.7 and other dependeinces.  We recommend setting up the environment using Anaconda: `conda env create -f environment.yml`     We provide an interactive demo showing how we can use anycost GAN to enable interactive image editing. To run the demo:  ```bash python demo.py ```  If your computer contains a CUDA GPU  try running with: ```bash FORCE_NATIVE=1 python demo.py ```  You can find a video recording of the demo [here](https://youtu.be/_yEziPl9AkM?t=90).     """;Computer Vision;https://github.com/mit-han-lab/anycost-gan
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/zzdang/match_fashion
"""Requirement **Java 1.8+** The repository already contains **WordNet 2.1**  **VerbNet 3.0** and **StanfordCoreNLP API 4.0**. Language model must be downloaded. * Copy `NLPPipeline/src/` on your project * Go to `src/lib/` and follow `""IMPORTANT -model download.txt""` to download the language model  Requirements: - Python 3.5+ - NLTK 3.0+  Requirements: - Python 3.5+ - NLTK 3.0+  The experiments in BabyAI require Python 3.6.9 (the latest version of Python supported by Google Colab). In order to install the required dependencies call pip install . inside the babyai_rb folder.    * --rb: the required bolt    To run the translator  in the `Cfg` folder enter:  ``` python ./CFG2LTLf.py --pathNL './cfg_nl.txt' --pathLTLf './cfg.ltlf' --sentence 'Go to a red ball' ``` use `--sentence` to input the sentence to translate  `--pathNL` to specify the path to the NL CFG and `--pathLTLf` to specify the path to the LTLf CFG.   To run the translator  in the `LambdaCalculus` folder enter:  ``` python ./NL2LTLf.py --path './mappings.csv' --sentence 'Go to a red ball' --set_pronouns 'True' ``` use `--sentence`  to input the sentence to translate  `--path` to specify the path to the mapping `.csv` file and `--set_pronouns` to enable/disable pronoun handling.   All auxiliary classes contains JavaDocs. To translate a sentence use `NL2LTLTranslator.translate(sentence)`. The class `NL2LTLTranslator` contains a main method with some examples.   To visualize a demo of the agent execute `babyai_rb/scripts/enjoy.py` specifying the environment  the model and the  restraining bolt (and additionally `rb-prop`). For example to visualize the model trained previously (Experiment 1) call ``` python babyai_rb/scripts/enjoy.py --env BabyAI-GoToRedBall-v0 --model $MODEL --rb VisitBoxAndPickRestrainingBolt --rb-prop 0 ``` where `$MODEL` is the name of the trained model in `babyai_rb/scripts/models/`.   """;Reinforcement Learning;https://github.com/GiadaSimionato/Reasoning_Agents_2020
"""Please follow the instructions of py-faster-rcnn [here](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to setup VOC and COCO datasets (Part of COCO is done). The steps involve downloading data and optionally creating soft links in the ``data`` folder. Since faster RCNN does not rely on pre-computed proposals  it is safe to ignore the steps that setup proposals.  If you find it useful  the ``data/cache`` folder created on my side is also shared [here](http://ladoga.graphics.cs.cmu.edu/xinleic/tf-faster-rcnn/cache.tgz).   1. Clone the repository   ```Shell   git clone https://github.com/endernewton/tf-faster-rcnn.git   ```  2. Update your -arch in setup script to match your GPU   ```Shell   cd tf-faster-rcnn/lib   #: Change the GPU architecture (-arch) if necessary   vim setup.py   ```    | GPU model  | Architecture |   | ------------- | ------------- |   | TitanX (Maxwell/Pascal) | sm_52 |   | GTX 960M | sm_50 |   | GTX 1080 (Ti) | sm_61 |   | Grid K520 (AWS g2.2xlarge) | sm_30 |   | Tesla K80 (AWS p2.xlarge) | sm_37 |    **Note**: You are welcome to contribute the settings on your end if you have made the code work properly on other GPUs. Also even if you are only using CPU tensorflow  GPU based code (for NMS) will be used by default  so please set **USE_GPU_NMS False** to get the correct output.   3. Build the Cython modules   ```Shell   make clean   make   cd ..   ```  4. Install the [Python COCO API](https://github.com/pdollar/coco). The code requires the API to access COCO dataset.   ```Shell   cd data   git clone https://github.com/pdollar/coco.git   cd coco/PythonAPI   make   cd ../../..   ```     - Due to the randomness in GPU training with Tensorflow especially for VOC  the best numbers are reported (with 2-3 attempts) here. According to my experience  for COCO you can almost always get a very close number (within ~0.2%) despite the randomness.      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz     tar -xzvf vgg_16_2016_08_28.tar.gz      cd ../..     For Resnet101  you can set up like:      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz     tar -xzvf resnet_v1_101_2016_08_28.tar.gz      cd ../..     ./experiments/scripts/train_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/train_faster_rcnn.sh 1 coco res101     ./experiments/scripts/test_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/test_faster_rcnn.sh 1 coco res101   1. Download pre-trained model   ```Shell   #: Resnet101 for voc pre-trained on 07+12 set   ./data/scripts/fetch_faster_rcnn_models.sh   ```   **Note**: if you cannot download the models through the link  or you want to try more models  you can check out the following solutions and optionally update the downloading script:   - Another server [here](http://xinlei.sp.cs.cmu.edu/xinleic/tf-faster-rcnn/).   - Google drive [here](https://drive.google.com/open?id=0B1_fAEgxdnvJSmF3YUlZcHFqWTQ).  2. Create a folder and a soft link to use the pre-trained model   ```Shell   NET=res101   TRAIN_IMDB=voc_2007_trainval+voc_2012_trainval   mkdir -p output/${NET}/${TRAIN_IMDB}   cd output/${NET}/${TRAIN_IMDB}   ln -s ../../../data/voc_2007_trainval+voc_2012_trainval ./default   cd ../../..   ```  3. Demo for testing on custom images   ```Shell   #: at repository root   GPU_ID=0   CUDA_VISIBLE_DEVICES=${GPU_ID} ./tools/demo.py   ```   **Note**: Resnet101 testing probably requires several gigabytes of memory  so if you encounter memory capacity issues  please install it with CPU support only. Refer to [Issue 25](https://github.com/endernewton/tf-faster-rcnn/issues/25).  4. Test with pre-trained Resnet101 models   ```Shell   GPU_ID=0   ./experiments/scripts/test_faster_rcnn.sh $GPU_ID pascal_voc_0712 res101   ```   **Note**: If you cannot get the reported numbers (79.8 on my side)  then probably the NMS function is compiled improperly  refer to [Issue 5](https://github.com/endernewton/tf-faster-rcnn/issues/5).   """;Computer Vision;https://github.com/vietlinhtspt/NewFasterRCNN
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.  git clone https://github.com/affinelayer/pix2pix-tensorflow.git  cd pix2pix-tensorflow   ```sh #: clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git cd pix2pix-tensorflow #: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir facades_train \   --max_epochs 200 \   --input_dir facades/train \   --which_direction BtoA #: test the model python pix2pix.py \   --mode test \   --output_dir facades_test \   --input_dir facades/val \   --checkpoint facades_train ```  The test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.  If you have Docker installed  you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:  ```sh #: train the model python tools/dockrun.py python pix2pix.py \       --mode train \       --output_dir facades_train \       --max_epochs 200 \       --input_dir facades/train \       --which_direction BtoA #: test the model python tools/dockrun.py python pix2pix.py \       --mode test \       --output_dir facades_test \       --input_dir facades/val \       --checkpoint facades_train ```   <img src=""docs/combine.png"" width=""900px""/>  ```sh #: Resize source images python tools/process.py \   --input_dir photos/original \   --operation resize \   --output_dir photos/resized #: Create images with blank centers python tools/process.py \   --input_dir photos/resized \   --operation blank \   --output_dir photos/blank #: Combine resized images with blanked images python tools/process.py \   --input_dir photos/resized \   --b_dir photos/blank \   --operation combine \   --output_dir photos/combined #: Split into train/val set python tools/split.py \   --dir photos/combined ```  The folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.   """;Computer Vision;https://github.com/JobQiu/hackrice
"""All code runs on Python 3.6.7 using [PyTorch version 1.0.0](https://github.com/pytorch/pytorch/tree/v1.0.0).  Our implementations build on the `torch.distributed` package in PyTorch  which provides an interface for exchanging tensors between multiple machines. The `torch.distributed` package in PyTorch v.1.0.0 can use different backends. We recommmend using NCCL for all algorithms (this is the default).  To install the Stochastic Gradient Push library  via pip: ```bash git clone https://github.com/facebookresearch/stochastic_gradient_push.git cd stochastic_gradient_push pip install . ```  If you want to use the parsing scripts to parse results  you can instead do: ```bash git clone https://github.com/facebookresearch/stochastic_gradient_push.git cd stochastic_gradient_push pip install -e .[parse] ```   The job_scripts/ directory contains the following files:   In all cases  the scripts will need to be editied/modified in order to run on your cluster/setup. They also contain instructions on how to modify the script  e.g.  to vary the number of nodes or other parameters.   """;General;https://github.com/facebookresearch/stochastic_gradient_push
"""python 3.7.1  pip install -r requirements.txtで環境を揃えることができます。   """;Audio;https://github.com/zassou65535/WaveGAN
"""This repo was tested with Ubuntu 16.04.5 LTS  Python 3.5  PyTorch 0.4.0  and CUDA 9.0. But it should be runnable with recent PyTorch versions >=0.4.0  **Note:** It seems to us that training with Pytorch version >= 1.0 yields slightly worse results. If you find the similar discrepancy and figure out the problem  please report this since we are trying to fix it as well.    --tb_path /path/to/tensorboard    --tb_path path/to/tensorboard \    --save_path /path/to/save \   --tb_path /path/to/tensorboard \   Note:    I have implemented and tested MoCo and InsDis on a ImageNet100 subset (but the code allows one to train on full ImageNet simply by setting the flag `--dataset imagenet`):  The pre-training stage:  - For InsDis:     ```     CUDA_VISIBLE_DEVICES=0 1 2 3 python train_moco_ins.py \      --batch_size 128 --num_workers 24 --nce_k 16384 --softmax     ``` - For MoCo:     ```     CUDA_VISIBLE_DEVICES=0 1 2 3 python train_moco_ins.py \      --batch_size 128 --num_workers 24 --nce_k 16384 --softmax --moco     ```    The linear evaluation stage: - For both InsDis and MoCo (lr=10 is better than 30 on this subset  for full imagenet please switch to 30):     ```     CUDA_VISIBLE_DEVICES=0 python eval_moco_ins.py --model resnet50 \      --model_path /path/to/model --num_workers 24 --learning_rate 10     ```    The comparison of `CMC` (using YCbCr)  `MoCo` and `InsDIS` on my ImageNet100 subset  is tabulated as below:  |          |Arch | #Params(M) | Loss  | #Negative  | Accuracy | |----------|:----:|:---:|:---:|:---:|:---:| |  InsDis | ResNet50 | 24  | NCE  | 16384  |  --  | |  InsDis | ResNet50 | 24  | Softmax-CE  | 16384  |  69.1  | |  MoCo | ResNet50 | 24  | NCE  | 16384  |  --  | |  MoCo | ResNet50 | 24  | Softmax-CE  | 16384  |  73.4  | |  CMC | 2xResNet50half | 12  | NCE  | 4096  |  --  | |  CMC | 2xResNet50half | 12  | Softmax-CE  | 4096  |  75.8  |   For any questions  please contact Yonglong Tian (yonglong@mit.edu).   """;General;https://github.com/HobbitLong/CMC
"""Create a directory `./data/imagenet` under the root of this repository. This directory should contain:  * `train.txt`  * `train/`  * `val.txt`  * `val/`  The `*.txt` files are lists of labeled images as used in Caffe. See the [Caffe ImageNet tutorial](http://caffe.berkeleyvision.org/gathered/examples/imagenet.html) (specifically the `get_ilsvrc_aux.sh` script) to download them  or prepare them yourself as follows. `train.txt` lists image paths relative to `./data/imagenet/train` and integer labels (`val.txt` is analogous):      n01440764/n01440764_10026.JPEG 0     n01440764/n01440764_10027.JPEG 0     n01440764/n01440764_10029.JPEG 0     n01440764/n01440764_10040.JPEG 0     [...]     n15075141/n15075141_9933.JPEG 999     n15075141/n15075141_9942.JPEG 999     n15075141/n15075141_999.JPEG 999     n15075141/n15075141_9993.JPEG 999  Relative to the root of this repository  the first image listed above should be located at `./data/imagenet/train/n01440764/n01440764_10026.JPEG`.   Create a directory `./data/mnist` under the root of this repository. This directory should contain the MNIST data files (or symlinks to them) with these names:      t10k-images.idx3-ubyte     t10k-labels.idx1-ubyte     train-images.idx3-ubyte     train-labels.idx1-ubyte  The `train_mnist.sh` script trains a ""permutation-invariant"" BiGAN (by default) on the MNIST dataset. MNIST training takes about 30 minutes on a Titan X GPU (400 epochs at ~3.3 seconds per epoch).   You should see output like the following:   For the (joint) latent regressor baselines  change the OBJECTIVE=... setting appropriately (see MNIST instructions above).   You should see output like the following:   To download and install these weights at the locations assumed in eval_model.sh (see below)  do the following from the root of this repository:   for classification experiments  use philkr's ""future"" version of Caffe linked from voc-classification (see below)   To run eval_model.sh yourself  follow these steps:   Modify the variables (CAFFE_DIR  MAGIC_DIR  CLASS_DIR) near the top of eval_model.sh specifying the paths where you installed these packages.       python resize_imageset.py -r -j 4 ${SIZE} ./data/imagenet ./data/imagenet${SIZE}  With an argument of `--raw_size 72` (for example)  `train_gan.py` will automatically check if the presized image directory `./data/imagenet72` exists before falling back to `./data/imagenet`.   """;General;https://github.com/jeffdonahue/bigan
"""Einloggen und dann die erste Stufe der Installation starten  der Rechner rebootet danach automatisch: ```        sudo su        cd ~        git clone https://github.com/wiegehtki/zoneminder-jetson.git        mv ~/zoneminder-jetson ~/zoneminder        cp zoneminder/*sh .        sudo chmod +x *sh ```  Danach die Anpassungen vornehmen und die Installation starten:  ```        ./Install.sh       ```   * Mit Zoneminder Eure IP-Kameras einbinden und mobil verfügbar machen * Den Livestream mit OpenCV und YOLO auf Objekte untersuchen * Erkannte Objekte  z.B. Personen  zuverlässig melden lassen * Gesichter trainieren (Achtung: Datenschutzgesetz beachten!) * Dokument zu Yolo(v4): https://arxiv.org/abs/2004.10934 * Infos zum Darknet framework: http://pjreddie.com/darknet/ * Infos zu OpenCV findet Ihr hier: https://opencv.org/    Einführung und Softwareinstallation: https://youtu.be/USUBtwMYVYg  Inbetriebnahme: https://youtu.be/oek1nLKK53E   In diesem Projekt kommt eine NVIDIA® Grafikkarte zum Einsatz um den Prozessor von rechenintensiven Verarbeitungen zu befreien. Dazu setzen wir NVIDIA®'s CUDA® und cuDNN® ein. CUDA® ist eine Technologie  die es erlaubt Programmteile durch den Grafikprozessor abarbeiten zu lassen während die NVIDIA® CUDA® Deep Neural Network Bibliothek (cuDNN) eine GPU-beschleunigte Bibliothek mit Primitiven für tiefe neuronale Netzwerke darstellt. Solche Primitive  typischerweise neuronale Netzwerkschichten genannt  sind die grundlegenden Bausteine tiefer Netzwerke. cuDNN® und CUDA® samt Treiber sind bereits in JP4.6 enthalten.  Der Script geht davon aus  dass es sich um eine neu aufgesetzte Maschine handelt  falls nicht  müsst Ihr entsprechende Anpassungen machen oder die Befehle per Hand ausführen um sicher zu gehen  dass eine vorhandene Installation nicht beeinträchtigt wird. Empfohlen wird daher  ein verfügbares Testsystem zu nutzen welches neu aufgesetzt werden kann.  Ohne diese Anpassungen wird die Installation nicht funktionieren. Daher ist dieser Schritt besonders sorgfältig durchzuführen.          cd ~   objectconfig.ini:  Diese Datei muss nur dann angepasst werden  wenn das vor-trainierte Model gewechselt werden soll. Ich habe hier yolov4 mit GPU-Unterstützung vor- eingestellt. Sollte man KEINE GPU zur Unterstützung zur Verfügung haben  kann der entsprechende Eintrag notfalls auch auf CPU geändert werden.          object_processor=gpu    Nach der Installation einen reboot ausführen.   Falls nicht  hier die Download-Links:   """;Computer Vision;https://github.com/wiegehtki/zoneminder-jetson
"""1. Install necessary packages: ```     conda create -n tensorflow_gpu pip python=3.5     source activate tensorflow_gpu     pip install --upgrade tensorflow-gpu==1.3.0     pip3 install -r SCNN-Tensorflow/lane-detection-model/requirements.txt ```  2. Download VGG-16:  Download the vgg.npy [here](https://github.com/machrisaa/tensorflow-vgg) and put it in SCNN-Tensorflow/lane-detection-model/data.  3. Pre-trained model for testing:  Download the pre-trained model [here](https://drive.google.com/open?id=1-E0Bws7-v35vOVfqEXDTJdfovUTQ2sf5).   Note that path/to/image_name_list should be like test_img.txt. Now  you get the probability maps from our model. To get the final performance  you need to follow SCNN to get curve lines from probability maps as well as calculate precision  recall and F1-measure.   Note that path/to/CULane-dataset/ should contain files like train_gt.txt and val_gt.txt.   """;Computer Vision;https://github.com/cardwing/Codes-for-Lane-Detection
"""nmt - https://github.com/hlamba28/Word-Level-Eng-Mar-NMT/blob/master/WordLevelEngMarNMT.ipynb <br/>   """;Natural Language Processing;https://github.com/lakshmichaitanyach/project_2
"""<a href=""https://pypi.org/project/transformer-implementations/"">PyPi</a>  ```bash $ pip install transformer-implementations ```  or  ```bash python setup.py build python setup.py install ```           <img alt=""PyPi Version"" src=""https://img.shields.io/pypi/v/transformer-implementations"">           <img alt=""PyPi Downloads"" src=""https://img.shields.io/pypi/dm/transformer-implementations"">           <img alt=""Package Status"" src=""https://img.shields.io/pypi/status/transformer-implementations"">   In <a href=""https://github.com/UdbhavPrasad072300/Transformer-Implementations/blob/main/notebooks/"">notebooks</a> directory there is a notebook on how to use each of these models for their intented use; such as image classification for Vision Transformer (ViT) and others. Check them out!  ```python from transformer_package.models import ViT  image_size = 28 #: Model Parameters channel_size = 1 patch_size = 7 embed_size = 512 num_heads = 8 classes = 10 num_layers = 3 hidden_size = 256 dropout = 0.2  model = ViT(image_size               channel_size               patch_size               embed_size               num_heads               classes               num_layers               hidden_size               dropout=dropout).to(DEVICE)              prediction = model(image_tensor) ```   """;General;https://github.com/UdbhavPrasad072300/Transformer-Implementations
"""Depending on your operating system  you may need the recompile the MEX files in the matlabPyrTools toolbox. If so:   1. Copy your outputs into the ```your_results``` folder in the base directory. 2. Copy the [validation/test set](https://www.pirm2018.org/dataset.html) (HR images only) into the ```self_validation_HR``` folder. 3. Download the [Ma et al. code](https://github.com/chaoma99/sr-metric)  and extract it into the ```utils/sr-metric-master``` folder. 4. Run the ```evaluate_results.m``` script.   """;Computer Vision;https://github.com/roimehrez/PIRM2018
"""An unofficical low-resolution (32 x 32) implementation of BigBiGAN   Paper: https://arxiv.org/abs/1907.02544  Python 3.6   Matplotlib 3.1.1   Change them to fit in your GPU according to your VRAM(>=6 GB recommended).   Set up the flags in ```main.py```. In terminal  enter ```python3 main.py``` to execute the training.  MNIST ![mnist](https://github.com/LEGO999/BIgBiGAN/blob/master/fig/mnist2.png) Fashion-MNIST ![fmnist](https://github.com/LEGO999/BIgBiGAN/blob/master/fig/fmnist-22.png) CIFAR10 ![cifar10](https://github.com/LEGO999/BigBiGAN-TensorFlow2.0/blob/master/fig/cifar10-con-49.png)  """;General;https://github.com/LEGO999/BigBiGAN-TensorFlow2.0
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   For R-FCN/Faster R-CNN\: 1. Please download COCO and VOC 2007+2012 datasets  and make sure it looks like this:  	``` 	./data/coco/ 	./data/VOCdevkit/VOC2007/ 	./data/VOCdevkit/VOC2012/ 	```  2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  For DeepLab\: 1. Please download Cityscapes and VOC 2012 datasets and make sure it looks like this:  	``` 	./data/cityscapes/ 	./data/VOCdevkit/VOC2012/ 	``` 2. Please download argumented VOC 2012 annotations/image lists  and put the argumented annotations and the argumented train/val lists into:  	``` 	./data/VOCdevkit/VOC2012/SegmentationClass/ 	./data/VOCdevkit/VOC2012/ImageSets/Main/ 	```      Respectively.     2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  1. Clone the Deformable ConvNets repository  and we'll call the directory that you cloned Deformable-ConvNets as ${DCN_ROOT}. ``` git clone https://github.com/msracver/Deformable-ConvNets.git ```  2. For Windows users  run ``cmd .\init.bat``. For Linux user  run `sh ./init.sh`. The scripts will build cython module automatically and create some folders.  3. Install MXNet: 	 	**Note: The MXNet's Custom Op cannot execute parallelly using multi-gpus after this [PR](https://github.com/apache/incubator-mxnet/pull/6928). We strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.5).**  	***Quick start***  	3.1 Install MXNet and all dependencies by  	``` 	pip install -r requirements.txt 	``` 	If there is no other error message  MXNet should be installed successfully.  	 	***Build from source (alternative way)***  	3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by 	``` 	git clone --recursive https://github.com/dmlc/mxnet.git 	git checkout 998378a 	git submodule update 	#: if it's the first time to checkout  just use: git submodule update --init --recursive 	``` 	3.3 Compile MXNet 	``` 	cd ${MXNET_ROOT} 	make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1 	``` 	3.4 Install the MXNet Python binding by 	 	***Note: If you will actively switch between different versions of MXNet  please follow 3.5 instead of 3.4*** 	``` 	cd python 	sudo python setup.py install 	``` 	3.5 For advanced users  you may put your Python packge into `./external/mxnet/$(YOUR_MXNET_PACKAGE)`  and modify `MXNET_VERSION` in `./experiments/rfcn/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`. Thus you can switch among different versions of MXNet quickly.  4. For Deeplab  we use the argumented VOC 2012 dataset. The argumented annotations are provided by [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html) dataset. For convenience  we provide the converted PNG annotations and the lists of train/val images  please download them from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMRhVImMI1jRrsxDg).   We provide trained deformable convnet models  including the deformable R-FCN & Faster R-CNN models trained on COCO trainval  and the deformable DeepLab model trained on CityScapes train.  1. To use the demo with our pre-trained deformable models  please download manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMSjehIcCgAhvEAHw) or [BaiduYun](https://pan.baidu.com/s/1dFlPFED)  and put it under folder `model/`.  	Make sure it looks like this: 	``` 	./model/rfcn_dcn_coco-0000.params 	./model/rfcn_coco-0000.params 	./model/fpn_dcn_coco-0000.params 	./model/fpn_coco-0000.params 	./model/rcnn_dcn_coco-0000.params 	./model/rcnn_coco-0000.params 	./model/deeplab_dcn_cityscapes-0000.params 	./model/deeplab_cityscapes-0000.params 	./model/deform_conv-0000.params 	./model/deform_psroi-0000.params 	``` 2. To run the R-FCN demo  run 	``` 	python ./rfcn/demo.py 	``` 	By default it will run Deformable R-FCN and gives several prediction results  to run R-FCN  use 	``` 	python ./rfcn/demo.py --rfcn_only 	``` 3. To run the DeepLab demo  run 	``` 	python ./deeplab/demo.py 	``` 	By default it will run Deformable Deeplab and gives several prediction results  to run DeepLab  use 	``` 	python ./deeplab/demo.py --deeplab_only 	``` 4. To visualize the offset of deformable convolution and deformable psroipooling  run 	``` 	python ./rfcn/deform_conv_demo.py 	python ./rfcn/deform_psroi_demo.py 	```    1. All of our experiment settings (GPU #  dataset  etc.) are kept in yaml config files at folder `./experiments/rfcn/cfgs`  `./experiments/faster_rcnn/cfgs` and `./experiments/deeplab/cfgs/`. 2. Eight config files have been provided so far  namely  R-FCN for COCO/VOC  Deformable R-FCN for COCO/VOC  Faster R-CNN(2fc) for COCO/VOC  Deformable Faster R-CNN(2fc) for COCO/VOC  Deeplab for Cityscapes/VOC and Deformable Deeplab for Cityscapes/VOC  respectively. We use 8 and 4 GPUs to train models on COCO and on VOC for R-FCN  respectively. For deeplab  we use 4 GPUs for all experiments.  3. To perform experiments  run the python scripts with the corresponding config file as input. For example  to train and test deformable convnets on COCO with ResNet-v1-101  use the following command     ```     python experiments\rfcn\rfcn_end2end_train_test.py --cfg experiments\rfcn\cfgs\resnet_v1_101_coco_trainval_rfcn_dcn_end2end_ohem.yaml     ```     A cache folder would be created automatically to save the model and the log under `output/rfcn_dcn_coco/`. 4. Please find more details in config files and in our code.   """;Computer Vision;https://github.com/stupidZZ/pyc_repo
"""For example  to train ViP with 8 GPU on a single node  run:   bash ./distributed_train.sh vip-t-001 configs/vip_t_bs1024.yaml 8   bash ./distributed_train.sh vip-s-001 configs/vip_s_bs1024.yaml 8   bash ./distributed_train.sh vip-m-001 configs/vip_m_bs1024.yaml 8   bash ./distributed_train.sh vip-b-001 configs/vip_b_bs1024.yaml 8   """;Computer Vision;https://github.com/kevin-ssy/ViP
"""You-Only-Look-Once (YOLO) newtork was introduced by Joseph Redmon et al.  Three versions were implemented in C  with the framework called [darknet](https://github.com/pjreddie/darknet) (paper: [v1](https://arxiv.org/abs/1506.02640)  [v2](https://arxiv.org/abs/1612.08242)  [v3](https://arxiv.org/abs/1804.02767)).  This repo implements the Nueral Network (NN) model of YOLOv3 in the PyTorch framework  aiming to ease the pain when the network needs to be modified or retrained.  There are a number of implementations existing in the open source domain   e.g.  [eriklindernoren/PyTorch-YOLOv3](https://github.com/eriklindernoren/PyTorch-YOLOv3)  [ayooshkathuria/pytorch-yolo-v3](https://github.com/ayooshkathuria/pytorch-yolo-v3)  [ultralytics/yolov3](https://github.com/ultralytics/yolov3)  etc. However  majority of them relies on ""importing"" the configuration file from the original darknet framework. In this work  the model is built from scratch using PyTorch.  Additionally  both inference and training part are implemented.  The original weights trained by the authors are converted to .pt file. It can be used as a baseline for transfer learning.  **This project is licensed under BSD 3-Clause ""Revised"" License.**   COCO Average Precision at IoU=0.5 (AP<sub>50</sub>)   ├── requirements.txt   To train on COCO dataset  first you have to download the dataset from COCO dataset website.   Secondly  pycocotools  which serves as the Python API for COCO dataset needs to be installed.  Please follow the instructions on their github repo to install pycocotools.                  [--weight-path WEIGHT_PATH] [--cpu-only] [--from-ckpt]     --annot-path ANNOT_PATH     --weight-path WEIGHT_PATH                           output directory.                           to output directory     --class-path CLASS_PATH   * GPU: Nvidia GeForce GTX 1080 Ti   AP<sub>50</sub> on COCO val2017:   Before cloning the repo to your local machine  make sure that `git-lfs` is installed. See details about `git-lfs`  see [this link](https://www.atlassian.com/git/tutorials/git-lfs#installing-git-lfs).  After `git-lfs` is installed. Run the following command to see sample detection results. ``` git lfs install git clone https://github.com/westerndigitalcorporation/YOLOv3-in-PyTorch cd YOLOv3-in-PyTorch pip install -r requirements.txt cd src python3 main.py test --save-img  ``` Detections will be saved in the `output` folder.   """;Computer Vision;https://github.com/westerndigitalcorporation/YOLOv3-in-PyTorch
"""Most of the progress in computer vision has centered around object detection and  semantic segmentation in images. For image classification  popular networks have  been ResNet  VGG Network  and GoogleNet. We have seen strong image  segmentation architectures such as FCN  SegNet  UNet  and PSPNet. When it has come to video data  the most common approach has been to deploy fast  object detection algorithms on each frame of the video  such as YOLO and  RetinaNet. While this approach is effective  there is certainly room for  improvement. By performing fast object detection frame-by-frame  all of the previous  timestep information is lost  and each timestep is just a brand-new image to the   object detection algorithm. The goal of this project was to investigate the   incorporation of previous timestep information to increase object detection   in video data. This project also provides code for performing object detection  on video data.   To run an pretrained RetinaNet-Resnet model on video data  visit [demo](demo.ipynb) and follow the instructions. To train a RetinaNet-Resnet model  go to  [keras-retinanet](https://github.com/fizyr/keras-retinanet).   """;General;https://github.com/RichardMathewsII/YOLBO
"""In an effort to determine the high-risk customers  the bank observed 10 000 customers over six months. The dataset was partitioned into a training set of 8 000 samples and a testing set of 2 000 samples. They collected a variety of datapoints they figured would be indicative of retention (or  more technically  ""churn""). An excerpt of  the dataset is provided below  |RowNumber   	| CustomerID  	| Surname  	| CreditScore  	| Geography  	| Gender  	| Age  	| Tenure  	| Balance  	| NumOfProducts  	| HasCrCard  	| IsActiveMember  	| EstimatedSalary  	| Exited  	| |---	        |---	          |---	      |---	          |---	        |---	      |---	  |---	      |---	      |---	            |---	        |---	              |---	              |---	      | | 1   	      | 15634602  	  | Hargrave  | 619   	      | France    	| Female  	| 42   	| 2       	| 0       	| 1             	| 1          	| 1               	| 101348.88       	| 1  	      | | 49   	      | 15766205  	  | Yin       | 550   	      | Germany    	| Male    	| 38   	| 2       	| 103391.38 | 1             	| 0          	| 1               	| 90878.13         	| 0  	      |  I didn't collect this dataset. So far as I know  it was generated. If you're interested in obtaining the full dataset  please reach out to me and I can send it to you. I won't be hosting it in this repository to protect the work of its creator.   Python 3.6.6   """;General;https://github.com/patconrey/ANN-Example
"""bash scripts/download_pretrained_scribble_dataset.sh   bash scripts/download_pretrained_sketchy_dataset.sh   bash scripts/download_pretrained_car_outlines.sh   bash scripts/download_pretrained_emoji.sh   One can select patches from the autocompletions using the select patch option. One can cycle through the completions using the scroll button on the mouse and left click to select the patch.   bash scripts/prepare_autocomplete_emoji.sh   bash scripts/prepare_autocomplete_scribble_dataset.sh   - Clone this repo: ``` git clone https://github.com/arnabgho/iSketchNFill cd iSketchNFill ``` - Install PyTorch 1.0+ and dependencies from http://pytorch.org - Install Torchvision - Install all requirements ``` pip install -r requirements.txt ```    """;General;https://github.com/arnabgho/iSketchNFill
"""In order to dowload the South African speech language used in the project  please visit [Sadilar](https://repo.sadilar.org/handle/20.500.12185/7) where the NCHLT  NCHLT Auxliary and Lwazi data can be downloaded. The data can then be extracted to your working directory.   """;General;https://github.com/ruanvdmerwe/triplet-entropy-loss
"""If you installed the optional packages above  you can generate videos that play the soundscapes alongside   one trained on the hand  the other on the table dataset. You may provide the same config name for both   You may run your AEV2A model live by taking a video with your Android phone and listening to the corresponding audio representation at the same time. In our implementation  the captured video is streamed to your PC  where it gets translated into sound  so you may listen to it. Ideally  you would place your phone inside a VR helmet/cardbox  fastening the camera at eye level; headphones are essential.  `run_proto.py` runs the live demo. Similarly to the dataset generation phase  you can set whether to apply the more sophisticated CORF edge detection algorithm (Matlab required)  just Sobel  or nothing at all.  To set up your Android phone with the trained AEV2A model  you first need to:  1. Install the IP Webcam app from Google Play  launch it and set the video resolution to `320x240` under Video preferences 2. Connect your phone via USB to the computer that runs the script 3. Turn USB tethering on the phone  but turn off WiFi and mobile data 4. Launch the IP Webcam app and press ""Start server"". 5. Start the `run_proto.py` script with parameters providing whether to run in ""test"" or ""fast"" mode (test mode shows how the contour image and the decoder reconstructed image looks like real time)  the edge detection algo to apply (`corf`  `sobel` or `nothing`)  the mobile ip of your phone (displayed in the IP Webcam app)  the name of the model configuration and postfix identifier  if used any.  ```bash python run_proto.py test corf mobile_ip config_name model_name_postfix ```  After the model is loaded  you should be seeing three windows of images showing the original  contour and reconstruction stages. The audio should be playing at the same time  too.   """;Audio;https://github.com/csiki/v2a
"""``` python main.py --datasource=omniglot --metatrain_iterations=40000 --meta_batch_size=32 --update_batch_size=1 --update_lr=0.4 --num_updates=1 --logdir=logs/omniglot5way/  ``` ``` python main.py --datasource=omniglot --metatrain_iterations=40000 --meta_batch_size=32 --update_batch_size=1 --update_lr=0.4 --num_updates=1 --logdir=logs/omniglot5way/  --train=False --test_set=True  ```   """;General;https://github.com/foolyc/Meta-SGD
"""getModel(NUM_CLASS name='se_resnet50') 是抽象出来的模型函数，方便以后修改   """;Computer Vision;https://github.com/eatamath/metallic
"""Machine translation is a natural language processing task that aims to translate natural languages using computers automatically. Recent several years have witnessed the rapid development of end-to-end neural machine translation  which has become the new mainstream method in practical MT systems.  THUMT is an open-source toolkit for neural machine translation developed by [the Natural Language Processing Group at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/site2/index.php?lang=en). The website of THUMT is: [http://thumt.thunlp.org/](http://thumt.thunlp.org/).   | PyTorch | Transformer | MLE | SGD  Adadelta  Adam | N.A. |   The online demo of THUMT is available at [http://translate.thumt.cn/](http://101.6.5.207:3892/). The languages involved include Ancient Chinese  Arabic  Chinese  English  French  German  Indonesian  Japanese  Portuguese  Russian  and Spanish.   """;Natural Language Processing;https://github.com/THUNLP-MT/THUMT
"""This post describes how I used the eo-learn and fastai libraries to create a machine learning data pipeline that can classify crop types from satellite imagery. I used this pipeline to enter Zindi’s [Farm Pin Crop Detection Challenge](https://zindi.africa/competitions/farm-pin-crop-detection-challenge). I may not have won the contest but I learnt some great techniques for working with remote-sensing data which I detail in this post.  Here are the preprocessing steps I followed:  1. divided an area of interest into a grid of ‘patches’   1. loaded imagery from disk   1. masked out cloud cover   1. added NDVI and euclidean norm features   1. resampled the imagery to regular time intervals   1. added raster layers with the targets and identifiers.  I reframed the problem of crop type classification as a semantic segmentation task and trained a U-Net with a ResNet50 encoder on multi-temporal multi-spectral data using image augmentation and mixup to prevent over-fitting.  My solution borrows heavily from the approach outlined by [Matic Lubej](undefined) in his [three](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-1-2471e8098195) [excellent](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-2-bd9aa86f8500) [posts](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-3-c62ed9ecd405) on land cover classification with [eo-learn](https://github.com/sentinel-hub/eo-learn).  The python notebooks I created can be found in this github repository: [https://github.com/simongrest/farm-pin-crop-detection-challenge](https://github.com/simongrest/farm-pin-crop-detection-challenge)   From my 12 patches I randomly sampled 64 x 64 pixel ‘patchlets’ to train my model. I kept the patch size small as the fields themselves are relatively small and the provided Sentinel2 imagery has a maximum spatial resolution of 10m . This means a square field 1 hectare in size (10 000m²) is appears in the imagery as an area of 32 x 32 pixels.  I sampled the patchlets in a manner that ensured that each patchlet contained at least a part of a training field. For each patchlet I saved two pickle files  one containing the input imagery and the other the raster layer with the crop types.  For the input imagery I chose to include six channels  the three visible bands (red  green and blue)  near infra-red and the calculated NDVI and euclidean norm. When I resampled the imagery by interpolating through time I ended up with eight different time points. In order to get a rank three tensor  I simply stacked the six channels at each of the eight time points to get a 48-channel image.  ![NDVI and visible images at a single time point along with the corresponding target crop types for nine randomly sampled 64x64 training ‘patchlets’](https://cdn-images-1.medium.com/max/4228/1*LN8c786HRp40pyXxRvCp6w.png)*NDVI and visible images at a single time point along with the corresponding target crop types for nine randomly sampled 64x64 training ‘patchlets’*   """;Computer Vision;https://github.com/simongrest/farm-pin-crop-detection-challenge
"""Use Pytesseract [https://pypi.org/project/pytesseract/]   """;Computer Vision;https://github.com/Luckygyana/Invo-AI
"""  ""name"": ""hotel_tagging""    |span_sim         | Replacing a span with similar a one               |   """;Computer Vision;https://github.com/rit-git/Snippext_public
"""Watershed: https://docs.opencv.org/master/d3/db4/tutorial_py_watershed.html     - performed voronoi using scipy on centroids   """;Computer Vision;https://github.com/Simurgh818/UCSF_QBI_Hackathon_ImageSegmentor
"""conda 4.3.30  Python 3.6.3   numpy 1.13.3  matplotlib 2.1.0   ![Alt text](/notebook/findings.png?raw=true """")  Based on the somewhat lackluster results of the Transfer Learning investigation  and inspired by Stanford ML Group's [CheXNet](https://stanfordmlgroup.github.io/projects/chexnet/)  I also decided to train a DenseNet [1] network to perform binary classification on the same NIH chest x-ray dataset  albeit on a single category (""Pulmonary Fibrosis""). In this case  the binary classification is a one-vs-all (a.k.a. one-vs-rest) approach for the lung disease category: ""Pulmonary Fibrosis""; samples from this single class are taken against a random sample of the other 14 categories (including ""No Finding""). This approach seemed to have achieved near state of the art results  with an AUROC of far greater than 0.05 of previously published results [2]. Due to my surprise  I immediately gathered another distinct chest x-ray dataset (Indiana University - IU)  which also included Pulmonary Fibrosis examples  and I tested that dataset.    """;Computer Vision;https://github.com/b5510546671/Chest-Xrays-Leaning
""": get output tensor   """;Audio;https://github.com/benmoseley/simple-wavenet
"""python: 3.7  Pytorch: 1.6.0   """;General;https://github.com/crashmoon/MAML-Pytorch-Multi-GPUs
"""DeepLab is a series of image semantic segmentation models  whose latest version  i.e. v3+  proves to be the state-of-art. Its major contribution is the use of atrous spatial pyramid pooling (ASPP) operation at the end of the encoder. While the model works extremely well  its open source code is hard to read (at least from my personal perspective). Here we re-implemented DeepLab V3  the earlier version of v3+ (which only additionally employs the decoder architecture)  in a much simpler and more understandable way.   To install dependencies  please run the following command to install everything required automatically:  ```bash $ chmod +x install_dependencies.sh $ pip install -r requirements.txt $ ./install_dependencies.sh ``` If found permission problems  please run the following command instead:  ```bash $ chmod +x install_dependencies.sh $ pip install -r requirements.txt $ sudo ./install_dependencies.sh ```                           Labels directory                           Labels augmented directory     --log_dir LOG_DIR     TensorBoard log directory   For simplicity  please run the following command in terminal:   To show some demos  please run the following command in terminal:  ```bash $ python test_demo.py ```  Image| Label | Prediction | :-------------------------:|:-------------------------:|:-------------------------: ![](data/demos/deeplab/resnet_101_voc2012/image_0.jpg)  |  ![](data/demos/deeplab/resnet_101_voc2012/image_0_label.png) |  ![](data/demos/deeplab/resnet_101_voc2012/image_0_prediction.png)  Image| Label | Prediction | :-------------------------:|:-------------------------:|:-------------------------: ![](data/demos/deeplab/resnet_101_voc2012/image_1.jpg)  |  ![](data/demos/deeplab/resnet_101_voc2012/image_1_label.png) |  ![](data/demos/deeplab/resnet_101_voc2012/image_1_prediction.png)  Image| Label | Prediction | :-------------------------:|:-------------------------:|:-------------------------: ![](data/demos/deeplab/resnet_101_voc2012/image_2.jpg)  |  ![](data/demos/deeplab/resnet_101_voc2012/image_2_label.png) |  ![](data/demos/deeplab/resnet_101_voc2012/image_2_prediction.png)  Image| Label | Prediction | :-------------------------:|:-------------------------:|:-------------------------: ![](data/demos/deeplab/resnet_101_voc2012/image_3.jpg)  |  ![](data/demos/deeplab/resnet_101_voc2012/image_3_label.png) |  ![](data/demos/deeplab/resnet_101_voc2012/image_3_prediction.png)   Just put some JPG-format images into `demo_dir` and run the following command in the terminal.  ```bash $ python test_any_image.py ``` Results will be written into same folder. Make sure that proper model trained and a checkpoint is saved in `models_dir`. See the script for details.  Contributed by [pinaxe1](https://github.com/leimao/DeepLab_v3/pull/7). Will modify to accept arguments and multiple image formats.   """;Computer Vision;https://github.com/leimao/DeepLab-V3
"""참고한 코드(baseline):https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/object_detection/YOLO  참고한 유튜브: https://www.youtube.com/watch?v=n9_XyCGr-MI     """;Computer Vision;https://github.com/shkim960520/YOLO-v1-for-studying
"""```bash cd ${PRJROOT}/detection  #: Labeled and Unlabeled datasets DATASET=coco_train2017.1@10 UNLABELED_DATASET=${DATASET}-unlabeled  #: PATH to save trained models CKPT_PATH=result/${DATASET}  #: PATH to save pseudo labels for unlabeled data PSEUDO_PATH=${CKPT_PATH}/PSEUDO_DATA  #: Train with 8 GPUs export CUDA_VISIBLE_DEVICES=0 1 2 3 4 5 6 7 ```   __tensorpack with a compatible version is already included at third_party/tensorpack.__ `bash cd ${PRJROOT}/third_party pip install --upgrade git+https://github.com/tensorpack/tensorpack.git`   ```bash cd ${PRJROOT}  sudo apt install python3-dev python3-virtualenv python3-tk imagemagick virtualenv -p python3 --system-site-packages env3 . env3/bin/activate pip install -r requirements.txt  #: Make sure your tensorflow version is 1.14 not only in virtual environment but also in #: your machine  1.15 can cause OOM issues. python -c 'import tensorflow as tf; print(tf.__version__)'  #: install coco apis pip3 install 'git+https://github.com/cocodataset/cocoapi.git#:subdirectory=PythonAPI' ```   ```bash export PRJROOT=/path/to/your/project/directory/STAC export DATAROOT=/path/to/your/dataroot export COCODIR=$DATAROOT/coco export VOCDIR=$DATAROOT/voc export PYTHONPATH=$PYTHONPATH:${PRJROOT}/third_party/FasterRCNN:${PRJROOT}/third_party/auto_augment:${PRJROOT}/third_party/tensorpack ```   Besides instruction at here  detection/scripts/coco/train_stac.sh   . scripts/coco/train_stg1.sh.       TRAIN.EVAL_PERIOD=20 \   . scripts/coco/eval_stg1.sh.  : Check pseudo path       mkdir -p ${PSEUDO_PATH}   . scripts/coco/train_stg2.sh.       TRAIN.EVAL_PERIOD=20 \   """;General;https://github.com/google-research/ssl_detection
"""If you want to go somewhere regarding implementation  please skip this part.    YOLOv3 is a light-weight but powerful one-stage object detector  which means it regresses the positions of objects and predict the probability of objects directly from the feature maps of CNN. Typical example of one-state detector will be YOLO and SSD series.On the contrary   two stage detector like R-CNN  Fast R-CNN and Faster R-CNN may include Selective Search  Support Vector Machine (SVM) and Region Proposal Network (RPN) besides CNN. Two-stage detectors will be sightly more accurate but much slower.   YOLOv3 consists of 2 parts: feature extractor and detector. Feature extractor is a Darknet-53 without its fully connected layer  which is originally designed for classification task on ImageNet dataset.    ![darknet](/fig/Darknet.png)   *Darknet-53 architecture(Source: YOLOv3: An Incremental Improvement https://arxiv.org/abs/1804.02767)*  Detector uses multi-scale fused features to predict the position and the class of the corresponding object.   ![yolov3](/fig/yolo.png)*(Source: https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)*  As you can see from the picture above  there are 3 prediction scales in total. For example  if the spatial resolution of an input image is 32N X 32N  the output of the first prediction convolution layer(strides32) will be N X N X (B X (C+5)). B indicates amount of anchors at this scale and C stands for probabilities of different classes. 5 represents 5 different regressions  the  horizontal offset t_x  the vertical offset t_y  resizing factor of the given anchor height t_hand width t_wand objectness score o (whether an object exists in this square of the checkerboard). The second prediction layer will output feature maps of 2N X 2N X (B X (C+5)). And the third prediction output will be much finer  which is 4N X 4N X (B X (C+5).  Reading papers of YOLO  YOLOv2 and YOLOv3  I summarize the loss function of YOLOv3 as follows:   ![](/fig/loss1.PNG) <!-- $$ L_{Localization} = \lambda_1\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}[(t_{x} - t_{\hat{x}})^2 + (t_{y} - t_{\hat{y}})^2] \\L_{Shaping} =\lambda_2\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}[(t_{w} - t_{\hat{w}})^2 + (t_{h} - t_{\hat{h}})^2]\\ L_{objectness-obj} =\lambda_3\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}\log(o_{ij})$$ $$L_{objectness-noobj} =\lambda_4\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}\log(1-o_{ij}) \\L_{class} =\lambda_5\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}\sum_{c\in classes}[p_{\hat{ij}}(c)\log(p_{ij}(c))+ (1-p_{\hat{ij}}(c))\log(1-p_{ij}(c))]) \\ L_{Scale_{1}} = L_{Localization} + L_{Shaping} + L_{objectness-obj} + L_{objectness-noobj} + L_{class} \\ L_{total} = L_{Scale_{1}}+L_{Scale_{2}}+L_{Scale_{3}}$$ -->   This is my implementation of YOLOv3 using TensorFlow 2.0 backend. The main purpose of this project is to get me familiar with deep learning and specific concepts in domain object detection. Two usages are provided: * Object detection based on official pre-trained weights in COCO * Object detection of optic nerve on Indian Diabetic Retinopathy Image Dataset (IDRiD) using fine tuning.  ![nerve](/fig/optics_nerve.png) *Fundus and the corresponding optic nerve*  The following content will be provided in this repo: * Introduction of YOLOv3 * Object detection based on the official pre-trained weights * Object detection - fine tuning on IDRiD       Name | anchors   Following YOLOv2:   [![](http://img.youtube.com/vi/6mWNgng6CfY/0.jpg)](http://www.youtube.com/watch?v=6mWNgng6CfY """")    https://www.youtube.com/watch?v=6mWNgng6CfY&t=3s  ![](/fig/k-means.gif)  What in the red frame line is our ground truth. ![](/fig/detection1.gif)  """;Computer Vision;https://github.com/LEGO999/YOLOV3-TF2
"""Change the `cudatoolkit` version compatible to your machine. ```bash conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0 pip install git+https://github.com/wilson1yan/VideoGPT.git ```   sudo apt-get install llvm-9-dev  DS_BUILD_SPARSE_ATTN=1 pip install deepspeed   sh scripts/preprocess/bair/create_bair_dataset.sh datasets/bair   Alternatively  the code supports a dataset with the following directory structure:   sh scripts/preprocess/ucf101/create_ucf_dataset.sh datasets/ucf101   You may need to install unrar and unzip for the code to work correctly.   Note that both pre-trained models use sparse attention. For purposes of fine-tuning  you will need to install sparse attention  however  sampling does not required sparse attention to be installed.   VideoGPT models can be sampled using the scripts/sample_videogpt.py. You can specify a path to a checkpoint during training  or the name of a pretrained model. You may need to install ffmpeg: sudo apt-get install ffmpeg   """;General;https://github.com/wilson1yan/VideoGPT
"""You should have the latest versions for (as of 7/2017): * keras * nltk * numpy * pandas * tensorflow (1.3.0 or greater  with CUDA 8.0 and cuDNN 6.0 or greater) * unidecode * sacremoses ([see issue regarding this](https://github.com/jmyrberg/finnlem/issues/1))  After this  clone this repository to your local machine.  Update 10.9.2020: You could also try to first clone and then run `pip install -r requirements.txt` at the root of this repository. This will install the latest versions of the required packages automatically  but notice that the very latest versions of some of the packages might nowadays be incompatible with the source code provided here. Feel free to make a pull request with fixed versions of the packages  in case you manage to run the source code successfully :)   python -m dict_train   python -m model_train   python -m model_decode   * one source document per line  or   To use tensorboard  run command python -m tensorflow.tensorboard --logdir=model_dir     Three-steps are required in order to get from zero to making predictions with a trained model:  1. **Dictionary training**: Dictionary is created from training documents  which are processed the same way as the Seq2Seq model inputs later on. 	Dictionary handles vocabulary/integer mappings required by Seq2Seq. 2. **Model training**: Seq2Seq model is trained in batches with training documents that contain source and target. 3. **Model decoding**: Unseen source documents are fed into Seq2Seq model  which makes predictions on the target.   """;Sequential;https://github.com/jmyrberg/finnlem
"""*This is my entry into the [Bristol-Myers Squibb Molecular Translation](https://www.kaggle.com/c/bms-molecular-translation)  Kaggle competition. The notebook is publicly available at https://www.kaggle.com/c/bms-molecular-translation/discussion/url.*  Kagglers have coalesced around ""Attention is What You Need"" models  so I ask  *Is attention really all you need?*  This notebook include features to test that out: Enable/Disable CNN text feature extraction before the decoder self-attention; Increase model parameters without harming inference speed using decoder heads in series; and Experiment with my trainable & parallelizable alternative to beam search.  ----  """;Computer Vision;https://github.com/mvenouziou/Project-Attention-Is-What-You-Get
"""1.  Clone the repo: ```bash git clone https://github.com/eliahuhorwitz/DeepSIM.git cd DeepSIM ``` 2. Create a new environment and install the libraries: ```bash python3.7 -m venv deepsim_venv source deepsim_venv/bin/activate pip install -r requirements.txt ```  <br> <br>   - Train DeepSIM on the ""face"" video using both edges and segmentations (bash ./scripts/train_face_vid_seg_edges.sh):   Train DeepSIM on the ""car"" image using segmentation only (bash ./scripts/train_car_seg.sh):   Train DeepSIM on the ""face"" image using edges only (bash ./scripts/train_face_edges.sh):   Test DeepSIM on the ""face"" video using both edges and segmentations (bash ./scripts/test_face_vid_seg_edges.sh):   Test DeepSIM on the ""car"" image using segmentation only (bash ./scripts/test_car_seg.sh):   Test DeepSIM on the ""face"" image using edges only (bash ./scripts/test_face_edges.sh):   Train DeepSIM on the ""face"" video using both edges and segmentations with sheer  rotations  ""cutmix""  and canny sigma augmentations (bash ./scripts/train_face_vid_seg_edges_all_augmentations.sh):   """;Computer Vision;https://github.com/eliahuhorwitz/DeepSIM
"""**Requirements/Dependencies**  - Windows10 (You can find the solution for Linux or macOS [here](https://github.com/ErikGDev/instance-segmentation)) - Python ≥ 3.6 - PyTorch ≥ 1.3 - [torchvision](https://github.com/pytorch/vision/) that matches the PyTorch installation. 	You can install them together at [pytorch.org](https://pytorch.org) to make sure of this. Please ensure that your version of CUDA is also compatible when installing. You can run this code without CUDA  but it will be much slower  e.g. 10x slower but not really test it. - OpenCV `pip3 install opencv-python` - Intel RealSense SDK 2.0 Installation: [here](https://www.intelrealsense.com/sdk-2/) - PyRealSense `pip3 install pyrealsense2` - Build Detectron2:   + **Build Detectron2 from Source**       [Windows] Install Visual C++ Build tools form [this link](https://answers.microsoft.com/en-us/windows/forum/windows_10-windows_install/microsoft-visual-c-140-is-required-in-windows-10/f0445e6b-d461-4e40-b44f-962622628de7).  Then restart your PC  then you also need to upgrade Python setup tools  by running this command: `pip3 install --upgrade setuptools`.       Then you can install Detectron2 from source by running:      ```bash      [Note: This should be the easiest way to build Detectron2 in Windows10!]      pip install git+https://github.com/facebookresearch/detectron2.git      #: (add --user if you don't have permission)       #: Or  to install it from a local clone:      git clone https://github.com/facebookresearch/detectron2.git      cd detectron2 && pip3 install -e .       #: Or if you are on macOS      #: CC=clang CXX=clang++ pip install -e .      ```       If the installation is not proper  you may see the error of ""cannot import name '_C' #157"" when running the `main_xxx.py`.       For more details on the installation of Detectron2 and its dependencies  please refer to the [official Detectron2 GitHub](https://github.com/facebookresearch/detectron2).  **After Installation**  1. Clone or download this repository. 2. To perform instance segmentation straight from a D435 camera attached to a USB port:   * Run one of the two python files i.e. `main_xxx_win10.py`   * If using .bag files:     * Type 'python3 main_xxx_win10.py --file={filename}' where {filename} is the name of the input .bag file. To create .bag files  use d435_to_file.py in [this repository](https://github.com/ErikGDev/instance-segmentation/tree/master/tools).  ---  _(For conveniently recalling the background  I here copy and paste most of the content from [this awesome rep](https://github.com/ErikGDev/instance-segmentation) as below.)_  |  | Backbone | AP | AP<sub>50</sub> | AP<sub>75</sub> | AP<sub>S</sub> | AP<sub>M</sub> | AP<sub>L</sub> | | :--- | :--- | :---: | :---: | :---: |  :---:  | :---: | :---: | | Original Mask R-CNN   | ResNet-101-FPN  | 35.7 | 58.0 | 37.8 | 15.5 | 38.1 | 52.4 | | Matterport Mask R-CNN | ReSNet-101-FPN | 38.0 | 55.8 | <b>41.3</b> | 17.9 | <b>45.5</b> | <b>55.9</b> | | Detectron2 Mask R-CNN | ReSNet-101-FPN | <b>38.6</b> | <b>60.4</b> | <b>41.3</b> | <b>19.5</b> | 41.3 | 55.3 |  Validation tests were perfomed on the segmentation masks created on the **2017 COCO** validation dataset. The standard COCO validation metrics include average AP over IoU thresholds  AP<sub>50</sub>  AP<sub>75</sub>  and AP<sub>S</sub>  AP<sub>M</sub> and AP<sub>L</sub> (AP at different scales). These results were then compared to COCO validation results from the [original paper](https://arxiv.org/abs/1703.06870) and a popular [Mask R-CNN implementation by Matterport](https://github.com/matterport/Mask_RCNN). Clearly  Detectron2's Mask R_CNN outperforms the original Mask R_CNN and Matterport's Mask R_CNN with respect to average precision. It also outperformed SOTA COCO segmentation competition winners from the [2015 and 2016 challenge](http://cocodataset.org/#detection-leaderboard).   """;Computer Vision;https://github.com/bowu1004/instance_segmentation_RealSense
"""To run this organized notebook  you need the following packages: pytorch  PIL  cv2.   """;General;https://github.com/gaetandi/cheXpert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/Charliesgithub20221030/BERT
"""```bash git clone https://github.com/fastai/fastai cd fastai tools/run-after-git-clone pip install -e "".[dev]"" git pull  ```  Run train_search_nb.ipynb to search for genotype. ~5 hours on 1 v100 gpu for 1 run.\   """;General;https://github.com/tinhb92/rnn_darts_fastai
"""Pytorch based implementation of faster rcnn framework.For details about faster R-CNN please refer to the paper [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497) by Shaoqing Ren  Kaiming He  Ross Girshick  Jian Sun   This detection framework has the following features:   * It can be run as pure python code  and also pure based on pytorch framework  no need to build * It is easily trained by only running a train.py script  just set the data root dir * It has many backbone networks. like vgg  resnet-fpn  mobilenet  high resolution net(HRNet) * It can be a really detection framework. You only need to change super parameters in config file and get different models to compare different model * It's memory-efficient (about 3GB for vgg16)    git clone https://github.com/AlphaJia/pytorch-faster-rcnn.git    tar xvf train2017.zip   tar xvf val2017.zip   tar xvf test2017.zip   tar xvf annotations_trainval2017.zip   --backbone_pretrained_weights: backbone pretrained weights  None or path   --gpu_id: cuda device gpu ID   ![img](imgs/demo1.png)    """;Computer Vision;https://github.com/AlphaJia/pytorch-faster-rcnn
"""github地址： https://github.com/JonathanRaiman/tensorflow-infogan   """;Computer Vision;https://github.com/chenyuqi990215/infogan
"""Clone repository and update python path ``` cd ~ git clone https://github.com/heye0507/individualNLPClassifier.git cd individualNLPClassifier/ docker build -t nlp_classifier . docker run -it nlp_classifier ```    PSM_nlp can be used as a python package   python3 setup.py build   path = Path(os.getcwd() + '/demo_data')   :#:#: Please note you will need GPU to run the following line #:#:#:   path = Path(os.getcwd() + '/demo_data')   Run the following two lines for demo  - Take Basilica dataset(s) - Using AWD_LSTM pre-trained general classification model  fine tune classification head on new dataset - Save the Pytorch model for in the designated path ``` source activate nlp_model_gen python3 runner.py --input-file=demo_data/ ```  """;General;https://github.com/heye0507/individualNLPClassifier
"""1. Download and extract [LJSpeech dataset](https://keithito.com/LJ-Speech-Dataset/). 2. Put LJSpeech dataset in `data`. 3. Unzip `alignments.zip`. 4. Put [Nvidia pretrained waveglow model](https://drive.google.com/file/d/1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx/view?usp=sharing) in the `waveglow/pretrained_model` and rename as `waveglow_256channels.pt`; 5. Run `python3 preprocess.py`.   """;Audio;https://github.com/xcmyz/FastSpeech
"""```python img = torch.ones([1  3  224  224])  model = CvT(224  3  1000)  parameters = filter(lambda p: p.requires_grad  model.parameters()) parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000 print('Trainable Parameters: %.3fM' % parameters)  out = model(img)  print(""Shape of out :""  out.shape)  #: [B  num_classes] ```  """;Computer Vision;https://github.com/rishikksh20/convolution-vision-transformers
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/chrisseiler96/bert-client-server-tests
"""1. Install and use the included Ananconda environment ``` $ conda env create -f environment/[linux-cpu|linux-gpu|mac]-env.yml $ source activate rlkit (rlkit) $ python examples/ddpg.py ``` Choose the appropriate `.yml` file for your system. These Anaconda environments use MuJoCo 1.5 and gym 0.10.5. You'll need to [get your own MuJoCo key](https://www.roboti.us/license.html) if you want to use MuJoCo.  2. Add this repo directory to your `PYTHONPATH` environment variable or simply run: ``` pip install -e . ```  3. (Optional) Copy `conf.py` to `conf_private.py` and edit to override defaults: ``` cp rlkit/launchers/conf.py rlkit/launchers/conf_private.py ```  4. (Optional) If you plan on running the Skew-Fit experiments or the HER example with the Sawyer environment  then you need to install [multiworld](https://github.com/vitchyr/multiworld).  DISCLAIMER: the mac environment has only been tested without a GPU.  For an even more portable solution  try using the docker image provided in `environment/docker`. The Anaconda env should be enough  but this docker image addresses some of the rendering issues that may arise when using MuJoCo 1.5 and GPUs. The docker image supports GPU  but it should work without a GPU. To use a GPU with the image  you need to have [nvidia-docker installed](https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)).       - Requires multiworld to be installed       version   To get started  checkout the example scripts  linked above.  Use new multiworld code that requires explicit environment registration.   The initial release for 0.2 has the following major changes:   Upgraded to PyTorch v0.4   You can use a GPU by calling   If you are using doodad (see below)  simply use the use_gpu flag:    - LOCAL_LOG_DIR is the directory set by rlkit.launchers.config.LOCAL_LOG_DIR. Default name is 'output'.   If you have rllab installed  you can also visualize the results   to visualize all experiments with a prefix of exp_prefix. To only visualize a single run  you can do   """;Reinforcement Learning;https://github.com/rail-berkeley/rlkit
"""A little ""fingering exercise"" to get a feel for Julia  may also contain   To run it  start julia  then run   Make instructions for how to start easily from the command line.   Make it possible to run this thing  with snapshotting  for weeks.  Make the flux calculations run using a GPU   that any priority now.   setup  so I need to work on that.  Chess is a complicated game with   Don't be cute  use denormalized tables  one per metric    Using the FEN viewser  from the chrome market may be sufficient: https://chrome.google.com/webstore/detail/simplechessboard/hppnfmeaoiochhjdlojgflkfedncdokl   """;Reinforcement Learning;https://github.com/la3lma/Chez
"""<p align=""justify""> The following commands learn a graph embedding  cluster centers and writes them to disk. The node representations are ordered by the ID. </p> <p align=""justify""> Creating an MNMF embedding of the default dataset with the default hyperparameter settings. Saving the embedding  cluster centres and the log file at the default path. </p>  ```sh $ python src/main.py ```  Turning off the model saving.  ```sh $ python src/main.py --dump-matrices False ```  Creating an embedding of an other dataset the `Facebook Companies`. Saving the output and the log in a custom place.  ```sh $ python src/main.py --input data/company_edges.csv  --embedding-output output/embeddings/company_embedding.csv --cluster-mean-output output/cluster_means/company_means.csv ```  Creating a clustered embedding of the default dataset in 128 dimensions and 10 cluster centers.  ```sh $ python src/main.py --dimensions 128 --clusters 10 ``` -------------------------------------------------------------------  **License**  - [GNU](https://github.com/benedekrozemberczki/M-NMF/blob/master/LICENSE)  --------------------------------------------------------------------  """;Graphs;https://github.com/benedekrozemberczki/M-NMF
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/1wy/bert
"""You can use the models for sampling by entering   First download your data and put it into the `./data` folder.  To train a new model  first create a config script similar to the ones provided in the `./configs` folder.  You can then train you model using ``` python train.py PATH_TO_CONFIG ```  To compute the inception score for your model and generate samples  use ``` python test.py PATH_TO_CONFIG ```  Finally  you can create nice latent space interpolations using ``` python interpolate.py PATH_TO_CONFIG ``` or ``` python interpolate_class.py PATH_TO_CONFIG ```   """;General;https://github.com/LMescheder/GAN_stability
""" **BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.    In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant    See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/thanhlong1997/bert_quora
"""Dataset Readme: https://github.com/gulvarol/grocerydataset/blob/master/README.md       https://storage.googleapis.com/open_source_datasets/ShelfImages.tar.gz       https://github.com/gulvarol/grocerydataset   - Implements a Single Shot Detector  https://arxiv.org/abs/1512.02325     """;Computer Vision;https://github.com/arpytanshu/ssd-object-detection
"""Install the Python bindings for   python3 setup.py install in the efmaral directory).   If Theano and BNAS are installed  you should be able to simply run `hnmt.py`. Run with the `--help` argument to see the available command-line options.  Training a model on the Europarl corpus can be done like this:      python3 hnmt.py --source europarl-v7.sv-en.en \                     --target europarl-v7.sv-en.sv \                     --source-tokenizer word \                     --target-tokenizer char \                     --source-vocabulary 50000 \                     --max-source-length 30 \                     --max-target-length 180 \                     --batch-size 32 \                     --training-time 24 \                     --log en-sv.log \                     --save-model en-sv.model  This will create a model with a hybrid encoder (with 50k vocabulary size and character-level encoding for the rest) and character-based decoder  filtering out sentences longer than 30 words (source) or 180 characters (target) and training for 24 hours. Development set cross-entropy and some other statistics appended to this file  which is usually the best way of monitoring training. Training loss and development set translations will be written to stdout  so redirecting this or using `tee` is recommended.  The resulting model can be used like this:      python3 hnmt.py --load-model en-sv.model \                     --translate test.en --output test.sv \                     --beam-size 10  Note that when training a model from scratch  parameters can be set on the commandline or otherwise the hard-coded defaults are ued. When continuing training or doing translation (i.e. whenever the ``--load-model`` argument is used)  the defaults are encoded in the given model file  although some of these (that do not change the network structure) can still be overridden by commandline arguments.  For instance  the model above will assume that input files need to be tokenized  but passing a pre-tokenized (space-separated) input can be done as follows:      python3 hnmt.py --load-model en-sv.model \                     --translate test.en --output test.sv \                     --source-tokenizer space \                     --beam-size 10   """;General;https://github.com/Waino/hnmt
"""- Implementations of RL algorithms with tensorflow - Run in ""CartPole-v1"" environment.   """;Reinforcement Learning;https://github.com/InSpaceAI/RL-Zoo
"""CenterNet is a framework for object detection with deep convolutional neural networks. You can use the code to train and evaluate a network for object detection on the MS-COCO dataset.  * It achieves state-of-the-art performance (an AP of 47.0%) on one of the most challenging dataset: MS-COCO.  * Our code is written in Python  based on [CornerNet](https://github.com/princeton-vl/CornerNet).  *More detailed descriptions of our approach and code will be made available soon.*  **If you encounter any problems in using our code  please contact Kaiwen Duan: kaiwen.duan@vipl.ict.ac.cn.**   ``` cd <CenterNet dir>/data/coco/PythonAPI make ```   Please first install [Anaconda](https://anaconda.org) and create an Anaconda environment using the provided package list. ``` conda create --name CenterNet --file conda_packagelist.txt ```  After you create the environment  activate it. ``` source activate CenterNet ``` attention：dowmload [The suitable version](https://conda.anaconda.org/pytorch/linux-64/pytorch-1.0.0-py3.6_cuda9.0.176_cudnn7.4.1_1.tar.bz2) ``` conda install pytorch-1.0.0-py3.6_cuda9.0.176_cudnn7.4.1_1.tar.bz2 ```  python setup.py install --user   """;Computer Vision;https://github.com/DaiJianBo/CenterNet-duan-2080Ti
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-91-download-archive  OpenCV 3.3.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.3.0/opencv-3.3.0-vc14.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;General;https://github.com/tianhai123/yolov3
"""sh download_models.sh   """;Computer Vision;https://github.com/leongatys/PytorchNeuralStyleTransfer
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   Install <a href=""https://www.tensorflow.org/get_started/os_setup"" target=""_blank"">TensorFlow</a>. You may also need to install h5py. The code has been tested with Python 2.7  TensorFlow 1.0.1  CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04.  If you are using PyTorch  you can find a third-party pytorch implementation <a href=""https://github.com/fxia22/pointnet.pytorch"" target=""_blank"">here</a>.  To install h5py for Python: ```bash sudo apt-get install libhdf5-dev sudo pip install h5py ```   Created by <a href=""http://charlesrqi.com"" target=""_blank"">Charles R. Qi</a>  <a href=""http://ai.stanford.edu/~haosu/"" target=""_blank"">Hao Su</a>  <a href=""http://cs.stanford.edu/~kaichun/"" target=""_blank"">Kaichun Mo</a>  <a href=""http://geometry.stanford.edu/member/guibas/"" target=""_blank"">Leonidas J. Guibas</a> from Stanford University.  ![prediction example](https://github.com/charlesq34/pointnet/blob/master/doc/teaser.png)   cd part_seg  sh download_data.sh   To train a model to classify point clouds sampled from 3D shapes:      python train.py  Log files and network parameters will be saved to `log` folder in default. Point clouds of <a href=""http://modelnet.cs.princeton.edu/"" target=""_blank"">ModelNet40</a> models in HDF5 files will be automatically downloaded (416MB) to the data folder. Each point cloud contains 2048 points uniformly sampled from a shape surface. Each cloud is zero-mean and normalized into an unit sphere. There are also text files in `data/modelnet40_ply_hdf5_2048` specifying the ids of shapes in h5 files.  To see HELP for the training script:      python train.py -h  We can use TensorBoard to view the network architecture and monitor the training progress.      tensorboard --logdir log  After the above training  we can evaluate the model and output some visualizations of the error cases.      python evaluate.py --visu  Point clouds that are wrongly classified will be saved to `dump` folder in default. We visualize the point cloud by rendering it into three-view images.  If you'd like to prepare your own data  you can refer to some helper functions in `utils/data_prep_util.py` for saving and loading HDF5 files.   * <a href=""http://stanford.edu/~rqi/pointnet2/"" target=""_blank"">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a> by Qi et al. (NIPS 2017) A hierarchical feature learning framework on point clouds. The PointNet++ architecture applies PointNet recursively on a nested partitioning of the input point set. It also proposes novel layers for point clouds with non-uniform densities. * <a href=""http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w13/Engelmann_Exploring_Spatial_Context_ICCV_2017_paper.pdf"" target=""_blank"">Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</a> by Engelmann et al. (ICCV 2017 workshop). This work extends PointNet for large-scale scene segmentation. * <a href=""https://arxiv.org/abs/1710.04954"" target=""_blank"">PCPNET: Learning Local Shape Properties from Raw Point Clouds</a> by Guerrero et al. (arXiv). The work adapts PointNet for local geometric properties (e.g. normal and curvature) estimation in noisy point clouds. * <a href=""https://arxiv.org/abs/1711.06396"" target=""_blank"">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a> by Zhou et al. from Apple (arXiv) This work studies 3D object detection using LiDAR point clouds. It splits space into voxels  use PointNet to learn local voxel features and then use 3D CNN for region proposal  object classification and 3D bounding box estimation. * <a href=""https://arxiv.org/abs/1711.08488"" target=""_blank"">Frustum PointNets for 3D Object Detection from RGB-D Data</a> by Qi et al. (arXiv) A novel framework for 3D object detection with RGB-D data. The method proposed has achieved first place on KITTI 3D object detection benchmark on all categories (last checked on 11/30/2017).  """;Computer Vision;https://github.com/bt77/pointnet
"""```python #: Box is a nice wrapper to create an object from a json dict args = Box({     ""seed"": 42      ""task_name"": 'imdb_reviews_lm'      ""model_name"": 'roberta-base'      ""model_type"": 'roberta'      ""train_batch_size"": 16      ""learning_rate"": 4e-5      ""num_train_epochs"": 20      ""fp16"": True      ""fp16_opt_level"": ""O2""      ""warmup_steps"": 1000      ""logging_steps"": 0      ""max_seq_length"": 512      ""multi_gpu"": True if torch.cuda.device_count() > 1 else False })  DATA_PATH = Path('../lm_data/') LOG_PATH = Path('../logs') MODEL_PATH = Path('../lm_model_{}/'.format(args.model_type))  DATA_PATH.mkdir(exist_ok=True) MODEL_PATH.mkdir(exist_ok=True) LOG_PATH.mkdir(exist_ok=True)   ```   This repo is tested on Python 3.6+.   PyTorch-Transformers can be installed by pip as follows:  pip install fast-bert   pip install [--editable] .   pip install git+https://github.com/kaushaltrivedi/fast-bert.git  You will also need to install NVIDIA Apex.  git clone https://github.com/NVIDIA/apex  cd apex   """;General;https://github.com/utterworks/fast-bert
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/rolux/stylegan2tpu
"""If you want to use OpenPose without installing or writing any code  simply [download and use the latest Windows portable version of OpenPose](doc/installation/0_index.md#windows-portable-demo)!  Otherwise  you could [build OpenPose from source](doc/installation/0_index.md#compiling-and-running-openpose-from-source). See the [installation doc](doc/installation/0_index.md) for all the alternatives.     OS: Ubuntu (20  18  16  14)  Windows (10  8)  Mac OSX  Nvidia TX2.  Hardware compatibility: CUDA (Nvidia GPU)  OpenCL (AMD GPU)  and non-GPU (CPU-only) versions.   Simply use the OpenPose Demo from your favorite command-line tool (e.g.  Windows PowerShell or Ubuntu Terminal). E.g.  this example runs OpenPose on your webcam and displays the body keypoints: ``` #: Ubuntu ./build/examples/openpose/openpose.bin ``` ``` :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi ```  You can also add any of the available flags in any order. E.g.  the following example runs on a video (`--video {PATH}`)  enables face (`--face`) and hands (`--hand`)  and saves the output keypoints on JSON files on disk (`--write_json {PATH}`). ``` #: Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi --face --hand --write_json output_json_folder/ ``` ``` :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi --face --hand --write_json output_json_folder/ ```  Optionally  you can also extend OpenPose's functionality from its Python and C++ APIs. After [installing](doc/installation/0_index.md) OpenPose  check its [official doc](doc/00_index.md) for a quick overview of all the alternatives and tutorials.     """;General;https://github.com/CMU-Perceptual-Computing-Lab/openpose
"""Just run `pip install`.  ``` $pip install git+https://github.com/Guillem96/efficientdet-tf ```   Installation 💿   $ python -m efficientdet.train --help     --save-dir DIRECTORY            Directory to save model weights  [required]   The command below is the one that we should use if we want to train the model with   $ python -m efficientdet.train \     --test-dataset DIRECTORY        Path to annotations and images  [required]    --images-path DIRECTORY         Base path to images. Required when using     --checkpoint PATH               Path to model checkpoint  [required]   """;General;https://github.com/Guillem96/efficientdet-tf
"""Inspired by the structure of Receptive Fields (RFs) in human visual systems  we propose a novel RF Block (RFB) module  which takes the relationship between the size and eccentricity of RFs into account  to enhance the discriminability and robustness of features. We further  assemble the RFB module to the top of SSD with a lightweight CNN model  constructing the RFB Net detector. You can use the code to train/evaluate the RFB Net for object detection. For more details  please refer to our [arXiv paper](https://arxiv.org/pdf/1711.07767.pdf).   <img align=""right"" src=""https://github.com/ruinmessi/RFBNet/blob/master/doc/rfb.png"">  &nbsp; &nbsp;   - Install [PyTorch-0.2.0](http://pytorch.org/) by selecting your environment on the website and running the appropriate command. - Clone this repository. This repository is mainly based on [ssd.pytorch](https://github.com/amdegroot/ssd.pytorch) and [Chainer-ssd](https://github.com/Hakuyume/chainer-ssd)  a huge thank to them.   * Note: We currently only support Python 3+. - Compile the nms and coco tools: ```Shell ./make.sh ``` *Note*: Check you GPU architecture support in utils/build.py  line 131. Default is: ```  'nvcc': ['-arch=sm_52'  ``` - Install [pyinn](https://github.com/szagoruyko/pyinn) for MobileNet backbone: ```Shell pip install git+https://github.com/szagoruyko/pyinn.git@master ``` - Then download the dataset by following the [instructions](#download-voc2007-trainval--test) below and install opencv.  ```Shell conda install opencv ``` Note: For training  we currently  support [VOC](http://host.robots.ox.ac.uk/pascal/VOC/) and [COCO](http://mscoco.org/).    Install the MS COCO dataset at /path/to/coco from official website  default is ~/data/COCO. Following the instructions to prepare minival2014 and valminusminival2014 annotations. All label files (.json) should be under the COCO/annotations/ folder. It should have this basic structure   First download the fc-reduced VGG-16 PyTorch base network weights at:    https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth   mkdir weights  cd weights   -v: choose backbone version  RFB_VGG  RFB_E_VGG or RFB_mobile.   """;General;https://github.com/dishen12/py03
"""GraphSage:  https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf (https://github.com/williamleif/GraphSAGE)</br>   """;Graphs;https://github.com/printfer/CS6490_NetworkSecurity_FinalProject
"""This repository build on the work of https://github.com/tkipf/gae  (N.  Thomas  and Max. “Variational Graph Auto-Encoders.” ArXiv.org  21 Nov. 2016  arxiv.org/abs/1611.07308.) to learn representations of scientific citation data into a latent space. If latent space is then a representation of the the underlying *information* in the dataset  investigating the latent space and their correlations with features  we can begin to understand how the features interact with each other in a complex networks - such as the scientific citation network.   """;Graphs;https://github.com/Omairss/RepresentationLearning
"""            ""both""                ""both""    """;Natural Language Processing;https://github.com/kamalkraj/BERT-SQuAD
"""In the directory `pytorch/`  we provide a PyTorch-based implementation of PrRoI Pooling. It requires PyTorch 1.0+ and only supports CUDA (CPU mode is not implemented). Since we use PyTorch JIT for cxx/cuda code compilation  to use the module in your code  simply do:  ``` from prroi_pool import PrRoIPool2D  avg_pool = PrRoIPool2D(window_height  window_width  spatial_scale) roi_features = avg_pool(features  rois)  #: for those who want to use the ""functional""  from prroi_pool.functional import prroi_pool2d roi_features = prroi_pool2d(features  rois  window_height  window_width  spatial_scale) ```    **!!! Please first checkout to the branch pytorch0.4.**  In the directory `pytorch/`  we provide a PyTorch-based implementation of PrRoI Pooling. It requires PyTorch 0.4 and only supports CUDA (CPU mode is not implemented). To use the PrRoI Pooling module  first goto `pytorch/prroi_pool` and execute `./travis.sh` to compile the essential components (you may need `nvcc` for this step). To use the module in your code  simply do:  ``` from prroi_pool import PrRoIPool2D  avg_pool = PrRoIPool2D(window_height  window_width  spatial_scale) roi_features = avg_pool(features  rois)  #: for those who want to use the ""functional""  from prroi_pool.functional import prroi_pool2d roi_features = prroi_pool2d(features  rois  window_height  window_width  spatial_scale) ```  Here   - RoI is an `m * 5` float tensor of format `(batch_index  x0  y0  x1  y1)`  following the convention in the original Caffe implementation of RoI Pooling  although in some frameworks the batch indices are provided by an integer tensor. - `spatial_scale` is multiplied to the RoIs. For example  if your feature maps are down-sampled by a factor of 16 (w.r.t. the input image)  you should use a spatial scale of `1/16`. - The coordinates for RoI follows the [L  R) convension. That is  `(0  0  4  4)` denotes a box of size `4x4`.  """;Computer Vision;https://github.com/Qinhj07/ATOMCode
""" - pytorch 0.4  - tensorboardX (using tensorboard with pytorch  if you do not want to use tensorboard  set USEBOARD as False)  - soundfile  - h5py  - numpy  - you can choose one between these two.   if you only use musdb18  you should have 50.wav  51.wav  to 199.wav.    - trainForRandomGen.py (use ccmixter and musdb as dataset to train model)  - trainchinese.py (use chinese songs as dataset to train model)  - trainclassify.py (use classification instead regression  classification can also generalize as good as regression but much more noise)  """;Computer Vision;https://github.com/ShichengChen/WaveUNet
"""In order to install darknet_ros  clone the latest version using SSH (see how to set up an SSH key) from this repository into your catkin workspace and compile the package using ROS.  cd catkin_workspace/src  git clone --recursive https://github.com/AkellaSummerResearch/darknet_ros.git  cd ../  To maximize performance  make sure to build in Release mode. You can specify the build type by setting   catkin build darknet_ros -DCMAKE_BUILD_TYPE=Release  Darknet on the CPU is fast (approximately 1.5 seconds on an Intel Core i7-6700HQ CPU @ 2.60GHz × 8) but it's like 500 times faster on GPU! You'll have to have an Nvidia GPU and you'll have to install CUDA. The CMakeLists.txt file automatically detects if you have CUDA installed or not. CUDA is a parallel computing platform and application programming interface (API) model created by Nvidia. If you do not have CUDA on your System the build process will switch to the CPU version of YOLO. If you are compiling with CUDA  you might receive the following build error:  nvcc fatal : Unsupported gpu architecture 'compute_61'.  This means that you need to check the compute capability (version) of your GPU. You can find a list of supported GPUs in CUDA here: CUDA - WIKIPEDIA. Simply find the compute capability of your GPU and add it into darknet_ros/CMakeLists.txt. Simply add a similar line like   You will see the image above popping up.   yolo_model/config_file/name (string)   yolo_model/weight_file/name (string)   In order to use your own detection objects you need to provide your weights and your cfg file inside the directories:      catkin_workspace/src/darknet_ros/darknet_ros/yolo_network_config/weights/     catkin_workspace/src/darknet_ros/darknet_ros/yolo_network_config/cfg/  In addition  you need to create your config file for ROS where you define the names of the detection objects. You need to include it inside:      catkin_workspace/src/darknet_ros/darknet_ros/config/  Then in the launch file you have to point to your new config file in the line:      <rosparam command=""load"" ns=""darknet_ros"" file=""$(find darknet_ros)/config/your_config_file.yaml""/>   In order to get YOLO ROS: Real-Time Object Detection for ROS to run with your robot  you will need to adapt a few parameters. It is the easiest if duplicate and adapt all the parameter files that you need to change from the `darkned_ros` package. These are specifically the parameter files in `config` and the launch file from the `launch` folder.   """;Computer Vision;https://github.com/softbankrobotics-research/darknet_ros
"""This repo uses [*Simple Baselines*](http://openaccess.thecvf.com/content_ECCV_2018/html/Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper.html) as the baseline method for Pose Estimation.   [Res2Net](https://github.com/gasvn/Res2Net) is a powerful backbone architecture that can be easily implemented into state-of-the-art models by replacing the bottleneck with Res2Net module. More detail can be found on [ ""Res2Net: A New Multi-scale Backbone Architecture""](https://arxiv.org/pdf/1904.01169.pdf)   **For MPII data**  please download from [MPII Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/). The original annotation files are in matlab format. We have converted them into json format  you also need to download them from [OneDrive](https://1drv.ms/f/s!AhIXJn_J-blW00SqrairNetmeVu4) or [GoogleDrive](https://drive.google.com/drive/folders/1En_VqmStnsXMdldXA6qpqEyDQulnmS3a?usp=sharing). Extract them under {POSE_ROOT}/data  and make them look like this: ``` ${POSE_ROOT} |-- data `-- |-- mpii     `-- |-- annot         |   |-- gt_valid.mat         |   |-- test.json         |   |-- train.json         |   |-- trainval.json         |   `-- valid.json         `-- images             |-- 000001163.jpg             |-- 000003072.jpg ```  **For COCO data**  please download from [COCO download](http://cocodataset.org/#download)  2017 Train/Val is needed for COCO keypoints training and validation. We also provide person detection result of COCO val2017 and test-dev2017 to reproduce our multi-person pose estimation results. Please download from [OneDrive](https://1drv.ms/f/s!AhIXJn_J-blWzzDXoz5BeFl8sWM-) or [GoogleDrive](https://drive.google.com/drive/folders/1fRUDNUDxe9fjqcRZ2bnF_TKMlO0nB_dk?usp=sharing). Download and extract them under {POSE_ROOT}/data  and make them look like this: ``` ${POSE_ROOT} |-- data `-- |-- coco     `-- |-- annotations         |   |-- person_keypoints_train2017.json         |   `-- person_keypoints_val2017.json         |-- person_detection_results         |   |-- COCO_val2017_detections_AP_H_56_person.json         |   |-- COCO_test-dev2017_detections_AP_H_609_person.json         `-- images             |-- train2017             |   |-- 000000000009.jpg             |   |-- 000000000025.jpg             |   |-- 000000000030.jpg             |   |-- ...              `-- val2017                 |-- 000000000139.jpg                 |-- 000000000285.jpg                 |-- 000000000632.jpg                 |-- ...  ```   6. Init output(training model output directory) and log(tensorboard log directory) directory:     ```    mkdir output     mkdir log    ```     Your directory tree should look like this:     ```    ${POSE_ROOT}    ├── data    ├── experiments    ├── lib    ├── log    ├── models    ├── output    ├── tools     ├── README.md    └── requirements.txt    ```  7. Download pretrained models of Res2Net following the instruction from [Res2Net backbone pretrained models](https://github.com/gasvn/Res2Net). Please change the path to pretrained models **(PRETRAINED: )** in config files:  `experiments/coco/res2net/res2net50_4s_26w_256x192_d256x3_adam_lr1e-3.yaml`    ```    ${POSE_ROOT}     `-- models         `-- pytorch             |-- imagenet             |   |-- res2net50_26w_4s-06e79181.pth             |   |-- res2net101_26w_4s-02a759a1.pth             |   |-- resnet50-19c8e357.pth             |   |-- resnet101-5d3b4d8f.pth             |   `-- resnet152-b121ed2d.pth             |-- pose_coco             |   |-- (pretrained model for res2net_pose will be soon available)             |   |-- pose_resnet_101_256x192.pth             |   |-- pose_resnet_101_384x288.pth             |   |-- pose_resnet_152_256x192.pth             |   |-- pose_resnet_152_384x288.pth             |   |-- pose_resnet_50_256x192.pth             |   `-- pose_resnet_50_384x288.pth             `-- pose_mpii                 |-- pose_resnet_101_256x256.pth                 |-- pose_resnet_152_256x256.pth                 `-- pose_resnet_50_256x256.pth     ```      1. Install pytorch >= v1.0.0  2. Clone this repo  and we'll call the directory that you cloned as ${POSE_ROOT}. 3. Install dependencies:    ```    pip install -r requirements.txt    ``` 4. Make libs:    ```    cd ${POSE_ROOT}/lib    make    ``` 5. Install [COCOAPI](https://github.com/cocodataset/cocoapi):    ```    #: COCOAPI=/path/to/clone/cocoapi    git clone https://github.com/cocodataset/cocoapi.git $COCOAPI    cd $COCOAPI/PythonAPI    #: Install into global site-packages    make install    #: Alternatively  if you do not have permissions or prefer    #: not to install the COCO API into global site-packages    python3 setup.py install --user    ```    Note that instructions like      --cfg experiments/coco/res2net/res2net50_4s_26w_256x192_d256x3_adam_lr1e-3.yaml \       --cfg experiments/coco/res2net/res2net50_4s_26w_256x192_d256x3_adam_lr1e-3.yaml   """;Computer Vision;https://github.com/Res2Net/Res2Net-Pose-Estimation
"""The datasets are taken from the original MAF repository. Follow the instructions to get them.  Tests check invertibility  you can run them as   """;Computer Vision;https://github.com/ikostrikov/pytorch-flows
"""- Create an Anaconda environment with Python3.6 ``` conda create -n instahide python=3.6 ``` - Run the following command to install dependencies ``` conda activate instahide pip install -r requirements.txt ```  """;Computer Vision;https://github.com/Hazelsuko07/InstaHide
"""recurrentshop下载地址: https://github.com/farizrahman4u/recurrentshop    seq2seq 下载地址: https://github.com/farizrahman4u/seq2seq         私人地址: 链接: https://pan.baidu.com/s/1g6l4_IDkLdLAjvrWf5sheQ 密码: fxy3   seq2seq源码: https://github.com/farizrahman4u/seq2seq    seq2seq源码需求: https://github.com/farizrahman4u/recurrentshop    beamsearch源码参考: https://github.com/yanwii/seq2seq    bucket源码参考: https://github.com/1228337123/tensorflow-seq2seq-chatbot-zh   """;Natural Language Processing;https://github.com/EuphoriaYan/ChatRobot-For-Keras2
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/zapplea/bert
"""```bash $ pip install slot_attention ```   ```python import torch from slot_attention import SlotAttention  slot_attn = SlotAttention(     num_slots = 5      dim = 512      iters = 3   #: iterations of attention  defaults to 3 )  inputs = torch.randn(2  1024  512) slot_attn(inputs) #: (2  5  512) ```  After training  the network is reported to be able to generalize to slightly different number of slots (clusters). You can override the number of slots used by the `num_slots` keyword in forward.  ```python slot_attn(inputs  num_slots = 8) #: (2  8  512) ```   """;General;https://github.com/lucidrains/slot-attention
"""OS version: Ubuntu 18.04  NVIDIA diver version: 465.27  Cuda version: 11.3  Cudnn version:   Python version: 3.6.9  Python packages installation:   pip3 install -i https://mirrors.aliyun.com/pypi/simple  -r requirements.txt  sh run/train_example.sh  sh run/test.sh   pytorch-image-models: https://github.com/rwightman/pytorch-image-models  Vit-Pytorch: https://github.com/lucidrains/vit-pytorch    DeiT: https://github.com/facebookresearch/deit  Swin-Transformer: https://github.com/microsoft/Swin-Transformer   """;Computer Vision;https://github.com/holdfire/CLS
"""        ""source"": ""WikiTableQuestions_lily""    """;Natural Language Processing;https://github.com/Yale-LILY/dart
"""This whole project was made in Google Colab  so Please use [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb) nootebook in order to make this operantion work  other wise you can check our install.sh an change the paths to your liking  but I wouldn't suggest doing so... - First of all make sure that you are using *GPU* enviroment. - Than open the file content : <code> cd /content </code> - Clone the git repository: <code> !git clone https://github.com/Vakihito/SentimentYoutube.git </code>  - Open the file SentimentYoutube : <code> cd SentimentYoutube/ </code>  - Install packages requirements: <code> !pip3 install -r requirements.txt </code>  - Install enviroment requirements: <code> !./install.sh </code>  - Open the enviroment file : <code> cd /content/vqa-maskrcnn-benchmark </code> - Thats all  now you are good to go !!!  ---   Please see the following tutorial notebooks for a guide on how to use **SentimentYoutube** on your projects:      - **Tutorial 1**: [Introduction](https://colab.research.google.com/drive/1JEmHlvQ2CFP6Zwdoi1TQyu_Fjzb7J2kW?usp=sharing)  ---  Please see the following examples notebooks for a guide on how to use **SentimentYoutube** on your projects:      - **How to use** : [Introduction](https://colab.research.google.com/drive/1JEmHlvQ2CFP6Zwdoi1TQyu_Fjzb7J2kW?usp=sharing) - **Using Explain** : [Explain Notebook](https://colab.research.google.com/drive/1EWV1Qss2IxqUReJe1GO4N0AyUviZGCeZ?usp=sharing) - **Example of the application** : [Video](https://studio.youtube.com/video/eWX21QAbqNE/edit)  ---   """;General;https://github.com/Vakihito/SentimentYoutube
"""This is a PyTorch implementation of the __L__ ow Rank F __a__ ctorization for Compact __M__ ulti-Head __A__ ttention (LAMA) mechanism and the corresponding pooler introduced in the paper: ""[Low Rank Factorization for Compact Multi-Head Self-Attention](https://arxiv.org/abs/1912.00835)"".  ![](img/figure_1.jpg)  > Figure 1 from [Low Rank Factorization for Compact Multi-Head Self-Attention](https://arxiv.org/abs/1912.00835).  Note: I am _not_ one of the authors on the paper.   from modules.lama import LAMA   from modules.lama_encoder import LAMAEncoder   The only dependency is PyTorch. Installation instructions can be found [here](https://pytorch.org/get-started/locally/).   """;General;https://github.com/JohnGiorgi/compact-multi-head-self-attention-pytorch
"""The codebase is tested on the following setting.   pip install git+https://github.com/moskomule/homura@v2020.07   : !wget https://github.com/moskomule/senet.pytorch/releases/download/archive/seresnet50-60a8950a85b2b.pkl   """;Computer Vision;https://github.com/moskomule/senet.pytorch
"""It is currently distributed as a source only PyTorch extension. So you need a propely set up toolchain and CUDA compilers to install. 1) _Toolchain_ - In conda the `cxx_linux-64` package provides an appropriate toolchain. However there can still be compatbility issues with this depending on system. You can also try with the system toolchian. 2) _CUDA Toolkit_ - The [nVidia CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) is required in addition to drivers to provide needed headers and tools. Get the appropriate version for your Linux distro from nVidia or check for distro specific instructions otherwise.  _It is important your CUDA Toolkit matches the version PyTorch is built for or errors can occur. Currently PyTorch builds for v10.0 and v9.2._   This is a PyTorch CUDA implementation of the Mish activation by Diganta Misra (https://github.com/digantamisra98/).   """;General;https://github.com/thomasbrandon/mish-cuda
"""minimal requirements  can be installed with pip in a python3 virtualenv (pip install -r requirements.txt)   """;Audio;https://github.com/adrienchaton/BERGAN
"""Before running any of the code  be sure to install the requirements.  Links to the datasets are below.  python -m virtualenv env  source env/bin/activate  python -m pip install -r requirements.txt   Links from the lecture slides are listed below  under the name of the lecture they are from.   Maestro MIDI Dataset:  https://magenta.tensorflow.org/datasets/maestro   ECG dataset from Kaggle:  https://www.kaggle.com/shayanfazeli/heartbeat   """;Natural Language Processing;https://github.com/nlinc1905/dsilt-tsa
"""  Using pip:   .. code:: bash    pip install git+https://github.com/petko-nikolov/pysemseg           """;Computer Vision;https://github.com/petko-nikolov/pysemseg
"""**Windows portable version**: Simply download and use the latest version from the [Releases](https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases) section.  Otherwise  check [doc/installation.md](doc/installation.md) for instructions on how to build OpenPose from source.     OS: Ubuntu (14  16)  Windows (8  10)  Mac OSX  Nvidia TX2.   CUDA (Nvidia GPU)  OpenCL (AMD GPU)  and CPU-only (no GPU) versions.   Most users do not need the OpenPose C++/Python API  but can simply use the OpenPose Demo:  - **OpenPose Demo**: To easily process images/video/webcam and display/save the results. See [doc/demo_overview.md](doc/demo_overview.md). E.g.  run OpenPose in a video with: ``` #: Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi ```  - **Calibration toolbox**: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See [doc/modules/calibration_module.md](doc/modules/calibration_module.md).  - **OpenPose C++ API**: If you want to read a specific input  and/or add your custom post-processing function  and/or implement your own display/saving  check the C++ API tutorial on [examples/tutorial_api_cpp/](examples/tutorial_api_cpp/) and [doc/library_introduction.md](doc/library_introduction.md). You can create your custom code on [examples/user_code/](examples/user_code/) and quickly compile it with CMake when compiling the whole OpenPose project. Quickly **add your custom code**: See [examples/user_code/README.md](examples/user_code/README.md) for further details.  - **OpenPose Python API**: Analogously to the C++ API  find the tutorial for the Python API on [examples/tutorial_api_python/](examples/tutorial_api_python/).  - **Adding an extra module**: Check [doc/library_add_new_module.md](./doc/library_add_new_module.md).  - **Standalone face or hand detector**:     - **Face** keypoint detection **without body** keypoint detection: If you want to speed it up (but also reduce amount of detected faces)  check the OpenCV-face-detector approach in [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     - **Use your own face/hand detector**: You can use the hand and/or face keypoint detectors with your own face or hand detectors  rather than using the body detector. E.g.  useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     """;General;https://github.com/xaoch/rapJetson2
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/kiko441500/google_bert
"""* base docker environment: https://hub.docker.com/r/evariste/autodl  * pre requirements ```bash $ apt update $ apt install python3-tk ```  * clone and init. the repository ```bash $ git clone https://github.com/kakaobrain/autoclint.git && cd autoclint $ #: 3rd parties libarary $ git submodule init $ git submodule update $ #: download pretrained models $ wget https://download.pytorch.org/models/resnet18-5c106cde.pth -O ./models/resnet18-5c106cde.pth $ #: download public datasets $ cd autodl && python download_public_datasets.py && cd .. ```  * run public datasets ```bash $ python autodl/run_local_test.py -time_budget=1200 -code_dir='./' -dataset_dir='autodl/AutoDL_public_data/Munster/'; cp autodl/AutoDL_scoring_output/learning-curve-*.png ./results $ python autodl/run_local_test.py -time_budget=1200 -code_dir='./' -dataset_dir='autodl/AutoDL_public_data/Chucky/'; cp autodl/AutoDL_scoring_output/learning-curve-*.png ./results $ python autodl/run_local_test.py -time_budget=1200 -code_dir='./' -dataset_dir='autodl/AutoDL_public_data/Pedro/'; cp autodl/AutoDL_scoring_output/learning-curve-*.png ./results $ python autodl/run_local_test.py -time_budget=1200 -code_dir='./' -dataset_dir='autodl/AutoDL_public_data/Decal/'; cp autodl/AutoDL_scoring_output/learning-curve-*.png ./results $ python autodl/run_local_test.py -time_budget=1200 -code_dir='./' -dataset_dir='autodl/AutoDL_public_data/Hammer/'; cp autodl/AutoDL_scoring_output/learning-curve-*.png ./results ```  * (optional) display learning curve ```bash $ #: item2 utils to visualize learning curve $ wget https://www.iterm2.com/utilities/imgcat -O bin/imgcat; chmod 0677 bin/imgcat $ bin/imgcat ./results/learning-curve-*.png ```   experiment environment: CodaLab (UNKNOWN)   """;Computer Vision;https://github.com/songyadong106/111
"""Training of this magnitude definitely needed some beefed up hardware and since I'm a console guy (PS4)  I resorted to the EC instances Amazon provides (https://aws.amazon.com/ec2/instance-types/). Udacity's Amazon credits came in handy!  At first  I tried the g2.xlarge instance that Udacity's project on Traffic sign classifier had suggested (did that on my laptop back then) but the memory or the compute capability was nowhere near sufficient  since TF apparently drops to CPU and RAM after detecting that there isn't sufficient capacity on the GPU.  In the end  p2.xlarge EC2 instance were what I trained my network on. There was ~10GB GPU memory utilization and ~92% GPU at peak. My network trained pretty well on this setup.  NOTE: I faced a lot of issues when getting setup on the remote instance due to issues with certain libraries being out of date and anaconda not having those updates. Luckily Amazon released its latest (v6 at time) deep learning Ubuntu AMI which worked just fine out of the box. So if you are using EC2  make sure to test sample code and library imports in python first to make sure the platform is ready for your code.      """;General;https://github.com/makatx/YOLO_ResNet
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;General;https://github.com/qilei123/maskrcnn_benchmark_ROP
"""tensorboard --logdir=[SAVE PATH] --port=[YOUR PORT]   After unzip the checkpoints into your own path  you can run ``` python eval.py --load_path saved_models/cifar10_400/model_best.pth --dataset cifar10 --num_classes 10 ```   ``` python train.py --rank 0 --gpu [0/1/...] @@@other args@@@ ```   ``` python train.py --world-size 1 --rank 0 @@@other args@@@ ```   **With V100x4 GPUs  CIFAR10 training takes about 16 hours (0.7 days)  and CIFAR100 training takes about 62 hours (2.6 days).**  - single node   ``` python train.py --world-size 1 --rank 0 --multiprocessing-distributed @@@other args@@@ ```  - multiple nodes (assuming two nodes) ``` #: at node 0 python train.py --world-size 2 --rank 0 --dist_url [rank 0's url] --multiprocessing-distributed @@@@other args@@@@ #: at node 1 python train.py --world-size 2 --rank 1 --dist_url [rank 0's url] --multiprocessing-distributed @@@@other args@@@@ ```   """;General;https://github.com/LeeDoYup/FixMatch-pytorch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/thecodemasterk/BERT
"""Create a Python 3 virtual environment and activate: ``` virtualenv -p python3 env source ./env/bin/activate ``` Install requirements by running: ``` pip install -r requirements.txt ``` Then export project to python path: ``` export PYTHONPATH=$PATH_TO_REPO/performer ``` To test the scripts  run `pytest` in the root directory  you may wish to install `pytest` separately   `Performer` inherites from a lightly modified version of tf-nightly's `MultiHeadAttention` and is made to be fully compatible with the parents' use cases  with added flexibility for performing attention in linear time and space complexity. Currently masked attention is not supported. ```python from performer.networks.linear_attention import Performer  layer = Performer(num_heads=2  #: Number of attention heads                   key_dim=2  #: Size of each attention head for query and key                   attention_method='linear'  #: attention method  'linear' or 'quadratic' 		  supports=2  #: only used in 'linear' attention  number of random features 		  attention_axes=None #: axes over which the attention is applied. 		  ) query = tf.keras.Input(shape=[8  16]) key = tf.keras.Input(shape=[4  16]) output_tensor = layer([query  key]) print(output_tensor.shape) #: (None  8  16) ```  `Performer` supports attention in any arbituary axis  below is an example of 2D self-attention over a 5D input tensor on axes 2 and 3.  ```python layer = Performer(num_heads=2  key_dim=2  attention_method='linear'                    supports=10  attention_axes=(2  3)) input_tensor = tf.keras.Input(shape=[5  3  4  16]) output_tensor = layer([input_tensor  input_tensor]) print(output_tensor.shape) #: (None  5  3  4  16) ```     """;General;https://github.com/xl402/performer
"""```bash #: Ubuntu 18.04 sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt install nvidia-driver-430 #: Windows/Other https://www.nvidia.com/Download/index.aspx ```  conda env create -f conda-cpu.yml  conda activate tracker-cpu  : Tensorflow GPU  conda env create -f conda-gpu.yml  conda activate tracker-gpu   pip install -r requirements.txt  : TensorFlow GPU  pip install -r requirements-gpu.txt   ![Demo of Object Tracker](data/helpers/demo.gif)   This is a demo of running the object tracker using the above command for running the object tracker on your webcam. ![Webcam Demo](data/helpers/webcam_demo.gif)   """;Computer Vision;https://github.com/theAIGuysCode/yolov3_deepsort
"""Place any image dataset with ImageNet-style directory structure (at least 1 subfolder) to fit the dataset into pytorch ImageFolder.   for TPU VM with Pytorch Lightning   [ ] Change current DALL-E code to fully support latest updates from DALLE-pytorch   [x] Resolve SIGSEGV issue with large TPU Pods pytorch-xla #3028   [x] Resolve SIGSEGV issue with large TPU Pods pytorch-xla #3068   """;Computer Vision;https://github.com/tgisaturday/dalle-lightning
"""--gc_cardinality=377 is required   """;Audio;https://github.com/ibab/tensorflow-wavenet
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;General;https://github.com/BIYTC/mobilenet_maskrcnn
"""vedaseg is an open source semantic segmentation toolbox based on PyTorch.   1. Create a conda virtual environment and activate it.  ```shell conda create -n vedaseg python=3.6.9 -y conda activate vedaseg ```  2. Install PyTorch and torchvision following the [official instructions](https://pytorch.org/)  *e.g.*   ```shell conda install pytorch torchvision -c pytorch ```  3. Clone the vedaseg repository.  ```shell git clone https://github.com/Media-Smart/vedaseg.git cd vedaseg vedaseg_root=${PWD} ```  4. Install dependencies.  ```shell pip install -r requirements.txt ```   cd ${vedaseg_root}   cd ${vedaseg_root}/data   Download the COCO-2017 dataset.   cd ${vedaseg_root}   cd ${vedaseg_root}/data  mkdir COCO2017 &amp;&amp; cd COCO2017   Firstly  install volksdep following the official instructions.   """;Computer Vision;https://github.com/Media-Smart/vedaseg
"""For the experiments  the following losses are also implemented:   Training LeNet on MNIST using cross-entropy loss and no label corruption: ``` python3 train.py dataset=mnist model=lenet loss=ce dataset.train.corrupt_prob=0.0 ```  Training a ResNet-50 on CIFAR-10 using the partially Huberised cross-entropy loss (PHuber-CE) with τ=2  and label corruption probability ρ of 0.2:  ``` python3 train.py dataset=cifar10 model=resnet50 loss=phuber_ce loss.tau=2 dataset.train.corrupt_prob=0.2 ```  Training a ResNet-50 on CIFAR-100 using the Generalized Cross Entropy loss (GCE) and label corruption probability ρ of 0.6  with [mixed precision](https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/):  ``` python3 train.py dataset=cifar100 model=resnet50 loss=gce dataset.train.corrupt_prob=0.6 mixed_precision=true ```   Training LeNet on MNIST using cross-entropy loss  and varying label corruption probability ρ (0.0  0.2  0.4 and 0.6). This uses [Hydra's multi-run flag](https://hydra.cc/docs/tutorials/basic/running_your_app/multi-run) for parameter sweeps:  ``` python3 train.py --multirun dataset=mnist model=lenet loss=ce dataset.train.corrupt_prob=0.0 0.2 0.4 0.6 ```   """;General;https://github.com/dmizr/phuber
"""ResNet50 based examples are included. Example scripts are included under ```./scripts/``` directory. ImageNet data should be included under ```./data/ImageNet/``` with foler named ```train``` and ```val```.  ``` #: To train with BAM (ResNet50 backbone) python train_imagenet.py --ngpu 4 --workers 20 --arch resnet --depth 50 --epochs 100 --batch-size 256 --lr 0.1 --att-type BAM --prefix RESNET50_IMAGENET_BAM ./data/ImageNet #: To train with CBAM (ResNet50 backbone) python train_imagenet.py --ngpu 4 --workers 20 --arch resnet --depth 50 --epochs 100 --batch-size 256 --lr 0.1 --att-type CBAM --prefix RESNET50_IMAGENET_CBAM ./data/ImageNet ```   """;General;https://github.com/Jongchan/attention-module
"""Install PyTorch 0.2   For data setup  run ./getdata.sh.   PyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed.   """;Natural Language Processing;https://github.com/jhave/RERITES-AvgWeightDescentLSTM-PoetryGeneration
"""First  clone the codes into your catkin workspace. You can skip `dxslam` or `ORB_SLAM2` if only needing the other. ``` cd YOUR_CATKIN_WS/src/ git clone https://github.com/cedrusx/dxslam_ros git clone https://github.com/cedrusx/deep_features git clone https://github.com/ivipsourcecode/dxslam git clone https://github.com/cedrusx/ORB_SLAM2  #: this is a fork of raulmur/ORB_SLAM2 with a few bug fixings ```  If you already have the `dxslam` or `ORB_SLAM2` repo in your system and do not want to clone again  you can simply put them into the `dxslam_ros` folder  or alongside your catkin workspace. Then `dxslam_ros` would find it during compiling. See the `SLAM_SEARCH_PATH` in [`CMakeLists.txt`](https://github.com/cedrusx/dxslam_ros/blob/dxslam/CMakeLists.txt).  Then  build `dxslam` and/or `ORB_SLAM2` as normal. Please check the README of these repos to set up the prerequisites. ``` cd dxslam ./build.sh cd ..  cd ORB_SLAM2 ./build.sh cd .. ```  If you use `dxslam`  you'll also need to set up ROS+Python3 environment for `deep_features`. Check its [README](https://github.com/cedrusx/deep_features).  Finally  build `dxslam_ros` with your favorate catkin tool: ``` cd YOUR_CATKIN_WS . /opt/ros/VERSION/setup.bash catkin build    #: OR catkin_make ```   """;Computer Vision;https://github.com/cedrusx/dxslam_ros
"""logits->[B H  W  1]  """;General;https://github.com/GXYM/Focal-loss
"""The idea of this notebook is to investigate what Lime allows to do and how it can be used. There is also a small description of how Lime works in the vincity of a specific sample.   """;General;https://github.com/LaurentLava/Lime
"""* **What:**    * The authors propose an alternative generator architecture for generative adversarial    networks  borrowing from style transfer literature    * The new generator improves the state-of-the-art in terms    of traditional distribution quality metrics   * The authors propose two new  automated methods to quantify interpolation quality    and disentanglement  that are applicable to any generator architecture   * The authors introduce a new  highly varied and high-quality dataset of    human faces The (FFHQ  Flickr-Faces-HQ).    * The authors proposed the way to control different features of the generated image and made   the model more explainable. See theirs official video (https://www.youtube.com/watch?v=kSLJriaOumA&feature=youtu.be) * **Method:**  The idea of creating Mapping Network with the transformed/inner latent space is used  instead of using input  latent code. This mapping part consists of 8 MLP blocks(8 gave the best performance)  which gives __new__ `W-space`  latent code  that is passed to the __Generator__ as a style which consists of 18 conv layers(two for each resolution 4 ** 2 - 1024 ** 2).  See [#Architecture] figure.    Through the experimental process it was found out that starting image at the top of `g-Generator` can be learn   constant tensor instead of a real image. Following the architecture this image then in each block   follows the next process. First conv layer. Then summed with _per channel scaled  noise_ (scaling is learned as B-parameter). Then again conv. Then AdaIN block.    AdaIN Block can be described the next way:  ![AdaIN](assets/adain.png)  where `x_i` is input per channel feature passed vertically from top and `(y_s_i  y_b_i)` are _style_ parameters  got from transformation of `w-vector` from input `W` latent space. To get these style parameters this vector is   first passed through learned affine transformation.       **To sum it all up**  the main interesting points are: the input `4x4x512` tensor is always the same and we   only get styles from our latent variable. Moreover they say that at each scale changing our _style_ inputs   we can change different **very** localised parts of generated image (See video).      > Copying the styles corresponding to coarse spatial resolutions (4 ** 2–8 ** 2) brings high-level aspects such as pose  general hair style  face shape  and eyeglasses from source B  while all colors(eyes  hair  lighting) and finer facial features resemble A. If we instead copy the styles of middle resolutions (16 ** 2–32 ** 2) from B  we inheritsmaller scale facial features  hair style  eyes open/closed from B  while the pose  general face shape  and eyeglasses from A are preserved.Finally  copying the fine styles (64 ** 2–1024 ** 2)   By the way most of the architecture/training process is based on https://arxiv.org/pdf/1710.10196.pdf (Baseline configuration is the Progressive GAN).  The changes made to architecture: - The baseline using bilinear up/downsampling operations  longer training  and tuned hyperparameters - Adding the mapping network and AdaIN operations - Simplify the architecture by removing the traditional input layer and starting the image synthesis from a learned 4 × 4 × 512 constant tensor - Adding the noise inputs - Adding the mixing regularization(I didn't mention it. The idea is the following: in the learning process we don't  take one latent variable  but take two. Then for one part of the network we feed as style first variable and then the other. That way the model separate the influence and doesn't entangle it. At test time see [Mixing] figure below)  ![Architecture](assets/photo5197353128075307948.jpg)  Figure _Architecture_ * **W** - an intermediate latent space * **AdaIN** - adaptive *style* instance normalization * **A** - learned affine transform * **B** - learned per-channel scaling fac-tors to the noise input  ![Mixture](assets/mixture.png)  _Figure Mixing_. Two sets of images were generated from their respective latent codes (sources A and B); the rest of the images were generated bycopying a specified subset of styles from source B and taking the rest from source A.   * **Proposed metrics:**        - Perceptual path length:     ![Path_length](assets/path_length.png)     where `z1  z2` are latent variables; `t` is random value in `(0 1)` interval;     `eps` = `1e-4`; `G` stands for generator; `slerp` - spherical      interpolation; `d(x1  x2)` - `l2(vgg(x1)  vgg(x2))`;     The idea is that for less entangled and informative latent code interpolation of latent-space      vectors should not yield surprisingly non-linear changes in the image. Quantative results in the Table 3.     - Linear separability. Tries to find out the separability in the input     latent code. Given the features of generated images as Male/Female  find out     how informative for this info is input latent code.     The idea is the next: given good dataset as CELEBA-HQ train good classifier of      a given feature; then generate images using our **Generator** and label using trained     classifier so we have <latent variable  class of interest> mapping; then train SVM on this     data and exponent of cross entropy of this model will be the score of Linear Separability.     See table:     ![Metrics](assets/metrics.png)  * **Results:**    * **Frechet inception distance (FID)**  ![FID](assets/photo5195447889172737473.jpg)       """;Computer Vision;https://github.com/irynakostyshyn/A-Style-Based-Generator-Architecture-for-GAN
"""``` python   #: https://gist.github.com/weiaicunzai/e623931921efefd4c331622c344d8151   """;General;https://github.com/SpikeKing/mobilenet_v3
"""2021/12/17 哈工大讯飞联合实验室推出模型裁剪工具包TextPruner。查看：https://github.com/airaria/TextPruner   2021/1/27 所有模型已支持TensorFlow 2，请通过transformers库进行调用或下载。https://huggingface.co/hfl   2020/3/11 为了更好地了解需求，邀请您填写[调查问卷](https://wj.qq.com/s2/5637766/6281)，以便为大家提供更好的资源。  2020/2/26 哈工大讯飞联合实验室发布[知识蒸馏工具TextBrewer](https://github.com/airaria/TextBrewer)   - XLNet: https://github.com/zihangdai/xlnet  - Malaya: https://github.com/huseinzol05/Malaya/tree/master/xlnet   """;Natural Language Processing;https://github.com/ymcui/Chinese-XLNet
"""Import `optimizer.py`  then add the prefix `Lookahead` before the name of [arbitrary optimizer](http://mxnet.incubator.apache.org/api/python/optimization/optimization.html?highlight=opt#module-mxnet.optimizer).  ```python import optimizer optimizer.LookaheadSGD(k=5  alpha=0.5  learning_rate=1e-3) ```   ```bash python mnist.py --optimizer sgd --seed 42 python mnist.py --optimizer lookaheadsgd --seed 42 ```  """;General;https://github.com/mnikitin/LookaheadOptimizer-mx
"""Makefile fixes with default build supporting all NVIDIA architectures   """;General;https://github.com/jolibrain/caffe
"""All datasets used in this repository can be found from [face.evoLVe.PyTorch's Data-Zoo](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch#Data-Zoo).  Note:  - Both training and testing dataset are ""Align_112x112"" version.   - You can run python ./dataset_checker.py to check if the dataloader work.  Download LFW  Aged30 and CFP-FP datasets  then extract them to /your/path/to/test_dataset. These testing data are already binary files  so it's not necessary to do any preprocessing. The directory structure should be like bellow.   test_dataset: '/your/path/to/test_dataset'   python take_pic.py -o path/to/dataset -n name   """;Computer Vision;https://github.com/3P2S/arcface
"""- https://github.com/thunlp/GNNPapers   """;Computer Vision;https://github.com/LFhase/Research_Navigation
"""For example  the XCiT-S12/16 model can be trained using the following command     ![Detection and Instance segmentation result for an ultra high resolution image 6000x4000](https://user-images.githubusercontent.com/8495451/122474488-962c7380-cfc3-11eb-9e9c-51beda07740b.jpg) )   <!--  First  clone the repo ``` git clone https://github.com/facebookresearch/XCiT.git ```  Then  you can install the required packages including: [Pytorch](https://pytorch.org/) version 1.7.1  [torchvision](https://pytorch.org/vision/stable/index.html) version 0.8.2 and [Timm](https://github.com/rwightman/pytorch-image-models) version 0.4.8 ``` pip install -r requirements.txt ```  Download and extract the [ImageNet](https://imagenet.stanford.edu/) dataset. Afterwards  set the ```--data-path``` argument to the corresponding extracted ImageNet path.  For full details about all the available arguments  you can use ``` python main.py --help ```  For detection and segmentation downstream tasks  please check:  + COCO Object detection and Instance segmentation: [XCiT Detection](detection/)  + ADE20k Semantic segmentation: [XCiT Semantic Segmentation](semantic_segmentation/)  ---   """;Computer Vision;https://github.com/facebookresearch/xcit
"""- Pytorch=1.7.1 (with cuda 11.0)   - scipy=1.4.1   - Downlod the weights from this website (https://download.pytorch.org/models/resnet50-0676ba61.pth)   Second  you need to download the KonIQ-10k dataset.   """;Computer Vision;https://github.com/anse3832/MUSIQ
"""         python train.py -c 2 -p your_project_name --batch_size 8 --lr 1e-3 \      --load_weights last \      --head_only True                 pip install pycocotools numpy opencv-python tqdm tensorboard tensorboardX pyyaml webcolors     pip install torch==1.4.0     pip install torchvision==0.5.0            [2020-04-14] for those who needs help or can't get a good result after several epochs  check out this tutorial. You can run it on colab with GPU support.   Check out this tutorial if you are new to this. You can run it on colab with GPU support.    -w /path/to/your/weights   If you like this repository  or if you'd like to support the author for any reason  you can donate to the author. Feel free to send me your name or introducing pages  I will make sure your name(s) on the sponsors list.    Sincerely thank you for your generosity.            datasets/         -coco2017/             -train2017/                 -000000000001.jpg                 -000000000002.jpg                 -000000000003.jpg             -val2017/                 -000000000004.jpg                 -000000000005.jpg                 -000000000006.jpg             -annotations                 -instances_train2017.json                 -instances_val2017.json       project_name: coco     train_set: train2017     val_set: val2017     num_gpus: 4       obj_list: ['person'  'bicycle'  'car'  ...]                 python train.py -c 2 -p your_project_name --batch_size 8 --lr 1e-3 \      --load_weights /path/to/your/weights/efficientdet-d2.pth \      --head_only True                python train.py -c 2 -p your_project_name --batch_size 8 --lr 1e-3 --debug True           """;Computer Vision;https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;Computer Vision;https://github.com/mathieuorhan/darknet
"""Framework for experimental analysis of adversarial example attacks on policy learning in Deep RL. Attack methodologies are detailed in our paper ""Whatever Does Not Kill Deep Reinforcement Learning  Makes It Stronger"" (Behzadan & Munir  2017 - https://arxiv.org/abs/1712.09344 ).    This project provides an interface between [@openai/baselines](https://github.com/openai/baselines) and [@tensorflow/cleverhans](https://github.com/tensorflow/cleverhans) to facilitate the crafting and implementation of adversarial example attacks on deep RL algorithms. We would also like to thank [@andrewliao11/NoisyNet-DQN](https://github.com/andrewliao11/NoisyNet-DQN) for inspiring solutions to implementing the [NoisyNet](https://arxiv.org/abs/1706.10295) algorithm for DQN.   Two example scripts are included.   - enjoy-adv.py : sample implementation of test-time FGSM attack on pre-trained DQN Atari agents. - train.py: sample implementation of training-time FGSM attack on DQN Atari agents.  Some example executions on the Breakout game environment are:   - Test-time  No attack  testing a DQN model of Breakout trained without parameter noise:  ``` $> python3 enjoy-adv.py --env Breakout --model-dir ./data/Breakout/model-173000 --video ./Breakout.mp4 ```  - Test-time  No attack  testing a DQN model of Breakout trained with parameter noise (NoisyNet implementation): ``` $> python3 enjoy-adv.py --env Breakout --noisy --model-dir ./data/Breakout/model-173000 --video ./Breakout.mp4 ```  - Test-time  Whitebox FGSM attack  testing a DQN model of Breakout trained without parameter noise: ``` $> python3 enjoy-adv.py --env Breakout --model-dir ./data/Breakout/model-173000 --attack fgsm --video ./Breakout.mp4 ```  - Test-time  Whitebox FGSM attack  testing a DQN model of Breakout trained with parameter noise (NoisyNet implementation): ``` $> python3 enjoy-adv.py --env Breakout --noisy --model-dir ./data/Breakout/model-173000 --attack fgsm --video ./Breakout.mp4 ```  - Test-time  Blackbox FGSM attack  testing a DQN model of Breakout trained without parameter noise: ``` $> python3 enjoy-adv.py --env Breakout --model-dir ./data/Breakout/model-173000 --attack fgsm --blackbox --model-dir2 ./data/Breakout/model-173000-2 --video ./Breakout.mp4 ```  - Test-time  Blackbox FGSM attack  testing a DQN model of Breakout trained with parameter noise (NoisyNet implementation)  replica model trained without parameter noise: ``` $> python3 enjoy-adv.py --env Breakout --noisy --model-dir ./data/Breakout/model-173000 --attack fgsm --blackbox --model-dir2 ./data/Breakout/model-173000-2 --video ./Breakout.mp4 ```  - Test-time  Blackbox FGSM attack  testing a DQN model of Breakout trained with parameter noise (NoisyNet implementation)  replica model trained with parameter noise: ``` $> python3 enjoy-adv.py --env Breakout --noisy --model-dir ./data/Breakout/model-173000 --attack fgsm --blackbox --model-dir2 ./data/Breakout/model-173000-2 --noisy2 --video ./Breakout.mp4 ```  - Training-time  Whitebox attack  no parameter noise  injecting adversarial example with 20% probability:  ``` $> python3 train.py --env Breakout --save-dir ./data/Breakout/ --attack fgsm --num-steps 200000000 --attack-prob 0.2  ```  - Training-time  Whitebox attack  NoisyNet parameter noise  injecting adversarial example with 100% probability:  ``` $> python3 train.py --env Breakout --noisy --save-dir ./data/Breakout/ --attack fgsm --num-steps 200000000 --attack-prob 1.0 ```  """;Reinforcement Learning;https://github.com/behzadanksu/rl-attack
"""```bash $ pip install transformer-in-transformer-flax ```   ```python from jax import random from jax import numpy as jnp from transformer_in_transformer_flax import TransformerInTransformer  TNTConfig  #:example configuration for TNT-B config = TNTConfig(     num_classes = 1000      depth = 12      image_size = 224      patch_size = 16      transformed_patch_size = 4      inner_dim = 40      inner_heads = 4      inner_dim_head = 64      inner_r = 4      outer_dim = 640      outer_heads = 10      outer_dim_head = 64      outer_r = 4 )  rng = random.PRNGKey(seed=0) model = TransformerInTransformer(config=config) params = model.init(rng  jnp.ones((1  224  224  3)  dtype=config.dtype)) img = random.uniform(rng  (2  224  224  3)) logits = model.apply(params  img) #: (2  1000) ```   """;Computer Vision;https://github.com/NZ99/transformer_in_transformer_flax
"""joblib == 0.11     파이썬 버전 확인은 python --version으로 확인 하실 수 있습니다.   """;General;https://github.com/mindmapper15/Voice-Converter
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   For R-FCN/Faster R-CNN\: 1. Please download COCO and VOC 2007+2012 datasets  and make sure it looks like this:  	``` 	./data/coco/ 	./data/VOCdevkit/VOC2007/ 	./data/VOCdevkit/VOC2012/ 	```  2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  For DeepLab\: 1. Please download Cityscapes and VOC 2012 datasets and make sure it looks like this:  	``` 	./data/cityscapes/ 	./data/VOCdevkit/VOC2012/ 	``` 2. Please download argumented VOC 2012 annotations/image lists  and put the argumented annotations and the argumented train/val lists into:  	``` 	./data/VOCdevkit/VOC2012/SegmentationClass/ 	./data/VOCdevkit/VOC2012/ImageSets/Main/ 	```      Respectively.     2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  1. Clone the Deformable ConvNets repository  and we'll call the directory that you cloned Deformable-ConvNets as ${DCN_ROOT}. ``` git clone https://github.com/msracver/Deformable-ConvNets.git ```  2. For Windows users  run ``cmd .\init.bat``. For Linux user  run `sh ./init.sh`. The scripts will build cython module automatically and create some folders.  3. Install MXNet: 	 	**Note: The MXNet's Custom Op cannot execute parallelly using multi-gpus after this [PR](https://github.com/apache/incubator-mxnet/pull/6928). We strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.5).**  	***Quick start***  	3.1 Install MXNet and all dependencies by  	``` 	pip install -r requirements.txt 	``` 	If there is no other error message  MXNet should be installed successfully.  	 	***Build from source (alternative way)***  	3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by 	``` 	git clone --recursive https://github.com/dmlc/mxnet.git 	git checkout 998378a 	git submodule update 	#: if it's the first time to checkout  just use: git submodule update --init --recursive 	``` 	3.3 Compile MXNet 	``` 	cd ${MXNET_ROOT} 	make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1 	``` 	3.4 Install the MXNet Python binding by 	 	***Note: If you will actively switch between different versions of MXNet  please follow 3.5 instead of 3.4*** 	``` 	cd python 	sudo python setup.py install 	``` 	3.5 For advanced users  you may put your Python packge into `./external/mxnet/$(YOUR_MXNET_PACKAGE)`  and modify `MXNET_VERSION` in `./experiments/rfcn/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`. Thus you can switch among different versions of MXNet quickly.  4. For Deeplab  we use the argumented VOC 2012 dataset. The argumented annotations are provided by [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html) dataset. For convenience  we provide the converted PNG annotations and the lists of train/val images  please download them from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMRhVImMI1jRrsxDg).   We provide trained deformable convnet models  including the deformable R-FCN & Faster R-CNN models trained on COCO trainval  and the deformable DeepLab model trained on CityScapes train.  1. To use the demo with our pre-trained deformable models  please download manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMSjehIcCgAhvEAHw) or [BaiduYun](https://pan.baidu.com/s/1dFlPFED)  and put it under folder `model/`.  	Make sure it looks like this: 	``` 	./model/rfcn_dcn_coco-0000.params 	./model/rfcn_coco-0000.params 	./model/fpn_dcn_coco-0000.params 	./model/fpn_coco-0000.params 	./model/rcnn_dcn_coco-0000.params 	./model/rcnn_coco-0000.params 	./model/deeplab_dcn_cityscapes-0000.params 	./model/deeplab_cityscapes-0000.params 	./model/deform_conv-0000.params 	./model/deform_psroi-0000.params 	``` 2. To run the R-FCN demo  run 	``` 	python ./rfcn/demo.py 	``` 	By default it will run Deformable R-FCN and gives several prediction results  to run R-FCN  use 	``` 	python ./rfcn/demo.py --rfcn_only 	``` 3. To run the DeepLab demo  run 	``` 	python ./deeplab/demo.py 	``` 	By default it will run Deformable Deeplab and gives several prediction results  to run DeepLab  use 	``` 	python ./deeplab/demo.py --deeplab_only 	``` 4. To visualize the offset of deformable convolution and deformable psroipooling  run 	``` 	python ./rfcn/deform_conv_demo.py 	python ./rfcn/deform_psroi_demo.py 	```    1. All of our experiment settings (GPU #  dataset  etc.) are kept in yaml config files at folder `./experiments/rfcn/cfgs`  `./experiments/faster_rcnn/cfgs` and `./experiments/deeplab/cfgs/`. 2. Eight config files have been provided so far  namely  R-FCN for COCO/VOC  Deformable R-FCN for COCO/VOC  Faster R-CNN(2fc) for COCO/VOC  Deformable Faster R-CNN(2fc) for COCO/VOC  Deeplab for Cityscapes/VOC and Deformable Deeplab for Cityscapes/VOC  respectively. We use 8 and 4 GPUs to train models on COCO and on VOC for R-FCN  respectively. For deeplab  we use 4 GPUs for all experiments.  3. To perform experiments  run the python scripts with the corresponding config file as input. For example  to train and test deformable convnets on COCO with ResNet-v1-101  use the following command     ```     python experiments\rfcn\rfcn_end2end_train_test.py --cfg experiments\rfcn\cfgs\resnet_v1_101_coco_trainval_rfcn_dcn_end2end_ohem.yaml     ```     A cache folder would be created automatically to save the model and the log under `output/rfcn_dcn_coco/`. 4. Please find more details in config files and in our code.   """;Computer Vision;https://github.com/qilei123/DeformableConvV2_crop
"""Check `README.md` under `data` for more details.  Check  `vlbert_tasks.yml` for more details.     1. Create a fresh conda environment  and install all dependencies.  ```text conda create -n vilbert python=3.6 conda activate vilbert git clone https://github.com/jiasenlu/vilbert_beta cd vilbert_beta pip install -r requirements.txt ```  2. Install pytorch ``` conda install pytorch torchvision cudatoolkit=10.0 -c pytorch ```  3. Install apx  follows https://github.com/NVIDIA/apex  4. compile tools  ``` cd tools/refer make ```  2: To test on held out validation split  use the following command:    """;Natural Language Processing;https://github.com/dw-dengwei/vilbert
"""device : (Optional) ""cpu"" or ""cuda"". Please specify cuda when uses gpu acceleration. Default is ""cpu"".   A simple example of calculating SWD on GPU.  ```python import torch from swd import swd  torch.manual_seed(123) #: fix seed x1 = torch.rand(1024  3  128  128)  #: 1024 images  3 chs  128x128 resolution x2 = torch.rand(1024  3  128  128) out = swd(x1  x2  device=""cuda"") #: Fast estimation if device=""cuda"" print(out) #: tensor(53.6950) ```   """;Computer Vision;https://github.com/koshian2/swd-pytorch
"""Original script and models from Resnet & Densenet came from https://github.com/kuangliu/pytorch-cifar   Python 3.8.5  GCC 9.3.0  Pytorch 1.7.0+cu110  NVIDIA-SMI 455.38  Driver Version: 455.38  CUDA Version: 11.1  Ubuntu 20.04.1 LTS  GEFORCE RTX 3090   """;General;https://github.com/pepe78/cifar_pytorch
"""Space exploration is now the fastest-growing subject. This was  however at the cost of billions worth of projects and twice the amount effort. Obi-Wan Kenobi is an integrated system of a space rover updated and modified to use minimum energy with the greatest efficiency possible. It aims to revolutionize the entire industry  rendering it a profitable organization. This is managed by a new arm designed to indulge the biggest and most accurate amount of information while also not sacrificing the energy and a brand new software that takes the camera’s 2-D image and process it with sensors on the rover to produce a high-quality 3-D environment. How We Addressed This Challenge Obi-Wan Kenobi is space rover designed to gather information about foreign plants and there composition in a new way. We have made it with many implementations improving on the previous rovers using new technics  mechanics  and software in astronomical ways. Obi-Wan Kenobi aims to take the prior models of space rovers and modify on them in ways that revolutionize the entire subject of space exploration by maximizing the amount of information it can indulge and allaying every drop of wasted energy putting it into more work.     ·        https://mars.nasa.gov/mars2020/spacecraft/rover/arm/   ·        https://nasa3d.arc.nasa.gov/models   Presentation address: https://prezi.com/view/IVcKdFSYdpR1vnmgnOYO/   """;Computer Vision;https://github.com/ahmedabdel-hady/Space-Xplores-Nasaspaceapp2020
""".. image:: https://dev.azure.com/deepset/FARM/_apis/build/status/deepset-ai.FARM?branchName=master       :alt: Build   Installation &lt;https://github.com/deepset-ai/FARM#installation&gt;_   git clone https://github.com/deepset-ai/FARM.git  cd FARM  pip install -r requirements.txt  pip install --editable .   From PyPi::  pip install farm  Note: On windows you might need :code:pip install farm -f https://download.pytorch.org/whl/torch_stable.html to install PyTorch correctly   any optimizer from PyTorch  Apex or Transformers  any learning rate schedule from PyTorch or Transformers   """;Natural Language Processing;https://github.com/deepset-ai/FARM
"""`pip` is recommended to install DeepTables:  ```bash pip install tensorflow==2.4.2 deeptables ```  Note: * Tensorflow is required by DeepTables  install it before running DeepTables.  * DeepTables was tested with TensorFlow version 2.0 to 2.4  install the tested version please.  **GPU** Setup (Optional)  To use DeepTables with GPU devices  install `tensorflow-gpu` instead of `tensorflow`.  ```bash pip install tensorflow-gpu==2.4.2 deeptables ```   ***Verify the installation***:  ```bash python -c ""from deeptables.utils.quicktest import test; test()"" ```   Please refer to the official docs at [https://deeptables.readthedocs.io/en/latest/](https://deeptables.readthedocs.io/en/latest/). * [Quick Start](https://deeptables.readthedocs.io/en/latest/quick_start.html) * [Examples](https://deeptables.readthedocs.io/en/latest/examples.html) * [ModelConfig](https://deeptables.readthedocs.io/en/latest/model_config.html) * [Models](https://deeptables.readthedocs.io/en/latest/models.html) * [Layers](https://deeptables.readthedocs.io/en/latest/layers.html) * [AutoML](https://deeptables.readthedocs.io/en/latest/automl.html)   ```python import numpy as np from deeptables.models import deeptable  deepnets from deeptables.datasets import dsutils from sklearn.model_selection import train_test_split  #:loading data df = dsutils.load_bank() df_train  df_test = train_test_split(df  test_size=0.2  random_state=42)  y = df_train.pop('y') y_test = df_test.pop('y')  #:training config = deeptable.ModelConfig(nets=deepnets.DeepFM) dt = deeptable.DeepTable(config=config) model  history = dt.fit(df_train  y  epochs=10)  #:evaluation result = dt.evaluate(df_test y_test  batch_size=512  verbose=0) print(result)  #:scoring preds = dt.predict(df_test) ```   """;General;https://github.com/DataCanvasIO/DeepTables
"""Currently  models.py contains most of the code to train and create the models. To use different modes  uncomment the parts of the code that you need.  Note the difference between the *_network objects and *_model objects.  - The *_network objects refer to the helper classes which create and manage the Keras models  load and save weights and  set whether the model can be trained or not. - The *_models objects refer to the underlying Keras model.   **Note**: The training images need to be stored in a subdirectory. Assume the path to the images is `/path-to-dir/path-to-sub-dir/*.png`  then simply write the path as `coco_path = /path-to-dir`. If this does not work  try `coco_path = /path-to-dir/` with a trailing slash (/)  To just create the pretrain model: ``` srgan_network = SRGANNetwork(img_width=32  img_height=32  batch_size=1) srgan_model = srgan_network.build_srgan_pretrain_model()  #: Plot the model from keras.utils.visualize_util import plot plot(srgan_model  to_file='SRGAN.png'  show_shapes=True) ```  To pretrain the SR network: ``` srgan_network = SRGANNetwork(img_width=32  img_height=32  batch_size=1) srgan_network.pre_train_srgan(iamges_path  nb_epochs=1  nb_images=50000) ```  ** NOTE **: There may be many cases where generator initializations may lead to completely solid validation images.  Please check the first few iterations to see if the validation images are not solid images.  To counteract this  a pretrained generator model has been provided  from which you can restart training. Therefore the model can continue learning without hitting a bad initialization.  To pretrain the Discriminator  network: ``` srgan_network = SRGANNetwork(img_width=32  img_height=32  batch_size=1) srgan_network.pre_train_discriminator(iamges_path  nb_epochs=1  nb_images=50000  batchsize=16) ```  To train the full network (Does NOT work properly right now  Discriminator is not correctly trained): ``` srgan_network = SRGANNetwork(img_width=32  img_height=32  batch_size=1) srgan_network.train_full_model(coco_path  nb_images=80000  nb_epochs=10) ```   """;Computer Vision;https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks
"""You can use fastai without any installation by using [Google Colab](https://colab.research.google.com/). In fact  every page of this documentation is also available as an interactive notebook - click ""Open in colab"" at the top of any page to open it (be sure to change the Colab runtime to ""GPU"" to have it run fast!) See the fast.ai documentation on [Using Colab](https://course.fast.ai/start_colab) for more information.  You can install fastai on your own machines with conda (highly recommended)  as long as you're running Linux or Windows (NB: Mac is not supported). For Windows  please see the ""Running on Windows"" for important notes.  If you're using [miniconda](https://docs.conda.io/en/latest/miniconda.html) (recommended) then run (note that if you replace `conda` with [mamba](https://github.com/mamba-org/mamba) the install process will be much faster and more reliable): ```bash conda install -c fastchan fastai ```  ...or if you're using [Anaconda](https://www.anaconda.com/products/individual) then run: ```bash conda install -c fastchan fastai anaconda ```  To install with pip  use: `pip install fastai`. If you install with pip  you should install PyTorch first by following the PyTorch [installation instructions](https://pytorch.org/get-started/locally/).  If you plan to develop fastai yourself  or want to be on the cutting edge  you can use an editable install (if you do this  you should also use an editable install of [fastcore](https://github.com/fastai/fastcore) to go with it.) First install PyTorch  and then:  ```  git clone https://github.com/fastai/fastai pip install -e ""fastai[dev]"" ```    Plain PyTorch   nbdev_test_nbs or make test  For all the tests to pass  you'll need to install the following optional dependencies:  pip install ""sentencepiece&lt;0.1.90"" wandb tensorboard albumentations pydicom opencv-python scikit-image pyarrow kornia \   """;Computer Vision;https://github.com/fastai/fastai
"""1. First  install suitable TensorFlow version. See instruction [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md).  2. Install other project dependencies:      `./install_deps.sh`   """;Sequential;https://github.com/sakurusurya2000/RNN
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-91-download-archive  OpenCV 3.3.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.3.0/opencv-3.3.0-vc14.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;General;https://github.com/teemoeric/projet
"""SegNet requires a modified version of Caffe to run. Please download and compile caffe-segnet to use these models:  This version supports cudnn v2 acceleration. @TimoSaemann has a branch supporting a more recent version of Caffe (Dec 2016) with cudnn v5.1:   This repository contains all the files for you to complete the 'Getting Started with SegNet' and the 'Bayesian SegNet' tutorials here: http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html  Please see this link for detailed instructions.   If you would just like to try out an example model  then you can find the model used in the [SegNet webdemo](http://mi.eng.cam.ac.uk/projects/segnet/) in the folder ```Example_Models/```. You will need to download the weights separately using the link in the [SegNet Model Zoo](https://github.com/alexgkendall/SegNet-Tutorial/blob/master/Example_Models/segnet_model_zoo.md).  First open ```Scripts/webcam_demo.py``` and edit line 14 to match the path to your installation of SegNet. You will also need a webcam  or alternatively edit line 39 to input a video file instead. To run the demo use the command:  ```python Scripts/webcam_demo.py --model Example_Models/segnet_model_driving_webdemo.prototxt --weights /Example_Models/segnet_weights_driving_webdemo.caffemodel --colours /Scripts/camvid12.png```   Use docker to compile caffe and run the examples. In order to run caffe on the gpu using docker  please install nvidia-docker (see https://github.com/NVIDIA/nvidia-docker or using ansbile: https://galaxy.ansible.com/ryanolson/nvidia-docker/)  to run caffe on the CPU: ``` docker build -t bvlc/caffe:cpu ./cpu  #: check if working docker run -ti bvlc/caffe:cpu caffe --version #: get a bash in container to run examples docker run -ti --volume=$(pwd):/SegNet -u $(id -u):$(id -g) bvlc/caffe:cpu bash ```  to run caffe on the GPU: ``` docker build -t bvlc/caffe:gpu ./gpu #: check if working docker run -ti bvlc/caffe:gpu caffe device_query -gpu 0 #: get a bash in container to run examples docker run -ti --volume=$(pwd):/SegNet -u $(id -u):$(id -g) bvlc/caffe:gpu bash ```   A number of example models for indoor and outdoor road scene understanding can be found in the [SegNet Model Zoo](https://github.com/alexgkendall/SegNet-Tutorial/blob/master/Example_Models/segnet_model_zoo.md).   """;Computer Vision;https://github.com/fathimazarin/SegNet
"""기본 linux 명령의 이해와 실습 : linux.md   GPU 설정   데이터 : Fire.tar.gz  spatial_envelope_static_8outdoorcategories.tar.gz   윈도우 환경에서 linux command HowTo : how_to_linux_command_on_windows.md   bynamic robotics 1 : https://www.youtube.com/watch?v=_sBBaNYex3E  bynamic robotics 2 : https://www.youtube.com/watch?v=94nnAOZRg8k  cart pole : https://www.youtube.com/watch?v=XiigTGKZfks  bidirectional RNN : https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66   동영상 스타일 변환 : https://www.youtube.com/watch?v=Khuj4ASldmU   Papers with code : https://paperswithcode.com/       jupyter           cd  pwd  ls          mkdir  rm  cp           git  wget       gpu           python           pip       numpy       pandas       compile   """;Computer Vision;https://github.com/dhrim/t-academy_2021
"""Welcome to PaddleSeg! PaddleSeg is an end-to-end image segmentation development kit developed based on [PaddlePaddle](https://www.paddlepaddle.org.cn)  which covers a large number of high-quality segmentation models in different directions such as *high-performance* and *lightweight*. With the help of modular design  we provide two application methods: *Configuration Drive* and *API Calling*. So one can conveniently complete the entire image segmentation application from training to deployment through configuration calls or API calls.  *  Run the following command. If you can train normally  you have installed it successfully.  ```shell python train.py --config configs/quick_start/bisenet_optic_disc_512x512_1k.yml ```    Support to construct a customized segmentation framework with the *API Calling* method for flexible development.  ```shell pip install paddleseg ```    System Requirements: * PaddlePaddle >= 2.0.0 * Python >= 3.6+  Highly recommend you install the GPU version of PaddlePaddle  due to the large overhead of segmentation models  otherwise  it could be out of memory while running the models. For more detailed installation tutorials  please refer to the official website of [PaddlePaddle](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/2.0/install/)。    [x] COCO stuff   * [Installation](./docs/install.md) * [Get Started](./docs/whole_process.md) *  Prepare Datasets    * [Preparation of Annotation Data](./docs/data/marker/marker.md)    * [Annotating Tutorial](./docs/data/transform/transform.md)    * [Custom Dataset](./docs/data/custom/data_prepare.md)  *  Custom Development     * [Detailed Configuration File](./docs/design/use/use.md)     * [Create Your Own Model](./docs/design/create/add_new_model.md)     * [PR Tutorial](./docs/pr/pr/pr.md)     * [Model Guideline](./docs/pr/pr/style_cn.md) * [Model Training](/docs/train/train.md) * [Model Evaluation](./docs/evaluation/evaluate/evaluate.md) * [Prediction](./docs/predict/predict.md)  * Model Export     * [Export Inference Model](./docs/model_export.md)     * [Export ONNX Model](./docs/model_export_onnx.md)  *  Model Deploy     * [Paddle Inference (Python)](./docs/deployment/inference/python_inference.md)     * [Paddle Inference (C++)](./docs/deployment/inference/cpp_inference.md)     * [Paddle Lite](./docs/deployment/lite/lite.md)     * [Paddle Serving](./docs/deployment/serving/serving.md)     * [Paddle JS](./docs/deployment/web/web.md)     * [Benchmark](./docs/deployment/inference/infer_benchmark.md)  * Model Compression     * [Quantization](./docs/slim/quant/quant.md)     * [Distillation](./docs/slim/distill/distill.md)     * [Prune](./docs/slim/prune/prune.md)  *  API Tutorial     * [API Documention](./docs/apis/README.md)     * [API Application](./docs/api_example.md) *  Description of Important Modules     * [Data Augmentation](./docs/module/data/data.md)     * [Loss Description](./docs/module/loss/losses_en.md)     * [Tricks](./docs/module/tricks/tricks.md) * Description of Classical Models     * [DeeplabV3](./docs/models/deeplabv3.md)     * [UNet](./docs/models/unet.md)     * [OCRNet](./docs/models/ocrnet.md)     * [Fast-SCNN](./docs/models/fascnn.md) * [Static Graph Version](./docs/static/static.md) * [FAQ](./docs/faq/faq/faq.md)   """;Computer Vision;https://github.com/PaddlePaddle/PaddleSeg
"""<img src="".demoimages/brighton-starry.JPG"" width=""400"" /> <img src="".demoimages/brighton-wave.JPG"" width=""400"" /> <img src="".demoimages/brighton-muse.jpg"" width=""400"" />  The purpose of this project is to provide an interactive user-friendly notebook for users of all programming competency to engage with. The notebook clearly lays out to the user the steps in using the Notebook. Feel free to fork the repository and implement your own published improvements!   The main file is a Python Notebook called 'StyleTransferSystem.ipynb'. The helper python '.py' files are also needed in the same directory as these notebooks.    """;General;https://github.com/samwatts98/Fast-Neural-Style-Transfer-with-Laplacian-Loss-TensorFlow-1.13
"""The authors present a novel approach to incorporate language information into extracting visual features by conditioning the Batch Normalization parameters on the language. They apply Conditional Batch Normalization (CBN) to a pre-trained ResNet and show that this significantly improves performance on visual question answering tasks. </br>   This repository is compatible with python 2. </br> - Follow instructions outlined on [PyTorch Homepage](https://pytorch.org/) for installing PyTorch (Python2).  - The python packages required are ``` nltk ``` ``` tqdm ``` which can be installed using pip. </br>   To download the VQA dataset please use the script 'scripts/vqa_download.sh': </br>   """;Computer Vision;https://github.com/ap229997/Conditional-Batch-Norm
"""following changes    [x] Change backbone to the one from YOLO9000   per gpu] with heavy augmentations [brightness  saturation  contrast]   """;Computer Vision;https://github.com/Everina/car-detection-yolo
"""--cpu           #: do not use GPU   --cpu           #: do not use GPU   Run VQ-VAE-2 training using the config `task_name` found in `hps.py`. Defaults to `cifar10`: ``` python main-vqvae.py --task task_name ```  Evaluate VQ-VAE-2 from parameters `state_dict_path` on task `task_name`. Defaults to `cifar10`: ``` python main-vqvae.py --task task_name --load-path state_dict_path --evaluate ```  Other useful flags: ``` --no-save       #: disables saving of files during training --cpu           #: do not use GPU --batch-size    #: overrides batch size in cfg.py  useful for evaluating on larger batch size --no-tqdm       #: disable tqdm status bars --no-save       #: disables saving of files --no-amp        #: disables using native AMP (Automatic Mixed Precision) operations --save-jpg      #: save all images as jpg instead of png  useful for extreme resolutions ```   Run level `level` PixelSnail discrete prior training using the config `task_name` found in `hps.py` using latent dataset saved at path `latent_dataset.pt` and VQ-VAE `vqvae_path` to dequantize conditioning variables. Defaults to `cifar10`: ``` python main-pixelsnail.py latent_dataset.pt vqvae_path.pt level --task task_name ```  Other useful flags: ``` --cpu           #: do not use GPU --load-path     #: resume from saved state on disk --batch-size    #: overrides batch size in cfg.py  useful for evaluating on larger batch size --save-jpg      #: save all images as jpg instead of png  useful for extreme resolutions --no-tqdm       #: disable tqdm status bars --no-save       #: disables saving of files ```   """;Computer Vision;https://github.com/vvvm23/vqvae-2
"""With recent advancements in machine learning techniques  researchers have demonstrated remarkable achievements in image synthesis (BigGAN  StyleGAN)  textual understanding (GPT-3)  and other areas of text and image manipulation. This hands-on workshop introduces state-of-the-art techniques for text-to-image translation  where textual prompts are used to guide the generation of visual imagery. Participants will gain experience with Open AI's CLIP network and Google's BigGAN  using free Google Colab notebooks which they can apply to their own work after the event. We will discuss other relationships between text and image in art and literature; consider the strengths and limitations of these new techniques; and relate these computational processes to human language  perception  and visual expression and imagination. __Please bring a text you would like to experiment with!__   Click on the links below to open the corresponding notebooks in google colab. You can only run one at a time.   """;Computer Vision;https://github.com/roberttwomey/machine-imagination-workshop
"""<details> <summary>[Click to expand]</summary>  The 3D point clouds  camera poses  and intrinsic parameters are preprocessed together to allow for fast data loading during training. These files are generated using the scripts `pixloc/pixlib/preprocess_[cmu|megadepth].py`. Such data is also hosted [here](https://cvg-data.inf.ethz.ch/pixloc_CVPR2021/training/) and can be download via:  ``` python -m pixloc.download --select CMU MegaDepth --training ```  This also downloads the training split of the *CMU* dataset. The undistorted MegaDepth data (images) can be downloaded [from the D2-Net repository](https://github.com/mihaidusmanu/d2-net#downloading-and-preprocessing-the-megadepth-dataset).  </details>   PixLoc is built with Python >=3.6 and PyTorch. The package `pixloc` includes code for both training and evaluation. Installing the package locally also installs the minimal dependencies listed in `requirements.txt`:  ``` bash git clone https://github.com/cvg/pixloc/ cd pixloc/ pip install -e . ```  Generating visualizations and animations requires extra dependencies that can be installed with: ```bash pip install -e .[extra] ```  Paths to the datasets and to training and evaluation outputs are defined in [`pixloc/settings.py`](pixloc/settings.py). The default structure is as follows:  ``` . ├── datasets     #: public datasets └── outputs     ├── training #: checkpoints and training logs     ├── hloc     #: 3D models and retrieval for localization     └── results  #: outputs of the evaluation ```   We provide a convenient script to download all assets for one or multiple datasets using:   python -m pixloc.run_[7Scenes|Cambridge|Aachen|CMU|RobotCar]  #: choose one   python -m pixloc.pixlib.train pixloc_cmu_reproduce \   T_w2c = T_w2c.cuda()   #: behaves like a torch.Tensor   Have a look at the Jupyter notebook [`demo.ipynb`](./notebooks/demo.ipynb) to localize an image and animate the predictions in 2D and 3D. This requires downloading the pre-trained weights and the data for either the *Aachen Day-Night* or *Extended CMU Seasons* datasets using:  ``` python -m pixloc.download --select checkpoints Aachen CMU --CMU_slices 2 ```  <p align=""center"">   <a href=""./notebooks/demo.ipynb""><img src=""assets/viewer.gif"" width=""60%""/></a>   <br /><em>3D viewer in the demo notebook.</em> </p>   """;Computer Vision;https://github.com/cvg/pixloc
"""```python #: Construct Model output = model(x y teacher_forcing)  #:OPTIMIZER  cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output  labels=y) cost = tf.multiply(cost tf_pad_mask) #:mask used to remove loss effect due to PADS cost = tf.reduce_mean(cost)  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate beta1=0.9 beta2=0.98 epsilon=1e-9).minimize(cost)  #:wanna add some temperature?  """"""temperature = 0.7 scaled_output = tf.log(output)/temperature softmax_output = tf.nn.softmax(scaled_output)""""""  #:(^Use it with ""#:prediction_int = np.random.choice(range(vocab_len)  p=array.ravel())"")  softmax_output = tf.nn.softmax(output)  ```   Randomly shuffling the complete dataset (not yet embedded with word2vec embeddings which was learned just now)   and then splitting it into train  validation and test set   ```python shuffled_indices = np.arange(len(eng)) np.random.shuffle(shuffled_indices)  shuffled_vectorized_eng = [] shuffled_vectorized_beng = []  for i in xrange(len(eng)):     shuffled_vectorized_eng.append(vectorized_eng[shuffled_indices[i]])     shuffled_vectorized_beng.append(vectorized_beng[shuffled_indices[i]])  train_len = int(.75*len(eng)) val_len = int(.15*len(eng))  train_eng = shuffled_vectorized_eng[0:train_len] train_beng = shuffled_vectorized_beng[0:train_len]  val_eng = shuffled_vectorized_eng[train_len:val_len] val_beng = shuffled_vectorized_beng[train_len:val_len]  test_eng = shuffled_vectorized_eng[train_len+val_len:] test_beng = shuffled_vectorized_beng[train_len+val_len:] ```   ```python import tensorflow as tf import math  #:https://www.tensorflow.org/tutorials/word2vec embedding_size = 256 vocabulary_size_eng = len(vocab_eng) vocabulary_size_beng = len(vocab_beng)  #: Placeholders for inputs train_inputs = tf.placeholder(tf.int32  shape=[batch_size]) train_labels = tf.placeholder(tf.int32  shape=[batch_size 1])  ```   ```python batch_size = 128  def generate_batch(inputs labels batch_size):     rand = random.sample((np.arange(len(inputs))) batch_size)     batch_inputs=[]     batch_labels=[]     for i in xrange(batch_size):         batch_inputs.append(inputs[int(rand[i])])         batch_labels.append(labels[int(rand[i])])     batch_inputs = np.asarray(batch_inputs np.int32)     batch_labels = np.asarray(batch_labels np.int32)     return batch_inputs batch_labels      ```   source: https://gist.github.com/nealrs/96342d8231b75cf4bb82   :source: https://gist.github.com/nealrs/96342d8231b75cf4bb82     ""y'alls"": ""you alls""      ""you'd"": ""you had""      ""you'll"": ""you you will""     ""you'll've"": ""you you will have""     ""you're"": ""you are""     ""you've"": ""you have""   test = ['who' 'are' 'you'] #: Enter tokenized text here   ```python def most_similar_eucli_eng(x):     xminusy = np.subtract(np_embedding_eng x)     sq_xminusy = np.square(xminusy)     sum_sq_xminusy = np.sum(sq_xminusy 1)     eucli_dists = np.sqrt(sum_sq_xminusy)     return np.argsort(eucli_dists)      def vec2word_eng(vec):   #: converts a given vector representation into the represented word      most_similars = most_similar_eucli_eng(np.asarray(vec np.float32))     return vocab_eng[most_similars[0]]      ```   """;General;https://github.com/JRC1995/Machine-Translation-Transformers
"""``` git clone github.com/gtegner/mine-pytorch cd mine-pytorch pip install -e . ```  Some of the code uses Pytorch Lightning [4] for training and evaluation.    MINE relies on a statistics network `T` which takes as input two variables X  Y and estimates the mutual information MI(X Y).  ```python from mine.models.mine import Mine statistics_network = nn.Sequential(     nn.Linear(x_dim + y_dim  100)      nn.ReLU()      nn.Linear(100  100)      nn.ReLU()      nn.Linear(100  1) )  mine = Mine(     T = statistics_network      loss = 'mine' #:mine_biased  fdiv     method = 'concat' )  joint_samples = np.random.multivariate_normal(mu = np.array([0 0])  cov = np.array([[1  0.2]  [0.2  1]]))  X  Y = joint_samples[:  0]  joint_samples[:  1]  mi = mine.optimize(X  Y  iters = 100) ```   """;Computer Vision;https://github.com/gtegner/mine-pytorch
"""This project requires python >= 3.6 and tensorflow >= 1.8.  The other dependencies can be installed with `conda`.  ```bash conda env create -f=environment.yml ```  The following packages are installed.  - pyspark=2.3.1 - librosa==0.6.1  - matplotlib=2.2.2 - hypothesis=3.59.1 - docopt=0.6.2   This implementation was tested with Tesla K20c (4.94GiB GPU memory).   """;Audio;https://github.com/TanUkkii007/wavenet
"""`); otherwise the implementation may be different.  ---  * Clone the this repository: ``` git clone https://github.com/cmhungsteve/SSTDA.git ``` * Download the [Dataset](https://www.dropbox.com/s/kc1oyz79rr2znmh/Datasets.zip?dl=0) folder  which contains the features and the ground truth labels. (~30GB)  * To avoid the difficulty for downloading the whole file  we also divide it into multiple files:     * [GTEA](https://www.dropbox.com/s/f0bxg73l62v9yo6/gtea.zip?dl=0)  [50Salads](https://www.dropbox.com/s/1vzeidtkzjef7vy/50salads.zip?dl=0)  [Breakfast-part1](https://www.dropbox.com/s/xgeffaqs5cbbs4l/breakfast_part1.zip?dl=0)  [Breakfast-part2](https://www.dropbox.com/s/mcpj4ny85c1xgvv/breakfast_part2.zip?dl=0)  [Breakfast-part3](https://www.dropbox.com/s/o8ba90n7o3rxqun/breakfast_part3.zip?dl=0)  [Breakfast-part4](https://www.dropbox.com/s/v1vx55zud97x544/breakfast_part4.zip?dl=0)  [Breakfast-part5](https://www.dropbox.com/s/e635tkwpd22dlt9/breakfast_part5.zip?dl=0) * Extract it so that you have the `Datasets` folder. * The default path for the dataset is `../../Datasets/action-segmentation/` if the current location is `./action-segmentation-DA/`. If you change the dataset path  you need to edit the scripts as well.  ---  """;General;https://github.com/cmhungsteve/SSTDA
"""one  easily digestible sentence/quote. I will use the Wikiquote dataset to find latent   How has this problem been solved before? If you feel like you are addressing    300 000 quotes on that source.    """;Natural Language Processing;https://github.com/Jesse-Kerr/dsi-capstone-final-proposals
"""GPUs: K80 ($0.14/hr)  T4 ($0.11/hr)  V100 ($0.74/hr) CUDA with Nvidia Apex FP16/32     Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [Notebook](https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb) <a href=""https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a> * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) << highly recommended * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart)  ![Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/yolov3?logo=docker) * [A TensorRT Implementation of YOLOv3 and YOLOv4](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp)     """;Computer Vision;https://github.com/mahavir-GPI/yolov3
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/TSLNIHAOGIT/bert_run
"""- Install python 3.6 or higher and install dependencies from requirements.txt ```     pip3 install -r requirements.txt ```  - Train and test word vectors: ``` word2vec.py [-h] [-test TEST] -model FO [-train FI] [-cbow CBOW]                    [-negative NEG] [-dim DIM] [-alpha ALPHA] [-window WIN]                    [-min-count MIN_COUNT] [-processes NUM_PROCESSES]                    [-epochs EPOCHS]  optional arguments:   -h  --help            show this help message and exit   -test TEST            Load trained model and test it   -model FO             Output model file -> if test is enabled  then provide                         the path for the model to test   -train FI             Training file   -cbow CBOW            1 for CBOW  0 for skip-gram   -negative NEG         Number of negative examples (>0) for negative                         sampling  0 for hierarchical softmax   -dim DIM              Dimensionality of word embeddings   -alpha ALPHA          Starting alpha   -window WIN           Max window length   -min-count MIN_COUNT  Min count for words used to learn <unk>   -processes NUM_PROCESSES                         Number of processes   -epochs EPOCHS        Number of training epochs ```   """;Natural Language Processing;https://github.com/chikalabouka/INF8225-TP4
""" **loss** MeanSquaredError<br /> **metrics** MeanAbsoluteError<br /> **optimizer** Adam with learning rate scheduler<br /> **monitors** val_loss<br /> **loss_weights** [0.2  0.8] (When multiple heads i.e. Inception_v3)    ```shell pip install smartflow ```  ```shell git clone https://github.com/ThanasisMattas/smartflow.git ```   | Requirements           |   | matplotlib >= 3.3.2    |   | pandas >= 1.1.3        |   | scipy >= 1.5.3         |   On-devise  using the [normalization_layer()].    <img src=""media/input-pred-gt_visualizations/it_00364.png"" width=800>   """;General;https://github.com/ThanasisMattas/smartflow
"""Large-scale language models show promising text generation capabilities  but users cannot easily control this generation process. We release *CTRL*  a 1.6 billion-parameter conditional transformer language model  trained to condition on control codes that specify domain  subdomain  entities  relationships between entities  dates  and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text  preserving the advantages of unsupervised learning while providing more explicit control over text generation.  Paper link: https://arxiv.org/abs/1909.05858  Blog link: https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/  The code currently supports two functionalities: 1. Generating from a trained model  two models are available for download - one with a sequence length of 256 and another with a sequence length of 512 -- they are trained with word-level vocabularies and through a sliding window approach can generate well beyond their trained sequence lengths.  2. Source attribution - given a prompt  prints the perplexity of the prompt conditional on each domain control code (see Section 5 of the paper).   Please refer to the argument flags for more details regarding the options available for either.    The repo now supports (experimental) inference on PyTorch; Collaboratory: https://colab.research.google.com/drive/1nDh3ayRPJGK5ciPO2D3TFkYZFqclBWHY. Simply install PyTorch via pip install torch and run python pytorch_generation.py with the same flags as the base generation.py script except one exception: unlike the base version  here  the model_path requires the path to the .data file and not just the ckpt folder (see collaboratory for example).   You should now be able to run inference on K80/T4/P100/similar GPUs using the lower_memory branch. We quantized certain weights to fp16 which reduced memory usage. Simply clone the repo and git checkout lower_memory. Here is a collaboratory link that demonstrates this functionality: https://colab.research.google.com/drive/1hVveBQShDru1Mjnhe4C21uQv4A2eH1tV   Get Involved   As a generate note  you don't have to use greedy sampling. You can switch to topk or nucleus through the appropriate argument flags.     Text: I have been trying for months now and it seems like no one is willing to give me one.   Here are the steps to get generating:  1. Install the dependencies  This code relies on [TensorFlow 1.14](https://www.tensorflow.org/install) and [fastBPE](https://github.com/glample/fastBPE).   TensorFlow can be installed via `pip install tensorflow[-gpu]==1.14`. fastBPE installation instructions can be found in the GitHub repository linked above. We highly recommend experimenting within a virtualenv or Docker image.   **For inference on PyTorch  please see the update on `Sep 23` at the top of this README. If you use PyTorch  you can skip Step 2.**  2. Patch the `/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/keras.py` (or equivalent  if installed elsewhere) by running   ```patch -b <path_to_tensorflow_estimator_package>/python/estimator/keras.py estimator.patch```  We highly recommend experimenting within a virtualenv or Docker image since the workflow involves patching a TensorFlow file to support some custom functionality. This step is not optional; skipping this step will cause errors (irrespective of device).  If you run into OOM issues because of GPU memory exhaustion  please use the `lower_memory` branch. See the (Sep 19  2019) update at the top of this README for details.    3. Get the model files from `gs://sf-ctrl/seqlen256_v1.ckpt/` or `gs://sf-ctrl/seqlen512_v1.ckpt/`.  A 36-layer model is also available at `gs://sf-ctrl/seqlen256_36layers_v0.ckpt/`.   The model architecture is identical for both checkpoints. The former is trained with lower training sequence length (256) while the latter is trained with a larger one (512). We plan to update the models (with the appropriate version tags) as we continue to train them longer and on more data. **Our current recommendation is to use the `256_v1` model unless you have a strong reason not to. If you have no preference for domain  `Links` is always a good first choice.**  [With `gsutil` installed](https://cloud.google.com/storage/docs/gsutil_install)  you can simply run `gsutil -m cp -r gs://sf-ctrl/seqlen256_v1.ckpt/ .` for copying the model checkpoint over.   Without `gsutil`  you can follow the route recommended @ https://github.com/salesforce/ctrl/issues/7#issuecomment-531303214  4. Run the generation script `generation.py` or the source attribution script `source_attribution.py`.   The `generation.py` prompts the user to input text and then prints the continuation.  The `source_attribution.py` promps the user to input text and then prints a sorted list of domains and the perplexity of the text conditional on each individual domain.     Please create a GitHub issue if you have any questions  suggestions  requests or bug-reports.  We welcome PRs!  """;Natural Language Processing;https://github.com/algharak/CTRL
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/abcxs/maskrcnn-contest
"""<a href=""https://pypi.org/project/commit/""><img alt=""PyPI"" src=""https://img.shields.io/pypi/v/commit""></a>   Even if you don't have a GPU  you can still serve the flask server by using the ngrok setting in commit_autosuggestions.ipynb.   To run this project  you need a flask-based inference server (GPU) and a client (commit module). If you don't have a GPU  don't worry  you can use it through Google Colab.   First  install the package through pip. ```shell script $ pip install commit ```  Set the endpoint for the flask server configured in step 1 through the commit configure command. (For example  if the endpoint is http://127.0.0.1:5000  set it as follows: `commit configure --endpoint http://127.0.0.1:5000`) ```shell script $ commit configure --help        Usage: commit configure [OPTIONS]  Options:   --profile TEXT   unique name for managing each independent settings   --endpoint TEXT  endpoint address accessible to the server (example :                    http://127.0.0.1:5000/)  [required]    --help           Show this message and exit. ```  All setup is done! Now  you can get a commit message from the AI with the command commit.  ```shell script $ commit --help           Usage: commit [OPTIONS] COMMAND [ARGS]...  Options:   --profile TEXT       unique name for managing each independent settings   -f  --file FILENAME  patch file containing git diff (e.g. file created by                        `git add` and `git diff --cached > test.diff`)    -v  --verbose        print suggested commit message more detail.   -a  --autocommit     automatically commit without asking if you want to                        commit    --help               Show this message and exit.  Commands:   configure ```   """;Natural Language Processing;https://github.com/graykode/commit-autosuggestions
"""__FastText + SVM__: We use 300-dimensional word vectors constructed by a FastText language model pre-trained with the Wikipedia corpus ([Joulin et al.  2016](https://arxiv.org/pdf/1612.03651.pdf)). Averaged word embeddings are used as the representation of the document. For preprocessing  all text is converted to lowercase and we remove all punctuation and stop words. SVM is used as the classifier. We tune the hyper-parameters of the SVM classifier using a grid-search based on 5-fold cross-validation performed on the training set  after that  we re-train the classifier with optimised hyper-parameters. This hyper-parameter tuning method is applied in RoBERTa + SVM as well.  __RoBERTa + SVM__: We use 768-dimensional word vectors generated by a pre-trained RoBERTa language model ([Liu et al.  2019](https://arxiv.org/pdf/1907.11692.pdf)). We do not fine-tune the pre-trained language model and use the averaged word vectors as the representation of the document. Since all BERT-based models are configured to take as input a maximum of 512 tokens  we divided the long documents with _W_ words into _k = W/511_ fractions  which is then fed into the model to infer the representation of each fraction (each fraction has a ""\<S\>"" token in front of 511 tokens  so  512 tokens in total). Based on the approach of ([Sun et al.  2020](https://arxiv.org/pdf/1905.05583.pdf))  the vector of each fraction is the average embeddings of words in that fraction  and the representation of the whole text sequence is the mean of all _k_ fraction vectors. For preprocessing  the only operation performed is to convert all tokens to lowercase. SVM is used as the classifier.  __Fine-tuned RoBERTa__: For the document classification task  fine-tuning RoBERTa means adding a softmax layer on top of the RoBERTa encoder output and fine-tuning all parameters in the model.  In this experiment  we fine-tune the same 768-dimensional pre-trained RoBERTa model with a small training set. The settings of all hyper-parameters follow ([Liu et al.  2019](https://arxiv.org/pdf/1907.11692.pdf)). we set the learning rate to _1*10<sup>-4_ and the batch size to 4  and use the Adam optimizer with _epsilon_ equals to _1*10<sup>-8_ through hyperparameter tuning. However  since we assume that the amount of labelled data available for training is small  we do not have the luxury of a hold out validation set to use to implement early stopping during model fine tuning. Instead  after training for 15 epochs we roll back to the model with the lowest loss based on the training dataset. This rollback strategy is also applied to HAN and HBM due to the limited number of instances in training sets. For preprocessing  the only operation performed is to convert all tokens to lowercase.      __Hierarchical Attention Network__: Following ([Yang et al.  2016](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf))  we apply two levels of Bi-GRU with attention mechanism for document classification. All words are first converted to word vectors using GloVe ([Pennington et al.  2014](https://aclanthology.org/D14-1162.pdf)) (300 dimension version pre-trained using the wiki gigaword corpus) and fed into a word-level Bi-GRU with attention mechanism to form sentence vectors. After that  a sentence vector along with its context sentence vectors are input into sentence-level Bi-GRU with attention mechanism to form the document representation which is then passed to a softmax layer for final prediction. For preprocessing  the only operation performed is to convert all tokens to lowercase  and separate documents into sentences. We apply Python NLTK sent_tokenize function to split documents into sentences.  __Hierarchical BERT Model__: For HBM  we set the number of BERT layers to 4  and the maximum number of sentences to 114  64  128  128  100  and 64 for the _Movie Review_  _Multi-domain Customer Review_  _Blog Author Gender_  _Guardian 2013_  _Reuters_ and _20 Newsgroups_ datasets respectively  these values are based on the length of documents in these datasets. After some preliminary experiments  we set the attention head to 1  the learning rate to _2*10<sup>-5_  dropout probability to 0.01  used 50 epochs  set the batch size to 4 and used the Adam optimizer with _epsilon_ equals to _1*10<sup>-8_. The only text preprocessing operation performed is to convert all tokens to lowercase and split documents into sentences. We apply Python NLTK sent_tokenize function to split documents into sentences.   """;Natural Language Processing;https://github.com/GeorgeLuImmortal/Hierarchical-BERT-Model-with-Limited-Labelled-Data
"""Simply fork notebook into your local directory.   Assuming you have all necessary modules installed. Through your command prompt  move to the local repository and run the command:   These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.   """;General;https://github.com/pmallari/Emotion_Recognition_PyTorch
"""OpenCV: For using cascade files   Cython :  To generate CPython extension modules           Download or clone this repo.           Get the requirments by typing in the command.          pip install -r requirements.txt           You are good to go.   4.Install the Python Algorithmia client using the command “pip install algorithmia“.   """;Computer Vision;https://github.com/backSpace001/PMR_WEB_APP1.0
"""Link to Omniglot Dataset: https://github.com/brendenlake/omniglot   """;General;https://github.com/infinitemugen/MAML-Pytorch
"""1. Clone this repository 2. Install dependencies     ```bash       pip3 install -r requirements.txt     ``` 3. Run setup from the repository root directory      ```bash       python3 setup.py install     ``` 4. The stone weights are already present in logs folder   While making your own dataset  follow the following steps:   bash    """;Computer Vision;https://github.com/deolipankaj/Stone_Detection_MRCNN
"""review : https://ropiens.tistory.com/73   """;Computer Vision;https://github.com/RobotMobile/cv-deep-learning-paper-review
"""Bare minimum code requirement for evaluation of an image.  ```python from hcat.unet import Unet_Constructor as Unet import torch.nn as nn from hcat import dataloader  transforms as t   data = dataloader.stack(path='./Data'                          joint_transforms=[t.to_float()  t.reshape()]                          image_transforms=[t.normalize()]                          )  model = Unet(image_dimmensions=2              in_channels=4               out_channels=1               feature_sizes=[8 16 32 64 128]               kernel={'conv1': (3  3  2)  'conv2': (3  3  1)}               upsample_kernel=(2  2  2)               max_pool_kernel=(2  2  1)               upsample_stride=(2  2  1)               dilation=1               groups=1).to('cpu')    image  mask  pwl = data[0]  out = model.forward(image.float()) ```      """;Computer Vision;https://github.com/buswinka/HcUnet
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;General;https://github.com/lipengfeizju/Detection
"""- Dont sample from a Uniform distribution  ![cube](images/cube.png ""Cube"")  - Sample from a gaussian distribution  ![sphere](images/sphere.png ""Sphere"")  - When doing interpolations  do the interpolation via a great circle  rather than a straight line from point A to point B - Tom White's [Sampling Generative Networks](https://arxiv.org/abs/1609.04468) ref code https://github.com/dribnet/plat has more details    - Label Smoothing  i.e. if you have two target labels: Real=1 and Fake=0  then for each incoming sample  if it is real  then replace the label with a random number between 0.7 and 1.2  and if it is a fake sample  replace it with 0.0 and 0.3 (for example).   - Salimans et. al. 2016 - make the labels the noisy for the discriminator: occasionally flip the labels when training the discriminator   - Experience Replay   - Keep a replay buffer of past generations and occassionally show them   - Keep checkpoints from the past of G and D and occassionaly swap them out for a few iterations - All stability tricks that work for deep deterministic policy gradients - See Pfau & Vinyals (2016)   - optim.Adam rules!   - See Radford et. al. 2015 - Use SGD for discriminator and ADAM for generator   - if you have labels available  training the discriminator to also classify the samples: auxillary GANs   - Provide noise in the form of dropout (50%). - Apply on several layers of our generator at both training and test time - https://arxiv.org/pdf/1611.07004v1.pdf    """;General;https://github.com/kingcheng2000/GAN
"""error: ```  pip install pipenv Traceback (most recent call last):   File ""/usr/bin/pip3""  line 9  in <module>     from pip import main ImportError: cannot import name 'main' ```  solution: ``` export PATH=""${HOME}/.local/bin:$PATH"" ```  Follow instructions here:  https://stackabuse.com/pytesseract-simple-python-optical-character-recognition/   sudo apt install tesseract-ocr sudo apt install libtesseract-dev  pipenv install all the stuff.   pip install trdg   WIP https://github.com/mvoelk/ssd_detectors   """;General;https://github.com/llsouder/screen-ocr
"""```bash $ pip install big-sleep ```   You will be able to have the GAN dream up images using natural language with a one-line command in the terminal.   You can now train more than one phrase using the delimiter ""|""   ```bash $ dream ""a pyramid made of ice"" ```  Images will be saved to wherever the command is invoked   """;General;https://github.com/lucidrains/big-sleep
"""The library supports python 3.5+ in linux and window platform.  To install the library: ```bash pip install numpy requests nlpaug ``` or install the latest version (include BETA features) from github directly ```bash pip install numpy git+https://github.com/makcedward/nlpaug.git ``` or install over conda ```bash conda install -c makcedward nlpaug ```  If you use BackTranslationAug  ContextualWordEmbsAug  ContextualWordEmbsForSentenceAug and AbstSummAug  installing the following dependencies as well ```bash pip install torch>=1.6.0 transformers>=4.11.3 sentencepiece ```  If you use LambadaAug  installing the following dependencies as well ```bash pip install simpletransformers>=0.61.10 ```  If you use AntonymAug  SynonymAug  installing the following dependencies as well ```bash pip install nltk>=3.4.5 ```  If you use WordEmbsAug (word2vec  glove or fasttext)  downloading pre-trained model first and installing the following dependencies as well ```bash from nlpaug.util.file.download import DownloadUtil DownloadUtil.download_word2vec(dest_dir='.') #: Download word2vec model DownloadUtil.download_glove(model_name='glove.6B'  dest_dir='.') #: Download GloVe model DownloadUtil.download_fasttext(model_name='wiki-news-300d-1M'  dest_dir='.') #: Download fasttext model  pip install gensim>=4.1.2 ```  If you use SynonymAug (PPDB)  downloading file from the following URI. You may not able to run the augmenter if you get PPDB file from other website ```bash http://paraphrase.org/#:/download ```  If you use PitchAug  SpeedAug and VtlpAug  installing the following dependencies as well ```bash pip install librosa>=0.7.1 matplotlib ```   | Installation | How to install this library |   *   [Quick Example](https://github.com/makcedward/nlpaug/blob/master/example/quick_example.ipynb) *   [Example of Augmentation for Textual Inputs](https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb) *   [Example of Augmentation for Multilingual Textual Inputs ](https://github.com/makcedward/nlpaug/blob/master/example/textual_language_augmenter.ipynb) *   [Example of Augmentation for Spectrogram Inputs](https://github.com/makcedward/nlpaug/blob/master/example/spectrogram_augmenter.ipynb) *   [Example of Augmentation for Audio Inputs](https://github.com/makcedward/nlpaug/blob/master/example/audio_augmenter.ipynb) *   [Example of Orchestra Multiple Augmenters](https://github.com/makcedward/nlpaug/blob/master/example/flow.ipynb) *   [Example of Showing Augmentation History](https://github.com/makcedward/nlpaug/blob/master/example/change_log.ipynb) *   How to train [TF-IDF model](https://github.com/makcedward/nlpaug/blob/master/example/tfidf-train_model.ipynb) *   How to train [LAMBADA model](https://github.com/makcedward/nlpaug/blob/master/example/lambada-train_model.ipynb) *   How to create [custom augmentation](https://github.com/makcedward/nlpaug/blob/master/example/custom_augmenter.ipynb) *   [API Documentation](https://nlpaug.readthedocs.io/en/latest/)   """;Computer Vision;https://github.com/makcedward/nlpaug
"""Run the following cell to load the packages and dependencies that are going to be useful for your journey!   from matplotlib.pyplot import imshow   You are now ready to implement non-max suppression. The key steps are:    Note: The ""None"" dimension of the output tensors has obviously to be less than max_boxes. Note also that this   WARNING:tensorflow:From D:\ProgramFiles\Anaconda3\envs\tf1cpu\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.  Instructions for updating:   """;Computer Vision;https://github.com/Donvink/Car_Detection_for_Autonomous_Driving
"""```bash $ pip install ponder-transformer ```   ```python import torch from ponder_transformer import PonderTransformer  model = PonderTransformer(     num_tokens = 20000      dim = 512      max_seq_len = 512 )  mask = torch.ones(1  512).bool()  x = torch.randint(0  20000  (1  512)) y = torch.randint(0  20000  (1  512))  loss = model(x  labels = y  mask = mask) loss.backward() ```  Now you can set the model to `.eval()` mode and it will terminate early when all samples of the batch have emitted a halting signal  ```python import torch from ponder_transformer import PonderTransformer  model = PonderTransformer(     num_tokens = 20000      dim = 512      max_seq_len = 512      causal = True )  x = torch.randint(0  20000  (2  512)) mask = torch.ones(2  512).bool()  model.eval() #: setting to eval makes it return the logits as well as the halting indices  logits  layer_indices = model(x   mask = mask) #: (2  512  20000)  (2)  #: layer indices will contain  for each batch element  which layer they exited ```   """;General;https://github.com/lucidrains/ponder-transformer
"""All you need to use centermask2 is [detectron2](https://github.com/facebookresearch/detectron2). It's easy!     you just install [detectron2](https://github.com/facebookresearch/detectron2) following [INSTALL.md](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md).    Prepare for coco dataset following [this instruction](https://github.com/facebookresearch/detectron2/tree/master/datasets).   cd centermask2   one should execute:  cd centermask2   cd centermask2   """;General;https://github.com/mahdi-darvish/centermask
"""TBA   TBA   """;Sequential;https://github.com/evtaktasheva/dependency_extraction
"""The required packages are managed with pipenv and can be installed using pipenv install. Please see the pipenv documentation for more information.   To install and train a model.  ```shell pipenv install pipenv shell python train.py ```  To visualize losses and reconstructions.  ```shell tensorboard --logdir ./logs/ ```   """;Computer Vision;https://github.com/sarus-tech/tf2-published-models
"""    ensemble.set_scheduler(         ""CosineAnnealingLR""                          ensemble.set_optimizer(         ""Adam""                                   |       Ensemble Name      |  Type  |      Source Code      |   .. _pytorch: https://pytorch.org/  .. _pypi: https://pypi.org/project/torchensemble/   """;General;https://github.com/xuyxu/Ensemble-Pytorch
""" - GPU accelerated FASTdetection (uncomment Macro CUDA_ACC_FAST in ORBextractor.h to enable it);   You could also follow the example calls at rosrun_cmd.md for your own sensor / sequence.   We use Pangolin for visualization and user interface. Dowload and install instructions can be found at: https://github.com/stevenlovegrove/Pangolin.  We use OpenCV to manipulate images and features. Dowload and install instructions can be found at: http://opencv.org. Required at leat 2.4.3. Tested with OpenCV 2.4.11 and OpenCV 3.2.  Required by g2o (see below). Download and install instructions can be found at: http://eigen.tuxfamily.org. Required at least 3.1.0.   Execute build_ros.sh script:  chmod +x build_ros.sh    ./build_ros.sh   Clone the repository: ``` git clone https://github.com/raulmur/ORB_SLAM2.git ORB_SLAM2 ```  We provide a script `build.sh` to build the *Thirdparty* libraries and *ORB-SLAM2*. Please make sure you have installed all required dependencies (see section 2). Execute: ``` cd ORB_SLAM2 chmod +x build.sh ./build.sh ```  This will create **libORB_SLAM2.so**  at *lib* folder and the executables **mono_tum**  **mono_kitti**  **rgbd_tum**  **stereo_kitti**  **mono_euroc** and **stereo_euroc** in *Examples* folder.   This is a demo of augmented reality where you can use an interface to insert virtual cubes in planar regions of the scene. The node reads images from topic `/camera/image_raw`.    ```   rosrun ORB_SLAM2 MonoAR PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE   ```     """;General;https://github.com/ivalab/gf_orb_slam2
"""In this repository  we release two best models **detoxGPT** and **condBERT** (see [Methodology](https://github.com/skoltech-nlp/rudetoxifier#methodology) for more details). You can try detoxification inference example in this [notebook](https://github.com/skoltech-nlp/rudetoxifier/blob/main/notebooks/rudetoxifier_inference.ipynb) or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1lSXh8PHGeKTLtuhxYCwHL74qG-V-pkLK?usp=sharing).   Also  you can test our models via [web-demo](https://detoxifier.nlp.zhores.net/) or you can pour out your anger on our [Telegram bot](https://t.me/rudetoxifierbot).  ***  """;Natural Language Processing;https://github.com/skoltech-nlp/rudetoxifier
"""$ pip install deepaugment   Default configurations are as following:   ```Python from deepaugment.deepaugment import DeepAugment  deepaug = DeepAugment(my_images  my_labels)  best_policies = deepaug.optimize(300) ```   ```Python deepaug = DeepAugment(""cifar10"")  best_policies = deepaug.optimize(300) ```   ```Python from keras.datasets import fashion_mnist  #: my configuration my_config = {     ""model"": ""basiccnn""      ""method"": ""bayesian_optimization""      ""train_set_size"": 2000      ""opt_samples"": 3      ""opt_last_n_epochs"": 3      ""opt_initial_points"": 10      ""child_epochs"": 50      ""child_first_train_epochs"": 0      ""child_batch_size"": 64 }  (x_train  y_train)  (x_test  y_test) = fashion_mnist.load_data() #: X_train.shape -> (N  M  M  3) #: y_train.shape -> (N) deepaug = DeepAugment(iamges=x_train  labels=y_train  config=my_config)  best_policies = deepaug.optimize(300) ```   """;Computer Vision;https://github.com/abcp4/DAPytorch
"""More complex decision logic are possible  like ""at least N path among M with N<M""  but not possible apriori with softmax layer build to choose one decision to each stage of decision tree.   """;Computer Vision;https://github.com/PierrickPochelu/word_tree_label
""" The code has been tested with Python 3  PyTorch 1.7.1 on Windows. GPU is not required to run the code. You also need other dependencies (e.g.  numpy  pandas  matplotlib  seaborn) which can be installed via:   ```sh $ pip install numpy $ pip install torch $ pip install pandas $ pip install scikit-learn $ pip install matplotlib $ pip install seaborn ```   You can generate the simulated data with the following command:   """;Graphs;https://github.com/basiralab/RG-Select
"""- [逻辑回归](/src/main/java/com/pt/ml/algorithm/LogisticRegression.scala)  - [最大熵模型及自定义实现](/src/main/java/com/pt/ml/algorithm/MaxEntropy.java)  - [决策树分类及调优总结(CART树)](/src/main/java/com/pt/ml/algorithm/DecisionTree.scala)  - [spark GBDT算法](/src/main/java/com/pt/ml/algorithm/GradientBoostTree.scala)  - [spark 随机森林算法](/src/main/java/com/pt/ml/algorithm/RandomForest.scala)  - [线性支撑向量机](/src/main/java/com/pt/ml/algorithm/SurportVectorMerchine.scala)  - [xgboost for scala](/src/main/java/com/pt/ml/algorithm/Xgboost4jScala.scala)  - [xgboost for spark](/src/main/java/com/pt/ml/algorithm/Xgboost4jSpark.scala)  - [LDA 聚类](/src/main/java/com/pt/ml/algorithm/LdaCluster.scala)  - [spark生成tfRecord文件以及python读取的demo](/src/main/java/com/pt/ml/generate/tfrecord/GenerateTfrecordBySpark.scala)  - [二分类的ROC曲线、PR曲线、阈值与PR曲线、阈值F1曲线绘制](/src/main/java/com/pt/ml/util/BinaryClassEvaluation.scala)  - [多分类评估指标计算 精确度、加权准确率召回率、F1值、各类别的准确率-召回率-F1值](/src/main/java/com/pt/ml/util/MultiClassEvaluation.scala)  - [Ansj分词](/src/main/java/com/pt/ml/process/AnsjSegmenterUtil.java)  - [JieBa分词](/src/main/java/com/pt/ml/process/JiebaSegmenterUtil.java)  - [自定义简易分词](/src/main/java/com/pt/ml/process/CustomSegmenter.java)  - [数据标准化与归一化](/src/main/java/com/pt/ml/process/Scaler.scala)  - [TF-IDF计算](/src/main/java/com/pt/ml/process/TfIdf.scala)  - [生成词向量](/src/main/java/com/pt/ml/process/WordToVector.scala)  - [连续特征离散化](/src/main/java/com/pt/ml/process/Discretization.scala)  - [计算编辑距离](/src/main/java/com/pt/ml/example/EditDistanceDemo.java)  - [SimHash算法](/src/main/java/com/pt/ml/process/SimHash.java)  - [PCA降维](/src/main/java/com/pt/ml/process/Pca.scala)  - [TSNE降维](/src/main/java/com/pt/ml/process/TSNEStandard.java)  - [fastText for java训练](/src/main/java/com/pt/ml/algorithm/FastText4J.scala)  - [fastText for java 词向量模型使用](/src/main/java/com/pt/ml/deeplearning/nlp/Word2VecFastText.java)  - [java 绘制点、线、柱状图](/src/main/java/com/pt/ml/visualization)  - [deeplearning4j - 单机前向传播神经网络](/src/main/java/com/pt/ml/deeplearning/BpNeuralNetwork.java)  - [deeplearning4j - 单机卷积神经网络](/src/main/java/com/pt/ml/deeplearning/CnnNeuralNetwork.java)  - [deeplearning4j - spark版本卷积神经网络](/src/main/java/com/pt/ml/deeplearning/CnnNeuralNetworkSpark.scala)  - [deeplearning4j - 单机循环神经网络](/src/main/java/com/pt/ml/deeplearning/LstmClassification.java)  - [deeplearning4j - spark版本循环神经网络](/src/main/java/com/pt/ml/deeplearning/LstmClassificationSpark.scala)  - [deeplearning4j - 词向量使用](/src/main/java/com/pt/ml/deeplearning/nlp/Word2VecDeeplearning4j.java)    """;Natural Language Processing;https://github.com/luckyPT/jvm-ml
"""OpenPose GitHub repo: https://github.com/CMU-Perceptual-Computing-Lab/openpose   """;General;https://github.com/saadbutt32/Conversion-of-Pakistan-Sign-Languag-into-Text-and-Speech-using-OpenPose-and-Machine-Learning
"""I've been searching for a Tensorflow implementation of YOLOv2 for a while but the darknet version and derivatives are not really easy to understand. This one is an hopefully easier-to-understand version of Tiny YOLOv2. The weight extraction  weights structure  weight assignment  network  inference and postprocessing are made as simple as possible.  The output of this implementation on the test image ""dog.jpg"" is the following:  ![alt text](https://github.com/simo23/tinyYOLOv2/blob/master/dog_output.jpg ""YOLOv2 output"")  Just to be clear  this implementation is called ""tiny-yolo-voc"" on pjreddie's site and can be found here:  ![alt text](https://github.com/simo23/tinyYOLOv2/blob/master/pjsite.png ""YOLOv2 site"")   The code runs at ~15fps on my laptop which has a 2GB Nvidia GeForce GTX 960M GPU   - Clone the project and place it where you want - Download the binary file (~60MB) from pjreddie's site: https://pjreddie.com/media/files/yolov2-tiny-voc.weights and place it into the folder where the scripts are - Launch test.py or test_webcam.py. Change the input_img_path and the weights_path in the main if you want  now the network has ""dog.jpg"" as input_img. The code is now configured to run with weights and input image in the same folder as the script.  ```python python3 test.py ```  - If you are launching them for the first time  the weights will be extracted from the binary file and a ckpt will be created. Next time only the ckpt will be used!   I've been struggling on understanding how the binary weights file was written. I hope to save you some time by explaining how I imported the weights into a Tensorflow network:  - Download the binary file from pjreddie's site: https://pjreddie.com/media/files/yolov2-tiny-voc.weights  - Extract the weights from binary to a numpy float32 array with  weight_array = np.fromfile(weights_path  dtype='f') - Delete the first 4 numbers because they are not relevant - Define a function ( load_conv_layer ) to take a part of the array and assign it to the Tensorflow variables of the net - IMPORTANT: the weights order is [ 'biases' 'gamma' 'moving_mean' 'moving_variance' 'kernel']  - IMPORTANT: the 'biases' here refer to the beta value of the Batch Normalization. It does not refer to the biases that must be added after the conv2d because they are set all to zero! ( According to the paper by Ioffe et al. https://arxiv.org/abs/1502.03167 )  - IMPORTANT: the kernel weights are written in Caffe style which means they have shape = (out_dim  in_dim  height  width). They must be converted into Tensorflow style which has shape = (height  width  in_dim  out_dim) - IMPORTANT: in order to obtain the correct results from the weights they need to be DENORMALIZED according to Batch Normalization. It can be done in two ways: define the network with Batch Normalization and use the weights as they are OR define the net without BN ( this implementation ) and DENORMALIZE the weights. ( details are in weights_loader.py ) - In order to verify that the weights extraction is succesfull  I check the total number of params with the number of weights into the weight file. They are both 15867885 in my case.   Another key point is how the predictions tensor is made. It is a 13x13x125 tensor. To process it better:  - Convert the tensor to have shape = 13x13x5x25 = grid_cells x n_boxes_in_each_cell x n_predictions_for_each_box - The 25 predictions are: 2 coordinates and 2 shape values (x y h w)  1 Objectness score  20 Class scores - Now access to the tensor in an easy way! E.g. predictions[row  col  b  :4] will return the 2 coords and shape of the ""b"" B-Box which is in the [row col] grid cell - They must be postprocessed according to the parametrization of YOLOv2. In my implementation it is made like this:   ```python  #: Pre-defined anchors shapes! #: They are not coordinates of the boxes  they are height and width of the 5 anchors defined by YOLOv2 anchors = [1.08 1.19   3.42 4.41   6.63 11.38   9.42 5.11   16.62 10.52] image_height = image_width = 416 n_grid_cells = 13 n_b_boxes = 5  for row in range(n_grid_cells):   for col in range(n_grid_cells):     for b in range(n_b_boxes):        tx  ty  tw  th  tc = predictions[row  col  b  :5]              #: IMPORTANT: (416) / (13) = 32! The coordinates and shape values are parametrized w.r.t center of the grid cell       #: They are parameterized to be in [0 1] so easier for the network to predict and learn       #: With the iterations on every grid cell at [row col] they return to their original positions              #: The x y coordinates are: (pre-defined coordinates of the grid cell [row col] + parametrized offset)*32        center_x = (float(col) + sigmoid(tx)) * 32.0       center_y = (float(row) + sigmoid(ty)) * 32.0        #: Also the width and height must return to the original value by looking at the shape of the anchors       roi_w = np.exp(tw) * anchors[2*b + 0] * 32.0       roi_h = np.exp(th) * anchors[2*b + 1] * 32.0              #: Compute the final objectness score (confidence that there is an object in the B-Box)        final_confidence = sigmoid(tc)        class_predictions = predictions[row  col  b  5:]       class_predictions = softmax(class_predictions)        ```  YOLOv2 predicts parametrized values that must be converted to full size by multiplying them by 32! You can see other EQUIVALENT ways to do this but this one works fine. I've seen someone who  instead of multiplying by 32  divides by 13 and then multiplies by 416 which at the end equals a single multiplication by 32.    """;Computer Vision;https://github.com/simo23/tinyYOLOv2
"""The model relies on custom TensorFlow ops that are compiled on the fly using [NVCC](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html).   To set up the environment e.g. for cuda-10.0: ```python export PATH=/usr/local/cuda-10.0/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda10.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} ```  To test that your NVCC installation is working correctly  run: ```python nvcc test_nvcc.cu -o test_nvcc -run | CPU says hello. | GPU says hello. ```   We explored the GANformer model on 4 datasets for images and scenes: [CLEVR](https://cs.stanford.edu/people/jcjohns/clevr/)  [LSUN-Bedrooms](https://www.yf.io/p/lsun)  [Cityscapes](https://www.cityscapes-dataset.com/) and [FFHQ](https://github.com/NVlabs/ffhq-dataset). The model can be trained on other datasets as well. We trained the model on `256x256` resolution. Higher resolutions are supported too. The model will automatically adapt to the resolution of the images in the dataset.  The [`prepare_data.py`](prepare_data.py) can either prepare the datasets from our catalog or create new datasets.   ⬜️ Releasing Pytorch version (coming soon!)   To prepare the datasets from the catalog  run the following command:   | GAN        | 25.02        | 12.16        | 13.18      | 11.57      |   <div align=""center"">   <img src=""https://cs.stanford.edu/people/dorarad/faces.png"" style=""float:left"" width=""750px"">   <br>   <img src=""https://cs.stanford.edu/people/dorarad/bedroom.png"" style=""float:left"" width=""750px"">   <br>   <img src=""https://cs.stanford.edu/people/dorarad/clevr_new.png"" style=""float:left"" width=""750px"">   <br>   <img src=""https://cs.stanford.edu/people/dorarad/cities_small.png"" style=""float:left"" width=""750px""> </div>   """;General;https://github.com/dorarad/gansformer
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;Computer Vision;https://github.com/buptdbj/darknet-windows-linux
"""*** __Epsilon decay:__ 0.9995  __Batch-size:__ 64  __Gamma:__ 0.75  __Prioritized fraction:__ 0.25     """;Reinforcement Learning;https://github.com/matthewsparr/Deep-Zork
"""To use a different dataset  you just need to modify the CreateTfRecord.ipynb notebook inside the dataset/ folder  to suit your needs.   Note: You do not need both datasets.   All the serving scripts are placed inside: ./serving/.   Create a python3 virtual environment and install the dependencies from the serving_requirements.txt file;   Create a python2 virtual environment and install the dependencies from the client_requirements.txt file;  From the python2 env  run the deeplab_client.ipynb notebook;   """;Computer Vision;https://github.com/sthalles/deeplab_v3
"""The objective of this project is to learn more about conditional generative models. Having worked with GANs  it seems beneficial to study more about adding additional descriptive information with the input image to produce models that are able to distinctly represent specific subjects in the generated data. It seems to be a part of how users can select specific features or labels for the model to generate. As an early step of looking at this and taking into account the limitations of resources and time  this project will be experimenting with the vanilla variational autoencoder and a conditional variational autoencoder.    Install pytorch from here: https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/  For the Jetson TX2  Pytorch cannot be installed using the method described on the above Github page. An alternate version for the GPU must be downloaded from the NVIDIA site as indicated by the link above.    For a short version of the code. Open the terminal on the computer and cd into the folder with demo.py and run in python3 with the following command. The demo trains on the MNIST data for 10 epochs and with 6000 training samples and 1000 training samples.  ``` cd vae python3 demo.py ```  or open terminal and run  ``` bash demo.sh ```  """;General;https://github.com/jenyliu/DLA_interview
"""Run the code with  `python3 main.py`.   """;Reinforcement Learning;https://github.com/RLeike/connect-four
""" - To install  `cd` into the root directory and type `pip install -e .`  - To interactively view moving to landmark scenario (see others in ./scenarios/): `bin/interactive.py --scenario simple.py`  - Known dependencies: Python (3.5.4)  OpenAI gym (0.10.5)  numpy (1.14.5)  - To use the environments  look at the code for importing them in `make_env.py`.   """;Reinforcement Learning;https://github.com/jansenkeith501/CS295-MADDPG
"""프로젝트 소개 홈페이지 : https://readymag.com/u53026007/1140897/   참고 사이트 : https://github.com/AntreasAntoniou/DAGAN   """;Computer Vision;https://github.com/namepen/AI_school_proj
"""Before installing we recommend setting up a clean virtual environment.  From the package directory install the requirements and then the package.  $ pip install -r requirements.txt  $ python setup.py install   - [Tutorial Notebooks](https://github.com/MSRDL/Deep4Cast/blob/master/docs/examples)   """;Sequential;https://github.com/MSRDL/Deep4Cast
"""This implementation is based on [mmdetection](https://github.com/open-mmlab/mmdetection)(v1.0.0). Please refer to [INSTALL.md](docs/INSTALL.md) for installation and dataset preparation.   For your convenience  we provide the following trained models on COCO (more models are coming soon).   Once the installation is done  you can download the provided models and use [inference_demo.py](demo/inference_demo.py) to run a quick demo.   """;Computer Vision;https://github.com/WXinlong/SOLO
"""预训练的模型： https://github.com/DataXujing/DIoU_YOLO_V3/releases/tag/Mask_model 可下载在网上找图像或视频测试！  训练数据下载地址：百度云盘链接： https://pan.baidu.com/s/1J8SwsZ9F5XFbUlHpEVQK3A  提取码：h9sl   GPU=1 #:如果使用GPU设置为1，CPU设置为0   知乎：阿扣是谁哇 https://zhuanlan.zhihu.com/p/103922982?utm_source=wechat_session   """;Computer Vision;https://github.com/DataXujing/DIoU_YOLO_V3
"""  """;General;https://github.com/zhangbo2008/best_pytorch_transformer
"""CompressionVAE is distributed through PyPI under the name `cvae` (https://pypi.org/project/cvae/). To install the latest version  simply run ``` pip install cvae ``` Alternatively  to locally install CompressionVAE  clone this repository and run the following command from the CompressionVAE root directory. ``` pip install -e . ```   We can achieve this with the following code   To use CVAE to learn an embedding function  we first need to import the cvae library. ``` from cvae import cvae ```  When creating a CompressionVAE object for a new model  it needs to be provided a training dataset.  For small datasets that fit in memory we can directly follow the sklean convention. Let's look at this case first and take MNIST as an example.  First  load the MNIST data. (Note: this example requires scikit-learn which is not installed with CVAE. You might have to install it first by running `pip install sklearn`.) ``` from sklearn.datasets import fetch_openml mnist = fetch_openml('mnist_784'  version=1  cache=True) X = mnist.data ```   The example above shows the simplest usage of CVAE. However  if desired a user can take much more control over the system and customize the model and training processes.   """;Computer Vision;https://github.com/maxfrenzel/CompressionVAE
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   Install <a href=""https://www.tensorflow.org/get_started/os_setup"" target=""_blank"">TensorFlow</a>. You may also need to install h5py. The code has been tested with Python 2.7  TensorFlow 1.0.1  CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04.  If you are using PyTorch  you can find a third-party pytorch implementation <a href=""https://github.com/fxia22/pointnet.pytorch"" target=""_blank"">here</a>.  To install h5py for Python: ```bash sudo apt-get install libhdf5-dev sudo pip install h5py ```   Created by <a href=""http://charlesrqi.com"" target=""_blank"">Charles R. Qi</a>  <a href=""http://ai.stanford.edu/~haosu/"" target=""_blank"">Hao Su</a>  <a href=""http://cs.stanford.edu/~kaichun/"" target=""_blank"">Kaichun Mo</a>  <a href=""http://geometry.stanford.edu/member/guibas/"" target=""_blank"">Leonidas J. Guibas</a> from Stanford University.  ![prediction example](https://github.com/charlesq34/pointnet/blob/master/doc/teaser.png)   cd part_seg  sh download_data.sh   To train a model to classify point clouds sampled from 3D shapes:      python train.py  Log files and network parameters will be saved to `log` folder in default. Point clouds of <a href=""http://modelnet.cs.princeton.edu/"" target=""_blank"">ModelNet40</a> models in HDF5 files will be automatically downloaded (416MB) to the data folder. Each point cloud contains 2048 points uniformly sampled from a shape surface. Each cloud is zero-mean and normalized into an unit sphere. There are also text files in `data/modelnet40_ply_hdf5_2048` specifying the ids of shapes in h5 files.  To see HELP for the training script:      python train.py -h  We can use TensorBoard to view the network architecture and monitor the training progress.      tensorboard --logdir log  After the above training  we can evaluate the model and output some visualizations of the error cases.      python evaluate.py --visu  Point clouds that are wrongly classified will be saved to `dump` folder in default. We visualize the point cloud by rendering it into three-view images.  If you'd like to prepare your own data  you can refer to some helper functions in `utils/data_prep_util.py` for saving and loading HDF5 files.   * <a href=""http://stanford.edu/~rqi/pointnet2/"" target=""_blank"">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a> by Qi et al. (NIPS 2017) A hierarchical feature learning framework on point clouds. The PointNet++ architecture applies PointNet recursively on a nested partitioning of the input point set. It also proposes novel layers for point clouds with non-uniform densities. * <a href=""http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w13/Engelmann_Exploring_Spatial_Context_ICCV_2017_paper.pdf"" target=""_blank"">Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</a> by Engelmann et al. (ICCV 2017 workshop). This work extends PointNet for large-scale scene segmentation. * <a href=""https://arxiv.org/abs/1710.04954"" target=""_blank"">PCPNET: Learning Local Shape Properties from Raw Point Clouds</a> by Guerrero et al. (arXiv). The work adapts PointNet for local geometric properties (e.g. normal and curvature) estimation in noisy point clouds. * <a href=""https://arxiv.org/abs/1711.06396"" target=""_blank"">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a> by Zhou et al. from Apple (arXiv) This work studies 3D object detection using LiDAR point clouds. It splits space into voxels  use PointNet to learn local voxel features and then use 3D CNN for region proposal  object classification and 3D bounding box estimation. * <a href=""https://arxiv.org/abs/1711.08488"" target=""_blank"">Frustum PointNets for 3D Object Detection from RGB-D Data</a> by Qi et al. (arXiv) A novel framework for 3D object detection with RGB-D data. The method proposed has achieved first place on KITTI 3D object detection benchmark on all categories (last checked on 11/30/2017).  """;Computer Vision;https://github.com/charlesq34/pointnet
"""  * There have been minor changes with the 1.1.0 update. Now we support PyTorch 1.1.0 by default  and please use the legacy branch if you prefer older version.   Differences between Torch version   git clone https://github.com/thstkdgus35/EDSR-PyTorch  cd EDSR-PyTorch   We recommend you to pre-process the images before training. This step will decode all png files and save them as binaries. Use --ext sep_reset argument on your first run. You can skip the decoding part and use saved binaries with --ext sep argument.   cd src       #: You are now in */EDSR-PyTorch/src   Add --chop_forward argument to your script to enable it.   Now PyTorch 0.3.1 is a default. Use legacy/0.3.0 branch if you use the old version.   If you cannot make the binary pack  use the default setting (--ext img).   Please use the legacy/0.3.1 branch if you are using the old version of PyTorch.   with --pre_train download  pretrained models will be automatically downloaded from the server.   We support PyTorch 1.0.0. If you prefer the previous versions of PyTorch  use legacy branches.   You can test our super-resolution algorithm with your images. Place your images in ``test`` folder. (like ``test/<your_image>``) We support **png** and **jpeg** files.  Run the script in ``src`` folder. Before you run the demo  please uncomment the appropriate line in ```demo.sh``` that you want to execute. ```bash cd src       #: You are now in */EDSR-PyTorch/src sh demo.sh ```  You can find the result images from ```experiment/test/results``` folder.  | Model | Scale | File name (.pt) | Parameters | ****PSNR** | |  ---  |  ---  | ---       | ---        | ---  | | **EDSR** | 2 | EDSR_baseline_x2 | 1.37 M | 34.61 dB | | | | *EDSR_x2 | 40.7 M | 35.03 dB | | | 3 | EDSR_baseline_x3 | 1.55 M | 30.92 dB | | | | *EDSR_x3 | 43.7 M | 31.26 dB | | | 4 | EDSR_baseline_x4 | 1.52 M | 28.95 dB | | | | *EDSR_x4 | 43.1 M | 29.25 dB | | **MDSR** | 2 | MDSR_baseline | 3.23 M | 34.63 dB | | | | *MDSR | 7.95 M| 34.92 dB | | | 3 | MDSR_baseline | | 30.94 dB | | | | *MDSR | | 31.22 dB | | | 4 | MDSR_baseline | | 28.97 dB | | | | *MDSR | | 29.24 dB |  *Baseline models are in ``experiment/model``. Please download our final models from [here](https://cv.snu.ac.kr/research/EDSR/model_pytorch.tar) (542MB) **We measured PSNR using DIV2K 0801 ~ 0900  RGB channels  without self-ensemble. (scale + 2) pixels from the image boundary are ignored.  You can evaluate your models with widely-used benchmark datasets:  [Set5 - Bevilacqua et al. BMVC 2012](http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html)   [Set14 - Zeyde et al. LNCS 2010](https://sites.google.com/site/romanzeyde/research-interests)   [B100 - Martin et al. ICCV 2001](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/)   [Urban100 - Huang et al. CVPR 2015](https://sites.google.com/site/jbhuang0604/publications/struct_sr).  For these datasets  we first convert the result images to YCbCr color space and evaluate PSNR on the Y channel only. You can download [benchmark datasets](https://cv.snu.ac.kr/research/EDSR/benchmark.tar) (250MB). Set ``--dir_data <where_benchmark_folder_located>`` to evaluate the EDSR and MDSR with the benchmarks.  You can download some results from [here](https://cv.snu.ac.kr/research/EDSR/result_image/edsr-results.tar). The link contains **EDSR+_baseline_x4** and **EDSR+_x4**. Otherwise  you can easily generate result images with ``demo.sh`` scripts.   """;General;https://github.com/LEEPEIQIN/EDSR
"""We provide the probabilities  embedding and [labels](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/labels.npy) of each image in the ImageNet validation so that the results can be reproduced easily.  Embedding files are matrixes of size 50000 by 2048 for all models except for PNASNet where the size is 50000 by 4320  embeddings are extracted after the last spatial pooling. The softmax are matrixes of sizes 50000 by 1000 it representing the probability of each class for each image.  | Model | Softmax | Embedding | |:---:|:------------------------------------------------------------:|:------------------------------------------------------------:| | FixResNet-50|[FixResNet50_Softmax.npy](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50_softmax.npy) |[FixResNet50Embedding.npy](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50_embedding.npy) | | FixResNet-50 (*)|[FixResNet50_Softmax_v2.npy](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50_softmax_v2.npy) |[FixResNet50Embedding_v2.npy](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50_embedding_v2.npy) | | FixResNet-50 CutMix|[FixResNet50_CutMix_Softmax.npy](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50CutMix_softmax.npy) |[FixResNet50_CutMix_Embedding.npy](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50CutMix_embedding.npy) | | FixResNet-50 CutMix (*)|[FixResNet50_CutMix_Softmax_v2.npy](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50CutMix_softmax_v2.npy) |[FixResNet50_CutMix_Embedding_v2.npy](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50CutMix_embedding_v2.npy) | | FixPNASNet-5|[FixPNASNet_Softmax.npy](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/PNASNet_softmax.npy) |[FixPNASNet_Embedding.npy](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/PNASNet_embedding.npy) | | FixResNeXt-101 32x48d|[FixResNeXt101_32x48d_Softmax.npy](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/IGAM_Resnext101_32x48d_softmax.npy) |[FixResNeXt101_32x48d_Embedding.npy](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/IGAM_Resnext101_32x48d_embedding.npy) | | FixResNeXt-101 32x48d  (*)|[FixResNeXt101_32x48d_Softmax_v2.npy](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/IGAM_Resnext101_32x48d_softmax_v2.npy) |[FixResNeXt101_32x48d_Embedding_v2.npy](https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/IGAM_Resnext101_32x48d_embedding_v2.npy) |  (*)  We use Horizontal flip  shifted Center Crop and color jittering for fine-tuning (described in [transforms_v2.py](transforms_v2.py))   The FixRes code requires * Python 3.6 or higher * PyTorch 1.0 or higher  and the requirements highlighted in [requirements.txt](requirements.txt) (for Anaconda)   ```bash #: FixResNeXt-101 32x48d python main_evaluate_imnet.py --input-size 320 --architecture 'IGAM_Resnext101_32x48d' --weight-path 'ResNext101_32x48d.pth' #: FixResNet-50 python main_evaluate_imnet.py --input-size 384 --architecture 'ResNet50' --weight-path 'ResNet50.pth'  #:FixPNASNet-5 python main_evaluate_imnet.py --input-size 480 --architecture 'PNASNet' --weight-path 'PNASNet.pth' ```  The following code give results that corresponds to table 2 in the paper :  ```bash #: FixResNeXt-101 32x48d python main_evaluate_softmax.py --architecture 'IGAM_Resnext101_32x48d' --save-path 'where_softmax_and_labels_are_saved'  #: FixPNASNet-5 python main_evaluate_softmax.py --architecture 'PNASNet' --save-path 'where_softmax_and_labels_are_saved'  #: FixResNet50 python main_evaluate_softmax.py --architecture 'ResNet50' --save-path 'where_softmax_and_labels_are_saved' ```  ```bash #: FixResNeXt-101 32x48d python main_extract.py --input-size 320 --architecture 'IGAM_Resnext101_32x48d' --weight-path 'ResNeXt101_32x48d.pth' --save-path 'where_output_will_be_save' #: FixResNet-50 python main_extract.py --input-size 384 --architecture 'ResNet50' --weight-path 'ResNet50.pth' --save-path 'where_output_will_be_save'  #: FixPNASNet-5 python main_extract.py --input-size 480 --architecture 'PNASNet' --weight-path 'PNASNet.pth' --save-path 'where_output_will_be_save' ```   ```bash #: FixResNeXt-101 32x48d python main_finetune.py --input-size 320 --architecture 'IGAM_Resnext101_32x48d' --epochs 1 --batch 8 --num-tasks 32 --learning-rate 1e-3  #: FixResNet-50 python main_finetune.py --input-size 384 --architecture 'ResNet50' --epochs 56 --batch 64 --num-tasks 8 --learning-rate 1e-3  #: FixPNASNet-5 python main_finetune.py --input-size 480 --architecture 'PNASNet' --epochs 1 --batch 64 --num-tasks 8 --learning-rate 1e-4 ```  ```bash #: ResNet50 python main_resnet50_scratch.py --batch 64 --num-tasks 8 --learning-rate 2e-2  ```   """;Computer Vision;https://github.com/facebookresearch/FixRes
"""```bash python setup.py install ```  Requirements: tensorflow (1.x)  networkx  numpy  scikit-learn  scipy    """;Graphs;https://github.com/deezer/gravity_graph_autoencoders
"""To install requirements:  pip install -r requirements.txt   """;Computer Vision;https://github.com/podgorskiy/StyleGan
"""* First  download a dataset  e.g. apple2orange  ```bash $ bash download_dataset.sh apple2orange ```  * Write the dataset to tfrecords  ```bash $ python3 build_data.py ```  Check `$ python3 build_data.py --help` for more details.   Python 3.6.0   My pretrained models are available at https://github.com/vanhuyz/CycleGAN-TensorFlow/releases   """;General;https://github.com/RezisEwig/cyclegan
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	``` 	 2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```   	 4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. Follow the next sections to download pre-computed object proposals and pre-trained ImageNet models   1. Clone the Fast R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/fast-rcnn.git   ```    2. We'll call the directory that you cloned Fast R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*        **Note 1:** If you didn't clone Fast R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `fast-rcnn` branch (or equivalent detached state). This will happen automatically *if you follow these instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```      4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```      5. Download pre-computed Fast R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_fast_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `fast_rcnn_models`. See `data/README.md` for details.   Requirements: hardware  Basic installation   1. Clone the Fast R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/fast-rcnn.git   ```    2. We'll call the directory that you cloned Fast R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*        **Note 1:** If you didn't clone Fast R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `fast-rcnn` branch (or equivalent detached state). This will happen automatically *if you follow these instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```      4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```      5. Download pre-computed Fast R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_fast_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `fast_rcnn_models`. See `data/README.md` for details.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  **Python**  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007. The object proposals are pre-computed in order to reduce installation requirements.  **Note:** If the demo crashes Caffe because your GPU doesn't have enough memory  try running the demo with a small network  e.g.  `./tools/demo.py --net caffenet` or with `--net vgg_cnn_m_1024`. Or run in CPU mode `./tools/demo.py --cpu`. Type `./tools/demo.py -h` for usage.  **MATLAB**  There's also a *basic* MATLAB demo  though it's missing some minor bells and whistles compared to the Python version. ```Shell cd $FRCN_ROOT/matlab matlab #: wait for matlab to start...  #: At the matlab prompt  run the script: >> fast_rcnn_demo ```  Fast R-CNN training is implemented in Python only  but test-time detection functionality also exists in MATLAB. See `matlab/fast_rcnn_demo.m` and `matlab/fast_rcnn_im_detect.m` for details.  **Computing object proposals**  The demo uses pre-computed selective search proposals computed with [this code](https://github.com/rbgirshick/rcnn/blob/master/selective_search/selective_search_boxes.m). If you'd like to compute proposals on your own images  there are many options. Here are some pointers; if you run into trouble using these resources please direct questions to the respective authors.  1. Selective Search: [original matlab code](http://disi.unitn.it/~uijlings/MyHomepage/index.php#page=projects1)  [python wrapper](https://github.com/sergeyk/selective_search_ijcv_with_python) 2. EdgeBoxes: [matlab code](https://github.com/pdollar/edges) 3. GOP and LPO: [python code](http://www.philkr.net/) 4. MCG: [matlab code](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/) 5. RIGOR: [matlab code](http://cpl.cc.gatech.edu/projects/RIGOR/)  Apologies if I've left your method off this list. Feel free to contact me and ask for it to be included.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	``` 	 2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```   	 4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. Follow the next sections to download pre-computed object proposals and pre-trained ImageNet models   **Train** a Fast R-CNN detector. For example  train a VGG16 network on VOC 2007 trainval:  ```Shell ./tools/train_net.py --gpu 0 --solver models/VGG16/solver.prototxt \ 	--weights data/imagenet_models/VGG16.v2.caffemodel ```  If you see this error  ``` EnvironmentError: MATLAB command 'matlab' not found. Please add 'matlab' to your PATH. ```  then you need to make sure the `matlab` binary is in your `$PATH`. MATLAB is currently required for PASCAL VOC evaluation.  **Test** a Fast R-CNN detector. For example  test the VGG 16 network on VOC 2007 test:  ```Shell ./tools/test_net.py --gpu 1 --def models/VGG16/test.prototxt \ 	--net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel ```  Test output is written underneath `$FRCN_ROOT/output`.  **Compress** a Fast R-CNN model using truncated SVD on the fully-connected layers:  ```Shell ./tools/compress_net.py --def models/VGG16/test.prototxt \ 	--def-svd models/VGG16/compressed/test.prototxt \     --net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel #: Test the model you just compressed ./tools/test_net.py --gpu 0 --def models/VGG16/compressed/test.prototxt \ 	--net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000_svd_fc6_1024_fc7_256.caffemodel ```   """;Computer Vision;https://github.com/devsoft123/fast-cnn
"""Yolo v4 source code: https://github.com/AlexeyAB/darknet   Useful links: https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&sk=6039748846bbcf1d960c3061542591d7   1.2 make   """;Computer Vision;https://github.com/SeventhBlue/trainDarknet-yolov3
"""We provide a preprocessed version of the IWSLT 2014 German-English translation task used in (Ranzato et al.  2015) [[script]](https://github.com/harvardnlp/BSO/blob/master/data_prep/MT/prepareData.sh). To download the dataset:  ```bash wget http://www.cs.cmu.edu/~pengchey/iwslt2014_ende.zip unzip iwslt2014_ende.zip ```  Running the script will extract a`data/` folder which contains the IWSLT 2014 dataset. The dataset has 150K German-English training sentences. The `data/` folder contains a copy of the public release of the dataset. Files with suffix `*.wmixerprep` are pre-processed versions of the dataset from Ranzato et al.  2015  with long sentences chopped and rared words replaced by a special `<unk>` token. You could use the pre-processed training files for training/developing (or come up with your own pre-processing strategy)  but for testing you have to use the **original** version of testing files  ie.  `test.de-en.(de|en)`.   Each runnable script (`nmt.py`  `vocab.py`) is annotated using `dotopt`. Please refer to the source file for complete usage.  First  we extract a vocabulary file from the training data using the command:  ```bash python vocab.py \     --train-src=data/train.de-en.de.wmixerprep \     --train-tgt=data/train.de-en.en.wmixerprep \     data/vocab.json ```  This generates a vocabulary file `data/vocab.json`.  The script also has options to control the cutoff frequency and the size of generated vocabulary  which you may play with.  To start training and evaluation  simply run `scripts/train.sh`.  After training and decoding  we call the official evaluation script `multi-bleu.perl` to compute the corpus-level BLEU score of the decoding results against the gold-standard.   """;General;https://github.com/pcyin/pytorch_basic_nmt
"""For training: - `python mergan.py --dataset mnist --result_path mnist_SFT/` Sequential Fine Tuning - `python mergan.py --dataset mnist --RA --RA_factor 1e-3  --result_path mnist_RA_1e_3/` MeRGAN Replay Alignment - `python mergan.py --dataset mnist --JTR --result_path mnist_JTR/` MeRGAN Joint Training with Replay - `python joint.py --dataset mnist --result_path mnist_joint/` Joint Training  For testing: - `python mergan.py --dataset mnist --test  --result_path result/mnist_RA_1e_3/` - `python joint.py --dataset mnist --test --result_path result/mnist_joint/`   """;General;https://github.com/WuChenshen/MeRGAN
"""bash ./download_datasets.sh apple2orange  Please ensure that you have the following directory tree structure in your repository.       bash ./scripts/train_base.sh apple2orange   Check the folder name in checkpoints directory (e.g.  apple2orange).       bash ./scripts/test_base.sh apple2orange 2018_10_16_14_49_55   In recent experiments  we found that  spectral normaliation (SN) can help stabilize the training stage. So we add SN in this implementation. You may need to update your pytorch to 0.4.1 to support SN  or use an old version without SN.   """;General;https://github.com/Xiaoming-Yu/SingleGAN
"""- Source code:https://github.com/AlexeyAB/darknet   Download weight   For coco dataset you can use tool/coco_annotatin.py.   Performance on MS COCO val2017 set (using pretrained DarknetWeights from https://github.com/AlexeyAB/darknet)   | Pytorch (416*416) |    0.466         |     0.704   |      0.591   |   Pytorch version Recommended:   Install onnxruntime  pip install onnxruntime   Pytorch version Recommended:   Install onnxruntime  pip install onnxruntime   TensorRT version Recommended: 7.0  7.1   1: Thanks:github:https://github.com/onnx/onnx-tensorflow  2: Run git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow  Run pip install -e .  Note:Errors will occur when using ""pip install onnx-tf""  at least for me it is recommended to use source code installation  1. Compile the DeepStream Nvinfer Plugin   cd DeepStream/nvdsinfer_custom_impl_Yolo        make   """;Computer Vision;https://github.com/160209-wyj/pytorch-yolov4
"""The same learning algorithm was used to train agents for each of the ten OpenAI Gym MuJoCo continuous control environments. The only difference between evaluations was the number of episodes used per training batch  otherwise all options were the same. The exact code used to generate the OpenAI Gym submissions is in the **`aigym_evaluation`** branch.  Here are the key points:  * Proximal Policy Optimization (similar to TRPO  but uses gradient descent with KL loss terms)  \[1\] \[2\] * Value function approximated with 3 hidden-layer NN (tanh activations):     * hid1 size = obs_dim x 10     * hid2 size = geometric mean of hid1 and hid3 sizes     * hid3 size = 5 * Policy is a multi-variate Gaussian parameterized by a 3 hidden-layer NN (tanh activations):     * hid1 size = obs_dim x 10     * hid2 size = geometric mean of hid1 and hid3 sizes     * hid3 size = action_dim x 10     * Diagonal covariance matrix variables are separately trained * Generalized Advantage Estimation (gamma = 0.995  lambda = 0.98) \[3\] \[4\] * ADAM optimizer used for both neural networks * The policy is evaluated for 20 episodes between updates  except:     * 50 episodes for Reacher     * 5 episodes for Swimmer     * 5 episodes for HalfCheetah     * 5 episodes for HumanoidStandup * Value function is trained on current batch + previous batch * KL loss factor and ADAM learning rate are dynamically adjusted during training * Policy and Value NNs built with TensorFlow   """;Reinforcement Learning;https://github.com/magnusja/ppo
"""1. Setup python 3 virtual environment. If you dont have ```virtualenv```  install it with  ``` pip install virtualenv ```  2. Then create the environment with  ``` virtualenv -p $(which python3) env ```  3. Activate the environment  ``` source env/bin/activate ```  4. Install tensorflow  ``` pip install tensorflow==1.8.0 ```  5. Clone the repository  ``` git clone https://gitlab.com/codetendolkar/tacotron-2-explained.git ```  6. Run the training script  ``` cd tacotron2 python train.py ```   1. Encoder-Decoder architectures contain more complexities then standard DNNs. Implementing one helps you master concepts you would otherwise overlook   """;Audio;https://github.com/codetendolkar/tacotron-2-explained
"""```python TRAIN = ""train/"" TEST = ""test/""   #: Load ""X"" (the neural network's training and testing inputs)  def load_X(X_signals_paths):     X_signals = []      for signal_type_path in X_signals_paths:         file = open(signal_type_path  'r')         #: Read dataset from disk  dealing with text files' syntax         X_signals.append(             [np.array(serie  dtype=np.float32) for serie in [                 row.replace('  '  ' ').strip().split(' ') for row in file             ]]         )         file.close()      return np.transpose(np.array(X_signals)  (1  2  0))  X_train_signals_paths = [     DATASET_PATH + TRAIN + ""Inertial Signals/"" + signal + ""train.txt"" for signal in INPUT_SIGNAL_TYPES ] X_test_signals_paths = [     DATASET_PATH + TEST + ""Inertial Signals/"" + signal + ""test.txt"" for signal in INPUT_SIGNAL_TYPES ]  X_train = load_X(X_train_signals_paths) X_test = load_X(X_test_signals_paths)   #: Load ""y"" (the neural network's training and testing outputs)  def load_y(y_path):     file = open(y_path  'r')     #: Read dataset from disk  dealing with text file's syntax     y_ = np.array(         [elem for elem in [             row.replace('  '  ' ').strip().split(' ') for row in file         ]]          dtype=np.int32     )     file.close()      #: Substract 1 to each output class for friendly 0-based indexing     return y_ - 1  y_train_path = DATASET_PATH + TRAIN + ""y_train.txt"" y_test_path = DATASET_PATH + TEST + ""y_test.txt""  y_train = load_y(y_train_path) y_test = load_y(y_test_path)  ```   #: Get LSTM cell output   Instructions for updating:   matplotlib.rc('font'  **font)   ```python #: Note: Linux bash commands start with a ""!"" inside those ""ipython notebook"" cells  DATA_PATH = ""data/""  !pwd && ls os.chdir(DATA_PATH) !pwd && ls  !python download_dataset.py  !pwd && ls os.chdir("".."") !pwd && ls  DATASET_PATH = DATA_PATH + ""UCI HAR Dataset/"" print(""\n"" + ""Dataset is now located at: "" + DATASET_PATH)  ```      /home/ubuntu/pynb/LSTM-Human-Activity-Recognition     data	 LSTM_files  LSTM_OLD.ipynb  README.md     LICENSE  LSTM.ipynb  lstm.py	     screenlog.0     /home/ubuntu/pynb/LSTM-Human-Activity-Recognition/data     download_dataset.py  source.txt      Downloading...     --2017-05-24 01:49:53--  https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip     Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249     Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.     HTTP request sent  awaiting response... 200 OK     Length: 60999314 (58M) [application/zip]     Saving to: ‘UCI HAR Dataset.zip’      100%[======================================>] 60 999 314  1.69MB/s   in 38s          2017-05-24 01:50:31 (1.55 MB/s) - ‘UCI HAR Dataset.zip’ saved [60999314/60999314]      Downloading done.      Extracting...     Extracting successfully done to /home/ubuntu/pynb/LSTM-Human-Activity-Recognition/data/UCI HAR Dataset.     /home/ubuntu/pynb/LSTM-Human-Activity-Recognition/data     download_dataset.py  __MACOSX  source.txt  UCI HAR Dataset  UCI HAR Dataset.zip     /home/ubuntu/pynb/LSTM-Human-Activity-Recognition     data	 LSTM_files  LSTM_OLD.ipynb  README.md     LICENSE  LSTM.ipynb  lstm.py	     screenlog.0      Dataset is now located at: data/UCI HAR Dataset/    ```python  #: Graph input/output x = tf.placeholder(tf.float32  [None  n_steps  n_input]) y = tf.placeholder(tf.float32  [None  n_classes])  #: Graph weights weights = {     'hidden': tf.Variable(tf.random_normal([n_input  n_hidden]))  #: Hidden layer weights     'out': tf.Variable(tf.random_normal([n_hidden  n_classes]  mean=1.0)) } biases = {     'hidden': tf.Variable(tf.random_normal([n_hidden]))      'out': tf.Variable(tf.random_normal([n_classes])) }  pred = LSTM_RNN(x  weights  biases)  #: Loss  optimizer and evaluation l2 = lambda_loss_amount * sum(     tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables() ) #: L2 loss prevents this overkill neural network to overfit the data cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y  logits=pred)) + l2 #: Softmax loss optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) #: Adam Optimizer  correct_pred = tf.equal(tf.argmax(pred 1)  tf.argmax(y 1)) accuracy = tf.reduce_mean(tf.cast(correct_pred  tf.float32))  ```   """;Natural Language Processing;https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition
"""The repo has been forked initially from https://github.com/Diego999/pyGAT by   """;Graphs;https://github.com/weiyangfb/PyTorchSparseGAT
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;Computer Vision;https://github.com/Uemuet/imet
