# NMT Practice-1: Pytorch Implementation of the Transformer model in 《Attention is all you need》
Hi, there, this is just a personal practice project of implementing SOTA NMT model -- Transformer in [Attention is All You Need](https://arxiv.org/abs/1706.03762)" (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, arxiv, 2017).

# Requirement
- python 3.6+
- pytorch 0.4.1+
- tqdm
- numpy

<p align="center">
<img src="http://imgur.com/1krF2R6.png" width="250">
</p>

# Usage

## script description:
- parameters.py contains the parameter environment setting
- Transformer package comtains the main architecture of Transformer Encoder-Decoder Model
- to be continued





# Acknowledgement
- some scripts and the dataset preprocessing steps of the project are borrowed from [OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py).
- main structure and useful functions are borrowed from [jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch)

