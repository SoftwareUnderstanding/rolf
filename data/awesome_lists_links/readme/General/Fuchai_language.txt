## How to use?

### Scraping with the node.js app

Run `node /scraper/main_scraper.js`. Requires npm install `@octokit/rest`.

Modify the `/scraper/stack file` to list the users to start scraping with. The results are periodically saved in 
scraper/items as numbered json files.

It calls the octokit github API to get followers of the users in the stack file. It knows to detect users that
have been scraped.



### Sort the scripts

Run `/runclone.py`. It will clone and sort the files into /trdata directory. It knows to keep
track of all languages, and make the data for each langauge relatively the same.

It will pick up where you left off. Set reset=True to sort from the beginning.

### Serialize the data

Run serialization with `/runrepickle.py`. Caution! Very slow due to large number of small file I/O.

Data serialization is important, because usually your training files contains thousands of small files.
Your file access will be limited by hard-drive file system access.

You need to setup the serialized data path manually. Put it in your SSD.

Serialization is a compression algorithm based on the frequency of the words. The compression dictionary needs to be
manually generated by `running generate_dictionary()` at `/deeplearning/preprocessing.py`. This whole process will take
a few hours.

### Train the model

Run `/runtrain.py`.

The model is a state of art mixed objective model. It has an accuracy of 96% on balanced validation set at the moment.
It's a mixed objective model modified with Transformer's encoder. It trains very fast on serialized data.

https://www.aaai.org/Papers/AAAI/2019/AAAI-SachanD.7236.pdf

https://arxiv.org/abs/1706.03762

Training knows to pick up where you left off, logs automatically, saves model periodically, prints variuos statistics,
 etc. Everything works.

### Tests

You can find tests parallel to the modules being tested. The tests do not assure the quality of the models.

### Model Release

The trained model can be found in the default model loading directory: `/deeplearning/saves`. It is around 1MB.
