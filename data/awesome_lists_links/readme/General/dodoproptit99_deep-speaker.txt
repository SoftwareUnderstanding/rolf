# deep-speaker
## Speaker identification with Deep Speaker 

Dataset: Private

Reference Paper: https://arxiv.org/pdf/1705.02304.pdf

Reference Code: https://github.com/philipperemy/deep-speaker

## Introduction
The embeddings generated by Deep Speaker can be used for many tasks, including speaker identification, verification, and clustering. We experiment with ResCNN architectures to extract the acoustic features, then mean pool to produce utterance-level speaker embeddings, and train using triplet loss based on cosine similarity.

## About my code
This part i will use vietnamese, if i have time in the future, i will translate it to english.

#### ```pre_process.py```

#### Mục đích: Xử lý file wav. Trích chọn đặc trưng file raw audio, lưu dưới dạng file numpy array.

#### Chi tiết: 

  ```def VAD(audio)``` : Hàm tiền xử lí audio, lọc âm trắng theo từng chunk_size (s).
  
  ```def read_audio(filename, sample_rate=SAMPLE_RATE)``` : Hàm đọc file raw audio, dùng thư viện librosa để load
  âm thanh với tần số lấy mẫu là SAMPLE_RATE = 16000. Sample_rate là gì? Vậy tại sao không lấy mẫu tại 44000? 
  Tham khảo [tại đây.](https://librosa.github.io/blog/2019/07/17/resample-on-load/) 
  
  ```def normalize_frames(m,epsilon=1e-12)``` : Hàm chuẩn hóa đầu vào mạng deep learning, nó cho phép sử dụng tốt hơn 
  learning rate, khởi tạo hiệu quả hơn. Chi tiết tại đây: [Batch normalize](https://arxiv.org/pdf/1502.03167.pdf)
  
  ```def extract_features(signal, target_sample_rate=SAMPLE_RATE)``` : 
  Bước đầu tiên trong bất kỳ hệ thống nhận dạng giọng nói tự động nào là trích xuất các tính năng, 
  tức là xác định các thành phần của tín hiệu âm thanh tốt để xác định nội dung ngôn ngữ và 
  loại bỏ tất cả các nội dung khác mang thông tin như nhiễu nền, cảm xúc, v.v. Để hiểu rõ hơn về mel filterbank, 
  tham khảo [MFCC](http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)
  
  Sau khi qua hàm này, raw audio trở thành 1 np.array với shape (num_frames, nfilt(trong fbank), 1)
  
#### ```models.py```

#### Chi tiết:

Model được implement lại giống y hệt trong paper, sử dụng 4 res_block với số lượng filters lần lượt là 64, 128, 256, 512 
để trích chọn đặc trưng của tín hiệu âm thanh.


#### ```random_batch.py```

#### Mục đích: Tạo batch random để làm input cho việc train những batch đầu.

#### Chi tiết:

  ```def clipped_audio(audio)``` : Hàm chuẩn hóa audio, cắt độ dài audio về đúng num_frames = 160.
  
  ```def random_batch``` : Model train triplet loss cần input như sau: 1 file anchor, 1 file positive, 1 file negative. Hàm này sẽ lấy ra input theo kiểu chọn ngẫu nhiên từ thư mục chứa data, cách thức như sau:
  
  Lấy ra ```libri``` lưu lại tên speaker và filename trong thư mục chứa data_train, chọn random 1 speaker làm anchor, lấy 2 file .npy thuộc anchor này. Chọn random 1 speaker khác làm negative_speaker, lấy 1 file .npy thuộc speaker này. 
  
#### ```select_batch.py```

#### Mục đích: Tạo input cho việc train triplet loss được hiệu quả và hội tụ nhanh hơn. Các batch sẽ được chọn theo phương pháp sau: Chọn ngẫu nhiên 1/2 speaker, tính cosine_similarity của tất cả file thuộc speaker được chọn với tất cả 

#### Chi tiết: 

  ``` def matrix_cosine_similarity(x1, x2)```: So sánh khoảng cách cosine giữa 2 embedding. Giá trị trả về nằm trong khoảng [-1;1]. Sims càng cao nghĩa là 2 embedding đó thuộc về cùng 1 speaker.
  
  
