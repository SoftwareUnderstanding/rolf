{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from os import remove\n",
    "import pandas as pd\n",
    "#from keras.preprocessing.text import text_to_word_sequence\n",
    "from numpy import loadtxt\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "\n",
    "TEXT = 'Text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(data : pd.DataFrame) -> None:\n",
    "\tdata = data\n",
    "\n",
    "def remove_stop_words(text : str, stop_words):\n",
    "\t'''\n",
    "\tFunction to remove a list of words\n",
    "\t@param x : (str) text \n",
    "\t@param stop_word: (list) list of stopwords to delete \n",
    "\t@return: (str) new string without stopwords \n",
    "\t'''\n",
    "\n",
    "\ttoken_list = word_tokenize(text)\t# tokenize text \n",
    "\treturn [token for token in token_list if token not in stop_words]\n",
    "\t\n",
    "def remove_codeblocks(text):\n",
    "\treturn re.sub('```.*?```', ' ', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "\t#print(string.punctuation)\n",
    "\t#print('Pre: ', text)\n",
    "\t#punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "\tres = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\t#print('Post: ', res)\n",
    "\t#exit(0)\n",
    "\treturn res\n",
    "\t#return ''.join(new_text)\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "\tencoded_string = text.encode(\"ascii\", \"ignore\")\n",
    "\treturn encoded_string.decode()\n",
    "\n",
    "def stemming(text, porter_stemmer):\n",
    "\tstem_text = [porter_stemmer.stem(word) for word in text]\n",
    "\treturn stem_text\n",
    "\n",
    "def lemmatizer(text, wordnet_lemmatizer):\n",
    "\tlemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "\treturn lemm_text\n",
    "\n",
    "def remove_one_char_and_number_words(text):\n",
    "\tres = [word for word in text if word.isdigit() == False and len(word) > 1]\n",
    "\treturn res\n",
    "\n",
    "def remove_links(text):\n",
    "\tregex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "\treturn (re.sub(regex, '', text))\n",
    "\n",
    "def remove_links2(text):\n",
    "\treturn ' '.join([token for token in text.split(' ') if 'http' not in token])\n",
    "\n",
    "def get_keys(self,text, l):\n",
    "\tdict1 = {}\n",
    "\tfor eachStr in text:\n",
    "\t\tif eachStr in dict1.keys():\n",
    "\t\t\tcount = dict1[eachStr]\n",
    "\t\t\tcount = count + 1\n",
    "\t\t\tdict1[eachStr.lower()] = count\n",
    "\t\telse: dict1[eachStr.lower()] = 1\n",
    "\tremekys = []\n",
    "\tfor key in dict1:\n",
    "\t\tif dict1[key] < l or len(key) <= 2:\n",
    "\t\t\tremekys.append(key)\n",
    "\tfor key in remekys:\n",
    "\t\tdel dict1[key]\n",
    "\treturn ' '.join(list(dict1.keys()))\n",
    "\n",
    "def keep_only_common(text):\n",
    "\treturn [token for token in text if token in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"# RecAdam    ## Introduction    We provide **RecAdam** (Recall Adam) optimizer to facilitate fine-tuning deep pretrained language models (e.g., BERT, ALBERT) with less forgetting.    For a detailed description and experimental results, please refer to our paper: [Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting](https://www.aclweb.org/anthology/2020.emnlp-main.634/) (Accepted by EMNLP 2020).    ## Environment    ```bash  python >= 3.6  pytorch >= 1.0.0  transformers >= 2.5.1  ```  ## Files    - `RecAdam.py`: this file includes the RecAdam optimizer implementation,   which is modified from AdamW optimizer implementation [`optimization.py`](https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py) by [Huggingface Transformers](https://github.com/huggingface/transformers).    - `run_glue_with_RecAdam.py`: this file is an example to run GLUE tasks with RecAdam optimizer,  and is modified from the GLUE example [`run_glue.py`](https://github.com/huggingface/transformers/blob/c44a17db1b87e31ad4c232e48d19a2700e8b690d/examples/run_glue.py) by [Huggingface Transformers](https://github.com/huggingface/transformers).     ## Run GLUE tasks    GLUE tasks can be download from  [GLUE data](https://gluebenchmark.com/tasks) by running  [this script](https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e)  and unpacked to some directory `$GLUE_DIR`.    ### With ALBERT-xxlarge model    For ALBERT-xxlarge, we use the same hyperparameters following [ALBERT paper](https://arxiv.org/pdf/1909.11942.pdf),  except for the maximum sequence length, which we set to 128 rather than 512.    As for the hyperparameters of RecAdam,   we choose the sigmoid annealing function,  set the coefficient of the quadratic penalty to 5,000,   select the best k in \\{0.05, 0.1, 0.2, 0.5, 1\\},  select the best t_0 in \\{100, 250, 500\\} for small tasks and \\{250, 500, 1,000} for large tasks.    Here is an example script to get started:  ```bash  export GLUE_DIR=/path/to/glue  export TASK_NAME=CoLA    python run_glue_with_RecAdam.py \\    --model_type albert \\    --model_name_or_path /path/to/model \\    --log_path /path/to/log \\    --task_name $TASK_NAME \\    --do_train \\    --do_eval \\    --do_lower_case \\    --data_dir $GLUE_DIR/$TASK_NAME \\    --max_seq_length 128 \\    --per_gpu_train_batch_size 16 \\    --learning_rate 1e-5 \\    --warmup_steps 320 \\    --max_steps 5336 \\    --output_dir /path/to/output/$TASK_NAME/ \\    --evaluate_during_training \\    --train_logging_steps 25 \\    --eval_logging_steps 100 \\    --albert_dropout 0.0 \\    --optimizer RecAdam \\    --recadam_anneal_fun sigmoid \\    --recadam_anneal_t0 1000 \\    --recadam_anneal_k 0.1 \\    --recadam_pretrain_cof 5000.0 \\    --logging_Euclid_dist   ```    ### With BERT-base model    For BERT-base, we use the same hyperparameters following [BERT paper](https://arxiv.org/pdf/1810.04805.pdf).  We set the learning rate to 2e-5, and find that the model has not converged on each GLUE task after 3 epochs fine-tuning.  To make sure the convergence of vanilla fine-tuning, we increase the training step for each task   (61,360 on MNLI, 56,855 on QQP, 33,890 on QNLI, 21,050 on SST, 13,400 on CoLA, 9,000 on STS, 11,500 on MRPC, 7,800 on RTE),   and achieve better baseline scores on the dev set of GLUE benchmark.      As for the hyperparameters of RecAdam,   we choose the sigmoid annealing function,  set the coefficient of the quadratic penalty to 5,000,   select the best k and t_0 in \\{0.05, 0.1, 0.2, 0.5, 1\\} and \\{250, 500, 1,000\\} respectively.     Here is an example script to get started:  ```bash  export GLUE_DIR=/path/to/glue  export TASK_NAME=STS-B    python run_glue_with_RecAdam.py \\    --model_type bert \\    --model_name_or_path /path/to/model \\    --log_path /path/to/log \\    --task_name $TASK_NAME \\    --do_train \\    --do_eval \\    --do_lower_case \\    --data_dir $GLUE_DIR/$TASK_NAME \\    --max_seq_length 128 \\    --per_gpu_train_batch_size 32 \\    --learning_rate 2e-5 \\    --max_steps 9000 \\    --output_dir /path/to/output/$TASK_NAME/ \\    --evaluate_during_training \\    --train_logging_steps 50 \\    --eval_logging_steps 180 \\    --optimizer RecAdam \\    --recadam_anneal_fun sigmoid \\    --recadam_anneal_t0 1000 \\    --recadam_anneal_k 0.1 \\    --recadam_pretrain_cof 5000.0 \\    --logging_Euclid_dist   ```    ### Citation  If you find RecAdam useful, please cite [our paper](https://www.aclweb.org/anthology/2020.emnlp-main.634/):  ```bibtex  @inproceedings{recadam,      title = \"Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting\",      author = \"Chen, Sanyuan  and  Hou, Yutai  and  Cui, Yiming  and  Che, Wanxiang  and  Liu, Ting  and  Yu, Xiangzhan\",      booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",      month = nov,      year = \"2020\",      address = \"Online\",      publisher = \"Association for Computational Linguistics\",      url = \"https://www.aclweb.org/anthology/2020.emnlp-main.634\",      pages = \"7870--7881\",  }  ``` \"\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/readme.csv', sep=';')\n",
    "print(data[TEXT][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"# RecAdam    ## Introduction    We provide **RecAdam** (Recall Adam) optimizer to facilitate fine-tuning deep pretrained language models (e.g., BERT, ALBERT) with less forgetting.    For a detailed description and experimental results, please refer to our paper: [Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting](https://www.aclweb.org/anthology/2020.emnlp-main.634/) (Accepted by EMNLP 2020).    ## Environment       ## Files    - `RecAdam.py`: this file includes the RecAdam optimizer implementation,   which is modified from AdamW optimizer implementation [`optimization.py`](https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py) by [Huggingface Transformers](https://github.com/huggingface/transformers).    - `run_glue_with_RecAdam.py`: this file is an example to run GLUE tasks with RecAdam optimizer,  and is modified from the GLUE example [`run_glue.py`](https://github.com/huggingface/transformers/blob/c44a17db1b87e31ad4c232e48d19a2700e8b690d/examples/run_glue.py) by [Huggingface Transformers](https://github.com/huggingface/transformers).     ## Run GLUE tasks    GLUE tasks can be download from  [GLUE data](https://gluebenchmark.com/tasks) by running  [this script](https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e)  and unpacked to some directory `$GLUE_DIR`.    ### With ALBERT-xxlarge model    For ALBERT-xxlarge, we use the same hyperparameters following [ALBERT paper](https://arxiv.org/pdf/1909.11942.pdf),  except for the maximum sequence length, which we set to 128 rather than 512.    As for the hyperparameters of RecAdam,   we choose the sigmoid annealing function,  set the coefficient of the quadratic penalty to 5,000,   select the best k in \\{0.05, 0.1, 0.2, 0.5, 1\\},  select the best t_0 in \\{100, 250, 500\\} for small tasks and \\{250, 500, 1,000} for large tasks.    Here is an example script to get started:       ### With BERT-base model    For BERT-base, we use the same hyperparameters following [BERT paper](https://arxiv.org/pdf/1810.04805.pdf).  We set the learning rate to 2e-5, and find that the model has not converged on each GLUE task after 3 epochs fine-tuning.  To make sure the convergence of vanilla fine-tuning, we increase the training step for each task   (61,360 on MNLI, 56,855 on QQP, 33,890 on QNLI, 21,050 on SST, 13,400 on CoLA, 9,000 on STS, 11,500 on MRPC, 7,800 on RTE),   and achieve better baseline scores on the dev set of GLUE benchmark.      As for the hyperparameters of RecAdam,   we choose the sigmoid annealing function,  set the coefficient of the quadratic penalty to 5,000,   select the best k and t_0 in \\{0.05, 0.1, 0.2, 0.5, 1\\} and \\{250, 500, 1,000\\} respectively.     Here is an example script to get started:       ### Citation  If you find RecAdam useful, please cite [our paper](https://www.aclweb.org/anthology/2020.emnlp-main.634/):    \"\n"
     ]
    }
   ],
   "source": [
    "NEWCOLNAME = TEXT\n",
    "\t\t\n",
    "#Remove codeblocks\n",
    "data[NEWCOLNAME] = data[TEXT].apply(lambda x: remove_codeblocks(x))\n",
    "\n",
    "print(data[TEXT][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"# RecAdam    ## Introduction    We provide **RecAdam** (Recall Adam) optimizer to facilitate fine-tuning deep pretrained language models (e.g., BERT, ALBERT) with less forgetting.    For a detailed description and experimental results, please refer to our paper: [Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less (Accepted by EMNLP 2020).    ## Environment       ## Files    - `RecAdam.py`: this file includes the RecAdam optimizer implementation,   which is modified from AdamW optimizer implementation by [Huggingface    - `run_glue_with_RecAdam.py`: this file is an example to run GLUE tasks with RecAdam optimizer,  and is modified from the GLUE example by [Huggingface     ## Run GLUE tasks    GLUE tasks can be download from  [GLUE by running  [this  and unpacked to some directory `$GLUE_DIR`.    ### With ALBERT-xxlarge model    For ALBERT-xxlarge, we use the same hyperparameters following [ALBERT  except for the maximum sequence length, which we set to 128 rather than 512.    As for the hyperparameters of RecAdam,   we choose the sigmoid annealing function,  set the coefficient of the quadratic penalty to 5,000,   select the best k in \\{0.05, 0.1, 0.2, 0.5, 1\\},  select the best t_0 in \\{100, 250, 500\\} for small tasks and \\{250, 500, 1,000} for large tasks.    Here is an example script to get started:       ### With BERT-base model    For BERT-base, we use the same hyperparameters following [BERT  We set the learning rate to 2e-5, and find that the model has not converged on each GLUE task after 3 epochs fine-tuning.  To make sure the convergence of vanilla fine-tuning, we increase the training step for each task   (61,360 on MNLI, 56,855 on QQP, 33,890 on QNLI, 21,050 on SST, 13,400 on CoLA, 9,000 on STS, 11,500 on MRPC, 7,800 on RTE),   and achieve better baseline scores on the dev set of GLUE benchmark.      As for the hyperparameters of RecAdam,   we choose the sigmoid annealing function,  set the coefficient of the quadratic penalty to 5,000,   select the best k and t_0 in \\{0.05, 0.1, 0.2, 0.5, 1\\} and \\{250, 500, 1,000\\} respectively.     Here is an example script to get started:       ### Citation  If you find RecAdam useful, please cite [our    \"\n"
     ]
    }
   ],
   "source": [
    "#Remove links\n",
    "data[NEWCOLNAME] = data[NEWCOLNAME].apply(lambda x : remove_links2(x))\n",
    "print(data[TEXT][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RecAdam       Introduction    We provide   RecAdam    Recall Adam  optimizer to facilitate fine tuning deep pretrained language models  e g   BERT  ALBERT  with less forgetting     For a detailed description and experimental results  please refer to our paper   Recall and Learn  Fine tuning Deep Pretrained Language Models with Less  Accepted by EMNLP 2020         Environment          Files       RecAdam py   this file includes the RecAdam optimizer implementation    which is modified from AdamW optimizer implementation by  Huggingface       run_glue_with_RecAdam py   this file is an example to run GLUE tasks with RecAdam optimizer   and is modified from the GLUE example by  Huggingface        Run GLUE tasks    GLUE tasks can be download from   GLUE by running   this  and unpacked to some directory   GLUE_DIR          With ALBERT xxlarge model    For ALBERT xxlarge  we use the same hyperparameters following  ALBERT  except for the maximum sequence length  which we set to 128 rather than 512     As for the hyperparameters of RecAdam    we choose the sigmoid annealing function   set the coefficient of the quadratic penalty to 5 000    select the best k in   0 05  0 1  0 2  0 5  1     select the best t_0 in   100  250  500   for small tasks and   250  500  1 000  for large tasks     Here is an example script to get started            With BERT base model    For BERT base  we use the same hyperparameters following  BERT  We set the learning rate to 2e 5  and find that the model has not converged on each GLUE task after 3 epochs fine tuning   To make sure the convergence of vanilla fine tuning  we increase the training step for each task    61 360 on MNLI  56 855 on QQP  33 890 on QNLI  21 050 on SST  13 400 on CoLA  9 000 on STS  11 500 on MRPC  7 800 on RTE     and achieve better baseline scores on the dev set of GLUE benchmark       As for the hyperparameters of RecAdam    we choose the sigmoid annealing function   set the coefficient of the quadratic penalty to 5 000    select the best k and t_0 in   0 05  0 1  0 2  0 5  1   and   250  500  1 000   respectively      Here is an example script to get started            Citation  If you find RecAdam useful  please cite  our     \n"
     ]
    }
   ],
   "source": [
    "#Remove punctuation\n",
    "data[NEWCOLNAME]= data[NEWCOLNAME].apply(lambda x: remove_punctuation(x))\n",
    "print(data[TEXT][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   recadam       introduction    we provide   recadam    recall adam  optimizer to facilitate fine tuning deep pretrained language models  e g   bert  albert  with less forgetting     for a detailed description and experimental results  please refer to our paper   recall and learn  fine tuning deep pretrained language models with less  accepted by emnlp 2020         environment          files       recadam py   this file includes the recadam optimizer implementation    which is modified from adamw optimizer implementation by  huggingface       run_glue_with_recadam py   this file is an example to run glue tasks with recadam optimizer   and is modified from the glue example by  huggingface        run glue tasks    glue tasks can be download from   glue by running   this  and unpacked to some directory   glue_dir          with albert xxlarge model    for albert xxlarge  we use the same hyperparameters following  albert  except for the maximum sequence length  which we set to 128 rather than 512     as for the hyperparameters of recadam    we choose the sigmoid annealing function   set the coefficient of the quadratic penalty to 5 000    select the best k in   0 05  0 1  0 2  0 5  1     select the best t_0 in   100  250  500   for small tasks and   250  500  1 000  for large tasks     here is an example script to get started            with bert base model    for bert base  we use the same hyperparameters following  bert  we set the learning rate to 2e 5  and find that the model has not converged on each glue task after 3 epochs fine tuning   to make sure the convergence of vanilla fine tuning  we increase the training step for each task    61 360 on mnli  56 855 on qqp  33 890 on qnli  21 050 on sst  13 400 on cola  9 000 on sts  11 500 on mrpc  7 800 on rte     and achieve better baseline scores on the dev set of glue benchmark       as for the hyperparameters of recadam    we choose the sigmoid annealing function   set the coefficient of the quadratic penalty to 5 000    select the best k and t_0 in   0 05  0 1  0 2  0 5  1   and   250  500  1 000   respectively      here is an example script to get started            citation  if you find recadam useful  please cite  our     \n"
     ]
    }
   ],
   "source": [
    "#Remove non ascii\n",
    "#data[NEWCOLNAME]= data[NEWCOLNAME].apply(lambda x: remove_non_ascii(x))\n",
    "#This does not improve the prformance at all and it is pretty slow\n",
    "\n",
    "#Transfor to lowercase\n",
    "data[NEWCOLNAME] = data[NEWCOLNAME].apply(lambda x: x.lower())\n",
    "print(data[TEXT][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'network', 'install', 'run', 'file', 'used', 'result', 'paper', 'python', 'using', 'code', 'model', 'training', 'implementation', 'use']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_46302/2807967690.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'network'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'used'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'result'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'paper'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'using'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'code'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'implementation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'use'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNEWCOLNAME\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNEWCOLNAME\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_46302/2807967690.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'network'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'used'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'result'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'paper'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'using'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'code'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'implementation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'use'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNEWCOLNAME\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNEWCOLNAME\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_46302/4126987299.py\u001b[0m in \u001b[0;36mremove_stop_words\u001b[0;34m(text, stop_words)\u001b[0m\n\u001b[1;32m     10\u001b[0m \t'''\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtoken_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# tokenize text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \"\"\"\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \"\"\"\n\u001b[0;32m-> 1334\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \"\"\"\n\u001b[0;32m-> 1334\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \"\"\"\n\u001b[1;32m   1364\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after_tok\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "#Remove stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['network', 'install', 'run', 'file', 'used', 'result', 'paper', 'python', 'using', 'code', 'model', 'training', 'implementation', 'use', 'please']\n",
    "print(stop_words)\n",
    "data[NEWCOLNAME] = data[NEWCOLNAME].apply(lambda x : remove_stop_words(x, stop_words))\n",
    "\n",
    "print(data[TEXT][2])\n",
    "\n",
    "#Keep only common words\n",
    "#data[NEWCOLNAME] = data[NEWCOLNAME].apply(lambda x : keep_only_common(x))\n",
    "\n",
    "#Stemming\n",
    "#porter_stemmer = PorterStemmer()\n",
    "#data[NEWCOLNAME]=data[NEWCOLNAME].apply(lambda x: stemming(x, porter_stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recadam', 'introduction', 'provide', 'recadam', 'recall', 'adam', 'optimizer', 'facilitate', 'fine', 'tuning', 'deep', 'pretrained', 'language', 'models', 'e', 'g', 'bert', 'albert', 'less', 'forgetting', 'detailed', 'description', 'experimental', 'results', 'please', 'refer', 'recall', 'learn', 'fine', 'tuning', 'deep', 'pretrained', 'language', 'models', 'less', 'accepted', 'emnlp', '2020', 'environment', 'files', 'recadam', 'py', 'includes', 'recadam', 'optimizer', 'modified', 'adamw', 'optimizer', 'huggingface', 'run_glue_with_recadam', 'py', 'example', 'glue', 'tasks', 'recadam', 'optimizer', 'modified', 'glue', 'example', 'huggingface', 'glue', 'tasks', 'glue', 'tasks', 'download', 'glue', 'running', 'unpacked', 'directory', 'glue_dir', 'albert', 'xxlarge', 'albert', 'xxlarge', 'hyperparameters', 'following', 'albert', 'except', 'maximum', 'sequence', 'length', 'set', '128', 'rather', '512', 'hyperparameters', 'recadam', 'choose', 'sigmoid', 'annealing', 'function', 'set', 'coefficient', 'quadratic', 'penalty', '5', '000', 'select', 'best', 'k', '0', '05', '0', '1', '0', '2', '0', '5', '1', 'select', 'best', 't_0', '100', '250', '500', 'small', 'tasks', '250', '500', '1', '000', 'large', 'tasks', 'example', 'script', 'get', 'started', 'bert', 'base', 'bert', 'base', 'hyperparameters', 'following', 'bert', 'set', 'learning', 'rate', '2e', '5', 'find', 'converged', 'glue', 'task', '3', 'epochs', 'fine', 'tuning', 'make', 'sure', 'convergence', 'vanilla', 'fine', 'tuning', 'increase', 'step', 'task', '61', '360', 'mnli', '56', '855', 'qqp', '33', '890', 'qnli', '21', '050', 'sst', '13', '400', 'cola', '9', '000', 'sts', '11', '500', 'mrpc', '7', '800', 'rte', 'achieve', 'better', 'baseline', 'scores', 'dev', 'set', 'glue', 'benchmark', 'hyperparameters', 'recadam', 'choose', 'sigmoid', 'annealing', 'function', 'set', 'coefficient', 'quadratic', 'penalty', '5', '000', 'select', 'best', 'k', 't_0', '0', '05', '0', '1', '0', '2', '0', '5', '1', '250', '500', '1', '000', 'respectively', 'example', 'script', 'get', 'started', 'citation', 'find', 'recadam', 'useful', 'please', 'cite']\n"
     ]
    }
   ],
   "source": [
    "print(data[TEXT][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recadam', 'introduction', 'provide', 'recadam', 'recall', 'adam', 'optimizer', 'facilitate', 'fine', 'tuning', 'deep', 'pretrained', 'language', 'models', 'bert', 'albert', 'less', 'forgetting', 'detailed', 'description', 'experimental', 'results', 'please', 'refer', 'recall', 'learn', 'fine', 'tuning', 'deep', 'pretrained', 'language', 'models', 'less', 'accepted', 'emnlp', 'environment', 'files', 'recadam', 'py', 'includes', 'recadam', 'optimizer', 'modified', 'adamw', 'optimizer', 'huggingface', 'run_glue_with_recadam', 'py', 'example', 'glue', 'tasks', 'recadam', 'optimizer', 'modified', 'glue', 'example', 'huggingface', 'glue', 'tasks', 'glue', 'tasks', 'download', 'glue', 'running', 'unpacked', 'directory', 'glue_dir', 'albert', 'xxlarge', 'albert', 'xxlarge', 'hyperparameters', 'following', 'albert', 'except', 'maximum', 'sequence', 'length', 'set', 'rather', 'hyperparameters', 'recadam', 'choose', 'sigmoid', 'annealing', 'function', 'set', 'coefficient', 'quadratic', 'penalty', 'select', 'best', 'select', 'best', 't_0', 'small', 'tasks', 'large', 'tasks', 'example', 'script', 'get', 'started', 'bert', 'base', 'bert', 'base', 'hyperparameters', 'following', 'bert', 'set', 'learning', 'rate', '2e', 'find', 'converged', 'glue', 'task', 'epochs', 'fine', 'tuning', 'make', 'sure', 'convergence', 'vanilla', 'fine', 'tuning', 'increase', 'step', 'task', 'mnli', 'qqp', 'qnli', 'sst', 'cola', 'sts', 'mrpc', 'rte', 'achieve', 'better', 'baseline', 'scores', 'dev', 'set', 'glue', 'benchmark', 'hyperparameters', 'recadam', 'choose', 'sigmoid', 'annealing', 'function', 'set', 'coefficient', 'quadratic', 'penalty', 'select', 'best', 't_0', 'respectively', 'example', 'script', 'get', 'started', 'citation', 'find', 'recadam', 'useful', 'please', 'cite']\n"
     ]
    }
   ],
   "source": [
    "data[NEWCOLNAME]=data[NEWCOLNAME].apply(lambda x: remove_one_char_and_number_words(x))\n",
    "before_lem = data[TEXT][2]\n",
    "print(data[TEXT][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recadam', 'introduction', 'provide', 'recadam', 'recall', 'adam', 'optimizer', 'facilitate', 'fine', 'tuning', 'deep', 'pretrained', 'language', 'model', 'bert', 'albert', 'le', 'forgetting', 'detailed', 'description', 'experimental', 'result', 'please', 'refer', 'recall', 'learn', 'fine', 'tuning', 'deep', 'pretrained', 'language', 'model', 'le', 'accepted', 'emnlp', 'environment', 'file', 'recadam', 'py', 'includes', 'recadam', 'optimizer', 'modified', 'adamw', 'optimizer', 'huggingface', 'run_glue_with_recadam', 'py', 'example', 'glue', 'task', 'recadam', 'optimizer', 'modified', 'glue', 'example', 'huggingface', 'glue', 'task', 'glue', 'task', 'download', 'glue', 'running', 'unpacked', 'directory', 'glue_dir', 'albert', 'xxlarge', 'albert', 'xxlarge', 'hyperparameters', 'following', 'albert', 'except', 'maximum', 'sequence', 'length', 'set', 'rather', 'hyperparameters', 'recadam', 'choose', 'sigmoid', 'annealing', 'function', 'set', 'coefficient', 'quadratic', 'penalty', 'select', 'best', 'select', 'best', 't_0', 'small', 'task', 'large', 'task', 'example', 'script', 'get', 'started', 'bert', 'base', 'bert', 'base', 'hyperparameters', 'following', 'bert', 'set', 'learning', 'rate', '2e', 'find', 'converged', 'glue', 'task', 'epoch', 'fine', 'tuning', 'make', 'sure', 'convergence', 'vanilla', 'fine', 'tuning', 'increase', 'step', 'task', 'mnli', 'qqp', 'qnli', 'sst', 'cola', 'sts', 'mrpc', 'rte', 'achieve', 'better', 'baseline', 'score', 'dev', 'set', 'glue', 'benchmark', 'hyperparameters', 'recadam', 'choose', 'sigmoid', 'annealing', 'function', 'set', 'coefficient', 'quadratic', 'penalty', 'select', 'best', 't_0', 'respectively', 'example', 'script', 'get', 'started', 'citation', 'find', 'recadam', 'useful', 'please', 'cite']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "data[NEWCOLNAME]=data[NEWCOLNAME].apply(lambda x: lemmatizer(x, wordnet_lemmatizer))\n",
    "\n",
    "after_lem = data[TEXT][2]\n",
    "print(data[TEXT][2])\n",
    "\n",
    "# Remove meaningless words\n",
    "#l = len(data[NEWCOLNAME])\n",
    "#data[NEWCOLNAME]=data[NEWCOLNAME].apply(lambda x: get_keys(x, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before and not after\n",
      "{'files', 'scores', 'models', 'less', 'results', 'tasks', 'epochs'}\n",
      "After and not before\n",
      "{'epoch', 'score', 'model', 'file', 'result', 'le'}\n"
     ]
    }
   ],
   "source": [
    "print('Before and not after')\n",
    "print(set(before_lem) - set(after_lem))\n",
    "print('After and not before')\n",
    "print(set(after_lem) - set(before_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join tokens\n",
    "data[NEWCOLNAME]=data[NEWCOLNAME].apply(lambda x: ' '.join(x))\n",
    "#print('Final: \\n', data.head())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d5a85906b5019a95b914e054e1849c800da3b42d1a581ee9f65c61f2a58cefa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
