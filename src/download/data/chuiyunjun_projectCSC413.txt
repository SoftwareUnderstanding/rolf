# Two new Adam optimizers, AdaBelief and Padam
**Authors: Haozhe Sun, Chi Zhang, Zhaoyang Zhu

## | [Report](./writeup/report.pdf) |

The project analyzes two new adam optimizers, [AdaBelief](https://juntang-zhuang.github.io/adabelief/) and [Padam](https://arxiv.org/pdf/1806.06763.pdf)([source code](https://github.com/uclaml/Padam)), and compare them with other conventional optimizers ([Adam](https://arxiv.org/abs/1412.6980?source=post_page---------------------------), [SGD + Momentum](https://ruder.io/optimizing-gradient-descent/index.html)) in the scenario of image classification. We examine the performance of these optimization algorithms on simplified versions of AlexNet, VGGNet, ResNet using the [EMNIST](https://arxiv.org/abs/1702.05373) dataset.
