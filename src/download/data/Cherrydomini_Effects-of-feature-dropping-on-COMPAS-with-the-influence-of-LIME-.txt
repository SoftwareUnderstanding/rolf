# Effects-of-feature-dropping-on-COMPAS-with-the-influence-of-LIME-
The paper and project for the IJCC(International Joint Conference on Convergence) conference February 2021

Abstract 
	The current status of a prevalent model being used in the justice systems, COMPAS, only has an average accuracy of 65% accuracy and 0.7 AUC, which is much too low for a field that is affecting thousands of lives and meant to keep people unable to operate productively in society in a separate environment. The prevalence of this model in the US, despite its low accuracy and bias against certain groups of people is putting an unnecessarily large flag on people belonging to sensitive groups (people that are being biased against) and not flagging people in the other groups high enough, allowing them to slip from notice in the system and continue to cause unnecessary harm in society. The model and its purpose are very important and have the potential for growth as a permanent fixture in the justice system, but without improved accuracy and explainability it will continue to go ignored by some justice officials and used with a high degree of failure by others. To show the true possibilities and use of this system, we will be creating a simple explain and upgrade scenario with this dataset. Using LIME we will explain the importance of each feature to this model’s outcome and then remove sensitive and latent sensitive features to compare its accuracy and show how improvements can be made to this model simply by being more aware of the black-box’s working and adjusting the features in accordance with that new knowledge. In the paper we will test three different models on the COMPAS dataset and see how the removal of sensitive and latent sensitive features affect the accuracy.

Keywords:  COMPAS, decile score, model fairness, explainable ai

1. INTRODUCTION
	Explainability has become increasingly more important for the world of AI as a whole, but it has seen its greatest level of demand in model fairness. One of the earlier datasets that introduced the importance of the model fairness concept, COMPAS, is a great example of a dataset that needs model explainability, as the closed nature of the algorithm and lack of explanation has led people affected by the COMPAS models decision to question its legitimacy [2]. Official responses from the makers of COMPAS and people that support the system lean on the fact that both humans and the COMPAS model have a predictive accuracy between 65% [6] and 68% [10], and an acceptable AUC of 0.7 which has been made an acceptable standard in the case of the counties using the COMPAS model [6]. As noted by other researchers, whether complicated or simple, the algorithms built to train models for this data set all achieve around the same accuracy because it eventually gets to a point where it can no longer discern what is and is not valid information for judgement using the cox proportional hazard model [6]. However, in our research about this model and dataset, we came across a paper that mentioned how the dataset published and used by ProPublica was actually incorrect and until the time that article was created in 2019 no one had seemed to notice or mention changing the dataset to get more accurate results [8], therefore because most of these studies were using the dataset provided by ProPublica they also had the same error. 
However, of the different papers that we have explored, to our knowledge there has been no mention of using white-box to help with the identification of information used and how valid it was in the decision making process that the machine created. This also means that adjustments of the data or model to improve accuracy were not tested. To test whether or not the model can be improved upon by using the white-box method, we will use a modified version of the dataset released by ProPublica and, using LIME, a white-box model to identify which features were most important to come up with the results. From there we will adjust those results and return the model to see if there is an improvement in accuracy of the overall model and a lowering of the unfair bias. Explainability of an algorithm has been becoming increasingly important in recent years as it is hard to deny that being able to explain to a user why results have come up the way they have is extremely valuable in decision making.
 In one case a defendant that was sentenced to nine years due to the original COMPAS model attempted to get his sentence lowered on the grounds that the black-box model was not reliable in its decision reasoning, however the judge dismissed his claims under the reason of his opinion being the same as the original judges even without the use of COMPAS, however in a different scenario a judge that relied on COMPAS as a new judge admitted he no longer used it in his pretrial decision making and also that in many of his pretrial decisions he said he often released first-time low-level offenders without bond, however, when asked why in this one case he raised the bond to $1,000 instead of going with the system recommended bond of $0 he claimed no memory of the case or if he relied on the scores or not to make his decision [11]. We have known for years that humans are naturally subject to bias and the bias becomes even stronger when we are distressed in any way, making judge and jury alike incapable of being completely fair. If innocent people’s entire lives are being affected by a decision made in an instant that cannot be fully recalled years later to gain reasoning and grounds on why, then we need a more stable solution that can be called into question and explained at any time with a reasonable amount of accuracy. While this model's official accuracy is around 68% and its AUC score is 0.7 [10], in any other field where human lives are affected such as medicine, or architecture, this threshold of success would be unacceptable. Our goal is to improve on the current accuracy of the COMPAS model as a whole and retain that accuracy after treating the data for sensitive and latent sensitive features.

2. METHOD 
	Our goal for this paper is to improve the accuracy of the COMPAS dataset by pinpointing exactly how much baseless bias from features affects the model and adjusting the importance of those in an attempt to increase reliance on actually important features.  The LIME model can be used on all the common data types, such as image, text, and tabular, but in our case, we will only be using it on tabular data. To see the effects that feature dropping has on the COMPAS dataset we tested three different models, Logistic Regression, Gaussian Naive Bayes, and Support Vector Machine, on three variations of the COMPAS dataset, original, sensitive features removed, and sensitive features and decile score removed. First we standardized the data, removed outlying data from the dataset and dropped cells that had missing data. From there we ran the models and calculated the accuracy and used LIME’s tabular feature explanation to understand the importance of each feature to the outcome of the prediction. If certain features are affecting the model’s prediction despite not being a valid reason to be used, they will be removed and another round of experiments performed to see how the updated and more fair model compares to the original. The experiment was done in this way to test common and popular ways of analyzing tabular data on the COMPAS dataset that is usually only used to justify or deny its validity as an actually useful and fair tool that should be implemented in criminal justice offices. The results displayed include the accuracy of each model and the influence each feature has on the model’s predictions. We will also take a single prediction and compare it against the actual outcome. We will serve as the human in the loop, removing sensitive and other possible sensitive features from the dataset.


3. EXPERIMENTAL RESULTS


Table 1. Accuracy of test models when recorded on the original dataset
Unmodified unstandardized dataset
Original
Prejudice removed
Prejudice and decile scores removed
Logistic Regression
83.8%
83.7%
82.8%
Gaussian Naive Bayes
82.3%
81.5%
80.5%
Support Vector Machine
83.5%
82.7%
82.7%


Table 2. Accuracy of test models when recorded on the modified dataset
Modified
Original
 Age, Sex, and Race removed 
Age, sex Race, and decile scores removed
Logistic Regression
86.1%
83.7%
84%
Gaussian Naive Bayes
82.5%
81.5%
81.4%
Support Vector Machine
85.2%
83.9%
83.9%



	In our experiments we used three separate models on three versions of the dataset to test how dropping features affect the accuracy of the models. First, we must point out how the original dataset put together by ProPublica actually had inaccuracies that affected the data over all and decreased the accuracy of the model. The dataset they used was supposed to collect and analyze data on defendants over a two-year period ending on April 1st, or 2016 [2]. This means that they should have stopped collecting all information after April 1, 2014, however because they only stopped collecting data on non-recidivate people and continued to collect information on people that recidivated after April 1, 2014, their data is skewed in favor of a narrative that supports high repeat offenders in general as pointed out by Barenstein [8]. To account for how this could affect the accuracy we removed all counts of offenders after April 1, 2014. We also ran the dataset before the removal, and we can see a general 1% raise in accuracy total among all the models. Figure 1 shows the difference between the original dataset and the dataset modified to remove all instances from the dataset after april 1st, 2014. Table 1 are all of the accuracy values of the different experiments before fixing the dataset and Table 2 is after the dataset was modified.

<<Images have to be bigger. Texts have to be readable.>>


In the first experiment we used the features that seemed most likely to be used in the decision-making process as well as the sensitive features that were identified by ProPublica as being problematic to the original COMPAS model. These features are listed in Fig 2. in their entirety. Other features not included, such as recent charge description and recent charge degree were also considered but due to the inconsistency with which they are actually used and the confusion about the meaning of the abbreviations, for this shorter study it was deemed not necessary to be used at this time.  Amongst the first set of models as well as all the models in general. The accuracy of the Logistic Regression model was highest, coming in at 86.1%, however in this example we see that the prediction for all three models was wrong. While the offender did recidivate within two years, the models were not able to predict that. Violent decile score, decile score, and age are all large contributing factors as to why the offender was seen less likely to recidivate. Currently we have not done research or read any in depth surveys to validate whether or not age is an appropriate feature to consider when looking at a person’s likelihood to recidivate, but it is generally considered an unfair bias. Additionally, as you will see later on in the paper, the decile score too is calculated from factors that are more socioeconomic than crime pattern based. Despite its high accuracy the scores appear to be based at least partially on sensitive features and considering that the accuracy of this model is not much higher than models that do not rely on this feature, it is still possible to increase accuracy without relying on these features. 
Figures 3 and 4 both correctly predict that the offender did not recidivate within two years, but as stated before there is no set importance of each feature. The value of a feature goes from most important to least important from top to bottom, but between the models the feature importance shifts despite the dataset being the same. We also ran the same experiment multiple times; however the feature importance was constantly changing. We can also see from Fig. 3 and 4 that priors (prior counts of charges) are not taken very seriously. Despite the person having four prior charges in Fig. 6, the model does not yet count it against them heavily, however to our knowledge having more than two prior charges is generally a cause for concern in the justice system. This experiment further justifies the important role that explainable AI has in the field of model fairness. To further trust in this model, we think an added element of consistency would also bring more clarity and usefulness to AI in real world applications.

4. RELATED WORK 
The field of explainable AI, also known as XAI, has increasingly become important due to the impact it has on the useability of machine learning systems because of the mysterious nature of the black box. Models that impact everyday humans have been known to have unfair bias, and in the face of this are not being used or used sparingly due to their negative impact [2]. Small scale studies have been done to show that the more a model explains how it came to its decision, the more a person will be swayed to believe in its process and ability to make decisions [1]. The COMPAS dataset has been at the center of many experiments and research thanks to the study done by ProPublica that stated the algorithm had racial, age and gender bias against defendants, receiving an unfairly high score in terms of recidivism and violent recidivism compared to their white, older, and male counterparts [2]. However almost immediately afterwards there was backlash from both the company that produced the algorithm [10] and other academic studies as well [6].
 The company stated that in their program that there was no such bias and that the discrepancy in the racial judgments specifically were simply due to common machine error and affected black and white offenders the same. They backed up their statement by saying the model passed the 0.70 AUC threshold that was set as a standard in the criminal justice sector. Another independent study [6] backed up this claim by reproducing the COMPAS experiment and noting that there was not a distinct line between the races and also claimed that the problem is not the data but instead that predicting recidivism to a degree higher than around 65% is unlikely due to the nature of recidivism. According to a follow up study, by Dressel and Farid [6], the model is not necessarily more biased towards black people than white, rather that similar amounts of data are incorrect. In fact, the overall accuracy of the system was pegged at 65% which is only a 2% difference when COMPAS results were put against non-professional individuals and groups that predicted recidivism. Despite the assertion that there is no inherent racial bias in their own charts it showed that false positive rate (FPR) for African Americans was significantly higher at 44.9% than false positives for Caucasians at 23.5% and false negative rate (FNR) was significantly lower for African Americans at 28% than it was for Caucasians at 47.7%. While the removal or addition of the race label to the dataset was tested, the results remained similar with FPR being higher in African American defendants and FNR being higher in Caucasian defendants. A counterpoint to the validity of  model fairness, made by a  non-scientific paper, calls into question the benefits of treating data for bias at all due to the fluid nature of race [9]. We would argue against that because many recently uncovered studies were used to suppress people racially using scientific and math based methods [12]. 
To combat the earlier paper’s beliefs that it is the nature of recidivism to have low accuracy, ProPublica’s accuracy of 63% is notably low compared to NorthPointe’s, the company that created COMPAS, owning 68% accuracy rating. However, it was shown that ProPublica actually may have been the cause of their own lower accuracy as there was oversight on the actual correctness of the data. A paper published in 2019 [8] revealed that ProPublica actually had overlooked a crucial part of the original experiment, which was the two-year limit. The data for recidivated defendants continued collecting data after the two-year limit while data for non-recidivated defendants stopped at two years, tipping the scale in favor of high recidivism and causing a 40% increase in recidivism to appear in the data and influencing the algorithms training and testing stages. However, this only affected the overall correctness of the data and the bias for white and against black defendants stayed relatively the same. In the official research paper statement by NorthPointe research department, the authors again criticized ProPublica’s results because even with their results, they could not explain the reason why they believed blacks to be discriminated against, further stating that the results they had were not specific enough [10].

5. CONCLUSION

There is a lot of depth to the problem with COMPASs’ accuracy that comes from data that is not purely scientific.
In Northpointe’s analysis of the decile scores they have the input factors for general recidivism risk scale (GRRS) as criminal involvement, drug problems, age at assessment, age at first adjudication, number of prior arrests, arrest rate, and vocational educational scale [10]. Some of these factors in themselves actually lend hand to bias which increases the decile score for black defendants. 
Policing is done more heavily in predominantly black and latino areas and certain laws put in place, such as “Stop and frisk” were proven to disproportionately target black citizens five times more, and Latino citizens three times more than their white counterparts leading to earlier and more frequent adjudications. This increased number of interactions between citizens and law enforcement made them more suspicious in the eyes of the law whether they were actually caught doing something or not because once a person’s name and information is entered into the system other law enforcement will get a flag to monitor that person more closely whether or not they saw a crime take place. 
Vocational education is also higher in these areas that have more policing and drugs because the areas are poorer. Additionally, traditional four-year education is not as much of an option in these areas, however to our knowledge having a vocational education should not contribute to higher likelihood to recidivate after committing a crime. 

To back up our claims of the decile score not being as fair as it may seem, we have pulled information from the dataset where the decile score does not seem to match up to the current crime or criminal history of the offender in question. Take into account this defendant in the dataset. ID: 10976 Arleen Martin. Charged with domestic strangling and given a recidivism and violent recidivism score of just 1, based on that she is thought to be lower risk, but she did later recidivate. That can be seen as general oversight based on the criteria of the decile scoring system, however if you take into account the next two cases, the difference in decile scores and the bias created by the decile score is more easily seen. The first defendant, ID: 10764 Tyler Flower age 20 with two prior charges, was given a decile score and violent decile score of 10 for felony battery that caused great bodily harm. However, ID: 10675 Michael Bentivegna, also with two prior charges and a current charge of felony battery that caused great bodily harm received a decile score and violent decile score of 3. Both offenders also had similar recent charges. Michael was caught fleeing the scene of an auto accident that caused property damage, Tyler was caught fleeing or eluding in a high-speed chase, though whether or not there were damages was not recorded. As stated earlier, the decile score takes into account features of the potential criminals life that are indicative of their surrounding which relates more to their socioeconomic background rather than actual indications about this person and their likelihood to recommit a crime. Therefore, if the decile score is influencing the algorithm it is further confusing the algorithm because the scoring itself is not consistent with actual signs of possible criminal activity, rather with the background of a person’s environment.
In the future we will do more research on how the decile score is calculated and look for better alternatives to the current one used in the COMPAS model. We will also do more research into creating an analyzing tool for this specific dataset and model using human-in-the-loop to analyze which features are having the highest effect on data and removing or penalizing these features that are biased to create fairer and more consistently accurate data. This combined with a model fairness based pre-processing model has the potential to positively impact datasets that have bias and sensitivity issues in both accuracy and fairness.

ACKNOWLEDGMENT
This work was supported by Basic Science Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Education (NRF-2018R1D1A1A02050166).


REFERENCES:
(1) M. Riberio, S. Singh, C. Guestrin. “Why Should I Trust You?” Explaining the Predictions of Any Classifier arXiv, 2016, https://arxiv.org/abs/1602.04938
(2) J. Larson, S. Mattu, L. Kirchen, J. Angwin. How We Analyzed the COMPAS Recidivism Algorithm, https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm
(3)  H. Baniecki, W. Kretowicz, P. Piatyszek, J. Wisniewski, P. Biecek. “dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python”, arXiv, 2020, https://arxiv.org/abs/2012.14406
(4) X. Zhao, X. Huang , V. Robu , D. Flynn. “BayLIME: Bayesian Local Interpretable Model-Agnostic Explanations” , arXiv 2020, https://arxiv.org/abs/2012.03058
(5) N. Fouladgar, K. Fr¨amling. “XAI-P-T: A Brief Review of Explainable Artificial Intelligence from Practice to Theory”, arXiv 2020, https://arxiv.org/abs/2012.09636
(6) J. Dressel, H. Farid “The accuracy, fairness, and limits of predicting recidivism”, Science Advances Journal, Vol. 4, no. 1, Jan 2018
(7) M.  Zakershahrak, S. Ghodratnama. “Are We On The Same Page? Hierarchical Explanation Generation for Planning Tasks in Human-Robot Teaming using Reinforcement Learning”, ArXiv 2020 abs/2012.11792, 
(8) M. Barenstein. “ProPublica’s COMPAS Data Revisited”, arXiv 2019  arXiv:1906.04711 [econ.GN]
(9) S. Corbett-Davies,  S. Goel. “The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning”, arXiv 2018, arXiv:1808.00023 [cs.CY]
(10) W. Dieterich, C. Mendoza, T. Brennan. COMPAS Risk Scales: Demonstrating Accuracy Equity and Predictive Parity, http://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf
(11) J. Angwin, J. Larson, S. Mattu, L. Kirchner, ProPublica .“There’s software used across the country to predict future criminals. And it’s biased against blacks.”, https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing
(12) A. Hannam E. Denton, A. Smart, J. Smith-Loud, “Towards a Critical Race Methodology in Algorithmic Fairness”,  arXiv 2019, arXiv:1912.03593 [cs.CY]
