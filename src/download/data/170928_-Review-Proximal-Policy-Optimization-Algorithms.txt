# -Review-Proximal-Policy-Optimization-Algorithms
[Review]
> John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov  
> OpenAI  
> https://arxiv.org/pdf/1707.06347.pdf  
[Reference for git]   
> https://www.slideshare.net/WoongwonLee/trpo-87165690
> https://medium.com/@jonathan_hui/rl-the-math-behind-trpo-ppo-d12f6c745f33

## [Mathematics]
> ì´ repositoryì—ì„œëŠ” í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìˆ˜í•™ì ì¸ í•¨ìˆ˜ë“¤ì— ëŒ€í•´ì„œ ê³µë¶€í•˜ê³  ì ìœ¼ë ¤ê³  í•©ë‹ˆë‹¤.  
> ì¶”í›„ì— GAIL ê³¼ TRPO ì—ì„œë„ ê·¼ê°„ì´ ë˜ëŠ” ìˆ˜ì‹ë“¤ì´ë¯€ë¡œ PPO ë…¼ë¬¸ì˜ ì´í•´ëŠ” ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.  

ê¸°ë³¸ì ìœ¼ë¡œ Reinforcement Learningì€ discounted expected rewardë¥¼ ìµœëŒ€í™” í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.  
ì´ expected discounted rewardë¥¼ Î· (ì—íƒ€) ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•©ë‹ˆë‹¤.  
 
![image](https://user-images.githubusercontent.com/40893452/46075159-986f9380-c1c4-11e8-8cd5-48616389ce29.png)

### MM Algorithm

PPO ì™€ TRPO ëŠ” Minorize-Maximization (MM) ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì‹ì´ êµ¬ì„± ë©ë‹ˆë‹¤.  
ì´ ì•Œê³ ë¦¬ì¦˜ì€ "iterative method"ë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•©ë‹ˆë‹¤. 
MM ì•Œê³ ë¦¬ì¦˜ì—ì„œëŠ” ë§¤ iteration ë§ˆë‹¤ ìœ„ ê·¸ë¦¼ì˜ "íŒŒë€ìƒ‰ ì„ "ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” surrogate function M ì„ ì°¾ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.  

ì´ "surrogate function"ì´ ê°–ëŠ” ì˜ë¯¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  
(1) expected discounted reward Î· (ì—íƒ€) ì˜ "lower bound function"  
(2) í˜„ì¬ ê°€ì§€ê³  ìˆëŠ” ì •ì±… (policy)ë¥¼ ë”°ë¼ ê·¼ì‚¬í•œ Î· (ì—íƒ€)   
(3) ìµœì í™” (optimize) í•˜ê¸° ì‰¬ìš´ í•¨ìˆ˜  
> optimizeê°€ ì‰¬ìš´ ì´ìœ ëŠ” ì´ surrogate functionì„ quadratic equationìœ¼ë¡œ ê·¼ì‚¬í•  ê²ƒì´ê¸° ë•Œë¬¸ ...  

ë§¤ MM ì•Œê³ ë¦¬ì¦˜ì˜ iteration ë§ˆë‹¤ optimal point M ì„ ì°¾ìŠµë‹ˆë‹¤.  
ê·¸ë¦¬ê³ , point Mì„ "í˜„ì¬ ì‚¬ìš©í•  ì •ì±… (policy)"ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.  
![image](https://user-images.githubusercontent.com/40893452/46075518-a5d94d80-c1c5-11e8-86fa-47498fa061f8.png)

ì–»ê²Œ ëœ M ì •ì±…ì„ ê¸°ë°˜ìœ¼ë¡œ lower boundë¥¼ ë‹¤ì‹œ ê³„ì‚° í•˜ë©° ì´ ê³¼ì •ì„ ê³„ì† ë°˜ë³µí•˜ëŠ” ê²ƒì´ MM ì•Œê³ ë¦¬ì¦˜ ì…ë‹ˆë‹¤.  
MM ì•Œê³ ë¦¬ì¦˜ì„ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ê²ƒìœ¼ë¡œì¨ ì •ì±… (policy)ë¥¼ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒë˜ì–´ ê°‘ë‹ˆë‹¤.  

### Objective Function

![image](https://user-images.githubusercontent.com/40893452/46075612-fea8e600-c1c5-11e8-9e1c-625051e8234c.png)

ìœ„ì˜ objective functionì€ ë‹¤ìŒê³¼ ê°™ì´ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
(1) adavantage function (*expected reward minus baseline to reduction variance*)ì„ ìµœëŒ€í™” í•˜ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤.  
(2) ì—…ë°ì´íŠ¸ í•˜ëŠ” ìƒˆ ì •ì±… (policy)ê°€ í•™ìŠµ ì´ì „ì˜ ì •ì±… (old policy)ë¡œë¶€í„° ë„ˆë¬´ í¬ê²Œ ë³€í™”í•˜ì§€ ì•Šë„ë¡ (not too different) ì œí•œ í•©ë‹ˆë‹¤.  

ìˆ˜ì‹ì—ì„œ ì‚¬ìš©ë˜ëŠ” notationë“¤ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ ë˜ë©°, ì¼ë°˜ì ì¸ ê°•í™”í•™ìŠµ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ë˜ì–´ ì˜¤ë˜ ê°œë…ì´ ê·¸ëŒ€ë¡œ ì ìš©ë©ë‹ˆë‹¤.  
![image](https://user-images.githubusercontent.com/40893452/46076239-efc33300-c1c7-11e8-9702-24678c598ac0.png)

advantageì˜ ìˆ˜ì‹ì„ í†µí•´ì„œ ìš°ë¦¬ëŠ” 2ê°€ì§€ì˜ ë‹¤ë¥¸ ì •ì±… (policy)ë¥¼ ì‚¬ìš©í•´ì„œ í•œìª½ì˜ policyì˜ rewardë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46076881-e0dd8000-c1c9-11e8-8cb8-f073f740ff3f.png)

ìœ„ì˜ ê³¼ì •ì„ í†µí•´ì„œ í˜„ì¬ trajectoriesë¥¼ ë§Œë“œëŠ” phi' ì •ì±…ê³¼ baselineì„ êµ¬ì„±í•˜ëŠ” phi ì •ì±…ê°„ì˜ ê´€ê³„ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
ìµœì¢… ê²°ê³¼ì—ì„œ Î·(phi)ë¥¼ ìš°ë³€ìœ¼ë¡œ ë„˜ê²¨ì£¼ë©´ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46076907-f3f05000-c1c9-11e8-8522-3ae2427598e8.png)

expectation (ê¸°ëŒ“ê°’) advantageëŠ” ìš°ë³€ì˜ sigma_s p(s) * sigma_a phi(a|s) * A(s, a) ë¡œ ë³€í™˜ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
ì•ì˜ ë‘ sigmaë¡œ ë¬¶ì¸ ë¶€ë¶„ë“¤ì€ í™•ë¥ ì´ë©°, í•´ë‹¹ í™•ë¥ ì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” ê°’ì„ A(s, a)ë¡œ ë³´ë©´  
í”íˆ ì´í•´í•  ìˆ˜ ìˆëŠ” expectationì— ëŒ€í•œ ìˆ˜ì‹ì´ êµ¬ì„±ë©ë‹ˆë‹¤.  

### Function ğ“›

MM ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ì„œ ìš°ë¦¬ëŠ” í˜„ì¬ ì •ì±… (current policy)ì—ì„œ Î· (ì—íƒ€) expected discounted rewardë¥¼ ê·¼ì‚¬í•˜ëŠ” ê²ƒìœ¼ë¡œ lower boundë¥¼ ì°¾ê³ ì í•©ë‹ˆë‹¤.  
![image](https://user-images.githubusercontent.com/40893452/46077127-bdff9b80-c1ca-11e8-99c9-b2f149c77160.png)

ê·¸ëŸ¼ function Lì€ function Mì˜ lower bound equation ì¤‘ ì¼ë¶€ê°€ ë©ë‹ˆë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46077159-db346a00-c1ca-11e8-936c-0e5264bf066b.png)

M = L(theta) - C * KL  ì˜ ì‹ì—ì„œ second termì¸ KLì€ KL-Divergenceë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46090282-586fd700-c1eb-11e8-8c53-d64c553bf5c9.png)

ë‹¤ì‹œ objective functionì˜ ì‹ì— ëŒ€í•´ì„œ ìƒê°í•´ ë´…ì‹œë‹¤.   
![image](https://user-images.githubusercontent.com/40893452/46076907-f3f05000-c1c9-11e8-8522-3ae2427598e8.png)

ìœ„ ì‹ì€ ë‹¤ìŒê³¼ ê°™ì€ 2ê°€ì§€ caseê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
(1) ì–´ë–¤ actionì— ë”°ë¼ "positive advantage" ê°€ ë°œìƒ.  
(2) "negative advantage" ( A(s, a) <= 0 ) ê°€ ë°œìƒ.  

ì´ë•Œ, (1)ì˜ ê²½ìš°ëŠ” í•´ë‹¹ actionì„ ê°•í™”ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ policyë¥¼ ì¼ë°˜ì ì¸ policy gradient ì²˜ëŸ¼ updateí•œë‹¤ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.  
ê·¸ëŸ¬ë‚˜, (2)ì˜ ê²½ìš°ì—ëŠ” ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤.   

(2)ì˜ ê²½ìš°, discounted state distribution of new policy ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ì–´ë ¤ì›Œ ì§‘ë‹ˆë‹¤.  

Î·(Î¸i) = Î·(Î¸i) ê°€ ë˜ë©´, advantage A(s,a) = 0 ì´ ëœë‹¤.   
ê·¸ë¡œì¸í•´, function L ì‹ì˜ ìš°ë³€ì—ì„œ advantageì˜ termì´ ì—†ì–´ì§€ê³  ë‹¤ìŒê³¼ ê°™ì´ ë³€í•œë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46091277-7807ff00-c1ed-11e8-89fb-f1daf14a5624.png)

function Lì„ Î¸ì— ëŒ€í•´ì„œ ë¯¸ë¶„í•˜ë©´ ìœ„ì™€ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.   
> |Î¸=Î¸i í‘œê¸°ëŠ” Î¸ê°€ Î¸i ì¸ ì ì—ì„œì˜ ë¯¸ë¶„ ê°’ì„ ì˜ë¯¸í•˜ê²Œ ë©ë‹ˆë‹¤.  

KL(Î¸i, Î¸i) = 0 ì´ê¸° ë–„ë¬¸ì—, surrogate function Mì€ "local apporximation"ì„ ìˆ˜í–‰í•˜ê²Œ ë©ë‹ˆë‹¤.  
ì´ëŠ” objective functionì˜ ê´€ì ì—ì„œ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46094198-50686500-c1f4-11e8-82fd-ca5e191a2e75.png)
> Kakade & Langford (2002) ê°€ ì¦ëª… í•˜ì˜€ìŠµë‹ˆë‹¤.  

local approximationì€ ìƒˆë¡œìš´ ì •ì±… policyë¥¼ update í•˜ëŠ” ê°œë…ì´ ì•„ë‹ˆë¼, old policyë¥¼ update í•˜ëŠ” ê²ƒìœ¼ë¡œ  
local approximationì„ improve í•˜ì—¬ë„ ì „ì²´ objective functionì´ improve ëœë‹¤ëŠ” ê²ƒì´ ìœ„ì˜ ë…¼ë¬¸ì— ì¦ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  

ê·¸ëŸ¬ë‚˜, local approximationì„ í†µí•´ì„œ improve í•˜ëŠ” policyê°€ ì–´ëŠì •ë„ ë³€í•´ì•¼ì§€ objective functionì˜ improveë¥¼ ë³´ì¥í•˜ëŠ”ì§€ëŠ”  
ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.  

ê·¸ëŸ¬ë¯€ë¡œ, lower boundë¥¼ ì •ì˜í•˜ê³  ì´ lower boundë¥¼ update í•˜ëŠ” ê²ƒìœ¼ë¡œ ì´ê²ƒì„ ë³´ì¥í•˜ê³ ì í•˜ëŠ” ê²ƒì´ ì´ ë…¼ë¬¸ì˜ íŠ¹ì§•ì´ ë©ë‹ˆë‹¤.  




### Lower bound of function M
> TRPO paperì˜ appendixì—ì„œ 2ì¥ì— ê±¸ì³ì„œ ì¦ëª…í•˜ëŠ” ë‚´ìš©ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.  
> ìœ„ì˜ ë‚´ìš©ë“¤ì„ í¬í•¨í•´ì„œ ì´ ë‚´ìš©ë“¤ì€ https://medium.com/@jonathan_hui/rl-the-math-behind-trpo-ppo-d12f6c745f33 ì˜ ë²ˆì—­ ê³¼ ì œ ì´í•´ì˜ ì¶”ê°€ê°€ ë‹´ê¸´ ë‚´ìš©ë“¤ì…ë‹ˆë‹¤.

ìƒˆë¡œìš´ ì •ì±… (policy)ì˜ expected discounted reward Î·(new) ì˜ lower boundëŠ” function M ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë©ë‹ˆë‹¤.    
![image](https://user-images.githubusercontent.com/40893452/46093600-b94edd80-c1f2-11e8-980f-9bc9a4e3963c.png)
    
Dtv ëŠ” the total variation divergence ë¼ê³  í•©ë‹ˆë‹¤.   
Dtv ëŠ” ì´ ë…¼ë¬¸ì—ì„œ KL-Divergenceë¡œ ëŒ€ì²´ë˜ë¯€ë¡œ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ì§€ ì•Šìœ¼ì…”ë„ ë©ë‹ˆë‹¤.    

ê·¸ëŸ¬ë©´ ì‹ì´ ë‹¤ìŒê³¼ ê°™ì´ ë³€í•˜ê²Œ ë©ë‹ˆë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46094540-2ebbad80-c1f5-11e8-9491-138130c75457.png)

ì´ì œ ìœ„ ì‹ì— ë”°ë¼ ìš°ë³€ì˜ lower boundë¥¼ ì˜ë¯¸í•˜ëŠ” termë“¤ì„ ìµœì í™” ì‹œì¼œì£¼ë©´ ë©ë‹ˆë‹¤.  

### Monotonically improving guarantee
ì–´ë–¤ ì •ì±…ì´ updateê°€ ë  ë•Œ, ì´ì „ì˜ policy ë³´ë‹¤ ì„±ëŠ¥ì´ í–¥ìƒë  ê²ƒì´ë¼ëŠ” ê²ƒì„ ë³´ì¥í•˜ëŠ” ê²ƒ ì´ í•„ìš”í•©ë‹ˆë‹¤.  
ìœ„ì—ì„œ lower bound ì‹ Mì„ ìµœì í™”í•˜ì—¬ ì–»ì€ ìƒˆë¡œìš´ ì •ì±…ì´ ì´ì „ ì •ì±…ë³´ë‹¤ expected discounted reward ì¸¡ë©´ì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥ì„  
ë³´ì´ëŠ” ê²ƒì„ ë³´ì¥í•´ì•¼í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.  
ì´ê²ƒì´ ë³´ì¥ ëœë‹¤ë©´, ì§€ì†ì ìœ¼ë¡œ ì •ì±…ì„ update í•˜ëŠ” ê²ƒìœ¼ë¡œ optimal policyì— ë„ë‹¬í•˜ê²Œ ëœë‹¤ëŠ” ê²ƒì´ ë³´ì¥ë˜ê¸° ë•Œë¬¸ì—  
ë§¤ìš° ì¤‘ìš”í•œ ê°œë…ì…ë‹ˆë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46095648-04b7ba80-c1f8-11e8-93de-abcc81a2879a.png)

ìœ„ì˜ ì‚¬ì§„ì€ ìƒˆë¡œìš´ ì •ì±…ì´ ê¸°ì¡´ì˜ ì •ì±… ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒì„ ë³´ì¥í•˜ëŠ” iteration algorithmì…ë‹ˆë‹¤.   
ê·¸ëŸ¬ë‚˜, ëª¨ë“  ì •ì±…ë“¤ ì¤‘ KL divergenceì˜ maximumì„ êµ¬í•˜ëŠ” ê²ƒì€ ì—°ì‚° ì¸¡ë©´ì—ì„œ ë¶ˆê°€ëŠ¥ í•©ë‹ˆë‹¤.  

ê·¸ëŸ¬ë¯€ë¡œ, ì´ ì œì•½ ì¡°ê±´ì„ ì™„í™”ì‹œí‚´ê³¼ ë™ì‹œì— KL divergenceì˜ í‰ê·  (mean) ê°’ì„ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.  
![image](https://user-images.githubusercontent.com/40893452/46095782-552f1800-c1f8-11e8-8424-ce1e36c3e86b.png)

ì™„í™”ì‹œí‚¨ ê·œì¹™ì— ë”°ë¼ ìµœì í™” í•˜ê³ ì í•˜ëŠ” ì‹ì´ ë‹¤ìŒê³¼ ê°™ì´ ë³€í•©ë‹ˆë‹¤.  
![image](https://user-images.githubusercontent.com/40893452/46095908-b35bfb00-c1f8-11e8-8871-ba164c856d5b.png)

![image](https://user-images.githubusercontent.com/40893452/46095965-ddadb880-c1f8-11e8-918d-28f570e66afd.png)

> ìœ„ì˜ ë‘ ì‚¬ì§„ì€ ì´ì›…ì› ì”¨ì˜ https://www.slideshare.net/WoongwonLee/trpo-87165690 ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìœ¼ë©°,  
> ì›Œë‚™ íë¦„ì´ ì¢‹ì•„ì„œ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤. ì„¤ëª…ë„ í•„ìš”ì—†ë„¤ìš”.   

ìœ„ì˜ ìµœì¢…ì ì¸ maximize ì‹ê³¼ ì œì•½ ì¡°ê±´ ì‹ì„ í’€ê¸° ìœ„í•´ì„œ, "lagrangian duality" ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46096695-d1c2f600-c1fa-11e8-8ec5-95388eabdef5.png)

ìœ„ ì‹ì˜ betaëŠ” lagrangian multiplier ì…ë‹ˆë‹¤.

### Optimizing the objective function
ìœ„ì˜ ë³€í˜•ëœ objective functionì„ ì‹¤ì œë¡œ í’€ê¸° ìœ„í•´ì„œëŠ” ì–´ë ¤ìš´ ìˆ˜ì‹ë“¤ì„ ì´í•´í•´ì•¼í•©ë‹ˆë‹¤.   
(1) maximize í•´ì•¼í•˜ëŠ” surrogate function Lì— ëŒ€í•œ "first order"   
(2) KL-Divergenceì˜ "second order"  
Taylor Seriesë¥¼ ê¸°ë°˜ìœ¼ë¡œ Lê³¼ KL-Divergenceì˜ expectationì„ (1), (2)ë¥¼ í†µí•´ì„œ ê·¼ì‚¬í•©ë‹ˆë‹¤.    
ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.  
![image](https://user-images.githubusercontent.com/40893452/46097041-c4f2d200-c1fb-11e8-869d-bcf89ca6bfff.png)

gëŠ” policy gradient ì´ë©°, HëŠ” "Fisher Information Matrix (FIM)" ì…ë‹ˆë‹¤.  

ìœ„ì˜ ì‹ì„ í† ëŒ€ë¡œ optimization ë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.    
![image](https://user-images.githubusercontent.com/40893452/46097155-19964d00-c1fc-11e8-861f-61f445a6c8c1.png)

ì´ ì‹ì˜ í•´ëŠ” ë‹¤ìŒê³¼ ê°™ì´ Natural Policy Gradient ë…¼ë¬¸ì— ì‹¤ë¦° ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í’€ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46097527-27989d80-c1fd-11e8-9fb6-333116cc5f8b.png)

![image](https://user-images.githubusercontent.com/40893452/46097552-397a4080-c1fd-11e8-92e2-ad19ae0d0e17.png)

> ![image](https://user-images.githubusercontent.com/40893452/46097732-9675f680-c1fd-11e8-8e9c-4b9b49a9e738.png)
> ![image](https://user-images.githubusercontent.com/40893452/46097766-ad1c4d80-c1fd-11e8-9257-2e5d19a75ce0.png)


ê·¸ëŸ¬ë‚˜ ì´ ì‹ì—ì„œ FIM (H) ì˜ inverse matrixë¥¼ êµ¬í•˜ëŠ” ê²ƒì€ computation ì¸¡ë©´ì—ì„œ ë§¤ìš° ë¹„íš¨ìœ¨ ì ì…ë‹ˆë‹¤.  
ê·¸ëŸ¬ë¯€ë¡œ, TRPO ì—ì„œëŠ” ìœ„ ì‹ìœ¼ í•´ë‹¹ ë¶€ë¶„ì„ ì¶”ì • í•˜ëŠ” ê²ƒìœ¼ë¡œ ëŒ€ì²´ í•©ë‹ˆë‹¤.  
![image](https://user-images.githubusercontent.com/40893452/46097647-675f8500-c1fd-11e8-9e9a-1c6732df7f99.png)

ìœ„ì˜ ì‹ì€ "Conjugate gradient method"ì— ì˜í•´ì„œ í’€ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.   
Conjugate gradient methodì€ gradient descentì™€ ë¹„ìŠ·í•˜ì§€ë§Œ ìµœëŒ€ N íšŒ ë°˜ë³µì—ì„œ ìµœì  ì ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
> ì—¬ê¸°ì„œ Nì€ ëª¨ë¸ì˜ ë§¤ê°œ ë³€ìˆ˜ ìˆ˜ì…ë‹ˆë‹¤.

ê·¸ëŸ¬ë¯€ë¡œ, CG ê¸°ë²•ì„ ì ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì•Œê³ ë¦¬ì¦˜ì´ ë³€í™”ë©ë‹ˆë‹¤.  

![image](https://user-images.githubusercontent.com/40893452/46097897-0b493080-c1fe-11e8-9e1e-15a57006cf3c.png)



