# Wasserstein Generative Adversarial Networks (WGANS)

Implementation of the  paper ["Wasserstein GAN"](https://arxiv.org/abs/1701.07875)

## What is a WGAN ?

Wasserstein Generative Adversarial Network or WGAN is a recently introduced variant of the popular Generative Adversarial Networks or GAN's. 
It is commonly known in the machine learning community that GAN's are notoriously difficult to train  and suffer from the following issues:

1) Training the Generator and the Discriminator in GAN's is difficult. This is due to the nature of the Adversarial loss itself which induces a minmax game between the Generator and Discriminator networks such that one tries to fool the other until they reach a Nash equilibrium(at least theoretically !!) which is a state in which the Generator produces realistic samples which the Discriminator is unable to distinguish from the original samples. In practice this is very difficult to achieve using Gradient descent based training methods and results in model oscillation.

2) The training of the GAN's is majorly dependent on the architecture of the Generator and the Discriminator. It is seen that the DCGAN's(https://github.com/kpandey008/DCGANS) which are the convolutional variants of GAN's tend to perform better than the traditional MLP based GAN's on several metrics (eg. Visual quality of samples, Better classification accuracy on some intrinsic tasks using the faetures learned by the Discriminator.)

3) GAN's are highly susceptible to Mode Collapse. **Mode Collapse** is a scenario in which GAN falls to a mode and drops it's generated samples around that mode. (eg. generating only samples of a single digit out of many digits when trained over the MNIST dataset)

4) GAN's might learn to draw samples which are visually pleasing but might lie outside the data manifold.(Seems like a paradox but this can happen!!)

The WGAN paper addresses some of these problems by using a distance measure called as **Wasserstein** distance or the **Earth's Movers Distance**. 
Specifically the paper addresses the following problems:

1) The Wasserstein distance is shown to be a better metric than other distance metrics like Jensen-Shannon Distance or the Total Variation Distance. Use of Wasserstein loss stablizes the GAN training. The authors claim that the use of this metric removes the dependence of GAN training to network architectural constraints.

2) Wasserstein provides a stable loss metric that can be used to assess the performance of the GAN. Until now there was no definite measure to assess the training performance of GAN's apart from manally inspecting the quality of samples generated by the Generator.

# Diving into the code

## Prerequisites

The following python packages must be installed for running the code

- Python 2.7 or Python 3.3+
- Tensorflow 0.12.1
- Numpy
- Matplotlib
- ImageIO
- Scikit-learn

## Applied GAN Structure
1. **Generator (DCGAN)**
<p align='center'>
   <img src="https://user-images.githubusercontent.com/37034031/43059677-9688883e-8e88-11e8-84a7-c8f0f6afeca6.png" width=700>
</p>

2. **Critic (DCGAN)**
<p align='center'>
   <img src="https://user-images.githubusercontent.com/37034031/43060075-47f274d0-8e8a-11e8-88ff-3211385c7544.png" width=500>
</p>

## Generated Images
1. **MNIST**
<p align='center'>
<img src="https://user-images.githubusercontent.com/37034031/43871185-659103d8-9bb6-11e8-848b-94ee5055cbe3.png" width=900>
</p>

 is much stable than the latter. However I feel that the sample quality can be increased by training a network with larger capacity over more number of steps. Also presence of batchnorm in both the generator and the discriminator helps stabilize training to a great extent.

## Unsupervised Anomaly Detection
After learn WGAN model with normal dataset (not contains anomalies), 

* Anomaly Detector calculates anomaly score of unseen images.


![Model Structure](assets/model_structure.jpeg)


When unseen data comes, the model tries to find latent variable z that generates input image using backpropagation. (similar with style transfer)
![Model Structure](assets/plot22.png)
Anomaly Score is based on residual and discrimination losses.
- Residual loss: L1 distance between generated image by z and unseen test image.
- Discrimination loss: L1 distacne between hidden representations of generated and test image, extracted by discriminators.

![Res_Loss](assets/res_loss.jpeg)


![Discrimination Loss](assets/dis_loss.jpeg)

Total Loss for finding latent variable z is weighted sum of the two. (defualt lambda = 0.1)


![Total Loss](assets/t_loss.jpeg)
## Conclusions

1) The Wasserstein GAN indeeds provides a stable gradient during traning provided the Lipschitz constraint is enforced on the Discriminator network.(This is probably the key takeaway from the paper). For me the training sometimes suffered from the exploding gradient problem.

2) I tested the WGAN's with multiple network architectures. The performance as well as the visual quality of the samples of the algorithm was not affected much. Thus the paper's claim of architectural robustness holds good.

3) However the model training still showed oscillations for me. Maybe a bit of hyperparameter tuning needed(However in the paper is claimed that the training is quite robust to the hyperparameter tuning).

4) Weight Clipping is probably not the best solution for enforcing the Lipschitz constraint on the Discriminator network. However, in this case it performs fairly well except in cases where the gradient explosion takes place.

5) WGAN's do not suffer from mode collapse which can be observed in the above results). This is because of the fact that the discriminator is not saturated during the training stage ini comparison to the standard GAN networks which can suffer from considerable mode collapse.

## Further Reading

Some good resources for knowing more about WGAN's and improving their training are :

1) The original WGAN paper: https://arxiv.org/pdf/1701.07875.pdf
1) The repo https://github.com/soumith/ganhacks provides a number of hacks to train GAN's in general in a stable setting.
2) The paper **Improved Training of Wasserstein GANs** by Gulrajani et.al is an excellent resource for knowing more about WGAN training. The paper can be found at https://arxiv.org/pdf/1704.00028.pdf
4) Tensorflow implementation of [Anomaly GAN (AnoGAN)](https://arxiv.org/abs/1703.05921).
## Author
MADALI Nabil
