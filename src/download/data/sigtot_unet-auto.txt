
# Our take on Image-to-Image Translation with Conditional Adversarial Networks

In 2016, Phillip Isola, et al. published their paper Image-to-Image Translation with Conditional Adversarial Networks. The paper describes the inner workings of a conditional Generative Adversarial Network (cGAN), that goes by the name of Pix2Pix, designed for general purpose image-to-image translation.

As part of TU Delft’s CS4240 Deep Learning course, we — Art van Liere, Pavel Suma, and Sigurd Totland — attempt to give the reproduction of the results achieved in said paper a try. That is, we try to develop an implementation of the cGAN, from scratch, that tries to mimic the results presented in the paper, *using only the paper*. More specifically, our goal is to achieve similar results as Table 1 of the paper (which is copied below).

![](https://cdn-images-1.medium.com/max/2000/0*3DM1e39MqNa_bqH0)

Here, [Cityscapes](https://www.cityscapes-dataset.com/) is a dataset that focuses on semantic understanding of urban street scenes, providing a semantic label and corresponding photo. Pix2Pix is *conditioned* (more on that later!) on the semantic label in order to generate a matching photo, as seen below.

![](https://cdn-images-1.medium.com/max/2000/0*1bPOHO5B786DJu3V)

The paper quantitatively evaluates the generative model by using pre-trained semantic classifier [FCN-8s](https://arxiv.org/abs/1411.4038) to compare the classification accuracy against the labels that these photos were synthesized from.

If our model can achieve results similar to what is shown in Table 1, we’ve succeeded in our goal of independent reproduction.

## Pix2Pix (introduction)

### What is a cGAN?

The Pix2Pix model is a cGAN; but what is a cGAN? First off, the cGAN is a type of generative adversarial network. This type of architecture is comprised of two models that work against each other:

1. The generator model generating new synthetic images, and

1. The discriminator network that tries to distinguish between real and fake images (from the dataset and from generated by the generator model, respectively).

The two models work against each other in the way that the generator tries to generate images that look real enough to fool the discriminator network, while the discriminator acts as a loss function, where the generated image is not compared to the ground truth, but to an ever-improving evaluator. As such, the two models are trained simultaneously in an adversarial process where, as the networks are trained, generated images get harder to tell apart from fake.

The cGAN is an extension of the regular GAN in that it takes a condition, or input image, from which it generates the output images. Thereby, the network can be steered towards the generation of a certain type of image beyond what it’s trained on.

### Why is Pix2Pix special?

Firstly, Pix2Pix’ importance stems from the way the loss function is defined. That is, it uses a of the objective of the conditional GAN is defined as

![](https://cdn-images-1.medium.com/max/2400/0*gN005BR94s3LKwN2)

where *G *tries to minimize this objective against an adversarial *D* that tries to maximize it. Since sole use of the adversarial loss supposedly lead to unstable optimizations, a standard L1 loss is utilized. This gives the final objective:

![](https://cdn-images-1.medium.com/max/2400/0*kMbJxqZWolsJTEno)

Secondly, unlike past work, the generator is implemented using a [“U-Net”-based architecture](https://arxiv.org/abs/1505.04597).

Lastly, for the discriminator, a convolutional “PatchGAN” classifier is used, which only penalizes structure at the scale of image patches.

## What can Pix2Pix do?

The image below shows Pix2Pix in action: the first column is the input image; the second column is the ground truth, and the last three columns are generated by Pix2Pix, but trained on a different loss functions.

![Pix2Pix in action (mask; ground; L1; cGAN; L1 + cGAN)](https://cdn-images-1.medium.com/max/2400/0*MiHOMUU3kg1G0zDc)*Pix2Pix in action (mask; ground; L1; cGAN; L1 + cGAN)*

## First implementation

This section follows our implementation chronologically.

### Generator

Getting a U-net generator network up and running was the first problem we attempted to tackle. The generator network used in Pix2Pix is a U-net architecture with skip connections between the encoder and decoder. More specifically, the network is defined as:

* encoder: *C64-C128-C256-C512-C512-C512-C512-C512*

* decoder: *CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128*

Here, as defined in the paper, *Ck* denotes a *Convolution-BatchNorm-ReLU* block of output channel size of *k*, while *CDk* just adds a dropout layer to this block. After the last layer, convolution is applied to map the output into 3 channels with hyperbolic tangent function.

Upsampling and downsampling in the network is only done by the convolutional layers and transposed convolutional layers. It is achieved by using stride equal to 2 and spatial filters of size 4x4. In the encoder we set padding of convolutional layers to 1. However, for the decoder we had to compute the padding dynamically because it has to have the exact shape as the skip input to the layer.

### Discriminator

For the discriminator, we tried to mimic so-called PatchGAN. Instead of comparing the generated image and the real one by each pixel, this model looks at multiple different patches, decides how real it looks on scale 0–1 and then averages these results into single output.

Although it sounds complicated, in reality it all again boils down to fully convolutional network with following architecture: *C64-C128-C256-C512. *This is again followed by one convolution that outputs single channel.

In addition, both models are not applying batch normalization in the first layer block.Weights of convolutional layers in both networks were initialized using pytorch *normal_* function with zero mean and 0.02 variance.

### Training loop

Our training loop makes use of pytorch built-in cityscapes dataloader. We load the images one by one and apply several transformations to each of them in this order:

* Upscale to 286x286;

* Randomly crop back to 256x256 on both input and target image (with the same seed);

* Randomly flip horizontally.

The image below showcases a simplified version of our main training step which contains forward and backward passes of both generator and discriminator.

![Flow diagram describing the inner workings of the cGAN.](https://cdn-images-1.medium.com/max/2400/0*ynFuXrgbS5AMHQmC)*Flow diagram describing the inner workings of the cGAN.*

The model is trained in standard two step way. Since the model we are recreating is “conditional GAN”, the generator gets image mask as an input and outputs single image which we feed into discriminator along with the input mask and it outputs prediction whether this image is real or fake. We compute loss of the generator using mean square of one minus this prediction. Along with L1 loss between our generated image and real image we get our combined loss which is then back-propagated.

The discriminator is also conditioned on the input, thus we first feed it image mask concatenated with our generated image and then we again use mean squared error loss of this predicted output and same sized tensor of zeros. Analogously for the ground truth image, we concatenate it with the image mask and compute mean squared error but this time with tensor of zeroes. These two losses are combined into total loss and used for discriminator back-propagation.

### Pitfalls

This section serves as a description of things we got stuck on during the implementation.

The first setback we encountered was concatenating output of the previous layer and skip-connected output in the decoder part of the generator. To make them align, we had to pad one of the inputs to match the shape of the other manually, using the difference of weight and height of the two matrices.

Another major thing we overlooked at the beginning was that we applied random transformations separately on input images and targets which resulted in mismatches, which after concatenation made no sense to the discriminator.

## Results

### Quantitative results

Because of the limited resources that are discussed later, we chose to omit basic GAN and GAN+L1 in the ablation study. Using the pre-trained FCN model to segment our generated outputs we observed results in Table 2. Per-pixel accuracy compares only differences in single pixels, per-class accuracy compares whether the pixel belongs to the same class in both images and class IOU (intersection over union) measures intersection of same class areas in both images over union of these areas.

![Comparison of the results found in the original paper and our reproduced results.](https://cdn-images-1.medium.com/max/2400/0*pETGdVKDuV4uxPUM)*Comparison of the results found in the original paper and our reproduced results.*

### Qualitative

Qualitatively, our results look somewhat similar to the results in the paper, although they are less detailed and more blurry. The cGAN and L1 resemble the results from the paper whereas the L1 + cGAN is considerably worse. In the discussion section below we explore this discrepancy further.

![](https://cdn-images-1.medium.com/max/2400/0*jPsZj40aEXQYWoEJ)

## Discussion

### Why our ablation results differ from the paper

In the paper, the cGAN + L1 loss clearly outperforms the cGAN alone and even more so the L1. In our case however, the cGAN + L1 performs worse than cGAN alone. While we think this is strange, we also note that while training the cGAN + L1, the outputs get qualitatively more realistic in the early epochs, but then turns progressively more blurry and “L1-like” in the later. We believe this is because the generator gets heavily rewarded by the lamda=100 scaled L1 objective and ignores the relatively small gains from the GAN objective. Moreover, looking at the loss curves for the two networks, the discriminator loss goes to zero after a while, meaning it always “guesses right” and effectively zeroes the generator’s gradient in directions minimizing the GAN loss. In this stage, it is no surprise that the generator is forced to minimize only the L1 loss.

![](https://cdn-images-1.medium.com/max/2000/0*6nQmwKy_4nVtdqav)

### On evaluating GANs: Are the FCN-8s scores really a good metric?

The paper uses a FCN-8s semantic segmentation network to evaluate the generated images on the grounds that if the generated images look real, an FCN segmenter trained on real images should segment the generated ones more or less correctly. Although this premise sounds promising, for this particular application we believe the results might mislead, especially when trying to compare the different ablations. In particular, we had a bug that caused images not to be scaled down to 256x256 before being inserted into the cGAN, which in turn made the outputs very blurry and gray and not remotely convincing to the human eye. Nevertheless, the FCN provided 63% pixel accuracy, 22% class accuracy and 16% IoU, in other words values almost exactly matching those in the paper! We believe this is partly caused by the cityscapes images being dominated by homogenous roads, buildings and greens, which can easily be represented in a blurry unconvincing image. These easy classifications then dominate the scores, while more difficult ones like people and cars carry less weight. This exemplifies how objectively and quantitatively evaluating GAN performance is still an open research topic and that care must be taken when creating an evaluation metric or when reading other people’s results.

### Limited access to compute

When training and evaluating the models, one of our major struggles was limited and unreliable access to compute. The models are said to take around 12 hours to train on a Titan X GPU, which on its own is quite a long time for all three models we were going to train. Moreover, due to the many moving parts in this system, we had to run the training several times before seeing promising results. In our training, we experimented with both Google Colab as well as Google Cloud Platform.

### Google Colab

The free-access business model of Google Colab is grounded on not giving guaranteed hardware access to anyone. This makes training compute-heavy models highly unreliable. In the first weeks of our project we used Colab for training, but after a while, Colab started to limit our GPU access in favor of less compute-heavy users. This meant we would not be granted access to a GPU-enabled runtime, and the few times we did, it would be disconnected after a few epochs of training.

Evaluating the model with the FCN model also proved to be difficult on Colab as their GPUs did not have enough memory to process the 1024x2048px images. To circumvent this we had to crop the images to 700x700px as this was the biggest size we could fit in the GPU. Whether the results would be different had we used the full size is hard to say, but we did get *worse* results for sizes smaller than 700px.

### Google Cloud Platform

As Colab had proved itself unfit for our task, we turned to Google Cloud Platform (GCP) to get reliable compute access. This, however, turned out to be an expensive endeavor, and our 300€ in free trial credits disappeared much faster than we had imagined. This probably stems from multiple factors, mainly being inexperience with GCP from our side. It seems that adding persistent disks as well as the (highly expensive) Nvidia V100 graphics card to our VM instance made it money hungry even when it was turned off. We suspect there might be more cost-effective ways to set this up, which is why we are blaming this mainly on inexperience with GCP.

### Evaluation script from Pix2Pix is difficult to run

[The Pix2Pix Github repo](https://github.com/phillipi/pix2pix) provides many useful tools for reproducing the results in the paper, however it falls short in a couple of places. We opted for reimplementing and retraining the models, but use the cityscapes evaluation code from the repo as done in the paper.

We stumbled through three issues while trying to run their evaluate.py script.

* First, confusingly, the repo provides scripts to download the datasets, including Cityscapes, but this is only meant for training and the evaluation script instead expects the original form of the dataset, downloaded from the original source.

* Second, the evaluation script expects a script called `labels.py` to exist in the dataset directory, but this is not provided in the repo and we instead had to download it from [here](https://github.com/mcordts/cityscapesScripts).

* Lastly, the evaluation script relies on an outdated version of scipy to resize and save images. It uses scipy.misc.resize which was deprecated in v1.0.0 and later removed. At first, thinking that what library one uses to resize images should not impact the results, we tried to substitute it with the resize function from [PIL](https://pillow.readthedocs.io/en/stable/), but like other Github users, we were not able to reproduce results even closely resembling the ground truth values in the paper. Thankfully [another user discovered](https://github.com/phillipi/pix2pix/issues/148#issuecomment-610059908) that the script relied on using the removed scipy.misc.resize to work, and with this we too were able to reproduce the results. We still do not know what makes this resize function special, but suspect it might be due to some of the undocumented side effects that are briefly mentioned in the [v1.0.0 deprecation notice](https://github.com/scipy/scipy/releases/tag/v1.0.0).

Common for all our problems with this script was that the answers were nowhere to be found in the repo documentation, but was instead spread around in closed Github issues, sometimes in discussions taking place way after the issue was closed. We believe this raises the question of how much of a maintainer role researchers should be expected to fill when open sourcing code from their papers. On one hand, one may argue that if the researchers are serious about making their papers reproducible, the reproduction code should be easy to run.

However, actively maintaining a code repository takes a lot of time that would likely be better spent by the researchers on new research. Regardless, we would love to see more researchers make use of techniques such as version locking or even containerization tools like Docker to make replication more streamlined and reduce the need for maintenance. We would also love for dataset creators to open source their datasets and for researchers to use open source datasets when benchmarking their methods.
