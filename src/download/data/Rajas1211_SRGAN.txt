# SRGAN
Super Resolution using Generative Adversarial Network (GAN)

## Method

The Generative Adversarial Networks (GANs) are composed of two sub Networks. The two subnetworks are referred to as Generator and Discriminator. These two networks work against one another or are adversarial to one another. In due process of working against each other they get trained.
When it comes to Super-resolution the Generator is fed with Low Resolution images. These low resolution images are the down sampled versions of the high resolution images from the training dataset. The Generator works on the low resolution image to output high resolution image. This high resolution image output acts as an input to the Discriminator. The Discriminator categorises the images as Fake or Real. The high resolution version of the same image is made available to the Discriminator for comparison. The error is calculated using appropriate loss function. Using back propagation and Gradient Descent the weights of the Generator are adjusted. The same process is continued for each image in the training Dataset. Suitable number of epochs and Batch size is selected.
The Generator just gets the low resolution images to train on and never sees the high resolution image used as labels. It simply modifies the weights based on back-propogation after categorisation by Discriminator.
On the other hand, Discriminator trains on both the fake images generated by Generator and the Real Images. As the Generator learns to produce better and better images; Discriminator gets better into categorising them. The loss function for the Discriminator is hence different from Generator.
Both the Networks train alternatively to avoid leakage. When Generator learns to generate realistic high resolution images which consistently succeed in fooling Discriminator, the training is stopped. Now only Generator is used for image generation.


It is an implementation of the paper: https://arxiv.org/abs/1609.04802

## How to run the project?
1. Mount your Google Drive
2. Create the following folders in Google Drive. Edit the folder paths in the code:
    a. training: Folder containing images for training. For this project 800 images from DIV2K dataset were used
    b. testing: FOlder containing images to be tested/used for super resolution
    c. logs: Folder to save log
    d. results
    e: models: to save weights
3. Training can be avoided by using the pretrained weights. Download the pretrained weights from repository and make a copy in Google drive. Update the path in Testing path.

Following is the result of training the network on 800 images and 30,000 Epochs. The results can be substantially improved using more images (~5000) and increasing number of Epochs (~1,00,000).

![door](https://user-images.githubusercontent.com/60413242/89576630-723ac600-d827-11ea-8dc0-5c2d73dcae1d.png)
![face image](https://user-images.githubusercontent.com/60413242/89576635-736bf300-d827-11ea-854b-18c2e6d6ba47.png)
![floating house](https://user-images.githubusercontent.com/60413242/89576639-74048980-d827-11ea-8aa5-35985648b6e1.png)
![man dessert](https://user-images.githubusercontent.com/60413242/89576642-749d2000-d827-11ea-98bd-51c52ece951f.png)
![marked image](https://user-images.githubusercontent.com/60413242/89576645-7535b680-d827-11ea-9db6-0d330ec6f4b8.jpg)
![pebbles](https://user-images.githubusercontent.com/60413242/89576647-7535b680-d827-11ea-9812-e855c5566a06.png)
![star fish](https://user-images.githubusercontent.com/60413242/89576648-75ce4d00-d827-11ea-8d1f-bb0213c610e6.png)
