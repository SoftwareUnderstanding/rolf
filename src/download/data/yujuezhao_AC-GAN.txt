# Keras Implementation of AC-GAN 

Inspired by [link](<https://github.com/eriklindernoren/Keras-GAN/tree/master/acgan>)

[Code](<https://github.com/yujuezhao/AC-GAN/blob/master/acgan/acgan.py>)

Paper: <https://arxiv.org/abs/1610.09585>

#### Example

```
$ cd acgan/
$ python3 acgan.py
```

<img src='https://github.com/yujuezhao/AC-GAN/blob/master/acgan/images/created_gif.gif?raw=true' width=60%>

[generate_gif](<https://github.com/yujuezhao/AC-GAN/blob/master/acgan/gernerate_gif.py>)



#### My Implementation on Jupyter Notebook

[Jupyter notebook](<https://github.com/yujuezhao/AC-GAN/blob/master/acgan/acgan.ipynb>)

##### The Network Architecture 

Inspired by DC-GAN [Code](<https://github.com/eriklindernoren/Keras-GAN/blob/master/dcgan/dcgan.py>) & [Paper](<https://arxiv.org/abs/1511.06434>) 

<img src='https://raw.githubusercontent.com/yujuezhao/AC-GAN/master/acgan/images/architecture.PNG' width='80%'>

> Architecture guidelines for stable Deep Convolutional GANs
>
> * Replace any pooling layers with strided convolutions (discriminator) and fractional-strided
>   convolutions (generator).
> * Use batchnorm in both the generator and the discriminator.
> * Remove fully connected hidden layers for deeper architectures.
> * Use ReLU activation in generator for all layers except for the output, which uses Tanh.
> * Use LeakyReLU activation in the discriminator for all layers.

##### Generator

> In the ACGAN, every generated sample has a corresponding class label, $c ∼ p_c$ in addition to the noise z. G uses both to generate images $X_{fake} = G(c, z)$.

```python
noise = Input(shape=(latent_dim, ))
label = Input(shape=(1, ), dtype='int32')
    
label_embedding = Embedding(num_classes, 100)(label)
label_embedding = Flatten()(label_embedding)
model_input = multiply([noise, label_embedding])
```

What `keras.layers.Embedding` does is abstract in the [documentation](<https://keras.io/layers/embeddings/#embedding>) of keras.  To better understand, I turned to a [tutorial](<https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/>) about embedding. See [preliminary embedding example](<https://github.com/yujuezhao/AC-GAN/blob/master/acgan/Preliminary%20Embedding%20Model.ipynb>) and [GloVe example](<https://github.com/yujuezhao/AC-GAN/blob/master/acgan/GloVe%20example.ipynb>)

*fractional-strided convolutions* in Generator

<img src='https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_transposed.gif' width='20%'>

Besides,  using `keras.layers.convolutional.UpSampling2D` & `keras.layers.convolutional.Conv2D` to implement *fractional-strided convolutions* is to eliminate ['checkerboard artefacts'](<https://distill.pub/2016/deconv-checkerboard/>) . 

##### Discriminator

> The discriminator gives both a probability distribution over sources and a probability distribution over the class labels

```python
validity = Dense(1, activation="sigmoid")(features)
label = Dense(num_classes, activation="softmax")(features)
```

##### Loss Definition

```python
losses = ['binary_crossentropy', 'sparse_categorical_crossentropy']
```

`'sparse_categorical_crossentropy'` vs `'categorical_crossentropy'`  [see](<https://jovianlin.io/cat-crossentropy-vs-sparse-cat-crossentropy/>)

##### Training

>  The objective function has two parts: the loglikelihood of the correct source, $L_S$, and the log-likelihood
> of the correct class, $L_C$ . ... ... D is trained to maximize $L_S + L_C$ while G is trained to
> maximize $L_C − L_S$.

```python
discriminator.compile(loss=losses, optimizer=optimizer, metrics=['accuracy'])
```

the *combined* model is stacked generator and discriminator

```python
combined.compile(loss=losses, optimizer=optimizer)
```

First, train *discriminator* 

[`keras.models.Model.train_on_batch`](<https://keras.io/models/model/#train_on_batch>)

```python
gen_imgs = generator.predict([noise, sampled_labels])

d_loss_real = discriminator.train_on_batch(imgs, [valid, img_labels])
d_loss_fake = discriminator.train_on_batch(gen_imgs, [fake, sampled_labels])
d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
```

Then, train *generator* to fool the discriminator

```python
g_loss = combined.train_on_batch([noise, sampled_labels], [valid, sampled_labels])
```

the *combined* model is stacked generator and discriminator

##### Result 

epoch=2000 is adequate to yield compelling generative results. See the session in Jupyter Notebook



#### Load Saved Weights

[Jupyter Notebook](<https://github.com/yujuezhao/AC-GAN/blob/master/acgan/load_saved_weights.ipynb>)

see Keras [documentation](<https://keras.io/models/about-keras-models/#about-keras-models>) for `save_weights()` and `load_weights()`and [post](<https://stackoverflow.com/questions/47266383/save-and-load-weights-in-keras>) on stackoverflow

