# DeepLearning_Architecture



## 1. Transformer:Attention Is All You Need pytorch로 논문 구현하기(1주차) 

논문
- https://arxiv.org/abs/1706.03762

참고 블로그 
- https://paul-hyun.github.io/transformer-01/ (논문구현1 pytorch)
- https://cpm0722.github.io/pytorch-implementation/transformer (논문구현2 pytorch)


논문 참고 유튜브
- https://www.youtube.com/watch?v=AA621UofTUA&t=5s (동빈나)
- https://www.youtube.com/watch?v=x_8cp4Vdnak (DSBA 연구실)

참고 깃허브
- https://github.com/huggingface/transformers


## 2. Transformer 모델 챗봇 실습하기 (2주차)

- 위로로 답해주는 챗봇
- 데이터셋 : https://github.com/songys/Chatbot_data

## 2. GPT (예정)

논문 
- https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf

참고 블로그
- https://lsjsj92.tistory.com/617 (논문리뷰)
- https://dsbook.tistory.com/321 (논문리뷰)

## 3. BERT ()

논문
- https://arxiv.org/abs/1810.04805

참고 블로그
- http://yonghee.io/bert_binary_classification_naver/ (BERT로 영화리뷰데이터 분류)
- https://ebbnflow.tistory.com/163?category=895676 (캐글문제 분류)
-  https://inhyeokyoo.github.io/project/nlp/bert-issue/ (파이토치로 BERT 구현하기)
