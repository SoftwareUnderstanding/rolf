# MobileBERT paddle

ä½¿ç”¨ `paddlepaddle` å®ç° `transformers` ä¸­æä¾›çš„ `MobileBERT` æ¨¡å‹ã€‚


è®ºæ–‡ï¼š **MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices**

https://arxiv.org/abs/2004.02984


huggingface æ¨¡å‹é¡µé¢ï¼š  

https://huggingface.co/google/mobilebert-uncased


transformers æºä»£ç ï¼š

https://github.com/huggingface/transformers/tree/master/src/transformers/models/mobilebert


## è®ºæ–‡å’Œä»£ç è§£æ

æœ¬æ–‡å¯¹bertæ¨¡å‹è¿›è¡Œäº†çŸ¥è¯†è¿ç§»ï¼ŒæŠŠå¤§æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å°æ¨¡å‹ä¸­ã€‚

æ·±å±‚ç½‘ç»œä¼šå¾ˆéš¾è®­ç»ƒï¼Œå°¤å…¶æ˜¯åœ¨å°æ¨¡å‹ä¸­æˆ‘ä»¬æŠŠæ¨¡å‹çš„â€œè…°â€æ”¶çš„å¾ˆç´§ï¼Œè¿™æ ·å°±æ›´ä¸å®¹æ˜“è®­ç»ƒäº†ã€‚æ‰€ä»¥è¿™é‡Œä½œè€…é‡‡å–çš„æ–¹æ³•æ˜¯ï¼Œå…ˆè®­ç»ƒä¸€ä¸ªå¤§å°ºå¯¸çš„ç½‘ç»œä½œä¸ºæ•™å¸ˆï¼Œç„¶ååœ¨å°æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰ç½‘ç»œçš„è®¾è®¡ä¸­ï¼ŒæŠŠæ¯ä¸€å±‚çš„ feature map è®¾è®¡æˆç›¸åŒçš„å½¢çŠ¶ã€‚è¿™æ ·ï¼Œå°±å¯ä»¥åœ¨è®­ç»ƒæ—¶ï¼Œè®©è¿™ä¸¤ä¸ªæ¨¡å‹å°½é‡å¯¹é½ã€‚

### æ¨¡å‹è®¾è®¡

è¿™é‡Œç»“åˆè®ºæ–‡å’Œä»£ç ï¼Œå¯¹æ¨¡å‹è®¾è®¡è¿›è¡Œä»‹ç»ã€‚æˆ‘æƒ³ç”¨ä¸€ä¸ªå…ˆæ€»ååˆ†çš„æ–¹æ³•æ¥è®²ã€‚

å…ˆè‡ªä¸Šè€Œä¸‹ï¼Œè®²è§£æ¨¡å‹çš„æ„é€ ï¼Œå†ä»ä¸‹åˆ°ä¸Šï¼Œçœ‹æ¯ä¸€ä¸ªå­ç½‘ç»œçš„å®ç°ã€‚

```python
class MobileBertModel(MobileBertPreTrainedModel):
    def __init__(self, config, add_pooling_layer=True):
        super().__init__(config)
        self.config = config
        self.embeddings = MobileBertEmbeddings(config)
        self.encoder = MobileBertEncoder(config)
        self.pooler = MobileBertPooler(config) if add_pooling_layer else None
```

`MobileBertModel` åŒ…æ‹¬ embedding å’Œ encoderï¼Œä»¥åŠä¸€ä¸ª poolerã€‚
ï¼ˆåŸæ–‡ä¸­æ²¡æœ‰ poolerï¼Œ æ‰€ä»¥æˆ‘ä»¬ä¸€ä¼šå„¿å…ˆçœ‹çœ‹ pooleræ˜¯å¹²ä»€ä¹ˆçš„ã€‚ï¼‰

![](static/table_1.png)




### ğŸŒŠ Pooler

æˆ‘ä»¬å…ˆçœ‹è¿™ä¸ªâ€œå¯æœ‰å¯æ— â€çš„ pooler:

```python
class MobileBertPooler(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.do_activate = config.classifier_activation
        if self.do_activate:
            self.dense = nn.Linear(512, 512)

    def forward(self, hidden_states):
        # We "pool" the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        if not self.do_activate:
            return first_token_tensor
        else:
            pooled_output = self.dense(first_token_tensor)
            pooled_output = paddle.tanh(pooled_output)
            return pooled_output
```

è§£é‡Šï¼šencoderæœ€åä¼šè¾“å‡º `(batch_size, 24, 512)` ç»´çš„å‘é‡ã€‚

24æ˜¯bodyçš„æ•°é‡ï¼Œ512æ˜¯è®¾ç½®çš„embeddingç»´åº¦ï¼ŒMobileBERTçš„bodyè®¾è®¡åˆšå¥½æ˜¯è¿›512å‡º512ã€‚

è¿™é‡Œpoolerçš„æ„æ€æ˜¯ç›´æ¥æ‹¿ç¬¬ä¸€ä¸ª512ä½œä¸ºæ¨¡å‹è¾“å‡ºï¼Œæˆ–è€…æ˜¯å†åŠ ä¸€å±‚ Linearï¼Œè¿˜æ˜¯è¾“å‡º 512ã€‚ 

å› æ­¤ï¼Œè¿™ä¸ª Pooler ç¡®å®æ˜¯ â€œå¯æœ‰å¯æ— â€ï¼Œ å“ˆå“ˆã€‚


### ğŸ© Embedding

```python
class MobileBertEmbeddings(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.trigram_input = True
        self.embedding_size = 128
        self.hidden_size = 512

        self.word_embeddings = nn.Embedding(config.vocab_size, 128, padding_idx=0)
        self.position_embeddings = nn.Embedding(512, 512)
        self.token_type_embeddings = nn.Embedding(2, 512)

        embed_dim_multiplier = 3 if self.trigram_input else 1
        embedded_input_size = self.embedding_size * embed_dim_multiplier
        self.embedding_transformation = nn.Linear(embedded_input_size, 512)

        self.LayerNorm = NoNorm(512)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
```

ï¼ˆå…¶ä¸­éƒ¨åˆ† config é¡¹è¢«æˆ‘æ¢æˆäº†æ•°å­—ã€‚ï¼‰
å¾ˆæ¸…æ¥šï¼Œä¸€ä¸ªword embeddingï¼Œä¸€ä¸ª position embeddingï¼Œä¸€ä¸ª token type embeddingã€‚

```python
    def forward(self, input_ids, token_type_ids, position_ids, inputs_embeds):
        if self.trigram_input:
            inputs_embeds = paddle.concat([
                nn.functional.pad(inputs_embeds[:, 1:], [0, 0, 0, 1, 0, 0], value=0),
                inputs_embeds,
                nn.functional.pad(inputs_embeds[:, :-1], [0, 0, 1, 0, 0, 0], value=0),
            ], axis=2)

        if self.trigram_input or self.embedding_size != self.hidden_size:
            inputs_embeds = self.embedding_transformation(inputs_embeds)

        position_embeddings = self.position_embeddings(position_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)
        
        embeddings = inputs_embeds + position_embeddings + token_type_embeddings

        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings
```

ï¼ˆä¸ºæ–¹ä¾¿é˜…è¯»ï¼Œéƒ¨åˆ†åˆ å»ã€‚ï¼‰

ç”¨3-gramçš„è¯ï¼Œå°±è¦æŠŠ input å·¦é”™å¼€1ä½ï¼Œå³é”™å¼€1ä½ï¼Œconcatèµ·æ¥ï¼Œå°±å¾—åˆ°äº†3-gramå‘é‡ã€‚

å†è¿‡ä¸€ä¸ª embedding å˜æˆ512ã€‚MobileBERTçš„ä¸€ä¸ªè®¾è®¡å°±æ˜¯è¾“å…¥çš„embeddingå°ºå¯¸æ˜¯128ï¼Œç»è¿‡ä¸€ä¸ªfcå˜æˆ512ã€‚


### ğŸ­ Encoder

Encoder æ˜¯æ¨¡å‹çš„ä¸»å¹²éƒ¨åˆ†ã€‚

```python
class MobileBertEncoder(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.layer = nn.LayerList(
            [MobileBertLayer(config) for _ in range(24)])
```

è®ºæ–‡ä¸­çš„å›¾æœç„¶è¯šä¸æ¬ºæˆ‘ï¼ŒçœŸçš„å°±æ˜¯24ä¸ªbodyçš„éƒ¨åˆ†ä¸²èµ·æ¥ã€‚

```python
    def forward(
            self,
            hidden_states,
            attention_mask=None,
            head_mask=None,
            output_attentions=False,     # æ˜¯å¦è¦è¾“å‡º attention
            output_hidden_states=False,  # æ˜¯å¦è¦è¾“å‡ºéšè—çŠ¶æ€
            return_dict=True,
    ):
        all_hidden_states = () if output_hidden_states else None
        all_attentions = () if output_attentions else None


        for i, layer_module in enumerate(self.layer):  # æ¯æ¬¡è¿‡ä¸€ä¸ªlayer
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            layer_outputs = layer_module(
                hidden_states,
                attention_mask,
                head_mask[i],
                output_attentions,
            )

            hidden_states = layer_outputs[0]  # <-  éšè—çŠ¶æ€ç”¨äºä¸‹ä¸€å±‚è¾“å…¥

            if output_attentions:
                all_attentions = all_attentions + (layer_outputs[1],)  # <- æœ¬å±‚ attention 

        # Add last layer
        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        if not return_dict:
            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if
                         v is not None)
        return BaseModelOutput(last_hidden_state=hidden_states,
                               hidden_states=all_hidden_states,
                               attentions=all_attentions)
```

å¾ˆç›´è§‚ï¼Œå°±æ˜¯æŠŠ24ä¸ª Layer å±‚è¿‡äº†ä¸€éï¼Œæ¯ä¸€å±‚ä¼šè¾“å‡ºè‡ªå·±çš„ hidden stateï¼Œæœ€åä¸€å±‚çš„ hidden state ä½œä¸ºencoder æœ€ç»ˆè¾“å‡ºã€‚


### ğŸ„ MobileBERTLayer

```python
class MobileBertLayer(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.use_bottleneck = True
        self.num_feedforward_networks = 4

        self.attention = MobileBertAttention(config)
        self.intermediate = MobileBertIntermediate(config)
        self.output = MobileBertOutput(config)
        if self.use_bottleneck: self.bottleneck = Bottleneck(config)
        if config.num_feedforward_networks > 1:
            self.ffn = nn.LayerList([FFNLayer(config) for _ in range(config.num_feedforward_networks - 1)])
```

è¿™é‡Œè¦å‚è€ƒä¸Šé¢è®ºæ–‡é‡Œçš„å›¾ã€‚

![](static/table_1_mobile_bert.png)

è¿™é‡Œæœ‰å‡ å¤„ MobileBERT çš„è®¾è®¡éœ€è¦è¯´ä¸€ä¸‹ï¼š

ï¼ˆ1ï¼‰å®½è¿›å®½å‡ºä¸­é—´çª„ï¼Œä¸ºäº†æ–¹ä¾¿å’Œæ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºå¯¹é½ã€‚

![](static/fig_1.png)

ç”¨å¤§ç™½è¯è¯´ï¼ŒMobileBERTå°±æ˜¯æŠŠä¸­é—´å˜çª„äº†ã€‚
Figure 1(b)æ˜¯teacherï¼Œå®ƒèµ°ä¸¤æ¡è·¯ï¼Œåœ¨å›¾ä¸­æ ‡ä¸ºçº¢è‰²å’Œè“è‰²ï¼Œè¿™ä¸¤æ¡è·¯å°±æ˜¯1(a)ä¸­æ™®é€šBERTçš„è·¯ï¼Œå…³é”®å°±åœ¨äºMobileBERTçš„è®¾è®¡æ˜¯å›¾ä¸­è¯å‡ºæ¥çš„è·¯å°ºå¯¸æ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯ä¸­é—´çš„è‚šå­å°ºå¯¸ä¸ä¸€æ ·ï¼Œè¿™æ ·å°±å¯ä»¥èŠ‚çº¦æ¨¡å‹çš„å°ºå¯¸äº†ã€‚

æ³¨æ„è§‚å¯Ÿ(b)å’Œ(c)çš„Linearå¼€å£æ–¹å‘ä¸ä¸€æ ·ï¼Œteacheræ˜¯ä»å°åˆ°å¤§ï¼Œåˆ©äºæ¨¡å‹çš„è®­ç»ƒï¼›mobileæ˜¯ä»å¤§åˆ°å°ï¼Œå‹ç¼©æ¨¡å‹å°ºå¯¸ã€‚

ï¼ˆ2ï¼‰Stacked FFN

MobileBERT æŠŠè‚šå­ç¼©å°äº†ï¼Œè¿™æ ·å¯¼è‡´ MHA å’Œ FFN çš„å‚æ•°æ¯”ä¾‹å‘ç”Ÿäº†å˜åŒ–ï¼Œè¿™æ ·ä¿¡æ¯çš„è¡¨ç¤ºå°±ä¸åŒ¹é…äº†ï¼Œæ‰€ä»¥ä½œè€…æŠŠFFNå¤šåŠ äº†å‡ å±‚ã€‚è¿™é‡Œæ˜¯ä»1å±‚å˜æˆäº†4å±‚å åŠ ã€‚"carefully balanced".

ï¼ˆ3ï¼‰NoNorm

ä¸ºäº†èŠ‚çº¦è®¡ç®—ï¼ŒMobileBERT æŠŠæ‰€æœ‰çš„ Layer Normalization æ¢æˆäº† NoNorm æ“ä½œï¼Œå…·ä½“å‚è§åŸæ–‡ã€‚NoNorm æ“ä½œè®¡ç®—çš„æ˜¯ Hadamard ç§¯ï¼Œä¹Ÿå°±æ˜¯ä¸¤ä¸ªçŸ©é˜µæŒ‰ä½ç½®åˆ†åˆ«ç›¸ä¹˜ã€‚

```python
class NoNorm(nn.Layer):
    def __init__(self, feat_size: Union[int, Tuple], eps=None):
        super().__init__()

        if isinstance(feat_size, int): feat_size = (feat_size, )

        bias = paddle.zeros(feat_size)
        weight = paddle.ones(feat_size)

        self.bias = paddle.create_parameter(
            shape=bias.shape, dtype=bias.dtype,
            is_bias=True,
            default_initializer=paddle.nn.initializer.Assign(bias)
        )

        self.weight = paddle.create_parameter(
            shape=weight.shape, dtype=weight.dtype,
            default_initializer=paddle.nn.initializer.Assign(weight)
        )

    def forward(self, input_tensor):
        return input_tensor * self.weight + self.bias  # Hadamard product
```

è¿™é‡Œçš„å¥½å¤„å°±æ¯”è¾ƒç„å¦™äº†ï¼Œåæ­£æˆ‘ä¸æ˜¯ç‰¹åˆ«æ˜ç™½ã€‚ã€‚ã€‚æ˜¾è€Œæ˜“è§çš„æ˜¯è®¡ç®—æ¯”è¾ƒæ–¹ä¾¿ï¼Œå…¶ä»–æ–¹é¢å°±ä¸å¤ªæ¸…æ¥šäº†ã€‚ã€‚ã€‚å¦å¤–ï¼Œæ¨¡å‹ç”¨reluè€Œä¸æ˜¯geluã€‚



å¥½äº†ï¼Œç°åœ¨çœ‹ä¸€ä¸‹ Layer å†…çš„ forward æ“ä½œï¼š
```python
    def forward(
        self,
        hidden_states,
        attention_mask=None,
        head_mask=None,
        output_attentions=None,
    ):
        # 1. è¿‡ bottleneck ï¼ˆFCï¼‰
        if self.use_bottleneck:
            query_tensor, key_tensor, value_tensor, layer_input = self.bottleneck(hidden_states)
        else:
            query_tensor, key_tensor, value_tensor, layer_input = [hidden_states] * 4

        # 2. è¿‡ MHA
        self_attention_outputs = self.attention(
            query_tensor,
            key_tensor,
            value_tensor,
            layer_input,
            attention_mask,
            head_mask,
            output_attentions=output_attentions,
        )
        attention_output = self_attention_outputs[0]
        s = (attention_output,)
        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights

        # 3. è¿‡ stacked FFN
        if self.num_feedforward_networks != 1:
            for i, ffn_module in enumerate(self.ffn):
                attention_output = ffn_module(attention_output)
                s += (attention_output,)

        # 4. æ¢å¤å°ºå¯¸çš„ FC
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.output(intermediate_output, attention_output, hidden_states)

        outputs = (
            (layer_output,)
            + outputs
            + (
                paddle.to_tensor(1000),
                query_tensor,
                key_tensor,
                value_tensor,
                layer_input,
                attention_output,
                intermediate_output,
            )
            + s
        )
        return outputs
```

æ¥ä¸‹æ¥å°±è¿™4ä¸ªéƒ¨åˆ†æ¥çœ‹æ¯ä¸ªæ¨¡å—ã€‚

#### ğŸ¼ Bottleneck

```python
class BottleneckLayer(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.intra_bottleneck_size)
        self.LayerNorm = NoNorm()

    def forward(self, hidden_states):
        layer_input = self.dense(hidden_states)
        layer_input = self.LayerNorm(layer_input)
        return layer_input

class Bottleneck(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.key_query_shared_bottleneck = True
        self.use_bottleneck_attention = False
        self.input = BottleneckLayer(config)
        if self.key_query_shared_bottleneck:
            self.attention = BottleneckLayer(config)

    def forward(self, hidden_states):
        bottlenecked_hidden_states = self.input(hidden_states)

        shared_attention_input = self.attention(hidden_states)
        return (shared_attention_input, shared_attention_input, hidden_states, bottlenecked_hidden_states)
```

å…¶å®Bottleneckå°±æ˜¯FCï¼Œä¸Šé¢çº¢è“ä¸¤æ¡è·¯ï¼Œè¿™é‡Œå¯¹åº”ä¸¤ä¸ªä¸åŒç”¨é€”çš„ BottleneckLayerã€‚


#### ğŸ© Attention

```python
class MobileBertAttention(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.self = MobileBertSelfAttention(config)
        self.output = MobileBertSelfOutput(config)

class MobileBertSelfAttention(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.true_hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = nn.Linear(config.true_hidden_size, self.all_head_size)
        self.key = nn.Linear(config.true_hidden_size, self.all_head_size)
        self.value = nn.Linear(
            config.true_hidden_size if config.use_bottleneck_attention else config.hidden_size, self.all_head_size
        )
        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

class MobileBertSelfOutput(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.true_hidden_size, config.true_hidden_size)
        self.LayerNorm = NoNorm()
```

MHAæ¯ä¸ªæ˜¯4ä¸ªheadï¼Œåœ¨ SelfAttention ä¸­å®ç°äº†ã€‚æœ€åä¸€ä¸ª FC + NoNorm è¾“å‡º SelfOutput ã€‚


#### ğŸ° FFN

```python
class MobileBertIntermediate(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.true_hidden_size, config.intermediate_size)
        self.intermediate_act_fn = F.relu

    def forward(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        return hidden_states

class FFNOutput(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.intermediate_size, config.true_hidden_size)
        self.LayerNorm = NoNorm()

    def forward(self, hidden_states, residual_tensor):
        layer_outputs = self.dense(hidden_states)
        layer_outputs = self.LayerNorm(layer_outputs + residual_tensor) # Add & Norm
        return layer_outputs

class FFNLayer(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.intermediate = MobileBertIntermediate(config)
        self.output = FFNOutput(config)

    def forward(self, hidden_states):
        intermediate_output = self.intermediate(hidden_states)
        layer_outputs = self.output(intermediate_output, hidden_states)
        return layer_outputs
```

Intermediate å°±æ˜¯ FC + Reluã€‚ä¸ºä»€ä¹ˆå« Intermediate å‘¢ï¼Ÿè¿™æ˜¯transformerså®ç°é‡Œé¢çš„åå­—ï¼Œæˆ‘è®¤ä¸ºå®ƒçš„æ„æ€æ˜¯ä½œä¸ºä¸¤ä¸ªç»´åº¦çš„ä¸­ä»‹ï¼Œå°±æ˜¯ä¸ºäº†è½¬æ¢ç»´åº¦ä½¿ç”¨çš„ FC ã€‚

FFNOutput å®ç°äº† Add & Normã€‚

æ²¡å•¥è¯´çš„äº†ï¼Œå›¾é‡Œæœ‰ï¼ŒFFN = [128 -> 512 -> 128] * 4

æœ€åä¸€å±‚çš„ Linear ç”¨çš„å°±æ˜¯Intermediateã€‚

æœ€åï¼Œä»å°è‚šå­å†æ¢å¤åˆ°512éœ€è¦åå‘çš„bottleneckç»“æ„ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯å‰é¢çš„bottleneckåè¿‡æ¥ï¼Œæ‰€ä»¥æˆ‘å·ä¸‹æ‡’å°±ä¸è¯´äº†ï¼Œå¯ä»¥è‡ªå·±çœ‹ä»£ç ã€‚

å¤§åŠŸå‘Šæˆï¼ŒMobileBERTçš„ç½‘ç»œç»“æ„åŸºæœ¬å°±æ˜¯è¿™æ ·äº†ã€‚å…·ä½“ç»†èŠ‚å‚è€ƒä»£ç ä¸­çš„ forward æ“ä½œã€‚


## ä¸ ğŸ˜€huggingface ä¸Šçš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¯¹é½

```python
# transformers pytorch ç‰ˆæœ¬
from transformers.models.mobilebert import MobileBertModel, MobileBertTokenizer

mb = MobileBertModel.from_pretrained('google/mobilebert-uncased')

tk = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')

sentence = "Advancing the state of the art: We work on computer science problems that define the technology of today and tomorrow."

i = tk(sentence, return_tensors='pt')

mb.eval()
o = mb(**i)
```


```python
# paddle ç‰ˆæœ¬
import paddle
from mobile_bert_model import *
from config import MobileBertConfig
from ppnlp_tokenizer import MobileBertTokenizer

config = MobileBertConfig()
model = MobileBertModel(config, add_pooling_layer=False)
pretrained_weights = paddle.load('path/to/your/converted/weight')
model.load_dict(pretrained_weights)
tokenizer = MobileBertTokenizer('./mobilebert-uncased/vocab.txt', do_lower_case=True)
# ä½¿ç”¨ ppnlp çš„ tokenizer åŠ è½½äº†é¢„è®­ç»ƒæ¨¡å‹çš„ vocab æ–‡ä»¶ï¼Œå¯ä»¥å…¼å®¹

def tk(s):
    d = tokenizer(s)
    for k, v in d.items():
        d[k] = paddle.to_tensor((v, ))
    return d

sentence = "Advancing the state of the art: We work on computer science problems that define the technology of today and tomorrow."

i = tk(sentence)

model.eval()
o = model(**i)
```

p.s. "Advancing the state of the art: We work on computer science problems that define the technology of today and tomorrow." æ˜¯ google åœ¨ huggingface ä¸Šå†™çš„ Research interests.


è¾“å‡ºï¼š

![](static/align_res.png)

ä¸Šæ–¹æ˜¯transformersçš„è¾“å‡ºç»“æœï¼Œä¸‹æ–¹æ˜¯paddleç‰ˆçš„è¾“å‡ºç»“æœã€‚

ç”±äºfloatçš„ç²¾åº¦é—®é¢˜ï¼Œfloatæ˜¯æ•°å€¼è¶Šå¤§è¶Šä¸ç²¾ç¡®ï¼Œå› æ­¤è¿™ä¸ªè¾“å‡ºçš„æ•°å€¼å¾ˆå¤§çš„æ—¶å€™ï¼Œç²¾åº¦å°±ä¼šå¾ˆå·®ã€‚ã€‚ã€‚

å¦‚æœåªçœ‹æœ‰æ•ˆæ•°å­—ä½çš„è¯ï¼Œç²¾åº¦è¿˜æ˜¯ä¸é”™çš„ï¼Œä½†æ˜¯ç”±äºè¿™æ˜¯floatï¼Œè¶Šå¤§åœ¨æ•°è½´ä¸Šè¶Šç¨€ç–ï¼Œè¿™æ•°è‚¯å®šå°±è¶Šä¸ç²¾ç¡®äº†ã€‚è™½ç„¶æˆ‘ä»¬çœ‹ç€æ•°åŸºæœ¬ä¸Šéƒ½å¯¹ï¼Œå®é™…ä¸Šç”¨ allclose æ¯”è¾ƒæ—¶ï¼Œç²¾åº¦åªæœ‰`rtol=0.75`ã€‚ã€‚ã€‚


