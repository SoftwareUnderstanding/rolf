# Attention !
[![license](https://img.shields.io/badge/License-Apache_2.0-brightgreen.svg)](https://github.com/philipperemy/keras-attention-mechanism/blob/master/LICENSE) [![dep1](https://img.shields.io/badge/Tensorflow-2.0+-brightgreen.svg)](https://www.tensorflow.org/) [![dep2](https://img.shields.io/badge/Keras-2.0+-brightgreen.svg)](https://keras.io/) 

# Luong's attention
```
Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025.
```
https://arxiv.org/abs/1508.04025


# Addition of two numbers

### Sequence length = 10
<img src='AttentionLayer-multiplicative/seqlen10.gif' />

### Sequence length = 30
<img src='AttentionLayer-multiplicative/seqlen30.gif' />

