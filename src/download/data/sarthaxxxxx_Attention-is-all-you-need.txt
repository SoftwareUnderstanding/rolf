# Attention-is-all-you-need
Transformer model architecture. 


PyTorch implementation of 'Attention is All You Need'. 
Paper link : https://arxiv.org/pdf/1706.03762.pdf

The model architecture might not be 100% accurate, but is enough to give a major insight into how 'Transformers' work, as suggested by the authors. The current model is designed to show the simplicity of transformer models and self-attention. As such they will not scale as far as the bigger transformers.

The main() consists of a randomly chosen set of inputs and targets, just for the sake of checking if the model works or not. (NOT FROM ANY DATASET)


