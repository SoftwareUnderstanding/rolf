{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Text Classification</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Packages</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 22:59:39.076072: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-13 22:59:39.076088: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from googletrans import Translator\n",
    "import sklearn\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "\n",
    "import string\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- Call tqdm to see progress bar with pandas\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n",
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to extract the true, false positive and true false negative\n",
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"content\">Content</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Parameters](#part_1)\n",
    "- [List of Models](#part_2)\n",
    "- [List of Metrics for the Model Selection](#part_3)\n",
    "- [Sand box to load Data](#part_4)\n",
    "- [Start Pipeline](#part_5)\n",
    "    - [Prepare data to ML classic](#part_5_1)\n",
    "- [Machine Learning](#part_6)\n",
    "    - [Class Weights](#part_6_1)\n",
    "    - [Save Unique Labels](#part_6_2)\n",
    "    - [DataFrame for the Results](#part_6_3)\n",
    "    - [One-Hot encoding](#part_6_4)\n",
    "    - [TF-IDF](#part_6_5)\n",
    "    - [Load Pre-trained Model FastText](#part_6_6)\n",
    "    - [Word Embeddings](#part_6_7)\n",
    "    - [Multinomial Naive Bayes](#part_6_8)\n",
    "    - [Logistic Regression](#part_6_9)\n",
    "    - [SVM](#part_6_10)\n",
    "    - [k-NN](#part_6_11)\n",
    "    - [RandomForest](#part_6_12)\n",
    "    - [Stochastic Gradient Descent](#part_6_13)\n",
    "    - [Gradient Boosting](#part_6_14)\n",
    "    - [XGBoost Classifier](#part_6_15)\n",
    "    - [Adaboost Classifier](#part_6_16)\n",
    "    - [Catboost Classifier](#part_6_17)\n",
    "    - [LightGBM](#part_6_18)\n",
    "    - [ExtraTreesClassifier](#part_6_19)\n",
    "- [Deep Learning](#part_7)\n",
    "    - [Shallow Neural Networks](#part_7_1)\n",
    "    - [Deep Neural Networks](#part_7_2)\n",
    "    - [Recurrent Neural Networks (RNN)](#part_7_3)\n",
    "    - [Convolutional Neural Networks (CNN)](#part_7_4)\n",
    "    - [Long Short Terme Memory (LSTM)](#part_7_5)\n",
    "    - [CNN-LSTM](#part_7_6)\n",
    "    - [CNN-GRU](#part_7_7)\n",
    "    - [Gated Recurrent Unit (GRU)](#part_7_8)\n",
    "    - [Biderectional RNN](#part_7_9)\n",
    "    - [Biderectional LSTM](#part_7_10)\n",
    "    - [Bidirectional GRU](#part_7_11)\n",
    "    - [Recurent Convulotional Neural Nerworks (RCNN)](#part_7_12)\n",
    "    - [Transformers](#part_7_13)\n",
    "- [Results](#part_8)\n",
    "- [Visualization](#part_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><a id=\"part_1\">Parameters</a></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part allows you to determine the text column to classify as well as the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 22:59:47.315510: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-02-13 22:59:47.315536: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-13 22:59:47.315565: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (computer): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "TEXT           = \"Text\"\n",
    "LABEL          = \"Label\"\n",
    "NAME_SAVE_FILE = \"model_selection_results_multiclasses\" # put just the name the .csv will be added at the end\n",
    "\n",
    "# global parameters\n",
    "num_gpu                = len(tf.config.experimental.list_physical_devices('GPU'))   # detect the number of gpu\n",
    "CV_splits              = 5        # Number of splits for cross-validation and k-folds\n",
    "save_results           = True     # if you want an output file containing all the results\n",
    "lang                   = False    # test if you want to use Google API detection\n",
    "sample                 = False     # use just a sample of data\n",
    "nb_sample              = 5000     # default value of rows if sample selected\n",
    "save_model             = True     # concat all the data representation\n",
    "root_dir               = \"../results/models1/\"       # Place here the path where you want your models stored or use /path/to/your/folder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name file \n",
    "NAME_ENCODER                  = \"encoder.sav\"\n",
    "NAME_COUNT_VECT_MODEL         = \"count_vect_model.sav\"\n",
    "NAME_TF_IDF_MODEL             = \"TF_IDF_model.sav\"\n",
    "NAME_TF_IDF_NGRAM_MODEL       = \"TF_IDF_ngram_model.sav\"\n",
    "NAME_TF_IDF_NGRAM_CHAR_MODEL  = \"TF_IDF_ngram_chars_model.sav\"\n",
    "NAME_TOKEN_EMBEDDINGS         = \"token_embeddings.sav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><a id=\"part_2\">List of Models</a></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models \n",
    "multinomial_naive_bayes= True\n",
    "logistic_regression    = False\n",
    "svm_model              = False\n",
    "k_nn_model             = True\n",
    "sgd                    = True\n",
    "random_forest          = False\n",
    "gradient_boosting      = True\n",
    "xgboost_classifier     = True\n",
    "adaboost_classifier    = False \n",
    "catboost_classifier    = False \n",
    "lightgbm_classifier    = False \n",
    "extratrees_classifier  = False\n",
    "shallow_network        = False\n",
    "deep_nn                = True\n",
    "rnn                    = True\n",
    "lstm                   = True\n",
    "cnn                    = True\n",
    "gru                    = True\n",
    "cnn_lstm               = True\n",
    "cnn_gru                = True\n",
    "bidirectional_rnn      = True\n",
    "bidirectional_lstm     = True\n",
    "bidirectional_gru      = True\n",
    "rcnn                   = True\n",
    "transformers           = False\n",
    "pre_trained            = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Create folders to save models</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder can not be created\n"
     ]
    }
   ],
   "source": [
    "if save_model:\n",
    "    # will create the folder to save all the models\n",
    "    try:\n",
    "        dir_name =  NAME_SAVE_FILE\n",
    "        os.makedirs(os.path.join(root_dir,dir_name))\n",
    "        print(\"The folder is created\")\n",
    "    except:\n",
    "        print(\"The folder can not be created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><a id=\"part_3\">List of Metrics for the Model Selection</a></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can put all the metrics you want (included in sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_metrics = {'acc': accuracy_score,\n",
    "               'balanced_accuracy': balanced_accuracy_score,\n",
    "               'prec': precision_score,\n",
    "               'recall': recall_score,\n",
    "               'f1-score': f1_score,\n",
    "               'tp': tp, 'tn': tn,\n",
    "               'fp': fp, 'fn': fn,\n",
    "               'cohens_kappa':cohen_kappa_score,\n",
    "               'matthews_corrcoef':matthews_corrcoef,\n",
    "               \"roc_auc\":roc_auc_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><a id=\"part_4\">Sand Box to Load Data</a></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sandbox is the working area of your data if it has not been processed before using the pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for preprocessing\n",
    "def remove_upper_case( text):\n",
    "    '''\n",
    "    Function to transform upper string in title words\n",
    "    @param text: (str) text \n",
    "    @return: (str) text without upper words \n",
    "    '''\n",
    "    sentences = text.split(\"\\n\")\n",
    "    new_sentences = []\n",
    "    for i in sentences:\n",
    "        words = text.split()\n",
    "        stripped = [w.title() if w.isupper() else w for w in words]\n",
    "        new_sentences.append(\" \".join(stripped))\n",
    "    return \"\\n\".join(new_sentences)\n",
    "  \n",
    "def remove_URL( text):\n",
    "    '''\n",
    "    Function to remove url from text.\n",
    "    @param text: (str) sentence\n",
    "    @return: (str) clean text\n",
    "\n",
    "    '''\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "    \n",
    "    \n",
    "def remove_html( text):\n",
    "    '''\n",
    "    Function regex to clean text from html balises.\n",
    "    @param text: (str) sentence \n",
    "    @return: (str) clean text \n",
    "    '''\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "    \n",
    "    \n",
    "\n",
    "def remove_emoji( text):\n",
    "    '''\n",
    "    Function to remove emojis, symbols and pictograms etc from text\n",
    "    @param text: (str) sentences \n",
    "    @return: (str) clean text \n",
    "    '''\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "#df = pd.read_csv(\"../projet_classification_mails/mails_clean_concat_ref_folders_train_2.csv\", sep=\";\")\n",
    "#df_test  = pd.read_csv(\"../projet_classification_mails/mails_clean_concat_ref_folders_test_2.csv\", sep=\";\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/somef_data.csv\", sep=\";\")\n",
    "\n",
    "\n",
    "df[LABEL][(df[LABEL]!=\"ft_notaire\") & (df[LABEL]!=\"annulation\") ] = \"other\" \n",
    "#df.drop_duplicates(inplace=True)\n",
    " \n",
    "#df_test[LABEL][(df_test[LABEL]!=\"ft_notaire\") & (df_test[LABEL]!=\"annulation\") ] = \"other\" #  \n",
    "#df_test.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "#f = df[~(df[LABEL]==\"ft\") & ~(df[LABEL]==\"co\")]\n",
    "print(df[LABEL].value_counts())\n",
    "print(df_test[LABEL].value_counts())\n",
    "print(df[TEXT].isnull().sum())\n",
    "#df[TEXT][df[TEXT].isnull()] = \"empty\"\n",
    "df.dropna(subset=[TEXT], inplace=True)\n",
    "df_test.dropna(subset=[TEXT], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false --no-raise-error\n",
    "#!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "#!tar -xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_sentiment_analysis_dataset(data_path, seed=123):\n",
    "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data_path: string, path to the data directory.\n",
    "        seed: int, seed for randomizer.\n",
    "\n",
    "    # Returns\n",
    "        A tuple of training and validation data.\n",
    "        Number of training samples: 25000\n",
    "        Number of test samples: 25000\n",
    "        Number of categories: 2 (0 - negative, 1 - positive)\n",
    "\n",
    "    # References\n",
    "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
    "\n",
    "        Download and uncompress archive from:\n",
    "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    \"\"\"\n",
    "    #imdb_data_path = os.path.join(data_path, '')\n",
    "    \n",
    "    \n",
    "    # Load the training data\n",
    "    dataframe_train = pd.read_csv('../data/somef_data_train.csv', sep=';')\n",
    "    train_texts = dataframe_train['Text']\n",
    "    train_labels = dataframe_train['Label']\n",
    "\n",
    "    #train_texts = []\n",
    "    #train_labels = []\n",
    "    #for category in ['pos', 'neg']:\n",
    "    #    train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "    #    for fname in tqdm(sorted(os.listdir(train_path))):\n",
    "    #        if fname.endswith('.txt'):\n",
    "    #            with open(os.path.join(train_path, fname)) as f:\n",
    "    #                train_texts.append(f.read())\n",
    "    #            train_labels.append(0 if category == 'neg' else 1)\n",
    "    print(\"\\nTrain done\\n\")\n",
    "\n",
    "    \n",
    "    # Load the validation data.\n",
    "    dataframe_test = pd.read_csv('../data/somef_data_test.csv', sep=';')\n",
    "    test_texts = dataframe_test['Text']\n",
    "    test_labels = dataframe_test['Label']\n",
    "    \n",
    "    #test_texts = []\n",
    "    #test_labels = []\n",
    "    #for category in ['pos', 'neg']:\n",
    "    #    test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "    #    for fname in tqdm(sorted(os.listdir(test_path))):\n",
    "    #        if fname.endswith('.txt'):\n",
    "    #            with open(os.path.join(test_path, fname)) as f:\n",
    "    #                test_texts.append(f.read())\n",
    "    #            test_labels.append(0 if category == 'neg' else 1)\n",
    "    print(\"\\nTest done\\n\")\n",
    "    # Shuffle the training data and labels.\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train done\n",
      "\n",
      "\n",
      "Test done\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'remove_upper_case' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3484/2896558333.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLABEL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_upper_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_html\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'remove_upper_case' is not defined"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#%%script false --no-raise-error\n",
    "(x_train, y_train), (x_test, y_test) = load_imdb_sentiment_analysis_dataset(\"../data/somef_data.csv\")\n",
    "\n",
    "df = pd.DataFrame(data=[x_train, y_train], index=[TEXT, LABEL]).T\n",
    "df = df.append(pd.DataFrame(data=[x_test, y_test], index=[TEXT, LABEL]).T)\n",
    "\n",
    "df_test = pd.DataFrame(data=[x_test, y_test], index=[TEXT, LABEL]).T\n",
    "\n",
    "df[TEXT] = df[TEXT].apply(remove_upper_case)\n",
    "df[TEXT] = df[TEXT].apply(remove_URL)\n",
    "df[TEXT] = df[TEXT].apply(remove_html)\n",
    "df[TEXT] = df[TEXT].apply(remove_emoji)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test[df_test[LABEL]==\"ft_notaire\"].iloc[60].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><a id=\"part_5\">Sart Pipeline</a></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang_google( x):\n",
    "        '''\n",
    "        Function to detect the language of the string\n",
    "        @param x: (str) sentences of text to detect language\n",
    "        @return: (str or nan) language of the sentence\n",
    "        '''\n",
    "        translate = Translator()\n",
    "        try:\n",
    "            return translate.detect(x).lang\n",
    "        except:\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lang:\n",
    "    # ---- Language detection of the text\n",
    "    df.loc[:,\"language\"] = df[TEXT].progress_apply(detect_lang_google)\n",
    "    # ---- Extract most frequent language \n",
    "    language = df.language.value_counts().index.tolist()[0]\n",
    "    print(f\"The language most present in the dataset is {language}\")\n",
    "else:\n",
    "    language=\"en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3><a id=\"part_5_1\">Prepare data for ML Classic</a></h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample:\n",
    "    df_save = df.copy()\n",
    "    df = df.sample(nb_sample, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8559, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load stopwords \n",
    "#if language==\"fr\":\n",
    "#    stop_word = np.loadtxt(\"../stopwords/stopwords-fr.txt\", dtype=str)\n",
    "#if language==\"en\":\n",
    "stop_word = np.loadtxt(\"stopwords_en.txt\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words( x, stop_word):\n",
    "        '''\n",
    "        Function to remove a list of words\n",
    "        @param x : (str) text \n",
    "        @param stop_word: (list) list of stopwords to delete \n",
    "        @return: (str) new string without stopwords \n",
    "        '''\n",
    "        x_new = text_to_word_sequence(x)    # tokenize text \n",
    "        x_ = []\n",
    "        for i in x_new:\n",
    "            if i not in stop_word:\n",
    "                x_.append(i)\n",
    "        return \" \".join(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8559/8559 [00:06<00:00, 1248.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"- Download the [ImageNet dataset](http://imag...</td>\n",
       "      <td>General</td>\n",
       "      <td>download imagenet dataset http image net org d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"To get the result  the steps have been listed...</td>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>get result steps listed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"example  if the selected action is N (north) ...</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>example selected action n north transition cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"[x] multi-gpu support   You also can use your...</td>\n",
       "      <td>General</td>\n",
       "      <td>x multi gpu support also use image data follow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Fully Convolutional Networks (FCNs) are a nat...</td>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>fully convolutional networks fcns natural exte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                   Label  \\\n",
       "0  \"- Download the [ImageNet dataset](http://imag...                 General   \n",
       "1  \"To get the result  the steps have been listed...         Computer Vision   \n",
       "2  \"example  if the selected action is N (north) ...  Reinforcement Learning   \n",
       "3  \"[x] multi-gpu support   You also can use your...                 General   \n",
       "4  \"Fully Convolutional Networks (FCNs) are a nat...         Computer Vision   \n",
       "\n",
       "                                             Text_sw  \n",
       "0  download imagenet dataset http image net org d...  \n",
       "1                            get result steps listed  \n",
       "2  example selected action n north transition cel...  \n",
       "3  x multi gpu support also use image data follow...  \n",
       "4  fully convolutional networks fcns natural exte...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,TEXT+\"_sw\"] = df.loc[:,TEXT].progress_apply(lambda x : remove_stop_words(x, stop_word))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"- Download the [ImageNet dataset](http://imag...</td>\n",
       "      <td>General</td>\n",
       "      <td>download imagenet dataset http image net org d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"To get the result  the steps have been listed...</td>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>get result steps listed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"example  if the selected action is N (north) ...</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>example selected action n north transition cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"[x] multi-gpu support   You also can use your...</td>\n",
       "      <td>General</td>\n",
       "      <td>x multi gpu support also use image data follow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Fully Convolutional Networks (FCNs) are a nat...</td>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>fully convolutional networks fcns natural exte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                   Label  \\\n",
       "0  \"- Download the [ImageNet dataset](http://imag...                 General   \n",
       "1  \"To get the result  the steps have been listed...         Computer Vision   \n",
       "2  \"example  if the selected action is N (north) ...  Reinforcement Learning   \n",
       "3  \"[x] multi-gpu support   You also can use your...                 General   \n",
       "4  \"Fully Convolutional Networks (FCNs) are a nat...         Computer Vision   \n",
       "\n",
       "                                             Text_sw  \n",
       "0  download imagenet dataset http image net org d...  \n",
       "1                            get result steps listed  \n",
       "2  example selected action n north transition cel...  \n",
       "3  x multi gpu support also use image data follow...  \n",
       "4  fully convolutional networks fcns natural exte...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean rows which are empty after proceding of stopwords removal \n",
    "if df[TEXT+\"_sw\"].isnull().sum()>0:\n",
    "    print(\"Empty text\")\n",
    "    df.dropna(subset=[TEXT+\"_w\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6\"><h1>Machine Learning</h1></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1712/1712 [00:01<00:00, 1221.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"This is a Project for Continuous Control Deep...</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>project continuous control deep reinforcement ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"- To install  `cd` into the root directory an...</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>install cd root directory type pip install e i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"| 2 | Federated Learning on MNIST using PyTor...</td>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>2 federated learning mnist using pytorch pysyf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"GPU: 1  8GB  GM204GL [Tesla M60]   \"</td>\n",
       "      <td>General</td>\n",
       "      <td>gpu 1 8gb gm204gl tesla m60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"torch  tensorboard (2.1 or higher)  biopython...</td>\n",
       "      <td>General</td>\n",
       "      <td>torch tensorboard 2 1 higher biopython see req...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                   Label  \\\n",
       "0  \"This is a Project for Continuous Control Deep...  Reinforcement Learning   \n",
       "1  \"- To install  `cd` into the root directory an...  Reinforcement Learning   \n",
       "2  \"| 2 | Federated Learning on MNIST using PyTor...         Computer Vision   \n",
       "3              \"GPU: 1  8GB  GM204GL [Tesla M60]   \"                 General   \n",
       "4  \"torch  tensorboard (2.1 or higher)  biopython...                 General   \n",
       "\n",
       "                                             Text_sw  \n",
       "0  project continuous control deep reinforcement ...  \n",
       "1  install cd root directory type pip install e i...  \n",
       "2  2 federated learning mnist using pytorch pysyf...  \n",
       "3                        gpu 1 8gb gm204gl tesla m60  \n",
       "4  torch tensorboard 2 1 higher biopython see req...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.loc[:,TEXT+\"_sw\"] = df_test.loc[:,TEXT].progress_apply(lambda x : remove_stop_words(x, stop_word))\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Computer Vision                3758\n",
       " General                        2910\n",
       " Natural Language Processing     900\n",
       " Reinforcement Learning          480\n",
       " Sequential                      226\n",
       " Graphs                          159\n",
       " Audio                           126\n",
       " Name: Label, dtype: int64,\n",
       " Computer Vision                752\n",
       " General                        582\n",
       " Natural Language Processing    180\n",
       " Reinforcement Learning          96\n",
       " Sequential                      45\n",
       " Graphs                          32\n",
       " Audio                           25\n",
       " Name: Label, dtype: int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[LABEL].value_counts(), df_test[LABEL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "# ML classic \n",
    "train_x_sw, valid_x_sw, y_train_sw, y_valid_sw = model_selection.train_test_split(df[TEXT+\"_sw\"], df[LABEL], random_state=42, stratify=df[LABEL].values, test_size=0.2)\n",
    "\n",
    "# For Embeddings\n",
    "train_x, valid_x, y_train, y_valid = model_selection.train_test_split(df[TEXT], df[LABEL], random_state=42, stratify=df[LABEL], test_size=0.2)\n",
    "\n",
    "\n",
    "#train_x_sw, valid_x_sw, y_train_sw, y_valid_sw = df[TEXT+\"_sw\"].values, df_test[TEXT+\"_sw\"].values, df[LABEL].values, df_test[LABEL].values  \n",
    "#train_x, valid_x, y_train, y_valid = df[TEXT].values, df_test[TEXT].values, df[LABEL].values, df_test[LABEL].values \n",
    "\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "tgit \n",
    "valid_y_sw = encoder.transform(y_valid_sw)\n",
    "train_y = encoder.transform(y_train)\n",
    "valid_y = encoder.transform(y_valid)\n",
    "\n",
    "#if save_model:\n",
    "#    # save the model to disk\n",
    "#    filename = NAME_ENCODER\n",
    "#    pickle.dump(encoder, open(os.path.join(root_dir, dir_name, NAME_ENCODER), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_1\"><h3>Class Weights</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the class weight with sklearn \n",
    "class_weights = class_weight.compute_class_weight(class_weight ='balanced', classes =np.unique(y_train), y =y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weight: 9.6846\tclass: Audio\n",
      "Class weight: 0.3254\tclass: Computer Vision\n",
      "Class weight: 0.4202\tclass: General\n",
      "Class weight: 7.7019\tclass: Graphs\n",
      "Class weight: 1.3585\tclass: Natural Language Processing\n",
      "Class weight: 2.5472\tclass: Reinforcement Learning\n",
      "Class weight: 5.4041\tclass: Sequential\n"
     ]
    }
   ],
   "source": [
    "print(*[f'Class weight: {round(i[0],4)}\\tclass: {i[1]}' for i in zip(class_weights, np.unique(y_train))], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset is imbalanced (ratio=0.034)\n"
     ]
    }
   ],
   "source": [
    "# Determined if the dataset is balanced or imbalanced \n",
    "ratio = np.min(df[LABEL].value_counts()) / np.max(df[LABEL].value_counts())\n",
    "if ratio > 0.1:      # Ratio 1:10 -> limite blanced / imbalanced \n",
    "    balanced = True\n",
    "    print(f\"\\nThe dataset is balanced (ratio={round(ratio, 3)})\")\n",
    "else:\n",
    "    balanced = False\n",
    "    print(f\"\\nThe dataset is imbalanced (ratio={round(ratio, 3)})\")\n",
    "    #from imblearn.over_sampling import ADASYN\n",
    "    # put class for debalanced data \n",
    "    # in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_2\"><h3>Save Unique Labels</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the unique label corresponding to their encoding correspondance\n",
    "labels = df[LABEL].unique()\n",
    "test=pd.DataFrame(data=np.transpose([labels,encoder.transform(labels)]), columns=[\"labels\", \"encoding\"]).sort_values(by=[\"encoding\"])\n",
    "labels=test.labels.tolist()\n",
    "if any([0,1]) in labels and len(labels)==2:\n",
    "    labels[labels.index(0)] = \"negative\"\n",
    "    labels[labels.index(1)] = \"positive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_3\"><h3>DataFrame for the results</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_4\"><h3>One-Hot encoding (CountVectorizing)</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8559, 3)\n",
      "(8559,)\n",
      "(6847,)\n",
      "(1712,)\n",
      "(6847,)\n",
      "(1712, 48660)\n",
      "(1712,)\n",
      "(1712, 48660)\n",
      "CPU times: user 1.48 s, sys: 3.76 ms, total: 1.49 s\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(df[TEXT]+\"_sw\")\n",
    "print(df.shape)\n",
    "print((df[TEXT]+\"_sw\").shape)\n",
    "print(train_x_sw.shape)\n",
    "print(valid_x_sw.shape)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x_sw)\n",
    "xvalid_count =  count_vect.transform(valid_x_sw)\n",
    "print(train_x_sw.shape)\n",
    "print(xvalid_count.shape)\n",
    "print(valid_x_sw.shape)\n",
    "print(xvalid_count.shape)\n",
    "\n",
    "#if save_model:\n",
    "    # save the model to disk\n",
    "    #filename = NAME_COUNT_VECT_MODEL\n",
    "    #pickle.dump(count_vect, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       \"- Download the [ImageNet dataset](http://imag...\n",
       "1       \"To get the result  the steps have been listed...\n",
       "2       \"example  if the selected action is N (north) ...\n",
       "3       \"[x] multi-gpu support   You also can use your...\n",
       "4       \"Fully Convolutional Networks (FCNs) are a nat...\n",
       "                              ...                        \n",
       "1707    \"The same learning algorithm was used to train...\n",
       "1708    \"1. Setup python 3 virtual environment. If you...\n",
       "1709    \"```python TRAIN = \"train/\" TEST = \"test/\"   #...\n",
       "1710    \"The repo has been forked initially from https...\n",
       "1711    \"Datasets are stored as multi-resolution TFRec...\n",
       "Name: Text, Length: 8559, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[TEXT]+\"_sw\"[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_5\"><h3>TF-IDF</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word level tf-idf done\n",
      "ngram level tf-idf done\n",
      "characters level tf-idf done\n",
      "CPU times: user 20.6 s, sys: 232 ms, total: 20.8 s\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000)\n",
    "tfidf_vect.fit(df[TEXT])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "print(\"word level tf-idf done\")\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=10000)\n",
    "tfidf_vect_ngram.fit(df[TEXT])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "print(\"ngram level tf-idf done\")\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char',  ngram_range=(2,3), max_features=10000) #token_pattern=r'\\w{1,}',\n",
    "tfidf_vect_ngram_chars.fit(df[TEXT])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "print(\"characters level tf-idf done\")\n",
    "\n",
    "if save_model:\n",
    "    # save the model tf-idf to disk\n",
    "    filename = NAME_TF_IDF_MODEL\n",
    "    pickle.dump(tfidf_vect, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "\n",
    "    # save the model ngram to disk\n",
    "    filename = NAME_TF_IDF_NGRAM_MODEL\n",
    "    pickle.dump(tfidf_vect_ngram, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "    \n",
    "    # save the model ngram char to disk\n",
    "    filename = NAME_TF_IDF_NGRAM_CHAR_MODEL\n",
    "    pickle.dump(tfidf_vect_ngram_chars, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_6\"><h3>Load Pre-Trained model fastText</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.12 s, sys: 4.24 s, total: 6.36 s\n",
      "Wall time: 8.97 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if language==\"fr\":\n",
    "    #!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz\n",
    "    #!gunzip cc.fr.300.bin.gz\n",
    "    pretrained = fasttext.FastText.load_model('./cc.fr.300.bin')\n",
    "if language==\"en\":\n",
    "    #!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\n",
    "    #!unzip crawl-300d-2M-subword.zip\n",
    "    pretrained = fasttext.FastText.load_model('./crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_7\"><h3>Word Embeddings</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30784/30784 [00:00<00:00, 73909.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.51 s, sys: 32.2 ms, total: 2.54 s\n",
      "Wall time: 2.56 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# create a tokenizer \n",
    "token = Tokenizer(oov_token='<OOV>')\n",
    "token.fit_on_texts(df[TEXT])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=300)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=300)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "words = []\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = pretrained.get_word_vector(word) #embeddings_index.get(word)\n",
    "    words.append(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "if save_model:\n",
    "    filename = NAME_TOKEN_EMBEDDINGS\n",
    "    pickle.dump(token, open(os.path.join(root_dir, dir_name,filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(clf, x, y, X_test, y_test, name='classifier', cv=5, dict_scoring=None, fit_params=None, save=save_model):\n",
    "    '''\n",
    "    Function create a metric report automatically with cross_validate function.\n",
    "    @param clf: (model) classifier\n",
    "    @param x: (list or matrix or tensor) training x data\n",
    "    @param y: (list) label data \n",
    "    @param name: (string) name of the model (default classifier)\n",
    "    @param cv: (int) number of fold for cross-validation (default 5)\n",
    "    @param dict_scoring: (dict) dictionary of metrics and names\n",
    "    @param fit_aparams: (dict) add parameters for model fitting \n",
    "    @param save: (bool) determine if the model need to be saved\n",
    "    @return: (pandas.dataframe) dataframe containing all the results of the metrics \n",
    "    for each fold and the mean and std for each of them\n",
    "    '''\n",
    "    \n",
    "    '''{'acc': accuracy_score,\n",
    "               'balanced_accuracy': balanced_accuracy_score,\n",
    "               'prec': precision_score,\n",
    "               'recall': recall_score,\n",
    "               'f1-score': f1_score,\n",
    "               'tp': tp, 'tn': tn,\n",
    "               'fp': fp, 'fn': fn,\n",
    "               'cohens_kappa':cohen_kappa_score,\n",
    "               'matthews_corrcoef':matthews_corrcoef,\n",
    "               \"roc_auc\":roc_auc_score}'''\n",
    "    \n",
    "    \n",
    "    if dict_scoring!=None:\n",
    "        score = dict_scoring.copy() # save the original dictionary\n",
    "        for i in score.keys():\n",
    "            if len(set(y))>2:\n",
    "                if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                    score[i] = make_scorer(score[i], average = 'weighted') # make each function scorer\n",
    "                elif i==\"roc_auc\":\n",
    "                    score[i] = make_scorer(score[i], average = 'weighted', multi_class=\"ovo\",needs_proba=True) # make each function scorer\n",
    "                else:\n",
    "                    score[i] = make_scorer(score[i]) # make each function scorer\n",
    "                    \n",
    "            else:\n",
    "                score[i] = make_scorer(score[i]) # make each function scorer\n",
    "            \n",
    "    try:\n",
    "        scores = cross_validate(clf, x, y, scoring=score,\n",
    "                         cv=cv, return_train_score=False, n_jobs=-1,  fit_params=fit_params)\n",
    "    except:\n",
    "        scores = cross_validate(clf, x, y, scoring=score,\n",
    "                         cv=cv, return_train_score=False,  fit_params=fit_params)\n",
    "        \n",
    "     # Train test on the overall data\n",
    "    fit_start = time.time()\n",
    "    _model = clf\n",
    "    _model.fit(x, y)\n",
    "        \n",
    "    fit_end = time.time() - fit_start\n",
    "\n",
    "    \n",
    "    score_start = time.time()\n",
    "    y_pred = _model.predict(X_test)#>0.5).astype(int)\n",
    "    score_end = time.time() - score_start\n",
    "    \n",
    "    if save:\n",
    "        filename= name+\".sav\"\n",
    "        pickle.dump(_model, open(os.path.join(root_dir, dir_name,filename), 'wb'))\n",
    "    # initialisation \n",
    "    index = []\n",
    "    value = []\n",
    "    index.append(\"Model\")\n",
    "    value.append(name)\n",
    "    for i in scores:  # loop on each metric generate text and values\n",
    "        if i == \"estimator\":\n",
    "            continue\n",
    "        for j in enumerate(scores[i]):\n",
    "            index.append(i+\"_cv\"+str(j[0]+1))\n",
    "            value.append(j[1])\n",
    "        \n",
    "        \n",
    "        index.append(i+\"_mean\")\n",
    "        value.append(np.mean(scores[i]))\n",
    "        index.append(i+\"_std\")\n",
    "        value.append(np.std(scores[i]))\n",
    "    \n",
    "     # add metrics averall dataset on the dictionary \n",
    "    \n",
    "    for i in scores:    # compute metrics \n",
    "        if i == \"fit_time\":\n",
    "            \n",
    "            scores[i] = np.append(scores[i] ,fit_end)\n",
    "            index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "            value.append(fit_end)\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            \n",
    "            scores[i] = np.append(scores[i] ,score_end)\n",
    "            index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "            value.append(score_end)\n",
    "            continue\n",
    "              \n",
    "        \n",
    "        scores[i] = np.append(scores[i] ,score[i.split(\"test_\")[-1]](_model, X_test, y_test))\n",
    "        index.append(i.split(\"test_\")[-1]+'_overall')\n",
    "        value.append(scores[i][-1])\n",
    "    \n",
    "    return pd.DataFrame(data=value, index=index).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_8\"><h3>Multinomial Naive Bayes</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ciuciu/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 779 ms, sys: 1.37 s, total: 2.15 s\n",
      "Wall time: 4.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if multinomial_naive_bayes:\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(), xtrain_count,train_y_sw, xvalid_count, valid_y, name='NB_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(), xtrain_tfidf,train_y, xvalid_tfidf, valid_y, name='NB_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram,train_y, xvalid_tfidf_ngram, valid_y, name='NB_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars,train_y, xvalid_tfidf_ngram_chars, valid_y, name='NB_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_9\"><h3>Logistic Regression</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.48 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if logistic_regression:\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), xtrain_count,train_y_sw,xvalid_count, valid_y, name='LR_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), xtrain_tfidf,train_y, xvalid_tfidf, valid_y,name='LR_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), xtrain_tfidf_ngram,train_y, xvalid_tfidf_ngram, valid_y,name='LR_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), xtrain_tfidf_ngram_chars,train_y,xvalid_tfidf_ngram_chars, valid_y, name='LR_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_10\"><h3>SVM</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "svm_model = False\n",
    "if svm_model:\n",
    "    df_results = df_results.append(report(svm.SVC(), xtrain_count,train_y_sw,xvalid_count, valid_y, name='SVM_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), xtrain_tfidf,train_y, xvalid_tfidf, valid_y,name='SVM_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), xtrain_tfidf_ngram,train_y,xvalid_tfidf_ngram,valid_y, name='SVM_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(svm.SVC(), xtrain_tfidf_ngram_chars,train_y,xvalid_tfidf_ngram_chars, valid_y, name='SVM_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_11\"><h3>k-NN</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "k_nn_model = True\n",
    "if k_nn_model:\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), xtrain_count,train_y_sw,xvalid_count, valid_y, name='kNN_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), xtrain_tfidf,train_y,xvalid_tfidf, valid_y, name='kNN_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), xtrain_tfidf_ngram,train_y, xvalid_tfidf_ngram,valid_y,name='kNN_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(KNeighborsClassifier(n_neighbors=20, weights='distance', n_jobs=-1), xtrain_tfidf_ngram_chars,train_y,xvalid_tfidf_ngram_chars, valid_y, name='kNN_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_12\"><h3>RandomForest</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_forest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_forest' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if random_forest:\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), xtrain_count,train_y_sw, xvalid_count, valid_y,name='RF_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), xtrain_tfidf,train_y, xvalid_tfidf, valid_y,name='RF_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), xtrain_tfidf_ngram,train_y, xvalid_tfidf_ngram,valid_y, name='RF_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), xtrain_tfidf_ngram_chars,train_y,xvalid_tfidf_ngram_chars, valid_y, name='RF_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_13\"><h3>Stochastic Gradient Descent</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.42 s, sys: 4.66 s, total: 8.08 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if sgd:\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), xtrain_count,train_y_sw, xvalid_count, valid_y,name='SGD_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), xtrain_tfidf,train_y,xvalid_tfidf, valid_y, name='SGD_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), xtrain_tfidf_ngram,train_y, xvalid_tfidf_ngram,valid_y,name='SGD_N-Gram_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), xtrain_tfidf_ngram_chars,train_y, xvalid_tfidf_ngram_chars, valid_y, name='SGD_CharLevel_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_14\"><h3>Gradient Boosting</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 57s, sys: 203 ms, total: 5min 58s\n",
      "Wall time: 20min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), xtrain_count,train_y_sw, xvalid_count, valid_y,name='GB_Count_Vectors', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 3.45 s, total: 2min 18s\n",
      "Wall time: 7min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), xtrain_tfidf,train_y, xvalid_tfidf, valid_y,name='GB_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 9s, sys: 141 ms, total: 5min 9s\n",
      "Wall time: 14min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), xtrain_tfidf_ngram,train_y, xvalid_tfidf_ngram,valid_y, name='GB_N-Gram_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 20s, sys: 4.58 s, total: 9min 25s\n",
      "Wall time: 33min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), xtrain_tfidf_ngram_chars,train_y,xvalid_tfidf_ngram_chars, valid_y,  name='GB_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_15\"><h3>XGBoost Classifier</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the XGBoost have early stopping implemented with 10 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 57s, sys: 4.5 s, total: 13min 1s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,\\\n",
    "                         'eval_set':[(xvalid_count, valid_y_sw)]}\n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), xtrain_count,train_y_sw,xvalid_count, valid_y, name='XGB_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    else:\n",
    "        # run on CPU\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), xtrain_count,train_y_sw, xvalid_count, valid_y, name='XGB_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 24s, sys: 1.27 s, total: 10min 26s\n",
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,\\\n",
    "                         'eval_set':[(xvalid_tfidf, valid_y)]}\n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist', n_estimators=1000, subsample=0.8), xtrain_tfidf,train_y,xvalid_tfidf, valid_y, name='XGB_WordLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), xtrain_tfidf,train_y,xvalid_tfidf, valid_y, name='XGB_WordLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 54s, sys: 1.77 s, total: 16min 56s\n",
      "Wall time: 6min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,\\\n",
    "                         'eval_set':[(xvalid_tfidf_ngram, valid_y)]}\n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), xtrain_tfidf_ngram,train_y,  xvalid_tfidf_ngram, valid_y, name='XGB_N-Gram_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), xtrain_tfidf_ngram,train_y, xvalid_tfidf_ngram, valid_y,name='XGB_N-Gram_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50min 51s, sys: 11.1 s, total: 51min 2s\n",
      "Wall time: 19min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,\\\n",
    "                         'eval_set':[(xvalid_tfidf_ngram_chars, valid_y)]}\n",
    "    if num_gpu>0:    # Config for GPU\n",
    "        df_results = df_results.append(report(XGBClassifier(tree_method='gpu_hist',n_estimators=1000, subsample=0.8), xtrain_tfidf_ngram_chars,train_y, xvalid_tfidf_ngram_chars, valid_y, name='XGB_CharLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    else:\n",
    "        df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), xtrain_tfidf_ngram_chars,train_y, xvalid_tfidf_ngram_chars, valid_y, name='XGB_CharLevel_TF-IDF', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics, save=save_model))\n",
    "    if save_results:\n",
    "        df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_16\"><h3>Adaboost Classifier</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 12.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "if adaboost_classifier:\n",
    "    # work in progress\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), xtrain_count,train_y_sw, xvalid_count, valid_y, name='Adaboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), xtrain_tfidf,train_y,xvalid_tfidf, valid_y,name='Adaboost_WordLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), xtrain_tfidf_ngram,train_y, xvalid_tfidf_ngram, valid_y,name='Adaboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(AdaBoostClassifier(n_estimators=1000), xtrain_tfidf_ngram_chars,train_y, xvalid_tfidf_ngram_chars, valid_y,name='Adaboost_CharLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_17\"><h3>Catboost Classifier</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 14.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "if catboost_classifier:\n",
    "    # work in progress\n",
    "    if num_gpu>0:  # test gpu available\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), xtrain_count,train_y_sw, xvalid_count, valid_y, name='Catboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), xtrain_tfidf,train_y,xvalid_tfidf, valid_y,name='Catboost_WordLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), xtrain_tfidf_ngram,train_y, xvalid_tfidf_ngram, valid_y,name='Catboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10, task_type=\"GPU\"), xtrain_tfidf_ngram_chars,train_y, xvalid_tfidf_ngram_chars, valid_y,name='Catboost_CharLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "    else:\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), xtrain_count,train_y_sw, xvalid_count, valid_y, name='Catboost_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), xtrain_tfidf,train_y,xvalid_tfidf, valid_y,name='Catboost_WordLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), xtrain_tfidf_ngram,train_y, xvalid_tfidf_ngram, valid_y,name='Catboost_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics))\n",
    "        df_results = df_results.append(report(CatBoostClassifier(n_estimators=1000, early_stopping_rounds=10), xtrain_tfidf_ngram_chars,train_y, xvalid_tfidf_ngram_chars, valid_y,name='Catboost_CharLevel_TF-IDF', cv=CV_splits, dict_scoring=score_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_18\"><h3>LightGBM Classifier</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 17.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "if lightgbm_classifier:\n",
    "    # work in progress\n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(xvalid_count, valid_y)]}\n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_count,train_y_sw, xvalid_count, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_count,train_y_sw, xvalid_count, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(xvalid_tfidf, valid_y)]}\n",
    "    \n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf,train_y_sw, xvalid_tfidf, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf,train_y_sw, xvalid_tfidf, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(xvalid_tfidf_ngram, valid_y)]}\n",
    "    \n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf_ngram,train_y_sw, xvalid_tfidf_ngram, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf_ngram,train_y_sw, xvalid_tfidf_ngram, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    \n",
    "    fit_params = {'early_stopping_rounds':10,'eval_set':[(xvalid_tfidf_ngram_chars, valid_y)]}\n",
    "    \n",
    "    if num_gpu>0:\n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000, device = \"gpu\"), xtrain_tfidf_ngram_chars,train_y_sw, xvalid_tfidf_ngram_chars, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    else:   \n",
    "        df_results = df_results.append(report(LGBMClassifier(n_estimators = 1000), xtrain_tfidf_ngram_chars,train_y_sw, xvalid_tfidf_ngram_chars, valid_y, name='LGM_Count_Vectors', cv=CV_splits, fit_params=fit_params, dict_scoring=score_metrics))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center><a id=\"part_6_19\">ExtraTreesClassifier</a></center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 25s, sys: 3.45 s, total: 2min 28s\n",
      "Wall time: 5min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "if extratrees_classifier:\n",
    "    # work in progress\n",
    "    df_results = df_results.append(report(ExtraTreesClassifier(n_estimators=100,min_impurity_decrease=1e-7, random_state=42), xtrain_count,train_y_sw, xvalid_count, valid_y, name='ExtraTrees_Count_Vectors', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ExtraTreesClassifier(n_estimators=100,min_impurity_decrease=1e-7, random_state=42), xtrain_tfidf,train_y,xvalid_tfidf, valid_y,name='ExtraTrees_WordLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ExtraTreesClassifier(n_estimators=100,min_impurity_decrease=1e-7, random_state=42), xtrain_tfidf_ngram,train_y, xvalid_tfidf_ngram, valid_y,name='ExtraTrees_N-Gram_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    df_results = df_results.append(report(ExtraTreesClassifier(n_estimators=100,min_impurity_decrease=1e-7, random_state=42), xtrain_tfidf_ngram_chars,train_y, xvalid_tfidf_ngram_chars, valid_y,name='ExtraTrees_CharLevel_TF-IDF', cv=CV_splits,  dict_scoring=score_metrics, save=save_model))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_7\"><h2>Deep Learning</h2></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<h6>Back to top</h6>](#content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cohen’s kappa</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function [cohen_kappa_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score) computes [Cohen’s kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa) statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.\n",
    "\n",
    "The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).\n",
    "\n",
    "Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Balanced Accuracy</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the balanced accuracy\n",
    "\n",
    "The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.\n",
    "\n",
    "The best value is 1 and the worst value is 0 when adjusted=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Early Stopping, Model saving, Class weight configuration</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_w = {}\n",
    "for i in zip(range(len(class_weights)), class_weights):\n",
    "    class_w[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_NN(model, X, y, X_test, y_test,name=\"NN\", fit_params=None, scoring=None, n_splits=5, save=save_model, batch_size = 32,  use_multiprocessing=True):\n",
    "    '''\n",
    "    Function create a metric report automatically with cross_validate function.\n",
    "    @param model: (model) neural network model\n",
    "    @param X: (list or matrix or tensor) training X data\n",
    "    @param y: (list) label data \n",
    "    @param X_test: (list or matrix or tensor) testing X data\n",
    "    @param y_test: (list) label test data \n",
    "    @param name: (string) name of the model (default classifier)\n",
    "    @param fit_aparams: (dict) add parameters for model fitting \n",
    "    @param scoring: (dict) dictionary of metrics and names\n",
    "    @param n_splits: (int) number of fold for cross-validation (default 5)\n",
    "    @return: (pandas.dataframe) dataframe containing all the results of the metrics \n",
    "    for each fold and the mean and std for each of them\n",
    "    '''\n",
    "    # ---- Parameters initialisation\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='auto', patience=3)\n",
    "    seed = 42\n",
    "    k = 1\n",
    "    np.random.seed(seed)\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Creation of list for each metric\n",
    "    if scoring==None:        # create a dictionary if none is passed\n",
    "        dic_scoring = {}\n",
    "    if scoring!=None:        # save the dict \n",
    "        dic_score = scoring.copy()\n",
    "    \n",
    "    dic_score[\"fit_time\"] = None   # initialisation for time fitting and scoring\n",
    "    dic_score[\"score_time\"] = None\n",
    "    scorer = {}\n",
    "    for i in dic_score.keys(): \n",
    "        scorer[i] = []\n",
    "    \n",
    "    index = [\"Model\"]\n",
    "    results = [name]\n",
    "    # ---- Loop on k-fold for cross-valisation\n",
    "    for train, test in kfold.split(X, y):   # training NN on each fold \n",
    "        # create model\n",
    "        print(f\"k-fold : {k}\")\n",
    "        fit_start = time.time()\n",
    "        _model = tf.keras.models.clone_model(model)\n",
    "        if len(np.unique(y))==2: # binary\n",
    "            _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        else:  # multiclass \n",
    "            _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        _model.fit(X[train], y[train],\n",
    "                        epochs=1000, callbacks=[es], validation_data=(X[test], y[test]),\n",
    "                         verbose=False, batch_size = batch_size,  use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "        fit_end = time.time() - fit_start\n",
    "\n",
    "        score_start = time.time()\n",
    "        y_pred = (_model.predict(X[test])>0.5).astype(int)\n",
    "        score_end = time.time() - score_start\n",
    "        #if len(set(y))>2:\n",
    "        #    y_pred =np.argmax(y_pred,axis=1)\n",
    "        #print(y_test[0], y_pred[0])\n",
    "        if len(set(y))==2:\n",
    "            print(f\"Precision: {round(100*precision_score(y[test], y_pred), 3)}% , Recall: {round(100*recall_score(y[test], y_pred), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "        else: \n",
    "            print(f\"Precision: {round(100*precision_score(y[test], np.argmax(y_pred,axis=1), average='weighted'), 3)}% , Recall: \\\n",
    "        {round(100*recall_score(y[test], np.argmax(y_pred,axis=1), average='weighted'), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "        \n",
    "        \n",
    "        # ---- save each metric\n",
    "        for i in dic_score.keys():    # compute metrics \n",
    "            if i == \"fit_time\":\n",
    "                scorer[i].append(fit_end)\n",
    "                index.append(i+'_cv'+str(k))\n",
    "                results.append(fit_end)\n",
    "                continue\n",
    "            if i == \"score_time\":\n",
    "                scorer[i].append(score_end)\n",
    "                index.append(i+'_cv'+str(k))\n",
    "                results.append(score_end)\n",
    "                continue\n",
    "            \n",
    "            if len(set(y))>2:\n",
    "                if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                    scorer[i].append(dic_score[i](y[test], np.argmax(y_pred,axis=1), average = 'weighted')) # make each function scorer\n",
    "\n",
    "                elif i==\"roc_auc\":\n",
    "                    scorer[i].append(dic_score[i](to_categorical(y[test]), y_pred, average = 'macro', multi_class=\"ovo\")) # make each function scorer\n",
    "                else:\n",
    "                    scorer[i].append(dic_score[i]( y[test], np.argmax(y_pred,axis=1))) # make each function scorer\n",
    "\n",
    "            else:\n",
    "                scorer[i].append(dic_score[i]( y[test], y_pred)) # make each function scorer\n",
    "            #scorer[i].append(dic_score[i]( y[test], y_pred))\n",
    "            index.append(\"test_\"+i+'_cv'+str(k))\n",
    "            results.append(scorer[i][-1])\n",
    "        K.clear_session()\n",
    "        del _model\n",
    "        k+=1\n",
    "    \n",
    "    # Train test on the overall data\n",
    "    print(\"Overall train-test data\")\n",
    "    fit_start = time.time()\n",
    "    _model =  tf.keras.models.clone_model(model)\n",
    "    if len(np.unique(y))==2: # binary\n",
    "        _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    else:  # multiclass \n",
    "        _model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        _model.fit(X[train], y[train],\n",
    "                        epochs=1000, callbacks=[es], validation_data=(X[test], y[test]),\n",
    "                         verbose=False)\n",
    "    if save:\n",
    "        check_p = tf.keras.callbacks.ModelCheckpoint(os.path.join(root_dir, dir_name, name+\".h5\"), save_best_only=True)\n",
    "        _model.fit(X, y,epochs=1000, callbacks=[es, check_p], validation_split=0.2, batch_size = batch_size, \n",
    "                   verbose=False, use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "    else:\n",
    "        _model.fit(X, y,epochs=1000, callbacks=[es],  validation_split=0.2, batch_size = batch_size, \n",
    "                   verbose=False, use_multiprocessing=use_multiprocessing)\n",
    "        \n",
    "    fit_end = time.time() - fit_start\n",
    "\n",
    "    #_acc = _model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    score_start = time.time()\n",
    "    y_pred = (_model.predict(X_test)>0.5).astype(int)\n",
    "    score_end = time.time() - score_start\n",
    "    #if len(set(y))>2:\n",
    "    #    y_pred =np.argmax(y_pred,axis=1)\n",
    "    if len(set(y))==2:\n",
    "        print(f\"Precision: {round(100*precision_score(y_test, y_pred), 3)}% , Recall: {round(100*recall_score(y_test, y_pred), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "    else: \n",
    "        print(f\"Precision: {round(100*precision_score(y_test, np.argmax(y_pred,axis=1), average='weighted'), 3)}% , Recall: \\\n",
    "        {round(100*recall_score(y_test, np.argmax(y_pred,axis=1), average='weighted'), 3)}%, Time \\t {round(fit_end, 4)} ms\")\n",
    "\n",
    "    # Compute mean and std for each metric\n",
    "    for i in scorer: \n",
    "        \n",
    "        results.append(np.mean(scorer[i]))\n",
    "        results.append(np.std(scorer[i]))\n",
    "        if i == \"fit_time\":\n",
    "            index.append(i+\"_mean\")\n",
    "            index.append(i+\"_std\")\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            index.append(i+\"_mean\")\n",
    "            index.append(i+\"_std\")\n",
    "            continue\n",
    "        \n",
    "        index.append(\"test_\"+i+\"_mean\")\n",
    "        index.append(\"test_\"+i+\"_std\")\n",
    "    # add metrics averall dataset on the dictionary \n",
    "    for i in dic_score.keys():    # compute metrics \n",
    "        if i == \"fit_time\":\n",
    "            scorer[i].append(fit_end)\n",
    "            index.append(i+'_overall')\n",
    "            results.append(fit_end)\n",
    "            continue\n",
    "        if i == \"score_time\":\n",
    "            scorer[i].append(score_end)\n",
    "            index.append(i+'_overall')\n",
    "            results.append(score_end)\n",
    "            continue\n",
    "        \n",
    "        if len(set(y))>2:\n",
    "            if i in [\"prec\", \"recall\", \"f1-score\"]:\n",
    "                scorer[i].append(dic_score[i](y_test, np.argmax(y_pred,axis=1), average = 'weighted')) # make each function scorer\n",
    "\n",
    "            elif i==\"roc_auc\":\n",
    "                scorer[i].append(dic_score[i](to_categorical(y_test), y_pred, average = 'weighted', multi_class=\"ovo\")) # make each function scorer\n",
    "            else:\n",
    "                scorer[i].append(dic_score[i]( y_test, np.argmax(y_pred,axis=1))) # make each function scorer\n",
    "\n",
    "        else:\n",
    "            #scorer[i].append(dic_score[i]( y[test], y_pred))                             \n",
    "            scorer[i].append(dic_score[i](_model, X_test, y_test))\n",
    "        index.append(i+'_overall')\n",
    "        results.append(scorer[i][-1])\n",
    "    \n",
    "            \n",
    "    return pd.DataFrame(results, index=index).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_7_1\"><h3>Shallow Neural Networks</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_neural_networks(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a shallow neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) shallow neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 16)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      \n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "        \n",
    "      #keras.layers.Dense(6, activation=\"relu\"),\n",
    "      keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    return model # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 82.724% , Recall:         82.5%, Time \t 194.4298 ms\n",
      "k-fold : 2\n",
      "Precision: 85.186% , Recall:         85.3%, Time \t 156.3387 ms\n",
      "k-fold : 3\n",
      "Precision: 84.221% , Recall:         84.3%, Time \t 193.2475 ms\n",
      "k-fold : 4\n",
      "Precision: 81.264% , Recall:         80.7%, Time \t 180.3254 ms\n",
      "k-fold : 5\n",
      "Precision: 84.559% , Recall:         84.5%, Time \t 158.6139 ms\n",
      "Overall train-test data\n",
      "Precision: 82.642% , Recall:         82.561%, Time \t 207.1252 ms\n",
      "CPU times: user 34min 48s, sys: 2min 55s, total: 37min 44s\n",
      "Wall time: 18min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if shallow_network:\n",
    "    df_results = df_results.append(cross_validate_NN(shallow_neural_networks(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y,  name=\"Shallow_NN_WE\", scoring=score_metrics, \n",
    "                                                     n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ = pd.read_csv(NAME_SAVE_FILE+\".csv\", sep=\";\")\n",
    "#df_results = df_results.append(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_7_2\"><h3>Deep Neural Networks</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_networks(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a deep neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) deep neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 50)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 84.717% , Recall:         84.9%, Time \t 275.3164 ms\n",
      "k-fold : 2\n",
      "Precision: 84.63% , Recall:         84.7%, Time \t 236.9119 ms\n",
      "k-fold : 3\n",
      "Precision: 82.973% , Recall:         83.1%, Time \t 306.1804 ms\n",
      "k-fold : 4\n",
      "Precision: 83.693% , Recall:         83.7%, Time \t 200.7795 ms\n",
      "k-fold : 5\n",
      "Precision: 84.026% , Recall:         84.2%, Time \t 251.8813 ms\n",
      "Overall train-test data\n",
      "Precision: 83.427% , Recall:         83.562%, Time \t 343.1086 ms\n",
      "CPU times: user 1h 3min 53s, sys: 17min 33s, total: 1h 21min 26s\n",
      "Wall time: 26min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if deep_nn:\n",
    "    df_results = df_results.append(cross_validate_NN(deep_neural_networks(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y,  name=\"Deep_NN_WE\",scoring=score_metrics, \n",
    "                                                     n_splits=CV_splits , save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Deep Neural Networks variation 1</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_networks_var1(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a deep neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) deep neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(16, activation=\"relu\"),#tf.nn.swish),\n",
    "      keras.layers.Dense(1  if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 82.378% , Recall:         81.8%, Time \t 1559.4154 ms\n",
      "k-fold : 2\n",
      "Precision: 84.272% , Recall:         84.4%, Time \t 2106.2346 ms\n",
      "k-fold : 3\n",
      "Precision: 83.175% , Recall:         83.2%, Time \t 1223.3124 ms\n",
      "k-fold : 4\n",
      "Precision: 83.991% , Recall:         84.2%, Time \t 1581.2906 ms\n",
      "k-fold : 5\n",
      "Precision: 83.361% , Recall:         83.2%, Time \t 1805.29 ms\n",
      "Overall train-test data\n",
      "Precision: 82.708% , Recall:         82.258%, Time \t 1981.0032 ms\n",
      "CPU times: user 1h 51min 35s, sys: 5h 4min, total: 6h 55min 35s\n",
      "Wall time: 2h 50min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if deep_nn:\n",
    "    df_results = df_results.append(cross_validate_NN(deep_neural_networks_var1(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y, name=\"Deep_NN_var1_WE\", \n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Deep Neural Networks variation 2</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_networks_var2(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a deep neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) deep neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(32, activation='relu'),\n",
    "      keras.layers.Dense(16, activation='relu'),\n",
    "      keras.layers.Dense(1  if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 85.576% , Recall:         85.6%, Time \t 1114.6643 ms\n",
      "k-fold : 2\n",
      "Precision: 83.043% , Recall:         82.9%, Time \t 1744.8379 ms\n",
      "k-fold : 3\n",
      "Precision: 83.234% , Recall:         83.2%, Time \t 930.7575 ms\n",
      "k-fold : 4\n",
      "Precision: 82.865% , Recall:         82.2%, Time \t 1045.8892 ms\n",
      "k-fold : 5\n",
      "Precision: 85.054% , Recall:         84.7%, Time \t 1450.529 ms\n",
      "Overall train-test data\n",
      "Precision: 81.922% , Recall:         81.477%, Time \t 2707.8716 ms\n",
      "CPU times: user 1h 37min 28s, sys: 4h 25min 32s, total: 6h 3min\n",
      "Wall time: 2h 29min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if deep_nn:\n",
    "    df_results = df_results.append(cross_validate_NN(deep_neural_networks_var2(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y,  name=\"Deep_NN_var2_WE\",\n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_7_3\"><h3>Recurent Neural Network (RNN)</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a recurrent neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) recurrent neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.SimpleRNN(32, return_sequences=True, activation='tanh'),\n",
    "    keras.layers.SimpleRNN(32, return_sequences=True, activation='tanh'),\n",
    "    keras.layers.SimpleRNN(32, return_sequences=True, activation='tanh'),\n",
    "    keras.layers.SimpleRNN(32, activation=\"tanh\"),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "   \n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 68.413% , Recall:         68.4%, Time \t 782.389 ms\n",
      "k-fold : 2\n",
      "Precision: 67.519% , Recall:         67.5%, Time \t 1058.2341 ms\n",
      "k-fold : 3\n",
      "Precision: 68.558% , Recall:         69.0%, Time \t 775.5003 ms\n",
      "k-fold : 4\n",
      "Precision: 70.169% , Recall:         70.2%, Time \t 699.8425 ms\n",
      "k-fold : 5\n",
      "Precision: 69.972% , Recall:         70.3%, Time \t 772.2779 ms\n",
      "Overall train-test data\n",
      "Precision: 72.429% , Recall:         72.658%, Time \t 4246.7463 ms\n",
      "CPU times: user 3h 14min 26s, sys: 2h 10min 23s, total: 5h 24min 49s\n",
      "Wall time: 2h 19min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if rnn:\n",
    "    df_results = df_results.append(cross_validate_NN(create_rnn_model(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y, name=\"RNN_WE\",\n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_7_4\"><h3>Convolutional Neural Network (CNN)</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a convulational neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) convulational neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) +1, 100)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.Conv1D(128, 5, activation=\"relu\"),#tf.nn.swish), # padding='same'\n",
    "    keras.layers.MaxPooling1D(pool_size=4),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Conv1D(64, 5, activation=\"relu\"),#tf.nn.swish),\n",
    "    keras.layers.MaxPooling1D(pool_size=4),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Conv1D(32, 5, activation=\"relu\"),#tf.nn.swish),\n",
    "    keras.layers.GlobalMaxPooling1D(),\n",
    "    #keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 82.07% , Recall:         82.3%, Time \t 1530.0609 ms\n",
      "k-fold : 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-02389d38d484>\u001b[0m in \u001b[0;36mcross_validate_NN\u001b[0;34m(model, X, y, X_test, y_test, name, fit_params, scoring, n_splits, save, batch_size, use_multiprocessing)\u001b[0m\n\u001b[1;32m     49\u001b[0m                   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                   metrics=['accuracy'])\n\u001b[0;32m---> 51\u001b[0;31m         _model.fit(X[train], y[train],\n\u001b[0m\u001b[1;32m     52\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                          verbose=False, batch_size = batch_size,  use_multiprocessing=use_multiprocessing)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if cnn:\n",
    "    df_results = df_results.append(cross_validate_NN(create_conv_model(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y, name=\"CNN_WE\",\n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_7_4\"><h3>Long Short Term Memory (LSTM)</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a lstm for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model)lstm \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) +1, 100)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index)+1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.LSTM(32, activation='tanh'),\n",
    "    #keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 80.895% , Recall:         81.2%, Time \t 1335.5262 ms\n",
      "k-fold : 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-02389d38d484>\u001b[0m in \u001b[0;36mcross_validate_NN\u001b[0;34m(model, X, y, X_test, y_test, name, fit_params, scoring, n_splits, save, batch_size, use_multiprocessing)\u001b[0m\n\u001b[1;32m     49\u001b[0m                   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                   metrics=['accuracy'])\n\u001b[0;32m---> 51\u001b[0;31m         _model.fit(X[train], y[train],\n\u001b[0m\u001b[1;32m     52\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                          verbose=False, batch_size = batch_size,  use_multiprocessing=use_multiprocessing)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if lstm:\n",
    "    df_results = df_results.append(cross_validate_NN(create_lstm_model(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y, name=\"LSTM_WE\",\n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_7_6\"><h3>CNN – LSTM</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a convulational neural network lstm for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) convulational neural network lstm\n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    keras.layers.MaxPooling1D(pool_size=4),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.LSTM(32, activation='tanh'),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 82.373% , Recall:         82.4%, Time \t 1305.375 ms\n",
      "k-fold : 2\n",
      "Precision: 82.59% , Recall:         82.8%, Time \t 582.0354 ms\n",
      "k-fold : 3\n",
      "Precision: 79.324% , Recall:         78.1%, Time \t 530.9649 ms\n",
      "k-fold : 4\n",
      "Precision: 81.643% , Recall:         81.4%, Time \t 775.3419 ms\n",
      "k-fold : 5\n",
      "Precision: 82.409% , Recall:         82.6%, Time \t 843.6723 ms\n",
      "Overall train-test data\n",
      "Precision: 79.582% , Recall:         79.881%, Time \t 3882.9557 ms\n",
      "CPU times: user 2h 57min 26s, sys: 2h 38min 13s, total: 5h 35min 39s\n",
      "Wall time: 2h 12min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if cnn_lstm:\n",
    "    df_results = df_results.append(cross_validate_NN(create_cnn_lstm_model(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y, name=\"CNN_LSTM_WE\", \n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_7_7\"><h3>CNN – GRU</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_gru_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a convulational neural network GRU for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) convulational neural network GRU\n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    keras.layers.MaxPooling1D(pool_size=4),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.GRU(32, activation='tanh'),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 81.52% , Recall:         81.7%, Time \t 1118.5525 ms\n",
      "k-fold : 2\n",
      "Precision: 80.467% , Recall:         80.2%, Time \t 1224.2749 ms\n",
      "k-fold : 3\n",
      "Precision: 79.721% , Recall:         79.2%, Time \t 1546.9579 ms\n",
      "k-fold : 4\n",
      "Precision: 81.388% , Recall:         81.7%, Time \t 917.4834 ms\n",
      "k-fold : 5\n",
      "Precision: 80.803% , Recall:         80.2%, Time \t 1099.9662 ms\n",
      "Overall train-test data\n",
      "Precision: 76.759% , Recall:         76.992%, Time \t 1887.3444 ms\n",
      "CPU times: user 2h 47min 36s, sys: 3h 5min 26s, total: 5h 53min 3s\n",
      "Wall time: 2h 10min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if cnn_gru:\n",
    "    df_results = df_results.append(cross_validate_NN(create_cnn_gru_model(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y, name=\"CNN_GRU_WE\", \n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_7_8\"><h3>Gated Recurrent Units – GRU</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras.layers.GRU(\n",
    "    units, activation='tanh', recurrent_activation='sigmoid', use_bias=True,\n",
    "    kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
    "    bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None,\n",
    "    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "    recurrent_constraint=None, bias_constraint=None, dropout=0.0,\n",
    "    recurrent_dropout=0.0, implementation=2, return_sequences=False,\n",
    "    return_state=False, go_backwards=False, stateful=False, unroll=False,\n",
    "    time_major=False, reset_after=True, **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a GRU for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) GRU\n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.GRU(32, activation='tanh'),\n",
    "    #keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 79.34% , Recall:         79.5%, Time \t 1148.7309 ms\n",
      "k-fold : 2\n",
      "Precision: 79.238% , Recall:         78.6%, Time \t 1414.9595 ms\n",
      "k-fold : 3\n",
      "Precision: 78.787% , Recall:         78.5%, Time \t 1198.71 ms\n",
      "k-fold : 4\n",
      "Precision: 76.515% , Recall:         76.7%, Time \t 1334.6357 ms\n",
      "k-fold : 5\n",
      "Precision: 74.861% , Recall:         73.8%, Time \t 1640.1136 ms\n",
      "Overall train-test data\n",
      "Precision: 74.953% , Recall:         75.186%, Time \t 1882.148 ms\n",
      "CPU times: user 2h 10min 32s, sys: 4h 20min 45s, total: 6h 31min 18s\n",
      "Wall time: 2h 24min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gru:\n",
    "    df_results = df_results.append(cross_validate_NN(create_gru_model(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y, name=\"GRU_WE\", \n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_7_9\"><h3>Bidirectional RNN</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirec_rnn_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a bidirectionnal rnn for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) bidirectionnal rnn\n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        print(\"Pre-trained model used\")\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.Bidirectional(keras.layers.SimpleRNN(32, return_sequences=True, activation=\"tanh\")),\n",
    "    keras.layers.Bidirectional(keras.layers.SimpleRNN(32, return_sequences=True, activation=\"tanh\")),\n",
    "    keras.layers.Bidirectional(keras.layers.SimpleRNN(32, return_sequences=True, activation=\"tanh\")),\n",
    "    keras.layers.Bidirectional(keras.layers.SimpleRNN(32, activation=\"tanh\")),\n",
    "    #keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 73.593% , Recall:         72.5%, Time \t 1422.7513 ms\n",
      "k-fold : 2\n",
      "Precision: 71.179% , Recall:         71.5%, Time \t 1400.2444 ms\n",
      "k-fold : 3\n",
      "Precision: 74.185% , Recall:         73.9%, Time \t 1453.4991 ms\n",
      "k-fold : 4\n",
      "Precision: 73.89% , Recall:         73.7%, Time \t 1507.8053 ms\n",
      "k-fold : 5\n",
      "Precision: 67.987% , Recall:         66.9%, Time \t 1064.5525 ms\n",
      "Overall train-test data\n",
      "Precision: 71.138% , Recall:         71.435%, Time \t 1959.6311 ms\n",
      "CPU times: user 4h 54min 28s, sys: 2h 53min 19s, total: 7h 47min 47s\n",
      "Wall time: 2h 28min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if bidirectional_rnn:\n",
    "    df_results = df_results.append(cross_validate_NN(create_bidirec_rnn_model(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y, name=\"BiRNN_WE\",\n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_7_10\"><h3>Bidirectional LSTM</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirec_lstm_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a bidirectionnal lstm for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) bidirectionnal lstm\n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(32, activation=\"tanh\")),\n",
    "    #keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 79.252% , Recall:         79.3%, Time \t 1265.6101 ms\n",
      "k-fold : 2\n",
      "Precision: 76.696% , Recall:         74.9%, Time \t 642.1987 ms\n",
      "k-fold : 3\n",
      "Precision: 78.711% , Recall:         79.1%, Time \t 991.8199 ms\n",
      "k-fold : 4\n",
      "Precision: 78.48% , Recall:         78.3%, Time \t 750.2021 ms\n",
      "k-fold : 5\n",
      "Precision: 80.493% , Recall:         80.2%, Time \t 1214.6387 ms\n",
      "Overall train-test data\n",
      "Precision: 78.487% , Recall:         78.809%, Time \t 1905.5689 ms\n",
      "CPU times: user 3h 2min 51s, sys: 2h 44min 29s, total: 5h 47min 20s\n",
      "Wall time: 1h 53min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if bidirectional_lstm:\n",
    "    df_results = df_results.append(cross_validate_NN(create_bidirec_lstm_model(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y, name=\"BiLSTM_WE\",\n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_7_11\"><h3>Bidirectional GRU</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirec_gru_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a bidirectionnal gru for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) bidirectionnal gru\n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(32, activation=\"tanh\")),\n",
    "    #keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 79.258% , Recall:         79.1%, Time \t 2267.8968 ms\n",
      "k-fold : 2\n",
      "Precision: 78.649% , Recall:         78.6%, Time \t 1203.2646 ms\n",
      "k-fold : 3\n",
      "Precision: 79.784% , Recall:         80.2%, Time \t 736.2815 ms\n",
      "k-fold : 4\n",
      "Precision: 78.502% , Recall:         78.7%, Time \t 1113.8464 ms\n",
      "k-fold : 5\n",
      "Precision: 77.027% , Recall:         76.7%, Time \t 701.5721 ms\n",
      "Overall train-test data\n",
      "Precision: 75.993% , Recall:         75.489%, Time \t 2333.2186 ms\n",
      "CPU times: user 3h 13min 59s, sys: 3h 56min 42s, total: 7h 10min 41s\n",
      "Wall time: 2h 19min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if bidirectional_gru:\n",
    "    df_results = df_results.append(cross_validate_NN(create_bidirec_gru_model(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y, name=\"BiGRU_WE\",\n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_7_12\"><h3>Recurrent Convolutional Neural Network (RCNN)</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rcnn(X, word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a rcnn for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model)  rcnn\n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300,input_length=X.shape[1], weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.SpatialDropout1D(0.3),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(32,return_sequences=True, activation=\"tanh\")),\n",
    "    keras.layers.Convolution1D(32, 3, activation=\"tanh\"),\n",
    "    keras.layers.GlobalMaxPool1D(),\n",
    "    keras.layers.Dense(25, activation=\"relu\"),\n",
    "    #keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 84.964% , Recall:         84.6%, Time \t 2744.4586 ms\n",
      "k-fold : 2\n",
      "Precision: 81.62% , Recall:         81.7%, Time \t 1137.9364 ms\n",
      "k-fold : 3\n",
      "Precision: 83.124% , Recall:         82.3%, Time \t 1509.8658 ms\n",
      "k-fold : 4\n",
      "Precision: 84.312% , Recall:         84.5%, Time \t 2044.5137 ms\n",
      "k-fold : 5\n",
      "Precision: 82.687% , Recall:         82.9%, Time \t 1042.8026 ms\n",
      "Overall train-test data\n",
      "Precision: 80.08% , Recall:         79.031%, Time \t 3589.5849 ms\n",
      "CPU times: user 4h 55min 9s, sys: 5h 34min 17s, total: 10h 29min 27s\n",
      "Wall time: 3h 21min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if rcnn:\n",
    "    df_results = df_results.append(cross_validate_NN(create_rcnn(train_seq_x, word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y, name=\"RCNN_WE\",\n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Recurrent Convolutional Neural Network variation 1</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rcnn_var1(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a rcnn for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model)  rcnn\n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.SpatialDropout1D(0.3),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(32,return_sequences=True, activation=\"tanh\")),\n",
    "    keras.layers.Convolution1D(32, 3, activation=\"relu\"),\n",
    "    keras.layers.GlobalMaxPool1D(),\n",
    "    keras.layers.Dense(25, activation=\"relu\"),\n",
    "    #keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 83.302% , Recall:         82.9%, Time \t 1061.3561 ms\n",
      "k-fold : 2\n",
      "Precision: 82.192% , Recall:         82.0%, Time \t 1922.9588 ms\n",
      "k-fold : 3\n",
      "Precision: 84.137% , Recall:         84.3%, Time \t 1468.2757 ms\n",
      "k-fold : 4\n",
      "Precision: 81.854% , Recall:         81.7%, Time \t 1517.3627 ms\n",
      "k-fold : 5\n",
      "Precision: 76.477% , Recall:         71.8%, Time \t 626.5493 ms\n",
      "Overall train-test data\n",
      "Precision: 82.609% , Recall:         82.328%, Time \t 1845.3655 ms\n",
      "CPU times: user 3h 50min 23s, sys: 3h 31min 48s, total: 7h 22min 12s\n",
      "Wall time: 2h 21min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if rcnn:\n",
    "    df_results = df_results.append(cross_validate_NN(create_rcnn_var1(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y, name=\"RCNN_var1_WE\",\n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Recurrent Convulational Neural Network variation 2</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rcnn_var2(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a rcnn for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model)  rcnn\n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.SpatialDropout1D(0.3),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(32,return_sequences=True, activation=\"tanh\")),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(32,return_sequences=True, activation=\"tanh\")),\n",
    "    keras.layers.Convolution1D(32, 3, activation=\"relu\"),\n",
    "    keras.layers.GlobalMaxPool1D(),\n",
    "    keras.layers.Dense(25, activation=\"relu\"),\n",
    "    #keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 84.059% , Recall:         84.1%, Time \t 1434.183 ms\n",
      "k-fold : 2\n",
      "Precision: 82.61% , Recall:         82.5%, Time \t 2225.3979 ms\n",
      "k-fold : 3\n",
      "Precision: 81.091% , Recall:         81.4%, Time \t 1876.8313 ms\n",
      "k-fold : 4\n",
      "Precision: 82.126% , Recall:         82.3%, Time \t 2824.7587 ms\n",
      "k-fold : 5\n",
      "Precision: 82.59% , Recall:         82.8%, Time \t 2787.4094 ms\n",
      "Overall train-test data\n",
      "Precision: 81.509% , Recall:         80.883%, Time \t 2810.9364 ms\n",
      "CPU times: user 6h 57min 57s, sys: 4h 36min 2s, total: 11h 34min\n",
      "Wall time: 3h 53min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if rcnn:\n",
    "    df_results = df_results.append(cross_validate_NN(create_rcnn_var2(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y, name=\"RCNN_var2_WE\",\n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Recurrent Convulational Neural Network variation 3</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rcnn_var3(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.SpatialDropout1D(0.3),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(32,return_sequences=True, activation=\"tanh\")),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(32,return_sequences=True, activation=\"tanh\")),\n",
    "    keras.layers.Convolution1D(32, 3, activation=\"relu\"),\n",
    "    keras.layers.GlobalMaxPool1D(),\n",
    "    keras.layers.Dense(25, activation=\"relu\"),\n",
    "    #keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "Precision: 84.093% , Recall:         84.2%, Time \t 1692.9328 ms\n",
      "k-fold : 2\n",
      "Precision: 82.757% , Recall:         83.0%, Time \t 1840.3297 ms\n",
      "k-fold : 3\n",
      "Precision: 81.978% , Recall:         82.3%, Time \t 1602.0638 ms\n",
      "k-fold : 4\n",
      "Precision: 82.374% , Recall:         82.4%, Time \t 2219.6949 ms\n",
      "k-fold : 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-02389d38d484>\u001b[0m in \u001b[0;36mcross_validate_NN\u001b[0;34m(model, X, y, X_test, y_test, name, fit_params, scoring, n_splits, save, batch_size, use_multiprocessing)\u001b[0m\n\u001b[1;32m     49\u001b[0m                   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                   metrics=['accuracy'])\n\u001b[0;32m---> 51\u001b[0;31m         _model.fit(X[train], y[train],\n\u001b[0m\u001b[1;32m     52\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                          verbose=False, batch_size = batch_size,  use_multiprocessing=use_multiprocessing)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if rcnn:\n",
    "    df_results = df_results.append(cross_validate_NN(create_rcnn_var3(word_index, pre_trained=pre_trained), \n",
    "                                                     train_seq_x, train_y, valid_seq_x, valid_y,name=\"RCNN_var3_WE\",\n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_7_13\"><h3>Transformers</h3></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial available on Keras documentation, code example written by Apoorv Nandan (<a href=\"https://keras.io/examples/nlp/text_classification_with_transformer/\">source: keras.io</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super(MultiHeadSelfAttention, self).get_config().copy()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'projection_dim': self.projection_dim,\n",
    "            'query_dense': self.query_dense,\n",
    "            'key_dense': self.key_dense,\n",
    "            'value_dense': self.value_dense,\n",
    "            'combine_heads':self.combine_heads\n",
    "        })\n",
    "        return config\n",
    "        \n",
    "        \n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=tf.nn.swish), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super(TokenAndPositionEmbedding, self).get_config().copy()\n",
    "        config.update({\n",
    "            'token_emb': self.token_emb,\n",
    "            'pos_emb': self.pos_emb,\n",
    "           \n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformers_classifier(word_index, label=labels):\n",
    "    '''\n",
    "    Function to generate a rcnn for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model)  rcnn\n",
    "    '''\n",
    "    embed_dim = 32  # Embedding size for each token\n",
    "    num_heads = 2  # Number of attention heads\n",
    "    ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "    vocab_size = len(word_index)+1\n",
    "    maxlen = 300\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(20, activation=\"relu\")(x) #tf.nn.swish\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    #outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    outputs = keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transformers:\n",
    "    \n",
    "    df_results = df_results.append(cross_validate_NN(transformers_classifier(word_index, label=labels), \n",
    "                                                     train_seq_x[:25], train_y[:25], valid_seq_x, valid_y,name=\"transformers\",\n",
    "                                                     scoring=score_metrics, n_splits=CV_splits, save=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.to_csv(NAME_SAVE_FILE+\".csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_8\"><h2>Results</h2></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>test_acc_mean</th>\n",
       "      <th>test_acc_std</th>\n",
       "      <th>acc_overall</th>\n",
       "      <th>test_prec_mean</th>\n",
       "      <th>test_prec_std</th>\n",
       "      <th>prec_overall</th>\n",
       "      <th>test_recall_mean</th>\n",
       "      <th>test_recall_std</th>\n",
       "      <th>recall_overall</th>\n",
       "      <th>...</th>\n",
       "      <th>f1-score_overall</th>\n",
       "      <th>test_cohens_kappa_mean</th>\n",
       "      <th>test_cohens_kappa_std</th>\n",
       "      <th>cohens_kappa_overall</th>\n",
       "      <th>test_matthews_corrcoef_mean</th>\n",
       "      <th>test_matthews_corrcoef_std</th>\n",
       "      <th>matthews_corrcoef_overall</th>\n",
       "      <th>test_roc_auc_mean</th>\n",
       "      <th>test_roc_auc_std</th>\n",
       "      <th>roc_auc_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_CharLevel_TF-IDF</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.00909945</td>\n",
       "      <td>0.862302</td>\n",
       "      <td>0.881843</td>\n",
       "      <td>0.00921521</td>\n",
       "      <td>0.860685</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.00909945</td>\n",
       "      <td>0.862302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860069</td>\n",
       "      <td>0.810631</td>\n",
       "      <td>0.0149002</td>\n",
       "      <td>0.777349</td>\n",
       "      <td>0.811598</td>\n",
       "      <td>0.0150445</td>\n",
       "      <td>0.778761</td>\n",
       "      <td>0.967643</td>\n",
       "      <td>0.00363483</td>\n",
       "      <td>0.953255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_WordLevel_TF-IDF</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.0065909</td>\n",
       "      <td>0.857526</td>\n",
       "      <td>0.877886</td>\n",
       "      <td>0.00679435</td>\n",
       "      <td>0.856061</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.0065909</td>\n",
       "      <td>0.857526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854914</td>\n",
       "      <td>0.805469</td>\n",
       "      <td>0.0107944</td>\n",
       "      <td>0.769152</td>\n",
       "      <td>0.806202</td>\n",
       "      <td>0.0108582</td>\n",
       "      <td>0.771022</td>\n",
       "      <td>0.96488</td>\n",
       "      <td>0.00397542</td>\n",
       "      <td>0.956125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_WordLevel_TF-IDF</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.0105262</td>\n",
       "      <td>0.856477</td>\n",
       "      <td>0.864011</td>\n",
       "      <td>0.0105216</td>\n",
       "      <td>0.855243</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.0105262</td>\n",
       "      <td>0.856477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853898</td>\n",
       "      <td>0.780352</td>\n",
       "      <td>0.0175112</td>\n",
       "      <td>0.767547</td>\n",
       "      <td>0.782323</td>\n",
       "      <td>0.0172195</td>\n",
       "      <td>0.769427</td>\n",
       "      <td>0.957162</td>\n",
       "      <td>0.00486714</td>\n",
       "      <td>0.95306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_CharLevel_TF-IDF</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>0.0102098</td>\n",
       "      <td>0.854846</td>\n",
       "      <td>0.873863</td>\n",
       "      <td>0.0104262</td>\n",
       "      <td>0.853772</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>0.0102098</td>\n",
       "      <td>0.854846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852958</td>\n",
       "      <td>0.798283</td>\n",
       "      <td>0.0172035</td>\n",
       "      <td>0.764876</td>\n",
       "      <td>0.799193</td>\n",
       "      <td>0.0167372</td>\n",
       "      <td>0.76634</td>\n",
       "      <td>0.96358</td>\n",
       "      <td>0.00384267</td>\n",
       "      <td>0.954188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_Count_Vectors</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.00903327</td>\n",
       "      <td>0.854147</td>\n",
       "      <td>0.893359</td>\n",
       "      <td>0.00884756</td>\n",
       "      <td>0.85218</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.00903327</td>\n",
       "      <td>0.854147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852514</td>\n",
       "      <td>0.82949</td>\n",
       "      <td>0.0145859</td>\n",
       "      <td>0.765124</td>\n",
       "      <td>0.829702</td>\n",
       "      <td>0.0147007</td>\n",
       "      <td>0.765759</td>\n",
       "      <td>0.970659</td>\n",
       "      <td>0.00250749</td>\n",
       "      <td>0.953284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_WordLevel_TF-IDF</td>\n",
       "      <td>0.8848</td>\n",
       "      <td>0.00719444</td>\n",
       "      <td>0.853448</td>\n",
       "      <td>0.883637</td>\n",
       "      <td>0.00730045</td>\n",
       "      <td>0.851482</td>\n",
       "      <td>0.8848</td>\n",
       "      <td>0.00719444</td>\n",
       "      <td>0.853448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851265</td>\n",
       "      <td>0.814228</td>\n",
       "      <td>0.0121098</td>\n",
       "      <td>0.763248</td>\n",
       "      <td>0.814692</td>\n",
       "      <td>0.0119626</td>\n",
       "      <td>0.764437</td>\n",
       "      <td>0.970636</td>\n",
       "      <td>0.00368279</td>\n",
       "      <td>0.95269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_Count_Vectors</td>\n",
       "      <td>0.8844</td>\n",
       "      <td>0.00826075</td>\n",
       "      <td>0.85205</td>\n",
       "      <td>0.883198</td>\n",
       "      <td>0.00822523</td>\n",
       "      <td>0.853475</td>\n",
       "      <td>0.8844</td>\n",
       "      <td>0.00826075</td>\n",
       "      <td>0.85205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.847366</td>\n",
       "      <td>0.812462</td>\n",
       "      <td>0.0139428</td>\n",
       "      <td>0.757457</td>\n",
       "      <td>0.813851</td>\n",
       "      <td>0.0136913</td>\n",
       "      <td>0.763014</td>\n",
       "      <td>0.967112</td>\n",
       "      <td>0.00376917</td>\n",
       "      <td>0.954335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_CharLevel_Vectors</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.0102176</td>\n",
       "      <td>0.849604</td>\n",
       "      <td>0.859459</td>\n",
       "      <td>0.0106272</td>\n",
       "      <td>0.849511</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.0102176</td>\n",
       "      <td>0.849604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849557</td>\n",
       "      <td>0.772516</td>\n",
       "      <td>0.0157478</td>\n",
       "      <td>0.759593</td>\n",
       "      <td>0.774712</td>\n",
       "      <td>0.0166845</td>\n",
       "      <td>0.759593</td>\n",
       "      <td>0.944868</td>\n",
       "      <td>0.00666113</td>\n",
       "      <td>0.947845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_WordLevel_TF-IDF</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.0118996</td>\n",
       "      <td>0.848206</td>\n",
       "      <td>0.860856</td>\n",
       "      <td>0.0114233</td>\n",
       "      <td>0.847994</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.0118996</td>\n",
       "      <td>0.848206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848047</td>\n",
       "      <td>0.775652</td>\n",
       "      <td>0.0184513</td>\n",
       "      <td>0.756603</td>\n",
       "      <td>0.776928</td>\n",
       "      <td>0.0180545</td>\n",
       "      <td>0.756664</td>\n",
       "      <td>0.951549</td>\n",
       "      <td>0.00500477</td>\n",
       "      <td>0.947836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_Count_Vectors</td>\n",
       "      <td>0.8654</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.848555</td>\n",
       "      <td>0.864265</td>\n",
       "      <td>0.0110623</td>\n",
       "      <td>0.84796</td>\n",
       "      <td>0.8654</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.848555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.847637</td>\n",
       "      <td>0.782598</td>\n",
       "      <td>0.0175407</td>\n",
       "      <td>0.75535</td>\n",
       "      <td>0.783134</td>\n",
       "      <td>0.0174824</td>\n",
       "      <td>0.756082</td>\n",
       "      <td>0.947283</td>\n",
       "      <td>0.00547645</td>\n",
       "      <td>0.944847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTrees_WordLevel_TF-IDF</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>0.0126902</td>\n",
       "      <td>0.842731</td>\n",
       "      <td>0.861479</td>\n",
       "      <td>0.0140301</td>\n",
       "      <td>0.842815</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>0.0126902</td>\n",
       "      <td>0.842731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.837173</td>\n",
       "      <td>0.770733</td>\n",
       "      <td>0.0209644</td>\n",
       "      <td>0.743173</td>\n",
       "      <td>0.776164</td>\n",
       "      <td>0.021377</td>\n",
       "      <td>0.748232</td>\n",
       "      <td>0.950208</td>\n",
       "      <td>0.00493057</td>\n",
       "      <td>0.941467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_CharLevel_TF-IDF</td>\n",
       "      <td>0.8476</td>\n",
       "      <td>0.00915642</td>\n",
       "      <td>0.836556</td>\n",
       "      <td>0.847024</td>\n",
       "      <td>0.00926522</td>\n",
       "      <td>0.834948</td>\n",
       "      <td>0.8476</td>\n",
       "      <td>0.00915642</td>\n",
       "      <td>0.836556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833533</td>\n",
       "      <td>0.751194</td>\n",
       "      <td>0.015584</td>\n",
       "      <td>0.735467</td>\n",
       "      <td>0.754423</td>\n",
       "      <td>0.0155437</td>\n",
       "      <td>0.737432</td>\n",
       "      <td>0.943195</td>\n",
       "      <td>0.00530195</td>\n",
       "      <td>0.938909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTrees_Count_Vectors</td>\n",
       "      <td>0.8602</td>\n",
       "      <td>0.00810925</td>\n",
       "      <td>0.83411</td>\n",
       "      <td>0.862675</td>\n",
       "      <td>0.00791046</td>\n",
       "      <td>0.837423</td>\n",
       "      <td>0.8602</td>\n",
       "      <td>0.00810925</td>\n",
       "      <td>0.83411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.826643</td>\n",
       "      <td>0.769763</td>\n",
       "      <td>0.0141737</td>\n",
       "      <td>0.726363</td>\n",
       "      <td>0.776459</td>\n",
       "      <td>0.0130874</td>\n",
       "      <td>0.735142</td>\n",
       "      <td>0.952442</td>\n",
       "      <td>0.00384314</td>\n",
       "      <td>0.937039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deep_NN_WE</td>\n",
       "      <td>0.8412</td>\n",
       "      <td>0.00658483</td>\n",
       "      <td>0.835624</td>\n",
       "      <td>0.840078</td>\n",
       "      <td>0.00642038</td>\n",
       "      <td>0.834274</td>\n",
       "      <td>0.8412</td>\n",
       "      <td>0.00658483</td>\n",
       "      <td>0.835624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833864</td>\n",
       "      <td>0.741618</td>\n",
       "      <td>0.0106879</td>\n",
       "      <td>0.733869</td>\n",
       "      <td>0.74307</td>\n",
       "      <td>0.0108765</td>\n",
       "      <td>0.735069</td>\n",
       "      <td>0.866015</td>\n",
       "      <td>0.00529343</td>\n",
       "      <td>0.86167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_N-Gram_TF-IDF</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>0.00757364</td>\n",
       "      <td>0.834343</td>\n",
       "      <td>0.874902</td>\n",
       "      <td>0.0074137</td>\n",
       "      <td>0.832563</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>0.00757364</td>\n",
       "      <td>0.834343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833228</td>\n",
       "      <td>0.798658</td>\n",
       "      <td>0.0131188</td>\n",
       "      <td>0.734508</td>\n",
       "      <td>0.799077</td>\n",
       "      <td>0.0127333</td>\n",
       "      <td>0.734695</td>\n",
       "      <td>0.963773</td>\n",
       "      <td>0.00561452</td>\n",
       "      <td>0.939095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTrees_N-Gram_TF-IDF</td>\n",
       "      <td>0.8592</td>\n",
       "      <td>0.0110164</td>\n",
       "      <td>0.83411</td>\n",
       "      <td>0.85738</td>\n",
       "      <td>0.0111102</td>\n",
       "      <td>0.831544</td>\n",
       "      <td>0.8592</td>\n",
       "      <td>0.0110164</td>\n",
       "      <td>0.83411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83178</td>\n",
       "      <td>0.771568</td>\n",
       "      <td>0.0184695</td>\n",
       "      <td>0.733375</td>\n",
       "      <td>0.773215</td>\n",
       "      <td>0.018142</td>\n",
       "      <td>0.734216</td>\n",
       "      <td>0.952294</td>\n",
       "      <td>0.0065169</td>\n",
       "      <td>0.935372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_N-Gram_TF-IDF</td>\n",
       "      <td>0.8588</td>\n",
       "      <td>0.00910824</td>\n",
       "      <td>0.832712</td>\n",
       "      <td>0.858127</td>\n",
       "      <td>0.00916959</td>\n",
       "      <td>0.83085</td>\n",
       "      <td>0.8588</td>\n",
       "      <td>0.00910824</td>\n",
       "      <td>0.832712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.828692</td>\n",
       "      <td>0.769703</td>\n",
       "      <td>0.0152383</td>\n",
       "      <td>0.728141</td>\n",
       "      <td>0.77222</td>\n",
       "      <td>0.0151868</td>\n",
       "      <td>0.731017</td>\n",
       "      <td>0.954382</td>\n",
       "      <td>0.004382</td>\n",
       "      <td>0.943713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shallow_NN_WE</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.0082801</td>\n",
       "      <td>0.833062</td>\n",
       "      <td>0.836935</td>\n",
       "      <td>0.00869908</td>\n",
       "      <td>0.831685</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.0082801</td>\n",
       "      <td>0.833062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83119</td>\n",
       "      <td>0.737961</td>\n",
       "      <td>0.014799</td>\n",
       "      <td>0.729436</td>\n",
       "      <td>0.738796</td>\n",
       "      <td>0.014194</td>\n",
       "      <td>0.730773</td>\n",
       "      <td>0.866404</td>\n",
       "      <td>0.00951561</td>\n",
       "      <td>0.858388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_N-Gram_Vectors</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.00835703</td>\n",
       "      <td>0.83178</td>\n",
       "      <td>0.851241</td>\n",
       "      <td>0.00873682</td>\n",
       "      <td>0.829223</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.00835703</td>\n",
       "      <td>0.83178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.828984</td>\n",
       "      <td>0.760468</td>\n",
       "      <td>0.0134944</td>\n",
       "      <td>0.728153</td>\n",
       "      <td>0.761477</td>\n",
       "      <td>0.0135055</td>\n",
       "      <td>0.729554</td>\n",
       "      <td>0.945052</td>\n",
       "      <td>0.00465124</td>\n",
       "      <td>0.935383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTrees_CharLevel_TF-IDF</td>\n",
       "      <td>0.8446</td>\n",
       "      <td>0.0126744</td>\n",
       "      <td>0.829567</td>\n",
       "      <td>0.84661</td>\n",
       "      <td>0.0139058</td>\n",
       "      <td>0.832805</td>\n",
       "      <td>0.8446</td>\n",
       "      <td>0.0126744</td>\n",
       "      <td>0.829567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.822214</td>\n",
       "      <td>0.74338</td>\n",
       "      <td>0.0214327</td>\n",
       "      <td>0.718754</td>\n",
       "      <td>0.750313</td>\n",
       "      <td>0.0214873</td>\n",
       "      <td>0.727465</td>\n",
       "      <td>0.941362</td>\n",
       "      <td>0.00631539</td>\n",
       "      <td>0.93464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GB_N-Gram_TF-IDF</td>\n",
       "      <td>0.8532</td>\n",
       "      <td>0.0110887</td>\n",
       "      <td>0.824441</td>\n",
       "      <td>0.852692</td>\n",
       "      <td>0.0105319</td>\n",
       "      <td>0.823096</td>\n",
       "      <td>0.8532</td>\n",
       "      <td>0.0110887</td>\n",
       "      <td>0.824441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820536</td>\n",
       "      <td>0.760978</td>\n",
       "      <td>0.0192151</td>\n",
       "      <td>0.71369</td>\n",
       "      <td>0.762787</td>\n",
       "      <td>0.0184821</td>\n",
       "      <td>0.716994</td>\n",
       "      <td>0.956448</td>\n",
       "      <td>0.00573365</td>\n",
       "      <td>0.943482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RCNN_var1_WE</td>\n",
       "      <td>0.8054</td>\n",
       "      <td>0.0446256</td>\n",
       "      <td>0.823276</td>\n",
       "      <td>0.815922</td>\n",
       "      <td>0.0268298</td>\n",
       "      <td>0.826094</td>\n",
       "      <td>0.8054</td>\n",
       "      <td>0.0446256</td>\n",
       "      <td>0.823276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824119</td>\n",
       "      <td>0.678884</td>\n",
       "      <td>0.0864021</td>\n",
       "      <td>0.715948</td>\n",
       "      <td>0.688833</td>\n",
       "      <td>0.0702421</td>\n",
       "      <td>0.716362</td>\n",
       "      <td>0.834005</td>\n",
       "      <td>0.0508508</td>\n",
       "      <td>0.855137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deep_NN_var1_WE</td>\n",
       "      <td>0.8336</td>\n",
       "      <td>0.00924338</td>\n",
       "      <td>0.822577</td>\n",
       "      <td>0.834353</td>\n",
       "      <td>0.0066319</td>\n",
       "      <td>0.827082</td>\n",
       "      <td>0.8336</td>\n",
       "      <td>0.00924338</td>\n",
       "      <td>0.822577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816528</td>\n",
       "      <td>0.731965</td>\n",
       "      <td>0.0128676</td>\n",
       "      <td>0.705745</td>\n",
       "      <td>0.732594</td>\n",
       "      <td>0.0131335</td>\n",
       "      <td>0.715054</td>\n",
       "      <td>0.864453</td>\n",
       "      <td>0.00414177</td>\n",
       "      <td>0.840711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deep_NN_var2_WE</td>\n",
       "      <td>0.8372</td>\n",
       "      <td>0.0124483</td>\n",
       "      <td>0.814772</td>\n",
       "      <td>0.839546</td>\n",
       "      <td>0.0112927</td>\n",
       "      <td>0.819225</td>\n",
       "      <td>0.8372</td>\n",
       "      <td>0.0124483</td>\n",
       "      <td>0.814772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815756</td>\n",
       "      <td>0.738537</td>\n",
       "      <td>0.0204872</td>\n",
       "      <td>0.701071</td>\n",
       "      <td>0.739357</td>\n",
       "      <td>0.0201743</td>\n",
       "      <td>0.701993</td>\n",
       "      <td>0.869114</td>\n",
       "      <td>0.0124997</td>\n",
       "      <td>0.847362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN_WE</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.0121491</td>\n",
       "      <td>0.813141</td>\n",
       "      <td>0.820471</td>\n",
       "      <td>0.00513295</td>\n",
       "      <td>0.811562</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.0121491</td>\n",
       "      <td>0.813141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812124</td>\n",
       "      <td>0.686287</td>\n",
       "      <td>0.0144516</td>\n",
       "      <td>0.699728</td>\n",
       "      <td>0.693863</td>\n",
       "      <td>0.0122123</td>\n",
       "      <td>0.699961</td>\n",
       "      <td>0.841897</td>\n",
       "      <td>0.0103223</td>\n",
       "      <td>0.847224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model test_acc_mean test_acc_std acc_overall  \\\n",
       "0         XGB_CharLevel_TF-IDF         0.883   0.00909945    0.862302   \n",
       "0          GB_WordLevel_TF-IDF        0.8796    0.0065909    0.857526   \n",
       "0          LR_WordLevel_TF-IDF         0.865    0.0105262    0.856477   \n",
       "0          GB_CharLevel_TF-IDF        0.8754    0.0102098    0.854846   \n",
       "0            XGB_Count_Vectors         0.894   0.00903327    0.854147   \n",
       "0         XGB_WordLevel_TF-IDF        0.8848   0.00719444    0.853448   \n",
       "0             GB_Count_Vectors        0.8844   0.00826075     0.85205   \n",
       "0        SGD_CharLevel_Vectors          0.86    0.0102176    0.849604   \n",
       "0         SGD_WordLevel_TF-IDF         0.861    0.0118996    0.848206   \n",
       "0             LR_Count_Vectors        0.8654     0.010837    0.848555   \n",
       "0  ExtraTrees_WordLevel_TF-IDF        0.8604    0.0126902    0.842731   \n",
       "0          LR_CharLevel_TF-IDF        0.8476   0.00915642    0.836556   \n",
       "0     ExtraTrees_Count_Vectors        0.8602   0.00810925     0.83411   \n",
       "0                   Deep_NN_WE        0.8412   0.00658483    0.835624   \n",
       "0            XGB_N-Gram_TF-IDF        0.8752   0.00757364    0.834343   \n",
       "0     ExtraTrees_N-Gram_TF-IDF        0.8592    0.0110164     0.83411   \n",
       "0             LR_N-Gram_TF-IDF        0.8588   0.00910824    0.832712   \n",
       "0                Shallow_NN_WE        0.8382    0.0082801    0.833062   \n",
       "0           SGD_N-Gram_Vectors        0.8524   0.00835703     0.83178   \n",
       "0  ExtraTrees_CharLevel_TF-IDF        0.8446    0.0126744    0.829567   \n",
       "0             GB_N-Gram_TF-IDF        0.8532    0.0110887    0.824441   \n",
       "0                 RCNN_var1_WE        0.8054    0.0446256    0.823276   \n",
       "0              Deep_NN_var1_WE        0.8336   0.00924338    0.822577   \n",
       "0              Deep_NN_var2_WE        0.8372    0.0124483    0.814772   \n",
       "0                       CNN_WE         0.805    0.0121491    0.813141   \n",
       "\n",
       "  test_prec_mean test_prec_std prec_overall test_recall_mean test_recall_std  \\\n",
       "0       0.881843    0.00921521     0.860685            0.883      0.00909945   \n",
       "0       0.877886    0.00679435     0.856061           0.8796       0.0065909   \n",
       "0       0.864011     0.0105216     0.855243            0.865       0.0105262   \n",
       "0       0.873863     0.0104262     0.853772           0.8754       0.0102098   \n",
       "0       0.893359    0.00884756      0.85218            0.894      0.00903327   \n",
       "0       0.883637    0.00730045     0.851482           0.8848      0.00719444   \n",
       "0       0.883198    0.00822523     0.853475           0.8844      0.00826075   \n",
       "0       0.859459     0.0106272     0.849511             0.86       0.0102176   \n",
       "0       0.860856     0.0114233     0.847994            0.861       0.0118996   \n",
       "0       0.864265     0.0110623      0.84796           0.8654        0.010837   \n",
       "0       0.861479     0.0140301     0.842815           0.8604       0.0126902   \n",
       "0       0.847024    0.00926522     0.834948           0.8476      0.00915642   \n",
       "0       0.862675    0.00791046     0.837423           0.8602      0.00810925   \n",
       "0       0.840078    0.00642038     0.834274           0.8412      0.00658483   \n",
       "0       0.874902     0.0074137     0.832563           0.8752      0.00757364   \n",
       "0        0.85738     0.0111102     0.831544           0.8592       0.0110164   \n",
       "0       0.858127    0.00916959      0.83085           0.8588      0.00910824   \n",
       "0       0.836935    0.00869908     0.831685           0.8382       0.0082801   \n",
       "0       0.851241    0.00873682     0.829223           0.8524      0.00835703   \n",
       "0        0.84661     0.0139058     0.832805           0.8446       0.0126744   \n",
       "0       0.852692     0.0105319     0.823096           0.8532       0.0110887   \n",
       "0       0.815922     0.0268298     0.826094           0.8054       0.0446256   \n",
       "0       0.834353     0.0066319     0.827082           0.8336      0.00924338   \n",
       "0       0.839546     0.0112927     0.819225           0.8372       0.0124483   \n",
       "0       0.820471    0.00513295     0.811562            0.805       0.0121491   \n",
       "\n",
       "  recall_overall  ... f1-score_overall test_cohens_kappa_mean  \\\n",
       "0       0.862302  ...         0.860069               0.810631   \n",
       "0       0.857526  ...         0.854914               0.805469   \n",
       "0       0.856477  ...         0.853898               0.780352   \n",
       "0       0.854846  ...         0.852958               0.798283   \n",
       "0       0.854147  ...         0.852514                0.82949   \n",
       "0       0.853448  ...         0.851265               0.814228   \n",
       "0        0.85205  ...         0.847366               0.812462   \n",
       "0       0.849604  ...         0.849557               0.772516   \n",
       "0       0.848206  ...         0.848047               0.775652   \n",
       "0       0.848555  ...         0.847637               0.782598   \n",
       "0       0.842731  ...         0.837173               0.770733   \n",
       "0       0.836556  ...         0.833533               0.751194   \n",
       "0        0.83411  ...         0.826643               0.769763   \n",
       "0       0.835624  ...         0.833864               0.741618   \n",
       "0       0.834343  ...         0.833228               0.798658   \n",
       "0        0.83411  ...          0.83178               0.771568   \n",
       "0       0.832712  ...         0.828692               0.769703   \n",
       "0       0.833062  ...          0.83119               0.737961   \n",
       "0        0.83178  ...         0.828984               0.760468   \n",
       "0       0.829567  ...         0.822214                0.74338   \n",
       "0       0.824441  ...         0.820536               0.760978   \n",
       "0       0.823276  ...         0.824119               0.678884   \n",
       "0       0.822577  ...         0.816528               0.731965   \n",
       "0       0.814772  ...         0.815756               0.738537   \n",
       "0       0.813141  ...         0.812124               0.686287   \n",
       "\n",
       "  test_cohens_kappa_std cohens_kappa_overall test_matthews_corrcoef_mean  \\\n",
       "0             0.0149002             0.777349                    0.811598   \n",
       "0             0.0107944             0.769152                    0.806202   \n",
       "0             0.0175112             0.767547                    0.782323   \n",
       "0             0.0172035             0.764876                    0.799193   \n",
       "0             0.0145859             0.765124                    0.829702   \n",
       "0             0.0121098             0.763248                    0.814692   \n",
       "0             0.0139428             0.757457                    0.813851   \n",
       "0             0.0157478             0.759593                    0.774712   \n",
       "0             0.0184513             0.756603                    0.776928   \n",
       "0             0.0175407              0.75535                    0.783134   \n",
       "0             0.0209644             0.743173                    0.776164   \n",
       "0              0.015584             0.735467                    0.754423   \n",
       "0             0.0141737             0.726363                    0.776459   \n",
       "0             0.0106879             0.733869                     0.74307   \n",
       "0             0.0131188             0.734508                    0.799077   \n",
       "0             0.0184695             0.733375                    0.773215   \n",
       "0             0.0152383             0.728141                     0.77222   \n",
       "0              0.014799             0.729436                    0.738796   \n",
       "0             0.0134944             0.728153                    0.761477   \n",
       "0             0.0214327             0.718754                    0.750313   \n",
       "0             0.0192151              0.71369                    0.762787   \n",
       "0             0.0864021             0.715948                    0.688833   \n",
       "0             0.0128676             0.705745                    0.732594   \n",
       "0             0.0204872             0.701071                    0.739357   \n",
       "0             0.0144516             0.699728                    0.693863   \n",
       "\n",
       "  test_matthews_corrcoef_std matthews_corrcoef_overall test_roc_auc_mean  \\\n",
       "0                  0.0150445                  0.778761          0.967643   \n",
       "0                  0.0108582                  0.771022           0.96488   \n",
       "0                  0.0172195                  0.769427          0.957162   \n",
       "0                  0.0167372                   0.76634           0.96358   \n",
       "0                  0.0147007                  0.765759          0.970659   \n",
       "0                  0.0119626                  0.764437          0.970636   \n",
       "0                  0.0136913                  0.763014          0.967112   \n",
       "0                  0.0166845                  0.759593          0.944868   \n",
       "0                  0.0180545                  0.756664          0.951549   \n",
       "0                  0.0174824                  0.756082          0.947283   \n",
       "0                   0.021377                  0.748232          0.950208   \n",
       "0                  0.0155437                  0.737432          0.943195   \n",
       "0                  0.0130874                  0.735142          0.952442   \n",
       "0                  0.0108765                  0.735069          0.866015   \n",
       "0                  0.0127333                  0.734695          0.963773   \n",
       "0                   0.018142                  0.734216          0.952294   \n",
       "0                  0.0151868                  0.731017          0.954382   \n",
       "0                   0.014194                  0.730773          0.866404   \n",
       "0                  0.0135055                  0.729554          0.945052   \n",
       "0                  0.0214873                  0.727465          0.941362   \n",
       "0                  0.0184821                  0.716994          0.956448   \n",
       "0                  0.0702421                  0.716362          0.834005   \n",
       "0                  0.0131335                  0.715054          0.864453   \n",
       "0                  0.0201743                  0.701993          0.869114   \n",
       "0                  0.0122123                  0.699961          0.841897   \n",
       "\n",
       "  test_roc_auc_std roc_auc_overall  \n",
       "0       0.00363483        0.953255  \n",
       "0       0.00397542        0.956125  \n",
       "0       0.00486714         0.95306  \n",
       "0       0.00384267        0.954188  \n",
       "0       0.00250749        0.953284  \n",
       "0       0.00368279         0.95269  \n",
       "0       0.00376917        0.954335  \n",
       "0       0.00666113        0.947845  \n",
       "0       0.00500477        0.947836  \n",
       "0       0.00547645        0.944847  \n",
       "0       0.00493057        0.941467  \n",
       "0       0.00530195        0.938909  \n",
       "0       0.00384314        0.937039  \n",
       "0       0.00529343         0.86167  \n",
       "0       0.00561452        0.939095  \n",
       "0        0.0065169        0.935372  \n",
       "0         0.004382        0.943713  \n",
       "0       0.00951561        0.858388  \n",
       "0       0.00465124        0.935383  \n",
       "0       0.00631539         0.93464  \n",
       "0       0.00573365        0.943482  \n",
       "0        0.0508508        0.855137  \n",
       "0       0.00414177        0.840711  \n",
       "0        0.0124997        0.847362  \n",
       "0        0.0103223        0.847224  \n",
       "\n",
       "[25 rows x 22 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[[ \"Model\",\"test_acc_mean\",\"test_acc_std\", \"acc_overall\",\n",
    "                       \"test_prec_mean\", \"test_prec_std\", \"prec_overall\",\n",
    "                        \"test_recall_mean\",\"test_recall_std\", \"recall_overall\",\n",
    "                       \"test_f1-score_mean\", \"test_f1-score_std\", \"f1-score_overall\",\n",
    "                       \"test_cohens_kappa_mean\", \"test_cohens_kappa_std\", \"cohens_kappa_overall\",\n",
    "                       \"test_matthews_corrcoef_mean\",\"test_matthews_corrcoef_std\", \"matthews_corrcoef_overall\",\n",
    "                       \"test_roc_auc_mean\", \"test_roc_auc_std\",\"roc_auc_overall\"]].sort_values(\n",
    "    by=[\"matthews_corrcoef_overall\", \"recall_overall\"], ascending=False)[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_9\"><h2>Visualization</h2></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-575ed8c539c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"matthews_corrcoef_overall\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.85\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_' is not defined"
     ]
    }
   ],
   "source": [
    "df_=df_[df_results[\"matthews_corrcoef_overall\"]>0.85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25,25))\n",
    "#plt.axis([0.85,1,0.85,1])\n",
    "ax.scatter(df_[\"test_matthews_corrcoef_mean\"], df_[\"test_acc_mean\"])\n",
    "\n",
    "for i, txt in enumerate(df_[\"Model\"]):\n",
    "    ax.annotate(txt, (df_[\"test_matthews_corrcoef_mean\"].iloc[i], df_[\"test_acc_mean\"].iloc[i]))\n",
    "\n",
    "plt.xlabel(\"Matthews Corrcoef\")\n",
    "plt.ylabel(\"Recall\")\n",
    "\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
