[{'Text': ['We provide **RecAdam** (Recall Adam) optimizer to facilitate fine-tuning deep pretrained language models (e.g.  BERT  ALBERT) with less forgetting.  For a detailed description and experimental results  please refer to our paper: [Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting](https://www.aclweb.org/anthology/2020.emnlp-main.634/) (Accepted by EMNLP 2020).     --output_dir /path/to/output/$TASK_NAME/ \\     --output_dir /path/to/output/$TASK_NAME/ \\   ', 'Getting the source code   $ wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip   $ cd fastText-0.9.2  $ make   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ mkdir build &amp;&amp; cd build &amp;&amp; cmake ..  $ make &amp;&amp; make install   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ pip install .   You can also quantize a supervised model to reduce its memory usage with the following command:   You can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.  There is also the master branch that contains all of our most recent work  but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.   This library has two main use cases: word representation learning and text classification. These were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).   ', "1. Clone this repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/vincentzhang/roi-fcn.git    ```   If you didn't clone with the `--recursive` flag  then you'll need to manually clone the `caffe-roi` submodule:   ```Shell     git submodule update --init --recursive   ```  2. Build Caffe and pycaffe   **Note:** Caffe *must* be built with support for Python layers!     ```Shell     #: ROOT refers to the directory that you cloned this repo into.     cd $ROOT/caffe-roi     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html          #: In your Makefile.config  make sure to have this line uncommented             WITH_PYTHON_LAYER := 1         #: Unrelatedly  it's also recommended that you use CUDNN             USE_CUDNN := 1      #: Compile     make -j8 && make pycaffe     ```   You can download my [Makefile.config](https://drive.google.com/open?id=1NSeWp7INxGWUrSdCTCwol8NmV_0-Ar5k) for reference.  3. Build the Cython modules     ```Shell     cd $ROOT/lib     make     ```  4. Download the ImageNet pre-trained VGG16 weights (adapted to be fully convolutional):     ```Shell     cd $ROOT/data/scripts     ./fetch_vgg16_fcn.sh     ```      This will populate the `$ROOT/data/imagenet_models` folder with `VGG16.v2.fcn-surgery-all.caffemodel`.   ./experiments/scripts/test_socket_scratch_n_1e-4_fg150_roils.sh test all 4586 1 16 1   To run the demo  first download the pretrained weights: ```Shell cd $ROOT/data/scripts ./fetch_socket_models.sh ``` Run the demo script: ```Shell cd $ROOT python ./tools/demo.py ``` The demo runs the segmentation network trained on the acetabulum data used in the paper.  To show the generalization of the algorithm  the input images stored in `$ROOT/data/samples` are anonymized clinical images that are not in the training or testing dataset.   ", "1. Clone this repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/vincentzhang/roi-fcn.git    ```   If you didn't clone with the `--recursive` flag  then you'll need to manually clone the `caffe-roi` submodule:   ```Shell     git submodule update --init --recursive   ```  2. Build Caffe and pycaffe   **Note:** Caffe *must* be built with support for Python layers!     ```Shell     #: ROOT refers to the directory that you cloned this repo into.     cd $ROOT/caffe-roi     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html          #: In your Makefile.config  make sure to have this line uncommented             WITH_PYTHON_LAYER := 1         #: Unrelatedly  it's also recommended that you use CUDNN             USE_CUDNN := 1      #: Compile     make -j8 && make pycaffe     ```   You can download my [Makefile.config](https://drive.google.com/open?id=1NSeWp7INxGWUrSdCTCwol8NmV_0-Ar5k) for reference.  3. Build the Cython modules     ```Shell     cd $ROOT/lib     make     ```  4. Download the ImageNet pre-trained VGG16 weights (adapted to be fully convolutional):     ```Shell     cd $ROOT/data/scripts     ./fetch_vgg16_fcn.sh     ```      This will populate the `$ROOT/data/imagenet_models` folder with `VGG16.v2.fcn-surgery-all.caffemodel`.   ./experiments/scripts/test_socket_scratch_n_1e-4_fg150_roils.sh test all 4586 1 16 1   To run the demo  first download the pretrained weights: ```Shell cd $ROOT/data/scripts ./fetch_socket_models.sh ``` Run the demo script: ```Shell cd $ROOT python ./tools/demo.py ``` The demo runs the segmentation network trained on the acetabulum data used in the paper.  To show the generalization of the algorithm  the input images stored in `$ROOT/data/samples` are anonymized clinical images that are not in the training or testing dataset.   ", 'In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   To build the tf-record and pre-train the model  download the OpenWebText corpus (12G) and setup your data directory in build_data.sh and pretrain.sh. Then run  bash build_data.sh   bash pretrain.sh   bash finetune.sh   ', "The goal of Detectron is to provide a high-quality  high-performance codebase for object detection *research*. It is designed to be flexible in order to support rapid implementation and evaluation of novel research. Detectron includes implementations of the following object detection algorithms:  - [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017* - [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017* - [Faster R-CNN](https://arxiv.org/abs/1506.01497) - [RPN](https://arxiv.org/abs/1506.01497) - [Fast R-CNN](https://arxiv.org/abs/1504.08083) - [R-FCN](https://arxiv.org/abs/1605.06409)  using the following backbone network architectures:  - [ResNeXt{50 101 152}](https://arxiv.org/abs/1611.05431) - [ResNet{50 101 152}](https://arxiv.org/abs/1512.03385) - [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt) - [VGG16](https://arxiv.org/abs/1409.1556)  Additional backbone architectures may be easily implemented. For more details about these models  please see [References](#references) below.   Please find installation instructions for Caffe2 and Detectron in [`INSTALL.md`](INSTALL.md).   After installation  please see [`GETTING_STARTED.md`](GETTING_STARTED.md) for brief tutorials covering inference and training with Detectron.   To start  please check the [troubleshooting](INSTALL.md#troubleshooting) section of our installation instructions as well as our [FAQ](FAQ.md). If you couldn't find help there  try searching our GitHub issues. We intend the issues page to be a forum in which the community collectively troubleshoots problems.  If bugs are found  **we appreciate pull requests** (including adding Q&A's to `FAQ.md` and improving our installation instructions and troubleshooting documents). Please see [CONTRIBUTING.md](CONTRIBUTING.md) for more information about contributing to Detectron.   ", "The goal of Detectron is to provide a high-quality  high-performance codebase for object detection *research*. It is designed to be flexible in order to support rapid implementation and evaluation of novel research. Detectron includes implementations of the following object detection algorithms:  - [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017* - [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017* - [Faster R-CNN](https://arxiv.org/abs/1506.01497) - [RPN](https://arxiv.org/abs/1506.01497) - [Fast R-CNN](https://arxiv.org/abs/1504.08083) - [R-FCN](https://arxiv.org/abs/1605.06409)  using the following backbone network architectures:  - [ResNeXt{50 101 152}](https://arxiv.org/abs/1611.05431) - [ResNet{50 101 152}](https://arxiv.org/abs/1512.03385) - [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt) - [VGG16](https://arxiv.org/abs/1409.1556)  Additional backbone architectures may be easily implemented. For more details about these models  please see [References](#references) below.   Please find installation instructions for Caffe2 and Detectron in [`INSTALL.md`](INSTALL.md).   After installation  please see [`GETTING_STARTED.md`](GETTING_STARTED.md) for brief tutorials covering inference and training with Detectron.   To start  please check the [troubleshooting](INSTALL.md#troubleshooting) section of our installation instructions as well as our [FAQ](FAQ.md). If you couldn't find help there  try searching our GitHub issues. We intend the issues page to be a forum in which the community collectively troubleshoots problems.  If bugs are found  **we appreciate pull requests** (including adding Q&A's to `FAQ.md` and improving our installation instructions and troubleshooting documents). Please see [CONTRIBUTING.md](CONTRIBUTING.md) for more information about contributing to Detectron.   ", "1. Clone this repository 2. Install dependencies    ```bash    pip3 install -r requirements.txt    ``` 3. Run setup from the repository root directory     ```bash     python3 setup.py install     ```  3. Download pre-trained COCO weights (mask_rcnn_coco.h5) from the [releases page](https://github.com/matterport/Mask_RCNN/releases). 4. (Optional) To train or test on MS COCO install `pycocotools` from one of these repos. They are forks of the original pycocotools with fixes for Python3 and Windows (the official repo doesn't seem to be active anymore).      * Linux: https://github.com/waleedka/coco     * Windows: https://github.com/philferriere/cocoapi.     You must have the Visual C++ 2015 build tools on your path (see the repo for additional details)   We're providing pre-trained weights for MS COCO to make it easier to start. You can   python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=coco   python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5   python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=last   You can also run the COCO evaluation code with:   * [demo.ipynb](samples/demo.ipynb) Is the easiest way to start. It shows an example of using a model pre-trained on MS COCO to segment objects in your own images. It includes code to run object detection and instance segmentation on arbitrary images.  * [train_shapes.ipynb](samples/shapes/train_shapes.ipynb) shows how to train Mask R-CNN on your own dataset. This notebook introduces a toy dataset (Shapes) to demonstrate training on a new dataset.  * ([model.py](mrcnn/model.py)  [utils.py](mrcnn/utils.py)  [config.py](mrcnn/config.py)): These files contain the main Mask RCNN implementation.    * [inspect_data.ipynb](samples/coco/inspect_data.ipynb). This notebook visualizes the different pre-processing steps to prepare the training data.  * [inspect_model.ipynb](samples/coco/inspect_model.ipynb) This notebook goes in depth into the steps performed to detect and segment objects. It provides visualizations of every step of the pipeline.  * [inspect_weights.ipynb](samples/coco/inspect_weights.ipynb) This notebooks inspects the weights of a trained model and looks for anomalies and odd patterns.    ![Balloon Color Splash](assets/balloon_color_splash.gif)    ![Mapping Challenge](assets/mapping_challenge.png)   "], 'Label': ['Natural Language Processing', 'Natural Language Processing', 'Computer Vision', 'Computer Vision', 'Natural Language Processing', 'Computer Vision', 'Computer Vision', 'Computer Vision']}]