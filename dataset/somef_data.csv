Text;Label;Repo
"""* Download dataset for training. This can be any wav files with sample rate 24000Hz. * Edit configuration in utils/audio.py (hop_length must remain unchanged) * Process data: python process.py --wav_dir=""wavs"" --output=""data""   """;General;https://github.com/yanggeng1995/GAN-TTS
"""* Download dataset for training. This can be any wav files with sample rate 24000Hz. * Edit configuration in utils/audio.py (hop_length must remain unchanged) * Process data: python process.py --wav_dir=""wavs"" --output=""data""   """;Sequential;https://github.com/yanggeng1995/GAN-TTS
"""We provide **RecAdam** (Recall Adam) optimizer to facilitate fine-tuning deep pretrained language models (e.g.  BERT  ALBERT) with less forgetting.  For a detailed description and experimental results  please refer to our paper: [Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting](https://www.aclweb.org/anthology/2020.emnlp-main.634/) (Accepted by EMNLP 2020).     --output_dir /path/to/output/$TASK_NAME/ \     --output_dir /path/to/output/$TASK_NAME/ \   """;Natural Language Processing;https://github.com/Sanyuan-Chen/RecAdam
"""Getting the source code   $ wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip   $ cd fastText-0.9.2  $ make   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ mkdir build &amp;&amp; cd build &amp;&amp; cmake ..  $ make &amp;&amp; make install   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ pip install .   You can also quantize a supervised model to reduce its memory usage with the following command:   You can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.  There is also the master branch that contains all of our most recent work  but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.   This library has two main use cases: word representation learning and text classification. These were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).   """;Natural Language Processing;https://github.com/facebookresearch/fastText
"""In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   To build the tf-record and pre-train the model  download the OpenWebText corpus (12G) and setup your data directory in build_data.sh and pretrain.sh. Then run  bash build_data.sh   bash pretrain.sh   bash finetune.sh   """;General;https://github.com/yyht/Conv_Bert
"""In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   To build the tf-record and pre-train the model  download the OpenWebText corpus (12G) and setup your data directory in build_data.sh and pretrain.sh. Then run  bash build_data.sh   bash pretrain.sh   bash finetune.sh   """;Natural Language Processing;https://github.com/yyht/Conv_Bert
"""In this repo  we introduce a new architecture **ConvBERT** for pre-training based language model. The code is tested on a V100 GPU. For detailed description and experimental results  please refer to our NeurIPS 2020 paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496).   To build the tf-record and pre-train the model  download the OpenWebText corpus (12G) and setup your data directory in build_data.sh and pretrain.sh. Then run  bash build_data.sh   bash pretrain.sh   bash finetune.sh   """;Sequential;https://github.com/yyht/Conv_Bert
"""TensorBoard.dev are also provided for models to the   """;General;https://github.com/tensorflow/models
"""1. Clone this repository 2. Install dependencies    ```bash    pip3 install -r requirements.txt    ``` 3. Run setup from the repository root directory     ```bash     python3 setup.py install     ```  3. Download pre-trained COCO weights (mask_rcnn_coco.h5) from the [releases page](https://github.com/matterport/Mask_RCNN/releases). 4. (Optional) To train or test on MS COCO install `pycocotools` from one of these repos. They are forks of the original pycocotools with fixes for Python3 and Windows (the official repo doesn't seem to be active anymore).      * Linux: https://github.com/waleedka/coco     * Windows: https://github.com/philferriere/cocoapi.     You must have the Visual C++ 2015 build tools on your path (see the repo for additional details)   We're providing pre-trained weights for MS COCO to make it easier to start. You can   python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=coco   python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5   python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=last   You can also run the COCO evaluation code with:   * [demo.ipynb](samples/demo.ipynb) Is the easiest way to start. It shows an example of using a model pre-trained on MS COCO to segment objects in your own images. It includes code to run object detection and instance segmentation on arbitrary images.  * [train_shapes.ipynb](samples/shapes/train_shapes.ipynb) shows how to train Mask R-CNN on your own dataset. This notebook introduces a toy dataset (Shapes) to demonstrate training on a new dataset.  * ([model.py](mrcnn/model.py)  [utils.py](mrcnn/utils.py)  [config.py](mrcnn/config.py)): These files contain the main Mask RCNN implementation.    * [inspect_data.ipynb](samples/coco/inspect_data.ipynb). This notebook visualizes the different pre-processing steps to prepare the training data.  * [inspect_model.ipynb](samples/coco/inspect_model.ipynb) This notebook goes in depth into the steps performed to detect and segment objects. It provides visualizations of every step of the pipeline.  * [inspect_weights.ipynb](samples/coco/inspect_weights.ipynb) This notebooks inspects the weights of a trained model and looks for anomalies and odd patterns.    ![Balloon Color Splash](assets/balloon_color_splash.gif)    ![Mapping Challenge](assets/mapping_challenge.png)   """;Computer Vision;https://github.com/matterport/Mask_RCNN
"""""";Computer Vision;https://github.com/enalisnick/stick-breaking_dgms
"""""";General;https://github.com/enalisnick/stick-breaking_dgms
"""We make use of some pretrained models  that can be downloaded [here](https://drive.google.com/file/d/1TA-UWYVDkCkNPOy1INjUU9321s-HA6RF/view?usp=sharing). They are a subset of the [models](https://drive.google.com/file/d/1aXTmN2AyNLdZ8zOeyLzpVbRHZRZD0fW0/view?usp=sharing) provided with the code of the original paper. They need to be unzipped and put in the `./pretrained` folder  in the root directory of the repo.  The dataset ([CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html)) is automatically downloaded via `torchvision.datasets` when first running the experiment  and will be saved in the `data/` folder (more info [here](https://pytorch.org/docs/stable/torchvision/datasets.html#cifar)).  The paper is implemented and tested using Python 3.7. Dependencies are listed in [requirements.txt](requirements.txt).  For the moment  it is possible to run the experiment using [VGG nets](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) and [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) as reference models and [GDAS](https://arxiv.org/pdf/1910.04465.pdf)  [WRN](https://arxiv.org/pdf/1605.07146.pdf) and [PyramidNet](https://arxiv.org/pdf/1610.02915.pdf) as victim models.  In order to test our implemenation  install the dependencies with `pip3 install --user --requirement requirements.txt`  and run the following command:  ```bash python run.py ```  This will run the experiment on line 5 of table II of our report  with the following settings:  - Reference models: AlexNet+VGGs - Victim model: GDAS - Number of images: 1000 - Maximum queries per image: 10000 - 0 seed    And hyperparameters:  - eta_g = 0.1 - eta = 1/255 - delta = 0.1 - tau = 1.0 - epsilon = 8/255  N.B.: it takes 7 hours 45 minutes to run on a Google Cloud Platform n1-highmem-8 virtual machine  with 8 vCPU  52 GB memory and an Nvidia Tesla T4.  Moreover  the following settings can be used to customize the experiment:  ```bash usage: run.py [-h] [-ds {Dataset.CIFAR_10}]                      [--reference-models {vgg11_bn vgg13_bn vgg16_bn vgg19_bn AlexNet_bn} [{vgg11_bn vgg13_bn vgg16_bn vgg19_bn AlexNet_bn} ...]]                      [--victim-model {gdas wrn pyramidnet}]                      [--loss {ExperimentLoss.CROSS_ENTROPY ExperimentLoss.NEG_LL}]                      [--tau TAU] [--epsilon EPSILON] [--delta DELTA]                      [--eta ETA] [--eta_g ETA_G] [--n-images N_IMAGES]                      [--image-limit IMAGE_LIMIT]                      [--compare-gradients COMPARE_GRADIENTS]                      [--check-success CHECK_SUCCESS]                      [--show-images SHOW_IMAGES] [--seed SEED]  optional arguments:   -h  --help            show this help message and exit   -ds {Dataset.CIFAR_10}  --dataset {Dataset.CIFAR_10}                         The dataset to be used.   --reference-models {vgg11_bn vgg13_bn vgg16_bn vgg19_bn AlexNet_bn} [{vgg11_bn vgg13_bn vgg16_bn vgg19_bn AlexNet_bn} ...]                         The reference models to be used.   --victim-model {gdas wrn pyramidnet}                         The model to be attacked.   --loss {ExperimentLoss.CROSS_ENTROPY ExperimentLoss.NEG_LL}                         The loss function to be used   --tau TAU             Bandit exploration.   --epsilon EPSILON     The norm budget.   --delta DELTA         Finite difference probe.   --eta ETA             Image learning rate.   --eta_g ETA_G         OCO learning rate.   --n-images N_IMAGES   The number of images on which the attack has to be run   --image-limit IMAGE_LIMIT                         Limit of iterations to be done for each image   --compare-gradients COMPARE_GRADIENTS                         Whether the program should output a comparison between                         the estimated and the true gradients.   --check-success CHECK_SUCCESS                         Whether the attack on each image should stop if it has                         been successful.   --show-images SHOW_IMAGES                         Whether each image to be attacked  and its                         corresponding adversarial examples should be shown   --seed SEED           The random seed with which the experiment should be                         run  to be used for reproducibility purposes. ```  In order to run an experiment on 100 images in which the loss of the true model and the cosine similarity between the estimated and true gradient  for all 5000 iterations per image  regardless of the success of the attack (i.e. the one used for figures 1 and 2 of our report)  you should run  ```bash python3 run.py --check-success=False --n-images=100 --compare-gradients=True ```  N.B.: it takes around 20 hours to run the experiment on the aforementioned machine.  The experiment results are saved in the `outputs/` folder  in a file named `YYYY-MM-DD.HH-MM.npy` a dictionary exported with `numpy.save()`. The format of the dictionary is:  ```python experiment_info = {     'experiment_baseline': {         'victim_model': victim_model_name          'reference_model_names': reference_model_names          'dataset': dataset     }      'hyperparameters': {         'tau': tau          'epsilon': epsilon          'delta': delta          'eta': eta          'eta_g': eta_g     }      'settings': {         'n_images': n_images          'image_limit': image_limit          'compare_gradients': compare_gradients          'gpu': #: If the GPU has been used for the experiment          'seed': seed     }      'results': {         'queries': #: The number of queries run         'total_time' #: The time it took to run the experiment         #: The following are present only if compare_gradients == True         'gradient_products': #: The cosine similarities for each image         'true_gradient_norms': #: The norms of the true gradients for each image         'estimated_gradient_norms': #: The norms of the estimated gradients for each image         'true_losses': #: The true losses each iteration         'common_signs': #: The percentages of common signs between true and est gradients         'subs_common_signs': #: The percentages of common signs between subsequent gradients } ```  The file can be imported in Python using `np.load(output_path  allow_pickle=True).item()`.   """;Computer Vision;https://github.com/epfl-ml-reproducers/subspace-attack-reproduction
"""* Download dataset for training. This can be any wav files with sample rate 24000Hz. * Edit configuration in utils/audio.py (hop_length must remain unchanged) * Process data: python process.py --wav_dir=""wavs"" --output=""data""   """;Audio;https://github.com/yanggeng1995/GAN-TTS
"""```bash git clone https://github.com/shirans/cv_course_project.git ./install.sh ```  upload files to the instance: where cs231n-for-gpu is the name of the instance gcloud compute scp --recurse data/drive cs231n-for-gpu://home/shiran.s/cv_course_project/data pip3 install -r requirements.txt    #how to set up a GC machine: https://nirbenz.github.io/gce-tutorial/?fbclid=IwAR3SvNXBPxayIuM9T94SIa-qteWKUxkbQzf7TEg4CAu6R2AWXxbGwTQT3Dw  """;Computer Vision;https://github.com/shirans/cv_course_project
"""Source    """;Computer Vision;https://github.com/syahdeini/gan
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;Computer Vision;https://github.com/trongnghia00/darknet
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;General;https://github.com/trongnghia00/darknet
"""""";Computer Vision;https://github.com/sherdencooper/tricks-in-deeplearning
"""Please follow the instructions of py-faster-rcnn [here](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to setup VOC and COCO datasets (Part of COCO is done). The steps involve downloading data and optionally creating soft links in the ``data`` folder. Since faster RCNN does not rely on pre-computed proposals  it is safe to ignore the steps that setup proposals.   1. Clone the repository   ```Shell   git clone https://github.com/bareblackfoot/lddp-tf-faster-rcnn.git   ```  3. Build the Cython modules   ```Shell   make clean   make   cd ..   ```  4. Install the [Python COCO API](https://github.com/pdollar/coco). The code requires the API to access COCO dataset.   ```Shell   cd data   git clone https://github.com/pdollar/coco.git   cd coco/PythonAPI   make   cd ../../..   ```   | Method | mAP | mAP on Crowd | |:-:|:-:|:-:| | Faster R-CNN | 75.8% | 62.0% | | Faster R-CNN + LDPP | <b>76.6%</b> | <b>64.5%</b> |    | Method | mAP | mAP on Crowd | |:-:|:-:|:-:| | Faster R-CNN | <b>71.4%</b> | 57.7% | | Faster R-CNN + LDPP | 70.9% | <b>61.8%</b> |     mkdir -p output/${NET}/${TRAIN_IMDB}    cd output/${NET}/${TRAIN_IMDB}     cd ../../..      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz     tar -xzvf vgg_16_2016_08_28.tar.gz      cd ../..     For Resnet101  you can set up like:      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz     tar -xzvf resnet_v1_101_2016_08_28.tar.gz      cd ../..     ./experiments/scripts/train_lddp.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/train_lddp.sh 1 coco res101     ./experiments/scripts/test_lddp.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/test_lddp.sh 1 coco res101   """;Computer Vision;https://github.com/bareblackfoot/lddp-tf-faster-rcnn
"""- eval_input_reader ÈáåÈù¢ÁöÑshuffleÔºå Ëøô‰∏™ÊòØË∑üevalÊ≠•È™§ÁöÑÊï∞ÊçÆreaderÊúâÂÖ≥ÔºåÂ¶ÇÊûú‰∏ç‰ΩøÁî®GPUËøõË°åËÆ≠ÁªÉÁöÑËØùÔºåËøôÈáåÈúÄË¶Å‰ªéfalseÊîπÊàêtrueÔºå‰∏çÁÑ∂‰ºöÂØºËá¥ÈîôËØØÔºåËØ¶ÁªÜÂÜÖÂÆπÂèÇÈòÖ https://github.com/tensorflow/models/issues/1936   windows ÂëΩ‰ª§Ë°å‰∏çÊîØÊåÅ * Êìç‰ΩúÁ¨¶„ÄÇ   """;General;https://github.com/Tsejing/object_detection
"""- eval_input_reader ÈáåÈù¢ÁöÑshuffleÔºå Ëøô‰∏™ÊòØË∑üevalÊ≠•È™§ÁöÑÊï∞ÊçÆreaderÊúâÂÖ≥ÔºåÂ¶ÇÊûú‰∏ç‰ΩøÁî®GPUËøõË°åËÆ≠ÁªÉÁöÑËØùÔºåËøôÈáåÈúÄË¶Å‰ªéfalseÊîπÊàêtrueÔºå‰∏çÁÑ∂‰ºöÂØºËá¥ÈîôËØØÔºåËØ¶ÁªÜÂÜÖÂÆπÂèÇÈòÖ https://github.com/tensorflow/models/issues/1936   windows ÂëΩ‰ª§Ë°å‰∏çÊîØÊåÅ * Êìç‰ΩúÁ¨¶„ÄÇ   """;Computer Vision;https://github.com/Tsejing/object_detection
"""Following project was a part of my master thesis. Current version is a little bit modified and  much improved. The project is using modified algorithm Deterministic Policy Gradient  (Lillicrap et al.[arXiv:1509.02971](https://arxiv.org/pdf/1509.02971.pdf))  (written in Tensorflow) to control mobile robot. The main idea was to learn mobile robot navigate  to goal and also avoid obstacles. For obstacles avoidance  robot is using 5 ultrasonic sensors.   For navigation task  required information (like absolute pose) are taken from simulation   engine. That's a little hack. However  more realistic (like odometry) pose estimation can be    found in [gym-vrep](https://github.com/Souphis/gym-vrep).   All Python dependencies are in requirements.txt. Just run command:  pip install -r requirements.txt   """;Reinforcement Learning;https://github.com/Souphis/mobile_robot_rl
"""conda create -n py27 python=2.7  source activate py27  conda install pytorch=0.1.12 -c soumith  conda install torchvision   """;Computer Vision;https://github.com/mr-bulb/DL_basic_github
"""- [UCI](http://persoal.citius.usc.es/manuel.fernandez.delgado/papers/jmlr/data.tar.gz) - [Tox21](http://bioinf.jku.at/research/DeepTox/tox21.zip) - [HTRU2](https://archive.ics.uci.edu/ml/machine-learning-databases/00372/HTRU2.zip)  Multilayer Perceptron on MNIST (notebook)   Multilayer Perceptron on MNIST (python script)   Multilayer Perceptron on MNIST (notebook)   """;General;https://github.com/bioinf-jku/SNNs
"""""";Computer Vision;https://github.com/johnnyjana730/keras_dcgan_car
"""Setup Dependencies   Follow the installation guide on [Tensorflow Homepage][4] for installing Tensorflow-GPU or Tensorflow-CPU.   Follow instructions outlined on [Keras Homepage][5] for installing Keras.  Run a vanilla experiment using the following command at the directory root folder.   bash    bash    Use the following command to run the experiment finally.   bash    After replacing theREPRESENTATION_STRING``` run the following command:   """;General;https://github.com/yashkant/PNAS-Binarized-Neural-Networks
""":#:  NAME   Example scripts for data set interface  training and testing various models are provided under `experiments/`. - N-class semantic segmentation with various architectures (your data) - Generative Adversarial Networks (MNIST) - Variational Autoencoders (MNIST) - Multi-instance / bagged labels (MNIST)  I've recently revised the way I structure experiments in order to separate my experiments from the structure of this repository. New examples coming soon ^(TM).   """;General;https://github.com/BioImageInformatics/tfmodels
"""""";Computer Vision;https://github.com/chez8990/Image2ImageTranslation
"""""";General;https://github.com/chez8990/Image2ImageTranslation
"""""";Natural Language Processing;https://github.com/menajosep/AleatoricSent
"""""";Natural Language Processing;https://github.com/AranKomat/adapinp
"""1. Clone this repository     ```bash    git clone https://github.com/IAmSuyogJadhav/Brainy.git    ```     or [Download](https://github.com/IAmSuyogJadhav/Brainy/archive/master.zip) and then extract its contents.  2. Download the model from [here](https://drive.google.com/open?id=1U6o7FfR7Fue6ukIg_ciUoN0rjZs6QfrW) and put the file inside `app/static/models/` folder. Do not change the name (It should be `Model_1.h5`).  3. From the root folder of te repository  run: ```bash pip3 install -r requirements.txt ``` to install ll the dependencies. Note that this project is based on python3.  4. Start the app using following command  when in the main folder:     ```bash    flask run    ```     It may take a while.   5. Now open your browser and navigate to http://localhost:5000 and follow the instructions. It is recommended to have a Nvidia GPU  since it can speed up the prediction task manifolds.               <b>1.</b> The contracting/downsampling path <br />   """;Computer Vision;https://github.com/IAmSuyogJadhav/Brainy
"""``` python   You should install ü§ó Transformers in a virtual environment. If you're unfamiliar with Python virtual environments  check out the user guide.  First  create a virtual environment with the version of Python you're going to use and activate it.  Then  you will need to install at least one of Flax  PyTorch or TensorFlow.  Please refer to TensorFlow installation page  PyTorch installation page and/or Flax installation page regarding the specific install command for your platform.  When one of those backends has been installed  ü§ó Transformers can be installed using pip as follows:  pip install transformers  If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release  you must install the library from source.  Since Transformers version v4.0.0  we now have a conda channel: huggingface.  ü§ó Transformers can be installed using conda as follows:   conda install -c huggingface transformers  Follow the installation pages of Flax  PyTorch or TensorFlow to see how to install them with conda.   You can test most of our models directly on their pages from the [model hub](https://huggingface.co/models). We also offer [private model hosting  versioning  & an inference API](https://huggingface.co/pricing) for public and private models.  Here are a few examples:   In Natural Language Processing: - [Masked word completion with BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France) - [Name Entity Recognition with Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city) - [Text generation with GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+) - [Natural Language Inference with RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal) - [Summarization with BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct) - [Question answering with DistilBERT](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species) - [Translation with T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)  In Computer Vision: - [Image classification with ViT](https://huggingface.co/google/vit-base-patch16-224) - [Object Detection with DETR](https://huggingface.co/facebook/detr-resnet-50) - [Image Segmentation with DETR](https://huggingface.co/facebook/detr-resnet-50-panoptic)  In Audio: - [Automatic Speech Recognition with Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h) - [Keyword Spotting with Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)  **[Write With Transformer](https://transformer.huggingface.co)**  built by the Hugging Face team  is the official demo of this repo‚Äôs text generation capabilities.   1. Easy-to-use state-of-the-art models:     - High performance on natural language understanding & generation  computer vision  and audio tasks.     - Low barrier to entry for educators and practitioners.     - Few user-facing abstractions with just three classes to learn.     - A unified API for using all our pretrained models.  1. Lower compute costs  smaller carbon footprint:     - Researchers can share trained models instead of always retraining.     - Practitioners can reduce compute time and production costs.     - Dozens of architectures with over 20 000 pretrained models  some in more than 100 languages.  1. Choose the right framework for every part of a model's lifetime:     - Train state-of-the-art models in 3 lines of code.     - Move a single model between TF2.0/PyTorch/JAX frameworks at will.     - Seamlessly pick the right framework for training  evaluation and production.  1. Easily customize a model or an example to your needs:     - We provide examples for each architecture to reproduce the results published by its original authors.     - Model internals are exposed as consistently as possible.     - Model files can be used independently of the library for quick experiments.   - This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose  so that researchers can quickly iterate on each of the models without diving into additional abstractions/files. - The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops  you should use another library. - While we strive to present as many use cases as possible  the scripts in our [examples folder](https://github.com/huggingface/transformers/tree/master/examples) are just that: examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.   """;Natural Language Processing;https://github.com/huggingface/transformers
"""1) Python   """;General;https://github.com/dare0021/MemN2N_Bench
"""""";Computer Vision;https://github.com/Knight825/models-pytorch
"""""";General;https://github.com/Knight825/models-pytorch
"""""";Computer Vision;https://github.com/jiama843/tf-YOLO-pascalVOC
"""Please follow the instructions of py-faster-rcnn [here](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to setup VOC and COCO datasets (Part of COCO is done). The steps involve downloading data and optionally creating soft links in the ``data`` folder. Since faster RCNN does not rely on pre-computed proposals  it is safe to ignore the steps that setup proposals.  If you find it useful  the ``data/cache`` folder created on my side is also shared [here](http://ladoga.graphics.cs.cmu.edu/xinleic/tf-faster-rcnn/cache.tgz).   1. Clone the repository   ```Shell   git clone https://github.com/endernewton/tf-faster-rcnn.git   ```  2. Update your -arch in setup script to match your GPU   ```Shell   cd tf-faster-rcnn/lib   #: Change the GPU architecture (-arch) if necessary   vim setup.py   ```    | GPU model  | Architecture |   | ------------- | ------------- |   | TitanX (Maxwell/Pascal) | sm_52 |   | GTX 960M | sm_50 |   | GTX 1080 (Ti) | sm_61 |   | Grid K520 (AWS g2.2xlarge) | sm_30 |   | Tesla K80 (AWS p2.xlarge) | sm_37 |    **Note**: You are welcome to contribute the settings on your end if you have made the code work properly on other GPUs. Also even if you are only using CPU tensorflow  GPU based code (for NMS) will be used by default  so please set **USE_GPU_NMS False** to get the correct output.   3. Build the Cython modules   ```Shell   make clean   make   cd ..   ```  4. Install the [Python COCO API](https://github.com/pdollar/coco). The code requires the API to access COCO dataset.   ```Shell   cd data   git clone https://github.com/pdollar/coco.git   cd coco/PythonAPI   make   cd ../../..   ```     - Due to the randomness in GPU training with Tensorflow especially for VOC  the best numbers are reported (with 2-3 attempts) here. According to my experience  for COCO you can almost always get a very close number (within ~0.2%) despite the randomness.      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz     tar -xzvf vgg_16_2016_08_28.tar.gz      cd ../..     For Resnet101  you can set up like:      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz     tar -xzvf resnet_v1_101_2016_08_28.tar.gz      cd ../..     ./experiments/scripts/train_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/train_faster_rcnn.sh 1 coco res101     ./experiments/scripts/test_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/test_faster_rcnn.sh 1 coco res101   1. Download pre-trained model   ```Shell   #: Resnet101 for voc pre-trained on 07+12 set   ./data/scripts/fetch_faster_rcnn_models.sh   ```   **Note**: if you cannot download the models through the link  or you want to try more models  you can check out the following solutions and optionally update the downloading script:   - Another server [here](http://xinlei.sp.cs.cmu.edu/xinleic/tf-faster-rcnn/).   - Google drive [here](https://drive.google.com/open?id=0B1_fAEgxdnvJSmF3YUlZcHFqWTQ).  2. Create a folder and a soft link to use the pre-trained model   ```Shell   NET=res101   TRAIN_IMDB=voc_2007_trainval+voc_2012_trainval   mkdir -p output/${NET}/${TRAIN_IMDB}   cd output/${NET}/${TRAIN_IMDB}   ln -s ../../../data/voc_2007_trainval+voc_2012_trainval ./default   cd ../../..   ```  3. Demo for testing on custom images   ```Shell   #: at repository root   GPU_ID=0   CUDA_VISIBLE_DEVICES=${GPU_ID} ./tools/demo.py   ```   **Note**: Resnet101 testing probably requires several gigabytes of memory  so if you encounter memory capacity issues  please install it with CPU support only. Refer to [Issue 25](https://github.com/endernewton/tf-faster-rcnn/issues/25).  4. Test with pre-trained Resnet101 models   ```Shell   GPU_ID=0   ./experiments/scripts/test_faster_rcnn.sh $GPU_ID pascal_voc_0712 res101   ```   **Note**: If you cannot get the reported numbers (79.8 on my side)  then probably the NMS function is compiled improperly  refer to [Issue 5](https://github.com/endernewton/tf-faster-rcnn/issues/5).   """;Computer Vision;https://github.com/wangzpeng/tf-faster-rcnn
"""TensorBoard.dev are also provided for models to the   """;Natural Language Processing;https://github.com/tensorflow/models
"""""";Natural Language Processing;https://github.com/tomgoter/nlp_finalproject
"""Update BASE_DIR in config.ini with the absolute path to the current directory. <br/> Packages needed to run the code include:  * numpy * python * PyToch * argparse * configparser  The following results are were generated using the architectures listed above. \   """;Sequential;https://github.com/trevor-richardson/rnn_zoo
"""./compile.sh #Compile cython code   """;General;https://github.com/adefazio/point-saga
"""cine = gg2vec(""cine"" 7474 ""neo4j"" ""**"" ""name"" 400000 200 6 ""normal"" [] 1)   """;Natural Language Processing;https://github.com/palmagro/gg2vec
"""1) Clone this repository. 2) In the repository  execute `pip install . --user`.    Note that due to inconsistencies with how `tensorflow` should be installed     this package does not define a dependency on `tensorflow` as it will try to install that (which at least on Arch Linux results in an incorrect installation).    Please make sure `tensorflow` is installed as per your systems requirements. 3) Alternatively  you can run the code directly from the cloned  repository  however you need to run `python setup.py build_ext --inplace` to compile Cython code first. 4) Optionally  install `pycocotools` if you want to train / test on the MS COCO dataset by running `pip install --user git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI`.   : Using the installed script:   Note that the train script uses relative imports since it is inside the keras_retinanet package.   you will need to switch it to use absolute imports.  If you installed keras-retinanet correctly  the train script will be installed as retinanet-train.  However  if you make local modifications to the keras-retinanet repository  you should run the script directly from the repository.  That will ensure that your local changes will be used by the train script.   All models can be downloaded from the releases page.   For training on [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/)  run: ```shell #: Running directly from the repository: keras_retinanet/bin/train.py pascal /path/to/VOCdevkit/VOC2007  #: Using the installed script: retinanet-train pascal /path/to/VOCdevkit/VOC2007 ```  For training on [MS COCO](http://cocodataset.org/#home)  run: ```shell #: Running directly from the repository: keras_retinanet/bin/train.py coco /path/to/MS/COCO  #: Using the installed script: retinanet-train coco /path/to/MS/COCO ```  For training on Open Images Dataset [OID](https://storage.googleapis.com/openimages/web/index.html) or taking place to the [OID challenges](https://storage.googleapis.com/openimages/web/challenge.html)  run: ```shell #: Running directly from the repository: keras_retinanet/bin/train.py oid /path/to/OID  #: Using the installed script: retinanet-train oid /path/to/OID  #: You can also specify a list of labels if you want to train on a subset #: by adding the argument 'labels_filter': keras_retinanet/bin/train.py oid /path/to/OID --labels-filter=Helmet Tree  #: You can also specify a parent label if you want to train on a branch #: from the semantic hierarchical tree (i.e a parent and all children) (https://storage.googleapis.com/openimages/challenge_2018/bbox_labels_500_hierarchy_visualizer/circle.html) #: by adding the argument 'parent-label': keras_retinanet/bin/train.py oid /path/to/OID --parent-label=Boat ```   For training on [KITTI](http://www.cvlibs.net/datasets/kitti/eval_object.php)  run: ```shell #: Running directly from the repository: keras_retinanet/bin/train.py kitti /path/to/KITTI  #: Using the installed script: retinanet-train kitti /path/to/KITTI  If you want to prepare the dataset you can use the following script: https://github.com/NVIDIA/DIGITS/blob/master/examples/object-detection/prepare_kitti_data.py ```   For training on a [custom dataset]  a CSV file can be used as a way to pass the data. See below for more details on the format of these CSV files. To train using your CSV  run: ```shell #: Running directly from the repository: keras_retinanet/bin/train.py csv /path/to/csv/file/containing/annotations /path/to/csv/file/containing/classes  #: Using the installed script: retinanet-train csv /path/to/csv/file/containing/annotations /path/to/csv/file/containing/classes ```  In general  the steps to train on your own datasets are: 1) Create a model by calling for instance `keras_retinanet.models.backbone('resnet50').retinanet(num_classes=80)` and compile it.    Empirically  the following compile arguments have been found to work well: ```python model.compile(     loss={         'regression'    : keras_retinanet.losses.smooth_l1()          'classification': keras_retinanet.losses.focal()     }      optimizer=keras.optimizers.Adam(lr=1e-5  clipnorm=0.001) ) ``` 2) Create generators for training and testing data (an example is show in [`keras_retinanet.preprocessing.pascal_voc.PascalVocGenerator`](https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/preprocessing/pascal_voc.py)). 3) Use `model.fit_generator` to start training.   """;Computer Vision;https://github.com/fizyr/keras-retinanet
"""1) Clone this repository. 2) In the repository  execute `pip install . --user`.    Note that due to inconsistencies with how `tensorflow` should be installed     this package does not define a dependency on `tensorflow` as it will try to install that (which at least on Arch Linux results in an incorrect installation).    Please make sure `tensorflow` is installed as per your systems requirements. 3) Alternatively  you can run the code directly from the cloned  repository  however you need to run `python setup.py build_ext --inplace` to compile Cython code first. 4) Optionally  install `pycocotools` if you want to train / test on the MS COCO dataset by running `pip install --user git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI`.   : Using the installed script:   Note that the train script uses relative imports since it is inside the keras_retinanet package.   you will need to switch it to use absolute imports.  If you installed keras-retinanet correctly  the train script will be installed as retinanet-train.  However  if you make local modifications to the keras-retinanet repository  you should run the script directly from the repository.  That will ensure that your local changes will be used by the train script.   All models can be downloaded from the releases page.   For training on [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/)  run: ```shell #: Running directly from the repository: keras_retinanet/bin/train.py pascal /path/to/VOCdevkit/VOC2007  #: Using the installed script: retinanet-train pascal /path/to/VOCdevkit/VOC2007 ```  For training on [MS COCO](http://cocodataset.org/#home)  run: ```shell #: Running directly from the repository: keras_retinanet/bin/train.py coco /path/to/MS/COCO  #: Using the installed script: retinanet-train coco /path/to/MS/COCO ```  For training on Open Images Dataset [OID](https://storage.googleapis.com/openimages/web/index.html) or taking place to the [OID challenges](https://storage.googleapis.com/openimages/web/challenge.html)  run: ```shell #: Running directly from the repository: keras_retinanet/bin/train.py oid /path/to/OID  #: Using the installed script: retinanet-train oid /path/to/OID  #: You can also specify a list of labels if you want to train on a subset #: by adding the argument 'labels_filter': keras_retinanet/bin/train.py oid /path/to/OID --labels-filter=Helmet Tree  #: You can also specify a parent label if you want to train on a branch #: from the semantic hierarchical tree (i.e a parent and all children) (https://storage.googleapis.com/openimages/challenge_2018/bbox_labels_500_hierarchy_visualizer/circle.html) #: by adding the argument 'parent-label': keras_retinanet/bin/train.py oid /path/to/OID --parent-label=Boat ```   For training on [KITTI](http://www.cvlibs.net/datasets/kitti/eval_object.php)  run: ```shell #: Running directly from the repository: keras_retinanet/bin/train.py kitti /path/to/KITTI  #: Using the installed script: retinanet-train kitti /path/to/KITTI  If you want to prepare the dataset you can use the following script: https://github.com/NVIDIA/DIGITS/blob/master/examples/object-detection/prepare_kitti_data.py ```   For training on a [custom dataset]  a CSV file can be used as a way to pass the data. See below for more details on the format of these CSV files. To train using your CSV  run: ```shell #: Running directly from the repository: keras_retinanet/bin/train.py csv /path/to/csv/file/containing/annotations /path/to/csv/file/containing/classes  #: Using the installed script: retinanet-train csv /path/to/csv/file/containing/annotations /path/to/csv/file/containing/classes ```  In general  the steps to train on your own datasets are: 1) Create a model by calling for instance `keras_retinanet.models.backbone('resnet50').retinanet(num_classes=80)` and compile it.    Empirically  the following compile arguments have been found to work well: ```python model.compile(     loss={         'regression'    : keras_retinanet.losses.smooth_l1()          'classification': keras_retinanet.losses.focal()     }      optimizer=keras.optimizers.Adam(lr=1e-5  clipnorm=0.001) ) ``` 2) Create generators for training and testing data (an example is show in [`keras_retinanet.preprocessing.pascal_voc.PascalVocGenerator`](https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/preprocessing/pascal_voc.py)). 3) Use `model.fit_generator` to start training.   """;General;https://github.com/fizyr/keras-retinanet
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;Computer Vision;https://github.com/vantupham/darknet
"""- Mask R-CNN - MaskLab   """;Computer Vision;https://github.com/muramasa8191/DeepLearning
"""Sample - (follow similarly for CyclicLinearLR) ``` from cyclicLR import CyclicCosAnnealingLR import torch  optimizer = torch.optim.SGD(lr=1e-3) scheduler = CyclicCosAnnealingLR(optimizer milestones=[30 80] eta_min=1e-6) for epoch in range(100):   scheduler.step()   train(..)   validate(..) ``` >Note: scheduler.step() shown is called at every epoch. It can be called even in every batch. Remember to specify milestones in number of batches (and not number of epochs) in such as case.    """;General;https://github.com/Harshvardhan1/cyclic-learning-schedulers-pytorch
"""- To install  `cd` into the root directory and type `pip install -e .`  - To interactively view moving to landmark scenario (see others in ./scenarios/): `bin/interactive.py --scenario simple.py`  - Known dependencies: Python (3.5.4)  OpenAI gym (0.10.5)  numpy (1.14.5)  - To use the environments  look at the code for importing them in `make_env.py`.   """;Reinforcement Learning;https://github.com/lachisis/multiagent-particle-envs
"""`./src/model/model.py` contains the CPC models implementation  `./src/main.py` is the code for training the CPC models  `./src/spk_class.py` trains a NN speaker classifier  `./ivector/` contains the scripts for running an i-vectors speaker verification system.   An example of CPC and speaker classifier training can be found at ``` ./run.sh ```   """;General;https://github.com/jefflai108/Contrastive-Predictive-Coding-PyTorch
"""Tested on Nvidia GTX1080  AMD R9 295X2  Intel HD Graphics 630 GPU.   """;Computer Vision;https://github.com/mz24cn/clnet
"""The best way to install Explauto at the moment is to clone the repo and use it in [development mode](http://flowersteam.github.io/explauto/installation.html#as-a-developer). It is also available as a [python package](https://pypi.python.org/pypi/explauto/). The core of explauto depends on the following packages:  * [python](http://www.python.org) 2.7 or 3.* * [numpy](http://www.numpy.org) * [scipy](http://www.scipy.org) * [scikit-learn](http://scikit-learn.org/)  For more details  please refer to the [installation section](http://flowersteam.github.io/explauto/installation.html) of the documentation.  Most of Explauto's documentation is written as [IPython notebooks](http://ipython.org/notebook.html). If you do not know how to use them  please refer to the [dedicated section](http://flowersteam.github.io/explauto/notebook.html).  * [Full tutorial describing how to use the library](http://nbviewer.ipython.org/github/flowersteam/explauto/blob/master/notebook/full_tutorial.ipynb)  * More specific tutorials     * [Setting environments](http://nbviewer.ipython.org/github/flowersteam/explauto/blob/master/notebook/setting_environments.ipynb)     * [Learning sensorimotor models](http://nbviewer.ipython.org/github/flowersteam/explauto/blob/master/notebook/learning_sensorimotor_models.ipynb)     * [Summary of available sensorimotor and interest models](http://nbviewer.ipython.org/github/flowersteam/explauto/blob/master/notebook/summary_available_models.ipynb)     * [Learning sensorimotor models with sensorimotor context](http://nbviewer.ipython.org/github/flowersteam/explauto/blob/master/notebook/learning_with_sensorimotor_context.ipynb)     * [Learning sensorimotor models with context provided by environment](http://nbviewer.ipython.org/github/flowersteam/explauto/blob/master/notebook/learning_with_environment_context.ipynb)     * Comming soon: Autonomous exploration using interest models     * [Setting a basic experiment](http://nbviewer.ipython.org/github/flowersteam/explauto/blob/master/notebook/setting_basic_experiment.ipynb)     * [Comparing motor vs goal strategies](http://nbviewer.ipython.org/github/flowersteam/explauto/blob/master/notebook/comparing_motor_goal_stategies.ipynb)     * [Running pool of experiments](http://nbviewer.ipython.org/github/flowersteam/explauto/blob/master/notebook/running_experiment_pool.ipynb)     * [Introducing curiosity-driven exploration](http://nbviewer.ipython.org/github/flowersteam/explauto/blob/master/notebook/introducing_curiosity_learning.ipynb)     * [Poppy environment](http://nbviewer.ipython.org/github/flowersteam/explauto/blob/master/notebook/poppy_environment.ipynb)     * [Fast-forward a previous experiment](http://nbviewer.ipython.org/github/flowersteam/explauto/blob/master/notebook/fast_forward_experiment.ipynb)     * [Tutorial on Active Model Babbling and a comparison with Motor Babbling and Goal Babbling](http://nbviewer.jupyter.org/github/sebastien-forestier/ExplorationAlgorithms/blob/master/main.ipynb)     * [Goal Babbling with direct optimization](http://nbviewer.ipython.org/github/flowersteam/explauto/blob/master/notebook/goal_babbling_direct_optimization.ipynb)     * [Learning to produce sounds with the DIVA vocal synthesizer and Dynamic Movement Primitives](http://nbviewer.jupyter.org/github/flowersteam/explauto/blob/master/notebook/DivaDMP.ipynb)    """;General;https://github.com/flowersteam/explauto
"""2. Due to huge memory use with OS=8  Xception backbone should be trained with OS=16 and only inferenced with OS=8.   Model will return tensor of shape `(batch_size  height  width  num_classes)`. To obtain labels  you need to apply argmax to logits at exit layer. Example of predicting on `image1.jpg`:    ```python import numpy as np from PIL import Image from matplotlib import pyplot as plt  from model import Deeplabv3  #: Generates labels using most basic setup.  Supports various image sizes.  Returns image labels in same format #: as original image.  Normalization matches MobileNetV2  trained_image_width=512  mean_subtraction_value=127.5 image = np.array(Image.open('imgs/image1.jpg'))  #: resize to max dimension of images from training dataset w  h  _ = image.shape ratio = float(trained_image_width) / np.max([w  h]) resized_image = np.array(Image.fromarray(image.astype('uint8')).resize((int(ratio * h)  int(ratio * w))))  #: apply normalization for trained dataset images resized_image = (resized_image / mean_subtraction_value) - 1.  #: pad array to square image to match training images pad_x = int(trained_image_width - resized_image.shape[0]) pad_y = int(trained_image_width - resized_image.shape[1]) resized_image = np.pad(resized_image  ((0  pad_x)  (0  pad_y)  (0  0))  mode='constant')  #: make prediction deeplab_model = Deeplabv3() res = deeplab_model.predict(np.expand_dims(resized_image  0)) labels = np.argmax(res.squeeze()  -1)  #: remove padding and resize back to original image if pad_x > 0:     labels = labels[:-pad_x] if pad_y > 0:     labels = labels[:  :-pad_y] labels = np.array(Image.fromarray(labels.astype('uint8')).resize((h  w)))  plt.imshow(labels) plt.waitforbuttonpress() ```   ```python from model import Deeplabv3 deeplab_model = Deeplabv3(input_shape=(384  384  3)  classes=4)¬†  #:or you can use None as shape deeplab_model = Deeplabv3(input_shape=(None  None  3)  classes=4) ``` After that you will get a usual Keras model which you can train using `.fit` and `.fit_generator` methods.   """;Computer Vision;https://github.com/bonlime/keras-deeplab-v3-plus
"""Instructions for acquiring PTB and WT2 can be found here. While CIFAR-10 can be automatically downloaded by torchvision  ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions here.   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/liamcli/darts
"""TensorBoard.dev are also provided for models to the   """;Computer Vision;https://github.com/tensorflow/models
"""![](https://user-images.githubusercontent.com/10624937/43851024-320ba930-9aff-11e8-8493-ee547c6af349.gif ""Trained Agent"")  In this environment  a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus  the goal of your agent is to maintain its position at the target location for as many time steps as possible.  The observation space consists of 33 variables corresponding to position  rotation  velocity  and angular velocities of the arm. Each action is a vector with four numbers  corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.  This projects implements DDPG for continous control for the [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment.   Follow setup here.   """;Reinforcement Learning;https://github.com/prajwalgatti/DRL-Continuous-Control
"""""";Computer Vision;https://github.com/Tools4Project/4501Project
"""Currently  models.py contains most of the code to train and create the models. To use different modes  uncomment the parts of the code that you need.  Note the difference between the *_network objects and *_model objects.  - The *_network objects refer to the helper classes which create and manage the Keras models  load and save weights and  set whether the model can be trained or not. - The *_models objects refer to the underlying Keras model.   **Note**: The training images need to be stored in a subdirectory. Assume the path to the images is `/path-to-dir/path-to-sub-dir/*.png`  then simply write the path as `coco_path = /path-to-dir`. If this does not work  try `coco_path = /path-to-dir/` with a trailing slash (/)  To just create the pretrain model: ``` srgan_network = SRGANNetwork(img_width=32  img_height=32  batch_size=1) srgan_model = srgan_network.build_srgan_pretrain_model()  #: Plot the model from keras.utils.visualize_util import plot plot(srgan_model  to_file='SRGAN.png'  show_shapes=True) ```  To pretrain the SR network: ``` srgan_network = SRGANNetwork(img_width=32  img_height=32  batch_size=1) srgan_network.pre_train_srgan(iamges_path  nb_epochs=1  nb_images=50000) ```  ** NOTE **: There may be many cases where generator initializations may lead to completely solid validation images.  Please check the first few iterations to see if the validation images are not solid images.  To counteract this  a pretrained generator model has been provided  from which you can restart training. Therefore the model can continue learning without hitting a bad initialization.  To pretrain the Discriminator  network: ``` srgan_network = SRGANNetwork(img_width=32  img_height=32  batch_size=1) srgan_network.pre_train_discriminator(iamges_path  nb_epochs=1  nb_images=50000  batchsize=16) ```  To train the full network (Does NOT work properly right now  Discriminator is not correctly trained): ``` srgan_network = SRGANNetwork(img_width=32  img_height=32  batch_size=1) srgan_network.train_full_model(coco_path  nb_images=80000  nb_epochs=10) ```   """;General;https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks
"""Currently  models.py contains most of the code to train and create the models. To use different modes  uncomment the parts of the code that you need.  Note the difference between the *_network objects and *_model objects.  - The *_network objects refer to the helper classes which create and manage the Keras models  load and save weights and  set whether the model can be trained or not. - The *_models objects refer to the underlying Keras model.   **Note**: The training images need to be stored in a subdirectory. Assume the path to the images is `/path-to-dir/path-to-sub-dir/*.png`  then simply write the path as `coco_path = /path-to-dir`. If this does not work  try `coco_path = /path-to-dir/` with a trailing slash (/)  To just create the pretrain model: ``` srgan_network = SRGANNetwork(img_width=32  img_height=32  batch_size=1) srgan_model = srgan_network.build_srgan_pretrain_model()  #: Plot the model from keras.utils.visualize_util import plot plot(srgan_model  to_file='SRGAN.png'  show_shapes=True) ```  To pretrain the SR network: ``` srgan_network = SRGANNetwork(img_width=32  img_height=32  batch_size=1) srgan_network.pre_train_srgan(iamges_path  nb_epochs=1  nb_images=50000) ```  ** NOTE **: There may be many cases where generator initializations may lead to completely solid validation images.  Please check the first few iterations to see if the validation images are not solid images.  To counteract this  a pretrained generator model has been provided  from which you can restart training. Therefore the model can continue learning without hitting a bad initialization.  To pretrain the Discriminator  network: ``` srgan_network = SRGANNetwork(img_width=32  img_height=32  batch_size=1) srgan_network.pre_train_discriminator(iamges_path  nb_epochs=1  nb_images=50000  batchsize=16) ```  To train the full network (Does NOT work properly right now  Discriminator is not correctly trained): ``` srgan_network = SRGANNetwork(img_width=32  img_height=32  batch_size=1) srgan_network.train_full_model(coco_path  nb_images=80000  nb_epochs=10) ```   """;Computer Vision;https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks
"""You can create WordNet noun hypernym pairs as follows  ```shell python ../scripts/create_wordnet_noun_hierarchy.py ./wordnet_noun_hypernyms.tsv ```  and mammal subtree is created by  ```shell python ../scripts/create_mammal_subtree.py ./mammal_subtree.tsv ```   From the poincare-embeddings directory...  ```shell python3 -m venv venv source venv/bin/activate ```  if using windows:  ```shell python3 -m venv venv venv\Scripts\activate ```  Then run the following:  ```shell python3 -m pip install -r requirements.txt python3 -c ""import nltk; nltk.download('wordnet')"" ```   cd poincare-embedding  mkdir work &amp; cd work   Note: if that doesn't work  may need to run the following:   We assume that you are in work directory   ```shell cd poincare-embedding mkdir work & cd work ```   """;Natural Language Processing;https://github.com/TatsuyaShirakawa/poincare-embedding
"""Assue that you have put the focal_loss.py in your operator path  you can use:  ``` from your_operators.focal_loss import *  cls_prob = mx.sym.Custom(op_type='FocalLoss'  name = 'cls_prob'  data = cls_score  labels = label  alpha =0.25  gamma= 2)  ```   """;Computer Vision;https://github.com/unsky/focal-loss
"""Assue that you have put the focal_loss.py in your operator path  you can use:  ``` from your_operators.focal_loss import *  cls_prob = mx.sym.Custom(op_type='FocalLoss'  name = 'cls_prob'  data = cls_score  labels = label  alpha =0.25  gamma= 2)  ```   """;General;https://github.com/unsky/focal-loss
"""- Python 3.6.4 - Please see [requirements.txt](requirements.txt) for necessary modules   ![](walk-bot.gif)   """;Reinforcement Learning;https://github.com/Ostyk/walk-bot
"""Installation instructions   Windows Caffe   """;Computer Vision;https://github.com/weiliu89/caffe
"""This repository contains the code for the blog post: [Using Microsoft AI to Build a Lung-Disease Prediction Model using Chest X-Ray Images](https://blogs.technet.microsoft.com/machinelearning/2018/03/07/using-microsoft-ai-to-build-a-lung-disease-prediction-model-using-chest-x-ray-images/)  by Xiaoyong Zhu  George Iordanescu  Ilia Karmanov  data scientists from Microsoft  and Mazen Zawaideh  radiologist resident from University of Washington Medical Center.  In this repostory  we provide you the Keras code (`001-003 Jupyter Notebooks under AzureChestXRay_AMLWB\Code\02_Model`) and PyTorch code (`AzureChestXRay_AMLWB\Code\02_Model060_Train_pyTorch`). You should be able to run the code from scratch and get the below result using Azure Machine Learning platform or run it using your own GPU machine.   If you are using Azure Machine Learning as the training platform  all the dependencies should be installed. However  if you are trying out in your own environment  you should also install [keras-contrib](https://github.com/keras-team/keras-contrib) repository to run Keras code.  If you are trying out the lung detection algorithm  you need to install a few other additional libraries. Please refer to the `README.md` file under folder `AzureChestXRay\AzureChestXRay_AMLWB\Code\src\finding_lungs` for more details.   """;Computer Vision;https://github.com/Azure/AzureChestXRay
"""Download the dataset [Market-1501](http://www.liangzheng.com.cn/Project/project_reid.html) [[Google Drive]](https://drive.google.com/file/d/0B8-rUzbwVRk0c054eEozWG9COHM/view) [[Baidu Disk]](https://pan.baidu.com/s/1ntIi2Op)  Preparation: put the images with the same id in one folder. You may use  ```bash python prepare-market.py          #: for Market-1501 ``` Note to modify the dataset path to your own path.   - Install [PyTorch](http://pytorch.org/)  - Install torchvision from the source: ``` git clone https://github.com/pytorch/vision cd vision python setup.py install ``` - [Optional] You may skip it. Install APEX from the source: ``` git clone https://github.com/NVIDIA/apex.git cd apex python setup.py install --cuda_ext --cpp_ext ``` - Clone this repo: ```bash git clone https://github.com/NVlabs/DG-Net.git cd DG-Net/ ```  Our code is tested on PyTorch 1.0.0+ and torchvision 0.2.1+ .   - APEX to save GPU memory (fp16/fp32)   wget https://github.com/prasmussen/gdrive/releases/download/2.1.1/gdrive_2.1.1_linux_386.tar.gz  tar -xzvf gdrive_2.1.1_linux_386.tar.gz  gdrive download 126Gn90Tzpk3zWp2c7OBYPKc-ZjhptKDo   """;Computer Vision;https://github.com/NVlabs/DG-Net
"""To run the Neural Photo Editor  you will need: - Python  likely version 2.7. You may be able to use early versions of Python2  but I'm pretty sure there's some incompatibilities with Python3 in here. - [Theano](http://deeplearning.net/software/theano/)  development version.   - [lasagne](http://lasagne.readthedocs.io/en/latest/user/installation.html)  development version. - I highly recommend [cuDNN](https://developer.nvidia.com/cudnn) as speed is key  but it is not a dependency. - numpy  scipy  PIL  Tkinter and tkColorChooser  but it is likely that your python distribution already has those.   You will need Fuel along with the 64x64 version of celebA. See here for instructions on downloading and preparing it.    Note that you will need matplotlib. to do so.   """;Computer Vision;https://github.com/ajbrock/Neural-Photo-Editor
"""To run the Neural Photo Editor  you will need: - Python  likely version 2.7. You may be able to use early versions of Python2  but I'm pretty sure there's some incompatibilities with Python3 in here. - [Theano](http://deeplearning.net/software/theano/)  development version.   - [lasagne](http://lasagne.readthedocs.io/en/latest/user/installation.html)  development version. - I highly recommend [cuDNN](https://developer.nvidia.com/cudnn) as speed is key  but it is not a dependency. - numpy  scipy  PIL  Tkinter and tkColorChooser  but it is likely that your python distribution already has those.   You will need Fuel along with the 64x64 version of celebA. See here for instructions on downloading and preparing it.    Note that you will need matplotlib. to do so.   """;General;https://github.com/ajbrock/Neural-Photo-Editor
"""you can see how i choose hyperparameters below   * Jupyter notebook: [link](https://nbviewer.jupyter.org/github/simonjisu/NMT/blob/master/Neural_Machine_Translation_Tutorial.ipynb) * Preparing for demo   For 'HELP' please insert argument behind `main.py -h`. or you can just run   ``` $ cd model $ sh runtrain.sh ```   """;General;https://github.com/simonjisu/NMT
"""The suggested solution is to install Anaconda Python 3.6 version: https://www.anaconda.com/download/.  Then install the latest version of Pytorch and Torchvision by the command shown as below  pip install torch torchvision   """;General;https://github.com/hsvgbkhgbv/Thermostat-assisted-continuously-tempered-Hamiltonian-Monte-Carlo-for-Bayesian-learning
"""If you want to train the model and have Nvidia GPUs (like GTX 1080  GTX Titan  etc)  please setup CUDA environment and install tensorflow-gpu.   pip3 install -U tensorflow-gpu==1.1   You can check whether the GPU works by   If you don't have a GPU  you can still use the pretrained models and generate summaries using your CPU.   pip3 install -U tensorflow==1.1   """;General;https://github.com/thunlp/TensorFlow-Summarization
"""Python 3.6   """;General;https://github.com/jxz542189/dmn_plus
"""Download CIFAR10 or CIFAR100 dataset or prepare your own dataset like a dataloader defined in PyTorch   This project is compiled and run on Python 2.7 and PyTorch 0.4.0 Here are some necessaries dependencies: ``` torch 0.4.0 torchvision 0.2.1 numpy 1.14.3 tensorboardX 1.2 ``` use pip to install them first   """;Computer Vision;https://github.com/zym1119/MobileNetV2_pytorch_cifar
"""Download CIFAR10 or CIFAR100 dataset or prepare your own dataset like a dataloader defined in PyTorch   This project is compiled and run on Python 2.7 and PyTorch 0.4.0 Here are some necessaries dependencies: ``` torch 0.4.0 torchvision 0.2.1 numpy 1.14.3 tensorboardX 1.2 ``` use pip to install them first   """;General;https://github.com/zym1119/MobileNetV2_pytorch_cifar
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   For R-FCN/Faster R-CNN\: 1. Please download COCO and VOC 2007+2012 datasets  and make sure it looks like this:  	``` 	./data/coco/ 	./data/VOCdevkit/VOC2007/ 	./data/VOCdevkit/VOC2012/ 	```  2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  For DeepLab\: 1. Please download Cityscapes and VOC 2012 datasets and make sure it looks like this:  	``` 	./data/cityscapes/ 	./data/VOCdevkit/VOC2012/ 	``` 2. Please download argumented VOC 2012 annotations/image lists  and put the argumented annotations and the argumented train/val lists into:  	``` 	./data/VOCdevkit/VOC2012/SegmentationClass/ 	./data/VOCdevkit/VOC2012/ImageSets/Main/ 	```      Respectively.     2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  1. Clone the Deformable ConvNets repository  and we'll call the directory that you cloned Deformable-ConvNets as ${DCN_ROOT}. ``` git clone https://github.com/msracver/Deformable-ConvNets.git ```  2. For Windows users  run ``cmd .\init.bat``. For Linux user  run `sh ./init.sh`. The scripts will build cython module automatically and create some folders.  3. Install MXNet: 	 	**Note: The MXNet's Custom Op cannot execute parallelly using multi-gpus after this [PR](https://github.com/apache/incubator-mxnet/pull/6928). We strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.5).**  	***Quick start***  	3.1 Install MXNet and all dependencies by  	``` 	pip install -r requirements.txt 	``` 	If there is no other error message  MXNet should be installed successfully.  	 	***Build from source (alternative way)***  	3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by 	``` 	git clone --recursive https://github.com/dmlc/mxnet.git 	git checkout 998378a 	git submodule update 	#: if it's the first time to checkout  just use: git submodule update --init --recursive 	``` 	3.3 Compile MXNet 	``` 	cd ${MXNET_ROOT} 	make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1 	``` 	3.4 Install the MXNet Python binding by 	 	***Note: If you will actively switch between different versions of MXNet  please follow 3.5 instead of 3.4*** 	``` 	cd python 	sudo python setup.py install 	``` 	3.5 For advanced users  you may put your Python packge into `./external/mxnet/$(YOUR_MXNET_PACKAGE)`  and modify `MXNET_VERSION` in `./experiments/rfcn/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`. Thus you can switch among different versions of MXNet quickly.  4. For Deeplab  we use the argumented VOC 2012 dataset. The argumented annotations are provided by [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html) dataset. For convenience  we provide the converted PNG annotations and the lists of train/val images  please download them from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMRhVImMI1jRrsxDg).   We provide trained deformable convnet models  including the deformable R-FCN & Faster R-CNN models trained on COCO trainval  and the deformable DeepLab model trained on CityScapes train.  1. To use the demo with our pre-trained deformable models  please download manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMSjehIcCgAhvEAHw) or [BaiduYun](https://pan.baidu.com/s/1dFlPFED)  and put it under folder `model/`.  	Make sure it looks like this: 	``` 	./model/rfcn_dcn_coco-0000.params 	./model/rfcn_coco-0000.params 	./model/fpn_dcn_coco-0000.params 	./model/fpn_coco-0000.params 	./model/rcnn_dcn_coco-0000.params 	./model/rcnn_coco-0000.params 	./model/deeplab_dcn_cityscapes-0000.params 	./model/deeplab_cityscapes-0000.params 	./model/deform_conv-0000.params 	./model/deform_psroi-0000.params 	``` 2. To run the R-FCN demo  run 	``` 	python ./rfcn/demo.py 	``` 	By default it will run Deformable R-FCN and gives several prediction results  to run R-FCN  use 	``` 	python ./rfcn/demo.py --rfcn_only 	``` 3. To run the DeepLab demo  run 	``` 	python ./deeplab/demo.py 	``` 	By default it will run Deformable Deeplab and gives several prediction results  to run DeepLab  use 	``` 	python ./deeplab/demo.py --deeplab_only 	``` 4. To visualize the offset of deformable convolution and deformable psroipooling  run 	``` 	python ./rfcn/deform_conv_demo.py 	python ./rfcn/deform_psroi_demo.py 	```    1. All of our experiment settings (GPU #  dataset  etc.) are kept in yaml config files at folder `./experiments/rfcn/cfgs`  `./experiments/faster_rcnn/cfgs` and `./experiments/deeplab/cfgs/`. 2. Eight config files have been provided so far  namely  R-FCN for COCO/VOC  Deformable R-FCN for COCO/VOC  Faster R-CNN(2fc) for COCO/VOC  Deformable Faster R-CNN(2fc) for COCO/VOC  Deeplab for Cityscapes/VOC and Deformable Deeplab for Cityscapes/VOC  respectively. We use 8 and 4 GPUs to train models on COCO and on VOC for R-FCN  respectively. For deeplab  we use 4 GPUs for all experiments.  3. To perform experiments  run the python scripts with the corresponding config file as input. For example  to train and test deformable convnets on COCO with ResNet-v1-101  use the following command     ```     python experiments\rfcn\rfcn_end2end_train_test.py --cfg experiments\rfcn\cfgs\resnet_v1_101_coco_trainval_rfcn_dcn_end2end_ohem.yaml     ```     A cache folder would be created automatically to save the model and the log under `output/rfcn_dcn_coco/`. 4. Please find more details in config files and in our code.   """;Computer Vision;https://github.com/qilei123/DeformableConvV2
"""Download the [ImageNet](http://image-net.org/download-images) dataset and create pass through rec (following [tornadomeet's repository](https://github.com/tornadomeet/ResNet#imagenet) but using unchange mode)   """;Computer Vision;https://github.com/TuSimple/neuron-selectivity-transfer
"""""";Reinforcement Learning;https://github.com/ikostrikov/pytorch-rl
"""Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment:  ``` python -m ipykernel install --user --name drlnd --display-name ""drlnd"" ```  These steps only need to be performed once.      ‚Äã	The instructions at https://github.com/udacity/deep-reinforcement-learning indicate that you should enter the following on the command line to clone the the repository and install the dependencies:  ``` git clone https://github.com/udacity/deep-reinforcement-learning.git cd deep-reinforcement-learning/python pip install . ```  However  for Windows 10  this did not work for me. The pip command fails when it tries to install torch 0.4.0. This version may no longer be available. I edited the dependencies shown in the requirements.txt file in the directory and changed the line for torch from   `torch==0.4.0` to `torch==0.4.1`.   The pip command worked after the change. Otherwise you can install the required packages in the requirements folder manually. Sometimes these software packages change and you may need to refer to the specific instructions for an individual package. For example  https://pytorch.org/get-started/locally/ may be helpful for installing PyTorch.   If you clone the DRLND repository  the original files from the project can be found in the folder deep-reinforcement-learning/p3_collab-compet   - **Linux** or **Mac**:    In a terminal window  perform the following commands:  ``` conda create --name drlnd python=3.6 source activate drlnd ```  - **Windows**:    Make sure you are using the anaconda command line rather than the usual windows cmd.exe.   ``` conda create --name drlnd python=3.6  activate drlnd ```   ‚Äã	The installation of the software is accomplished with the package manager  conda. Installing Anaconda (https://www.anaconda.com/) will include conda as well as facilitate the installation of other data science software packages. The Jupyter Notebook App is also required for running this project and is installed automatically with Anaconda.   	The dependencies for this project can be installed by following the instructions at https://github.com/udacity/deep-reinforcement-learning#dependencies.  Required components include but are are not limited to Python 3.6 (I specifically used 3.6.6)  and PyTorch v0.4  and a version of the Unity ML-Agents toolkit. Note that ML-Agents are only supported on Microsoft Windows 10. I only used Windows 10  so cannot vouch for the accuracy of the instructions for other operating systems.   The github repository is https://github.com/snhwang/p3-collab-compet-SNH.git.  In a terminal window  specifically an Anaconda terminal window for Microsoft Windows  activate the conda environment if not already done:  Linux or Mac:  source activate drlnd  Make sure you are using the anaconda command line rather than the usual windows cmd.exe.   activate drlnd  Change directory to the p1_navigate_SNH folder. Run Jupyter Notebook:  jupyter notebook  Open the notebook Tennis-SNH.ipynb to train with the multi-agent environment. Before running code in a notebook  change the kernel to match the drlnd environment by using the drop-down Kernelmenu:  (taken from the Udacity instructions)   ‚Äã	The installation of the software is accomplished with the package manager  conda. Installing Anaconda (https://www.anaconda.com/) will include conda as well as facilitate the installation of other data science software packages. The Jupyter Notebook App is also required for running this project and is installed automatically with Anaconda.   	The dependencies for this project can be installed by following the instructions at https://github.com/udacity/deep-reinforcement-learning#dependencies.  Required components include but are are not limited to Python 3.6 (I specifically used 3.6.6)  and PyTorch v0.4  and a version of the Unity ML-Agents toolkit. Note that ML-Agents are only supported on Microsoft Windows 10. I only used Windows 10  so cannot vouch for the accuracy of the instructions for other operating systems.   Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment:  ``` python -m ipykernel install --user --name drlnd --display-name ""drlnd"" ```  These steps only need to be performed once.      """;Reinforcement Learning;https://github.com/snhwang/p3_collab-compet
"""This is implementation of real-valued non-volume preserving(real NVP) for Density Estimation(real NVP). I used Two Moons dataset of sklearn and tried to reproduce fig.1 in the paper.    """;Computer Vision;https://github.com/ANLGBOY/RealNVP-with-PyTorch
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/chen-xiong-yi/OwnBERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/chen-xiong-yi/OwnBERT
"""To run this code you need the following:  - a machine with multiple GPUs - Python3 - Numpy  TensorFlow and imageio packages: ``` pip install numpy tensorflow-gpu imageio ```   """;Computer Vision;https://github.com/openai/pixel-cnn
"""""";Reinforcement Learning;https://github.com/ShibiHe/Q-Optimality-Tightening
"""""";General;https://github.com/ShibiHe/Q-Optimality-Tightening
"""This project requires python >= 3.6 and tensorflow >= 1.8.  The other dependencies can be installed with `conda`.  ```bash conda env create -f=environment.yml ```  The following packages are installed.  - pyspark=2.3.1 - librosa==0.6.1  - matplotlib=2.2.2 - hypothesis=3.59.1 - docopt=0.6.2   This implementation was tested with Tesla K20c (4.94GiB GPU memory).   """;Audio;https://github.com/TanUkkii007/wavenet
"""This project requires python >= 3.6 and tensorflow >= 1.8.  The other dependencies can be installed with `conda`.  ```bash conda env create -f=environment.yml ```  The following packages are installed.  - pyspark=2.3.1 - librosa==0.6.1  - matplotlib=2.2.2 - hypothesis=3.59.1 - docopt=0.6.2   This implementation was tested with Tesla K20c (4.94GiB GPU memory).   """;Sequential;https://github.com/TanUkkii007/wavenet
"""``` $ python train.py --help  Usage: train.py [OPTIONS]    Trains the Deep Convolutional Generative Adversarial Network (DCGAN).    See https://arxiv.org/abs/1511.06434 for more details.    Args: optional arguments [python train.py --help]  Options:   -nd  --noise-dim INTEGER   Dimension of noise (1-D Tensor) to feed the                              generator.   -glr  --gen-lr FLOAT       Learning rate for minimizing generator loss                              during training.   -dlr  --disc-lr FLOAT      Learning rate for minimizing discriminator loss                              during training.   -bz  --batch-size INTEGER  Mini batch size to use during training.   -ne  --num-epochs INTEGER  Number of epochs for training models.   -se  --save-every INTEGER  Epoch interval to save model checkpoints during                              training.   -tb  --tensorboard-vis     Flag for TensorBoard Visualization.   --help                     Show this message and exit. ```   """;Computer Vision;https://github.com/pskrunner14/face-DCGAN
"""|ResNet50v1+BN | 76.00% | 92.98% | --  | stepwise decay | -- |[PyTorch Vision]|   """;General;https://github.com/switchablenorms/Switchable-Normalization
"""Fully Convolutional Networks (FCNs) are a natural extension of CNNs to tackle per pixel prediction problems such as semantic image segmentation. FCNs add upsampling layers to standard CNNs to recover the spatial resolution of the input at the output layer. In  order to compensate for the resolution loss induced by pooling layers  FCNs introduce skip connections between their downsampling  and upsampling paths. Skip connections help the upsampling path recover fine-grained information from the downsampling layers.  One evolution of CNNs are [Residual Networks](https://arxiv.org/abs/1512.03385) (ResNets). ResNets are designed to ease the training of  very deep networks by introducing a residual block that sums the non-linear transformation of the input and its identity mapping.  The identity mapping is implemented by means of a shortcut connection. ResNets can be extended to work as FCNs. ResNets incorporate  shortcut paths to FCNs and increase the number of connections within a network. This additional shortcut paths improve the segmentation  accuracy and also help the network to converge faster.  Recently another CNN architecture called [DenseNet](https://arxiv.org/abs/1608.06993) has been introduced. DenseNets are built from  *dense blocks* and pooling operations  where each dense block is an iterative concatenation of previous feature maps. This architecture  can be seen as an extension of ResNets  which performs iterative summation of previous feature maps. The result of this modification  is that DenseNets are more efficient in there parameter usage.  The [https://arxiv.org/abs/1611.09326](https://arxiv.org/abs/1611.09326) paper extends DenseNets to work as FCNs by adding an upsampling  path to recover the full input resolution.    Clone Github repo with CamVid data  git clone https://github.com/mostafaizz/camvid.git       --train-path ./camvid-preprocessed/camvid-384x480-train.tfrecords \       --train-path ./camvid-preprocessed/camvid-384x480-train.tfrecords \   Here are the color encodings for the labels:  ![""LabelsColorKey""](images/LabelsColorKey.jpg?raw=true ""LabelsColorKey"")  The following examples show the original image  the true label map and the predicted label map:  ![""camvid-segmentation-1""](images/camvid-segmentation-1.png?raw=true ""camvid-segmentation-1"")  ![""camvid-segmentation-1""](images/camvid-segmentation-2.png?raw=true ""camvid-segmentation-2"")  ![""camvid-segmentation-3""](images/camvid-segmentation-3.png?raw=true ""camvid-segmentation-3"")  ![""camvid-segmentation-4""](images/camvid-segmentation-4.png?raw=true ""camvid-segmentation-4"")  ![""camvid-segmentation-5""](images/camvid-segmentation-5.png?raw=true ""camvid-segmentation-5"")   """;General;https://github.com/asprenger/keras_fc_densenet
"""Fully Convolutional Networks (FCNs) are a natural extension of CNNs to tackle per pixel prediction problems such as semantic image segmentation. FCNs add upsampling layers to standard CNNs to recover the spatial resolution of the input at the output layer. In  order to compensate for the resolution loss induced by pooling layers  FCNs introduce skip connections between their downsampling  and upsampling paths. Skip connections help the upsampling path recover fine-grained information from the downsampling layers.  One evolution of CNNs are [Residual Networks](https://arxiv.org/abs/1512.03385) (ResNets). ResNets are designed to ease the training of  very deep networks by introducing a residual block that sums the non-linear transformation of the input and its identity mapping.  The identity mapping is implemented by means of a shortcut connection. ResNets can be extended to work as FCNs. ResNets incorporate  shortcut paths to FCNs and increase the number of connections within a network. This additional shortcut paths improve the segmentation  accuracy and also help the network to converge faster.  Recently another CNN architecture called [DenseNet](https://arxiv.org/abs/1608.06993) has been introduced. DenseNets are built from  *dense blocks* and pooling operations  where each dense block is an iterative concatenation of previous feature maps. This architecture  can be seen as an extension of ResNets  which performs iterative summation of previous feature maps. The result of this modification  is that DenseNets are more efficient in there parameter usage.  The [https://arxiv.org/abs/1611.09326](https://arxiv.org/abs/1611.09326) paper extends DenseNets to work as FCNs by adding an upsampling  path to recover the full input resolution.    Clone Github repo with CamVid data  git clone https://github.com/mostafaizz/camvid.git       --train-path ./camvid-preprocessed/camvid-384x480-train.tfrecords \       --train-path ./camvid-preprocessed/camvid-384x480-train.tfrecords \   Here are the color encodings for the labels:  ![""LabelsColorKey""](images/LabelsColorKey.jpg?raw=true ""LabelsColorKey"")  The following examples show the original image  the true label map and the predicted label map:  ![""camvid-segmentation-1""](images/camvid-segmentation-1.png?raw=true ""camvid-segmentation-1"")  ![""camvid-segmentation-1""](images/camvid-segmentation-2.png?raw=true ""camvid-segmentation-2"")  ![""camvid-segmentation-3""](images/camvid-segmentation-3.png?raw=true ""camvid-segmentation-3"")  ![""camvid-segmentation-4""](images/camvid-segmentation-4.png?raw=true ""camvid-segmentation-4"")  ![""camvid-segmentation-5""](images/camvid-segmentation-5.png?raw=true ""camvid-segmentation-5"")   """;Computer Vision;https://github.com/asprenger/keras_fc_densenet
"""source devel/setup.bash   you can close everything now   """;Computer Vision;https://github.com/m1234d/build18
"""Adjust the batch size if out of memory (OOM) occurs. It dependes on your gpu memory size and genotype.  - Search  ```shell python search.py --name cifar10 --dataset cifar10 ```  - Augment  ```shell #: genotype from search results python augment.py --name cifar10 --dataset cifar10 --genotype ""Genotype(     normal=[[('sep_conv_3x3'  0)  ('dil_conv_5x5'  1)]  [('skip_connect'  0)  ('dil_conv_3x3'  2)]  [('sep_conv_3x3'  1)  ('skip_connect'  0)]  [('sep_conv_3x3'  1)  ('skip_connect'  0)]]      normal_concat=range(2  6)      reduce=[[('max_pool_3x3'  0)  ('max_pool_3x3'  1)]  [('max_pool_3x3'  0)  ('skip_connect'  2)]  [('skip_connect'  3)  ('max_pool_3x3'  0)]  [('skip_connect'  2)  ('max_pool_3x3'  0)]]      reduce_concat=range(2  6) )"" ```  - with docker  ```shell $ docker run --runtime=nvidia -it khanrc/pytorch-darts:0.2 bash  #: you can run directly also $ docker run --runtime=nvidia -it khanrc/pytorch-darts:0.2 python search.py --name cifar10 --dataset cifar10 ```   """;General;https://github.com/khanrc/pt.darts
"""A summary of the performance can be produced by invoking the following command from inside the ```my_project``` folder or ```predictions``` sub-folder:  ``` mp summary  >> [***] SUMMARY REPORT FOR FOLDER [***] >> ./my_project/predictions/csv/ >>  >>  >> Per class: >> -------------------------------- >>    Mean dice by class  +/- STD    min    max   N >> 1               0.856    0.060  0.672  0.912  34 >> 2               0.891    0.029  0.827  0.934  34 >> 3               0.888    0.027  0.829  0.930  34 >> 4               0.802    0.164  0.261  0.943  34 >> 5               0.819    0.075  0.552  0.926  34 >> 6               0.863    0.047  0.663  0.917  34 >>  >> Overall mean: 0.853 +- 0.088 >> -------------------------------- >>  >> By views: >> -------------------------------- >> [0.8477811  0.50449719 0.16355361]          0.825 >> [ 0.70659414 -0.35532932  0.6119361 ]       0.819 >> [ 0.11799461 -0.07137918  0.9904455 ]       0.772 >> [ 0.95572575 -0.28795306  0.06059151]       0.827 >> [-0.16704373 -0.96459936  0.20406974]       0.810 >> [-0.72188903  0.68418977  0.10373322]       0.819 >> -------------------------------- ```   In order to train a model to solve a specific task  a set of manually  annotated images must be stored in a folder under the following structure:  ``` ./data_folder/ |- train/ |--- images/ |------ image1.nii.gz |------ image5.nii.gz |--- labels/ |------ image1.nii.gz |------ image5.nii.gz |- val/ |--- images/ |--- labels/ |- test/ |--- images/ |--- labels/ |- aug/ <-- OPTIONAL |--- images/ |--- labels/ ```  The names of these folders may be customized in the parameter file (see below)   but default to those shown above. The image and corresponding label map files  must be identically named.  The ```aug``` folder may store additional images that can be included during  training with a lower weight assigned in optimization.   ``` #: From GitHub git clone https://github.com/perslev/MultiPlanarUNet pip install -e MultiPlanarUNet ```  This package is still frequently updated and it is thus recommended to install  the package with PIP with the -e ('editable') flag so that the package can be  updated with recent changes on GitHub without re-installing:  ``` cd MultiPlanarUNet git pull ```  However  the package is also occasionally updated on PyPi for install with:  ``` #: Note: renamed MultiPlanarUNet -> mpunet in versions 0.2.4 pip install mpunet ```   mp init_project --name my_project --data_dir ./data_folder   Here  we prepare for a 5-CV setup. By default  the above command will create a   ``` usage: mp [script] [script args...]  Multi-Planar UNet (0.1.0) ------------------------- Available scripts: - cv_experiment - cv_split - init_project - predict - predict_3D - summary - train - train_fusion ... ```   Project initialization  model training  evaluation  prediction etc. can be  performed using the scripts located in ```MultiPlanarUNet.bin```. The script  named ```mp.py``` serves as an entry point to all other scripts  and it is used as follows:  ```bash #: Invoke the help menu mp --help  #: Launch the train script mp train [arguments passed to 'train'...]  #: Invoke the help menu of a sub-script mp train --help ```  You only need to specify the training data in the format described  below. Training  evaluation and prediction will be handled automatically if  using the above scripts.   """;Computer Vision;https://github.com/perslev/MultiPlanarUNet
""" **Training:**  `python dcgan.py --mode train --batch_size <batch_size>`    python dcgan.py --mode train --path ~/images --batch_size 128  **Image generation:** `python dcgan.py --mode generate --batch_size <batch_size>`  `python dcgan.py --mode generate --batch_size <batch_size> --nice` : top 5% images according to discriminator  python dcgan.py --mode generate --batch_size 128 ---   """;Computer Vision;https://github.com/mehdidc/hybrid-keras
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/ccie29441/Yolo-v4-and-Yolo-v3-v2-for-Windows-and-Linux
"""If you are running experiments on MLQA  it is important that you clearly state your experimental settings.    As mentioned in the paper  some instances that cannot be answered are generated by our annotation procedure. We will release these as a separate resource shortly here.    """;Natural Language Processing;https://github.com/facebookresearch/MLQA
"""* First  download a dataset  e.g. apple2orange  ```bash $ bash download_dataset.sh apple2orange ```  * Write the dataset to tfrecords  ```bash $ python3 build_data.py ```  Check `$ python3 build_data.py --help` for more details.   Python 3.6.0   My pretrained models are available at https://github.com/vanhuyz/CycleGAN-TensorFlow/releases   """;Computer Vision;https://github.com/WeiYangze/hibernate-demo
"""* First  download a dataset  e.g. apple2orange  ```bash $ bash download_dataset.sh apple2orange ```  * Write the dataset to tfrecords  ```bash $ python3 build_data.py ```  Check `$ python3 build_data.py --help` for more details.   Python 3.6.0   My pretrained models are available at https://github.com/vanhuyz/CycleGAN-TensorFlow/releases   """;General;https://github.com/WeiYangze/hibernate-demo
"""you can see how i choose hyperparameters below   * Jupyter notebook: [link](https://nbviewer.jupyter.org/github/simonjisu/NMT/blob/master/Neural_Machine_Translation_Tutorial.ipynb) * Preparing for demo   For 'HELP' please insert argument behind `main.py -h`. or you can just run   ``` $ cd model $ sh runtrain.sh ```   """;Natural Language Processing;https://github.com/simonjisu/NMT
"""""";Computer Vision;https://github.com/jiajunhua/dcgan_code
"""If you'd like to submit a pull request  you'll need to clone the repository;   """;Graphs;https://github.com/google-research/google-research
"""""";Computer Vision;https://github.com/gpandu/Object-detection
"""ËØ≠Ë®ÄÔºöPython 3.7  Ê°ÜÊû∂ÔºöPyTorch 1.2   """;Computer Vision;https://github.com/lyffly/Segmentation_with_my_dataset
"""Based on PyTorch   - Pytorch 1.3.0   """;General;https://github.com/Data-drone/attention_augmented_cnn
"""""";Computer Vision;https://github.com/TuSimple/resnet.mxnet
"""Large-Scale Learnable Graph Convolutional Networks provide an efficient way (LGCL and LGCN) for learnable graph convolution.  Detailed information about LGCL and LGCN is provided in (https://dl.acm.org/citation.cfm?id=3219947).   After configure the network  we can start to train. Run ``` python main.py ``` The training results on Cora dataset will be displayed.    """;Graphs;https://github.com/divelab/lgcn
"""Code adapted from https://github.com/chrischute/glow Adding conditioning layer to affine coupling layer. Tried conditioning for many domain. Applied style transfer using the property of conditional flow model. (reconstruct image giving different condition in forward and reverse procedure of Glow)    """;Computer Vision;https://github.com/5yearsKim/Conditional-Normalizing-Flow
"""Code adapted from https://github.com/chrischute/glow Adding conditioning layer to affine coupling layer. Tried conditioning for many domain. Applied style transfer using the property of conditional flow model. (reconstruct image giving different condition in forward and reverse procedure of Glow)    """;General;https://github.com/5yearsKim/Conditional-Normalizing-Flow
"""The model has been developed using PyTorch library. The Pytorch library is available over the main page: https://pytorch.org/   conda install pytorch torchvision -c pytorch  In addition to PyTorch  in this repository has been used also Numpy. Numpy is already installed in Anaconda  otherwise you can use:   PyTorch   Numpy    To set up your python environment to run the code in this repository  follow the instructions below.  1. Create (and activate) a new environment with Python 3.6.      - __Linux__ or __Mac__:      ```bash     conda create --name drlnd python=3.6     source activate drlnd     ```     - __Windows__:      ```bash     conda create --name drlnd python=3.6      activate drlnd     ```      2. Follow the instructions in [this repository](https://github.com/openai/gym) to perform a minimal install of OpenAI gym.       - Next  install the **classic control** environment group by following the instructions [here](https://github.com/openai/gym#classic-control).     - Then  install the **box2d** environment group by following the instructions [here](https://github.com/openai/gym#box2d).      3. Clone the repository (if you haven't already!)  and navigate to the `python/` folder.  Then  install several dependencies.  ```bash git clone https://github.com/udacity/deep-reinforcement-learning.git cd deep-reinforcement-learning/python pip install . ``` 4. Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment.   ```bash python -m ipykernel install --user --name drlnd --display-name ""drlnd"" ```  5. Before running code in a notebook  change the kernel to match the `drlnd` environment by using the drop-down `Kernel` menu.    """;Reinforcement Learning;https://github.com/IvanVigor/MADDPG-Unity
"""To install requirements:  pip install -r requirements.txt   """;Computer Vision;https://github.com/podgorskiy/StyleGan
"""The Jupyter Notebooks are self explanatory    """;Computer Vision;https://github.com/vaisakh-shaj/DeepLearning
"""""";General;https://github.com/alphadl/lookahead.pytorch
"""![FeatureVis](assets/FeatureVis.png)  In this work  we design a new loss function which merges the merits of both [NormFace](https://github.com/happynear/NormFace) and [SphereFace](https://github.com/wy1iu/sphereface). It is much easier to understand and train  and outperforms the previous state-of-the-art loss function (SphereFace) by 2-5% on MegaFace.    Requirements: My Caffe version https://github.com/happynear/caffe-windows. This version can also be compiled in Linux.   """;General;https://github.com/happynear/AMSoftmax
"""‰ª£Á†ÅÔºàjupyter notebook‰∏éÂÖ∂ÂØºÂá∫ÁöÑhtmlÊñá‰ª∂Ôºâ   """;Computer Vision;https://github.com/udacity/MLND-CN-Capstone-TGSImage
"""You can resolve this issue by using sys.setrecursionlimit(max_recursion_depth). You should increase max_recursion_depth until you get no error (Increasing this value might cause segmentation fault if you don't have enough memory).   1. Install [Theano](https://github.com/Theano/Theano) following its instruction. 2. Install [Keras](https://github.com/fchollet/keras) (I use new API at `keras-1` branch)  ``` $ cd keras $ git checkout keras-1 $ python setup.py install ```  3. Just run `python train.py`    """;General;https://github.com/dblN/stochastic_depth_keras
"""""";Computer Vision;https://github.com/simongrest/kaggle-freesound-audio-tagging-2019
"""""";General;https://github.com/JasonPlawinski/SuperResolution
"""""";Computer Vision;https://github.com/JasonPlawinski/SuperResolution
"""""";Reinforcement Learning;https://github.com/wtingda/DeepRLBreakout
"""""";Computer Vision;https://github.com/moelgendy/mobileNetV2
"""""";General;https://github.com/moelgendy/mobileNetV2
"""We have included a few sample MRI scans (including volumes and segmentations) in the `data/` folder. If you wish to use the datasets mentioned in the paper  you should download them directly from the respective dataset sites.   If you wish to use your own dataset  place your volume and segmentation files in the `data/` folder.  The data loading code in `src/mri_loader.py` expects each example to be stored as a volume file `{example_id}_vol.npz` and   if applicable  a corresponding `{example_id}_seg.npz` file  with the data stored in each file using the keys `vol_data`  and `seg_data` respectively. The functions `load_dataset_files` and `load_vol_and_seg` in `src/mri_loader.py` can be easily  modified to suit your data format.     This repo does not include any pre-trained models. You may train your own    You may train a segmentation model by specifying the GPU ID and dataset name.   """;Computer Vision;https://github.com/xamyzhao/brainstorm
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;General;https://github.com/vantupham/darknet
"""The pre-trained vgg_16.ckpt could be downloaded from http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz   """;Computer Vision;https://github.com/Stick-To/SSD-tensorflow
"""All unit tests in baselines can be run using pytest runner: ``` pip install pytest pytest-cov make pytest ```   Install the Stable Baselines package: ``` pip install stable-baselines[mpi] ```  This includes an optional dependency on MPI  enabling algorithms DDPG  GAIL  PPO1 and TRPO. If you do not need these algorithms  you can install without MPI: ``` pip install stable-baselines ```  Please read the [documentation](https://stable-baselines.readthedocs.io/) for more details and alternatives (from source  using docker).    **Note:** Stable-Baselines supports Tensorflow versions from 1.8.0 to 1.14.0. Support for Tensorflow 2 API is planned.   <sup><sup>(5): EDIT: you did it OpenAI! :cat:</sup></sup><br>   Github repo: https://github.com/araffin/rl-baselines-zoo   sudo apt-get update &amp;&amp; sudo apt-get install cmake libopenmpi-dev python3-dev zlib1g-dev  Installation of system packages on Mac requires Homebrew. With Homebrew installed  run the following:  brew install cmake openmpi  To install stable-baselines on Windows  please look at the documentation.   Most of the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms.  Here is a quick example of how to train and run PPO2 on a cartpole environment: ```python import gym  from stable_baselines.common.policies import MlpPolicy from stable_baselines.common.vec_env import DummyVecEnv from stable_baselines import PPO2  env = gym.make('CartPole-v1') #: Optional: PPO2 requires a vectorized environment to run #: the env is now wrapped automatically when passing it to the constructor #: env = DummyVecEnv([lambda: env])  model = PPO2(MlpPolicy  env  verbose=1) model.learn(total_timesteps=10000)  obs = env.reset() for i in range(1000):     action  _states = model.predict(obs)     obs  rewards  dones  info = env.step(action)     env.render()  env.close() ```  Or just train a model with a one liner if [the environment is registered in Gym](https://github.com/openai/gym/wiki/Environments) and if [the policy is registered](https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html):  ```python from stable_baselines import PPO2  model = PPO2('MlpPolicy'  'CartPole-v1').learn(10000) ```  Please read the [documentation](https://stable-baselines.readthedocs.io/) for more examples.    """;Reinforcement Learning;https://github.com/hill-a/stable-baselines
""" ```shell python3 -m pip install yolov4 ```   """;Computer Vision;https://github.com/hhk7734/tensorflow-yolov4
"""Word2Vec is a popular word embedding model which is used in major NLP models. Instead of representing every word in the document as one-hot vector  we learn the representation of word in form of an embedding based on the context and semantics around it  For more information  please follow https://arxiv.org/pdf/1301.3781.pdf    """;Natural Language Processing;https://github.com/rohith2506/word_embeddings
"""conda create -n py27 python=2.7  source activate py27  conda install pytorch=0.1.12 -c soumith  conda install torchvision   """;General;https://github.com/mr-bulb/DL_basic_github
"""This is a command-line tool  you just need to use `pip` to install the package  then  you will be able to use the command `getpaper` in your terminal.  -   Pre-requisite: `Python 3`  ```bash pip install arxiv-dl ```   Source Code Links   Configure the desired download destination via environment variables.   Let's say you want your papers get downloaded into ~/Documents/Papers.   Set the environment variable ARXIV_DOWNLOAD_FOLDER to your desired location.      bash   ```bash $ getpaper ""URL or ID"" ```  Example:  ```bash $ getpaper 1512.03385  #: OR $ getpaper https://arxiv.org/abs/1512.03385  #: OR $ getpaper https://arxiv.org/pdf/1512.03385.pdf ```  Deprecating Commands:  -   `add-paper` -   `dl-paper`   """;General;https://github.com/MarkHershey/arxiv-dl
"""This is a command-line tool  you just need to use `pip` to install the package  then  you will be able to use the command `getpaper` in your terminal.  -   Pre-requisite: `Python 3`  ```bash pip install arxiv-dl ```   Source Code Links   Configure the desired download destination via environment variables.   Let's say you want your papers get downloaded into ~/Documents/Papers.   Set the environment variable ARXIV_DOWNLOAD_FOLDER to your desired location.      bash   ```bash $ getpaper ""URL or ID"" ```  Example:  ```bash $ getpaper 1512.03385  #: OR $ getpaper https://arxiv.org/abs/1512.03385  #: OR $ getpaper https://arxiv.org/pdf/1512.03385.pdf ```  Deprecating Commands:  -   `add-paper` -   `dl-paper`   """;Computer Vision;https://github.com/MarkHershey/arxiv-dl
"""""";Computer Vision;https://github.com/vharaymonten/FaceGenerationWithGAN
"""""";General;https://github.com/comicencyclo/TransferLearning_DiscriminativeFineTuning
"""""";Natural Language Processing;https://github.com/comicencyclo/TransferLearning_DiscriminativeFineTuning
"""1) Clone this repo  2) Install the required packages:  ``` apt-get install tk-dev python-tk ```  3) Install the python packages: 	 ``` pip install pandas pip install pycocotools pip install opencv-python pip install requests  ```   python train.py --dataset coco --coco_path ../coco --depth 50   ![img1](https://github.com/yhenon/pytorch-retinanet/blob/master/images/1.jpg) ![img2](https://github.com/yhenon/pytorch-retinanet/blob/master/images/2.jpg) ![img4](https://github.com/yhenon/pytorch-retinanet/blob/master/images/4.jpg) ![img6](https://github.com/yhenon/pytorch-retinanet/blob/master/images/6.jpg) ![img7](https://github.com/yhenon/pytorch-retinanet/blob/master/images/7.jpg) ![img8](https://github.com/yhenon/pytorch-retinanet/blob/master/images/8.jpg)  """;Computer Vision;https://github.com/yhenon/pytorch-retinanet
"""1) Clone this repo  2) Install the required packages:  ``` apt-get install tk-dev python-tk ```  3) Install the python packages: 	 ``` pip install pandas pip install pycocotools pip install opencv-python pip install requests  ```   python train.py --dataset coco --coco_path ../coco --depth 50   ![img1](https://github.com/yhenon/pytorch-retinanet/blob/master/images/1.jpg) ![img2](https://github.com/yhenon/pytorch-retinanet/blob/master/images/2.jpg) ![img4](https://github.com/yhenon/pytorch-retinanet/blob/master/images/4.jpg) ![img6](https://github.com/yhenon/pytorch-retinanet/blob/master/images/6.jpg) ![img7](https://github.com/yhenon/pytorch-retinanet/blob/master/images/7.jpg) ![img8](https://github.com/yhenon/pytorch-retinanet/blob/master/images/8.jpg)  """;General;https://github.com/yhenon/pytorch-retinanet
"""./fairseq/scripts/aw/preprocess.sh preprocess.   Then to generate  use command ./scripts/gen_qg.sh.    To compute QAGS scores  we need to  1. generate questions 2. answer questions 3. compare answers    """;Sequential;https://github.com/W4ngatang/qags
"""PyTorch implementation of Twin Delayed DDPG (TD3) tested on the following environments:   - To test a preTrained network : run `test.py`  - To train a new network : run `train.py`   """;General;https://github.com/nikhilbarhate99/TD3-PyTorch-BipedalWalker-v2
"""PyTorch implementation of Twin Delayed DDPG (TD3) tested on the following environments:   - To test a preTrained network : run `test.py`  - To train a new network : run `train.py`   """;Reinforcement Learning;https://github.com/nikhilbarhate99/TD3-PyTorch-BipedalWalker-v2
"""""";General;https://github.com/wtingda/DeepRLBreakout
"""Acrobot from GYM : https://gym.openai.com/   """;Reinforcement Learning;https://github.com/Gouet/Acrobot-PPO
"""Due to the speed limitation of 20 FPS  we started with [YOLOv2-Tiny detector](https://pjreddie.com/darknet/yolov2/)  which consists of a backbone network for feature extraction and a detection network for candidate bounding box generation. Considering that there is no need to classify in our task  we reduced the detection network to a location network  in which a candidate bounding box is only represented by a confidence socre and a position.  However  with such a simple model  we were soon faced with the challenges of tiny objects  occlusions and distractions from the provided data set. In order to tackle to the aforementioned challenges  we investigated various network architectures for both training and inference.   <p align=""center""> <img src=""https://raw.githubusercontent.com/jndeng/DACSDC-DeepZ/master/Train/cfg/architecture.png"" alt=""network architecture"" width=""380px"" height=""400px""> </p>  We later combined [Feature Pyramid Network](https://arxiv.org/abs/1612.03144v2) to fuse fine-grained features with strong semantic features to enhance the ability in detecting small objects. Meanwhile  we utilized [Focal Loss](https://arxiv.org/abs/1708.02002) function to mitigate the imbalance between the single ground truth box and the candidate boxes at training phase  thereby partially resolving occlusions and distractions. With the combined techniques  we achieved the inference network as shown in the figure with an accuracy improvement of ~ 0.042.   Moreover  we used multithreading to accelerate the process of prediction by loading images and infering in parallel  which improved about 7 FPS on NVIDIA Jetson TX2.   The performance of our model is as follow:  | Self-Test Accuracy (mean IoU) | Organizer-Test Accuracy (mean IoU) | Speed (FPS on Jetson TX2) |:-----:|:-----:|:-----:| | 0.866 | 0.691 | ~25 |  **Note:**    We develop two projects for different purposes in this repository. Project `Train` is mainly used for model training and accuracy evaluation on powerful GPU(NVIDIA Titan X Pascal in our experiments). While project `Inference` is dedicated to inference on embedded GPU(NVIDIA Jetson TX2) with better optimization in speed and energy consumption.    ~~1. Download the raw dataset [dac_origin.tar (6.3GB)]() (about 100 000 images and the corresponding labels) and unzip it to `$TRAIN_ROOT/data/raw_dataset`.~~ 1. Download the official dataset  unzip it  rename and move the folder contains all subclass folders to `$TRAIN_ROOT/data/raw_dataset`. 2. Use the raw dataset `$TRAIN_ROOT/data/raw_dataset` to generate the proper dataset `$TRAIN_ROOT/data/train_dataset` for training. The entire process of dataset generation takes about 14GB of hard disk space  and the raw dataset will no longer be needed once we obtain `$TRAIN_ROOT/data/train_dataset`. ```Shell cd $TRAIN_ROOT/data/script python generate_dataset.py ``` 3. Randomly divide the entire dataset into two disjoint parts: training set and validation set according to 8:2 ratio. The result of division will be stored in `$TRAIN_ROOT/data/dataset` as the meta files. You can make a new division by yourself  or just apply the pre-divided dataset used in our experiments. ```Shell #: Make a new division cd $TRAIN_ROOT/data/script python divide_dataset_randomly.py ``` ```Shell #: Use a pre-divided dataset cd $TRAIN_ROOT/data/script python divide_dataset.py ```   *Prerequisites:*  * OpenCV  * CUDA/cuDNN  * Python2/Python2-Numpy  *Project download and installation:* 1. Download the source code on the appropriate devices respectively. Project `Train` is recommended using on device with powerful GPU. While project `Inference` should be used on NVIDIA Jetson TX2 in order to make a fair evaluation of speed. ```Shell #: You may use this command twice to download the source code on different devices git clone https://github.com/jndeng/DACSDC-DeepZ.git ``` 2. Build the source code of two projects separately on the corresponding device. We will use `$TRAIN_ROOT` and `$INFERENCE_ROOT` to call the directory of project `Train` and project `Inference` respectively. ```Shell #: For project 'Train' cd $TRAIN_ROOT make -j8 ``` ```Shell #: For project 'Inference' cd $INFERENCE_ROOT make -j8 ```  **Note:** 1. Our implementation is based on [Darknet framework](https://pjreddie.com/darknet/). You can also refer to the [installation guide](https://pjreddie.com/darknet/install/) of the original Darknet framework. 2. For convenience  we only implement the code for **single GPU mode**  which means **CPU mode** and **multiple GPUs mode** are not supported in both of our projects.    cd $TRAIN_ROOT/script  bash train_model.sh   cd $TRAIN_ROOT/script  bash valid_model.sh   cd $INFERENCE_ROOT/script   """;Computer Vision;https://github.com/jndeng/DACSDC-DeepZ
"""Due to the speed limitation of 20 FPS  we started with [YOLOv2-Tiny detector](https://pjreddie.com/darknet/yolov2/)  which consists of a backbone network for feature extraction and a detection network for candidate bounding box generation. Considering that there is no need to classify in our task  we reduced the detection network to a location network  in which a candidate bounding box is only represented by a confidence socre and a position.  However  with such a simple model  we were soon faced with the challenges of tiny objects  occlusions and distractions from the provided data set. In order to tackle to the aforementioned challenges  we investigated various network architectures for both training and inference.   <p align=""center""> <img src=""https://raw.githubusercontent.com/jndeng/DACSDC-DeepZ/master/Train/cfg/architecture.png"" alt=""network architecture"" width=""380px"" height=""400px""> </p>  We later combined [Feature Pyramid Network](https://arxiv.org/abs/1612.03144v2) to fuse fine-grained features with strong semantic features to enhance the ability in detecting small objects. Meanwhile  we utilized [Focal Loss](https://arxiv.org/abs/1708.02002) function to mitigate the imbalance between the single ground truth box and the candidate boxes at training phase  thereby partially resolving occlusions and distractions. With the combined techniques  we achieved the inference network as shown in the figure with an accuracy improvement of ~ 0.042.   Moreover  we used multithreading to accelerate the process of prediction by loading images and infering in parallel  which improved about 7 FPS on NVIDIA Jetson TX2.   The performance of our model is as follow:  | Self-Test Accuracy (mean IoU) | Organizer-Test Accuracy (mean IoU) | Speed (FPS on Jetson TX2) |:-----:|:-----:|:-----:| | 0.866 | 0.691 | ~25 |  **Note:**    We develop two projects for different purposes in this repository. Project `Train` is mainly used for model training and accuracy evaluation on powerful GPU(NVIDIA Titan X Pascal in our experiments). While project `Inference` is dedicated to inference on embedded GPU(NVIDIA Jetson TX2) with better optimization in speed and energy consumption.    ~~1. Download the raw dataset [dac_origin.tar (6.3GB)]() (about 100 000 images and the corresponding labels) and unzip it to `$TRAIN_ROOT/data/raw_dataset`.~~ 1. Download the official dataset  unzip it  rename and move the folder contains all subclass folders to `$TRAIN_ROOT/data/raw_dataset`. 2. Use the raw dataset `$TRAIN_ROOT/data/raw_dataset` to generate the proper dataset `$TRAIN_ROOT/data/train_dataset` for training. The entire process of dataset generation takes about 14GB of hard disk space  and the raw dataset will no longer be needed once we obtain `$TRAIN_ROOT/data/train_dataset`. ```Shell cd $TRAIN_ROOT/data/script python generate_dataset.py ``` 3. Randomly divide the entire dataset into two disjoint parts: training set and validation set according to 8:2 ratio. The result of division will be stored in `$TRAIN_ROOT/data/dataset` as the meta files. You can make a new division by yourself  or just apply the pre-divided dataset used in our experiments. ```Shell #: Make a new division cd $TRAIN_ROOT/data/script python divide_dataset_randomly.py ``` ```Shell #: Use a pre-divided dataset cd $TRAIN_ROOT/data/script python divide_dataset.py ```   *Prerequisites:*  * OpenCV  * CUDA/cuDNN  * Python2/Python2-Numpy  *Project download and installation:* 1. Download the source code on the appropriate devices respectively. Project `Train` is recommended using on device with powerful GPU. While project `Inference` should be used on NVIDIA Jetson TX2 in order to make a fair evaluation of speed. ```Shell #: You may use this command twice to download the source code on different devices git clone https://github.com/jndeng/DACSDC-DeepZ.git ``` 2. Build the source code of two projects separately on the corresponding device. We will use `$TRAIN_ROOT` and `$INFERENCE_ROOT` to call the directory of project `Train` and project `Inference` respectively. ```Shell #: For project 'Train' cd $TRAIN_ROOT make -j8 ``` ```Shell #: For project 'Inference' cd $INFERENCE_ROOT make -j8 ```  **Note:** 1. Our implementation is based on [Darknet framework](https://pjreddie.com/darknet/). You can also refer to the [installation guide](https://pjreddie.com/darknet/install/) of the original Darknet framework. 2. For convenience  we only implement the code for **single GPU mode**  which means **CPU mode** and **multiple GPUs mode** are not supported in both of our projects.    cd $TRAIN_ROOT/script  bash train_model.sh   cd $TRAIN_ROOT/script  bash valid_model.sh   cd $INFERENCE_ROOT/script   """;General;https://github.com/jndeng/DACSDC-DeepZ
"""Uses Unity-ML Banana Navigation environment: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#banana-collector   Extract the Banana_Windows_x86_64 folder. All code is contained in the ipynb notebook.   """;Reinforcement Learning;https://github.com/wmol4/Pytorch_DDQN_Unity_Navigation
"""[Remote sensing](https://www.usgs.gov/faqs/what-remote-sensing-and-what-it-used) is the science of obtaining information about objects or areas from a distance  typically from aircraft or satellites.  We realized the problem of satellite image classification as a [semantic segmentation](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) problem and built semantic segmentation algorithms in deep learning to tackle this.   We are providing some reasonably good pre-trained weights here so that the users don't need to train from scratch.   Clone the repository  change your present working directory to the cloned directory. Create folders with names `train_predictions` and `test_outputs` to save model predicted outputs on training and testing images (Not required now as the repo already contains these folders)  ``` $ git clone https://github.com/manideep2510/eye-in-the-sky.git $ cd eye-in-the-sky $ mkdir train_predictions $ mkdir test_outputs ```  For training the U-Net model and saving weights  run the below command  ``` $ python3 main_unet.py ```  To test the U-Net model  calculating accuracies  calculating confusion matrices for training and validation and saving predictions by the model on training  validation and testing images.  ``` $ python3 test_unet.py ```   """;Computer Vision;https://github.com/manideep2510/eye-in-the-sky
"""* Create separate virtual environment for the project using the provided `environment.yml` file ``` conda env create -f environment.yml conda activate navigation ```   Banana Collector Unity Environment   Environment Setup   Pytorch 1.0   ![trained agent](images/trained_agent.gif)   1. Clone the repository (if you haven't already!) ```bash git clone https://github.com/1jsingh/rl_navigation.git cd rl_navigation ```  2. Download the environment from one of the links below.  You need only select the environment that matches your operating system:           - Linux: [click here](https://drive.google.com/open?id=1hbezVc5oOthoQ2VF9c4RPWxsf5M8mxEh)     - Mac OSX: [click here](https://drive.google.com/open?id=1HTvJxRA24bJKsyzzfy3-J7eOo8XJYpF1N)      (_For AWS_) If you'd like to train the agent on AWS (and have not [enabled a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md))  then please use [this link](https://drive.google.com/open?id=1BpLCYfGcp7y5WPAPPmxxe0mVcYM1LG9N) to obtain the ""headless"" version of the environment.  You will **not** be able to watch the agent without enabling a virtual screen  but you will be able to train the agent.  (_To watch the agent  you should follow the instructions to [enable a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md)  and then download the environment for the **Linux** operating system above._)         3. Place the downloaded file in the `unity_envs` directory and unzip it.   ```   mkdir unity_envs && cd unity_envs   unzip Banana_Collector_Linux.zip   ```  4. Follow along with `Navigation.ipynb` to train your own RL agent.   """;Reinforcement Learning;https://github.com/1jsingh/rl_navigation
"""Build with NDK-17b   ./gradlew build  adb install app/build/outputs/apk/release/app-release.apk           runtime: cpu+gpu       bash      git clone https://github.com/XiaoMi/mace      cd mace      git checkout v0.11.0-rc1       sh   """;Computer Vision;https://github.com/nolanliou/PeopleSegmentationDemo
"""Build with NDK-17b   ./gradlew build  adb install app/build/outputs/apk/release/app-release.apk           runtime: cpu+gpu       bash      git clone https://github.com/XiaoMi/mace      cd mace      git checkout v0.11.0-rc1       sh   """;General;https://github.com/nolanliou/PeopleSegmentationDemo
"""""";Computer Vision;https://github.com/Xnsam/clothing_classification
"""``` #: [NOTE]: Install one by one package (tensorflow_cpu) seedotech@tensorflow:~/dev/tensorflow/object-detection/research/object_detection$ conda install -c anaconda protobuf (tensorflow_cpu) seedotech@tensorflow:~/dev/tensorflow/object-detection/research/object_detection$ pip install pillow (tensorflow_cpu) seedotech@tensorflow:~/dev/tensorflow/object-detection/research/object_detection$ pip install lxml (tensorflow_cpu) seedotech@tensorflow:~/dev/tensorflow/object-detection/research/object_detection$ pip install Cython (tensorflow_cpu) seedotech@tensorflow:~/dev/tensorflow/object-detection/research/object_detection$ pip install jupyter (tensorflow_cpu) seedotech@tensorflow:~/dev/tensorflow/object-detection/research/object_detection$ pip install matplotlib (tensorflow_cpu) seedotech@tensorflow:~/dev/tensorflow/object-detection/research/object_detection$ pip install pandas (tensorflow_cpu) seedotech@tensorflow:~/dev/tensorflow/object-detection/research/object_detection$ pip install opencv-python (tensorflow_cpu) seedotech@tensorflow:~/dev/tensorflow/object-detection/research/object_detection$ pip install pycocotools  #: Upgrade the tensorflow. It works with tensorflow 1.13.0 (tensorflow_cpu) seedotech@tensorflow:~/dev/tensorflow/object-detection/research/object_detection$ pip install --upgrade tensorflow  $ pip list | grep tensorflow	 	tensorflow           1.13.1   	tensorflow-estimator 1.13.0   ```   ``` #: Download the latest Anaconda $ wget https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh  $ chmod a+x Anaconda3-2019.03-Linux-x86_64.sh  #: Install the Anaconda  $ ./Anaconda3-2019.03-Linux-x86_64.sh  #: Activating the Installation $ source ~/.bashrc  #: Install essential development packages $ sudo apt install build-essential ```   $ git clone https://github.com/tensorflow/models.git tensorflow_models  $ git clone https://github.com/raycad/object-detection.git   $ conda create -n tensorflow_cpu pip python=3.6  : Activate the newly created virtual environment  $ conda activate tensorflow_cpu  : Deactivate created virtual environment  $ conda deactivate   Download the model from http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz ``` (tensorflow_cpu) seedotech@tensorflow:~/dev/tensorflow/object-detection/research/object_detection$ wget http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz #: Extract the faster_rcnn_inception_v2_coco_2018_01_28 folder to the ""object_detection"" folder (tensorflow_cpu) seedotech@tensorflow:~/dev/tensorflow/object-detection/research/object_detection$ tar -zxvf faster_rcnn_inception_v2_coco_2018_01_28.tar.gz  #: Create Label Map and Configure Training #: Update information of the files in the ""object-detection/research/object_detection/training"" folder. Note to set the absolute path for ""fine_tune_checkpoint""  ""train_input_reader"" and ""eval_input_reader""  Line 9. Change num_classes to the number of different objects you want the classifier to detect. For the above basketball  shirt  and shoe detector  it would be num_classes : 3 .  fine_tune_checkpoint: ""/home/seedotech/dev/tensorflow/object-detection/research/object_detection/faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt""  train_input_reader: {   tf_record_input_reader {     input_path: ""/home/seedotech/dev/tensorflow/object-detection/research/object_detection/train.record""   }   label_map_path: ""/home/seedotech/dev/tensorflow/object-detection/research/object_detection/training/labelmap.pbtxt"" }  eval_config: {   #: Change num_examples to the number of images you have in the /images/test directory.   num_examples: 16   #: Note: The below line limits the evaluation process to 10 evaluations.   #: Remove the below line to evaluate indefinitely.   max_evals: 10 }  eval_input_reader: {   tf_record_input_reader {     input_path: ""/home/seedotech/dev/tensorflow/object-detection/research/object_detection/test.record""   }   label_map_path: ""/home/seedotech/dev/tensorflow/object-detection/research/object_detection/training/labelmap.pbtxt""   shuffle: false   num_readers: 1 } ```   Edit the **line 51** in the **object_detection_image.py** file to the number of classes the object detector can identify  ``` $ python object_detection_image.py ```  ![Result](./doc/ret1.png)   """;Computer Vision;https://github.com/raycad/object-detection
"""""";Computer Vision;https://github.com/ardapekis/pixel-rnn
"""    brew install boost cmake zlib           sudo apt install libboost-dev libboost-program-options-dev libboost-filesystem-dev opencl-headers ocl-icd-libopencl1 ocl-icd-opencl-dev zlib1g-dev       You need a PC with a GPU  i.e. a discrete graphics card made by NVIDIA or AMD    Follow the instructions below to compile the leelaz and autogtp binaries in   contributing instructions below.  Contributing will start when you run autogtp.   If you are on macOS  Leela Zero is available through Homebrew  the de facto standard  package manager. You can install it with:  brew install leela-zero  If you are on Unix  you have to compile the program yourself. Follow   git clone https://github.com/leela-zero/leela-zero  cd leela-zero  git submodule update --init --recursive   cmake --build .   git clone https://github.com/leela-zero/leela-zero  cd leela-zero  git submodule update --init --recursive   cmake --build .   git clone https://github.com/leela-zero/leela-zero  cd leela-zero  git submodule update --init --recursive  cd msvc   to the Visual Studio version you have.  For Windows  you can use a release package  see ""I want to help"".   This requires a working installation of TensorFlow 1.4 or later:   [ ] Improve GPU batching in the search.   [ ] CUDA specific version using cuDNN or cuBLAS.            mkdir build && cd build                mkdir build && cd build                ./autogtp/autogtp    Leela Zero is not meant to be used directly. You need a graphical interface for it  which will interface with Leela Zero through the GTP protocol.  The engine supports the [GTP protocol  version 2](https://www.lysator.liu.se/~gunnar/gtp/gtp2-spec-draft2/gtp2-spec.html).  [Lizzie](https://github.com/featurecat/lizzie/releases) is a client specifically for Leela Zero which shows live search probilities  a win rate graph  and has an automatic game analysis mode. Has binaries for Windows  Mac  and Linux.  [Sabaki](http://sabaki.yichuanshen.de/) is a very nice looking GUI with GTP 2 capability.  [LeelaSabaki](https://github.com/SabakiHQ/LeelaSabaki) is modified to show variations and winning statistics in the game tree  as well as a heatmap on the game board.  [GoReviewPartner](https://github.com/pnprog/goreviewpartner) is a tool for automated review and analysis of games using bots (saved as .rsgf files)  Leela Zero is supported.  A lot of go software can interface to an engine via GTP  so look around.  Add the --gtp commandline option on the engine command line to enable Leela Zero's GTP support. You will need a weights file  specify that with the -w option.  All required commands are supported  as well as the tournament subset  and ""loadsgf"". The full set can be seen with ""list_commands"". The time control can be specified over GTP via the time\_settings command. The kgs-time\_settings extension is also supported. These have to be supplied by the GTP 2 interface  not via the command line!   At the end of the game  you can send Leela Zero a ""dump\_training"" command  followed by the winner of the game (either ""white"" or ""black"") and a filename  e.g:      dump_training white train.txt  This will save (append) the training data to disk  in the format described below  and compressed with gzip.  Training data is reset on a new game.   """;Reinforcement Learning;https://github.com/intenseG/BSK
"""```bash $ pip install slot_attention ```   ```python import torch from slot_attention import SlotAttention  slot_attn = SlotAttention(     num_slots = 5      dim = 512      iters = 3   #: iterations of attention  defaults to 3 )  inputs = torch.randn(2  1024  512) slot_attn(inputs) #: (2  5  512) ```  After training  the network is reported to be able to generalize to slightly different number of slots (clusters). You can override the number of slots used by the `num_slots` keyword in forward.  ```python slot_attn(inputs  num_slots = 8) #: (2  8  512) ```   """;General;https://github.com/lucidrains/slot-attention
"""Ê≠§‰ª£Á†ÅÁöÑÊñπÊ≥ïË∑üHerbert95Â§ßÁ•ûÁöÑbaselineÂàÜ‰∫´Â∑Æ‰∏çÂ§öÔºåÂè™ÊòØÊàëËøôÈáåÊòØKerasÔºå‰ªñÊòØÂü∫‰∫épytorchÔºö https://github.com/Herbert95/tianchi_lvcai        ‰æùËµñÂ∫ìÔºö tensorflow  Keras  opencv  pillow  sklearn  scipy  numpy  matplotlib   """;General;https://github.com/wangbinglin1995/tianchi
"""Ê≠§‰ª£Á†ÅÁöÑÊñπÊ≥ïË∑üHerbert95Â§ßÁ•ûÁöÑbaselineÂàÜ‰∫´Â∑Æ‰∏çÂ§öÔºåÂè™ÊòØÊàëËøôÈáåÊòØKerasÔºå‰ªñÊòØÂü∫‰∫épytorchÔºö https://github.com/Herbert95/tianchi_lvcai        ‰æùËµñÂ∫ìÔºö tensorflow  Keras  opencv  pillow  sklearn  scipy  numpy  matplotlib   """;Computer Vision;https://github.com/wangbinglin1995/tianchi
"""""";Computer Vision;https://github.com/kaichoulyc/course_segmentation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/nachiketaa/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/nachiketaa/bert
"""""";Computer Vision;https://github.com/ahmedbhna/VGG_Paper
"""Python 3.7  Pytorch 1.0.0  fastai 1.0.52   Note: we recommend starting with a single GPU  as running multiple GPU will require additional hyperparameter tuning.   %run train.py --woof 1 --size 256 --bs 64 --mixup 0.2 --sa 1 --epoch 5  --lr 3e-3  - woof: 0 for Imagenette  1 for Imagewoof (dataset will download automatically) - size: image size - bs: batch size - mixup: 0 for no mixup data augmentation - sa: 1 if we use SimpleSelfAttention  otherwise 0 - sym: 1 if we add symmetry to SimpleSelfAttention (need to have sa=1) - epoch: number of epochs - lr: learning rate - lrfinder: 1 to run learning rate finder  don't train - dump: 1 to print model  don't train - arch: default is 'xresnet50' - gpu: gpu to train on (by default uses all available GPUs??) - log: name of csv file to save training log to (folder path is displayed when running)   For faster training on multiple GPUs  you can try running: python -m fastai.launch train.py (not tested much)    Results using the original self-attention layer are added as a reference.    | Model | Dataset | Image Size | Epochs | Learning Rate |  """;Computer Vision;https://github.com/sdoria/SimpleSelfAttention
"""Python 3.7  Pytorch 1.0.0  fastai 1.0.52   Note: we recommend starting with a single GPU  as running multiple GPU will require additional hyperparameter tuning.   %run train.py --woof 1 --size 256 --bs 64 --mixup 0.2 --sa 1 --epoch 5  --lr 3e-3  - woof: 0 for Imagenette  1 for Imagewoof (dataset will download automatically) - size: image size - bs: batch size - mixup: 0 for no mixup data augmentation - sa: 1 if we use SimpleSelfAttention  otherwise 0 - sym: 1 if we add symmetry to SimpleSelfAttention (need to have sa=1) - epoch: number of epochs - lr: learning rate - lrfinder: 1 to run learning rate finder  don't train - dump: 1 to print model  don't train - arch: default is 'xresnet50' - gpu: gpu to train on (by default uses all available GPUs??) - log: name of csv file to save training log to (folder path is displayed when running)   For faster training on multiple GPUs  you can try running: python -m fastai.launch train.py (not tested much)    Results using the original self-attention layer are added as a reference.    | Model | Dataset | Image Size | Epochs | Learning Rate |  """;General;https://github.com/sdoria/SimpleSelfAttention
"""OS: Linux 16.02   CUDA: version 8.0  CUDNN: version 5.1   Use the code ```extract_mask.m``` to generate instance mask for the images in MS COCO 2017 training dataset.   We verify the generalization ability of our PSIS on instance segmentation task of MS COCO 2017. The instance segmetatnion results are shown belows:  |Training Sets | AP@0.50:0.95 | AP@0.50 | AP@0.75| AP@Small | AP@Med. | AP@Large |  AR@1 | AR@10 | AR@100 | AR@Small | AR@Med. | AR@Large  |  |--------------|:------------:|:-------:|:------:|:--------:|:-------:|:--------:|:-----:|:-----:|:------:|:--------:|:-------:|:----:| |   ori  |  35.9   | 57.7 | 38.4 |  19.2  | 39.7  |  49.7 | 30.5 | 47.3  | 49.6 |  29.7 |  53.8 |  65.8 | |  psis([model](https://drive.google.com/open?id=13kB4zvwR__O9vSGz9cvy4Br3C-mwSTGZ))  |  36.7   | 58.4 | 39.4 |  19.0  | 40.6  |  50.2 | 31.0 | 48.2  | 50.3 |  29.8 |  54.4 |  66.9 | | ori√ó2  |  36.6   | 58.2 | 39.2 |  18.5  |  40.3 |  50.4 | 31.0 | 47.7  | 49.7 |  29.5 |  53.5 |  66.6 | | psis√ó2([Coming Soon]()) |  37.1   | 58.8 | 39.9 |  19.3  |  41.2 |  50.8 | 31.1 | 47.7  | 50.4 |  30.2 |  54.5 |  67.9 |  Above results clearly show PSIS offers a new and complementary way to use instance masks for improving both detection and segmentation performance.   Here we show some examples of synthetic images generated by our IS strategy. The new (switched) instances are denoted in red boxes  and our instance-switching strategy can clearly preserve contextual coherence in the original images.  <img src=""https://github.com/Hwang64/PSIS/blob/master/img/examples.jpg"">  """;Computer Vision;https://github.com/Hwang64/PSIS
"""Each of our optimizer requires access to the current loss value. This is acheived by passing in a ``closure`` function  to the ``optimizer.step()`` method. The function ``closure()`` should be defined to return the current loss tensor after the forward pass.  Refer to lines ``351-357`` in [``Experiments/main.py``](Experiments/main.py) for an example of the usage.   """;General;https://github.com/bsvineethiitg/adams
"""check that $PYTHONPATH=/Users/KevinBu/tools/sandbox/lib/python3.7/site-packages/ python3 setup.py install  or on hpc module load python/3.7.3 && export PYTHONPATH=$PYTHONPATH:/hpc/users/buk02/tools/sandbox/lib/python3.7/site-packages/ python setup.py install --prefix=/hpc/users/buk02/tools/sandbox  Further information can be found at the original github. The code was adapted and forked from the original repository in late 2018. https://github.com/elcorto/imagecluster.git   """;Computer Vision;https://github.com/kbpi314/iclust
"""This is the pytorch implementation of Paper: Image Inpainting With Learnable Bidirectional Attention Maps (ICCV 2019) [paper](http://openaccess.thecvf.com/content_ICCV_2019/papers/Xie_Image_Inpainting_With_Learnable_Bidirectional_Attention_Maps_ICCV_2019_paper.pdf) [suppl](http://openaccess.thecvf.com/content_ICCV_2019/supplemental/Xie_Image_Inpainting_With_ICCV_2019_supplemental.pdf)   --logPath path_to_save_tensorboard_log --pretrain(optional) pretrained_model_path   """;Computer Vision;https://github.com/Vious/LBAM_Pytorch
"""This is the pytorch implementation of Paper: Image Inpainting With Learnable Bidirectional Attention Maps (ICCV 2019) [paper](http://openaccess.thecvf.com/content_ICCV_2019/papers/Xie_Image_Inpainting_With_Learnable_Bidirectional_Attention_Maps_ICCV_2019_paper.pdf) [suppl](http://openaccess.thecvf.com/content_ICCV_2019/supplemental/Xie_Image_Inpainting_With_ICCV_2019_supplemental.pdf)   --logPath path_to_save_tensorboard_log --pretrain(optional) pretrained_model_path   """;General;https://github.com/Vious/LBAM_Pytorch
"""""";Computer Vision;https://github.com/c1ph3rr/Wide-Residual-Networks
"""""";Natural Language Processing;https://github.com/NehaTamore/TextSimilarity
"""- Prerequisites : Have a GCP account with billing enabled - Create a GCP project (don't forget to adjust the GPU quotas for the considered region) - An endpoint protected by GCP IAP will be created for accessing kubeflow. Follow [these instructions](https://www.kubeflow.org/docs/gke/deploy/oauth-setup/) to create an OAuth client and then enter as IAP Oauth Client ID and Secret - Create a Kubeflow cluster on GKE  using the [Google Cloud Deploy tool](https://deploy.kubeflow.cloud/#/deploy) - Perform additional Setup for REMOTE Kubeflow pipeline execution  - Create GCP service account with the necessary permissons  and added as an 'IAP-secured Web App User'  - Create a storage bucket in Google Storage - Add an NFS server running in GKE inside the Kubeflow namespace - Create a GPU node pool with Autoscaling enabled in GKE   Follow the [setup instructions](./SETUP.md)    - ""Benchmarking"" a GPU datalab server at work      - One of the best features of  using GKE and Kubeflow together  it that you can setup your GKE Cluster with autoscaling. For example  you can add a GPU node-pools to your Kubeflow cluster  that scale on-demand GPU nodes from 0 to whatever value you want.   Include a Kubeflow Fairing version   Setup your local environment and your GKE cluster  clone the repository or import the source code  and execute the notebooks !   **Don't forger to customize in the Notebooks your bucket URL  Kubeflow cluster endpoints and client_id**   - `1-Pix2Pix-local.ipynb ` is expected to be executed locally in a Jupyter notebook running on anykind of GPU server (without Kubeflow) - `2-Pix2Pix-KFP.ipynb ` is expected to be executed inside the Kubeflow's Jupyter notebook running on a GKE cluster (with some GPU nodes pool available). - `3-Pix2Pix-KFP-REMOTE.ipynb ` is expected to be executed in a local Jupyter notebook instance (for instance running on a laptop without GPU)  and it will interact with a Kubeflow cluster running on GKE    """;Computer Vision;https://github.com/fdasilva59/Pix2Pix-Kubeflow-Demo
"""- Prerequisites : Have a GCP account with billing enabled - Create a GCP project (don't forget to adjust the GPU quotas for the considered region) - An endpoint protected by GCP IAP will be created for accessing kubeflow. Follow [these instructions](https://www.kubeflow.org/docs/gke/deploy/oauth-setup/) to create an OAuth client and then enter as IAP Oauth Client ID and Secret - Create a Kubeflow cluster on GKE  using the [Google Cloud Deploy tool](https://deploy.kubeflow.cloud/#/deploy) - Perform additional Setup for REMOTE Kubeflow pipeline execution  - Create GCP service account with the necessary permissons  and added as an 'IAP-secured Web App User'  - Create a storage bucket in Google Storage - Add an NFS server running in GKE inside the Kubeflow namespace - Create a GPU node pool with Autoscaling enabled in GKE   Follow the [setup instructions](./SETUP.md)    - ""Benchmarking"" a GPU datalab server at work      - One of the best features of  using GKE and Kubeflow together  it that you can setup your GKE Cluster with autoscaling. For example  you can add a GPU node-pools to your Kubeflow cluster  that scale on-demand GPU nodes from 0 to whatever value you want.   Include a Kubeflow Fairing version   Setup your local environment and your GKE cluster  clone the repository or import the source code  and execute the notebooks !   **Don't forger to customize in the Notebooks your bucket URL  Kubeflow cluster endpoints and client_id**   - `1-Pix2Pix-local.ipynb ` is expected to be executed locally in a Jupyter notebook running on anykind of GPU server (without Kubeflow) - `2-Pix2Pix-KFP.ipynb ` is expected to be executed inside the Kubeflow's Jupyter notebook running on a GKE cluster (with some GPU nodes pool available). - `3-Pix2Pix-KFP-REMOTE.ipynb ` is expected to be executed in a local Jupyter notebook instance (for instance running on a laptop without GPU)  and it will interact with a Kubeflow cluster running on GKE    """;General;https://github.com/fdasilva59/Pix2Pix-Kubeflow-Demo
"""Adjust the batch size if out of memory (OOM) occurs. It dependes on your gpu memory size and genotype.  - Search  ```shell python search.py --name cifar10 --dataset cifar10 ```  - Augment  ```shell #: genotype from search results python augment.py --name cifar10 --dataset cifar10 --genotype ""Genotype(     normal=[[('sep_conv_3x3'  0)  ('dil_conv_5x5'  1)]  [('skip_connect'  0)  ('dil_conv_3x3'  2)]  [('sep_conv_3x3'  1)  ('skip_connect'  0)]  [('sep_conv_3x3'  1)  ('skip_connect'  0)]]      normal_concat=range(2  6)      reduce=[[('max_pool_3x3'  0)  ('max_pool_3x3'  1)]  [('max_pool_3x3'  0)  ('skip_connect'  2)]  [('skip_connect'  3)  ('max_pool_3x3'  0)]  [('skip_connect'  2)  ('max_pool_3x3'  0)]]      reduce_concat=range(2  6) )"" ```  - with docker  ```shell $ docker run --runtime=nvidia -it khanrc/pytorch-darts:0.2 bash  #: you can run directly also $ docker run --runtime=nvidia -it khanrc/pytorch-darts:0.2 python search.py --name cifar10 --dataset cifar10 ```   """;General;https://github.com/abcp4/MyDarts
"""``` git clone https://github.com/neka-nat/distributed_rl.git cd distributed_rl poetry install ```  Install redis-server.  ``` sudo apt-get install redis-server ```  Setting Atari. https://github.com/openai/atari-py#roms   cd distributed_rl   Create AMI.  ``` packer build packer/ubuntu.json ```  Create key-pair.  ``` aws ec2 create-key-pair --key-name key --query 'KeyMaterial' --output text > ~/.ssh/key.pem chmod 400 ~/.ssh/key.pem ```  Run instances.  ``` cd aws python aws_run_instances.py aws_config.yaml ```  Run fabric for a learner.  ``` fab -H <Public IP of learner's instance> -u ubuntu -i ~/.ssh/key.pem learner_run ```  Run fabric for actors.  ``` fab -H <Public IP of actor's instance 1> <Public IP of actor's instance 2>  ... -u ubuntu -i ~/.ssh/key.pem actor_run:num_proc=15 leaner_host=<Public IP of learner's instance> ```  """;Reinforcement Learning;https://github.com/neka-nat/distributed_rl
"""Please follow [faster-rcnn](https://github.com/jwyang/faster-rcnn.pytorch/tree/pytorch-1.0) respository to setup the environment. In this project  we use Pytorch 1.0.1 and CUDA version is 10.0.130.    * **Cityscape and FoggyCityscape:** Download the [Cityscape](https://www.cityscapes-dataset.com/) dataset  see dataset preparation code in [DA-Faster RCNN](https://github.com/yuhuayc/da-faster-rcnn/tree/master/prepare_data). * **PASCAL_VOC 07+12:** Please follow the [instruction](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to prepare VOC dataset. * **Clipart:** Please follow the [instruction](https://github.com/naoto0804/cross-domain-detection/tree/master/datasets) to prepare Clipart dataset. * **Sim10k:** Download the dataset from this [website](https://fcav.engin.umich.edu/sim-dataset/).     """;Computer Vision;https://github.com/chaoqichen/HTCN
"""All unit tests in baselines can be run using pytest runner: ``` pip install pytest pytest-cov make pytest ```   Install the Stable Baselines package: ``` pip install stable-baselines[mpi] ```  This includes an optional dependency on MPI  enabling algorithms DDPG  GAIL  PPO1 and TRPO. If you do not need these algorithms  you can install without MPI: ``` pip install stable-baselines ```  Please read the [documentation](https://stable-baselines.readthedocs.io/) for more details and alternatives (from source  using docker).    **Note:** Stable-Baselines supports Tensorflow versions from 1.8.0 to 1.14.0. Support for Tensorflow 2 API is planned.   <sup><sup>(5): EDIT: you did it OpenAI! :cat:</sup></sup><br>   Github repo: https://github.com/araffin/rl-baselines-zoo   sudo apt-get update &amp;&amp; sudo apt-get install cmake libopenmpi-dev python3-dev zlib1g-dev  Installation of system packages on Mac requires Homebrew. With Homebrew installed  run the following:  brew install cmake openmpi  To install stable-baselines on Windows  please look at the documentation.   Most of the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms.  Here is a quick example of how to train and run PPO2 on a cartpole environment: ```python import gym  from stable_baselines.common.policies import MlpPolicy from stable_baselines.common.vec_env import DummyVecEnv from stable_baselines import PPO2  env = gym.make('CartPole-v1') #: Optional: PPO2 requires a vectorized environment to run #: the env is now wrapped automatically when passing it to the constructor #: env = DummyVecEnv([lambda: env])  model = PPO2(MlpPolicy  env  verbose=1) model.learn(total_timesteps=10000)  obs = env.reset() for i in range(1000):     action  _states = model.predict(obs)     obs  rewards  dones  info = env.step(action)     env.render()  env.close() ```  Or just train a model with a one liner if [the environment is registered in Gym](https://github.com/openai/gym/wiki/Environments) and if [the policy is registered](https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html):  ```python from stable_baselines import PPO2  model = PPO2('MlpPolicy'  'CartPole-v1').learn(10000) ```  Please read the [documentation](https://stable-baselines.readthedocs.io/) for more examples.    """;General;https://github.com/hill-a/stable-baselines
"""Create a file `~/.keras/keras.json` with the contents:  ``` {     ""image_dim_ordering"": ""tf""      ""epsilon"": 1e-07      ""floatx"": ""float32""      ""backend"": ""theano"" } ```   ```python setup.py install```   You can enforce execution on CPU by hiding all GPU resources:   Important: Switch keras backend to Theano and disable GPU execution (GPU memory is too limited for some of the experiments). GPU speedup for sparse operations is not that essential  so running this model on CPU will still be quite fast.  To replicate the experiments from our paper [1]  first run (for AIFB):  ``` python prepare_dataset.py -d aifb ```   Afterwards  train the model with:  ``` python train.py -d aifb --bases 0 --hidden 16 --l2norm 0. --testing ```   Note that Theano performs an expensive compilation step the first time a computational graph is executed. This can take several minutes to complete.  For the MUTAG dataset  run:  ``` python prepare_dataset.py -d mutag python train.py -d mutag --bases 30 --hidden 16 --l2norm 5e-4 --testing ```  For BGS  run:  ``` python prepare_dataset.py -d bgs python train.py -d bgs --bases 40 --hidden 16 --l2norm 5e-4 --testing ```  For AM  run:  ``` python prepare_dataset.py -d am python train.py -d am --bases 40 --hidden 10 --l2norm 5e-4 --testing ```  Note: Results depend on random seed and will vary between re-runs.   """;Graphs;https://github.com/tkipf/relational-gcn
"""""";General;https://github.com/tbmoon/LANL_Earthquake_Prediction
"""""";Natural Language Processing;https://github.com/tbmoon/LANL_Earthquake_Prediction
"""    $ python -m fastai.utils.show_install   `Deep Learning AMI (Ubuntu 16.04) Version 25.3`  GPU `p2.xlarge` for training :ballot_box_with_check:  `120 GB`   """;General;https://github.com/lukexyz/Language-Models
"""    $ python -m fastai.utils.show_install   `Deep Learning AMI (Ubuntu 16.04) Version 25.3`  GPU `p2.xlarge` for training :ballot_box_with_check:  `120 GB`   """;Natural Language Processing;https://github.com/lukexyz/Language-Models
"""Deep learning  also known as hierarchical learning or deep structured learning   is a type of machine learning that uses a layered algorithmic architecture to analyze data.  Unlike other types of machine learning  deep learning has an added advantage of being able to make decisions with  significantly less human intervention. While basic machine learning requires a programmer to identify whether a conclusion  is correct or not  deep learning can gauge the accuracy of its answers on its own due to the nature of its multi-layered structure.  The emergence of modern frameworks like PyTorch  has also made preprocessing of data more convenient.  Many of the filtering and normalization tasks that would involve a lot of manual tasks while using other machine learning techniques  are taken up automatically.  The essential characteristics of deep learning make it an ideal tool for giving the much needed impetus   to the field of automated medical diagnosis. With the right expertise  it can be leveraged to overcome several  limitations of conventional diagnosis done by medical practitioners  and take the dream of accurate and efficient  automated disease diagnosis to the realm of reality.  Given the team's vision to make healthcare better for everyone  everywhere  and having paid attention to the trends and  recent breakthroughs with deep learning  we decided to experiment with several variations of convolutional neural networks for this project.  Recent work has shown that convolutional networks can be substantially deeper  more accurate   and efficient to train if they contain shorter connections between layers close to the input and output.  We embraced this observation  and leveraged the power of the Dense Convolutional Network (DenseNet)   which connects each layer to every other layer in a feed-forward fashion.  Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer -  our network had L(L+1)/2 direct connections. We also experimented with other architectures  including residual networks  and used our  model variations as controls to validate each other.    * Version 4.1 - for images taken using both antero-posterior position (AP) and postero-anterior position (PA).   The distribution for Version 4.1 finally settled at:   The distribution for Version 4.2 finally settled at:   irrespective of X-ray position using dataset version 4.2.   Victor Mawusi Ayi | https://github.com/ayivima  Anju Mercian | https://github.com/amalphonse  George Christopoulos | https://github.com/geochri  Ashish Bairwa  | https://github.com/ashishbairwa  Pooja Vinod | https://github.com/poojavinod100   Ingus Terbets | https://github.com/ingus-t   Olivia Milgrom | https://github.com/OliviaMil  Tuan Hung Truong | https://github.com/tuanhung94  Marwa Qabeel | https://github.com/MarwaQabeel   Convolutional Neural Networks are inherently designed for image processing  and this sets them up for handling and gathering  insights from large images more efficiently.  Currently  some CNNs have approached  and even surpassed  the accuracy of human diagnosticians in identifying important  features  according to a number of diagnostic imaging studies. In June of 2018  a study in the Annals of Oncology  showed that a convolutional neural network trained to analyze dermatology images identified melanoma with ten percent  more specificity than medical practitioners.Additionally  researchers at the Mount Sinai Icahn School of Medicine have  developed a deep neural network capable of diagnosing crucial neurological conditions  such as stroke and brain hemorrhage   150 times faster than human radiologists. The tool took just 1.2 seconds to process the image  analyze its contents   and alert providers of a problematic clinical finding.  Evidently  there is great potential for the future of healthcare with smart tools which integrate deep learning.   """;Computer Vision;https://github.com/SGNovice/Disease-detection-using-chest-xrays
"""Makefile fixes with default build supporting all NVIDIA architectures   """;General;https://github.com/jolibrain/caffe
"""[PR12 ÎÖºÎ¨∏ Î¶¨Î∑∞] https://youtu.be/hKoJPqWLrI4   """;General;https://github.com/alsoj/Recommenders-movielens
"""My name is Sidney Kingsley and I am a third year student studying Digital Media Design (BSc) at the University of Plymouth. This is the git repository for my final year project. ======= My name is Sidney Kingsley and I am a final year student studying Digital Media Design (BSc) at the University of Plymouth. This is the git repository for my final year project. >>>>>>> e51bd872f2250558bfe7086cc4e07157e3c87de2  My final year project _(FYP)_ focuses on the **web** application of **machine learning** _(ML)_ to create  understand and interact with art.  """;Computer Vision;https://github.com/sidneykingsley/fyp
"""My name is Sidney Kingsley and I am a third year student studying Digital Media Design (BSc) at the University of Plymouth. This is the git repository for my final year project. ======= My name is Sidney Kingsley and I am a final year student studying Digital Media Design (BSc) at the University of Plymouth. This is the git repository for my final year project. >>>>>>> e51bd872f2250558bfe7086cc4e07157e3c87de2  My final year project _(FYP)_ focuses on the **web** application of **machine learning** _(ML)_ to create  understand and interact with art.  """;General;https://github.com/sidneykingsley/fyp
"""``` python   You should install ü§ó Transformers in a virtual environment. If you're unfamiliar with Python virtual environments  check out the user guide.  First  create a virtual environment with the version of Python you're going to use and activate it.  Then  you will need to install at least one of Flax  PyTorch or TensorFlow.  Please refer to TensorFlow installation page  PyTorch installation page and/or Flax installation page regarding the specific install command for your platform.  When one of those backends has been installed  ü§ó Transformers can be installed using pip as follows:  pip install transformers  If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release  you must install the library from source.  Since Transformers version v4.0.0  we now have a conda channel: huggingface.  ü§ó Transformers can be installed using conda as follows:   conda install -c huggingface transformers  Follow the installation pages of Flax  PyTorch or TensorFlow to see how to install them with conda.   You can test most of our models directly on their pages from the [model hub](https://huggingface.co/models). We also offer [private model hosting  versioning  & an inference API](https://huggingface.co/pricing) for public and private models.  Here are a few examples:   In Natural Language Processing: - [Masked word completion with BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France) - [Name Entity Recognition with Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city) - [Text generation with GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+) - [Natural Language Inference with RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal) - [Summarization with BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct) - [Question answering with DistilBERT](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species) - [Translation with T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)  In Computer Vision: - [Image classification with ViT](https://huggingface.co/google/vit-base-patch16-224) - [Object Detection with DETR](https://huggingface.co/facebook/detr-resnet-50) - [Image Segmentation with DETR](https://huggingface.co/facebook/detr-resnet-50-panoptic)  In Audio: - [Automatic Speech Recognition with Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h) - [Keyword Spotting with Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)  **[Write With Transformer](https://transformer.huggingface.co)**  built by the Hugging Face team  is the official demo of this repo‚Äôs text generation capabilities.   1. Easy-to-use state-of-the-art models:     - High performance on natural language understanding & generation  computer vision  and audio tasks.     - Low barrier to entry for educators and practitioners.     - Few user-facing abstractions with just three classes to learn.     - A unified API for using all our pretrained models.  1. Lower compute costs  smaller carbon footprint:     - Researchers can share trained models instead of always retraining.     - Practitioners can reduce compute time and production costs.     - Dozens of architectures with over 20 000 pretrained models  some in more than 100 languages.  1. Choose the right framework for every part of a model's lifetime:     - Train state-of-the-art models in 3 lines of code.     - Move a single model between TF2.0/PyTorch/JAX frameworks at will.     - Seamlessly pick the right framework for training  evaluation and production.  1. Easily customize a model or an example to your needs:     - We provide examples for each architecture to reproduce the results published by its original authors.     - Model internals are exposed as consistently as possible.     - Model files can be used independently of the library for quick experiments.   - This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose  so that researchers can quickly iterate on each of the models without diving into additional abstractions/files. - The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops  you should use another library. - While we strive to present as many use cases as possible  the scripts in our [examples folder](https://github.com/huggingface/transformers/tree/master/examples) are just that: examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.   """;General;https://github.com/huggingface/transformers
"""Please follow the instructions of py-faster-rcnn [here](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to setup VOC and COCO datasets (Part of COCO is done). The steps involve downloading data and optionally creating soft links in the ``data`` folder. Since faster RCNN does not rely on pre-computed proposals  it is safe to ignore the steps that setup proposals.  If you find it useful  the ``data/cache`` folder created on my side is also shared [here](http://ladoga.graphics.cs.cmu.edu/xinleic/tf-faster-rcnn/cache.tgz).   1. Clone the repository   ```Shell   git clone https://github.com/endernewton/tf-faster-rcnn.git   ```  2. Update your -arch in setup script to match your GPU   ```Shell   cd tf-faster-rcnn/lib   #: Change the GPU architecture (-arch) if necessary   vim setup.py   ```    | GPU model  | Architecture |   | ------------- | ------------- |   | TitanX (Maxwell/Pascal) | sm_52 |   | GTX 960M | sm_50 |   | GTX 1080 (Ti) | sm_61 |   | Grid K520 (AWS g2.2xlarge) | sm_30 |   | Tesla K80 (AWS p2.xlarge) | sm_37 |    **Note**: You are welcome to contribute the settings on your end if you have made the code work properly on other GPUs. Also even if you are only using CPU tensorflow  GPU based code (for NMS) will be used by default  so please set **USE_GPU_NMS False** to get the correct output.   3. Build the Cython modules   ```Shell   make clean   make   cd ..   ```  4. Install the [Python COCO API](https://github.com/pdollar/coco). The code requires the API to access COCO dataset.   ```Shell   cd data   git clone https://github.com/pdollar/coco.git   cd coco/PythonAPI   make   cd ../../..   ```     - Due to the randomness in GPU training with Tensorflow especially for VOC  the best numbers are reported (with 2-3 attempts) here. According to my experience  for COCO you can almost always get a very close number (within ~0.2%) despite the randomness.      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz     tar -xzvf vgg_16_2016_08_28.tar.gz      cd ../..     For Resnet101  you can set up like:      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz     tar -xzvf resnet_v1_101_2016_08_28.tar.gz      cd ../..     ./experiments/scripts/train_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/train_faster_rcnn.sh 1 coco res101     ./experiments/scripts/test_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/test_faster_rcnn.sh 1 coco res101   1. Download pre-trained model   ```Shell   #: Resnet101 for voc pre-trained on 07+12 set   ./data/scripts/fetch_faster_rcnn_models.sh   ```   **Note**: if you cannot download the models through the link  or you want to try more models  you can check out the following solutions and optionally update the downloading script:   - Another server [here](http://xinlei.sp.cs.cmu.edu/xinleic/tf-faster-rcnn/).   - Google drive [here](https://drive.google.com/open?id=0B1_fAEgxdnvJSmF3YUlZcHFqWTQ).  2. Create a folder and a soft link to use the pre-trained model   ```Shell   NET=res101   TRAIN_IMDB=voc_2007_trainval+voc_2012_trainval   mkdir -p output/${NET}/${TRAIN_IMDB}   cd output/${NET}/${TRAIN_IMDB}   ln -s ../../../data/voc_2007_trainval+voc_2012_trainval ./default   cd ../../..   ```  3. Demo for testing on custom images   ```Shell   #: at repository root   GPU_ID=0   CUDA_VISIBLE_DEVICES=${GPU_ID} ./tools/demo.py   ```   **Note**: Resnet101 testing probably requires several gigabytes of memory  so if you encounter memory capacity issues  please install it with CPU support only. Refer to [Issue 25](https://github.com/endernewton/tf-faster-rcnn/issues/25).  4. Test with pre-trained Resnet101 models   ```Shell   GPU_ID=0   ./experiments/scripts/test_faster_rcnn.sh $GPU_ID pascal_voc_0712 res101   ```   **Note**: If you cannot get the reported numbers (79.8 on my side)  then probably the NMS function is compiled improperly  refer to [Issue 5](https://github.com/endernewton/tf-faster-rcnn/issues/5).   """;Computer Vision;https://github.com/daxiapazi/faster-rcnn
"""If you want to implement this project.the environment need build as follow:  python==3.6    when you establish the environment then can implement this project in terminal by ""python train.py""   """;Computer Vision;https://github.com/2anchao/VovJpu
"""If you running on GPU  then change these lines init  update_ema    ``` python3 main.py -h usage: main.py [-h] --save_dir SAVE_DIR [--root ROOT] [--gpus GPUS]                [--num_workers NUM_WORKERS] [--model {mixs}] [--epoch EPOCH]                [--batch_size BATCH_SIZE] [--test] [--ema_decay EMA_DECAY]                [--optim {rmsprop adam}] [--lr LR] [--beta [BETA [BETA ...]]]                [--momentum MOMENTUM] [--eps EPS] [--decay DECAY]                [--scheduler {exp cosine none}]  Pytorch Mixnet  optional arguments:   -h  --help            show this help message and exit   --save_dir SAVE_DIR   Directory name to save the model   --root ROOT           The Directory of data path.   --gpus GPUS           Select GPU Numbers | 0 1 2 3 |   --num_workers NUM_WORKERS                         Select CPU Number workers   --model {mixs}        The type of mixnet.   --epoch EPOCH         The number of epochs   --batch_size BATCH_SIZE                         The size of batch   --test                Only Test   --ema_decay EMA_DECAY                         Exponential Moving Average Term   --optim {rmsprop adam}   --lr LR               Base learning rate when train batch size is 256.   --beta [BETA [BETA ...]]   --momentum MOMENTUM   --eps EPS   --decay DECAY   --scheduler {exp cosine none}                         Learning rate scheduler type ```  """;Computer Vision;https://github.com/zsef123/MixNet-PyTorch
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla T4'  total_memory=15079MB)   Using CUDA device0 _CudaDeviceProperties(name='Tesla T4'  total_memory=15079MB)   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/yrouphail/yolo-independent
"""1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   Requirements: hardware  Basic installation   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   To train and test a Faster R-CNN detector using the **alternating optimization** algorithm from our NIPS 2015 paper  use `experiments/scripts/faster_rcnn_alt_opt.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  (""alt opt"" refers to the alternating optimization training algorithm described in the NIPS paper.)  To train and test a Faster R-CNN detector using the **approximate joint training** method  use `experiments/scripts/faster_rcnn_end2end.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  This method trains the RPN module jointly with the Fast R-CNN network  rather than alternating between training the two. It results in faster (~ 1.5x speedup) training times and similar detection accuracy. See these [slides](https://www.dropbox.com/s/xtr4yd4i5e0vw8g/iccv15_tutorial_training_rbg.pdf?dl=0) for more details.  Artifacts generated by the scripts in `tools` are written in this directory.  Trained Fast R-CNN networks are saved under:  ``` output/<experiment directory>/<dataset name>/ ```  Test outputs are saved under:  ``` output/<experiment directory>/<dataset name>/<network snapshot name>/ ```  """;Computer Vision;https://github.com/lincaiming/py-faster-rcnn-update
"""""";Computer Vision;https://github.com/SyogoShibuya/Chainer-Cutmix
"""- For the input json data format:<br> each sample has the format as follows <br> **{'target': label of sample  <br> 'graph': all edges in the graph  each edge is represented as a triple: (source_id  edge_id  target_id)  <br> 'node_features': task-specific innitial annotation for each node in the graph <br> }**<br> (All ids start from 1) - To run the code  please use command **`python main.py`**. - To run it on GPU  please use command **`python main.py --cuda`**. <br> (For general use  you should only care about files without a suffix 'plus'  as those files are for specific use of ABox reasoning model. Specifically  for GGNN_plus  there is no need for you to specify the initial annotations for each node by yourself  the annotation for all nodes are stored in an embedding layer  which is also learnable during the training process. Experiments demonstrate that GGNN_plus outperforms GGNN on ABox Reasoning in terms of both efficiency and effectiveness.)  """;Graphs;https://github.com/entslscheia/GGNN_Reasoning
"""""";General;https://github.com/changhuixu/LSTM-sentiment-analysis
"""Running YOLO on the CPU is doable but very slow. The GPU-accelerated version of TensorFlow 1.x is much faster. On a portable  scaled down Turing-class GPU (GTX 1660 Ti) with 6 GB RAM we get up to 20 fps from the neural network alone - which is then reduced to half by the code after output parsing  display  etc (surely there's a lot of optimizations yet to be done - e.g. replace loops with vector operations).   Depending on your OS  you need to pick the serial port (COM3  /dev/ttyS0) that the Maestro is using  and inject commands into it via the maestro.py library.   """;Computer Vision;https://github.com/FlorinAndrei/TensorAim
"""Machine translation is a natural language processing task that aims to translate natural languages using computers automatically. Recent several years have witnessed the rapid development of end-to-end neural machine translation  which has become the new mainstream method in practical MT systems.  THUMT is an open-source toolkit for neural machine translation developed by [the Natural Language Processing Group at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/site2/index.php?lang=en).    """;General;https://github.com/insigh/THUMT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  BERT is based on the Transformer architecture introduced in [Attention is all you need](https://arxiv.org/abs/1706.03762).  Googles academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  Googles Github repository where the original english models can be found here: [https://github.com/google-research/bert](https://github.com/google-research/bert).  Included in the downloads below are PyTorch versions of the models based on the work of  NLP researchers from HuggingFace. [PyTorch version of BERT available](https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/)   The links to the models are here (right-click  'Save link as...' on the name):   """;General;https://github.com/af-ai-center/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  BERT is based on the Transformer architecture introduced in [Attention is all you need](https://arxiv.org/abs/1706.03762).  Googles academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  Googles Github repository where the original english models can be found here: [https://github.com/google-research/bert](https://github.com/google-research/bert).  Included in the downloads below are PyTorch versions of the models based on the work of  NLP researchers from HuggingFace. [PyTorch version of BERT available](https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/)   The links to the models are here (right-click  'Save link as...' on the name):   """;Natural Language Processing;https://github.com/af-ai-center/bert
"""Before installation  please change swap size.  At Pi terminal  open 'dphys-swapfile'.    ```shell sudo nano /etc/dphys-swapfile ``` and change CONF_SWAPSIZE. (default is set to 100.)   ```shell CONF_SWAPSIZE=2048 ``` Activate it.   ```shell sudo /etc/init.d/dphys-swapfile stop sudo /etc/init.d/dphys-swapfile start ``` Now you can install following things into your Pi.  PyTorch-0.2.0(499MB) - Follow [my guide](https://github.com/savageyusuff/MobilePose-Pi/blob/master/PyTorch_Installation_Guide.md).   torchvision-0.2.1(2.7MB)  - Please install it from **source**. Repo is [here](https://github.com/pytorch/vision).   OpenCV-3.4.0(1.8GB) - Follow [this guide](https://www.pyimagesearch.com/2018/09/26/install-opencv-4-on-your-raspberry-pi/).    Also  don't forget to install libraries. ```shell pip install -r requirements.txt ```  At the end  change CONF_SWAPSIZE to 100 again.       |  OpenCV_Installation_Guide.md  |  OpenCV installation guide  |  |  PyTorch_Installation_Guide.md  |  PyTorch installation guide  |   |  requirements.txt  |  libraries needed to run scripts  |    Of course  you can use your own models also.     You can train three models (shufflenet/mobilenet/resnet) on your PC.     Different from MobilePose-pytorch(original repo)  the command is   I recommend to do this via conda.    1.You need to set up PC environment.     """;Computer Vision;https://github.com/savageyusuff/MobilePose-Pi
"""This project makes use of the freely-available [Million Song Dataset](http://millionsongdataset.com/)  and its integration with the [Last.fm Dataset](http://millionsongdataset.com/lastfm/). The former provides a link between all the useful information about the tracks (such as title  artist or year) and the audio track themselves  whereas the latter contains tags information on some of the tracks. A preview of the audio tracks can be fetched from services such as 7Digital  but this is allegedly not an easy task.   If you are only interested in our final results  click [here](https://github.com/pukkapies/urop2019#results).  If you want to use some of our code  or try to re-train our model on your own  read on. We will assume you have access to the actual songs in the dataset. Here is the outline of the approach we followed:  1. Extracte all the useful information from the Million Song Dataset and clean both the audio tracks and the Last.fm tags database to produce our final 'clean' data;  2. Prepare a flexible data input pipeline and transform the data in a format which is easy to consume by the training algorithm;  3. Prepare a flexible training script which would allow for multiple experiments (such as slightly different architectures  slightly different versions of the tags database  or different training parameters);  4. Train our model and use it to make sensible tag predictions from a given input audio.  In the following sections  we will provide a brief tutorial of how you may use this repository to make genre predictions of your own  or carry out some further experiments.   System Requirements   calle.sonne18@imperial.ac.uk  chon.ho17@imperial.ac.uk  davide.gallo18@imperial.ac.uk / davide.gallo@pm.me  kevin.webster@imperial.ac.uk  """;General;https://github.com/pukkapies/urop2019
"""Xiaoshi: A Chinese to Chinese traditional poetry dataset (https://github.com/enhuiz/XiaoShi).   | En                                                                        | De (Model)                                                          | De (Ground Truth)                                                      | | ------------------------------------------------------------------------- | ------------------------------------------------------------------- | ---------------------------------------------------------------------- | | A man in an orange hat starring at something                              | ein mann mit orangefarbenem hut starrt etwas an                     | ein mann mit einem orangefarbenen hut der etwas <unk>                  | | A Boston Terrier is running on lush green grass in front of a white fence | ein kampfrichter rennt auf einer gr√ºnen wiese vor einem wei√üen zaun | ein boston terrier l√§uft √ºber saftig gr√ºnes gras vor einem wei√üen zaun | | A girl in karate uniform breaking a stick with a front kick               | ein m√§dchen in karateanz√ºgen folgt einem stock mit einem fu√ütritt   | ein m√§dchen in einem karateanzug bricht einen stock mit einem tritt    |   | Poetry                           | Chinese (Model)                                                      | Chinese (Ground Truth)                                                                                               | | -------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | | ÁπñÂπÑÂûÇÂûÇÈ©¨Ë∏èÊ≤ôÔºåÊ∞¥ÈïøÂ±±ËøúË∑ØÂ§öËä±„ÄÇ | ÈªÑ Êòè Êó∂ ÂàÜ Ôºå Â±± Ë∑Ø ÊóÅ Ëæπ ÁöÑ Ê†ë Êú® Ôºå Ê∞¥ Èù¢ ÂØπ ÁùÄ Á´π Êûó „ÄÇ          | È©¨ Âêé ÂûÇ ÁùÄ ‰ºû Áõñ Ôºå È©¨ ËπÑ Ë∏© ÁùÄ ÈªÑ Ê≤ô Ôºå Ê≤ô Ê≤ô Ôºå Ê≤ô Ê≤ô „ÄÇ Â±± ÈÅ• ÈÅ• Ôºå Ê∞¥ Ëå´ Ëå´ Ôºå Ê≤ø Ë∑Ø ËßÅ Âà∞ ÈÇ£ ‰πà Â§ö ÁöÑ Èáé Ëä± „ÄÇ | | Áúº‰∏≠ÂΩ¢ÂäøËÉ∏‰∏≠Á≠ñÔºåÁºìÊ≠•ÂæêË°åÈùô‰∏çÂìó„ÄÇ | ÂΩì Âπ¥ ÁöÑ Êñá Á´† ‰∏≠ ‰πã Âêé Ôºå ‰ªñ ‰ª¨ ‰∏ç Áü• ÈÅì ‰ªñ ‰ª¨ „ÄÇ                   | Áúº ËßÇ Êïå Êàë ÂΩ¢ Âäø Ôºå Êàò ÊúØ Êñπ Áï• Êó© Â∑≤ Êàê Á´π Âú® ËÉ∏ „ÄÇ ÂÖµ È©¨ Áºì Ê≠• Ââç Ëøõ Ôºå ‰∏â ÂÜõ ËÇÉ Èùô Êó† ‰∫∫ Âñß Âìó „ÄÇ                | | Êª°ËÖπËØó‰π¶Êº´Âè§‰ªäÔºåÈ¢ëÂπ¥ÊµÅËêΩÊòì‰º§ÂøÉ„ÄÇ | ÂΩì Âπ¥ Êàë ‰ª¨ Âú® Ëøô Êó∂ ËäÇ ÁöÑ Êó∂ ÂÄô Ôºå Êàë ÂøÉ ÊÉÖ ÂÜµ ÊòØ Êàë ÁöÑ ÂøÉ ÊÉÖ ÈÉé „ÄÇ | Ë£Ö Êª° ‰∏Ä ËÇö Â≠ê ËØó ‰π¶ Ôºå Âçö Âè§ ÈÄö ‰ªä „ÄÇ Ëøû Âπ¥ ÊµÅ ËêΩ ‰ªñ ‰π° Ôºå ÊúÄ Êòì ‰º§ ÊÉÖ „ÄÇ                                           |   """;General;https://github.com/enhuiz/transformer-pytorch
"""Xiaoshi: A Chinese to Chinese traditional poetry dataset (https://github.com/enhuiz/XiaoShi).   | En                                                                        | De (Model)                                                          | De (Ground Truth)                                                      | | ------------------------------------------------------------------------- | ------------------------------------------------------------------- | ---------------------------------------------------------------------- | | A man in an orange hat starring at something                              | ein mann mit orangefarbenem hut starrt etwas an                     | ein mann mit einem orangefarbenen hut der etwas <unk>                  | | A Boston Terrier is running on lush green grass in front of a white fence | ein kampfrichter rennt auf einer gr√ºnen wiese vor einem wei√üen zaun | ein boston terrier l√§uft √ºber saftig gr√ºnes gras vor einem wei√üen zaun | | A girl in karate uniform breaking a stick with a front kick               | ein m√§dchen in karateanz√ºgen folgt einem stock mit einem fu√ütritt   | ein m√§dchen in einem karateanzug bricht einen stock mit einem tritt    |   | Poetry                           | Chinese (Model)                                                      | Chinese (Ground Truth)                                                                                               | | -------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | | ÁπñÂπÑÂûÇÂûÇÈ©¨Ë∏èÊ≤ôÔºåÊ∞¥ÈïøÂ±±ËøúË∑ØÂ§öËä±„ÄÇ | ÈªÑ Êòè Êó∂ ÂàÜ Ôºå Â±± Ë∑Ø ÊóÅ Ëæπ ÁöÑ Ê†ë Êú® Ôºå Ê∞¥ Èù¢ ÂØπ ÁùÄ Á´π Êûó „ÄÇ          | È©¨ Âêé ÂûÇ ÁùÄ ‰ºû Áõñ Ôºå È©¨ ËπÑ Ë∏© ÁùÄ ÈªÑ Ê≤ô Ôºå Ê≤ô Ê≤ô Ôºå Ê≤ô Ê≤ô „ÄÇ Â±± ÈÅ• ÈÅ• Ôºå Ê∞¥ Ëå´ Ëå´ Ôºå Ê≤ø Ë∑Ø ËßÅ Âà∞ ÈÇ£ ‰πà Â§ö ÁöÑ Èáé Ëä± „ÄÇ | | Áúº‰∏≠ÂΩ¢ÂäøËÉ∏‰∏≠Á≠ñÔºåÁºìÊ≠•ÂæêË°åÈùô‰∏çÂìó„ÄÇ | ÂΩì Âπ¥ ÁöÑ Êñá Á´† ‰∏≠ ‰πã Âêé Ôºå ‰ªñ ‰ª¨ ‰∏ç Áü• ÈÅì ‰ªñ ‰ª¨ „ÄÇ                   | Áúº ËßÇ Êïå Êàë ÂΩ¢ Âäø Ôºå Êàò ÊúØ Êñπ Áï• Êó© Â∑≤ Êàê Á´π Âú® ËÉ∏ „ÄÇ ÂÖµ È©¨ Áºì Ê≠• Ââç Ëøõ Ôºå ‰∏â ÂÜõ ËÇÉ Èùô Êó† ‰∫∫ Âñß Âìó „ÄÇ                | | Êª°ËÖπËØó‰π¶Êº´Âè§‰ªäÔºåÈ¢ëÂπ¥ÊµÅËêΩÊòì‰º§ÂøÉ„ÄÇ | ÂΩì Âπ¥ Êàë ‰ª¨ Âú® Ëøô Êó∂ ËäÇ ÁöÑ Êó∂ ÂÄô Ôºå Êàë ÂøÉ ÊÉÖ ÂÜµ ÊòØ Êàë ÁöÑ ÂøÉ ÊÉÖ ÈÉé „ÄÇ | Ë£Ö Êª° ‰∏Ä ËÇö Â≠ê ËØó ‰π¶ Ôºå Âçö Âè§ ÈÄö ‰ªä „ÄÇ Ëøû Âπ¥ ÊµÅ ËêΩ ‰ªñ ‰π° Ôºå ÊúÄ Êòì ‰º§ ÊÉÖ „ÄÇ                                           |   """;Natural Language Processing;https://github.com/enhuiz/transformer-pytorch
"""- [ ] Add more instructions / help in the README on how to get an optimal environment with JAX up and running (with GPU support)   * Models with Tensorflow 'SAME' padding and TF origin weights are prefixed with tf_. Models with PyTorch trained weights and symmetric PyTorch style padding ('LIKE' here) are prefixed with pt_   Models by their config name w/ valid pretrained weights that should be working here:   Working with JAX I've found the best approach for having a working GPU compatible environment that performs well is to use Docker containers based on the latest NVIDIA NGC releases. I've found it challenging or flaky getting local conda/pip venvs or Tensorflow docker containers working well with good GPU performance  proper NCCL distributed support  etc. I use CPU JAX install in conda env for dev/debugging.  There are several container definitions in docker/. They use NGC containers as their parent image so you'll need to be setup to pull NGC containers: https://www.nvidia.com/en-us/gpu-cloud/containers/ . I'm currently using recent NGC containers w/ CUDA 11.1 support  the host system will need a very recent NVIDIA driver to support this but doesn't need a matching CUDA 11.1 / cuDNN 8 install.   * pt_git.Dockerfile - PyTorch 20.12 NGC as parent  CUDA 11.1  cuDNN 8. git (source install) of jaxlib  jax  objax  and flax.  * pt_pip.Dockerfile - PyTorch 20.12 NGC as parent  CUDA 11.1  cuDNN 8. pip (latest ver) install of jaxlib  jax  objax  and flax.  * tf_git.Dockerfile - Tensorflow 2 21.02 NGC as parent  CUDA 11.2  cuDNN 8. git (source install) of jaxlib  jax  objax  and flax.  * tf_pip.Dockerfile - Tensorflow 2 21.02 NGC as parent  CUDA 11.2  cuDNN 8. pip (latest ver) install of jaxlib  jax  objax  and flax.  The 'git' containers take some time to build jaxlib  they pull the masters of all respective repos so are up to the bleeding edge but more likely to have possible regression or incompatibilities that go with that. The pip install containers are quite a bit quicker to get up and running  based on the latest pip versions of all repos.   1. Make sure you have a recent version of docker and the NVIDIA Container Toolkit setup (https://github.com/NVIDIA/nvidia-docker)  2. Build the container `docker build -f docker/tf_pip.Dockerfile -t jax_tf_pip .` 3. Run the container  ideally map jeffnet and datasets (ImageNet) into the container     * For tf containers  `docker run --gpus all -it -v /path/to/tfds/root:/data/ -v /path/to/efficientnet-jax/:/workspace/jeffnet --rm --ipc=host jax_tf_pip`     * For pt containers  `docker run --gpus all -it -v /path/to/imagenet/root:/data/ -v /path/to/efficientnet-jax/:/workspace/jeffnet --rm --ipc=host jax_pt_pip` 4. Model validation w/ pretrained weights (once inside running container):     * For tf  in `worskpace/jeffnet`  `python tf_linen_validate.py /data/ --model tf_efficientnet_b0_ns`     * For pt  in `worskpace/jeffnet`  `python pt_objax_validate.py /data/validation --model pt_efficientnet_b0` 5. Training (within container)     * In `worskpace/jeffnet`  `tf_linen_train.py --config train_configs/tf_efficientnet_b0-gpu_24gb_x2.py --config.data_dir /data`   """;General;https://github.com/rwightman/efficientnet-jax
"""- [ ] Add more instructions / help in the README on how to get an optimal environment with JAX up and running (with GPU support)   * Models with Tensorflow 'SAME' padding and TF origin weights are prefixed with tf_. Models with PyTorch trained weights and symmetric PyTorch style padding ('LIKE' here) are prefixed with pt_   Models by their config name w/ valid pretrained weights that should be working here:   Working with JAX I've found the best approach for having a working GPU compatible environment that performs well is to use Docker containers based on the latest NVIDIA NGC releases. I've found it challenging or flaky getting local conda/pip venvs or Tensorflow docker containers working well with good GPU performance  proper NCCL distributed support  etc. I use CPU JAX install in conda env for dev/debugging.  There are several container definitions in docker/. They use NGC containers as their parent image so you'll need to be setup to pull NGC containers: https://www.nvidia.com/en-us/gpu-cloud/containers/ . I'm currently using recent NGC containers w/ CUDA 11.1 support  the host system will need a very recent NVIDIA driver to support this but doesn't need a matching CUDA 11.1 / cuDNN 8 install.   * pt_git.Dockerfile - PyTorch 20.12 NGC as parent  CUDA 11.1  cuDNN 8. git (source install) of jaxlib  jax  objax  and flax.  * pt_pip.Dockerfile - PyTorch 20.12 NGC as parent  CUDA 11.1  cuDNN 8. pip (latest ver) install of jaxlib  jax  objax  and flax.  * tf_git.Dockerfile - Tensorflow 2 21.02 NGC as parent  CUDA 11.2  cuDNN 8. git (source install) of jaxlib  jax  objax  and flax.  * tf_pip.Dockerfile - Tensorflow 2 21.02 NGC as parent  CUDA 11.2  cuDNN 8. pip (latest ver) install of jaxlib  jax  objax  and flax.  The 'git' containers take some time to build jaxlib  they pull the masters of all respective repos so are up to the bleeding edge but more likely to have possible regression or incompatibilities that go with that. The pip install containers are quite a bit quicker to get up and running  based on the latest pip versions of all repos.   1. Make sure you have a recent version of docker and the NVIDIA Container Toolkit setup (https://github.com/NVIDIA/nvidia-docker)  2. Build the container `docker build -f docker/tf_pip.Dockerfile -t jax_tf_pip .` 3. Run the container  ideally map jeffnet and datasets (ImageNet) into the container     * For tf containers  `docker run --gpus all -it -v /path/to/tfds/root:/data/ -v /path/to/efficientnet-jax/:/workspace/jeffnet --rm --ipc=host jax_tf_pip`     * For pt containers  `docker run --gpus all -it -v /path/to/imagenet/root:/data/ -v /path/to/efficientnet-jax/:/workspace/jeffnet --rm --ipc=host jax_pt_pip` 4. Model validation w/ pretrained weights (once inside running container):     * For tf  in `worskpace/jeffnet`  `python tf_linen_validate.py /data/ --model tf_efficientnet_b0_ns`     * For pt  in `worskpace/jeffnet`  `python pt_objax_validate.py /data/validation --model pt_efficientnet_b0` 5. Training (within container)     * In `worskpace/jeffnet`  `tf_linen_train.py --config train_configs/tf_efficientnet_b0-gpu_24gb_x2.py --config.data_dir /data`   """;Computer Vision;https://github.com/rwightman/efficientnet-jax
"""``` pip install torch numpy pillow mkdir model ```   The only dependencies are pytorch  numpy and pillow.   The example dataset is from the ISBI Challenge. More information here: http://brainiac2.mit.edu/isbi_challenge/.  A few outputs from the test dataset  after 300 iterations:  ![Output image 2](public/image2.png)  ![Output image 3](public/image3.png)  Download and put the files in the `data` directory. It should be like this: ``` data ‚îú‚îÄ‚îÄ test-volume.tif ‚îú‚îÄ‚îÄ train-labels.tif ‚îî‚îÄ‚îÄ train-volume.tif ```   """;Computer Vision;https://github.com/clemkoa/u-net
"""In order to use a different test dataset that the one which is hardcoded inside the code  specify the path as a last argument:   Check paths that are hardcoded inside the script  which leads to:   Download or clone whole repository in order to obtain all necessary files. If you need a dataset - you can download it from [here.](https://drive.google.com/drive/u/1/folders/1KfR5TjGstcA3SEXb2TKkHnoTVIoiCKDG) Save it inside the project folder (DATASETS/).    Use the *demo.py* file in order to test an algorithm.   Warning: the default RESULTS_FOLDER in the code leads to the *cars-classification-deep-learning/saved_models/20190701_1148* which does not contain file with weights. You can download weights from [here](https://drive.google.com/drive/folders/1HXAJUELObpp06H9VGNmopPiBopmHGjkb?usp=sharing) (*weights.best.hdf5*)  save it inside results folder (*saved_models/20190701_1148*) and run the *demo.py*.   Example usage:   The main folder should contain 2 subfolders: train dataset and test dataset. Each of them should contain more subdirectories created due to class (label) names (one subdirectory per class). You can prepare data on your own  using [original Cars Stanford Dataset](https://ai.stanford.edu/~jkrause/cars/car_dataset.html) or use mine (already sorted) - download from [here.](https://ai.stanford.edu/~jkrause/cars/car_dataset.html)  If you want to modify these images (resize  expand background  transform to grayscale) you can check functions inside the script called *data_preprocessing.py*.   If you want to use a new dataset  you also need to prepare them: save images inside correct subdirectories.   """;Computer Vision;https://github.com/kaamka/cars-classification-deep-learning
"""This project provides a PyTorch implementation about [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) based on [fairseq-py](https://github.com/facebookresearch/fairseq-py) (An official toolkit of facebook research). You can also use official code about *Attention is all you need* from [tensor2tensor](https://github.com/tensorflow/tensor2tensor).  If you use this code about cnn  please cite: ``` @inproceedings{gehring2017convs2s    author    = {Gehring  Jonas  and Auli  Michael and Grangier  David and Yarats  Denis and Dauphin  Yann N}    title     = ""{Convolutional Sequence to Sequence Learning}""    booktitle = {Proc. of ICML}    year      = 2017  } ``` And if you use this code about transformer  please cite: ``` @inproceedings{46201    title = {Attention is All You Need}    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin}    year  = {2017}    booktitle = {Proc. of NIPS}  } ``` Feel grateful for the contribution of the facebook research and google research. **Besides  if you get benefits from this repository  please give me a star.**   You first need to install PyTorch >= 0.4.0 and Python = 3.6. And then ``` pip install -r requirements.txt python setup.py build python setup.py develop ```  Generating binary data  please follow the script under [data/](data/)  i have provide a [run script](run_iwslt14_transformer.sh) for iwslt14.   """;General;https://github.com/StillKeepTry/Transformer-PyTorch
"""This project provides a PyTorch implementation about [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) based on [fairseq-py](https://github.com/facebookresearch/fairseq-py) (An official toolkit of facebook research). You can also use official code about *Attention is all you need* from [tensor2tensor](https://github.com/tensorflow/tensor2tensor).  If you use this code about cnn  please cite: ``` @inproceedings{gehring2017convs2s    author    = {Gehring  Jonas  and Auli  Michael and Grangier  David and Yarats  Denis and Dauphin  Yann N}    title     = ""{Convolutional Sequence to Sequence Learning}""    booktitle = {Proc. of ICML}    year      = 2017  } ``` And if you use this code about transformer  please cite: ``` @inproceedings{46201    title = {Attention is All You Need}    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin}    year  = {2017}    booktitle = {Proc. of NIPS}  } ``` Feel grateful for the contribution of the facebook research and google research. **Besides  if you get benefits from this repository  please give me a star.**   You first need to install PyTorch >= 0.4.0 and Python = 3.6. And then ``` pip install -r requirements.txt python setup.py build python setup.py develop ```  Generating binary data  please follow the script under [data/](data/)  i have provide a [run script](run_iwslt14_transformer.sh) for iwslt14.   """;Natural Language Processing;https://github.com/StillKeepTry/Transformer-PyTorch
"""""";General;https://github.com/cdebeunne/uncertainties_CNN
"""Clone this project to your machine.   ```bash git clone https://github.com/jiangsutx/SRN-Deblur.git cd SRN-Deblur ```  $ cd CycleGAN/CycleGAN_Code   $ cd CycleGAN/CycleGAN_Code   cd /CycleGAN   cd /CycleGAN   cd /SRN   If you have a GPU  please include --gpu argument  and add your gpu id to your command.   Otherwise  use --gpu=-1 for CPU.   python run_model.py --gpu=0   <img src=""./imgs/face.PNG"" width=""100%"" alt=""More Cases"">    """;Computer Vision;https://github.com/nahliabdelwahed/Face-Image-Deblurring
"""Clone this project to your machine.   ```bash git clone https://github.com/jiangsutx/SRN-Deblur.git cd SRN-Deblur ```  $ cd CycleGAN/CycleGAN_Code   $ cd CycleGAN/CycleGAN_Code   cd /CycleGAN   cd /CycleGAN   cd /SRN   If you have a GPU  please include --gpu argument  and add your gpu id to your command.   Otherwise  use --gpu=-1 for CPU.   python run_model.py --gpu=0   <img src=""./imgs/face.PNG"" width=""100%"" alt=""More Cases"">    """;General;https://github.com/nahliabdelwahed/Face-Image-Deblurring
"""""";General;https://github.com/Techget/gail-tf-sc2
"""| Shake-Shake(26 2x32d)   | 3.6        | 3.0        | 2.5         | 2.7 / 2.5        | Download |  | Shake-Shake(26 2x96d)   | 2.9        | 2.6        | 2.0         | 2.0 / 2.0        | Download |  | Shake-Shake(26 2x112d)  | 2.8        | 2.6        | 1.9         | 2.0 / 1.9        | Download |  | PyramidNet+ShakeDrop    | 2.7        | 2.3        | 1.5         | 1.8 / 1.7        | Download |   """;Computer Vision;https://github.com/kakaobrain/fast-autoaugment
"""You can use the [`main.py`](main.py) script in order to train the algorithm with MAML. ``` python main.py --env-name 2DNavigation-v0 --num-workers 20 --fast-lr 0.1 --max-kl 0.01 --fast-batch-size 20 --meta-batch-size 40 --num-layers 2 --hidden-size 100 --num-batches 500 --gamma 0.99 --tau 1.0 --cg-damping 1e-5 --ls-max-steps 15 ```  To evaluate the trained agent  just run ``` python experiments.py ``` Both scripts were tested with Python 3.6.   """;General;https://github.com/MoritzTaylor/maml-rl-tf2
"""``` SELFSUPERVISED (Jigsaw++  DeepClustering) | ‚îî‚îÄ‚îÄ‚îÄJigsawNet (Implementation based on Noroozi et al  brattoli git) |   ‚îÇ   train.py - Main Training Script |   ‚îÇ   baseline_conv.py/network_library.py - Basic network implementations |   ‚îÇ   auxiliaries.py - auxiliary functions  including permutation computation. |   ‚îÇ   CelebA_dataset_Jigsaw.py - PyTorch-dataset implementation for CelebA |   | |   ‚îî‚îÄ‚îÄ‚îÄPermutations (Folder of precomputated max. Hamming distance Permutations) |    ‚îî‚îÄ‚îÄ‚îÄDeepClusterNet (Implementation adapted from Repo) |   ‚îÇ   train.py      - Main Training script |   ‚îÇ   clustering.py - Holds clustering and target generation functions. |   ‚îÇ   CelebA_dataset_DeepCluster.py - PyTorch-dataset implementation for CelebA |   ‚îÇ   baseline_conv.py - Basic network implementations (AlexNet  MiniNet) |   ‚îÇ   auxiliaries.py - auxiliary functions to run train.py |    ‚îî‚îÄ‚îÄ‚îÄDatasets (CelebA) ```   """;General;https://github.com/Confusezius/selfsupervised_learning
"""  python     #: Then update estimator    cd examples/correlated_gaussians/   We provide code to showcase the two functionalities we just talked about   A naive differentiation of the previously defined loss_regularized term will probably fail to yield the expected result. A very probable reason for that is that I(X;Z) will most likely have gradients whose scale are much larger than those from the loss_gan. In order to maintain a certain balance in gradients' scale  on must use adaptive gradient clipping proposed in [1]. This simple trick consists in scaling the MI term such that its gradient norm doesn't exceed the one from the original loss. Concretely  one must instead minimize:  ``` scale = min( ||G(loss)||  ||G(I(X;Z))|| ) / ||G(I(X;Z))|| loss_regularized = loss_gan - beta * scale * I(X;Z) ``` where G(.) defines the gradient operator with respect to specified parameters.  We provide a simple example in 2D referred to as ""25 gaussians experiments"" where the target distribution is:  <img src=""https://github.com/mboudiaf/Mutual-Information-Variational-Bounds/blob/master/screens/gan_target.png"" width=""250"">  The simple GAN will produce  with the provided generator and discriminator architecture distributions like:  <img src=""https://github.com/mboudiaf/Mutual-Information-Variational-Bounds/blob/master/screens/gan _noreg.png"" width=""250"">  While the above plot clearly exposes a mode collapse  one can easily reduce this mode collapse by adding a MI regularization:  <img src=""https://github.com/mboudiaf/Mutual-Information-Variational-Bounds/blob/master/screens/gan_mine.png"" width=""250"">  To see the code for this example  first go the example directory ``` cd examples/gan/ ``` Then  run some tests to make sure the code doesn't yield any bug: ``` python3 run_test.py ``` Finally  to run the experiments  you can check all the available options in ""demo_gaussian.py""  and loop over any parameters by modifying the header of run_exp.py. Then simply run: ``` python3 run_exp.py ```    """;General;https://github.com/mboudiaf/Mutual-Information-Variational-Bounds
"""Check `README.md` under `data` for more details.     1. Create a fresh conda environment  and install all dependencies.  ```text conda create -n vilbert-mt python=3.6 conda activate vilbert-mt git clone --recursive https://github.com/facebookresearch/vilbert-multi-task.git cd vilbert-multi-task pip install -r requirements.txt ```  2. Install pytorch ``` conda install pytorch torchvision cudatoolkit=10.0 -c pytorch ```  3. Install apex  follows https://github.com/NVIDIA/apex  4. Install this codebase as a package in this environment. ```text python setup.py develop ```   Download link   Download link   """;Natural Language Processing;https://github.com/jialinwu17/tmpimgs
"""* /Data/ - includes all necessary data for solving task on ParaPhraser + scripts that make files with word embeddings (wordvec_rv_3.txt) and train/test data (train_PP.tsv and test_PP.tsv) * /Word2Vec/ - includes scripts for method Word2Vec + TF-IDF weighting * /BIMPM/ - includes scripts for method BiMPM   Application of NLP methods on russian dataset ParaPhraser for paraphrase identification problem.   """;Natural Language Processing;https://github.com/MariBax/Paraphrase-Identification
"""""";Computer Vision;https://github.com/Dycollapsar/Attention-Based-for-Medicalimaging
"""Check [INSTALL.md](INSTALL.md) for installation instructions which is orginate from [FCOS](https://github.com/tianzhi0549/FCOS#installation)     CUDA v9   Follow the instructions   : specify the number of GPU you can use.   Follow the instructions   """;Computer Vision;https://github.com/vov-net/VoVNet-FCOS
"""bash run-train.sh \[configuration\]   """;Graphs;https://github.com/chalothon/Graph-Convolutional-Networks-for-Relational-Link-Prediction
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/Code-Fight/darknet_study
"""""";General;https://github.com/kekmodel/FixMatch-pytorch
"""Under construction...   Under construction...   """;Sequential;https://github.com/mits58/Pointer-Networks
"""First  use your favorite models to get predictions for the test.txt data. Each prediction file should contain an integer prediction (0-5) for each sentence in test.txt (one per line). See example_pred_file.txt to ensure your file is similar.    ``` python3 analyze_predictions.py [prediction_files] ```  The script will print out the accuracy for each of the categories.   ``` python3 analyze_predictions.py example_pred.txt  Challenge dataset file: annotated.txt Testing predictions from example_pred.txt/ model               pos    neg    mixed    no-sent    spelling    desirable    idioms    strong    negated    w-know    amp.    comp.    irony    shift    emoji    modal    morph.    red.    vocab ----------------  -----  -----  -------  ---------  ----------  -----------  --------  --------  ---------  --------  ------  -------  -------  -------  -------  -------  --------  ------  ------- example_pred.txt   16.0   55.4     14.6        1.0        53.1         44.9      18.8      18.6       30.7      32.4    33.3     33.3     45.8     62.2     72.2     45.7       7.4    15.4     12.7  ```    """;Natural Language Processing;https://github.com/ltgoslo/assessing_and_probing_sentiment
"""Implement Word2Vec with NumPy.   """;General;https://github.com/yurayli/stanford-cs224n-sol
"""1. Create an anaconda environment. ```sh conda create -n=<env_name> python=3.6 anaconda conda activate <env_name> ``` 2. Install PyTorch. ```sh conda install pytorch torchvision cudatoolkit=10.1 -c pytorch ``` 3. Install pip packages. ```sh pip install -r requirements.txt ```   Clone LovaszSoftmax from bermanmaxim/LovaszSoftmax.  git clone https://github.com/bermanmaxim/LovaszSoftmax.git   """;Computer Vision;https://github.com/4uiiurz1/pytorch-nested-unet
"""""";General;https://github.com/autasi/demon_sgd
"""<table><tbody><tr><th align=""left"" bgcolor=#f8f8f8> </th> <td bgcolor=white> darknet weights </td><td bgcolor=white> darknet repo </td><td bgcolor=white> Ours (pytorch) </td><td bgcolor=white> Ours (pytorch) </td></tr>   We provide a Dockerfile to build an environment that meets the above requirements.   $ bash requirements/getcoco.sh   """;Computer Vision;https://github.com/DeNA/PyTorch_YOLOv3
"""""";Reinforcement Learning;https://github.com/alathiya/RL-Quadcoptor-Flying
"""Under construction...   Under construction...   """;Natural Language Processing;https://github.com/mits58/Pointer-Networks
"""more detail in https://tensorflow.google.cn/install   python train.py VERSION RES_BLOCKS   """;Computer Vision;https://github.com/tuanzhangCS/octconv_resnet
"""""";Computer Vision;https://github.com/vitskvara/GenModels.jl
"""""";General;https://github.com/vitskvara/GenModels.jl
"""Makefile fixes with default build supporting all NVIDIA architectures   """;Computer Vision;https://github.com/jolibrain/caffe
"""It is tested with pytorch-1.0.   """;Computer Vision;https://github.com/GOD-GOD-Autonomous-Vehicle/self-pointnet
"""""";General;https://github.com/amitz25/PCCoder
"""See `terraform`. This creates:   - EC2 instance (`p2.xlarge`  with CUDA installed)  - 2 S3 buckets to store training data  initial weights and configs  Additionally for deploying our trained model   - 1 S3 bucket to store images passed to the API  along with your model's predictions  - Lambda which runs our inference  - API Gateway which provides an API interface for the lambda  (and all the necessary security groups / policies etc)  The `setup.sh` script (which should be automatically run  should install the necessary dependencies)  To test      ./darknet  You should get the output:      usage: ./darknet <function>    For this you will need source images of meters. Once collected  I ran the following to give standardised names to the image files:      docker exec -it yolo_yolo_1 bash       On the local machine run      xhost +local:docker      docker-compose up --build  Then  to test  on the running container      cd darknet     ./darknet  You should get the output:      usage: ./darknet <function>  To test the OpenCV/Display run      ./darknet imtest data/eagle.jpg  (you should see eagles - but this will fail if you're running it on a headless EC2 instance.)   find /absolute/path/to/testing/ -name "".jpg"" > /path/to/train.txt   train  = /path/to/train.txt   ./darknet partial /path/to/cfg/spark-counter-yolov3-tiny.cfg /path/to/pretrained.weights pretrained.conv.11 11   Configuration follows as before  with the following changes:   https://pjreddie.com/darknet/install/ (Installation guide for darknet)   https://medium.com/@manivannan_data/how-to-train-yolov3-to-detect-custom-objects-ccbcafeb13d2 (updated version of below for yolov2)   """;Computer Vision;https://github.com/stephenharris/yolo-walkthrough
"""""";Computer Vision;https://github.com/vaibhavjindal/pix2pix-pytorch
"""""";General;https://github.com/vaibhavjindal/pix2pix-pytorch
"""This project was designed for: * Python 3.6 * TensorFlow 1.12.0  Please install requirements & project: ``` $ cd /path/to/project/ $ git clone https://github.com/filippogiruzzi/semantic_segmentation.git $ cd semantic_segmentation/ $ pip3 install -r requirements.txt $ pip3 install -e . --user --upgrade ```   Installation    The project semantic_segmentation/ has the following structure:   ``` $ cd /path/to/project/semantic_segmentation/semseg/ ```   """;Reinforcement Learning;https://github.com/filippogiruzzi/semantic_segmentation
"""This project was designed for: * Python 3.6 * TensorFlow 1.12.0  Please install requirements & project: ``` $ cd /path/to/project/ $ git clone https://github.com/filippogiruzzi/semantic_segmentation.git $ cd semantic_segmentation/ $ pip3 install -r requirements.txt $ pip3 install -e . --user --upgrade ```   Installation    The project semantic_segmentation/ has the following structure:   ``` $ cd /path/to/project/semantic_segmentation/semseg/ ```   """;Computer Vision;https://github.com/filippogiruzzi/semantic_segmentation
"""To do this  you can use the following script: https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh   """;General;https://github.com/d-li14/mobilenetv3.pytorch
"""To do this  you can use the following script: https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh   """;Computer Vision;https://github.com/d-li14/mobilenetv3.pytorch
"""``` pip install git+https://github.com/jonbarron/robust_loss_pytorch ```   git clone https://github.com/jonbarron/robust_loss_pytorch  cd robust_loss_pytorch/  pip install -e .[dev]   To use this code import `lossfun`  or `AdaptiveLossFunction` and call the loss function. `general.py` implements the ""general"" form of the loss  which assumes you are prepared to set and tune hyperparameters yourself  and `adaptive.py` implements the ""adaptive"" form of the loss  which tries to adapt the hyperparameters automatically and also includes support for imposing losses in different image representations. The probability distribution underneath the adaptive loss is implemented in `distribution.py`.  ``` from robust_loss_pytorch import lossfun ```  or  ``` from robust_loss_pytorch import AdaptiveLossFunction ```  A toy example of how this code can be used is in `example.ipynb`.   """;General;https://github.com/jonbarron/robust_loss_pytorch
"""If you'd like to submit a pull request  you'll need to clone the repository;   """;Computer Vision;https://github.com/google-research/google-research
"""If you'd like to submit a pull request  you'll need to clone the repository;   """;General;https://github.com/google-research/google-research
"""""";Computer Vision;https://github.com/iomanker/VQVAE-TF2
"""1. Install <a href=""https://docs.conda.io/en/latest/"">conda</a>. 2. Create and activate conda env ```bash conda create -n date python=3.7 conda activate date ``` 3. Install python packages. ``` pip install -r requirements.txt ```   """;General;https://github.com/mike-a-yen/date-translation
"""""";General;https://github.com/PriyalNarang/Tapas-Experiments
"""tensorflow + GPU   """;Reinforcement Learning;https://github.com/wangyy161/DDPG_CNN_Pendulum_practice
"""1. Disable cudnn batch normalization. Open `torch/nn/functional.py` and find the line with `torch.batch_norm` and replace the `torch.backends.cudnn.enabled` with `False`.  2. Clone this repo:  3. Compile corner pooling.     If you are using pytorch 0.4.1  rename ```$MS_CORNERNET_ROOT/lib/cpool_old``` to ```$MS_CORNERNET_ROOT/lib/cpool```  otherwise rename ```$MS_CORNERNET_ROOT/lib/cpool_new``` to ```$MS_CORNERNET_ROOT/lib/cpool```.     ```     cd $CornerNet_ROOT/lib/cpool     python setup.py install --user     ```  4. Compile NMS.     ```     cd $MS_CORNERNET_ROOT/lib/nms     make     ```  5. For KAIST training  Download KAIST dataset and put data into ```$CornerNet_ROOT/data/kaist/images``` and ```$CornerNet_ROOT/data/kaist/annotations```. Annotations should then be further separated into two directories ```train_sanitized``` and ```test_improved```   """;Computer Vision;https://github.com/egeonat/MS-CornerNet
"""This model consists of 2 generators and 2 discriminators. The two generators as U-net like CNNs. During the evaluation of the model  I directly used the pretrained salient objective detection model from Joker  https://github.com/Joker316701882/Salient-Object-Detection.  This is a project about image style transfer developed by Tao Liang  Tianrui Yu  Ke Han and Yifan Ruan. Our project contains three different models  one is in ""cycle_gan_unet"" directory which uses the u-net like cnn as generators  one is in ""Ukiyoe_codes"" directory which uses Resnet blocks as generators  which uses the model proposed in this paper https://arxiv.org/pdf/1703.10593.pdf  the other is in neural_style_transfer that implement sytle transfer using convolution neural network proposed in this paper https://arxiv.org/pdf/1508.06576.pdf.   Directly run the demo.ipynb notebook. You can see the original image and the transferred image.<br/> If you want to train the model by yourself  delete /baroque and /ukiyo_e directorys. And run the cycle_gan_model.ipynb notebook. You can set all the parameters in the initialization of the experiment class.   single_test.ipynb(for demo use):   run this notebook to show the Ukiyoe-style transfer result of 'test_image.jpg'. Make sure the image  latest_ukiyoe_G_A.pkl and './models' are in their original places<br/>  train.ipynb:  run this notebook to train a cycle-GAN that can transfer 'datasets/trainA' style to 'datasets/trainB' style. Training options can be found and revised in './options/train_options.py' and './options/base_options.py'<br/>  test.ipynb:  run this notebook to test the model in './checkpoints' file. Input the model name in './options/base_options.py'<br/> plot_losses.ipynb:   run this to plot losses given a loss log in './checkpoints'<br/>     Run the Neural_Style_Transfer.ipynb for demo.<br/> The notebook also stores model. If you want to change the network structure  choose one of content_layers_default and style_layers_default each and comment the others. For white noise input  consider decreasing the weight of style loss and increase the number of optimizing steps.     """;Computer Vision;https://github.com/CarpdiemLiang/style_transfer
"""This model consists of 2 generators and 2 discriminators. The two generators as U-net like CNNs. During the evaluation of the model  I directly used the pretrained salient objective detection model from Joker  https://github.com/Joker316701882/Salient-Object-Detection.  This is a project about image style transfer developed by Tao Liang  Tianrui Yu  Ke Han and Yifan Ruan. Our project contains three different models  one is in ""cycle_gan_unet"" directory which uses the u-net like cnn as generators  one is in ""Ukiyoe_codes"" directory which uses Resnet blocks as generators  which uses the model proposed in this paper https://arxiv.org/pdf/1703.10593.pdf  the other is in neural_style_transfer that implement sytle transfer using convolution neural network proposed in this paper https://arxiv.org/pdf/1508.06576.pdf.   Directly run the demo.ipynb notebook. You can see the original image and the transferred image.<br/> If you want to train the model by yourself  delete /baroque and /ukiyo_e directorys. And run the cycle_gan_model.ipynb notebook. You can set all the parameters in the initialization of the experiment class.   single_test.ipynb(for demo use):   run this notebook to show the Ukiyoe-style transfer result of 'test_image.jpg'. Make sure the image  latest_ukiyoe_G_A.pkl and './models' are in their original places<br/>  train.ipynb:  run this notebook to train a cycle-GAN that can transfer 'datasets/trainA' style to 'datasets/trainB' style. Training options can be found and revised in './options/train_options.py' and './options/base_options.py'<br/>  test.ipynb:  run this notebook to test the model in './checkpoints' file. Input the model name in './options/base_options.py'<br/> plot_losses.ipynb:   run this to plot losses given a loss log in './checkpoints'<br/>     Run the Neural_Style_Transfer.ipynb for demo.<br/> The notebook also stores model. If you want to change the network structure  choose one of content_layers_default and style_layers_default each and comment the others. For white noise input  consider decreasing the weight of style loss and increase the number of optimizing steps.     """;General;https://github.com/CarpdiemLiang/style_transfer
"""``` git clone github.com/gtegner/mine-pytorch cd mine-pytorch pip install -e . ```  Some of the code uses Pytorch Lightning [4] for training and evaluation.    MINE relies on a statistics network `T` which takes as input two variables X  Y and estimates the mutual information MI(X Y).  ```python from mine.models.mine import Mine statistics_network = nn.Sequential(     nn.Linear(x_dim + y_dim  100)      nn.ReLU()      nn.Linear(100  100)      nn.ReLU()      nn.Linear(100  1) )  mine = Mine(     T = statistics_network      loss = 'mine' #:mine_biased  fdiv     method = 'concat' )  joint_samples = np.random.multivariate_normal(mu = np.array([0 0])  cov = np.array([[1  0.2]  [0.2  1]]))  X  Y = joint_samples[:  0]  joint_samples[:  1]  mi = mine.optimize(X  Y  iters = 100) ```   """;Computer Vision;https://github.com/gtegner/mine-pytorch
"""                        lmdb database to use for (Required)                           lmdb database to use for testing (Required)                           per gpu   """;Computer Vision;https://github.com/usnistgov/semantic-segmentation-unet
"""""";Computer Vision;https://github.com/rfribeiro/face_detection
"""""";Reinforcement Learning;https://github.com/LuEE-C/PPO-Keras
"""""";General;https://github.com/CIFASIS/splitting_gan
"""""";Computer Vision;https://github.com/CIFASIS/splitting_gan
"""```bash pip install -r requirements.txt ```   """;Computer Vision;https://github.com/catalyst-team/gan
"""* [PyTorch](http://pytorch.org/) version >= 1.5.0 * Python version >= 3.6 * For training new models  you'll also need an NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl) * **To install fairseq** and develop locally:  ``` bash git clone https://github.com/pytorch/fairseq cd fairseq pip install --editable ./  #: on MacOS: #: CFLAGS=""-stdlib=libc++"" pip install --editable ./  #: to install the latest stable release (0.10.x) #: pip install fairseq ```  * **For faster training** install NVIDIA's [apex](https://github.com/NVIDIA/apex) library:  ``` bash git clone https://github.com/NVIDIA/apex cd apex pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" \   --global-option=""--deprecated_fused_adam"" --global-option=""--xentropy"" \   --global-option=""--fast_multihead_attn"" ./ ```  * **For large datasets** install [PyArrow](https://arrow.apache.org/docs/python/install.html#using-pip): `pip install pyarrow` * If you use Docker make sure to increase the shared memory size either with `--ipc=host` or `--shm-size`  as command line options to `nvidia-docker run` .   * May 2020: [Follow fairseq on Twitter](https://twitter.com/fairseq)   * December 2019: [fairseq 0.9.0 released](https://github.com/pytorch/fairseq/releases/tag/v0.9.0)   mixed precision training (trains faster with less GPU memory on NVIDIA tensor cores)   ``` python   Twitter: https://twitter.com/fairseq   The [full documentation](https://fairseq.readthedocs.io/) contains instructions for getting started  training new models and extending fairseq with new model types and tasks.   We provide pre-trained models and pre-processed  binarized test sets for several tasks listed below  as well as example training and evaluation commands.  * [Translation](examples/translation/README.md): convolutional and transformer models are available * [Language Modeling](examples/language_model/README.md): convolutional and transformer models are available  We also have more detailed READMEs to reproduce results from specific papers:  * [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale (Babu et al.  2021)](examples/wav2vec/xlsr/README.md) * [Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al.  2020)](examples/criss/README.md) * [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al.  2020)](examples/wav2vec/README.md) * [Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al.  2020)](examples/unsupervised_quality_estimation/README.md) * [Training with Quantization Noise for Extreme Model Compression ({Fan*  Stock*} et al.  2020)](examples/quant_noise/README.md) * [Neural Machine Translation with Byte-Level Subwords (Wang et al.  2020)](examples/byte_level_bpe/README.md) * [Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at.  2020)](examples/mbart/README.md) * [Reducing Transformer Depth on Demand with Structured Dropout (Fan et al.  2019)](examples/layerdrop/README.md) * [Jointly Learning to Align and Translate with Transformer Models (Garg et al.  2019)](examples/joint_alignment_translation/README.md) * [Levenshtein Transformer (Gu et al.  2019)](examples/nonautoregressive_translation/README.md) * [Facebook FAIR's WMT19 News Translation Task Submission (Ng et al.  2019)](examples/wmt19/README.md) * [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al.  2019)](examples/roberta/README.md) * [wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al.  2019)](examples/wav2vec/README.md) * [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al.  2019)](examples/translation_moe/README.md) * [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al.  2019)](examples/pay_less_attention_paper/README.md) * [Understanding Back-Translation at Scale (Edunov et al.  2018)](examples/backtranslation/README.md) * [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al.  2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel) * [Hierarchical Neural Story Generation (Fan et al.  2018)](examples/stories/README.md) * [Scaling Neural Machine Translation (Ott et al.  2018)](examples/scaling_nmt/README.md) * [Convolutional Sequence to Sequence Learning (Gehring et al.  2017)](examples/conv_seq2seq/README.md) * [Language Modeling with Gated Convolutional Networks (Dauphin et al.  2017)](examples/language_model/README.conv.md)   """;Natural Language Processing;https://github.com/pytorch/fairseq
"""   ```python      ```python    """;General;https://github.com/chizhu/BDC2019
"""* [PyTorch](http://pytorch.org/) version >= 1.5.0 * Python version >= 3.6 * For training new models  you'll also need an NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl) * **To install fairseq** and develop locally:  ``` bash git clone https://github.com/pytorch/fairseq cd fairseq pip install --editable ./  #: on MacOS: #: CFLAGS=""-stdlib=libc++"" pip install --editable ./  #: to install the latest stable release (0.10.x) #: pip install fairseq ```  * **For faster training** install NVIDIA's [apex](https://github.com/NVIDIA/apex) library:  ``` bash git clone https://github.com/NVIDIA/apex cd apex pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" \   --global-option=""--deprecated_fused_adam"" --global-option=""--xentropy"" \   --global-option=""--fast_multihead_attn"" ./ ```  * **For large datasets** install [PyArrow](https://arrow.apache.org/docs/python/install.html#using-pip): `pip install pyarrow` * If you use Docker make sure to increase the shared memory size either with `--ipc=host` or `--shm-size`  as command line options to `nvidia-docker run` .   * May 2020: [Follow fairseq on Twitter](https://twitter.com/fairseq)   * December 2019: [fairseq 0.9.0 released](https://github.com/pytorch/fairseq/releases/tag/v0.9.0)   mixed precision training (trains faster with less GPU memory on NVIDIA tensor cores)   ``` python   Twitter: https://twitter.com/fairseq   The [full documentation](https://fairseq.readthedocs.io/) contains instructions for getting started  training new models and extending fairseq with new model types and tasks.   We provide pre-trained models and pre-processed  binarized test sets for several tasks listed below  as well as example training and evaluation commands.  * [Translation](examples/translation/README.md): convolutional and transformer models are available * [Language Modeling](examples/language_model/README.md): convolutional and transformer models are available  We also have more detailed READMEs to reproduce results from specific papers:  * [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale (Babu et al.  2021)](examples/wav2vec/xlsr/README.md) * [Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al.  2020)](examples/criss/README.md) * [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al.  2020)](examples/wav2vec/README.md) * [Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al.  2020)](examples/unsupervised_quality_estimation/README.md) * [Training with Quantization Noise for Extreme Model Compression ({Fan*  Stock*} et al.  2020)](examples/quant_noise/README.md) * [Neural Machine Translation with Byte-Level Subwords (Wang et al.  2020)](examples/byte_level_bpe/README.md) * [Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at.  2020)](examples/mbart/README.md) * [Reducing Transformer Depth on Demand with Structured Dropout (Fan et al.  2019)](examples/layerdrop/README.md) * [Jointly Learning to Align and Translate with Transformer Models (Garg et al.  2019)](examples/joint_alignment_translation/README.md) * [Levenshtein Transformer (Gu et al.  2019)](examples/nonautoregressive_translation/README.md) * [Facebook FAIR's WMT19 News Translation Task Submission (Ng et al.  2019)](examples/wmt19/README.md) * [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al.  2019)](examples/roberta/README.md) * [wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al.  2019)](examples/wav2vec/README.md) * [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al.  2019)](examples/translation_moe/README.md) * [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al.  2019)](examples/pay_less_attention_paper/README.md) * [Understanding Back-Translation at Scale (Edunov et al.  2018)](examples/backtranslation/README.md) * [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al.  2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel) * [Hierarchical Neural Story Generation (Fan et al.  2018)](examples/stories/README.md) * [Scaling Neural Machine Translation (Ott et al.  2018)](examples/scaling_nmt/README.md) * [Convolutional Sequence to Sequence Learning (Gehring et al.  2017)](examples/conv_seq2seq/README.md) * [Language Modeling with Gated Convolutional Networks (Dauphin et al.  2017)](examples/language_model/README.conv.md)   """;Sequential;https://github.com/pytorch/fairseq
"""Official ScanNet dataset can be downloaded [here](http://www.scan-net.org/).  If you choose training without grid sampling  you need firstly run `ScanNet/prepare_scannet.py`  otherwise you can skip to training step.  Clone the repository: ``` git clone https://github.com/yanx27/PointASNL.git ```  Installation instructions for Ubuntu 16.04 (available at **CUDA10**):       * Make sure <a href=""https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html"">CUDA</a>  and <a href=""https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html"">cuDNN</a> are installed. Only this configurations has been tested:       - Python 3.6.9  TensorFlow 1.13.1  CUDA 10.1   * Follow <a href=""https://www.tensorflow.org/install/pip"">Tensorflow installation procedure</a>.             * Compile the customized Tensorflow operators by `sh complile_op.sh`.  N.B. If you installed Tensorflow in a virtual environment  it needs to be activated when running these scripts    $ cd ScanNet/   $ cd ScanNet/   $ cd ScanNet/   $ cd ScanNet/   $ cd SemanticKITTI/   $ cd SemanticKITTI/   """;Computer Vision;https://github.com/yanx27/PointASNL
"""    i.save(inp_dir/fn.name)   """;Computer Vision;https://github.com/Brainkite/Sketch_Augmented
"""    i.save(inp_dir/fn.name)   """;General;https://github.com/Brainkite/Sketch_Augmented
"""- Clone the repository  - `git clone https://github.com/ChielBruin/ros_faster_rcnn.git --recursive`  - run `git submodule update --init --recursive`  when the modules are not correctly cloned - Install py-faster-rcnn located in the libraries folder  - Follow the guide provided [here](https://github.com/rbgirshick/py-faster-rcnn#installation-sufficient-for-the-demo)  - If you are running Ubuntu 15 or 16  check [this](https://gist.github.com/wangruohui/679b05fcd1466bb0937f) or [this](https://github.com/BVLC/caffe/wiki/Ubuntu-16.04-or-15.10-Installation-Guide) guide (respectively) for the installation of the caffe dependency - Install all the needed ROS dependencies  - run `rosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO`  where $ROS_DISTRO is your desired version of ROS    """;Computer Vision;https://github.com/lukewenMX/RCNN_simple_detection
"""See [installation instructions](https://detectron2.readthedocs.io/tutorials/install.html).   See [Getting Started with Detectron2](https://detectron2.readthedocs.io/tutorials/getting_started.html)  and the [Colab Notebook](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5) to learn about basic usage.  Learn more at our [documentation](https://detectron2.readthedocs.org). And see [projects/](projects/) for some projects that are built on top of detectron2.   """;Computer Vision;https://github.com/facebookresearch/detectron2
"""""";Graphs;https://github.com/susurrant/flow-imputation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/brightmart/bert_customized
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/brightmart/bert_customized
"""* [5]https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a   visualise image that can maximise one class score   """;Computer Vision;https://github.com/RuoyuGuo/Visualising-Image-Classification-Models-and-Saliency-Maps
"""```r install.packages(""PythonEmbedInR""  repos=c(""http://cran.fhcrc.org""  ""http://ran.synapse.org"")) #: (Use your favorite CRAN mirror above.  See https://cran.r-project.org/mirrors.html for a list of available mirrors.) ```    This package is a modification of PythonInR which embeds a private copy of Python  isolated from any Python installation that might be on the host system. The documentation of the original package follows.   To allow a nearly one to one conversion from R to Python  PythonInR provides   NOTE PythonInR:::pySetSimple   | Python      | R                    | simplify     |   NOTE pyGetPoly   NOTE (bytes):   | pyExit           | Close Python                                       | pyExit()                                                           |   | pyGet            | Get a Python variable                              | pyGet('myPythonVariable')                                          |  | pyGet0           | Get a Python variable                              | pyGet0('myPythonVariable')                                         |   | pyVersion        | Returns the version of Python                      | pyVersion()                                                        |   * The Python package is available on Python search path   : build the .Rd files     setGeneric(name  def)   : package environment.  .NAMESPACE <- environment()       if (any(x$name == toOmit)) {   Note that we do not want the following from the synapseclient.entity module:       if (any(class$name == classToSkip)) {               if (any(x$name == methodsToOmit)) NULL else x;     syn.get()   Note that:   In this example  I will demonstrate how we generate the [synapser](https://github.com/Sage-Bionetworks/synapser) and the [synapserutils](https://github.com/Sage-Bionetworks/synapserutils) packages by wrapping the Python package [synapsePythonClient](https://github.com/Sage-Bionetworks/synapsePythonClient).  The `synapseutils` module in `synapsePythonClient` package has the following structure: ```     module: synapseutils         module: copy             function: copy             function: copyWiki             function: copyFileHandles         module: sync             function: syncToSynapse             function: syncFromSynapse         module: monitor             function: notifyMe ```   """;Natural Language Processing;https://github.com/Sage-Bionetworks/PythonEmbedInR
"""Requirements: Pytorch 1.4 and python 3.7    """;Computer Vision;https://github.com/feizc/Object-Detection-Pytorch
"""""";Graphs;https://github.com/ninoxjy/graph-embedding
"""<a name='vgg'></a>   <a name='googlenet'></a>   <a name='densenet'></a>   <a name='inceptionv3'></a>   <a name='inceptionv4'></a>   <a name='mobilenet'></a>   <a name='mobilenetv2'></a>   <a name='squeezenet'></a>   <a name='shufflenet'></a>   <a name='condensenet'></a>   <a name='xception'></a>   <a name='preactresnet'></a>   <a name='resattnet'></a>   <a name='polynet'></a>   <a name='pyramidnet'></a>   ```bash $ python main.py      --nets={NETS}      --batch_size={BATCH_SIZE}      --lr={LEARNING_RATE}      --epochs={EPOCHS} ```   """;Computer Vision;https://github.com/marload/ConvNets-TensorFlow2
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/Zehui127/SQUAD_BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/Zehui127/SQUAD_BERT
"""<a name='vgg'></a>   <a name='googlenet'></a>   <a name='densenet'></a>   <a name='inceptionv3'></a>   <a name='inceptionv4'></a>   <a name='mobilenet'></a>   <a name='mobilenetv2'></a>   <a name='squeezenet'></a>   <a name='shufflenet'></a>   <a name='condensenet'></a>   <a name='xception'></a>   <a name='preactresnet'></a>   <a name='resattnet'></a>   <a name='polynet'></a>   <a name='pyramidnet'></a>   ```bash $ python main.py      --nets={NETS}      --batch_size={BATCH_SIZE}      --lr={LEARNING_RATE}      --epochs={EPOCHS} ```   """;General;https://github.com/marload/ConvNets-TensorFlow2
"""NVIDIA GPUÔºàVRAM >= 32 GBÔºâ  CUDA + cuDNN   paddlepaddle-gpu==0.0.0Ôºànightly build ÁâàÊú¨Ôºâ   scipy = 1.6.2  h5py = 3.2.1  imageio = 2.9.0   ‚îú‚îÄ‚îÄ requirements.txt           #: È°πÁõÆÁöÑÂÖ∂ÂÆÉ‰æùËµñ   | ÊîØÊåÅÁ°¨‰ª∂ | GPU„ÄÅCPUÔºàRAM >= 16 GBÔºâ |   """;Computer Vision;https://github.com/GXU-GMU-MICCAI/PGAN-Paddle
"""It is recommended to use a virtual env before installing this  to avoid conflicting with other installed packages. Anaconda and Python offer virtual environment systems.  Clone the repository and cd into it:   ``` git clone https://github.com/jfpettit/flare.git cd flare ```  **The next step depends on your package manager.**  If you are using pip  pip install the ```requirements``` file:  ``` pip install -r requirements.txt ```  Alternatively  if you're using Anaconda  create a new Anaconda env from the ```environments.yml``` file  and activate your new conda environment:  ``` conda env create -f environment.yml conda activate flare ```  A third option  if you don't want to clone a custom environment or run through the ```requirements.txt``` file  is to simply pip install the repository via:  ``` pip install -e git+https://github.com/jfpettit/flare.git@98d6d3e74dfadc458b1197d995f6d60ef516f1ee#:egg=flare ```   **MPI parallelization will soon be removed. Work is being done to rebase the code using [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) which uses PyTorch's multiprocessing under the hood.**  **Flare supports parallelization via MPI!** So  you'll need to install [OpenMPI](https://www.open-mpi.org/) to run this code. [SpinningUp](https://spinningup.openai.com/en/latest/user/installation.html#installing-openmpi) provides the following installation instructions:   sudo apt-get update &amp;&amp; sudo apt-get install libopenmpi-dev  brew install openmpi  If using homebrew doesn't work for you  consider these instructions.  If you're on Windows  here is a link to some instructions.   If you wish to build your own actor-critic from scratch  then it is recommended to use the FireActorCritic as a template.   """;Reinforcement Learning;https://github.com/jfpettit/flare
"""This repo uses my pytorch implementation of the dtcwt: `pytorch_wavelets`__. You can install this however just by pip installing the requirements.txt. From the command line  the following 3 commands will allow you to run the experiments:  .. code::       git clone https://github.com/fbcotter/scatnet_learn     pip install -r requirements.txt     pip install .  __ https://github.com/fbcotter/pytorch_wavelets   the following code:   # A standard scatlayer expands the channel input from C to 7C - one    """;Computer Vision;https://github.com/fbcotter/scatnet_learn
"""""";General;https://github.com/sammyamajumdar/ResNet50
"""""";Computer Vision;https://github.com/sammyamajumdar/ResNet50
"""If you'd like to submit a pull request  you'll need to clone the repository;   """;Natural Language Processing;https://github.com/google-research/google-research
"""1. Install Python 3 2. `pip install virtualenv` 3. `make all`   """;General;https://github.com/rahulmadanahalli/manifold_mixup
"""""";Computer Vision;https://github.com/jarrydmartinx/generative-models
"""""";General;https://github.com/jarrydmartinx/generative-models
"""```bash pip install -r requirements.txt ```   """;General;https://github.com/catalyst-team/gan
"""The data set could found [Allen Institute for AI ARC](https://leaderboard.allenai.org/arc/submissions/public). The dataset contains 7 787 natural grade-school level multiple-choice SCIENCE questions. This dataset's level of difficulty requires far more powerful knowledge and reasoning capability than ever before datasets such SQuAD or SNLI. The data set has two partitions: EASY Set and CHALLENGE Set. And inside each set  it is also devided into train  test and development sets. A corpus is also given in the dataset which could be used as background inforamtion source. But the ARC challenge is not limited to this corpus knowledge and it could also be open book.  <b> Easy: </b>   Easy-Train Set: 2251 questions   Easy-Test Set: 2376 questions   Easy-Development Set: 570 questions    <b> Challenge: </b>   The Challenge Set contains only questions answer incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm.    Challenge-Train Set: 1119 questions   Challenge-Test Set: 1172 questions   Challenge-Development Set: 299 questions    <b> Reference: </b>   P. Clark  I. Cowhey  O. Etzioni  T. Khot  A. Sabharwal  C. Schoenick  and O. Tafjord. 2018. Think you have solved question answering? Try ARC  the AI2 reasoning challenge. CoRR  abs/1803.05457.  <b> Example: </b>   EASY: Which technology was developed most recently?   &nbsp; &nbsp; A. cellular telephone(correct)   &nbsp; &nbsp; B. television   &nbsp; &nbsp; C. refrigerator   &nbsp; &nbsp; D. airplane    CHALLENGE: Which technology was developed most recently?   &nbsp; &nbsp; A. cellular telephone   &nbsp; &nbsp; B. television   &nbsp; &nbsp; C. refrigerator   &nbsp; &nbsp; D. airplane (correct)   1. T5 model: t5_test.ipynb and t5_ARC.ipynb  2. BERT baseline model: arc_easy_BERT_base_model.ipynb and arc_challenge_BERT_base_model.ipynb  3. RoBERTa-base without/without knowl-edge: LSH_attention.ipynb  4. Report for the project: CSE_576_2020Spring_Project_ARC.pdf   """;Natural Language Processing;https://github.com/duanchi1230/NLP_Project_AI2_Reasoning_Challenge
"""""";Graphs;https://github.com/2myeonggyu/Graph-Embedding
"""""";General;https://github.com/aj9011/Wide-and-Deep
"""  Name | Packages    Name | Packages   Name | Packages   Github Repository : https://github.com/scikit-learn/scikit-learn  Installation   : Pip  pip install -U scikit-learn  : Conda  onda install scikit-learn   Installation    : Pip  pip install statsmodels   : Conda  conda install -c conda-forge statsmodels   Installation    : Pip  pip install pygam  : Conda  conda install -c conda-forge pygam   Installation    pip install -U skater   pip3 install --upgrade tensorflow   sudo pip install keras  pip install -U skater   conda install gxx_linux-64  pip3 install --upgrade tensorflow   sudo pip install keras  sudo pip install -U --no-deps --force-reinstall --install-option=""--rl=True"" skater==1.1.1b1  : Conda  conda install -c conda-forge Skater   Installation    : Pip  pip install pdpbox   Installation    : Pip  pip install lime   Installation   : Pip  pip install pycebox   Installation   : Pip  pip install git+git://github.com/christophM/rulefit.git   Github Repository : https://github.com/scikit-learn-contrib/skope-rules  Installation    : Pip  pip install skope-rules   Installation   : Pip  pip install alibi   Github Repository : https://github.com/kohpangwei/influence-release   Installation   : Pip  pip install shap  : Conda  conda install -c conda-forge shap   """;General;https://github.com/TooTouch/WhiteBox-Part2
"""Armstrong ‚Äî 'That's one small step for a man  one giant leap for mankind'.   Links to Download:   Alternate Links to Download:   Version 1.0  released on 17/11/2017.   """;Computer Vision;https://github.com/curto2/c
"""```bash $ pip install linear-attention-transformer ```   Language model  ```python import torch from linear_attention_transformer import LinearAttentionTransformerLM  model = LinearAttentionTransformerLM(     num_tokens = 20000      dim = 512      heads = 8      depth = 1      max_seq_len = 8192      causal = True                   #: auto-regressive or not     ff_dropout = 0.1                #: dropout for feedforward     attn_layer_dropout = 0.1        #: dropout right after self-attention layer     attn_dropout = 0.1              #: dropout post-attention     emb_dim = 128                   #: embedding factorization  to save on memory     dim_head = 128                  #: be able to fix the dimension of each head  making it independent of the embedding dimension and the number of heads     blindspot_size = 64             #: this gives the q(kv) attention a blindspot of 64 tokens back in the causal case  but gives back an order of magnitude return in memory savings. should be paired with local attention of at least a window size of this setting. setting this to 1 will allow for full q(kv) attention of past     n_local_attn_heads = 4          #: number of local attention heads for (qk)v attention. this can be a tuple specifying the exact number of local attention heads at that depth     local_attn_window_size = 128    #: receptive field of the local attention     reversible = True               #: use reversible nets  from Reformer paper     ff_chunks = 2                   #: feedforward chunking  from Reformer paper     ff_glu = True                   #: use GLU variant for feedforward     attend_axially = False          #: will fold the sequence by the local attention window size  and do an extra strided attention followed by a feedforward with the cheap q(kv) attention     shift_tokens = True             #: add single token shifting  for great improved convergence ).cuda()  x = torch.randint(0  20000  (1  8192)).cuda() model(x) #: (1  8192  512) ```  Transformer  ```python import torch from linear_attention_transformer import LinearAttentionTransformer  model = LinearAttentionTransformer(     dim = 512      heads = 8      depth = 1      max_seq_len = 8192      n_local_attn_heads = 4 ).cuda()  x = torch.randn(1  8192  512).cuda() model(x) #: (1  8192  512) ```  Encoder / decoder  ```python import torch from linear_attention_transformer import LinearAttentionTransformerLM  enc = LinearAttentionTransformerLM(     num_tokens = 20000      dim = 512      heads = 8      depth = 6      max_seq_len = 4096      reversible = True      n_local_attn_heads = 4      return_embeddings = True ).cuda()  dec = LinearAttentionTransformerLM(     num_tokens = 20000      dim = 512      heads = 8      depth = 6      causal = True      max_seq_len = 4096      reversible = True      receives_context = True      n_local_attn_heads = 4 ).cuda()  src = torch.randint(0  20000  (1  4096)).cuda() src_mask = torch.ones_like(src).bool().cuda()  tgt = torch.randint(0  20000  (1  4096)).cuda() tgt_mask = torch.ones_like(tgt).bool().cuda()  context = enc(src  input_mask = src_mask) logits = dec(tgt  context = context  input_mask = tgt_mask  context_mask = src_mask) ```   """;General;https://github.com/lucidrains/linear-attention-transformer
"""""";General;https://github.com/karenacorn99/explore-bert
"""""";Natural Language Processing;https://github.com/karenacorn99/explore-bert
"""minimal requirements  can be installed with pip in a python3 virtualenv (pip install -r requirements.txt)   """;Audio;https://github.com/adrienchaton/BERGAN
"""jupyter notebook classifier.ipynb  We note that advertisements extracted from street level imagery are   """;Computer Vision;https://github.com/gjp1203/LIV360SV
"""minimal requirements  can be installed with pip in a python3 virtualenv (pip install -r requirements.txt)   """;General;https://github.com/adrienchaton/BERGAN
"""| python | 3.8             |  | GPU    | RTX3090          |   | Cuda   | 11.2            |   git clone https://github.com/JunnYu/paddle_mpnet   cd paddlenlp   pip install -r requirements.txt  pip install -e .   cd ..   ÔºàË∂ÖÂèÇÊï∞ÈÅµÂæ™ÂéüËÆ∫ÊñáÁöÑ‰ªìÂ∫ì https://github.com/microsoft/MPNet/blob/master/MPNet/README.glue.mdÔºâ   cd task/glue  : ËøêË°åËÆ≠ÁªÉ(ÂÖ∂‰ªñÁöÑÂëΩ‰ª§Êü•Áúãtask/glue/train.sh)       --device gpu   ÈìæÊé•Ôºöhttps://pan.baidu.com/s/1slFx1fgaF0ifoCXG7Lv9mw    cd task/glue   ÈìæÊé•Ôºöhttps://pan.baidu.com/s/16W8JN0KsGRc84zyqo2kLVw    ÈìæÊé•Ôºöhttps://pan.baidu.com/s/1S55oc4maYOUa5e1vFw7d4w    ÈìæÊé•Ôºöhttps://pan.baidu.com/s/1e2vLAnSMZ4s28q0J_eynwg    """;Natural Language Processing;https://github.com/JunnYu/paddle-mpnet
"""Generative Adversarial Networks (GANs) are one of the most interesting ideas in computer science today. Here two models named Generator and Discriminator are trained simultaneously. As the name says Generator generates the fake images or we can say it generates a random noise and the Discriminator job is to classify whether the image is fake or not. Here the only job of Generator is to fake the Discriminator. In this project we are using DCGAN(Deep Convolutional Generative Adversarial Network). A DCGAN is a direct extension of the GAN described above  except that it explicitly uses convolutional and convolutional-transpose layers in the discriminator and generator  respectively. DCGANs actually comes under Unsupervised Learning and was first described by Radford et. al. in the paper Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks.  ![](https://miro.medium.com/max/2850/1*Mw2c3eY5khtXafe5W-Ms_w.jpeg)   """;Computer Vision;https://github.com/hunnurjirao/DCGAN
"""make build_images   make run_fine_tuning_sentiment   make run_test_sentiment   make run_inference_sentiment   make deploy_model     --task_name TEXT           The name of the task to train.  [required]                                              name=""input_ids"")                                          name=""input_mask"")                                           name=""segment_ids"")           name=""probabilities""   make build_images   make run_fine_tuning_sentiment   make run_load_model_to_train   make run_test_sentiment   make deploy_modelÔºåÈúÄÊ≥®ÊÑèSERVING_DIRÂÖßÁöÑÊ®°ÂûãÁâàÊú¨ËôüÊòØÂê¶ÊúâË°ùÁ™Å           name: 'sentiment'    ÈÉ®Â±¨Ë®≠ÂÆöÊ™î: /serving/models/models.config   - Á¢∫Ë™çË≥áÊñôÈõÜ(dataset/old_sentiment/)ÊòØÂê¶Â≠òÂú®Ôºå‰∏îÂåÖÂê´train.tsv„ÄÅtest.tsv   """;General;https://github.com/s9891326/Fine-Tuning-BERT
"""make build_images   make run_fine_tuning_sentiment   make run_test_sentiment   make run_inference_sentiment   make deploy_model     --task_name TEXT           The name of the task to train.  [required]                                              name=""input_ids"")                                          name=""input_mask"")                                           name=""segment_ids"")           name=""probabilities""   make build_images   make run_fine_tuning_sentiment   make run_load_model_to_train   make run_test_sentiment   make deploy_modelÔºåÈúÄÊ≥®ÊÑèSERVING_DIRÂÖßÁöÑÊ®°ÂûãÁâàÊú¨ËôüÊòØÂê¶ÊúâË°ùÁ™Å           name: 'sentiment'    ÈÉ®Â±¨Ë®≠ÂÆöÊ™î: /serving/models/models.config   - Á¢∫Ë™çË≥áÊñôÈõÜ(dataset/old_sentiment/)ÊòØÂê¶Â≠òÂú®Ôºå‰∏îÂåÖÂê´train.tsv„ÄÅtest.tsv   """;Natural Language Processing;https://github.com/s9891326/Fine-Tuning-BERT
"""""";General;https://github.com/chuk-yong/ELU-vs-ReLU-for-Image-Recognition
"""""";Computer Vision;https://github.com/zhuoyang125/simple_classifier
"""""";Computer Vision;https://github.com/JensSettelmeier/EfficientDet-DeepSORT-Tracker
"""Deep reinforcement learning has made significant strides in recent years  with results achieved in board games such as Go. However  there are a number of obstacles preventing such methods from being applied to more real-world situations. For instance  more realistic strategic situations often involve much larger spaces of possible states and actions  an environment state which is only partially observed  multiple agents to control  and a necessity for long-term strategies involving not hundreds but thousands or tens of thousands of steps. It has thus been suggested that creating learning algorithms which outperform humans in playing real-time strategy (RTS) video games would signal a more generalizable result about the ability of a computer to make decisions in the real world.  Of the current RTS games on the market  StarCraft II is one of the most popular. The recent release by Google‚Äôs DeepMind of SC2LE (StarCraft II Learning Environment) presents an interface with which to train deep reinforcement learners to compete in the game  both in smaller ‚Äúminigames‚Äù and on full matches. The SC2LE environment is described on [DeepMind's github repo.](https://github.com/deepmind/pysc2)   In this project  we focus on solving a variety of minigames  which capture various aspects of the full StarCraft II game. These minigames focus on tasks such as gathering resources  moving to waypoints  finding enemies  or skirmishing with units. In each case the player is given a homogeneous set of units (marines)  and a reward is based off the minigame (+5 for defeating each enemy roach in DefeatRoaches  for example).   To get started  follow the instructions on the [pysc2 repository](https://github.com/deepmind/pysc2). As described in their instructions  make sure that the environment is set up correctly by running:  ``` $ python -m pysc2.bin.agent --map Simple64 ```  Our project relies on a few more packages  that can be installed by running:  ``` $ pip install -r requirements.txt ```  We have tested our project using python 3 and pysc2 version 1.2  which is the main version currently available.  We are currently training our agents on a google cloud instance with a 4 core CPU and two Tesla K80 GPUs. This configuration might evolve during the project.   """;Reinforcement Learning;https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames
"""Deep reinforcement learning has made significant strides in recent years  with results achieved in board games such as Go. However  there are a number of obstacles preventing such methods from being applied to more real-world situations. For instance  more realistic strategic situations often involve much larger spaces of possible states and actions  an environment state which is only partially observed  multiple agents to control  and a necessity for long-term strategies involving not hundreds but thousands or tens of thousands of steps. It has thus been suggested that creating learning algorithms which outperform humans in playing real-time strategy (RTS) video games would signal a more generalizable result about the ability of a computer to make decisions in the real world.  Of the current RTS games on the market  StarCraft II is one of the most popular. The recent release by Google‚Äôs DeepMind of SC2LE (StarCraft II Learning Environment) presents an interface with which to train deep reinforcement learners to compete in the game  both in smaller ‚Äúminigames‚Äù and on full matches. The SC2LE environment is described on [DeepMind's github repo.](https://github.com/deepmind/pysc2)   In this project  we focus on solving a variety of minigames  which capture various aspects of the full StarCraft II game. These minigames focus on tasks such as gathering resources  moving to waypoints  finding enemies  or skirmishing with units. In each case the player is given a homogeneous set of units (marines)  and a reward is based off the minigame (+5 for defeating each enemy roach in DefeatRoaches  for example).   To get started  follow the instructions on the [pysc2 repository](https://github.com/deepmind/pysc2). As described in their instructions  make sure that the environment is set up correctly by running:  ``` $ python -m pysc2.bin.agent --map Simple64 ```  Our project relies on a few more packages  that can be installed by running:  ``` $ pip install -r requirements.txt ```  We have tested our project using python 3 and pysc2 version 1.2  which is the main version currently available.  We are currently training our agents on a google cloud instance with a 4 core CPU and two Tesla K80 GPUs. This configuration might evolve during the project.   """;General;https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames
"""```bash #: create a new environment: python3 -m venv env               #: Create a virtual environment source env/bin/activate           #: Activate virtual environment  #: step 1: install COCO API: #: Note: COCO API requires numpy to install. Ensure that you have numpy installed. #: e.g. pip install numpy pip install 'git+https://github.com/cocodataset/cocoapi.git#:subdirectory=PythonAPI'  #: step 2: install Fashionpedia API via pip pip install fashionpedia   #: step 3: test. You can test that you have correctly installed the fashionpedia api #:       by running the following command inside the repo. python test.py  #: do your work ...  deactivate  #: exit ```   Clone the repo first and then do the following steps inside the repo:   python3 -m venv env               #: Create a virtual environment  source env/bin/activate           #: Activate virtual environment  : step 1: install COCO API:  : Note: COCO API requires numpy to install. Ensure that you have numpy installed.  : e.g. pip install numpy  pip install 'git+https://github.com/cocodataset/cocoapi.git#:subdirectory=PythonAPI'  : step 2: install required packages  pip install -r requirements.txt  : step 3: test. You can test that you have correctly installed the fashionpedia api  :       by running the following command inside the repo.   For the task of instance segmentation with attribute localization  we present a strong baseline model named Attribute-Mask R-CNN that is built upon [Mask R-CNN](<https://arxiv.org/abs/1703.06870>) for Fashionpedia. Check out our [predictior demo](https://github.com/KMnP/fashionpedia-api/blob/master/baseline_predictor_demo.ipynb) and [paper](<https://arxiv.org/abs/2004.12276>) for more details.   ![baseline](images/baseline.png)     """;Computer Vision;https://github.com/KMnP/fashionpedia-api
"""         python train.py -c 2 -p your_project_name --batch_size 8 --lr 1e-5 \      --load_weights last \      --head_only True                 pip install pycocotools numpy opencv-python tqdm tensorboard tensorboardX pyyaml webcolors     pip install torch==1.4.0     pip install torchvision==0.5.0            official git version: https://github.com/google/automl/commit/006668f2af1744de0357ca3d400527feaa73c122   [2020-04-14] for those who needs help or can't get a good result after several epochs  check out this tutorial. You can run it on colab with GPU support.   Check out this tutorial if you are new to this. You can run it on colab with GPU support.    -w /path/to/your/weights   If you like this repository  or if you'd like to support the author for any reason  you can donate to the author. Feel free to send me your name or introducing pages  I will make sure your name(s) on the sponsors list.    Sincerely thank you for your generosity.            datasets/         -coco2017/             -train2017/                 -000000000001.jpg                 -000000000002.jpg                 -000000000003.jpg             -val2017/                 -000000000004.jpg                 -000000000005.jpg                 -000000000006.jpg             -annotations                 -instances_train2017.json                 -instances_val2017.json       project_name: coco     train_set: train2017     val_set: val2017     num_gpus: 4       obj_list: ['person'  'bicycle'  'car'  ...]                 python train.py -c 2 -p your_project_name --batch_size 8 --lr 1e-5 \      --load_weights /path/to/your/weights/efficientdet-d2.pth \      --head_only True                python train.py -c 2 -p your_project_name --batch_size 8 --lr 1e-5 --debug True           """;Computer Vision;https://github.com/Manugoyal12345/Yet-Another-EfficientDet-Pytorch
"""![Block](imgs/diagonal_illustration.png)  AdaHessian is a second order based optimizer for the neural network training based on PyTorch. The library supports the training of convolutional neural networks ([image_classification](https://github.com/amirgholami/adahessian/tree/master/image_classification)) and transformer-based models ([transformer](https://github.com/amirgholami/adahessian/tree/master/transformer)). Our TensorFlow implementation is [adahessian_tf](https://github.com/amirgholami/adahessian/tree/master/adahessian_tf).  Please see [this paper](https://arxiv.org/pdf/2006.00719.pdf) for more details on the AdaHessian algorithm.  For more details please see:  - [Video explanation of AdaHessian](https://www.youtube.com/watch?v=S87ancnZ0MM) - [AdaHessian paper](https://arxiv.org/pdf/2006.00719.pdf).   If you are interested to install the library through pip  then we recommend doing so through pytorch-optimizer package as follows:  ``` $ pip install torch_optimizer ```  ```python import torch_optimizer as optim  #: model = ... optimizer = optim.Adahessian(     m.parameters()      lr= 1.0      betas= (0.9  0.999)     eps= 1e-4      weight_decay=0.0      hessian_power=1.0  )       loss_fn(m(input)  target).backward(create_graph = True) #: create_graph=True is necessary for Hessian calculation optimizer.step() ```   Please first clone the AdaHessian library to your local system: ``` git clone https://github.com/amirgholami/adahessian.git ``` You can import the optimizer as follows:  ```python from optim_adahessian import Adahessian ... model = YourModel() optimizer = Adahessian(model.parameters()) ... for input  output in data:   optimizer.zero_grad()   loss = loss_function(output  model(input))   loss.backward(create_graph=True)  #: You need this line for Hessian backprop   optimizer.step() ... ```  Please note that the optim_adahessian is in the image_classification folder. We also have adapted the Adahessian implementation to be compatible with fairseq repo  which can be used for NLP tasks. This is the [link](https://github.com/amirgholami/adahessian/blob/master/transformer/fairseq/optim/adahessian.py) to that version  which can be found in transformer folder.   """;General;https://github.com/amirgholami/adahessian
"""Note that the following models are with bias wd = 0.   """;General;https://github.com/implus/PytorchInsight
"""Note that the following models are with bias wd = 0.   """;Computer Vision;https://github.com/implus/PytorchInsight
"""Please install the required package using pip install -r requirements.txt. (Package of Factorization Machine can be installed using the link in reference below.)   """;General;https://github.com/yil479/yelp_review
"""  <a href=""https://github.com/sdadas/polish-roberta/releases/download/models/roberta_base_fairseq.zip"">v0.9.0</a>     <a href=""https://github.com/sdadas/polish-roberta/releases/download/models-transformers-v3.4.0/roberta_base_transformers.zip"">v3.4</a>     <a href=""https://github.com/sdadas/polish-roberta/releases/download/models-v2/roberta_base_fairseq.zip"">v0.10.1</a>     <a href=""https://github.com/sdadas/polish-roberta/releases/download/models-v2/roberta_base_transformers.zip"">v4.4</a>     <a href=""https://github.com/sdadas/polish-roberta/releases/download/models/roberta_large_fairseq.zip"">v0.9.0</a>     <a href=""https://github.com/sdadas/polish-roberta/releases/download/models-transformers-v3.4.0/roberta_large_transformers.zip"">v3.4</a>   ```python import os from fairseq.models.roberta import RobertaModel  RobertaHubInterface from fairseq import hub_utils  model_path = ""roberta_large_fairseq"" loaded = hub_utils.from_pretrained(     model_name_or_path=model_path      data_name_or_path=model_path      bpe=""sentencepiece""      sentencepiece_vocab=os.path.join(model_path  ""sentencepiece.bpe.model"")      load_checkpoint_heads=True      archive_map=RobertaModel.hub_models()      cpu=True ) roberta = RobertaHubInterface(loaded['args']  loaded['task']  loaded['models'][0]) roberta.eval() input = roberta.encode(""Za≈º√≥≈Çciƒá gƒô≈õlƒÖ ja≈∫≈Ñ."") output = roberta.extract_features(input) print(output[0][1]) ```   ```python import torch  os from transformers import RobertaModel  AutoModel  PreTrainedTokenizerFast  model_dir = ""roberta_base_transformers"" tokenizer = PreTrainedTokenizerFast(tokenizer_file=os.path.join(model_dir  ""tokenizer.json"")) model: RobertaModel = AutoModel.from_pretrained(model_dir) input = tokenizer.encode(""Za≈º√≥≈Çciƒá gƒô≈õlƒÖ ja≈∫≈Ñ."") output = model(torch.tensor([input]))[0] print(output[0][1]) ```   """;Natural Language Processing;https://github.com/sdadas/polish-roberta
"""Mixup is a generic and straightforward data augmentation principle. In essence  mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so  mixup regularizes the neural network to favor simple linear behavior in-between training examples.  This repository contains the implementation used for the results in our paper (https://arxiv.org/abs/1710.09412).   * A computer running macOS or Linux * For training new models  you'll also need a NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl) * Python version 3.6 * A [PyTorch installation](http://pytorch.org/)   """;Computer Vision;https://github.com/CaoShuning/MIXUP
"""	Environment used is: Ubuntu-18.04 LTS  GTX TitanXP 12 GB   	- (Optional) create python3 virtual environment and activate it 	- sudo apt-get install protobuf-compiler (Install protobuf-compiler  if you are getting issues pelase do it from source) 	- pip -r install requirments.txt 	- cd src/models/research/ 	- protoc object_detection/protos/*.proto --python_out=. 	- export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim (If you want to add to the bash  please add this command to ~/.bashrc) 	- source ~/.bashrc 	- Change directory to the product_detection_chandrasekahr_pati Download initial weight model from [link](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_11_06_2017.tar.gz) then extract and place it in models folder      sh create_tf_records.sh       sh train.sh       sh ckpt_2_pb.sh   	Environment used is: Ubuntu-18.04 LTS  GTX TitanXP 12 GB   	- (Optional) create python3 virtual environment and activate it 	- sudo apt-get install protobuf-compiler (Install protobuf-compiler  if you are getting issues pelase do it from source) 	- pip -r install requirments.txt 	- cd src/models/research/ 	- protoc object_detection/protos/*.proto --python_out=. 	- export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim (If you want to add to the bash  please add this command to ~/.bashrc) 	- source ~/.bashrc 	- Change directory to the product_detection_chandrasekahr_pati Download initial weight model from [link](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_11_06_2017.tar.gz) then extract and place it in models folder  """;Computer Vision;https://github.com/chandra411/Product-Detection
"""```bash make ```  git clone https://github.com/sigmorphon2020/task0-data.git   """;General;https://github.com/AssafSinger94/sigmorphon-2020-inflection
"""```bash make ```  git clone https://github.com/sigmorphon2020/task0-data.git   """;Natural Language Processing;https://github.com/AssafSinger94/sigmorphon-2020-inflection
"""Note that the following reousrces can be download here.   | NumberBatch     | 300            | https://github.com/commonsense/conceptnet-numberbatch   | entities |   bash scripts/param_search_lm.sh csqa roberta-large   bash scripts/param_search_rn.sh csqa bert-large-uncased   For example  run the following command to train a RoBERTa-Large model on CommonsenseQA:   bash scripts/run_grn_csqa.sh   - Convert your dataset to  `{train dev test}.statement.jsonl`  in .jsonl format (see `data/csqa/statement/train.statement.jsonl`) - Create a directory in `data/{yourdataset}/` to store the .jsonl files - Modify `preprocess.py` and perform subgraph extraction for your data - Modify `utils/parser_utils.py` to support your own dataset - Tune `encoder_lr` `decoder_lr` and other important hyperparameters  modify `utils/parser_utils.py` and `{model}.py` to record the tuned hyperparameters  """;Graphs;https://github.com/INK-USC/MHGRN
"""""";Computer Vision;https://github.com/sathvikyesprabhu/brats-dl
"""  --env [ENV]           The OpenAI Gym environment to train on  e.g.     --warm_up [WARM_UP]   Following recommendation from OpenAI Spinning Up  the   The pretrained weights can be retrieved from these links:   make a local clone  make changes on the local copy   push to your GitHub account: git push origin   """;Reinforcement Learning;https://github.com/samuelmat19/DDPG-tf2
"""The required packages are managed with pipenv and can be installed using pipenv install. Please see the pipenv documentation for more information.   To install and train a model.  ```shell pipenv install pipenv shell python train.py ```  To visualize losses and reconstructions.  ```shell tensorboard --logdir ./logs/ ```   """;Computer Vision;https://github.com/sarus-tech/tf2-published-models
"""The required packages are managed with pipenv and can be installed using pipenv install. Please see the pipenv documentation for more information.   To install and train a model.  ```shell pipenv install pipenv shell python train.py ```  To visualize losses and reconstructions.  ```shell tensorboard --logdir ./logs/ ```   """;General;https://github.com/sarus-tech/tf2-published-models
"""Input image (608  608  3)  The input image goes through a CNN  resulting in a (19  19  5  85) dimensional output.  After flattening the last two dimensions  the output is a volume of shape (19  19  425). Each cell in a 19 x 19 grid over the input image gives 425 numbers: 425 = 5 x 85 because each cell contains predictions for 5 boxes  corresponding to 5 anchor boxes; 85 = 5 + 80 where 5 is because (pc  bx  by  bh  bw) has 5 numbers  and 80 is the number of classes we'd like to detect.  We then select only a few boxes based on score-thresholding--discarding boxes that have detected a class with a score less than the threshold  and non-max suppression‚Ää-‚Ääcomputing the Intersection over Union (IOU) and avoiding selecting overlapping boxes.  This gives the YOLO's final output.   """;Computer Vision;https://github.com/TheClub4/car-detection-yolov2
"""""";Computer Vision;https://github.com/jmysu/ESP32CAM-FaceDetect-Line-Notify
"""""";General;https://github.com/jmysu/ESP32CAM-FaceDetect-Line-Notify
"""The aim of this project is to dive into the field of action recognition and explore various techniques. Till now 2 models have been implemented <br/> 1 - The model.py  is a pytorch implementation of the paper - **A Closer Look at Spatiotemporal Convolutions for Action Recognition** Link to the paper is - **https://arxiv.org/abs/1711.11248v3**  2 - A pytorch implementation of MobileNets for less computational Models. Consult the paper - **MobileNetV2: Inverted Residuals and Linear Bottlenecks  - https://arxiv.org/abs/1801.04381v4**   """;Computer Vision;https://github.com/AD2605/Action-Recognition
"""The aim of this project is to dive into the field of action recognition and explore various techniques. Till now 2 models have been implemented <br/> 1 - The model.py  is a pytorch implementation of the paper - **A Closer Look at Spatiotemporal Convolutions for Action Recognition** Link to the paper is - **https://arxiv.org/abs/1711.11248v3**  2 - A pytorch implementation of MobileNets for less computational Models. Consult the paper - **MobileNetV2: Inverted Residuals and Linear Bottlenecks  - https://arxiv.org/abs/1801.04381v4**   """;General;https://github.com/AD2605/Action-Recognition
"""The Code is written in Python 3.7. If you don't have Python installed you can find it [here](https://www.python.org/downloads/). If you are using a lower version of Python you can upgrade using the pip package  ensuring you have the latest version of pip. To install the required packages and libraries  run this command in the project directory after [cloning](https://www.howtogeek.com/451360/how-to-clone-a-github-repository/) the repository:  ```bash #:#: Run > STEP 1 After unzipping the forked zip file of this project into your local machine  type the follwing command from the directory where you saved the project files in the command prompt:  pip install -r requirements.txt  This will install thw following libraries:  absl-py==0.9.0 astunparse==1.6.3 cachetools==4.1.1 certifi==2020.6.20 chardet==3.0.4 cycler==0.10.0 gast==0.3.3 google-auth==1.19.2 google-auth-oauthlib==0.4.1 google-pasta==0.2.0 grpcio==1.30.0 h5py==2.10.0 idna==2.10 importlib-metadata==1.7.0 imutils==0.5.3 joblib==0.16.0 Keras-Preprocessing==1.1.2 kiwisolver==1.2.0 Markdown==3.2.2 matplotlib==3.3.0 numpy==1.19.1 oauthlib==3.1.0 opencv-python==4.3.0.36 opt-einsum==3.3.0 Pillow==7.2.0 protobuf==3.12.2 pyasn1==0.4.8 pyasn1-modules==0.2.8 pyparsing==2.4.7 python-dateutil==2.8.1 requests==2.24.0 requests-oauthlib==1.3.0 rsa==4.6 scikit-learn==0.23.1 scipy==1.4.1 six==1.15.0 sklearn==0.0 tensorboard==2.2.2 tensorboard-plugin-wit==1.7.0 tensorflow==2.2.0 tensorflow-estimator==2.2.0 termcolor==1.1.0 threadpoolctl==2.1.0 urllib3==1.25.10   > STEP 2 Open Jupyter Notebook and run Data Augmentation and Preprocessing.ipynb in order to train your custom dataset within your loacl machine and preprocess the images meanwhile.  > STEP 3 Run detect_mask_from_webcam.py from the same directory of your project folder in the command prompt in order to test the detector in real- time using the webcam. ```   Directory Tree   Link: https://twitter.com/i/status/1268986094042992640   ‚îú‚îÄ‚îÄ requirements.txt   Link: [https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/demo%20video.mp4](https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/demo%20video.mp4)    """;Computer Vision;https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2
"""The Code is written in Python 3.7. If you don't have Python installed you can find it [here](https://www.python.org/downloads/). If you are using a lower version of Python you can upgrade using the pip package  ensuring you have the latest version of pip. To install the required packages and libraries  run this command in the project directory after [cloning](https://www.howtogeek.com/451360/how-to-clone-a-github-repository/) the repository:  ```bash #:#: Run > STEP 1 After unzipping the forked zip file of this project into your local machine  type the follwing command from the directory where you saved the project files in the command prompt:  pip install -r requirements.txt  This will install thw following libraries:  absl-py==0.9.0 astunparse==1.6.3 cachetools==4.1.1 certifi==2020.6.20 chardet==3.0.4 cycler==0.10.0 gast==0.3.3 google-auth==1.19.2 google-auth-oauthlib==0.4.1 google-pasta==0.2.0 grpcio==1.30.0 h5py==2.10.0 idna==2.10 importlib-metadata==1.7.0 imutils==0.5.3 joblib==0.16.0 Keras-Preprocessing==1.1.2 kiwisolver==1.2.0 Markdown==3.2.2 matplotlib==3.3.0 numpy==1.19.1 oauthlib==3.1.0 opencv-python==4.3.0.36 opt-einsum==3.3.0 Pillow==7.2.0 protobuf==3.12.2 pyasn1==0.4.8 pyasn1-modules==0.2.8 pyparsing==2.4.7 python-dateutil==2.8.1 requests==2.24.0 requests-oauthlib==1.3.0 rsa==4.6 scikit-learn==0.23.1 scipy==1.4.1 six==1.15.0 sklearn==0.0 tensorboard==2.2.2 tensorboard-plugin-wit==1.7.0 tensorflow==2.2.0 tensorflow-estimator==2.2.0 termcolor==1.1.0 threadpoolctl==2.1.0 urllib3==1.25.10   > STEP 2 Open Jupyter Notebook and run Data Augmentation and Preprocessing.ipynb in order to train your custom dataset within your loacl machine and preprocess the images meanwhile.  > STEP 3 Run detect_mask_from_webcam.py from the same directory of your project folder in the command prompt in order to test the detector in real- time using the webcam. ```   Directory Tree   Link: https://twitter.com/i/status/1268986094042992640   ‚îú‚îÄ‚îÄ requirements.txt   Link: [https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/demo%20video.mp4](https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/demo%20video.mp4)    """;General;https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2
"""NEZHA-PyTorch is the PyTorch version of NEZHA.   """;Natural Language Processing;https://github.com/huawei-noah/Pretrained-Language-Model
"""The implementation in this repo is designed to have the same command line interface as the [Transformer](https://github.com/chao-ji/tf-transformer) implementation. Follow that link for detailed instructions on data preparation  training  evaluation and attention weights visualization.   """;General;https://github.com/chao-ji/tf-seq2seq
"""* [PyTorch](http://pytorch.org/) version >= 1.5.0 * Python version >= 3.6 * For training new models  you'll also need an NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl) * **To install fairseq** and develop locally:  ``` bash git clone https://github.com/pytorch/fairseq cd fairseq pip install --editable ./  #: on MacOS: #: CFLAGS=""-stdlib=libc++"" pip install --editable ./  #: to install the latest stable release (0.10.x) #: pip install fairseq ```  * **For faster training** install NVIDIA's [apex](https://github.com/NVIDIA/apex) library:  ``` bash git clone https://github.com/NVIDIA/apex cd apex pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" \   --global-option=""--deprecated_fused_adam"" --global-option=""--xentropy"" \   --global-option=""--fast_multihead_attn"" ./ ```  * **For large datasets** install [PyArrow](https://arrow.apache.org/docs/python/install.html#using-pip): `pip install pyarrow` * If you use Docker make sure to increase the shared memory size either with `--ipc=host` or `--shm-size`  as command line options to `nvidia-docker run` .   * May 2020: [Follow fairseq on Twitter](https://twitter.com/fairseq)   * December 2019: [fairseq 0.9.0 released](https://github.com/pytorch/fairseq/releases/tag/v0.9.0)   mixed precision training (trains faster with less GPU memory on NVIDIA tensor cores)   ``` python   Twitter: https://twitter.com/fairseq   The [full documentation](https://fairseq.readthedocs.io/) contains instructions for getting started  training new models and extending fairseq with new model types and tasks.   We provide pre-trained models and pre-processed  binarized test sets for several tasks listed below  as well as example training and evaluation commands.  * [Translation](examples/translation/README.md): convolutional and transformer models are available * [Language Modeling](examples/language_model/README.md): convolutional and transformer models are available  We also have more detailed READMEs to reproduce results from specific papers:  * [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale (Babu et al.  2021)](examples/wav2vec/xlsr/README.md) * [Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al.  2020)](examples/criss/README.md) * [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al.  2020)](examples/wav2vec/README.md) * [Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al.  2020)](examples/unsupervised_quality_estimation/README.md) * [Training with Quantization Noise for Extreme Model Compression ({Fan*  Stock*} et al.  2020)](examples/quant_noise/README.md) * [Neural Machine Translation with Byte-Level Subwords (Wang et al.  2020)](examples/byte_level_bpe/README.md) * [Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at.  2020)](examples/mbart/README.md) * [Reducing Transformer Depth on Demand with Structured Dropout (Fan et al.  2019)](examples/layerdrop/README.md) * [Jointly Learning to Align and Translate with Transformer Models (Garg et al.  2019)](examples/joint_alignment_translation/README.md) * [Levenshtein Transformer (Gu et al.  2019)](examples/nonautoregressive_translation/README.md) * [Facebook FAIR's WMT19 News Translation Task Submission (Ng et al.  2019)](examples/wmt19/README.md) * [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al.  2019)](examples/roberta/README.md) * [wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al.  2019)](examples/wav2vec/README.md) * [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al.  2019)](examples/translation_moe/README.md) * [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al.  2019)](examples/pay_less_attention_paper/README.md) * [Understanding Back-Translation at Scale (Edunov et al.  2018)](examples/backtranslation/README.md) * [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al.  2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel) * [Hierarchical Neural Story Generation (Fan et al.  2018)](examples/stories/README.md) * [Scaling Neural Machine Translation (Ott et al.  2018)](examples/scaling_nmt/README.md) * [Convolutional Sequence to Sequence Learning (Gehring et al.  2017)](examples/conv_seq2seq/README.md) * [Language Modeling with Gated Convolutional Networks (Dauphin et al.  2017)](examples/language_model/README.conv.md)   """;General;https://github.com/pytorch/fairseq
"""""";General;https://github.com/morganmcg1/ImageNette_ImageWoof_ImageWang
"""Our models are trained with TPUs. It is recommended to run distributed training with TPUs when using our code for pretraining.  Our code can also run on a *single* GPU. It does not support multi-GPUs  for reasons such as global BatchNorm and contrastive loss across cores.  The code is compatible with both TensorFlow v1 and v2. See requirements.txt for all prerequisites  and you can also install them using the following command.  ``` pip install -r requirements.txt ```   `https://github.com/shawwn/tpunicorn`   kubectl get pods -w   bash babysit_tpus.sh   kubectl delete pods <pod-name>   bash stop_babysitting.sh  bash delete_cluster.sh   bash jobs/pretrain_ilsrc.sh 16 ar ar prj-selfsup-v2-22   bash get_ip.sh   To fine-tune a linear head (with a single GPU)  try the following command:   """;General;https://github.com/serre-lab/prj_selfsup
"""""";General;https://github.com/MinkaiXu/SobolevWassersteinGAN
"""""";Computer Vision;https://github.com/MinkaiXu/SobolevWassersteinGAN
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/RuiLiFeng/LAE
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;Computer Vision;https://github.com/RuiLiFeng/LAE
"""cd config/res34_loss_naive_nodice_hp_scse_ds_border    sh train.sh 32   """;General;https://github.com/Gjiangtao/A-Deep-Supervised-Edge-Optimization-Algorithm-for-Salt-Body-Segmentation
"""I'm playing with PyTorch on the CIFAR10 dataset.   """;General;https://github.com/kuangliu/pytorch-cifar
"""NOTE: You can directly open the code in Gihub Codespaces on the web to run them without downloading! Also  try github.dev.   Want to quickly learn transfer learningÔºüÊÉ≥Â∞ΩÂø´ÂÖ•Èó®ËøÅÁßªÂ≠¶‰π†ÔºüÁúã‰∏ãÈù¢ÁöÑÊïôÁ®ã„ÄÇ  - Books ‰π¶Á±ç   - **„ÄäËøÅÁßªÂ≠¶‰π†„ÄãÔºàÊù®Âº∫Ôºâ** [[Buy](https://item.jd.com/12930984.html)] [[English version](https://www.cambridge.org/core/books/transfer-learning/CCFFAFE3CDBC245047F1DEC71D9EF3C7)]   - **„ÄäËøÅÁßªÂ≠¶‰π†ÂØºËÆ∫„Äã(ÁéãÊôã‰∏ú„ÄÅÈôàÁõäÂº∫Ëëó)** [[Homepage](http://jd92.wang/tlbook)] [[Buy](https://item.jd.com/13283188.html)]  - Blogs ÂçöÂÆ¢   - [Zhihu blogs - Áü•‰πé‰∏ìÊ†è„ÄäÂ∞èÁéãÁà±ËøÅÁßª„ÄãÁ≥ªÂàóÊñáÁ´†](https://zhuanlan.zhihu.com/p/130244395) 	 - Video tutorials ËßÜÈ¢ëÊïôÁ®ã    - [Recent advance of transfer learning - 2021Âπ¥ÊúÄÊñ∞ËøÅÁßªÂ≠¶‰π†ÂèëÂ±ïÁé∞Áä∂Êé¢ËÆ®](https://www.bilibili.com/video/BV1N5411T7Sb)   - [Definitions of transfer learning area - ËøÅÁßªÂ≠¶‰π†È¢ÜÂüüÂêçËØçËß£Èáä](https://www.bilibili.com/video/BV1fu411o7BW) [[Article](https://zhuanlan.zhihu.com/p/428097044)]   - [Domain generalization - ËøÅÁßªÂ≠¶‰π†Êñ∞ÂÖ¥Á†îÁ©∂ÊñπÂêëÈ¢ÜÂüüÊ≥õÂåñ](https://www.bilibili.com/video/BV1ro4y1S7dd/)      - [Domain adaptation - ËøÅÁßªÂ≠¶‰π†‰∏≠ÁöÑÈ¢ÜÂüüËá™ÈÄÇÂ∫îÊñπÊ≥ï(‰∏≠Êñá)](https://www.bilibili.com/video/BV1T7411R75a/)    - [Transfer learning by Hung-yi Lee @ NTU - Âè∞ÊπæÂ§ßÂ≠¶ÊùéÂÆèÊØÖÁöÑËßÜÈ¢ëËÆ≤Ëß£(‰∏≠ÊñáËßÜÈ¢ë)](https://www.youtube.com/watch?v=qD6iD4TFsdQ)  - Brief introduction and slides ÁÆÄ‰ªã‰∏épptËµÑÊñô    - [Recent advance of transfer learning](http://jd92.wang/assets/files/l15_jiqizhixin.pdf)      - [Domain generalization survey](http://jd92.wang/assets/files/DGSurvey-ppt.pdf)      - [Brief introduction in Chinese](https://github.com/jindongwang/transferlearning/blob/master/doc/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B.md) 	- [PPT (English)](http://jd92.wang/assets/files/l03_transferlearning.pdf) | [PPT (‰∏≠Êñá)](http://jd92.wang/assets/files/l08_tl_zh.pdf) 	   - ËøÅÁßªÂ≠¶‰π†‰∏≠ÁöÑÈ¢ÜÂüüËá™ÈÄÇÂ∫îÊñπÊ≥ï Domain adaptation: [PDF](http://jd92.wang/assets/files/l12_da.pdf) ÔΩú [Video on Bilibili](https://www.bilibili.com/video/BV1T7411R75a/) | [Video on Youtube](https://www.youtube.com/watch?v=RbIsHNtluwQ&t=22s) 	   - Tutorial on transfer learning by Qiang Yang: [IJCAI'13](http://ijcai13.org/files/tutorial_slides/td2.pdf) | [2016 version](http://kddchina.org/file/IntroTL2016.pdf)  - Talk is cheap  show me the code Âä®ÊâãÊïôÁ®ã„ÄÅ‰ª£Á†Å„ÄÅÊï∞ÊçÆ    - [PytorchÂÆòÊñπËøÅÁßªÂ≠¶‰π†Á§∫ÊÑè‰ª£Á†Å](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) 	- [PytorchÁöÑfinetune Fine-tune based on Alexnet and Resnet](https://github.com/jindongwang/transferlearning/tree/master/code/AlexNet_ResNet) 	- [Áî®PytorchËøõË°åÊ∑±Â∫¶ÁâπÂæÅÊèêÂèñ](https://github.com/jindongwang/transferlearning/tree/master/code/feature_extractor) 	- [Êõ¥Â§ö More...](https://github.com/jindongwang/transferlearning/tree/master/code)  - [Transfer Learning Scholars and Labs - ËøÅÁßªÂ≠¶‰π†È¢ÜÂüüÁöÑËëóÂêçÂ≠¶ËÄÖ„ÄÅ‰ª£Ë°®Â∑•‰ΩúÂèäÂÆûÈ™åÂÆ§‰ªãÁªç](https://github.com/jindongwang/transferlearning/blob/master/doc/scholar_TL.md)  - [Negative transfer - Ë¥üËøÅÁßª](https://www.zhihu.com/question/66492194/answer/242870418)  - - -   """;General;https://github.com/jindongwang/transferlearning
"""If you'd like to submit a pull request  you'll need to clone the repository;   """;Reinforcement Learning;https://github.com/google-research/google-research
"""name = ""transformer""   """;General;https://github.com/ays-dev/keras-transformer
"""name = ""transformer""   """;Natural Language Processing;https://github.com/ays-dev/keras-transformer
""" Before training your own models  we recommend you to refer to the instructions described [here](https://github.com/hszhao/semseg). Then  you need to update the dataset paths in the configuration files.  Four GPUs with at least 11G memory on each are required for synchronized training. [PyTorch](https://pytorch.org/get-started/locally/) (>=1.0.1) and [Apex](https://github.com/NVIDIA/apex) are required for Sync-BN support. For apex  just follow the ""Quick Start"" part to install it.  For pretrained models  you can download them from here ([resnet50](https://hangzh.s3.amazonaws.com/encoding/models/resnet50-25c4b509.zip) and [resnet101](https://hangzh.s3.amazonaws.com/encoding/models/resnet101-2a57e44d.zip)) or my [google drive](https://drive.google.com/open?id=1jrm93o5ULjuOHaRQVakVF-e8MC8IAn1X). Then  create a new folder ""pretrained"" and put the pretrained models in it  like ``` mkdir pretrained mv downloaded_pretrained_model ./pretrained/ ```  For training  just run ``` sh tool/train.py dataset_name model_name ``` For instance  in our case  you can run ``` sh tool/train.py ade20k spnet50 ```  For test  ``` sh tool/test.py dataset_name model_name ``` At present  multi-GPU test is not supported. Will implement it later.    """;Computer Vision;https://github.com/Andrew-Qibin/SPNet
"""1. Clone repository (recursively): ```Shell git clone --recursive https://github.com/ASMIftekhar/VSGNet.git ``` 2. Download data annotations object detection results: ```Shell bash download_data.sh ``` You need to have wget and unzip packages to execute this script. Alternatively you can download the data from [here](https://drive.google.com/drive/folders/1J8mN63bNIrTdBQzq7Lpjp4qxMXgYI-yF?usp=sharing). If you execute the script then there will be two folders in the directory ""All\_data"" and ""infos"". This will take close to 10GB space. This contains both of the datasets and all the essential files. Also  if you just want to work with v-coco  download ""All_data_vcoco"" from the link.    Inside the All\_data folder you will find the following subdirectories.  **a.Data_vcoco**: It will contain all training and validation images of v-coco inside train2014 subdirectory and all test images of v-coco inside val2014 subdirectory.  **b.Annotations\_vcoco**: It will contain all annotations of training  validation and testing set in three json files. The annotations are taken from v-coco API and converted into our convenient format. For example  lets consider there is only one single image annotated with two verbs ""smile"" and ""hold"" along with two person and object bounding boxes. The annotation for this image will be arranged as follows:  ``` 	{image_id:[{'Verbs': 'hold'    	'object': {'obj_bbx': [305.84  59.12  362.34  205.22]}    	'person_bbx': [0.0  0.63  441.03  368.86]}   	{'Verbs': 'smile'    	'object': {'obj_bbx': []}    	person_bbx': [0.0  0.63  441.03  368.86]}]} ``` **c.Object\_Detections\_vcoco**: It will contain all object detection results for v-coco.   **d.v-coco**: It will contain original v-coco API. This is needed for doing evaluations.  **e.Data_hico**: It will contain all the training images of HICO-DET inside train2015 subdirectory and all test images of HICO_DET inside test2015 subdirectory.  **f.Annotations\_hico**: same as folder (b) but for HICO_DET dataset.  **g.Object\_Detections\_hico**: same as folder (c) but for HICO_DET dataset.  **h.bad\_Detections\_hico**: It will contain the list of images in HICO_DET dataset where our object detector fails to detect any person or object.  **j.hico\_infos**: It will contain additional files required to run training and testing in HICO_DET.  3. To install all packages (preferable to run in a python2 virtual environment): ``` pip2 install -r requirements.txt ``` For HICO_DET evaluation we will use python3 environment  to install those packages (preferable to run in a python3 virtual environment): ``` pip3 install -r requirements3.txt ``` Run only compute_map.sh in a python 3 enviornment. For all other use python 2 environment.  4. If you do not wish to move ""All\_data"" folder from the main directory then you dont need to do anything else to setup the repo. Otherwise you need to run setup.py with the location of All\_data. If you put it in /media/ssd2 with a new name of ""data"" then you need to execute the following command: ``` python2 setup.py -d /media/ssd2/data/ ```   bash compute_map.sh soa_paper_hico 20   bash compute_map.sh new_test 20   """;Computer Vision;https://github.com/ASMIftekhar/VSGNet
"""* Raw data needs to be written into `tfrecord` format to be decoded by `./data_loader.py`. The pre-processed data has been released from our work [PnP-AdaNet](https://github.com/carrenD/Medical-Cross-Modality-Domain-Adaptation). The training data can be downloaded [here](https://drive.google.com/file/d/1m9NSHirHx30S8jvN0kB-vkd7LL0oWCq3/view). The testing CT data can be downloaded [here](https://drive.google.com/file/d/1SJM3RluT0wbR9ud_kZtZvCY0dR9tGq5V/view). The testing MR data can be downloaded [here](https://drive.google.com/file/d/1Bm2uU4hQmn5L3GwXz6I0vuCN3YVMEc8S/view?usp=sharing). * Put `tfrecord` data of two domains into corresponding folders under `./data` accordingly. * Run `./create_datalist.py` to generate the datalists containing the path of each data.   * Install TensorFlow 1.10 and CUDA 9.0 * Clone this repo ``` git clone https://github.com/cchen-cc/SIFA cd SIFA ```   """;General;https://github.com/cchen-cc/SIFA
"""We propose AugMix  a data processing technique that mixes augmented images and enforces consistent embeddings of the augmented images  which results in increased robustness and improved uncertainty calibration. AugMix does not require tuning to work correctly  as with random cropping or CutOut  and thus enables plug-and-play data augmentation. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks  closing the gap between previous methods and the best possible performance by more than half in some cases. With AugMix  we obtain state-of-the-art on ImageNet-C  ImageNet-P and in uncertainty estimation when the train and test distribution do not match.  For more details please see our [ICLR 2020 paper](https://arxiv.org/pdf/1912.02781.pdf).   1.  Install PyTorch and other required python libraries with:      ```     pip install -r requirements.txt     ```  2.  Download CIFAR-10-C and CIFAR-100-C datasets with:      ```     mkdir -p ./data/cifar     curl -O https://zenodo.org/record/2535967/files/CIFAR-10-C.tar     curl -O https://zenodo.org/record/3555552/files/CIFAR-100-C.tar     tar -xvf CIFAR-100-C.tar -C data/cifar/     tar -xvf CIFAR-10-C.tar -C data/cifar/     ```  3.  Download ImageNet-C with:      ```     mkdir -p ./data/imagenet/imagenet-c     curl -O https://zenodo.org/record/2235448/files/blur.tar     curl -O https://zenodo.org/record/2235448/files/digital.tar     curl -O https://zenodo.org/record/2235448/files/noise.tar     curl -O https://zenodo.org/record/2235448/files/weather.tar     tar -xvf blur.tar -C data/imagenet/imagenet-c     tar -xvf digital.tar -C data/imagenet/imagenet-c     tar -xvf noise.tar -C data/imagenet/imagenet-c     tar -xvf weather.tar -C data/imagenet/imagenet-c     ```   The Jensen-Shannon Divergence loss term may be disabled for faster training at the cost of slightly lower performance by adding the flag `--no-jsd`.  Training recipes used in our paper:  WRN: `python cifar.py`  AllConv: `python cifar.py -m allconv`  ResNeXt: `python cifar.py -m resnext -e 200`  DenseNet: `python cifar.py -m densenet -e 200 -wd 0.0001`  ResNet-50: `python imagenet.py <path/to/imagenet> <path/to/imagenet-c>`   """;Computer Vision;https://github.com/google-research/augmix
"""""";Natural Language Processing;https://github.com/jpablou/Matching-The-Blanks-Ths
"""""";General;https://github.com/fengtong-xiao/DMBGN
"""This repository contains a [Torch](http://torch.ch) implementation for the [ResNeXt](https://arxiv.org/abs/1611.05431) algorithm for image classification. The code is based on [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch).  [ResNeXt](https://arxiv.org/abs/1611.05431) is a simple  highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous  multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension  which we call ‚Äúcardinality‚Äù (the size of the set of transformations)  as an essential factor in addition to the dimensions of depth and width.   ![teaser](http://vcl.ucsd.edu/resnext/teaser.png)  Requirements and Dependencies   """;Computer Vision;https://github.com/facebookresearch/ResNeXt
"""Please read [`DATASET.md`](DATASET.md) for downloading and preparing the Kinetics dataset.  **Note:** In this repo  we release the model which are trained with the same data as our paper.   Please find installation instructions for Caffe2 in [`INSTALL.md`](INSTALL.md). We also suggest to check the [Detectron installation](https://github.com/facebookresearch/Detectron/blob/master/INSTALL.md) and its issues if you had problems.   They can be downloaded from: pretrained_model.tar.gz. Extract the models to the current folder:   wget https://dl.fbaipublicfiles.com/video-nonlocal/pretrained_model.tar.gz  tar xzf pretrained_model.tar.gz   cd scripts   | <sub>run_c2d_baseline_400k.sh</sub> | 8 | - |  - | - | 71.9 | 90.0 | link | link |   The following two models were run by Xiaolong Wang with 4-GPU (GTX 1080) machines outside of Facebook after the internship. The training data is downloaded on 12/20/2017 (see DATASET.md)  which misses some videos due to invalid urls. The training schedule is shorter¬†(4-GPU 600k vs. 8-GPU 400k above). These changes lead to a slight accuracy drop.       run_i3d_baseline_400k.sh       run_i3d_nlnet_400k.sh       cd ../process_data/convert_models   """;General;https://github.com/facebookresearch/video-nonlocal-net
"""Please read [`DATASET.md`](DATASET.md) for downloading and preparing the Kinetics dataset.  **Note:** In this repo  we release the model which are trained with the same data as our paper.   Please find installation instructions for Caffe2 in [`INSTALL.md`](INSTALL.md). We also suggest to check the [Detectron installation](https://github.com/facebookresearch/Detectron/blob/master/INSTALL.md) and its issues if you had problems.   They can be downloaded from: pretrained_model.tar.gz. Extract the models to the current folder:   wget https://dl.fbaipublicfiles.com/video-nonlocal/pretrained_model.tar.gz  tar xzf pretrained_model.tar.gz   cd scripts   | <sub>run_c2d_baseline_400k.sh</sub> | 8 | - |  - | - | 71.9 | 90.0 | link | link |   The following two models were run by Xiaolong Wang with 4-GPU (GTX 1080) machines outside of Facebook after the internship. The training data is downloaded on 12/20/2017 (see DATASET.md)  which misses some videos due to invalid urls. The training schedule is shorter¬†(4-GPU 600k vs. 8-GPU 400k above). These changes lead to a slight accuracy drop.       run_i3d_baseline_400k.sh       run_i3d_nlnet_400k.sh       cd ../process_data/convert_models   """;Computer Vision;https://github.com/facebookresearch/video-nonlocal-net
"""`); otherwise the implementation may be different.  ---  * Clone the this repository: ``` git clone https://github.com/cmhungsteve/SSTDA.git ``` * Download the [Dataset](https://www.dropbox.com/s/kc1oyz79rr2znmh/Datasets.zip?dl=0) folder  which contains the features and the ground truth labels. (~30GB)  * To avoid the difficulty for downloading the whole file  we also divide it into multiple files:     * [GTEA](https://www.dropbox.com/s/f0bxg73l62v9yo6/gtea.zip?dl=0)  [50Salads](https://www.dropbox.com/s/1vzeidtkzjef7vy/50salads.zip?dl=0)  [Breakfast-part1](https://www.dropbox.com/s/xgeffaqs5cbbs4l/breakfast_part1.zip?dl=0)  [Breakfast-part2](https://www.dropbox.com/s/mcpj4ny85c1xgvv/breakfast_part2.zip?dl=0)  [Breakfast-part3](https://www.dropbox.com/s/o8ba90n7o3rxqun/breakfast_part3.zip?dl=0)  [Breakfast-part4](https://www.dropbox.com/s/v1vx55zud97x544/breakfast_part4.zip?dl=0)  [Breakfast-part5](https://www.dropbox.com/s/e635tkwpd22dlt9/breakfast_part5.zip?dl=0) * Extract it so that you have the `Datasets` folder. * The default path for the dataset is `../../Datasets/action-segmentation/` if the current location is `./action-segmentation-DA/`. If you change the dataset path  you need to edit the scripts as well.  ---  """;General;https://github.com/cmhungsteve/SSTDA
"""The self-supervised training stage requires the following components to be specified to the training script:  * data root folder: contains files (or soft links to them) without subfolders in `wav`  `mp3` or any Torchaudio-supported format.  * trainset statistics file to normalize each worker's output values  computed with the `make_trainset_statistics.py` script. * dataset configuration `data_cfg` file: contains pointers to train/valid/test splits  among other info. * front-end (encoder) configuration file: `cfg/frontend/PASE+.cfg` * workers' configuration file: `cfg/workers/workers+.cfg`    This framework can be installed locally by running:  ``` python setup.py install ```  This will allow you to import PASE modules from anywhere.   To make the dataset configuration file the following files have to be provided:   If you want to use the openSLR RIRs  you should run the following command to include the file pointers into the distortions config file:   """;General;https://github.com/santi-pdp/pase
"""Cutout is a simple regularization method for convolutional neural networks which consists of masking out random sections of input images during training. This technique simulates occluded examples and encourages the model to take more minor features into consideration when making decisions  rather than relying on the presence of a few major features.      ![Cutout applied to CIFAR-10](https://github.com/uoguelph-mlrg/Cutout/blob/master/images/cutout_on_cifar10.jpg ""Cutout applied to CIFAR-10"")  Bibtex:   ``` @article{devries2017cutout      title={Improved Regularization of Convolutional Neural Networks with Cutout}      author={DeVries  Terrance and Taylor  Graham W}      journal={arXiv preprint arXiv:1708.04552}      year={2017}   } ```   WideResNet model implementation from https://github.com/xternalz/WideResNet-pytorch     To train WideResNet 16-8 on SVHN with cutout:   """;Computer Vision;https://github.com/uoguelph-mlrg/Cutout
"""Causion: To install the library  please git clone the repository instead of downloading the zip file  since source files inside the folder ./pytorch/prroi_pool/src/ and tensorflow/prroi_pool/src/kernels/external are symbol-linked. Downloading the repository as a zip file will break these symbolic links. Also  there are reports indicating that Windows git versions also breaks the symbol links. See issues/58.   mkdir tensorflow/prroi_pool/build  cd tensorflow/prroi_pool/build   mkdir tensorflow/prroi_pool/build  cd tensorflow/prroi_pool/build   nmake BUILD=release   In the directory `pytorch/`  we provide a PyTorch-based implementation of PrRoI Pooling. It requires PyTorch 1.0+ and only supports CUDA (CPU mode is not implemented). Since we use PyTorch JIT for cxx/cuda code compilation  to use the module in your code  simply do:  ``` from prroi_pool import PrRoIPool2D  avg_pool = PrRoIPool2D(window_height  window_width  spatial_scale) roi_features = avg_pool(features  rois)  #: for those who want to use the ""functional""  from prroi_pool.functional import prroi_pool2d roi_features = prroi_pool2d(features  rois  window_height  window_width  spatial_scale) ```   **!!! Please first checkout to the branch pytorch0.4.**  In the directory `pytorch/`  we provide a PyTorch-based implementation of PrRoI Pooling. It requires PyTorch 0.4 and only supports CUDA (CPU mode is not implemented). To use the PrRoI Pooling module  first goto `pytorch/prroi_pool` and execute `./travis.sh` to compile the essential components (you may need `nvcc` for this step). To use the module in your code  simply do:  ``` from prroi_pool import PrRoIPool2D  avg_pool = PrRoIPool2D(window_height  window_width  spatial_scale) roi_features = avg_pool(features  rois)  #: for those who want to use the ""functional""  from prroi_pool.functional import prroi_pool2d roi_features = prroi_pool2d(features  rois  window_height  window_width  spatial_scale) ```  Here   - RoI is an `m * 5` float tensor of format `(batch_index  x0  y0  x1  y1)`  following the convention in the original Caffe implementation of RoI Pooling  although in some frameworks the batch indices are provided by an integer tensor. - `spatial_scale` is multiplied to the RoIs. For example  if your feature maps are down-sampled by a factor of 16 (w.r.t. the input image)  you should use a spatial scale of `1/16`. - The coordinates for RoI follows the [L  R) convension. That is  `(0  0  4  4)` denotes a box of size `4x4`.   In the directory `tensorflow/`  we provide a TensorFlow-based implementation of PrRoI Pooling. It tested TensorFlow 2.2 and only supports CUDA (CPU mode is not implemented). To compile the essential components  follow the instruction below  To use the PrRoI Pooling module  to compile the essential components (you may need `nvcc` for this step). To use the module in your code  simply do:  """;Computer Vision;https://github.com/vacancy/PreciseRoIPooling
"""Requirements:  * ```console   sudo apt-get install ffmpeg libsm6 libxext6  -y     (may be needed for open-cv python)   ```   * Python 3.9  ```console python3 -m venv ./.venv ```  pyre-configuration specifies paths specifically to **.venv** directory ```console pip3 install -r requirements ```   """;Computer Vision;https://github.com/ElectronicBabylonianLiterature/cuneiform-sign-detection
"""This repo constains the pytorch implementation for the CVPR2018 unsupervised learning paper [(arxiv)](https://arxiv.org/pdf/1805.01978.pdf).   Our code extends the pytorch implementation of imagenet classification in [official pytorch release](https://github.com/pytorch/examples/tree/master/imagenet).  Please refer to the official repo for details of data preparation and hardware configurations.  - supports python27 and [pytorch=0.4](http://pytorch.org)  - if you are looking for pytorch 0.3  please switch to tag v0.3  - clone this repo: `git clone https://github.com/zhirongw/lemniscate.pytorch`  - Training on ImageNet:    `python main.py DATAPATH --arch resnet18 -j 32 --nce-k 4096 --nce-t 0.07  --lr 0.03 --nce-m 0.5 --low-dim 128 -b 256 `    - parameter nce-k controls the number of negative samples. If nce-k sets to 0  the code also supports full softmax learning.   - nce-t controls temperature of the distribution. 0.07-0.1 works well in practice.   - nce-m stabilizes the learning process. A value of 0.5 works well in practice.   - learning rate is initialized to 0.03  a bit smaller than standard supervised learning.   - the embedding size is controlled by the parameter low-dim.  - During training  we monitor the supervised validation accuracy by K nearest neighbor with K=1  as it's faster  and gives a good estimation of the feature quality.  - Testing on ImageNet:    `python main.py DATAPATH --arch resnet18 --resume input_model.pth.tar -e` runs testing with default K=200 neighbors.  - Training on CIFAR10:    `python cifar.py --nce-k 0 --nce-t 0.1 --lr 0.03`    """;General;https://github.com/zhirongw/lemniscate.pytorch
"""1. Download and extract the [LJ Speech dataset](https://keithito.com/LJ-Speech-Dataset/) 2. Clone this repo: `git clone https://github.com/NVIDIA/tacotron2.git` 3. CD into this repo: `cd tacotron2` 4. Initialize submodule: `git submodule init; git submodule update` 5. Update .wav paths: `sed -i -- 's DUMMY ljs_dataset_folder/wavs g' filelists/*.txt`     - Alternatively  set `load_mel_from_disk=True` in `hparams.py` and update mel-spectrogram paths  6. Install [PyTorch 1.0] 7. Install [Apex] 8. Install python requirements or build docker image      - Install python requirements: `pip install -r requirements.txt`   1. Download our published [Tacotron 2] model 2. Download our published [WaveGlow] model 3. `jupyter notebook --ip=127.0.0.1 --port=31337` 4. Load inference.ipynb   N.b.  When performing Mel-Spectrogram to Audio synthesis  make sure Tacotron 2 and the Mel decoder were trained on the same mel-spectrogram representation.     """;Audio;https://github.com/bfs18/tacotron2
"""This framework was created in order to help compare learned and variational approaches to CT reconstruction in a systematic way. The implementation is based on python libraries **odl** and **pyTorch**. The list of implemented algorithms include:  * FBP (Filtered back-projection)  * TV (Total Variation) * ADR (Adversarial Regularizer): https://arxiv.org/abs/1805.11572 * LG (Learned gradient descent): https://arxiv.org/abs/1704.04058 * LPD (Learned primal dual): https://arxiv.org/abs/1707.06474 * FL (Fully learned): https://nature.com/articles/nature25988.pdf * FBP+U (FBP with a U-Net denoiser): https://arxiv.org/abs/1505.04597  In order to add your own algorithms to the list  create a new file in the **Algorithms** folder in the form *name*.py and use BaseAlg.py as the template.  """;Computer Vision;https://github.com/Zakobian/CT_framework_
"""In this hands-on project  the goal is to build a face detection model which includes building a face detector to locate the position of a face in an image.    """;General;https://github.com/sivaole/Face_Detection
"""In this hands-on project  the goal is to build a face detection model which includes building a face detector to locate the position of a face in an image.    """;Computer Vision;https://github.com/sivaole/Face_Detection
"""utils.py: Read data and data processing.<br> layer.py: Attention layer.<br> model.py: Graph attention model network.<br> main.py: Training  validation and testing.<br> You can run it throughÔºö ```python python main.py ```   Another pytorch implementation: https://github.com/Diego999/pyGAT<br>   In order to make it easier to compare my code with the tensorflow version     """;Graphs;https://github.com/taishan1994/pytorch_gat
"""""";Computer Vision;https://github.com/xahidbuffon/SUIM
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/tkhhhh/yolov3-master
"""""";Reinforcement Learning;https://github.com/SHRIVP/muzero
"""Manual: https://github.com/AlexeyAB/darknet/wiki   Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example   PyTorch > ONNX:    TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like CUDA  cudnn  ZED and build against those. It will also create a shared object library file to use darknet for code development.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows.  Install Visual Studio 2017 or 2019. In case you need to download it  please go here: Visual Studio Community  Install CUDA (at least v10.0) enabling VS Integration during installation.   PS Code\&gt;              git clone https://github.com/microsoft/vcpkg  PS Code\&gt;              cd vcpkg   PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base cuda cudnn]:x64-windows for a quicker install of dependencies  PS Code\vcpkg&gt;         cd ..  PS Code\&gt;              git clone https://github.com/AlexeyAB/darknet  PS Code\&gt;              cd darknet   Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   in Python: https://github.com/tzutalin/labelImg  in Python: https://github.com/Cartucho/OpenLabeling   in JavaScript: https://github.com/opencv/cvat   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux   * using `build.sh` or   * build `darknet` using `cmake` or   * set `LIBSO=1` in the `Makefile` and do `make` * on Windows   * using `build.ps1` or   * build `darknet` using `cmake` or   * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs:  * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h   * Python examples using the C API:     * https://github.com/AlexeyAB/darknet/blob/master/darknet.py     * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py  * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp   * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp  ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`      * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)  `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42)  ```cpp struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false);         std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/Demonhesusheng/darknet_v2
"""Please follow the instructions of py-faster-rcnn [here](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to setup VOC and COCO datasets (Part of COCO is done). The steps involve downloading data and optionally creating soft links in the ``data`` folder. Since faster RCNN does not rely on pre-computed proposals  it is safe to ignore the steps that setup proposals.  If you find it useful  the ``data/cache`` folder created on my side is also shared [here](http://ladoga.graphics.cs.cmu.edu/xinleic/tf-faster-rcnn/cache.tgz).   1. Clone the repository   ```Shell   git clone https://github.com/endernewton/tf-faster-rcnn.git   ```  2. Update your -arch in setup script to match your GPU   ```Shell   cd tf-faster-rcnn/lib   #: Change the GPU architecture (-arch) if necessary   vim setup.py   ```    | GPU model  | Architecture |   | ------------- | ------------- |   | TitanX (Maxwell/Pascal) | sm_52 |   | GTX 960M | sm_50 |   | GTX 1080 (Ti) | sm_61 |   | Grid K520 (AWS g2.2xlarge) | sm_30 |   | Tesla K80 (AWS p2.xlarge) | sm_37 |    **Note**: You are welcome to contribute the settings on your end if you have made the code work properly on other GPUs. Also even if you are only using CPU tensorflow  GPU based code (for NMS) will be used by default  so please set **USE_GPU_NMS False** to get the correct output.   3. Build the Cython modules   ```Shell   make clean   make   cd ..   ```  4. Install the [Python COCO API](https://github.com/pdollar/coco). The code requires the API to access COCO dataset.   ```Shell   cd data   git clone https://github.com/pdollar/coco.git   cd coco/PythonAPI   make   cd ../../..   ```     - Due to the randomness in GPU training with Tensorflow especially for VOC  the best numbers are reported (with 2-3 attempts) here. According to my experience  for COCO you can almost always get a very close number (within ~0.2%) despite the randomness.      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz     tar -xzvf vgg_16_2016_08_28.tar.gz      cd ../..     For Resnet101  you can set up like:      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz     tar -xzvf resnet_v1_101_2016_08_28.tar.gz      cd ../..     ./experiments/scripts/train_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/train_faster_rcnn.sh 1 coco res101     ./experiments/scripts/test_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/test_faster_rcnn.sh 1 coco res101   1. Download pre-trained model   ```Shell   #: Resnet101 for voc pre-trained on 07+12 set   ./data/scripts/fetch_faster_rcnn_models.sh   ```   **Note**: if you cannot download the models through the link  or you want to try more models  you can check out the following solutions and optionally update the downloading script:   - Another server [here](http://xinlei.sp.cs.cmu.edu/xinleic/tf-faster-rcnn/).   - Google drive [here](https://drive.google.com/open?id=0B1_fAEgxdnvJSmF3YUlZcHFqWTQ).  2. Create a folder and a soft link to use the pre-trained model   ```Shell   NET=res101   TRAIN_IMDB=voc_2007_trainval+voc_2012_trainval   mkdir -p output/${NET}/${TRAIN_IMDB}   cd output/${NET}/${TRAIN_IMDB}   ln -s ../../../data/voc_2007_trainval+voc_2012_trainval ./default   cd ../../..   ```  3. Demo for testing on custom images   ```Shell   #: at repository root   GPU_ID=0   CUDA_VISIBLE_DEVICES=${GPU_ID} ./tools/demo.py   ```   **Note**: Resnet101 testing probably requires several gigabytes of memory  so if you encounter memory capacity issues  please install it with CPU support only. Refer to [Issue 25](https://github.com/endernewton/tf-faster-rcnn/issues/25).  4. Test with pre-trained Resnet101 models   ```Shell   GPU_ID=0   ./experiments/scripts/test_faster_rcnn.sh $GPU_ID pascal_voc_0712 res101   ```   **Note**: If you cannot get the reported numbers (79.8 on my side)  then probably the NMS function is compiled improperly  refer to [Issue 5](https://github.com/endernewton/tf-faster-rcnn/issues/5).   """;Computer Vision;https://github.com/tigerofmurder/tf-faster-rcnn
"""    |‚Äî‚ÄîCOCO   """;Computer Vision;https://github.com/gakkifan2020/YoloV3_From_calmisential_workspace
"""""";Computer Vision;https://github.com/gokulprasadthekkel/pytorch-multi-class-focal-loss
"""""";General;https://github.com/gokulprasadthekkel/pytorch-multi-class-focal-loss
"""Yolo v4 source code: https://github.com/AlexeyAB/darknet   Useful links: https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&sk=6039748846bbcf1d960c3061542591d7   1.2 make   """;Computer Vision;https://github.com/SeventhBlue/trainDarknet-yolov3
"""""";General;https://github.com/Dycollapsar/Attention-Based-for-Medicalimaging
"""Training of this magnitude definitely needed some beefed up hardware and since I'm a console guy (PS4)  I resorted to the EC instances Amazon provides (https://aws.amazon.com/ec2/instance-types/). Udacity's Amazon credits came in handy!  At first  I tried the g2.xlarge instance that Udacity's project on Traffic sign classifier had suggested (did that on my laptop back then) but the memory or the compute capability was nowhere near sufficient  since TF apparently drops to CPU and RAM after detecting that there isn't sufficient capacity on the GPU.  In the end  p2.xlarge EC2 instance were what I trained my network on. There was ~10GB GPU memory utilization and ~92% GPU at peak. My network trained pretty well on this setup.  NOTE: I faced a lot of issues when getting setup on the remote instance due to issues with certain libraries being out of date and anaconda not having those updates. Luckily Amazon released its latest (v6 at time) deep learning Ubuntu AMI which worked just fine out of the box. So if you are using EC2  make sure to test sample code and library imports in python first to make sure the platform is ready for your code.      """;General;https://github.com/Rohed/ml-1
"""Training of this magnitude definitely needed some beefed up hardware and since I'm a console guy (PS4)  I resorted to the EC instances Amazon provides (https://aws.amazon.com/ec2/instance-types/). Udacity's Amazon credits came in handy!  At first  I tried the g2.xlarge instance that Udacity's project on Traffic sign classifier had suggested (did that on my laptop back then) but the memory or the compute capability was nowhere near sufficient  since TF apparently drops to CPU and RAM after detecting that there isn't sufficient capacity on the GPU.  In the end  p2.xlarge EC2 instance were what I trained my network on. There was ~10GB GPU memory utilization and ~92% GPU at peak. My network trained pretty well on this setup.  NOTE: I faced a lot of issues when getting setup on the remote instance due to issues with certain libraries being out of date and anaconda not having those updates. Luckily Amazon released its latest (v6 at time) deep learning Ubuntu AMI which worked just fine out of the box. So if you are using EC2  make sure to test sample code and library imports in python first to make sure the platform is ready for your code.      """;Computer Vision;https://github.com/Rohed/ml-1
"""""";Audio;https://github.com/s3nh/pytorch-tacotron2
"""``` $ cd CycleGAN/ $ python CycleGAN_model.py ``` An example of the generated adversarial examples is as follows:  <img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D1.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D2.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D3.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D4.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D5.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D6.jpg"" width=""290""/>    """;Computer Vision;https://github.com/Our4514/CAGFUZZ
"""``` $ cd CycleGAN/ $ python CycleGAN_model.py ``` An example of the generated adversarial examples is as follows:  <img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D1.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D2.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D3.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D4.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D5.jpg"" width=""290""/><img src=""https://github.com/QXL4515/CAGFuzz/blob/master/picture/D6.jpg"" width=""290""/>    """;General;https://github.com/Our4514/CAGFUZZ
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/dzqjorking/transpose
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/dzqjorking/transpose
"""REPVGG is a simple but powerful architecture of CNN which has a VGG like inference time .It runs 101% faster then RESNET 101  There are many complicated architecture which has better accuracy then simple architectures  but the drawback of this kind of architecture is that they are difficult to customize . And  has very high inference time .REPVGG has various advantages like   Ithas plain topology   just like its earlier models such as vgg 19 etc . Its architecture highly depends upon 3x3 kernels and ReLU. It has novel structural reparamaterization which decouple a training time of multi branch topology with a inference time plain architecture .You can also se training of REPVGG in google colab on CIFAR10 [here](https://github.com/imad08/model-zoo-submissions/blob/main/REPVGG/REPVGG_with_complete_reparamaterization_.ipynb)   ![fusing batch normalization and convolutions for reparametrization](https://media.arxiv-vanity.com/render-output/4507333/x1.png)   ```bash $ python3 main.py  ``` NOTE: on Colab Notebook use following command: ```python !git clone link-to-repo %run main.py  ```   """;Computer Vision;https://github.com/imad08/Repvgg_pytorch
"""""";Graphs;https://github.com/cmavro/Graph-InfoClust-GIC
"""""";Computer Vision;https://github.com/SaubanMusaddiq/Conditional-Image-Synthesis-using-DCGANS
"""Yolo v4 source code: https://github.com/AlexeyAB/darknet   Useful links: https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&sk=6039748846bbcf1d960c3061542591d7   """;Computer Vision;https://github.com/Hardly-Human/Yolo-Object-Detector
"""You can easily setup the enviornment with all required components ( data and notebooks ) with the help of Docker.  Here are the steps.   1. Install Docker on your local machine. You will required documentation on Docker website [ https://docs.docker.com/install/ ]  2. Make sure Docker is working fine. If you are not getting any error and able to see the docker   ```sh $ docker --version ```  3. Download the docker image and create container for the tutorial  ```sh $ docker run -it --rm -p 8888:8888 -p 0.0.0.0:6006:6006 meabhishekkumar/strata-ca-2018 ```   """;General;https://github.com/sandeepnair2812/Deep-Learning-Based-Search-and-Recommendation-System
"""GPUs: K80 ($0.14/hr)  T4 ($0.11/hr)  V100 ($0.74/hr) CUDA with Nvidia Apex FP16/32     Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [Notebook](https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb) <a href=""https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a> * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) << highly recommended * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart)  ![Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/yolov3?logo=docker) * [A TensorRT Implementation of YOLOv3 and YOLOv4](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp)     """;Computer Vision;https://github.com/thanhtran98/yolov3_jetbot
"""""";Computer Vision;https://github.com/Sezoir/DCGAN-Dog-Generator
"""<p align=""center"">   <img src=""https://raw.githubusercontent.com/GoldenCorgi/Trash-Recognition/master/static/demo.png"" alt=""demo""/> </p>   Open the notebook in colab below or download locally. Run all the cells and upload your own images. No coding necessary.   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GoldenCorgi/Trash-Recognition/blob/master/WebAppRecognition.ipynb)   """;Computer Vision;https://github.com/GoldenCorgi/Trash-Recognition
"""Simple image classification project using Tensorflow. It allows to train DNN model and classify images. Contains pretrained models for cat/dog/human recognition.    ./scripts/run_in_docker.sh python src/train.py --arch vgg_v2   vgg_v2 - VGG architecture bigger version.   ./scripts/run_in_docker.sh python src/train.py -h   To start working with tensorflow in docker run:  ``` ./scripts/run_in_docker.sh bash ```  or if you can work with GPU:  ``` ./scripts/run_in_docker_gpu.sh bash ```    """;Computer Vision;https://github.com/nidolow/image-classification
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/Lmath11/darknet
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/Lmath11/darknet
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/clasikman/darknet
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/symetra17/yolov4_train
"""python B0_global.py (training requires CUDA enabled GPU with VRAM of at least 4GB - tested on machine with 8GB VRAM)   NOTE: These instructions are for use on a Windows machine. These may not work for Linux  MacOS  or other Operating Systems   These papers can be found at the following links:   * COCO   ![](graphics/B0_KERNEL9/trnacc_B0.png)   ![](graphics/B0_KERNEL9/trnloss_B0.png)  We can clearly see from the above graphs that there is a logarithmic relationship between epoch and accuracy/loss.   """;Computer Vision;https://github.com/JacobM184/EfficientNet-for-Gun-detection
"""python B0_global.py (training requires CUDA enabled GPU with VRAM of at least 4GB - tested on machine with 8GB VRAM)   NOTE: These instructions are for use on a Windows machine. These may not work for Linux  MacOS  or other Operating Systems   These papers can be found at the following links:   * COCO   ![](graphics/B0_KERNEL9/trnacc_B0.png)   ![](graphics/B0_KERNEL9/trnloss_B0.png)  We can clearly see from the above graphs that there is a logarithmic relationship between epoch and accuracy/loss.   """;General;https://github.com/JacobM184/EfficientNet-for-Gun-detection
"""We use the same training data as [JDE](https://github.com/Zhongdao/Towards-Realtime-MOT). Please refer to their [DATA ZOO](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md) to download and prepare all the training data including Caltech Pedestrian  CityPersons  CUHK-SYSU  PRW  ETHZ  MOT17 and MOT16.   [2DMOT15](https://motchallenge.net/data/2D_MOT_2015/) and [MOT20](https://motchallenge.net/data/MOT20/) can be downloaded from the official webpage of MOT challenge. After downloading  you should prepare the data in the following structure: ``` MOT15    |‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîimages    |        ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîtrain    |        ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîtest    ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîlabels_with_ids             ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîtrain(empty) MOT20    |‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîimages    |        ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîtrain    |        ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîtest    ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîlabels_with_ids             ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîtrain(empty) ``` Then  you can change the seq_root and label_root in src/gen_labels_15.py and src/gen_labels_20.py and run: ``` cd src python gen_labels_15.py python gen_labels_20.py ``` to generate the labels of 2DMOT15 and MOT20. The seqinfo.ini files of 2DMOT15 can be downloaded here [[Google]](https://drive.google.com/open?id=1kJYySZy7wyETH4fKMzgJrYUrTfxKlN1w)  [[Baidu] code:8o0w](https://pan.baidu.com/s/1zb5tBW7-YTzWOXpd9IzS0g).   * Clone this repo  and we'll call the directory that you cloned as ${FAIRMOT_ROOT} * Install dependencies. We use python 3.7 and pytorch >= 1.2.0 ``` conda create -n FairMOT conda activate FairMOT conda install pytorch==1.2.0 torchvision==0.4.0 cudatoolkit=10.0 -c pytorch cd ${FAIRMOT_ROOT} pip install -r requirements.txt cd src/lib/models/networks/DCNv2_new sh make.sh ``` * We use [DCNv2](https://github.com/CharlesShang/DCNv2) in our backbone network and more details can be found in their repo.  * In order to run the code for demos  you also need to install [ffmpeg](https://www.ffmpeg.org/).   <img src=""assets/MOT15.gif"" width=""400""/>   <img src=""assets/MOT16.gif"" width=""400""/> <img src=""assets/MOT17.gif"" width=""400""/>   <img src=""assets/MOT20.gif"" width=""400""/>    | Dataset    |  MOTA | IDF1 | IDS | MT | ML | FPS | |--------------|-----------|--------|-------|----------|----------|--------| |2DMOT15  | 59.0 | 62.2 |  582 | 45.6% | 11.5% | 30.5 | |MOT16       | 68.7 | 70.4 | 953 | 39.5% | 19.0% | 25.9 | |MOT17       | 67.5 | 69.8 | 2868 | 37.7% | 20.8% | 25.9 | |MOT20       | 58.7 | 63.7 | 6013 | 66.3% | 8.5% | 13.2 |   All of the results are obtained on the [MOT challenge](https://motchallenge.net) evaluation server under the ‚Äúprivate detector‚Äù protocol. We rank first among all the trackers on 2DMOT15  MOT17 and the recently released (2020.02.29) MOT20. Note that our IDF1 score remarkably outperforms other one-shot MOT trackers by more than **10 points**. The tracking speed of the entire system can reach up to **30 FPS**.   sh experiments/all_dla34.sh  The default settings run tracking on the validation dataset from 2DMOT15. Using the DLA-34 baseline model  you can run:  cd src   cd src   cd src   sh experiments/ft_mot15_dla34.sh  sh experiments/ft_mot20_dla34.sh   cd src   <img src=""assets/MOT15.gif"" width=""400""/>   <img src=""assets/MOT16.gif"" width=""400""/> <img src=""assets/MOT17.gif"" width=""400""/>   <img src=""assets/MOT20.gif"" width=""400""/>    You can input a raw video and get the demo video by running src/demo.py and get the mp4 format of the demo video: ``` cd src python demo.py mot --load_model ../models/all_dla34.pth --conf_thres 0.4 ``` You can change --input-video and --output-root to get the demos of your own videos.  If you have difficulty building DCNv2 and thus cannot use the DLA-34 baseline model  you can run the demo with the HRNetV2_w18 baseline model (don't forget to comment lines with 'dcn' in src/libs/models/model.py if you do not build DCNv2):  ``` cd src python demo.py mot --load_model ../models/all_hrnet_v2_w18.pth --arch hrnet_18 --reid_dim 128 --conf_thres 0.4 ``` --conf_thres can be set from 0.3 to 0.7 depending on your own videos.   """;Computer Vision;https://github.com/FlorentijnD/FairMOT
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/Kate589/gan
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;Computer Vision;https://github.com/Kate589/gan
"""""";Computer Vision;https://github.com/VinishUchiha/Object-Detection-Using-Yolo4
"""        classes = ('plane'  'car'  'bird'  'cat'  'deer'                     'dog'  'frog'  'horse'  'ship'  'truck')          transforms = tv.transforms.ToTensor()          trainset = tv.datasets.CIFAR10(root=data_path                                         transform=transforms                                         download=download)          testset = tv.datasets.CIFAR10(root=data_path                                        transform=transforms  train=False                                        download=False)          return classes  trainset  testset       if __name__ == '__main__':           <p align=""center""> <img align=""center"" src =""https://github.com/akanimax/pro_gan_pytorch/blob/master/samples/celebA-HQ.gif""      height=80% width=80%/> </p> <br>      you wouldn't need to wrap the Generator into a DataParallel if you train on CPU. <br>   <p align=""center""> <img align=""center"" src =""https://github.com/akanimax/pro_gan_pytorch/blob/master/samples/demo.gif""  height=80% width=80%/> </p> <br>  The repository now includes a latent-space interpolation animation demo under the `samples/` directory. Just download all the pretrained weights from the above mentioned drive_link and put them in the `samples/`  directory alongside the `demo.py` script. Note that there are a few tweakable parameters at the beginning of the `demo.py` script so that you can play around with it. <br>  The demo loads up images for random points and then linearly interpolates among them to generate smooth  animation. You need to have a good GPU (atleast GTX 1070) to see formidable FPS in the demo. The demo however  can be optimized to do parallel generation of the images (It is completely sequential currently).  In order to load weights in the Generator  the process is the standard process for PyTorch model loading.          import torch as th     from pro_gan_pytorch import PRO_GAN as pg          device = th.device(""cuda"" if th.cuda.is_available() else ""cpu"")          gen = th.nn.DataParallel(pg.Generator(depth=9))     gen.load_state_dict(th.load(""GAN_GEN_SHADOW_8.pth""  map_location=str(device)))   1.) Install your appropriate version of PyTorch.  The torch dependency in this package uses the most basic ""cpu"" version. follow instructions on  <a href=""http://pytorch.org/""> http://pytorch.org </a> to  install the ""gpu"" version of PyTorch.<br>  2.)  Install this package using pip:          $ workon [your virtual environment]     $ pip install pro-gan-pth      3.) In your code:          import pro_gan_pytorch.PRO_GAN as pg    Use the modules `pg.Generator`  `pg.Discriminator` and  `pg.ProGAN`. Mostly  you'll only need the ProGAN   module for training. For inference  you will probably   need the `pg.Generator`.  4.) Example Code for CIFAR-10 dataset:      import torch as th     import torchvision as tv     import pro_gan_pytorch.PRO_GAN as pg               _  dataset  _ = setup_data(download=True)           """;Computer Vision;https://github.com/alexeyhorkin/ProGAN-PyTorch
"""1. install pillow  numpy and mpi4py 2. install cuda (10.2) version of pytrorch and torchvision          PAPER :       GAN  MD-GAN :         Generative Adversarial Network  Multi-Discriminator Generative Adversarial Networks for Distributed Datasets  		     AUTHORS	:            PAPER I - [ Ian J. Goodfellow   Jean Pouget-Abadie‚àó  Mehdi Mirza  Bing Xu  David Warde-Farley Sherjil Ozair‚Ä†  Aaron      Courville  Yoshua Bengio‚Ä° ]       PAPER II - [ Corentin Hardy  Erwan Le Merrer  Bruno Sericola ]        LINK :                 PAPER I :  https://arxiv.org/pdf/1406.2661.pdf                 PAPER II : https://arxiv.org/pdf/1811.03850v2.pdf  	************************************************************************************************************************************************************************************   	Main libraries :  		numpy			:	It's a multidimensional Array.  		mpi4py			:	MPI for Python supports convenient  pickle-based communication of generic Python object as well as fast  near C-speed  direct array data communication of buffer-provider objects.  		torch			:	An open source machine learning framework that accelerates the path from research prototyping to production deployment.(Official site)	  		torch.nn:		:	Base class for all neural network modules  our models is also subclass this class.		  		torch.optim		:	torch.optim is a package implementing various optimization algorithms. Most commonly used methods are already supported  and the interface is general enough  						 so that more sophisticated ones can be also easily integrated in the future.  		torch.utils.data	:	It represents a python iterable over a dataset.  		torch.nn.parallel	:	This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension.  		torchvision		:	The torchvision package consists of popular datasets  model architectures  and common image transformations for computer vision.  	Other libraries :  		random  os.   	************************************************************************************************************************************************************************************   	Important Variables :  		size			:	Represt the size of message pass to suffle the Discriminator using peer2peer fashion.  		rank			:	Push the Discriminator to use respective position (bcz we are using two discriminator  rank discriminator by 1/0 (1 : run next  0: currently running)) 		 		datasets		:	This varible is heart of the programme bcz this will download and store the dataset.   		dataloader		:	At the heart of PyTorch data loading utility is the torch.utils.data.DataLoader class  We are using one HUGE DATASET CIFAR10 in this implimentation.    	************************************************************************************************************************************************************************************   	Main Class :  		G()			:	This class is used to create generator Neural Network by using torch.nn.Module class.  		D()			:	This class is used to create Discriminator Neural Network by using torch.nn.Module class.   	************************************************************************************************************************************************************************************      	Main function :     		copyGenerator		:	Create a copy of generator to get the feedback of the generator to learn from them.  		shuffleDiscriminators	:	Shuffle the discriminator on the basis of rank after every 2 epochs.          THESE ARE SPECIAL FUNCTION USED FOR MD-GAN   	************************************************************************************************************************************************************************************   	Sudo code of implimentation of MD-GAN :  		Algorithm  1MD-GAN algorithm 				1:procedureWORKER(C Bn I L b) 				2:	InitializeŒ∏nforDn 				3:	fori‚Üê1toIdo 				4:		X(r)n‚ÜêSAMPLES(Bn b) 				5:		X(g)n X(d)n‚ÜêRECEIVEBATCHES(C) 				6:		forl‚Üê0toLdo 				7:			Dn‚ÜêDISCLEARNINGSTEP(Jdisc Dn) 				8:		end for 				9:	Fn‚Üê{‚àÇ ÃÉB(X(g)n)‚àÇxi|xi‚ààX(g)n} 				10:	SEND(C Fn).SendFnto server 				11:	ifimod (mEb) = 0then 				12:		Dn‚ÜêSWAP(Dn) 				13:	end if 				14:	end for 				15:end procedure 				16: 				17:procedureSWAP(Dn) 				18:	Wl‚ÜêGETRANDOMWORKER() 				19:	SEND(Wl Dn).SendDnto workerWl. 				20:	Dn‚ÜêRECEIVED().Receive a new discriminatorfrom another worker. 				21:	ReturnDn 				22:end procedure 				23: 				24:procedureSERVER(k I).Server C 				25:	InitializewforG 				26:	fori‚Üê1toIdo 				27:		forj‚Üê0tokdo 				28:			Zj‚ÜêGAUSSIANNOISE(b) 				29:			X(j)‚Üê{Gw(z)|z‚ààZj} 				30:		end for 				31:		X(d)1 ... X(d)n‚ÜêSPLIT(X(1) ... X(k)) 				32:		X(g)1 ... X(g)n‚ÜêSPLIT(X(1) ... X(k)) 				33:		for n‚Üê1toNdo 				34:			SEND(Wn (X(d)n X(g)n)) 				35:		end for 				36:		F1 ... FN‚ÜêGETFEEDBACKFROMWORKERS() 				37:		Compute‚àÜwaccording toF1 ... FN 				38:		for wi‚ààw do 				39:			wi‚Üêwi+ADAM(‚àÜwi) 				40:		end for 				41:	end for 				42:end procedure 				 ************************************************************************************************************************************************************************************ The results are really excited :   These are the generated image -    """;Computer Vision;https://github.com/SACHIN92012/Implementation-of-GAN-in-distributed-networks
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/DeligientSloth/bert-tensorflow
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/DeligientSloth/bert-tensorflow
"""""";Audio;https://github.com/MaxHolmberg96/WaveGAN
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   GPUs: K80 ($0.14/hr)  T4 ($0.11/hr)  V100 ($0.74/hr) CUDA with Nvidia Apex FP16/32     Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) < highly recommended!! * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Google Colab Notebook](https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb) with quick training  inference and testing examples * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart)  * [A TensorRT Implementation of YOLOv3 and YOLOv4](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp)    """;Computer Vision;https://github.com/harsh2011/Yolov3-Detector
"""We use the same training data as [JDE](https://github.com/Zhongdao/Towards-Realtime-MOT). Please refer to their [DATA ZOO](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md) to download and prepare all the training data including Caltech Pedestrian  CityPersons  CUHK-SYSU  PRW  ETHZ  MOT17 and MOT16.   [2DMOT15](https://motchallenge.net/data/2D_MOT_2015/) and [MOT20](https://motchallenge.net/data/MOT20/) can be downloaded from the official webpage of MOT challenge. After downloading  you should prepare the data in the following structure: ``` MOT15    |‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîimages    |        ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîtrain    |        ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîtest    ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîlabels_with_ids             ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîtrain(empty) MOT20    |‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîimages    |        ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîtrain    |        ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîtest    ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîlabels_with_ids             ‚îî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîtrain(empty) ``` Then  you can change the seq_root and label_root in src/gen_labels_15.py and src/gen_labels_20.py and run: ``` cd src python gen_labels_15.py python gen_labels_20.py ``` to generate the labels of 2DMOT15 and MOT20. The seqinfo.ini files of 2DMOT15 can be downloaded here [[Google]](https://drive.google.com/open?id=1kJYySZy7wyETH4fKMzgJrYUrTfxKlN1w)  [[Baidu] code:8o0w](https://pan.baidu.com/s/1zb5tBW7-YTzWOXpd9IzS0g).   * Clone this repo  and we'll call the directory that you cloned as ${FAIRMOT_ROOT} * Install dependencies. We use python 3.7 and pytorch >= 1.2.0 ``` conda create -n FairMOT conda activate FairMOT conda install pytorch==1.2.0 torchvision==0.4.0 cudatoolkit=10.0 -c pytorch cd ${FAIRMOT_ROOT} pip install -r requirements.txt cd src/lib/models/networks/DCNv2 sh make.sh ``` * We use [DCNv2](https://github.com/CharlesShang/DCNv2) in our backbone network and more details can be found in their repo.  * In order to run the code for demos  you also need to install [ffmpeg](https://www.ffmpeg.org/).   <img src=""assets/MOT15.gif"" width=""400""/>   <img src=""assets/MOT16.gif"" width=""400""/> <img src=""assets/MOT17.gif"" width=""400""/>   <img src=""assets/MOT20.gif"" width=""400""/>    | Dataset    |  MOTA | IDF1 | IDS | MT | ML | FPS | |--------------|-----------|--------|-------|----------|----------|--------| |2DMOT15  | 59.0 | 62.2 |  582 | 45.6% | 11.5% | 30.5 | |MOT16       | 68.7 | 70.4 | 953 | 39.5% | 19.0% | 25.9 | |MOT17       | 67.5 | 69.8 | 2868 | 37.7% | 20.8% | 25.9 | |MOT20       | 58.7 | 63.7 | 6013 | 66.3% | 8.5% | 13.2 |   All of the results are obtained on the [MOT challenge](https://motchallenge.net) evaluation server under the ‚Äúprivate detector‚Äù protocol. We rank first among all the trackers on 2DMOT15  MOT17 and the recently released (2020.02.29) MOT20. Note that our IDF1 score remarkably outperforms other one-shot MOT trackers by more than **10 points**. The tracking speed of the entire system can reach up to **30 FPS**.   sh experiments/all_dla34.sh  The default settings run tracking on the validation dataset from 2DMOT15. Using the DLA-34 baseline model  you can run:  cd src   cd src   cd src   sh experiments/ft_mot15_dla34.sh  sh experiments/ft_mot20_dla34.sh   cd src   <img src=""assets/MOT15.gif"" width=""400""/>   <img src=""assets/MOT16.gif"" width=""400""/> <img src=""assets/MOT17.gif"" width=""400""/>   <img src=""assets/MOT20.gif"" width=""400""/>    You can input a raw video and get the demo video by running src/demo.py and get the mp4 format of the demo video: ``` cd src python demo.py mot --load_model ../models/all_dla34.pth --conf_thres 0.4 ``` You can change --input-video and --output-root to get the demos of your own videos.  If you have difficulty building DCNv2 and thus cannot use the DLA-34 baseline model  you can run the demo with the HRNetV2_w18 baseline model:  ``` cd src python demo.py mot --load_model ../models/all_hrnet_v2_w18.pth --arch hrnet_18 --reid_dim 128 --conf_thres 0.4 ``` --conf_thres can be set from 0.3 to 0.7 depending on your own videos.   """;Computer Vision;https://github.com/ankitsinghsuraj/mot20
"""""";Computer Vision;https://github.com/jamiljami/Unet
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/halo090770/bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/halo090770/bert
"""""";Computer Vision;https://github.com/senthilva/CNN_CIFAR10_gradcam_cutout
"""This requires pytorch  tqdm  torchvision  tensorboardX  to be installed   Commit your changes: git commit -am 'Add some feature'   For training run `Python train.py`  For testing run `Python eval.py`   """;General;https://github.com/monishramadoss/SRGAN
"""This requires pytorch  tqdm  torchvision  tensorboardX  to be installed   Commit your changes: git commit -am 'Add some feature'   For training run `Python train.py`  For testing run `Python eval.py`   """;Computer Vision;https://github.com/monishramadoss/SRGAN
"""1. Create the dataset. So you can use your own images and add them in a folder  then run : ``` python data.py -p [path_of_the_dataset] -s [size_of_the_img] ``` 2. Then  you can train the model. Generated painting will be add in the out folder. ``` python runme.py -s [size_of_the_image] -e [epochs] -b [batch_size] -i [save_interval] ``` __TIPS__ : Depending on the power of your GPU  you may need to adapt the ""size"" of the training. To do so  you will have to adapt the image size or the batch_size. Moreover  depending on the quality of the data passed in parameter  it is possible that the network is not adapted to your needs  so you will have to review its structure.   """;Computer Vision;https://github.com/jokfun/FakePaint
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/hirohiro23/Darknet
"""``` import os import keras.backend as K  from data import DATA_SET_DIR from elmo.lm_generator import LMDataGenerator from elmo.model import ELMo  parameters = {     'multi_processing': False      'n_threads': 4      'cuDNN': True if len(K.tensorflow_backend._get_available_gpus()) else False      'train_dataset': 'wikitext-2/wiki.train.tokens'      'valid_dataset': 'wikitext-2/wiki.valid.tokens'      'test_dataset': 'wikitext-2/wiki.test.tokens'      'vocab': 'wikitext-2/wiki.vocab'      'vocab_size': 28914      'num_sampled': 1000      'charset_size': 262      'sentence_maxlen': 100      'token_maxlen': 50      'token_encoding': 'word'      'epochs': 10      'patience': 2      'batch_size': 1      'clip_value': 5      'cell_clip': 5      'proj_clip': 5      'lr': 0.2      'shuffle': True      'n_lstm_layers': 2      'n_highway_layers': 2      'cnn_filters': [[1  32]                      [2  32]                      [3  64]                      [4  128]                      [5  256]                      [6  512]                      [7  512]                     ]      'lstm_units_size': 400      'hidden_units_size': 200      'char_embedding_size': 16      'dropout_rate': 0.1      'word_dropout_rate': 0.05      'weight_tying': True  }  #: Set-up Generators train_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['train_dataset'])                                    os.path.join(DATA_SET_DIR  parameters['vocab'])                                    sentence_maxlen=parameters['sentence_maxlen']                                    token_maxlen=parameters['token_maxlen']                                    batch_size=parameters['batch_size']                                    shuffle=parameters['shuffle']                                    token_encoding=parameters['token_encoding'])  val_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['valid_dataset'])                                  os.path.join(DATA_SET_DIR  parameters['vocab'])                                  sentence_maxlen=parameters['sentence_maxlen']                                  token_maxlen=parameters['token_maxlen']                                  batch_size=parameters['batch_size']                                  shuffle=parameters['shuffle']                                  token_encoding=parameters['token_encoding'])  test_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['test_dataset'])                                  os.path.join(DATA_SET_DIR  parameters['vocab'])                                  sentence_maxlen=parameters['sentence_maxlen']                                  token_maxlen=parameters['token_maxlen']                                  batch_size=parameters['batch_size']                                  shuffle=parameters['shuffle']                                  token_encoding=parameters['token_encoding'])  #: Compile ELMo elmo_model = ELMo(parameters) elmo_model.compile_elmo(print_summary=True)  #: Train ELMo elmo_model.train(train_data=train_generator  valid_data=val_generator)  #: Persist ELMo Bidirectional Language Model in disk elmo_model.save(sampled_softmax=False)  #: Evaluate Bidirectional Language Model elmo_model.evaluate(test_generator)  #: Build ELMo meta-model to deploy for production and persist in disk elmo_model.wrap_multi_elmo_encoder(print_summary=True  save=True)  #: Load ELMo encoder elmo_model.load_elmo_encoder()  #: Get ELMo embeddings to feed as inputs for downstream tasks elmo_embeddings = elmo_model.get_outputs(test_generator  output_type='word'  state='mean')  #: BUILD & TRAIN NEW KERAS MODEL FOR DOWNSTREAM TASK (E.G.  TEXT CLASSIFICATION)  ```   """;Sequential;https://github.com/kafura-kafiri/tf2-elmo
"""``` import os import keras.backend as K  from data import DATA_SET_DIR from elmo.lm_generator import LMDataGenerator from elmo.model import ELMo  parameters = {     'multi_processing': False      'n_threads': 4      'cuDNN': True if len(K.tensorflow_backend._get_available_gpus()) else False      'train_dataset': 'wikitext-2/wiki.train.tokens'      'valid_dataset': 'wikitext-2/wiki.valid.tokens'      'test_dataset': 'wikitext-2/wiki.test.tokens'      'vocab': 'wikitext-2/wiki.vocab'      'vocab_size': 28914      'num_sampled': 1000      'charset_size': 262      'sentence_maxlen': 100      'token_maxlen': 50      'token_encoding': 'word'      'epochs': 10      'patience': 2      'batch_size': 1      'clip_value': 5      'cell_clip': 5      'proj_clip': 5      'lr': 0.2      'shuffle': True      'n_lstm_layers': 2      'n_highway_layers': 2      'cnn_filters': [[1  32]                      [2  32]                      [3  64]                      [4  128]                      [5  256]                      [6  512]                      [7  512]                     ]      'lstm_units_size': 400      'hidden_units_size': 200      'char_embedding_size': 16      'dropout_rate': 0.1      'word_dropout_rate': 0.05      'weight_tying': True  }  #: Set-up Generators train_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['train_dataset'])                                    os.path.join(DATA_SET_DIR  parameters['vocab'])                                    sentence_maxlen=parameters['sentence_maxlen']                                    token_maxlen=parameters['token_maxlen']                                    batch_size=parameters['batch_size']                                    shuffle=parameters['shuffle']                                    token_encoding=parameters['token_encoding'])  val_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['valid_dataset'])                                  os.path.join(DATA_SET_DIR  parameters['vocab'])                                  sentence_maxlen=parameters['sentence_maxlen']                                  token_maxlen=parameters['token_maxlen']                                  batch_size=parameters['batch_size']                                  shuffle=parameters['shuffle']                                  token_encoding=parameters['token_encoding'])  test_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['test_dataset'])                                  os.path.join(DATA_SET_DIR  parameters['vocab'])                                  sentence_maxlen=parameters['sentence_maxlen']                                  token_maxlen=parameters['token_maxlen']                                  batch_size=parameters['batch_size']                                  shuffle=parameters['shuffle']                                  token_encoding=parameters['token_encoding'])  #: Compile ELMo elmo_model = ELMo(parameters) elmo_model.compile_elmo(print_summary=True)  #: Train ELMo elmo_model.train(train_data=train_generator  valid_data=val_generator)  #: Persist ELMo Bidirectional Language Model in disk elmo_model.save(sampled_softmax=False)  #: Evaluate Bidirectional Language Model elmo_model.evaluate(test_generator)  #: Build ELMo meta-model to deploy for production and persist in disk elmo_model.wrap_multi_elmo_encoder(print_summary=True  save=True)  #: Load ELMo encoder elmo_model.load_elmo_encoder()  #: Get ELMo embeddings to feed as inputs for downstream tasks elmo_embeddings = elmo_model.get_outputs(test_generator  output_type='word'  state='mean')  #: BUILD & TRAIN NEW KERAS MODEL FOR DOWNSTREAM TASK (E.G.  TEXT CLASSIFICATION)  ```   """;Natural Language Processing;https://github.com/kafura-kafiri/tf2-elmo
"""""";Computer Vision;https://github.com/JifeiWang-WHU/Pytorch_Building_extraction
"""      https://github.com/tkipf/gcn.   See https://github.com/tkipf/gcn/issues/4#issuecomment-274445114  for a better explanation - with figures   """;Graphs;https://github.com/ChengSashankh/gcn-graph-classification
"""```python from Mixup import mixup  batch_x  batch_y = mixup(alpha  batch_x  batch_y) ```   """;Computer Vision;https://github.com/Yangget/Mixup_All-use
"""""";Computer Vision;https://github.com/Kaushal28/CutMix-Regularization-using-PyTorch
"""You will first need to launch a Virtual Machine (VM) on Google Cloud. Details about launching the VM can be found at the [Google Cloud Documentation](http://cloud/compute/docs/instances/create-start-instance).  In order to run training or eval on Cloud TPUs  you must set up the following variables based on your project  zone and GCS bucket appropriately. Please refer to the [Cloud TPU Quickstart](https://cloud.google.com/tpu/docs/quickstart) guide for more details.  ```sh export PROJECT=your_project_name export ZONE=your_project_zone export BUCKET=gs://yourbucket/ export TPU_NAME=t5-tpu export DATA_DIR=""${BUCKET}/your_data_dir"" export MODEL_DIR=""${BUCKET}/your_model_dir"" ```  Please use the following command to create a TPU device in the Cloud VM.  ```sh ctpu up --name=$TPU_NAME --project=$PROJECT --zone=$ZONE --tpu-size=v3-8  \         --tpu-only   --tf-version=1.15 --noconf ```    To install the T5 package  simply run:  ```sh pip install t5[gcp] ```   You may either use a new or pre-existing `Task`  or you may load examples from a preprocessed TSV file.   GPU Usage   Depending on your data source (see above)  you will need to prepare your data appropriately.   Make sure your files are accessible to the TPU (i.e.  are in a GCS bucket)  and you should be good to go!   You can also use greedy_decode.gin or sample_decode.gin instead of beam_search.gin in the command above.   You can also use beam_search.gin or greedy_decode.gin instead of sample_decode.gin in the command above.   The easiest way to try out T5 is with a free TPU in our [Colab Tutorial](https://tiny.cc/t5-colab).  Below we provide examples for how to pre-train  fine-tune  evaluate  and decode from a model from the command-line with our codebase. You can use these instructions to reproduce our results  fine-tune one of our released checkpoints with your own data and/or hyperparameters  or pre-train a model from scratch.   If you would like to use GPU instead of TPUs  you can modify the above commands by removing TPU-specific flags (`--tpu`  `--tpu_zone`  `--gcp_project`) and setting the gin params for `mesh_shape` and `mesh_devices` based on your desired setup.  For example  if your machine has access to 6 GPUs and you'd like to do 3-way model parallelism and 2-way data parallelism  the fine-tuning command above would become:  ```sh t5_mesh_transformer  \   --model_dir=""${MODEL_DIR}"" \   --t5_tfds_data_dir=""${DATA_DIR}"" \   --gin_file=""dataset.gin"" \   --gin_param=""utils.run.mesh_shape = 'model:3 batch:2'"" \   --gin_param=""utils.run.mesh_devices = ['gpu:0' 'gpu:1' 'gpu:2' 'gpu:3' 'gpu:4' 'gpu:5']"" \   --gin_param=""MIXTURE_NAME = 'glue_mrpc_v002'"" \   --gin_file=""gs://t5-data/pretrained_models/small/operative_config.gin"" ```  With a single GPU  the command is:  ```sh t5_mesh_transformer  \   --model_dir=""${MODEL_DIR}"" \   --t5_tfds_data_dir=""${DATA_DIR}"" \   --gin_file=""dataset.gin"" \   --gin_param=""utils.run.mesh_shape = 'model:1 batch:1'"" \   --gin_param=""utils.run.mesh_devices = ['gpu:0']"" \   --gin_param=""MIXTURE_NAME = 'glue_mrpc_v002'"" \   --gin_file=""gs://t5-data/pretrained_models/small/operative_config.gin"" ```    """;Natural Language Processing;https://github.com/KAGUYAHONGLAI/SRC
"""``` git clone https://github.com/ART-Group-it/KERMIT.git  pip install ./KERMIT/kerMIT ```  Presentation: https://slideslive.com/38938864   |       ‚îú‚îÄ‚îÄ KERMIT_encoder.ipynb     &lt;- Jupyter Notebook for saving the KERMIT encoded trees   - KERMIT can be used to enhance Transformers' performance on various linguistic tasks adding relevant syntactic information from parse trees - It is lightweight compared to a Transformer model - KERMIT decision can be interpreted using this library and it is possible to visualize heat parse trees.   - **KERMIT encoder** - Build syntactic input from a custom dataset [notebook 1](https://github.com/ART-Group-it/KERMIT/blob/master/examples/Notebooks/KERMIT_encoder.ipynb).  - **KERMIT + BERT mode**l - Train the model and save the weights [notebook 2](https://github.com/ART-Group-it/KERMIT/blob/master/examples/Notebooks/KERMIT_training.ipynb).  - **KERMITviz** - Visualize how much the syntax affects the final choice of the model [notebook 3](https://github.com/ART-Group-it/KERMIT/blob/master/examples/Notebooks/KERMITviz.ipynb) or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ART-Group-it/KERMIT/blob/master/examples/Notebooks/KERMITviz_Colab.ipynb)  - **New Version KERMITviz** - Visualize how much the syntax affects the final choice of the model [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ART-Group-it/KERMIT/blob/master/examples/Notebooks/KERMITviz__.ipynb)   """;General;https://github.com/ART-Group-it/KERMIT
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/kiruthick101/darknet
"""""";Computer Vision;https://github.com/vetlebh/unet
"""""";General;https://github.com/MiX-S/resnet_cifar10
"""""";Computer Vision;https://github.com/MiX-S/resnet_cifar10
"""            * No summary available !!!!!!         ''' style={'background-color':'#87ceeb'                                                                  'foreground-color':'red'                                                                  'color':'black'                                                                  'fontSize':12                                                                  'textAlign': 'left'                                                                  'verticalAlign':'top'                                                                  'position':'fixed'                                                                  'width':'50%'                                                                  'height':'18%'                                                                  'top':'80%'                                                                  'left':'27%'                                                                  'border-radius':10})   #######################   #if __name__ == '__main__':                 * Tracking is done quarterly at 3 different levels to give us a long term as well as short term indication of model performance         ''' style={'background-color':'#87ceeb'                                                                  'foreground-color':'red'                                                                  'color':'black'                                                                  'fontSize':12                                                                  'textAlign': 'left'                                                                  'verticalAlign':'top'                                                                  'position':'fixed'                                                                  'width':'50%'                                                                  'height':'18%'                                                                  'top':'80%'                                                                  'left':'27%'                                                                  'border-radius':10})     elif model_no == '111111':         return dcc.Markdown('''                   * Tracking is done quarterly at 3 different levels to give us a long term as well as short term indication of model performance         *Results: Q2 UTM Tracking is rates GREEN at overall level         * Segment level some deterioration is observed.         * Segment 1‚Äôs KS/PSI shifts are due to a known issue where the benchmark dataset with ‚Äúvery long ARF (Automated Response Format)‚Äù records (rich bureau history and FICO scores) were treated incorrectly by FICO during development         time. The validation dataset does not have this issue.         * Booked PSI shifts for segments 1  2  and 3 are caused by tightening risk tolerance since the 2010 benchmark time period.         * Segment 5's deterioration led to a deep dive in 2018 that found that 3 input attribute's had an amber rating for PSI (>10%)         ''' style={'background-color':'#87ceeb'                                                                  'foreground-color':'red'                                                                  'color':'black'                                                                  'fontSize':12                                                                  'textAlign': 'left'                                                                  'verticalAlign':'top'                                                                  'position':'fixed'                                                                  'width':'50%'                                                                  'height':'18%'                                                                  'top':'80%'                                                                  'left':'27%'                                                                  'border-radius':10})     elif model_no == '803456':         return dcc.Markdown('''           f = function([x  In(y  value=1)]  z)   f=th.fucntion([x] y)   import theano as th import os os.environ['KERAS_BACKEND']='theano' import keras as kr import numpy as np import keras as kr      app = dash.Dash(name)   app = dash.Dash(name)           } name=k           } name=k           } name=k   app = Flask(name)   app = dash.Dash(name   app = dash.Dash(name)       dash.dependencies.Output('op-table'  'children')    app = dash.Dash(name   def lin_re(X Y q):               W.set_value([0.1  0.9])  c = theano.tensor.dot(a  a)  html.Div([ html.Div([         html.Div([             dcc.Dropdown(                 id='xaxis-column'                  options=[{'label': i  'value': i} for i in cols]                  value='Numeric 1'             )              dcc.RadioItems(                 id='xaxis-type'                  options=[{'label': i  'value': i} for i in ['Linear'  'Log']]                  value='Linear'                  labelStyle={'display': 'inline-block'}             )         ]  style={'width': '48%'  'display': 'inline-block'})    html.Div([             dcc.Dropdown(                 id='yaxis-column'                  options=[{'label': i  'value': i} for i in cols]                  value='Numeric 2'             )              dcc.RadioItems(                 id='yaxis-type'                  options=[{'label': i  'value': i} for i in ['Linear'  'Log']]                  value='Linear'                  labelStyle={'display': 'inline-block'}             )         ] style={'width': '48%'  'float': 'right'  'display': 'inline-block'})                 ])  html.Div([              dcc.Dropdown(                 id='color-selector'                  options=[{'label': i  'value': i} for i in cols]                  value='TIER_GRP'         )         ] style={'width': '48%'  'display': 'inline-block'})   dcc.Graph(id='indicator-graphic')  ])    html.Div([        html.Div([     html.Div([             html.Div([                 dcc.Dropdown(                     id='xaxis-column'                      options=[{'label': i  'value': i} for i in num_cols]                      value=num_cols[0]                 )                  dcc.RadioItems(                     id='xaxis-type'                      options=[{'label': i  'value': i} for i in ['Linear'  'Log']]                      value='Linear'                      labelStyle={'display': 'inline-block'}                 )             ]      style={'width': '48%'  'display': 'inline-block'})       html.Div([                 dcc.Dropdown(                     id='yaxis-column'                      options=[{'label': i  'value': i} for i in num_cols]                      value=num_cols[0]                 )                  dcc.RadioItems(                     id='yaxis-type'                      options=[{'label': i  'value': i} for i in ['Linear'  'Log']]                      value='Linear'                      labelStyle={'display': 'inline-block'}                 )             ] style={'width': '48%'  'float': 'right'  'display': 'inline-block'})                         ])      html.Div([                  dcc.Dropdown(                     id='color-selector'                      options=[{'label': i  'value': i} for i in cat_cols]                      value=cat_cols[0]             )             ] style={'width': '48%'  'display': 'inline-block'})       dcc.Graph(id='indicator-graphic')      ])        #df=pd.read_csv('df.csv')   df_def = pd.read_csv('database/definitions.csv') ks=pd.read_csv('ks.csv') #print(ks.head())       def generate_table():     return html.Table(            html.Div(   [     html.H1(children='CCB Risk - Model Monitoring' style={'text-align': 'left' 'backgroundColor':' #5381ac' 'padding-bottom':'5px' 'padding-top':'5px'  ""color"":'white'       'fontFamily': 'Calibri' ""border"":""1px grey solid""  'padding-left':'20px'})           html.H2(children='31980 - CAF Full Spectrum Indirect Origination(Gen 5b)' style={'text-align': 'left' 'backgroundColor':' #92adce' 'padding-bottom':'5px' 'padding-top':'5px'  ""color"":'white'       'fontFamily': 'Calibri' ""border"":""1px grey solid""  'padding-left':'40px'  				   'font-size': '20px' })  html.Div([definition()] style={'padding-bottom':'20px'})                 html.Div([etb()] style={'padding-bottom':'2px'})           html.Div([generate_exe(em=dem)] style={'padding-bottom':'12px'  'fontColor': 'blue'})         html.H3(""Select the Plan (Peformance Measure)"" style={'text-align': 'left' 'backgroundColor':' #92adce' 'padding-bottom':'5px' 'padding-top':'5px'  ""color"":'white'       'fontFamily': 'Calibri' ""border"":""1px grey solid""  'padding-left':'40px'  				   'font-size': '20px' })   html.Div([dcc.Dropdown(id='table-dpdn'      options=[         {'label': 'Early Warning 1'  'value': 'ew1'}          {'label': 'Early Warning 2'  'value': 'ew2'}          {'label':'Model Performance by Model Segments' 'value':'mperf_ms'}     ]      value='ew1'  ) ] style={'padding-bottom':'25px'})                #html.Div([ #html.Div([ms_ttd()] style={'width':'20%' 'display': 'inline-block'})  html.Div([  html.Div(id='op-table' style={'display': 'inline-block'  'width':'80%'})  html.Div([ms_ttd()] style={'width':'20%' 'display': 'inline-block'})  ] style={'width':'100%'})  html.Br()                     #html.Div(id='op-table' style={'padding-bottom':'10px'  'width':'80%'})  html.H3(""Select the Performance Metric"" style={'text-align': 'left' 'backgroundColor':' #92adce' 'padding-bottom':'5px' 'padding-top':'5px'  ""color"":'white'       'fontFamily': 'Calibri' ""border"":""1px grey solid""  'padding-left':'40px'  				   'font-size': '20px' })                                                    html.Div([dcc.Dropdown(id='xaxis-column1'      options=[             {'label': 'KS'  'value': 'KS'}              {'label': 'PDO'  'value': 'PDO'}         ]          value='KS' ) ] style={'padding-bottom':'25px'} )   html.Div([dcc.Graph(id='graph11')] style={""border"":""1px grey solid"" 'padding-bottom':'15px'  'fontcolor':'white'})  html.Br()   html.Div([ html.H4(""Model performance by business segments"" style={'text-align': 'left' 'backgroundColor':' #92adce' 'padding-bottom':'5px' 'padding-top':'5px'  ""color"":'white'       'fontFamily': 'Calibri' ""border"":""1px grey solid""  'padding-left':'40px'  				   'font-size': '20px' })   html.Div(id='BS-table' style={'width':'80%' 'display': 'inline-block'})  html.Div([bs_ttd()] style={'width':'20%' 'display': 'inline-block'})  ] style={'width':'100%'})  html.Br()   html.H5(""Performance on model segment by decile"" style={'text-align': 'left' 'backgroundColor':' #92adce' 'padding-bottom':'5px' 'padding-top':'5px'  ""color"":'white'       'fontFamily': 'Calibri' ""border"":""1px grey solid""  'padding-left':'40px'  				   'font-size': '20px' })   html.Div([ 	dcc.Dropdown(     id='Segment-selection'      options=[{'label': i 'value': i} for i in list(ks['segment'].unique())]          value='Full Model'     )] style={'width':'25%' 'display': 'inline-block'})   html.Div(id = 'Segment-deep-dive' style={'padding-bottom':'10px'} )                ################## @app.callback(         Output('graph11'  'figure')                [Input('table-dpdn'  'value') Input('xaxis-column1'  'value')])          def update(model_value metric_value):     df=pd.read_csv('graph3.csv')        if model_value=='ew1':       model_value='Early Warning 1'     elif model_value=='ew2':       model_value='Early Warning 2'     elif model_value=='mperf_ms':       model_value= 'PERFORMANCE BY MODEL SEGMENT'          df1=df[df['Plan_Name']==model_value]               data=df1[df1['METRIC_KEY']==metric_value]               data=data.sort_values(by='Time_Frame')     #print(data['Time_Frame'].unique())          #print(df.head(50))          fig = make_subplots(rows=1  cols=10)     fig.add_bar(x=data['Time_Frame'][data['SEGMENT_ID'] == ""Full Model""].values y=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Full Model""].values marker=dict(color=colors)  row=1  col=1 showlegend=False name='Full Model' text=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Full Model""].values                textposition=""auto"" textangle =-90                textfont={                         'family':""calibri""                          ""size"":20                          ""color"":""white""                          } )     fig.add_bar(x=data['Time_Frame'][data['SEGMENT_ID'] == ""Segment 1 - No Record No TL""].values y=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 1 - No Record No TL""].values marker=dict(color=colors)       row=1  col=2 showlegend=False name='Segment 1 - No Record No TL' text=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 1 - No Record No TL""].values                textposition=""auto"" textangle =-90                textfont={                         'family':""calibri""                          ""size"":20                          ""color"":""white""                         }  )          fig.add_bar(x=data['Time_Frame'][data['SEGMENT_ID'] == ""Segment 2 - Thin File""].values y=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 2 - Thin File""].values marker=dict(color=colors)  row=1  col=3 showlegend=False name='Segment 2 - Thin File'  text=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 2 - Thin File""].values                textposition=""auto"" textangle =-90                textfont={                         'family':""calibri""                          ""size"":20                          ""color"":""white""                         } )     fig.add_bar(x=data['Time_Frame'][data['SEGMENT_ID'] == ""Segment 3 - Severe Derog""].values y=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 3 - Severe Derog""].values marker=dict(color=colors)       row=1  col=4 showlegend=False name='Segment 3 - Severe Derog' text=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 3 - Severe Derog""].values                textposition=""inside"" textangle =-90                textfont={                         'family':""calibri""                          ""size"":30                          ""color"":""white""                         }  )          fig.add_bar(x=data['Time_Frame'][data['SEGMENT_ID'] == ""Segment 4 - Delinquent under 90""].values y=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 4 - Delinquent under 90""].values marker=dict(color=colors)  row=1  col=5 showlegend=False name='Segment 4 - Delinquent under 90' text=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 4 - Delinquent under 90""].values                textposition=""inside"" textangle =-90                textfont={                         'family':""calibri""                          ""size"":30                          ""color"":""white""                         }  )     fig.add_bar(x=data['Time_Frame'][data['SEGMENT_ID'] == ""Segment 5 - New to Credit""].values y=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 5 - New to Credit""].values marker=dict(color=colors)  row=1  col=6 showlegend=False name='Segment 5 - New to Credit' text=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 5 - New to Credit""].values                textposition=""inside"" textangle =-90                textfont={                         'family':""calibri""                          ""size"":30                          ""color"":""white""                         }  )          fig.add_bar(x=data['Time_Frame'][data['SEGMENT_ID'] == ""Segment 6 - Mature Low Utilization""].values y=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 6 - Mature Low Utilization""].values marker=dict(color=colors)  row=1  col=7 showlegend=False name='Segment 6 - Mature Low Utilization' text=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 6 - Mature Low Utilization""].values                textposition=""inside"" textangle =-90                textfont={                         'family':""calibri""                          ""size"":30                          ""color"":""white""                         }  )     fig.add_bar(x=data['Time_Frame'][data['SEGMENT_ID'] == ""Segment 7 - Mature High Utilization""].values y=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 7 - Mature High Utilization""].values marker=dict(color=colors)  row=1  col=8 showlegend=False name='Segment 7 - Mature High Utilization' text=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 7 - Mature High Utilization""].values                textposition=""inside"" textangle =-90                textfont={                         'family':""calibri""                          ""size"":30                          ""color"":""white""                         }  )          fig.add_bar(x=data['Time_Frame'][data['SEGMENT_ID'] == ""Segment 8 - Mature Less Delinquent""].values y=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 8 - Mature Less Delinquent""].values marker=dict(color=colors)  row=1  col=9 showlegend=False name='Segment 8 - Mature Less Delinquent'  text=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 8 - Mature Less Delinquent""].values                textposition=""inside"" textangle =-90                textfont={                         'family':""calibri""                          ""size"":30                          ""color"":""white""                         } )     fig.add_bar(x=data['Time_Frame'][data['SEGMENT_ID'] == ""Segment 9 - Mature More Delinquent""].values y=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 9 - Mature More Delinquent""].values marker=dict(color=colors)  row=1  col=10 showlegend=False name='Segment 9 - Mature More Delinquent'  text=data['METRIC_VALUE'][data['SEGMENT_ID'] == ""Segment 9 - Mature More Delinquent""].values                textposition=""inside"" textangle =-90                textfont={                         'family':""calibri""                          ""size"":30                          ""color"":""white""                         } )                               #fig.update_traces(textposition='outside')          fig.layout.xaxis.title=None               fig.layout.xaxis.tickangle=-70          fig.layout.xaxis2.title=None     fig.layout.xaxis2.tickangle=-70          fig.layout.xaxis3.title=None     fig.layout.xaxis3.tickangle=-70          fig.layout.xaxis4.title=None     fig.layout.xaxis4.tickangle=-70          fig.layout.xaxis5.title=None     fig.layout.xaxis5.tickangle=-70          fig.layout.xaxis6.title=None     fig.layout.xaxis6.tickangle=-70          fig.layout.xaxis7.title=None     fig.layout.xaxis7.tickangle=-70          fig.layout.xaxis8.title=None     fig.layout.xaxis8.tickangle=-70          fig.layout.xaxis9.title=None     fig.layout.xaxis9.tickangle=-70          fig.layout.xaxis10.title=None     fig.layout.xaxis10.tickangle=-70          fig.layout.height=500     fig.layout.plot_bgcolor = 'white'          fig.update_yaxes(showticklabels=False)          fig = fig.update_layout({""annotations"": [       {         ""x"": 0.04          ""y"": 1.08          ""font"": {           ""size"": 11         }          ""text"": ""Full Model""          ""xref"": ""paper""          ""yref"": ""paper""          ""xanchor"": ""center""          ""yanchor"": ""top""          ""showarrow"": False               }        {         ""x"": 0.15          ""y"": 1.08          ""font"": {           ""size"": 11         }          ""text"": ""Segment 1 <br> No Record No TL""          ""xref"": ""paper""          ""yref"": ""paper""          ""xanchor"": ""center""          ""yanchor"": ""top""          ""showarrow"": False       }        {         ""x"": 0.25          ""y"": 1.08          ""font"": {           ""size"": 11         }          ""text"": ""Segment 2 <br>Thin File""          ""xref"": ""paper""          ""yref"": ""paper""          ""xanchor"": ""center""          ""yanchor"": ""middle""          ""showarrow"": False       }      {         ""x"": 0.35          ""y"": 1.08          ""font"": {           ""size"": 11         }           ""text"": ""Segment 3 <br> Severe Derog""          ""xref"": ""paper""          ""yref"": ""paper""          ""xanchor"": ""center""          ""yanchor"": ""middle""          ""showarrow"": False       }      {         ""x"": 0.45          ""y"": 1.08         ""font"": {           ""size"": 11         }          ""text"": ""Segment 4 <br> Delinquent under 90""          ""xref"": ""paper""          ""yref"": ""paper""          ""xanchor"": ""center""          ""yanchor"": ""middle""          ""showarrow"": False       }      {         ""x"": 0.55          ""y"": 1.08          ""font"": {           ""size"": 11         }          ""text"": ""Segment 5 <br> New to Credit""          ""xref"": ""paper""          ""yref"": ""paper""          ""xanchor"": ""center""          ""yanchor"": ""middle""          ""showarrow"": False       }      {         ""x"": 0.655          ""y"": 1.08          ""font"": {           ""size"": 11         }          ""text"": ""Segment 6 <br> Mature Low Utilization""          ""xref"": ""paper""          ""yref"": ""paper""          ""xanchor"": ""center""          ""yanchor"": ""middle""          ""showarrow"": False       }      {         ""x"": 0.758          ""y"": 1.08          ""font"": {           ""size"": 11         }          ""text"": ""Segment 7 <br> Mature High Utilization""          ""xref"": ""paper""          ""yref"": ""paper""          ""xanchor"": ""center""          ""yanchor"": ""middle""          ""showarrow"": False       }      {         ""x"": 0.86          ""y"": 1.08          ""font"": {           ""size"": 11         }          ""text"": ""Segment 8 <br> Mature Less Delinquent""          ""xref"": ""paper""          ""yref"": ""paper""          ""xanchor"": ""center""          ""yanchor"": ""middle""          ""showarrow"": False       }      {         ""x"": 0.96          ""y"": 1.08          ""font"": {           ""size"": 11         }          ""text"": ""Segment 9 <br> Mature More Delinquent""          ""xref"": ""paper""          ""yref"": ""paper""          ""xanchor"": ""center""          ""yanchor"": ""middle""          ""showarrow"": False       }      ]})      return fig.update_traces(marker_color=colors)         #return fig.show()  @app.callback(     dash.dependencies.Output('op-table'  'children')      [dash.dependencies.Input(""table-dpdn""  ""value"")]  )  def update_option(search_value):       return ew11(search_value)    @app.callback(     dash.dependencies.Output('Segment-deep-dive'  'children')      [dash.dependencies.Input('table-dpdn'  'value')      dash.dependencies.Input('Segment-selection'  'value')      ])        def update_options(search_valuer sr2):     #print(""Got here   BOOOOO!!!!!!!!!!!!!!!!!!!!!!!!!!!!"")     return ks_seg(search_valuer sr2)   @app.callback(     dash.dependencies.Output('BS-table'  'children')      [dash.dependencies.Input('table-dpdn'  'value')      ])        def update_options(search_valuebs):     #print(""Got here   BOOOOO!!!!!!!!!!!!!!!!!!!!!!!!!!!!"")     return m_perf_bseg(search_valuebs)   ################### """;Computer Vision;https://github.com/vaibhavtmnit/Theano-Projects
"""I used the pandas library to calculate summary statistics of the traffic signs data set:  * Number of training examples = 34799 * Number of validation examples = 4410 * Number of testing examples = 12630 * Image data shape = (32  32  3) * Number of classes = 43   Here are the results of the prediction:  | Image			        |     Prediction	        					|  |:---------------------:|:---------------------------------------------:|  | Children crossing     | Children crossing  							|  | Right-of-way          | Right-of-way									| | Priority road			| Priority road									| | Turn right ahead 		| Turn right ahead				 				| | Road work 			| Road work         							|   The model was able to correctly guess 5 of the 5 traffic signs  which gives an accuracy of 100%  which is close to 98% from the test set.   My final model results were: * training set accuracy of 99%+ * validation set accuracy of 99%+ * test set accuracy of 98%+  If a well known architecture was chosen: * Inception v4 was chosen for traffic sign classifier. * This is a very suitable architecture because it has a very high accuracy for classifier (general inception v4 can be used to classify 1000 classes)  and it is quite efficient. * It can be concluded this model works very well because all 3 data sets have very high accuracies  which means the model is not under or over fitting (balanced variance and bias).  Additional visualization of the validation accuracy is analyzed to understand what works or not.  Below is the validation set recall and precision by class.  Note that class 16 has a low recall (false negative)  meaning images from class 16 were predicted as some other clases.  In precision chart  class 41 has a low value (false positive).  It is likely that many class 16 images were misclassified as class 41.  ![alt text][image9] ![alt text][image10]  Images were pulled from classes 16 and 41 and quickly one can see that some class 16 images have red circular borders are quite faded so they could be similar to class 41 images.  Below are classes 16 (left) and 41 (right) sample images.  ![alt text][image11]   First  I normalized all training images to have float32 from 0 to 1.  I found the accuracy increases faster than -1 to 1 during network training. The matricies below simply show one random image before and after normalization.  Before normalization:  [[51 36 45 ... 80 79 73]   [47 34 38 ... 64 75 79]    [45 32 38 ... 61 68 71]    ...    [43 38 34 ... 46 42 37]    [44 36 31 ... 36 33 35]    [41 36 38 ... 52 48 50]]   After normalization:  [[0.1849315  0.08219178 0.14383562 ... 0.38356164 0.37671232 0.33561644]   [0.15753424 0.06849315 0.09589041 ... 0.2739726  0.34931508 0.37671232]    [0.14383562 0.05479452 0.09589041 ... 0.25342464 0.30136988 0.3219178 ]    ...    [0.13013698 0.09589041 0.06849315 ... 0.15068494 0.12328767 0.0890411 ]    [0.1369863  0.08219178 0.04794521 ... 0.08219178 0.06164384 0.07534247]    [0.11643836 0.08219178 0.09589041 ... 0.19178082 0.16438356 0.1780822 ]]     As mentioned before  data augmentation is applied to even out image quantity difference among classes  and to include variations of same images.  * sharpen or smoothing * random rotate image * random stretch/squeeze image  * random darken partial image * random move image  Here is an example of a traffic sign image before and after augmentation.  The image is stretched horizontally and partially darkened at the bottom.  ![alt text][image4] ![alt text][image5]  When all training images are added up  the quantity shows:  ![alt text][image6]  As a last step  the training set is shuffled to remove any order.    I used the pandas library to calculate summary statistics of the traffic signs data set:  * Number of training examples = 34799 * Number of validation examples = 4410 * Number of testing examples = 12630 * Image data shape = (32  32  3) * Number of classes = 43   You're reading it! and here is a link to my [project code](https://github.com/waynecoffee9/Traffic-Sign-Classifier/blob/master/CarND-Traffic-Sign-Classifier-Project/Traffic_Sign_Classifier-inception.ipynb) If you are unable to view it under github  use https://nbviewer.jupyter.org/ and paste the link to view.   You're reading it! and here is a link to my [project code](https://github.com/waynecoffee9/Traffic-Sign-Classifier/blob/master/CarND-Traffic-Sign-Classifier-Project/Traffic_Sign_Classifier-inception.ipynb) If you are unable to view it under github  use https://nbviewer.jupyter.org/ and paste the link to view.   First  I normalized all training images to have float32 from 0 to 1.  I found the accuracy increases faster than -1 to 1 during network training. The matricies below simply show one random image before and after normalization.  Before normalization:  [[51 36 45 ... 80 79 73]   [47 34 38 ... 64 75 79]    [45 32 38 ... 61 68 71]    ...    [43 38 34 ... 46 42 37]    [44 36 31 ... 36 33 35]    [41 36 38 ... 52 48 50]]   After normalization:  [[0.1849315  0.08219178 0.14383562 ... 0.38356164 0.37671232 0.33561644]   [0.15753424 0.06849315 0.09589041 ... 0.2739726  0.34931508 0.37671232]    [0.14383562 0.05479452 0.09589041 ... 0.25342464 0.30136988 0.3219178 ]    ...    [0.13013698 0.09589041 0.06849315 ... 0.15068494 0.12328767 0.0890411 ]    [0.1369863  0.08219178 0.04794521 ... 0.08219178 0.06164384 0.07534247]    [0.11643836 0.08219178 0.09589041 ... 0.19178082 0.16438356 0.1780822 ]]     As mentioned before  data augmentation is applied to even out image quantity difference among classes  and to include variations of same images.  * sharpen or smoothing * random rotate image * random stretch/squeeze image  * random darken partial image * random move image  Here is an example of a traffic sign image before and after augmentation.  The image is stretched horizontally and partially darkened at the bottom.  ![alt text][image4] ![alt text][image5]  When all training images are added up  the quantity shows:  ![alt text][image6]  As a last step  the training set is shuffled to remove any order.    My final model results were: * training set accuracy of 99%+ * validation set accuracy of 99%+ * test set accuracy of 98%+  If a well known architecture was chosen: * Inception v4 was chosen for traffic sign classifier. * This is a very suitable architecture because it has a very high accuracy for classifier (general inception v4 can be used to classify 1000 classes)  and it is quite efficient. * It can be concluded this model works very well because all 3 data sets have very high accuracies  which means the model is not under or over fitting (balanced variance and bias).  Additional visualization of the validation accuracy is analyzed to understand what works or not.  Below is the validation set recall and precision by class.  Note that class 16 has a low recall (false negative)  meaning images from class 16 were predicted as some other clases.  In precision chart  class 41 has a low value (false positive).  It is likely that many class 16 images were misclassified as class 41.  ![alt text][image9] ![alt text][image10]  Images were pulled from classes 16 and 41 and quickly one can see that some class 16 images have red circular borders are quite faded so they could be similar to class 41 images.  Below are classes 16 (left) and 41 (right) sample images.  ![alt text][image11]   Here are some of the visualized feature maps evaluated on the first new image (children crossing).  It seems some feature maps picked up the shape of the triangle.  Some feature maps picked up the shape of the human figures inside the triangle.  Some feature maps picked up the blue sky on the left.  ![alt text][image18] ![alt text][image19] ![alt text][image20]  """;Computer Vision;https://github.com/waynecoffee9/Traffic-Sign-Classifier
"""I used the pandas library to calculate summary statistics of the traffic signs data set:  * Number of training examples = 34799 * Number of validation examples = 4410 * Number of testing examples = 12630 * Image data shape = (32  32  3) * Number of classes = 43   Here are the results of the prediction:  | Image			        |     Prediction	        					|  |:---------------------:|:---------------------------------------------:|  | Children crossing     | Children crossing  							|  | Right-of-way          | Right-of-way									| | Priority road			| Priority road									| | Turn right ahead 		| Turn right ahead				 				| | Road work 			| Road work         							|   The model was able to correctly guess 5 of the 5 traffic signs  which gives an accuracy of 100%  which is close to 98% from the test set.   My final model results were: * training set accuracy of 99%+ * validation set accuracy of 99%+ * test set accuracy of 98%+  If a well known architecture was chosen: * Inception v4 was chosen for traffic sign classifier. * This is a very suitable architecture because it has a very high accuracy for classifier (general inception v4 can be used to classify 1000 classes)  and it is quite efficient. * It can be concluded this model works very well because all 3 data sets have very high accuracies  which means the model is not under or over fitting (balanced variance and bias).  Additional visualization of the validation accuracy is analyzed to understand what works or not.  Below is the validation set recall and precision by class.  Note that class 16 has a low recall (false negative)  meaning images from class 16 were predicted as some other clases.  In precision chart  class 41 has a low value (false positive).  It is likely that many class 16 images were misclassified as class 41.  ![alt text][image9] ![alt text][image10]  Images were pulled from classes 16 and 41 and quickly one can see that some class 16 images have red circular borders are quite faded so they could be similar to class 41 images.  Below are classes 16 (left) and 41 (right) sample images.  ![alt text][image11]   First  I normalized all training images to have float32 from 0 to 1.  I found the accuracy increases faster than -1 to 1 during network training. The matricies below simply show one random image before and after normalization.  Before normalization:  [[51 36 45 ... 80 79 73]   [47 34 38 ... 64 75 79]    [45 32 38 ... 61 68 71]    ...    [43 38 34 ... 46 42 37]    [44 36 31 ... 36 33 35]    [41 36 38 ... 52 48 50]]   After normalization:  [[0.1849315  0.08219178 0.14383562 ... 0.38356164 0.37671232 0.33561644]   [0.15753424 0.06849315 0.09589041 ... 0.2739726  0.34931508 0.37671232]    [0.14383562 0.05479452 0.09589041 ... 0.25342464 0.30136988 0.3219178 ]    ...    [0.13013698 0.09589041 0.06849315 ... 0.15068494 0.12328767 0.0890411 ]    [0.1369863  0.08219178 0.04794521 ... 0.08219178 0.06164384 0.07534247]    [0.11643836 0.08219178 0.09589041 ... 0.19178082 0.16438356 0.1780822 ]]     As mentioned before  data augmentation is applied to even out image quantity difference among classes  and to include variations of same images.  * sharpen or smoothing * random rotate image * random stretch/squeeze image  * random darken partial image * random move image  Here is an example of a traffic sign image before and after augmentation.  The image is stretched horizontally and partially darkened at the bottom.  ![alt text][image4] ![alt text][image5]  When all training images are added up  the quantity shows:  ![alt text][image6]  As a last step  the training set is shuffled to remove any order.    I used the pandas library to calculate summary statistics of the traffic signs data set:  * Number of training examples = 34799 * Number of validation examples = 4410 * Number of testing examples = 12630 * Image data shape = (32  32  3) * Number of classes = 43   You're reading it! and here is a link to my [project code](https://github.com/waynecoffee9/Traffic-Sign-Classifier/blob/master/CarND-Traffic-Sign-Classifier-Project/Traffic_Sign_Classifier-inception.ipynb) If you are unable to view it under github  use https://nbviewer.jupyter.org/ and paste the link to view.   You're reading it! and here is a link to my [project code](https://github.com/waynecoffee9/Traffic-Sign-Classifier/blob/master/CarND-Traffic-Sign-Classifier-Project/Traffic_Sign_Classifier-inception.ipynb) If you are unable to view it under github  use https://nbviewer.jupyter.org/ and paste the link to view.   First  I normalized all training images to have float32 from 0 to 1.  I found the accuracy increases faster than -1 to 1 during network training. The matricies below simply show one random image before and after normalization.  Before normalization:  [[51 36 45 ... 80 79 73]   [47 34 38 ... 64 75 79]    [45 32 38 ... 61 68 71]    ...    [43 38 34 ... 46 42 37]    [44 36 31 ... 36 33 35]    [41 36 38 ... 52 48 50]]   After normalization:  [[0.1849315  0.08219178 0.14383562 ... 0.38356164 0.37671232 0.33561644]   [0.15753424 0.06849315 0.09589041 ... 0.2739726  0.34931508 0.37671232]    [0.14383562 0.05479452 0.09589041 ... 0.25342464 0.30136988 0.3219178 ]    ...    [0.13013698 0.09589041 0.06849315 ... 0.15068494 0.12328767 0.0890411 ]    [0.1369863  0.08219178 0.04794521 ... 0.08219178 0.06164384 0.07534247]    [0.11643836 0.08219178 0.09589041 ... 0.19178082 0.16438356 0.1780822 ]]     As mentioned before  data augmentation is applied to even out image quantity difference among classes  and to include variations of same images.  * sharpen or smoothing * random rotate image * random stretch/squeeze image  * random darken partial image * random move image  Here is an example of a traffic sign image before and after augmentation.  The image is stretched horizontally and partially darkened at the bottom.  ![alt text][image4] ![alt text][image5]  When all training images are added up  the quantity shows:  ![alt text][image6]  As a last step  the training set is shuffled to remove any order.    My final model results were: * training set accuracy of 99%+ * validation set accuracy of 99%+ * test set accuracy of 98%+  If a well known architecture was chosen: * Inception v4 was chosen for traffic sign classifier. * This is a very suitable architecture because it has a very high accuracy for classifier (general inception v4 can be used to classify 1000 classes)  and it is quite efficient. * It can be concluded this model works very well because all 3 data sets have very high accuracies  which means the model is not under or over fitting (balanced variance and bias).  Additional visualization of the validation accuracy is analyzed to understand what works or not.  Below is the validation set recall and precision by class.  Note that class 16 has a low recall (false negative)  meaning images from class 16 were predicted as some other clases.  In precision chart  class 41 has a low value (false positive).  It is likely that many class 16 images were misclassified as class 41.  ![alt text][image9] ![alt text][image10]  Images were pulled from classes 16 and 41 and quickly one can see that some class 16 images have red circular borders are quite faded so they could be similar to class 41 images.  Below are classes 16 (left) and 41 (right) sample images.  ![alt text][image11]   Here are some of the visualized feature maps evaluated on the first new image (children crossing).  It seems some feature maps picked up the shape of the triangle.  Some feature maps picked up the shape of the human figures inside the triangle.  Some feature maps picked up the blue sky on the left.  ![alt text][image18] ![alt text][image19] ![alt text][image20]  """;General;https://github.com/waynecoffee9/Traffic-Sign-Classifier
"""""";Computer Vision;https://github.com/tsubasawb/DeepLearning_Paper
"""The Progressive GAN code repository contains a command-line tool for recreating bit-exact replicas of the datasets that we used in the paper. The tool also provides various utilities for operating on the datasets:  ``` usage: dataset_tool.py [-h] <command> ...      display             Display images in dataset.     extract             Extract images from dataset.     compare             Compare two datasets.     create_mnist        Create dataset for MNIST.     create_mnistrgb     Create dataset for MNIST-RGB.     create_cifar10      Create dataset for CIFAR-10.     create_cifar100     Create dataset for CIFAR-100.     create_svhn         Create dataset for SVHN.     create_lsun         Create dataset for single LSUN category.     create_celeba       Create dataset for CelebA.     create_celebahq     Create dataset for CelebA-HQ.     create_from_images  Create dataset from a directory full of images.     create_from_hdf5    Create dataset from legacy HDF5 archive.  Type ""dataset_tool.py <command> -h"" for more information. ```  The datasets are represented by directories containing the same image data in several resolutions to enable efficient streaming. There is a separate `*.tfrecords` file for each resolution  and if the dataset contains labels  they are stored in a separate file as well:  ``` > python dataset_tool.py create_cifar10 datasets/cifar10 ~/downloads/cifar10 > ls -la datasets/cifar10 drwxr-xr-x  2 user user         7 Feb 21 10:07 . drwxrwxr-x 10 user user        62 Apr  3 15:10 .. -rw-r--r--  1 user user   4900000 Feb 19 13:17 cifar10-r02.tfrecords -rw-r--r--  1 user user  12350000 Feb 19 13:17 cifar10-r03.tfrecords -rw-r--r--  1 user user  41150000 Feb 19 13:17 cifar10-r04.tfrecords -rw-r--r--  1 user user 156350000 Feb 19 13:17 cifar10-r05.tfrecords -rw-r--r--  1 user user   2000080 Feb 19 13:17 cifar10-rxx.labels ```  The ```create_*``` commands take the standard version of a given dataset as input and produce the corresponding `*.tfrecords` files as output. Additionally  the ```create_celebahq``` command requires a set of data files representing deltas with respect to the original CelebA dataset. These deltas (27.6GB) can be downloaded from [`datasets/celeba-hq-deltas`](https://drive.google.com/open?id=0B4qLcYyJmiz0TXY1NG02bzZVRGs).  **Note about module versions**: Some of the dataset commands require specific versions of Python modules and system libraries (e.g. pillow  libjpeg)  and they will give an error if the versions do not match. Please heed the error messages ‚Äì there is **no way** to get the commands to work other than installing these specific versions.   | Feature                           | TensorFlow version                            | Original Theano version   |   Pull the Progressive GAN code repository and add it to your PYTHONPATH environment variable.  Install the required Python packages with pip install -r requirements-pip.txt   Download karras2018iclr-celebahq-1024x1024.pkl from networks/tensorflow-version and place it in the same directory as the script.   """;Computer Vision;https://github.com/jiajunhua/tkarras-progressive_growing_of_gans
"""""";General;https://github.com/tsubasawb/DeepLearning_Paper
"""Recent Ubuntu releases come with python3 installed. I use pip3 for installing dependencies so install that with `sudo apt install python3-pip`. Install git if you don't already have it with `sudo apt install git`.  Then clone this repo with `git clone https://github.com/harvitronix/reinforcement-learning-car.git`. It has some pretty big weights files saved in past commits  so to just get the latest the fastest  do `git clone https://github.com/harvitronix/reinforcement-learning-car.git --depth 1`.   This is the physics engine used by the simulation. It just went through a pretty significant rewrite (v5) so you need to grab the older v4 version. v4 is written for Python 2 so there are a couple extra steps.  Go back to your home or downloads and get Pymunk 4:  `wget https://github.com/viblo/pymunk/archive/pymunk-4.0.0.tar.gz`  Unpack it:  `tar zxvf pymunk-4.0.0.tar.gz`  Update from Python 2 to 3:  `cd pymunk-pymukn-4.0.0/pymunk`  `2to3 -w *.py`  Install it:  `cd ..` `python3 setup.py install`  Now go back to where you cloned `reinforcement-learning-car` and make sure everything worked with a quick `python3 learning.py`. If you see a screen come up with a little dot flying around the screen  you're ready to go!   Install Pygame's dependencies with:  `sudo apt install mercurial libfreetype6-dev libsdl-dev libsdl-image1.2-dev libsdl-ttf2.0-dev libsmpeg-dev libportmidi-dev libavformat-dev libsdl-mixer1.2-dev libswscale-dev libjpeg-dev`  Then install Pygame itself:  `pip3 install hg+http://bitbucket.org/pygame/pygame`   These instructions are for a fresh Ubuntu 16.04 box. Most of the same should apply to OS X. If you have issues installing  feel free to open an issue with your error and I'll do my best to help.   NOTE: If you're coming here from parts 1 or 2 of the Medium posts  you want to visit the releases section and check out version 1.0.0  as the code has evolved passed that.   Full writeups that pertain to version 1.0.0 can be found here:   """;Reinforcement Learning;https://github.com/vsquareg/RL_ERA
"""""";Computer Vision;https://github.com/vkumaresan/Rotational-CoordConv
"""We implement v3 version (which is the latest version on June  2019.).   All the experiments are done on NVIDIA TESLA T4.   Note: You know the GPU/TPU won't get exactly the same results even we use fixed random seed.   The implementation is based on BERT [repository](https://github.com/google-research/bert)  which uses `AdamWeightDecayOptimizer` (appears in [`optimization.py`](https://github.com/google-research/bert/blob/master/optimization.py)) for pre-training and fine-tuning.  - Just use `LAMBOptimizer` as a regular optimizer in TensorFlow  similar to `Adam` or `AdamWeightDecayOptimizer`. - Find LAMB optimizer in `optimization.py`. - There is nothing special to tune other than initial `learning_rate`.    """;General;https://github.com/liuqiangict/lamb_optimizer
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla T4'  total_memory=15079MB)   Using CUDA device0 _CudaDeviceProperties(name='Tesla T4'  total_memory=15079MB)   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/jianmingwuhasco/yolov3
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/mqjinwon/tracking_darknet
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/mqjinwon/tracking_darknet
"""-------------------------------------------------------------- In order to design a fully autonomous Vehicle the following techniques have been used:          1. Waypoint Following techniques     2. Control     3. Traffic Light Detection and Classification      The Waypoint Following technique would take information from the traffic light detection and classification with the current waypoints in order to update the target velocities for each waypoint based on this information.  For Control part  I have designed a drive-by-wire (dbw) node that could take the target linear and angular velocities and publish commands for the throttle  brake  and steering of the car.   For Traffic Light Detection and classification  I have designed a classification node that would take the current waypoints of the car and an image taken from the car and determine if the closest traffic light was red or green.   ![alt text][image1]   -------------------------------------------------------------- This is the project repo for the final project of the Udacity Self-Driving Car Nanodegree-Capstone Project: Programming a Real Self-Driving Car.    [Install Docker](https://docs.docker.com/engine/installation/)  Build the docker container ```bash docker build . -t capstone ```  Run the docker file ```bash docker run -p 4567:4567 -v $PWD:/capstone -v /tmp/log:/root/.ros/ --rm -it capstone ```   * Be sure that your workstation is running Ubuntu 16.04 Xenial Xerus or Ubuntu 14.04 Trusty Tahir. [Ubuntu downloads can be found here](https://www.ubuntu.com/download/desktop). * If using a Virtual Machine to install Ubuntu  use the following configuration as minimum:   * 2 CPU   * 2 GB system memory   * 25 GB of free hard drive space    The Udacity provided virtual machine has ROS and Dataspeed DBW already installed  so you can skip the next two steps if you are using this.  * Follow these instructions to install ROS   * [ROS Kinetic](http://wiki.ros.org/kinetic/Installation/Ubuntu) if you have Ubuntu 16.04.   * [ROS Indigo](http://wiki.ros.org/indigo/Installation/Ubuntu) if you have Ubuntu 14.04. * [Dataspeed DBW](https://bitbucket.org/DataspeedInc/dbw_mkz_ros)   * Use this option to install the SDK on a workstation that already has ROS installed: [One Line SDK Install (binary)](https://bitbucket.org/DataspeedInc/dbw_mkz_ros/src/81e63fcc335d7b64139d7482017d6a97b405e250/ROS_SETUP.md?fileviewer=file-view-default) * Download the [Udacity Simulator](https://github.com/udacity/CarND-Capstone/releases).   Please use **one** of the two installation options  either native **or** docker installation.   cd CarND-Capstone/ros   | OpenCV | 3.2.0-dev | 2.4.8 |   1. Clone the project repository ```bash git clone https://github.com/udacity/CarND-Capstone.git ```  2. Install python dependencies ```bash cd CarND-Capstone pip install -r requirements.txt ``` 3. Make and run styx ```bash cd ros catkin_make source devel/setup.sh roslaunch launch/styx.launch ``` 4. Run the simulator   """;General;https://github.com/KarimDahawy/CapStone-Project
"""-------------------------------------------------------------- In order to design a fully autonomous Vehicle the following techniques have been used:          1. Waypoint Following techniques     2. Control     3. Traffic Light Detection and Classification      The Waypoint Following technique would take information from the traffic light detection and classification with the current waypoints in order to update the target velocities for each waypoint based on this information.  For Control part  I have designed a drive-by-wire (dbw) node that could take the target linear and angular velocities and publish commands for the throttle  brake  and steering of the car.   For Traffic Light Detection and classification  I have designed a classification node that would take the current waypoints of the car and an image taken from the car and determine if the closest traffic light was red or green.   ![alt text][image1]   -------------------------------------------------------------- This is the project repo for the final project of the Udacity Self-Driving Car Nanodegree-Capstone Project: Programming a Real Self-Driving Car.    [Install Docker](https://docs.docker.com/engine/installation/)  Build the docker container ```bash docker build . -t capstone ```  Run the docker file ```bash docker run -p 4567:4567 -v $PWD:/capstone -v /tmp/log:/root/.ros/ --rm -it capstone ```   * Be sure that your workstation is running Ubuntu 16.04 Xenial Xerus or Ubuntu 14.04 Trusty Tahir. [Ubuntu downloads can be found here](https://www.ubuntu.com/download/desktop). * If using a Virtual Machine to install Ubuntu  use the following configuration as minimum:   * 2 CPU   * 2 GB system memory   * 25 GB of free hard drive space    The Udacity provided virtual machine has ROS and Dataspeed DBW already installed  so you can skip the next two steps if you are using this.  * Follow these instructions to install ROS   * [ROS Kinetic](http://wiki.ros.org/kinetic/Installation/Ubuntu) if you have Ubuntu 16.04.   * [ROS Indigo](http://wiki.ros.org/indigo/Installation/Ubuntu) if you have Ubuntu 14.04. * [Dataspeed DBW](https://bitbucket.org/DataspeedInc/dbw_mkz_ros)   * Use this option to install the SDK on a workstation that already has ROS installed: [One Line SDK Install (binary)](https://bitbucket.org/DataspeedInc/dbw_mkz_ros/src/81e63fcc335d7b64139d7482017d6a97b405e250/ROS_SETUP.md?fileviewer=file-view-default) * Download the [Udacity Simulator](https://github.com/udacity/CarND-Capstone/releases).   Please use **one** of the two installation options  either native **or** docker installation.   cd CarND-Capstone/ros   | OpenCV | 3.2.0-dev | 2.4.8 |   1. Clone the project repository ```bash git clone https://github.com/udacity/CarND-Capstone.git ```  2. Install python dependencies ```bash cd CarND-Capstone pip install -r requirements.txt ``` 3. Make and run styx ```bash cd ros catkin_make source devel/setup.sh roslaunch launch/styx.launch ``` 4. Run the simulator   """;Computer Vision;https://github.com/KarimDahawy/CapStone-Project
"""Few months back when trying to search for Subtitles for a French Movie  I got an idea to build a mini-version of Neural Machine Translation system for French - English and see how it feels to build one. Courses CS 224n : Natural Language Processing with Deep learning and Sequence models from Coursera helped a lot in understanding Sequence models  although there is a long way to go!   Knowing that my Laptop doesn't have great configuration to train deep neural networks  I planned my experimentation on GCP. FYI  for a first time user free credits worth 300$ will be given. Lot of articles are online which shows step-by-step procedure for setting-up a GCP instance powered with GPU. The article in the [link](https://medium.com/google-cloud/using-a-gpu-tensorflow-on-google-cloud-platform-1a2458f42b0) explains the steps in very sane manner.    **example 1** : <br /> **French input** : comit√© pr√©paratoire de la conf√©rence des nations unies sur le commerce illicite des armes l√©g√®res sous tous ses aspects <br /> **Actual English Translation** : preparatory committee for the united nations conference on the illicit trade in small arms and light weapons in all its aspects <br /> **Model's English Translation** : preparatory committee for the united nations conference on the illicit trade in small arms and light weapons in all its aspects <br />   **example 2** : <br /> **French input** : il est grand temps que la communaut√© internationale applique cette r√©solution <br /> **Actual English Translation** : it was high time that the international community implemented that resolution <br /> **Model's English Translation** : it is high time that the international community should be adopted by the resolution <br />  **example 3** : <br /> **French input** : conclusions concert√©es sur l'√©limination de toutes les formes de discrimination et de violence √† l'√©gard des petites filles <br /> **Actual English Translation** : agreed conclusions on the elimination of all forms of discrimination and violence against the girl child <br /> **Model's English Translation** : conclusions conclusions on the elimination of all forms of discrimination and violence against the young people. <br />   """;General;https://github.com/HemaDevaSagar35/NeuralMachineTranslation-French2English
"""First step is having a dataset Dataset that can directly work - 	* Mnist 	* Celeba 	* Cifar10 	* stl10 	* LSun(Bedroom)  After downloading the dataset  You need to put dataset in dataset folder and set the dataset name in main.py   You can change epoch  batch size  print frequency(for image generation)  image size and learning rate in mani.py as well  after that you go and run main.py  ``` python3 main.py ```  """;Computer Vision;https://github.com/gagan16/DcGan-Tensorflow
"""""";Graphs;https://github.com/tkhkaeio/GNN_chemoinfo
"""- Prepare data lsit just like the way of original darkenet - No need to generate lmdbs as we directly use raw images in the data list to train    cd Root_Repo   cd Root_Repo   ``` git clone --recursive https://github.com/leon-liangwu/py-caffe-yolo.git pip install -r requirements.txt ```   """;Computer Vision;https://github.com/leon-liangwu/py-caffe-yolo
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/Talgin/facerec
"""""";General;https://github.com/scakc/QAwiki
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/abcxs/maskrcnn-contest
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;General;https://github.com/abcxs/maskrcnn-contest
"""- To install  `cd` into the root directory and type `pip install -e .`  - To interactively view moving to landmark scenario (see others in ./scenarios/): `bin/interactive.py --scenario simple.py`  - Known dependencies: Python (3.5.4)  OpenAI gym (0.10.5)  numpy (1.14.5)  - To use the environments  look at the code for importing them in `make_env.py`.   """;Reinforcement Learning;https://github.com/SintolRTOS/multi-agent_Example
"""* Install tesnorflow (skip this step if it's already installed test environment:tensorflow 2.4.0) * Install dependencies:  `pip install -r requirements.txt`   git clone https://github.com/wangermeng2021/FastClassification.git    cd FastClassification   """;General;https://github.com/wangermeng2021/FastClassification
"""Make sure these packages are installed: - PyTorch 0.4 - Torchvision  - NumPy  Packages can be installed via PyPi package repository or Anaconda.   Notice that you will need to download the dataset mentioned before to proceed for training part. You will also need to train network to use it.  Please follow the Jupyter Notebook file for more information.  Important notice: This GitHub repository includes files for education purposes only. These files should not use for a commercial usage.  """;Computer Vision;https://github.com/iamkucuk/DCGAN-Face-Generation
"""We implement v3 version (which is the latest version on June  2019.).   All the experiments are done on NVIDIA TESLA T4.   Note: You know the GPU/TPU won't get exactly the same results even we use fixed random seed.   The implementation is based on BERT [repository](https://github.com/google-research/bert)  which uses `AdamWeightDecayOptimizer` (appears in [`optimization.py`](https://github.com/google-research/bert/blob/master/optimization.py)) for pre-training and fine-tuning.  - Just use `LAMBOptimizer` as a regular optimizer in TensorFlow  similar to `Adam` or `AdamWeightDecayOptimizer`. - Find LAMB optimizer in `optimization.py`. - There is nothing special to tune other than initial `learning_rate`.    """;Natural Language Processing;https://github.com/liuqiangict/lamb_optimizer
"""python code from https://github.com/keepgallop/RIP/blob/master/RIP.py   """;Computer Vision;https://github.com/filipmu/Kaggle-APTOS-2019-Blindness
"""Requirement: Tensorflow-GPU 1.3.0  python: 3.6.0             """;Computer Vision;https://github.com/azy64/Deep-Learning
"""    name: A name for this operation (optional).     with ops.name_scope(name  ""dropout""  [x]) as name:      x = ops.convert_to_tensor(x  name=""x"")   """;General;https://github.com/lkeonwoo94/DL_cv-mdt_NVIDIA_Cert_Course_StudyPI
"""    name: A name for this operation (optional).     with ops.name_scope(name  ""dropout""  [x]) as name:      x = ops.convert_to_tensor(x  name=""x"")   """;Computer Vision;https://github.com/lkeonwoo94/DL_cv-mdt_NVIDIA_Cert_Course_StudyPI
"""__Info:__ As of 2020 the following installation process does not support GPU execution of the code anymore. Currently  there are no plans for updates to TensorFlow 2 or other versions.   Create virtual environment and activate: + (```conda config --append channels conda-forge```) + ```conda create -n <env> python=3.6``` + ```conda activate <env>```  Manual package installation: + ```conda install -c conda-forge tensorflow=1.8.0``` + ```conda install -c conda-forge keras=2.1.6``` + ```conda install -c conda-forge scikit-image```    Automatic package installation via ```requirements.txt```: + ```conda install --file requirements.txt```  Get the Sintel dataset for training or matching evaluation: + Download the training sequences from [[3]](#3) and extract them to the `/MPI-Sintel/training` folder.    This implementation provides three different modes of operation: + `dm_match_pair` computes matches for a pair of images based on given weights (`weights.h5py`). These matches are then visualized as optical flow via `flow_vis` [[4]](#4). + `dm_match` computes matches based on given weights for a set of testing images and evaluates the matching results by comparing with the ground truth. + `dm_train` trains the neural network.   The sequences on which training and the matching evaluation are performed are loaded from `MPI-Sintel/training/clean`. The path can be customized in the function `read_training_data()`  in `dm.py`. Weights are saved to and loaded from `weights.h5py`.   """;Computer Vision;https://github.com/vwegn/dm
"""  1. Make sure you have [Anaconda or Miniconda](https://conda.io/docs/download.html)   installed.   2. Clone repo with `git clone https://github.com/chrischute/real-nvp.git rnvp`.   3. Go into the cloned repo: `cd rnvp`.   4. Create the environment: `conda env create -f environment.yml`.   5. Activate the environment: `source activate rnvp`.   Make sure you've created and activated the conda environment as described above.     python train.py for the default configuration  or run   """;Computer Vision;https://github.com/P4ppenheimer/NormalizingFlows_rnvp
"""This allows you to greatly simplify the model  since it does not have to deal with the manual placement of tensors.  Instead  you just specify which GPU you'd like to use in the beginning of your script.   : Horovod: pin GPU to be used to process local rank (one GPU per process)   Congratulations!  If you made it this far  your fashion_mnist.py should now be fully distributed.  To verify  you can run the following command in the terminal  which should produce no output:   In this tutorial  you will learn how to apply Horovod to a [WideResNet](https://arxiv.org/abs/1605.07146) model  trained on the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset.   In `fashion_mnist.py`  we're using the filename of the last checkpoint to determine the epoch to resume training from in case of a failure:  ![image](https://user-images.githubusercontent.com/16640218/54185268-d35d3f00-4465-11e9-99eb-96d4b99f1d38.png)  As you scale your workload to multi-node  some of your workers may not have access to the filesystem containing the checkpoint.  For that reason  we make the first worker to determine the epoch to restart from  and *broadcast* that information to the rest of the workers.  To broadcast the starting epoch from the first worker  add the following code:  ```python #: Horovod: broadcast resume_from_epoch from rank 0 (which will have #: checkpoints) to other ranks. resume_from_epoch = hvd.broadcast(resume_from_epoch  0  name='resume_from_epoch') ```  ![image](https://user-images.githubusercontent.com/16640218/53534072-2de3bc00-3ab2-11e9-8cf1-7531542e3202.png) (see line 52-54)   """;Computer Vision;https://github.com/darkreapyre/HaaS-GitOps
"""""";Computer Vision;https://github.com/Nandu960/Road-Asset-Detection
"""This is a customized version of the repository https://github.com/jadore801120/attention-is-all-you-need-pytorch   """;General;https://github.com/text-machine-lab/transformerpy
"""This is a customized version of the repository https://github.com/jadore801120/attention-is-all-you-need-pytorch   """;Natural Language Processing;https://github.com/text-machine-lab/transformerpy
"""- TODO: 	- Load vocabulary. 	- Perform decoding after the translation. ---  """;General;https://github.com/MinghaoYan/COMP480FinalProject
"""- TODO: 	- Load vocabulary. 	- Perform decoding after the translation. ---  """;Natural Language Processing;https://github.com/MinghaoYan/COMP480FinalProject
"""Update to the latest version according to caffe version  with 5% mAP increase.   will open a window that will display the camera output together with the detections. You can play  with the detection threshold to get more or less detections.   Download the PASCAL VOC dataset  skip this step if you already have one.   cd /path/to/where_you_store_datasets/   : cd /path/to/incubator-mxnet/example/ssd  bash tools/prepare_pascal.sh  : or if you are using windows   : cd /path/to/incubator-mxnet/example/ssd  bash tools/prepare_pascal.sh  : or if you are using windows   : cd /path/to/incubator-mxnet/example/ssd   Download the COCO2014 dataset  skip this step if you already have one.   cd /path/to/where_you_store_datasets/   ln -s /path/to/COCO2014 /path/to/incubator-mxnet/example/ssd/data/coco   : cd /path/to/incubator-mxnet/example/ssd  bash tools/prepare_coco.sh  : or if you are using windows   : cd /path/to/incubator-mxnet/example/ssd   : cd /path/to/incubator-mxnet/example/ssd   : cd /path/to/incubator-mxnet/example/ssd   Converter from caffe is available at /path/to/incubator-mxnet/example/ssd/tools/caffe_converter   cd /path/to/incubator-mxnet/example/ssd/tools/caffe_converter   ![demo1](https://cloud.githubusercontent.com/assets/3307514/19171057/8e1a0cc4-8be0-11e6-9d8f-088c25353b40.png) ![demo2](https://cloud.githubusercontent.com/assets/3307514/19171063/91ec2792-8be0-11e6-983c-773bd6868fa8.png) ![demo3](https://cloud.githubusercontent.com/assets/3307514/19171086/a9346842-8be0-11e6-8011-c17716b22ad3.png)   * You will need python modules: `cv2`  `matplotlib` and `numpy`. If you use mxnet-python api  you probably have already got them. You can install them via pip or package managers  such as `apt-get`: ``` sudo apt-get install python-opencv python-matplotlib python-numpy ```  * Build MXNet: Follow the official instructions ``` #: for Ubuntu/Debian cp make/config.mk ./config.mk #: enable cuda  cudnn if applicable ``` Remember to enable CUDA if you want to be able to train  since CPU training is insanely slow. Using CUDNN is optional  but highly recommended.   * Download the pretrained model: [`ssd_resnet50_0712.zip`](https://github.com/zhreshold/mxnet-ssd/releases/download/v0.6/resnet50_ssd_512_voc0712_trainval.zip)  and extract to `model/` directory.  * Run ``` #: cd /path/to/incubator-mxnet/example/ssd #: download the test images python data/demo/download_demo_images.py #: run the demo python demo.py --gpu 0 #: play with examples: python demo.py --epoch 0 --images ./data/demo/dog.jpg --thresh 0.5 python demo.py --cpu --network resnet50 --data-shape 512 #: wait for library to load for the first time ``` * Check `python demo.py --help` for more options.   """;Computer Vision;https://github.com/hieubkset/ssd_ship_detection
"""See [installation instructions](https://detectron2.readthedocs.io/tutorials/install.html).   See [Getting Started with Detectron2](https://detectron2.readthedocs.io/tutorials/getting_started.html)  and the [Colab Notebook](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5) to learn about basic usage.  Learn more at our [documentation](https://detectron2.readthedocs.org). And see [projects/](projects/) for some projects that are built on top of detectron2.   """;General;https://github.com/facebookresearch/detectron2
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/Smiler-Jin/Yolov4
"""""";General;https://github.com/psh150204/MAML
"""In this work  we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN including only the most essential component in GCN‚Äîneighborhood aggregation‚Äîfor collaborative filtering     pytorch version results (stop at 1000 epochs):   run LightGCN on **Gowalla** dataset:  * command  ` cd code && python main.py --decay=1e-4 --lr=0.001 --layer=3 --seed=2020 --dataset=""gowalla"" --topks=[20] --recdim=64`  * log output  ```shell ... ====================== EPOCH[5/1000] BPR[sample time][16.2=15.84+0.42] [saved][[BPR[aver loss1.128e-01]] [0;30;43m[TEST][0m {'precision': array([0.03315359])  'recall': array([0.10711388])  'ndcg': array([0.08940792])} [TOTAL TIME] 35.9975962638855 ... ====================== EPOCH[116/1000] BPR[sample time][16.9=16.60+0.45] [saved][[BPR[aver loss2.056e-02]] [TOTAL TIME] 30.99874997138977 ... ```  *NOTE*:  1. Even though we offer the code to split user-item matrix for matrix multiplication  we strongly suggest you don't enable it since it will extremely slow down the training speed. 2. If you feel the test process is slow  try to increase the ` testbatch` and enable `multicore`(Windows system may encounter problems with `multicore` option enabled) 3. Use `tensorboard` option  it's good. 4. Since we fix the seed(`--seed=2020` ) of `numpy` and `torch` in the beginning  if you run the command as we do above  you should have the exact output log despite the running time (check your output of *epoch 5* and *epoch 116*).    """;Graphs;https://github.com/tanya525625/LightGCN-PyTorch
"""GPUs: K80 ($0.14/hr)  T4 ($0.11/hr)  V100 ($0.74/hr) CUDA with Nvidia Apex FP16/32     Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [Notebook](https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb) <a href=""https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a> * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) << highly recommended * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart)  ![Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/yolov3?logo=docker) * [A TensorRT Implementation of YOLOv3 and YOLOv4](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp)     """;Computer Vision;https://github.com/mayanks888/yolo_custom
"""The network is trained on an NVIDIA P100 TENSOR CORE GPU provided by Kaggle in the GPU accelerator version of Kaggle Kernels  however the weight tensors are converted from cuda tensors to CPU tenesors to allow for inferring on machines without a GPU.   Follow same steps to run the web app on a cloud vm. - `git clone` this repository. - [optional but recommended] Set up a virtual environement. - Run `pip install -r requiments.txt` to install all* the python libraries.         - *`opencv-python` needs to be installed using [these](https://docs.opencv.org/master/d2/de6/tutorial_py_setup_in_ubuntu.html) steps. - Download the weights of the Neural Network from [here](https://drive.google.com/file/d/1M6w0WOjm1nLkDxl43_iwvIM-lVtfJPD_/view?usp=sharing).  - Run `streamlit run app.py <path/of/the/weights_file(.ckpt)>` in the system CLI. - In a web browser of choice  open `localhost:8501`.     """;Computer Vision;https://github.com/thepooons/melanoma-comp-2020
"""""";Computer Vision;https://github.com/carinanorre/Brain-Tumour-Segmentation-Dissertation
"""``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result |____BERT_NER_pb.py   		    #: run model and eval result and transfer ckpt to saved model (pb) |____ner_local_pb.py         #:load pb and predict  ```    ``` bash run_ner.sh ```   """;Natural Language Processing;https://github.com/broccolik/BERT-NER
"""[Fontawesome](https://www.Fontawesome.com)   By typing python3.7 and hiting Enter  python shell will appear (version 3.7).  Leave the python shell and run the commands below:  $ git clone https://github.com/amirdy/dog-breed-classification.git    $ cd dog-breed-classification/Web_App    $ source ./env/bin/activate   (env)$ pip install -r requirements.txt   (env)$ pip install torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html   (env)$ pip install efficientnet_pytorch==0.6.3   - Python - Pytorch ( Using Google Colab Pro )   """;Computer Vision;https://github.com/amirdy/dog-breed-classification
"""**The Egohands Dataset**  The hand detector model is built using data from the [Egohands Dataset](http://vision.soic.indiana.edu/projects/egohands/) dataset. This dataset works well for several reasons. It contains high quality  pixel level annotations (>15000 ground truth labels) where hands are located across 4800 images. All images are captured from an egocentric view (Google glass) across 48 different environments (indoor  outdoor) and activities (playing cards  chess  jenga  solving puzzles etc).  <img src=""images/egohandstrain.jpg"" width=""100%"">  If you will be using the Egohands dataset  you can cite them as follows:  > Bambach  Sven  et al. ""Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions."" Proceedings of the IEEE International Conference on Computer Vision. 2015.  The Egohands dataset (zip file with labelled data) contains 48 folders of locations where video data was collected (100 images per folder). ``` -- LOCATION_X   -- frame_1.jpg   -- frame_2.jpg   ...   -- frame_100.jpg   -- polygons.mat  // contains annotations for all 100 images in current folder -- LOCATION_Y   -- frame_1.jpg   -- frame_2.jpg   ...   -- frame_100.jpg   -- polygons.mat  // contains annotations for all 100 images in current folder   ```  **Converting data to Tensorflow Format**  Some initial work needs to be done to the Egohands dataset to transform it into the format (`tfrecord`) which Tensorflow needs to train a model. This repo contains `egohands_dataset_clean.py` a script that will help you generate these csv files.  - Downloads the egohands datasets - Renames all files to include their directory names to ensure each filename is unique - Splits the dataset into train (80%)  test (10%) and eval (10%) folders. - Reads in `polygons.mat` for each folder  generates bounding boxes and visualizes them to ensure correctness (see image above). - Once the script is done running  you should have an images folder containing three folders - train  test and eval. Each of these folders should also contain a csv label document each - `train_labels.csv`  `test_labels.csv`  that can be used to generate `tfrecords`  > `python egohands_dataset_clean.py`  Note: While the egohands dataset provides four separate labels for hands (own left  own right  other left  and other right)  for my purpose  I am only interested in the general `hand` class and label all training data as `hand`. You can modify the data prep script to generate `tfrecords` that support 4 labels.  Next: convert your dataset + csv files to tfrecords. A helpful guide on this can be found [here](https://pythonprogramming.net/creating-tfrecord-files-tensorflow-object-detection-api-tutorial/).For each folder  you should be able to generate  `train.record`  `test.record` required in the training process.    Both examples above were run on a macbook pro CPU (i7  2.5GHz  16GB). Some fps numbers are:   The tensorflow object detection repo has a python file for exporting a checkpoint to frozen graph here.  You can copy it to the current directory and use it as follows     python     python   I exported the model using the Tensorflow.js converter and have it wrapped into an easy to use javascript library - [Handtrack.js](https://victordibia.github.io/handtrack.js/). You can do hand tracking in 3 lines of code  no installation  no model training  all in the browser.  <img src=""images/doodle.gif"" width=""100%"">  Learn more below  - Blog Post:  [Hand Tracking Interactions in the Browser using Tensorflow.js and 3 lines of code.](https://medium.com/@victor.dibia/handtrackjs-677c29c1d585) - Github: [Handtrack.js Github Repo](https://github.com/victordibia/handtrack.js) - Live Demo : [Handtrack.js Examples in the Browser](https://victordibia.github.io/handtrack.js/)   ![android_sample_1](images/android_sample_1.png)  The trained model checkpoints are converted to the [TensorFlow Lite](https://www.tensorflow.org/lite) format so that they can used in both Android and iOS apps.   The Android app which uses the hand tracking model from this repo is available here -> [shubham0204/Hand_Detection_TFLite_Android](https://github.com/shubham0204/Hand_Detection_TFLite_Android)  Also  a step-by-step guide on how to convert the model checkpoints to a TFLite model ( `.tflite` ) is available as a IPYNB notebook ( open it in Google Colab ) -> [shubham0204/Google_Colab_Notebooks/Hand_Tracking_Model_TFLite_Conversion.ipynb](https://github.com/shubham0204/Google_Colab_Notebooks/blob/main/Hand_Tracking_Model_TFLite_Conversion.ipynb)    A few people have used the handtracking sample code/models in creating some awesome projects and I'd like to highlight them here! - (Alphabot)[https://github.com/drewgillson/alphabot]: a screen-less interactive spelling primer powered by computer vision - (Wall Z the Robot)[https://challengerocket.com/megatran/Wall-Z-the-Robot-8a34db.html] - (Predicting hand pose)[https://github.com/MrEliptik/HandPose] : Using the output of a hand detector to predict hand pose by Victor Meunier. - (Hand Tracking Pong)[https://github.com/alvinwan/hand-tracking-pong]: Hand Tracking ping pong. - (AiryDraw!)[https://github.com/amirhossein-ahmadian/airydraw] is a simple augmented reality program which gives you the feeling that you can draw in the air just using your hand.  - (Gesture Recognition)[https://github.com/zzeitt/Gesture-Recognition] Uses this repo to  extract hand bounding boxes and runs a classifier on top of the extracted hand to detect hand gestures (fist  two fingers and open palm. Neat!)  - (Video Gesture Recognition)[https://github.com/ashwatc/Video_Gesture_Overlay] and Overlay (Using Machine Learning and Computer Vision). This project uses the handtracking models to prototype quick controls for video conferencing scenarios (e.g. I agree  yes  or stepping away from the video call). Check it out!  If you have created something cool  send me a note (or (tweet)[https://twitter.com/vykthur]) and I'll be happy to include it here!   """;Computer Vision;https://github.com/victordibia/handtracking
"""bash ./datasets/download_dataset.sh dataset_name   bash ./models/download_model.sh model_name   bash ./scripts/eval_cityscapes/download_fcn8s.sh  Then make sure ./scripts/eval_cityscapes/ is in your system's python path. If not  run the following command to add it   Now you can run the following command to evaluate your predictions:  python ./scripts/eval_cityscapes/evaluate.py --cityscapes_dir /path/to/original/cityscapes/dataset/ --result_dir /path/to/your/predictions/ --output_dir /path/to/output/directory/   Install it with: luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec   - Install torch and dependencies from https://github.com/torch/distro - Install torch packages `nngraph` and `display` ```bash luarocks install nngraph luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec ``` - Clone this repo: ```bash git clone git@github.com:phillipi/pix2pix.git cd pix2pix ``` - Download the dataset (e.g.  [CMP Facades](http://cmp.felk.cvut.cz/~tylecr1/facade/)): ```bash bash ./datasets/download_dataset.sh facades ``` - Train the model ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA th train.lua ``` - (CPU only) The same training command without using a GPU or CUDNN. Setting the environment variables ```gpu=0 cudnn=0``` forces CPU only ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA gpu=0 cudnn=0 batchSize=10 save_epoch_freq=5 th train.lua ``` - (Optionally) start the display server to view results as the model trains. ( See [Display UI](#display-ui) for more details): ```bash th -ldisplay.start 8000 0.0.0.0 ```  - Finally  test the model: ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA phase=val th test.lua ``` The test results will be saved to an html file here: `./results/facades_generation/latest_net_G_val/index.html`.   """;Computer Vision;https://github.com/pokurin123/pix2pix_try
"""bash ./datasets/download_dataset.sh dataset_name   bash ./models/download_model.sh model_name   bash ./scripts/eval_cityscapes/download_fcn8s.sh  Then make sure ./scripts/eval_cityscapes/ is in your system's python path. If not  run the following command to add it   Now you can run the following command to evaluate your predictions:  python ./scripts/eval_cityscapes/evaluate.py --cityscapes_dir /path/to/original/cityscapes/dataset/ --result_dir /path/to/your/predictions/ --output_dir /path/to/output/directory/   Install it with: luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec   - Install torch and dependencies from https://github.com/torch/distro - Install torch packages `nngraph` and `display` ```bash luarocks install nngraph luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec ``` - Clone this repo: ```bash git clone git@github.com:phillipi/pix2pix.git cd pix2pix ``` - Download the dataset (e.g.  [CMP Facades](http://cmp.felk.cvut.cz/~tylecr1/facade/)): ```bash bash ./datasets/download_dataset.sh facades ``` - Train the model ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA th train.lua ``` - (CPU only) The same training command without using a GPU or CUDNN. Setting the environment variables ```gpu=0 cudnn=0``` forces CPU only ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA gpu=0 cudnn=0 batchSize=10 save_epoch_freq=5 th train.lua ``` - (Optionally) start the display server to view results as the model trains. ( See [Display UI](#display-ui) for more details): ```bash th -ldisplay.start 8000 0.0.0.0 ```  - Finally  test the model: ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA phase=val th test.lua ``` The test results will be saved to an html file here: `./results/facades_generation/latest_net_G_val/index.html`.   """;General;https://github.com/pokurin123/pix2pix_try
"""executing the ` unet_learner` function will give you the modified unet with dropout. using the `DropOutAlexnet` class will give you the alexnet architecture with dropout added.   """;General;https://github.com/aredier/monte_carlo_dropout
"""""";Computer Vision;https://github.com/kamalkraj/Vision-Transformer
"""Tested on PyTorch 1.3 (torch  torchvision & scipy packages are required)   gpu: select GPU device (by id  e.g. 0)       python3 sparse_example.py --gpu 0 --sparsity adaptive --starget .9 --lv 10.0  -------------------------------------------------------------------------  """;Computer Vision;https://github.com/georgeretsi/SparsityLoss
"""- Mackay uses Gaussian approximation to get solutions to BNNs  approximates posterior centred around most probable parameter values which are found via optimisation. Error is estimated from Hessian. This approximation assumes posterior is unimodal  and breaks down when number of parameters approaches numbr of datapoints. Uses this analytic approximation to calculate the evidence. Picks value of hyperparameters which maximise evidence  which equivalently maximise the posterior of the hyperparameters given the data  for a uniform prior on the hyperparameters. Thus essentially assumes that marginalising over hyperparameters and taking their maximum are equal  i.e. the hyperparam posterior is also Gaussian. then looks at evidence values given these best hyperparameters and training set error to evaluate models. Uses some approximation of the evidence maximisation to update the hyperparameter.  - finds when a rubbish model used  evidence and test error not as correlated as when good model is used. Further  the evidence is low in some cases where test error is good. Uses this to deduce structure of model is wrong. Also sees Occam's hill.  - Likelihood variance is fixed. Initially one hyperparam used for all weights and biases. Found evidence and generalisation don‚Äôt correlate well. MacKay argues this is because the scales of the inputs  outputs and hidden layers are not the same  so one cannot expect scaling the weights by the same amount to work well. So then tried one hyperparam for hidden unit weights  one for hidden unit biases  then one for output weights and biases. This gave higher evidence  higher test set performance  and stronger correlation between the two  - Neal uses HMC to sample the BNN parameters  and Gibbs sampling to sample the hyperparameters. n.b. HMC requires gradient information  so can't be used to sample hyperparameters directly (to my knowledge). Also  HMC in essence has characteristics similar to common optimisation methods which use 'momentum' and 'velocity'.  - Neal also introduces concept of using Gaussian processes to introduce a prior over functions  which tells us what nn predicts mapping function to be without any data.  - From Neal it seems that sampling hyperparameters seems rather necessary to justify allowing NN to be arbitrarily big- if complex model is not needed  hyperparameters will 'quash' nodes which aren't important  according to the hyperparameter values assigned by the data during the training  and 'upweight' important nodes. Also avoids cross validation step.  - Uses stochastic/mini-batch methods.  - Neal's result (with same simple model as Mackay) on test data is similar to the Mackay best evidence model's results  but not as good as his best test error model results. Performance didn't necessarily get worse with larger networks for BNNs  but did for MAP estimates (though don't think this treated hyperparameters as stochastic).  - n.b. hyperparams in first layer indicate which inputs to network are important. using it generalises to test data better  as irrelevant attributes fit to noise in train. Furthermore  Neal scales hyperprior (gamma parameter w  which is mean precision) by number of units in previous layer i.e. for layer i w_i -> w_i * H_{i-1} for i >= 2 (note he doesn't scale first hidden layer). Note that he does not use this scaling on the biases  in particular  for hidden layers  biases are given standard hyperprior  and for output layer the biases aren't given a stochastic variance at all (instead they are usually fixed to a Gaussian with unit variance).  - larger network is  more uncertain it is to out of training distribution data.  - for bh  BNN does much better on test error than traditional (though I don't think this uses cross validation in traditional sense).  - Freitas uses reversible jump MCMC to sample neural network systems. reversible jump MCMC is necessary when number of parameters changes. This is the case here  as the number of radial basis functions (neurons) is allowed to vary in the analysis  resulting in a varying number of model parameters/hyperparameters throughout the sampling. Gives posteriors on number of functions  as well as the usual param/hyperparams ones.  - Also uses SMC to train NNs where data arrives one at a time. Idea is to model joint distribution of model parameters at each timestep  and appears to do a better job of predicting it with more time/data.  - Also does model selection  using posterior over number of basis functions. Can do this in sequential context as well.   - Finds reversible jump MCMC does as well as Mackay and Neal  and better than expectation maximisation algorithm (which is similar/equivalent to variational inference)  but is slower than EM algo.  - Gal provides the genius insight that stochastic draws from the distribution over neural networks can be done using traditional methods. Usually if using dropout regularisation  one disables the dropout once training is finished. Gal shows that using dropout during model deployment is equivalent to using variational inference to get a probabilistic model output. The parameters of the variational inference problem are determined by the dropout properties I believe. The higher the dropout probability  the stronger the prior on the inference problem.  - This essentially means a Bayesian approach can be used even for high dimensional problems  the training time is the same as that of maximisation methods  and during deployment  one is only limited by how many samples from the posterior one wants.  - Gal finds that this method exceeds traditional variational inference methods both in terms of speed and test set performance for most tasks  with the only doubts occurring in some CNNs. He also finds it outperforms traditional methods in terms of test set performance  with the added bonus that one gets an uncertainty estimate. The method however cannot give evidence estimates.   Note we do not currently have a user guide  but the code is well documented. If you are interested in learning more about running the code  please don't hesitate to send me an email at: `kj316@cam.ac.uk`.   The data used in our BNN experiments (in particular the training/test splits) can be found in the `data` repo.  The code which implements our BNNs can be found in the `forward_models` directory. The `Python` implementations of the BNNs (`NumPy`  `Keras`  and `TensorFlow` versions are available) are in the `forward_models/python_models` directory. The `C++` implementation is in the `forward_models/cpp_models` directory. The `MPE_examples` directory gives some basic examples of traditionally trained neural networks  implemented in `Keras` and `TensorFlow`.   """;General;https://github.com/SuperKam91/bnn
"""Alternatively  if you prefer to install dependencies with pip  please follow the instructions below:  ```virtualenv -p [PATH to python3.7 binary] hgcn```  ```source hgcn/bin/activate```  ```pip install -r requirements.txt```   If you don't have conda installed  please install it following the instructions [here](https://conda.io/projects/conda/en/latest/user-guide/install/index.html).  ```git clone https://github.com/HazyResearch/hgcn```  ```cd hgcn```  ```conda env create -f environment.yml```   source set_env.sh   We provide examples of training commands used to train HGCN and other graph embedding models for link prediction and node classification. In the examples below  we used a fixed random seed set to 1234 for reproducibility purposes. Note that results might slightly vary based on the machine used. To reproduce results in the paper  run each commad for 10 random seeds and average the results.   """;Natural Language Processing;https://github.com/HazyResearch/hgcn
"""bash ./datasets/download_dataset.sh dataset_name   bash ./models/download_model.sh model_name   bash ./scripts/eval_cityscapes/download_fcn8s.sh  Then make sure ./scripts/eval_cityscapes/ is in your system's python path. If not  run the following command to add it   Now you can run the following command to evaluate your predictions:  python ./scripts/eval_cityscapes/evaluate.py --cityscapes_dir /path/to/original/cityscapes/dataset/ --result_dir /path/to/your/predictions/ --output_dir /path/to/output/directory/   Install it with: luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec   - Install torch and dependencies from https://github.com/torch/distro - Install torch packages `nngraph` and `display` ```bash luarocks install nngraph luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec ``` - Clone this repo: ```bash git clone git@github.com:phillipi/pix2pix.git cd pix2pix ``` - Download the dataset (e.g.  [CMP Facades](http://cmp.felk.cvut.cz/~tylecr1/facade/)): ```bash bash ./datasets/download_dataset.sh facades ``` - Train the model ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA th train.lua ``` - (CPU only) The same training command without using a GPU or CUDNN. Setting the environment variables ```gpu=0 cudnn=0``` forces CPU only ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA gpu=0 cudnn=0 batchSize=10 save_epoch_freq=5 th train.lua ``` - (Optionally) start the display server to view results as the model trains. ( See [Display UI](#display-ui) for more details): ```bash th -ldisplay.start 8000 0.0.0.0 ```  - Finally  test the model: ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA phase=val th test.lua ``` The test results will be saved to an html file here: `./results/facades_generation/latest_net_G_val/index.html`.   """;Computer Vision;https://github.com/tianhai123/pix2pix
"""bash ./datasets/download_dataset.sh dataset_name   bash ./models/download_model.sh model_name   bash ./scripts/eval_cityscapes/download_fcn8s.sh  Then make sure ./scripts/eval_cityscapes/ is in your system's python path. If not  run the following command to add it   Now you can run the following command to evaluate your predictions:  python ./scripts/eval_cityscapes/evaluate.py --cityscapes_dir /path/to/original/cityscapes/dataset/ --result_dir /path/to/your/predictions/ --output_dir /path/to/output/directory/   Install it with: luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec   - Install torch and dependencies from https://github.com/torch/distro - Install torch packages `nngraph` and `display` ```bash luarocks install nngraph luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec ``` - Clone this repo: ```bash git clone git@github.com:phillipi/pix2pix.git cd pix2pix ``` - Download the dataset (e.g.  [CMP Facades](http://cmp.felk.cvut.cz/~tylecr1/facade/)): ```bash bash ./datasets/download_dataset.sh facades ``` - Train the model ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA th train.lua ``` - (CPU only) The same training command without using a GPU or CUDNN. Setting the environment variables ```gpu=0 cudnn=0``` forces CPU only ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA gpu=0 cudnn=0 batchSize=10 save_epoch_freq=5 th train.lua ``` - (Optionally) start the display server to view results as the model trains. ( See [Display UI](#display-ui) for more details): ```bash th -ldisplay.start 8000 0.0.0.0 ```  - Finally  test the model: ```bash DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA phase=val th test.lua ``` The test results will be saved to an html file here: `./results/facades_generation/latest_net_G_val/index.html`.   """;General;https://github.com/tianhai123/pix2pix
"""``` python   You should install ü§ó Transformers in a virtual environment. If you're unfamiliar with Python virtual environments  check out the user guide.  First  create a virtual environment with the version of Python you're going to use and activate it.  Then  you will need to install at least one of Flax  PyTorch or TensorFlow.  Please refer to TensorFlow installation page  PyTorch installation page and/or Flax installation page regarding the specific install command for your platform.  When one of those backends has been installed  ü§ó Transformers can be installed using pip as follows:  pip install transformers  If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release  you must install the library from source.  Since Transformers version v4.0.0  we now have a conda channel: huggingface.  ü§ó Transformers can be installed using conda as follows:   conda install -c huggingface transformers  Follow the installation pages of Flax  PyTorch or TensorFlow to see how to install them with conda.   You can test most of our models directly on their pages from the [model hub](https://huggingface.co/models). We also offer [private model hosting  versioning  & an inference API](https://huggingface.co/pricing) for public and private models.  Here are a few examples:   In Natural Language Processing: - [Masked word completion with BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France) - [Name Entity Recognition with Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city) - [Text generation with GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+) - [Natural Language Inference with RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal) - [Summarization with BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct) - [Question answering with DistilBERT](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species) - [Translation with T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)  In Computer Vision: - [Image classification with ViT](https://huggingface.co/google/vit-base-patch16-224) - [Object Detection with DETR](https://huggingface.co/facebook/detr-resnet-50) - [Image Segmentation with DETR](https://huggingface.co/facebook/detr-resnet-50-panoptic)  In Audio: - [Automatic Speech Recognition with Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h) - [Keyword Spotting with Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)  **[Write With Transformer](https://transformer.huggingface.co)**  built by the Hugging Face team  is the official demo of this repo‚Äôs text generation capabilities.   1. Easy-to-use state-of-the-art models:     - High performance on natural language understanding & generation  computer vision  and audio tasks.     - Low barrier to entry for educators and practitioners.     - Few user-facing abstractions with just three classes to learn.     - A unified API for using all our pretrained models.  1. Lower compute costs  smaller carbon footprint:     - Researchers can share trained models instead of always retraining.     - Practitioners can reduce compute time and production costs.     - Dozens of architectures with over 20 000 pretrained models  some in more than 100 languages.  1. Choose the right framework for every part of a model's lifetime:     - Train state-of-the-art models in 3 lines of code.     - Move a single model between TF2.0/PyTorch/JAX frameworks at will.     - Seamlessly pick the right framework for training  evaluation and production.  1. Easily customize a model or an example to your needs:     - We provide examples for each architecture to reproduce the results published by its original authors.     - Model internals are exposed as consistently as possible.     - Model files can be used independently of the library for quick experiments.   - This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose  so that researchers can quickly iterate on each of the models without diving into additional abstractions/files. - The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops  you should use another library. - While we strive to present as many use cases as possible  the scripts in our [examples folder](https://github.com/huggingface/transformers/tree/master/examples) are just that: examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.   """;Sequential;https://github.com/huggingface/transformers
"""``` python   You should install ü§ó Transformers in a virtual environment. If you're unfamiliar with Python virtual environments  check out the user guide.  First  create a virtual environment with the version of Python you're going to use and activate it.  Then  you will need to install at least one of Flax  PyTorch or TensorFlow.  Please refer to TensorFlow installation page  PyTorch installation page and/or Flax installation page regarding the specific install command for your platform.  When one of those backends has been installed  ü§ó Transformers can be installed using pip as follows:  pip install transformers  If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release  you must install the library from source.  Since Transformers version v4.0.0  we now have a conda channel: huggingface.  ü§ó Transformers can be installed using conda as follows:   conda install -c huggingface transformers  Follow the installation pages of Flax  PyTorch or TensorFlow to see how to install them with conda.   You can test most of our models directly on their pages from the [model hub](https://huggingface.co/models). We also offer [private model hosting  versioning  & an inference API](https://huggingface.co/pricing) for public and private models.  Here are a few examples:   In Natural Language Processing: - [Masked word completion with BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France) - [Name Entity Recognition with Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city) - [Text generation with GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+) - [Natural Language Inference with RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal) - [Summarization with BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct) - [Question answering with DistilBERT](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species) - [Translation with T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)  In Computer Vision: - [Image classification with ViT](https://huggingface.co/google/vit-base-patch16-224) - [Object Detection with DETR](https://huggingface.co/facebook/detr-resnet-50) - [Image Segmentation with DETR](https://huggingface.co/facebook/detr-resnet-50-panoptic)  In Audio: - [Automatic Speech Recognition with Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h) - [Keyword Spotting with Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)  **[Write With Transformer](https://transformer.huggingface.co)**  built by the Hugging Face team  is the official demo of this repo‚Äôs text generation capabilities.   1. Easy-to-use state-of-the-art models:     - High performance on natural language understanding & generation  computer vision  and audio tasks.     - Low barrier to entry for educators and practitioners.     - Few user-facing abstractions with just three classes to learn.     - A unified API for using all our pretrained models.  1. Lower compute costs  smaller carbon footprint:     - Researchers can share trained models instead of always retraining.     - Practitioners can reduce compute time and production costs.     - Dozens of architectures with over 20 000 pretrained models  some in more than 100 languages.  1. Choose the right framework for every part of a model's lifetime:     - Train state-of-the-art models in 3 lines of code.     - Move a single model between TF2.0/PyTorch/JAX frameworks at will.     - Seamlessly pick the right framework for training  evaluation and production.  1. Easily customize a model or an example to your needs:     - We provide examples for each architecture to reproduce the results published by its original authors.     - Model internals are exposed as consistently as possible.     - Model files can be used independently of the library for quick experiments.   - This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose  so that researchers can quickly iterate on each of the models without diving into additional abstractions/files. - The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops  you should use another library. - While we strive to present as many use cases as possible  the scripts in our [examples folder](https://github.com/huggingface/transformers/tree/master/examples) are just that: examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.   """;Audio;https://github.com/huggingface/transformers
"""""";Computer Vision;https://github.com/antonyvigouret/Pay-Attention-to-MLPs
"""""";General;https://github.com/antonyvigouret/Pay-Attention-to-MLPs
"""  you can run on a GPU (for free!). No modifications required - all dependencies    """;General;https://github.com/MayarLotfy/bayesianNN
"""""";Computer Vision;https://github.com/leemathew1998/GradientWeight
"""""";General;https://github.com/leemathew1998/GradientWeight
"""""";Reinforcement Learning;https://github.com/Kaixhin/ACER
"""""";General;https://github.com/TimSalimans/weight_norm
"""""";General;https://github.com/Kaixhin/ACER
"""Download a CycleGAN dataset using:   To ensure all modules in repo can be found you must update your PYTHONPATH environment variable:   The CycleGAN model takes approximately 20 hours to train to completion using a Tesla V100 GPU. To train run:   To train run:   To train run:   """;General;https://github.com/wkhademi/ImageEnhancement
"""Download a CycleGAN dataset using:   To ensure all modules in repo can be found you must update your PYTHONPATH environment variable:   The CycleGAN model takes approximately 20 hours to train to completion using a Tesla V100 GPU. To train run:   To train run:   To train run:   """;Computer Vision;https://github.com/wkhademi/ImageEnhancement
"""This class wraps around the general RL environment class to launch the CoppeliaSim with our custom scene. Additionally  in the beginning of every episode  it initialises the properties of the mating part: 2D position in the workspace (`setup_goal()` method)  as well as its colour.  The environment wrapper contains following methods:  * `get_observation()`  capture a grayscale image as an observation.  *  `distance_to_goal()`  compute the distance between the target and current position. The distance is used in reward design.     *  `success_check()`  check whether the goal state is reached. If yes  significantly boost agent's reward.     * `collision_check()`  check whether an agent collided with any object.     Episode termination occurs when the robot gets too far from the target  collides with any object in the environment or exceeds the maximum number of time steps. Those conditions are specified at the end of `step()` method and are checked at each step taken in the environment by the agent. Once the episode terminates  the whole cycle is repeated for the next episode.     One of the most exciting advancements  that has pushed the frontier of the Artificial Intelligence (AI) in recent years  is Deep Reinforcement Learning (DRL). DRL belongs to the family of machine learning algorithms. It assumes that intelligent machines can learn from their actions similar to the way humans learn from experience. Over the recent years we could witness some impressive [real-world applications of DRL](https://neptune.ai/blog/reinforcement-learning-applications). The algorithms allowed for major progress especially in the field of robotics. If you are interested in learning more about DRL  we encourage you to get familiar with the exceptional [**Introduction to RL**](https://spinningup.openai.com/en/latest) by OpenAI. We believe this is the best place to start your adventure with DRL.  The **goal of this tutorial is to show how you can apply DRL to solve your own robotic challenge**. For the sake of this tutorial we have chosen one of the classic assembly tasks: peg-in-hole insertion. By the time you finish the tutorial  you will understand how to create a complete  end-to-end pipeline for training the robot in the simulation using DRL.  The accompanying code together with all the details of the implementation can be found in our [GitHub repository](https://github.com/arrival-ltd/catalyst-rl-tutorial).   1. Download the **robot simulation platform**  CoppeliaSim  from [the official website](https://www.coppeliarobotics.com/downloads). This tutorial is compatible with the version 4.1.0.   2. Setup **toolkit for robot learning research**  PyRep  from their [github repository](https://github.com/stepjam/PyRep). PyRep library is built on top of CoppeliaSim to facilitate prototyping in python.   3. Create **an environment for the RL agent**: It could be either a simulation or a real environment. We limit ourselves to simulation for faster prototyping and training. The agent interacts with the environment to collect experience. This allows it to learn a policy which maximizes the expected (discounted) sum of future rewards and hence solves the designed task. Most RL practitioners are familiar with the [OpenAI Gym environments](https://gym.openai.com/envs/#classic_control)  a toolkit with toy environments used for developing and benchmarking reinforcement learning algorithms. However  our use case  robotic assembly task  is very specific. The goal is to train a robot to perform peg-in-hole insertion. This is why we created our simulation environment in [CoppeliaSim](https://www.coppeliarobotics.com). The simulator comes with various robot manipulators and grippers. For our tutorial  we picked UR5 robot with RG2 gripper (Figure 1).    ![](./images/sim_env.png)      <em>Figure 1: UR5 manipulator with a peg attached to its gripper. The mating part is placed on the ground in the scene. CoppeliaSim caters to a variety of different robotic tasks. Feel free to come up with your own challenge and design your own simulation! [RLBench](https://github.com/stepjam/RLBench/tree/master/rlbench/task_ttms) (the robot learning benchmark and learning environment) also provides more off-the-shelf  advanced simulation environments. </em>   4. Create **a gym environment wrapped around the simulation scene**:    ```python import os import cv2 import logging import numpy as np  from gym import Space from gym.spaces.box import Box from gym.spaces.dict import Dict from pyrep import PyRep  objects  from catalyst_rl.rl.core import EnvironmentSpec from catalyst_rl.rl.utils import extend_space   class CoppeliaSimEnvWrapper(EnvironmentSpec):     def __init__(self  visualize=True                   mode=""train""                   **params):         super().__init__(visualize=visualize  mode=mode)          #: Scene selection         scene_file_path = os.path.join(os.getcwd()  'simulation/UR5.ttt')          #: Simulator launch         self.env = PyRep()         self.env.launch(scene_file_path  headless=False)         self.env.start()         self.env.step()          #: Task related initialisations in Simulator         self.vision_sensor = objects.vision_sensor.VisionSensor(""Vision_sensor"")         self.gripper = objects.dummy.Dummy(""UR5_target"")         self.gripper_zero_pose = self.gripper.get_pose()         self.goal = objects.dummy.Dummy(""goal_target"")         self.goal_STL = objects.shape.Shape(""goal"")         self.goal_STL_zero_pose = self.goal_STL.get_pose()         self.grasped_STL = objects.shape.Shape(""Peg"")         self.stacking_area = objects.shape.Shape(""Plane"")         self.vision_sensor = objects.vision_sensor.VisionSensor(""Vision_sensor"")          self.step_counter = 0         self.max_step_count = 100         self.target_pose = None         self.initial_distance = None         self.image_width  self.image_height = 320  240         self.vision_sensor.set_resolution((self.image_width  self.image_height))         self._history_len = 1          self._observation_space = Dict(                 {""cam_image"": Box(0  255                                    [self.image_height  self.image_width  1]                                    dtype=np.uint8)})          self._action_space = Box(-1  1  (3 ))         self._state_space = extend_space(self._observation_space  self._history_len)      @property     def history_len(self):         return self._history_len      @property     def observation_space(self) -> Space:         return self._observation_space      @property     def state_space(self) -> Space:         return self._state_space      @property     def action_space(self) -> Space:         return self._action_space      def step(self  action):         done = False         info = {}         prev_distance_to_goal = self.distance_to_goal()          #: Make a step in simulation         self.apply_controls(action)         self.env.step()         self.step_counter += 1          #: Reward calculations         success_reward = self.success_check()         distance_reward = (prev_distance_to_goal - self.distance_to_goal()) / self.initial_distance          reward = distance_reward + success_reward          #: Check reset conditions         if self.step_counter > self.max_step_count:             done = True             logging.info('--------Reset: Timeout--------')         elif self.distance_to_goal() > 0.8:             done = True             logging.info('--------Reset: Too far from target--------')         elif self.collision_check():             done = True             logging.info('--------Reset: Collision--------')          return self.get_observation()  reward  done  info      def reset(self):         logging.info(""Episode reset..."")         self.step_counter = 0         self.env.stop()         self.env.start()         self.env.step()         self.setup_scene()         observation = self.get_observation()         return observation #: -------------- all methods above are required for any Gym environment  everything below is env-specific --------------      def distance_to_goal(self):         goal_pos = self.goal.get_position()         tip_pos = self.gripper.get_position()         return np.linalg.norm(np.array(tip_pos) - np.array(goal_pos))      def setup_goal(self):         goal_position = self.goal_STL_zero_pose[:3]         #: 2D goal randomization         self.target_pose = [goal_position[0] + (2 * np.random.rand() - 1.) * 0.1                              goal_position[1] + (2 * np.random.rand() - 1.) * 0.1                              goal_position[2]]         self.target_pose = np.append(self.target_pose                                       self.goal_STL_zero_pose[3:]).tolist()         self.goal_STL.set_pose(self.target_pose)          #: Randomizing the RGB of the goal and the plane         rgb_values_goal = list(np.random.rand(3 ))         rgb_values_plane = list(np.random.rand(3 ))         self.goal_STL.set_color(rgb_values_goal)         self.stacking_area.set_color(rgb_values_plane)          self.initial_distance = self.distance_to_goal()      def setup_scene(self):         self.setup_goal()         self.gripper.set_pose(self.gripper_zero_pose)      def get_observation(self):         cam_image = self.vision_sensor.capture_rgb()         gray_image = np.uint8(cv2.cvtColor(cam_image  cv2.COLOR_BGR2GRAY) * 255)         obs_image = np.expand_dims(gray_image  axis=2)         return {""cam_image"": obs_image}      def collision_check(self):         return self.grasped_STL.check_collision(             self.stacking_area) or self.grasped_STL.check_collision(self.goal_STL)      def success_check(self):         success_reward = 0.         if self.distance_to_goal() < 0.01:             success_reward = 0.01             logging.info('--------Success state--------')         return success_reward      def apply_controls(self  action):         gripper_position = self.gripper.get_position()         #: predicted action is in range (-1  1) so we are normalizing it to physical units         new_position = [gripper_position[i] + (action[i] / 200.) for i in range(3)]         self.gripper.set_position(new_position) ``` For our reinforcement learning project we use [Catalyst RL](https://github.com/Scitator/catalyst-rl-framework)  a distributed framework for reproducible RL research. This is just one of the elements of the marvellous [Catalyst](https://github.com/catalyst-team/catalyst) project. Catalyst is a [PyTorch ecosystem](https://pytorch.org/ecosystem/) framework for Deep Learning research and development. It focuses on reproducibility  rapid experimentation and codebase reuse. This means that the user can seamlessly run training loop with metrics  model checkpointing  advanced logging and distributed training support without the boilerplate code. We strongly encourage you to get familiar with the [Intro to Catalyst](https://medium.com/pytorch/catalyst-101-accelerated-pytorch-bd766a556d92) and incorporating the framework into your daily work!  We reuse its general Catalyst RL environment (`EnvironmentSpec`) class to create our custom environment. By inheriting from the `EnvironmentSpec`  you can quickly design your own environment  be it an [Atari game](https://gym.openai.com/envs/#atari)  [classic control task](https://gym.openai.com/envs/#classic_control) or [robotic simulation](https://gym.openai.com/envs/#robotics). Finally  we specify states/observations  actions and rewards using OpenAI's gym [spaces](https://gym.openai.com/docs/#spaces) type.    """;General;https://github.com/arrival-ltd/catalyst-rl-tutorial
"""This class wraps around the general RL environment class to launch the CoppeliaSim with our custom scene. Additionally  in the beginning of every episode  it initialises the properties of the mating part: 2D position in the workspace (`setup_goal()` method)  as well as its colour.  The environment wrapper contains following methods:  * `get_observation()`  capture a grayscale image as an observation.  *  `distance_to_goal()`  compute the distance between the target and current position. The distance is used in reward design.     *  `success_check()`  check whether the goal state is reached. If yes  significantly boost agent's reward.     * `collision_check()`  check whether an agent collided with any object.     Episode termination occurs when the robot gets too far from the target  collides with any object in the environment or exceeds the maximum number of time steps. Those conditions are specified at the end of `step()` method and are checked at each step taken in the environment by the agent. Once the episode terminates  the whole cycle is repeated for the next episode.     One of the most exciting advancements  that has pushed the frontier of the Artificial Intelligence (AI) in recent years  is Deep Reinforcement Learning (DRL). DRL belongs to the family of machine learning algorithms. It assumes that intelligent machines can learn from their actions similar to the way humans learn from experience. Over the recent years we could witness some impressive [real-world applications of DRL](https://neptune.ai/blog/reinforcement-learning-applications). The algorithms allowed for major progress especially in the field of robotics. If you are interested in learning more about DRL  we encourage you to get familiar with the exceptional [**Introduction to RL**](https://spinningup.openai.com/en/latest) by OpenAI. We believe this is the best place to start your adventure with DRL.  The **goal of this tutorial is to show how you can apply DRL to solve your own robotic challenge**. For the sake of this tutorial we have chosen one of the classic assembly tasks: peg-in-hole insertion. By the time you finish the tutorial  you will understand how to create a complete  end-to-end pipeline for training the robot in the simulation using DRL.  The accompanying code together with all the details of the implementation can be found in our [GitHub repository](https://github.com/arrival-ltd/catalyst-rl-tutorial).   1. Download the **robot simulation platform**  CoppeliaSim  from [the official website](https://www.coppeliarobotics.com/downloads). This tutorial is compatible with the version 4.1.0.   2. Setup **toolkit for robot learning research**  PyRep  from their [github repository](https://github.com/stepjam/PyRep). PyRep library is built on top of CoppeliaSim to facilitate prototyping in python.   3. Create **an environment for the RL agent**: It could be either a simulation or a real environment. We limit ourselves to simulation for faster prototyping and training. The agent interacts with the environment to collect experience. This allows it to learn a policy which maximizes the expected (discounted) sum of future rewards and hence solves the designed task. Most RL practitioners are familiar with the [OpenAI Gym environments](https://gym.openai.com/envs/#classic_control)  a toolkit with toy environments used for developing and benchmarking reinforcement learning algorithms. However  our use case  robotic assembly task  is very specific. The goal is to train a robot to perform peg-in-hole insertion. This is why we created our simulation environment in [CoppeliaSim](https://www.coppeliarobotics.com). The simulator comes with various robot manipulators and grippers. For our tutorial  we picked UR5 robot with RG2 gripper (Figure 1).    ![](./images/sim_env.png)      <em>Figure 1: UR5 manipulator with a peg attached to its gripper. The mating part is placed on the ground in the scene. CoppeliaSim caters to a variety of different robotic tasks. Feel free to come up with your own challenge and design your own simulation! [RLBench](https://github.com/stepjam/RLBench/tree/master/rlbench/task_ttms) (the robot learning benchmark and learning environment) also provides more off-the-shelf  advanced simulation environments. </em>   4. Create **a gym environment wrapped around the simulation scene**:    ```python import os import cv2 import logging import numpy as np  from gym import Space from gym.spaces.box import Box from gym.spaces.dict import Dict from pyrep import PyRep  objects  from catalyst_rl.rl.core import EnvironmentSpec from catalyst_rl.rl.utils import extend_space   class CoppeliaSimEnvWrapper(EnvironmentSpec):     def __init__(self  visualize=True                   mode=""train""                   **params):         super().__init__(visualize=visualize  mode=mode)          #: Scene selection         scene_file_path = os.path.join(os.getcwd()  'simulation/UR5.ttt')          #: Simulator launch         self.env = PyRep()         self.env.launch(scene_file_path  headless=False)         self.env.start()         self.env.step()          #: Task related initialisations in Simulator         self.vision_sensor = objects.vision_sensor.VisionSensor(""Vision_sensor"")         self.gripper = objects.dummy.Dummy(""UR5_target"")         self.gripper_zero_pose = self.gripper.get_pose()         self.goal = objects.dummy.Dummy(""goal_target"")         self.goal_STL = objects.shape.Shape(""goal"")         self.goal_STL_zero_pose = self.goal_STL.get_pose()         self.grasped_STL = objects.shape.Shape(""Peg"")         self.stacking_area = objects.shape.Shape(""Plane"")         self.vision_sensor = objects.vision_sensor.VisionSensor(""Vision_sensor"")          self.step_counter = 0         self.max_step_count = 100         self.target_pose = None         self.initial_distance = None         self.image_width  self.image_height = 320  240         self.vision_sensor.set_resolution((self.image_width  self.image_height))         self._history_len = 1          self._observation_space = Dict(                 {""cam_image"": Box(0  255                                    [self.image_height  self.image_width  1]                                    dtype=np.uint8)})          self._action_space = Box(-1  1  (3 ))         self._state_space = extend_space(self._observation_space  self._history_len)      @property     def history_len(self):         return self._history_len      @property     def observation_space(self) -> Space:         return self._observation_space      @property     def state_space(self) -> Space:         return self._state_space      @property     def action_space(self) -> Space:         return self._action_space      def step(self  action):         done = False         info = {}         prev_distance_to_goal = self.distance_to_goal()          #: Make a step in simulation         self.apply_controls(action)         self.env.step()         self.step_counter += 1          #: Reward calculations         success_reward = self.success_check()         distance_reward = (prev_distance_to_goal - self.distance_to_goal()) / self.initial_distance          reward = distance_reward + success_reward          #: Check reset conditions         if self.step_counter > self.max_step_count:             done = True             logging.info('--------Reset: Timeout--------')         elif self.distance_to_goal() > 0.8:             done = True             logging.info('--------Reset: Too far from target--------')         elif self.collision_check():             done = True             logging.info('--------Reset: Collision--------')          return self.get_observation()  reward  done  info      def reset(self):         logging.info(""Episode reset..."")         self.step_counter = 0         self.env.stop()         self.env.start()         self.env.step()         self.setup_scene()         observation = self.get_observation()         return observation #: -------------- all methods above are required for any Gym environment  everything below is env-specific --------------      def distance_to_goal(self):         goal_pos = self.goal.get_position()         tip_pos = self.gripper.get_position()         return np.linalg.norm(np.array(tip_pos) - np.array(goal_pos))      def setup_goal(self):         goal_position = self.goal_STL_zero_pose[:3]         #: 2D goal randomization         self.target_pose = [goal_position[0] + (2 * np.random.rand() - 1.) * 0.1                              goal_position[1] + (2 * np.random.rand() - 1.) * 0.1                              goal_position[2]]         self.target_pose = np.append(self.target_pose                                       self.goal_STL_zero_pose[3:]).tolist()         self.goal_STL.set_pose(self.target_pose)          #: Randomizing the RGB of the goal and the plane         rgb_values_goal = list(np.random.rand(3 ))         rgb_values_plane = list(np.random.rand(3 ))         self.goal_STL.set_color(rgb_values_goal)         self.stacking_area.set_color(rgb_values_plane)          self.initial_distance = self.distance_to_goal()      def setup_scene(self):         self.setup_goal()         self.gripper.set_pose(self.gripper_zero_pose)      def get_observation(self):         cam_image = self.vision_sensor.capture_rgb()         gray_image = np.uint8(cv2.cvtColor(cam_image  cv2.COLOR_BGR2GRAY) * 255)         obs_image = np.expand_dims(gray_image  axis=2)         return {""cam_image"": obs_image}      def collision_check(self):         return self.grasped_STL.check_collision(             self.stacking_area) or self.grasped_STL.check_collision(self.goal_STL)      def success_check(self):         success_reward = 0.         if self.distance_to_goal() < 0.01:             success_reward = 0.01             logging.info('--------Success state--------')         return success_reward      def apply_controls(self  action):         gripper_position = self.gripper.get_position()         #: predicted action is in range (-1  1) so we are normalizing it to physical units         new_position = [gripper_position[i] + (action[i] / 200.) for i in range(3)]         self.gripper.set_position(new_position) ``` For our reinforcement learning project we use [Catalyst RL](https://github.com/Scitator/catalyst-rl-framework)  a distributed framework for reproducible RL research. This is just one of the elements of the marvellous [Catalyst](https://github.com/catalyst-team/catalyst) project. Catalyst is a [PyTorch ecosystem](https://pytorch.org/ecosystem/) framework for Deep Learning research and development. It focuses on reproducibility  rapid experimentation and codebase reuse. This means that the user can seamlessly run training loop with metrics  model checkpointing  advanced logging and distributed training support without the boilerplate code. We strongly encourage you to get familiar with the [Intro to Catalyst](https://medium.com/pytorch/catalyst-101-accelerated-pytorch-bd766a556d92) and incorporating the framework into your daily work!  We reuse its general Catalyst RL environment (`EnvironmentSpec`) class to create our custom environment. By inheriting from the `EnvironmentSpec`  you can quickly design your own environment  be it an [Atari game](https://gym.openai.com/envs/#atari)  [classic control task](https://gym.openai.com/envs/#classic_control) or [robotic simulation](https://gym.openai.com/envs/#robotics). Finally  we specify states/observations  actions and rewards using OpenAI's gym [spaces](https://gym.openai.com/docs/#spaces) type.    """;Reinforcement Learning;https://github.com/arrival-ltd/catalyst-rl-tutorial
"""Please install the latest version from pip  old versions might suffer from bugs. Source code for up-to-date package is available in folder ```pypi_packages```.    * Check if the code is from the latest official implementation (adabelief-pytorch==0.1.0  adabelief-tf==0.1.0)       Default hyper-parameters are different from the old version.  * check all hyper-parameters  DO NOT simply use the default        >Epsilon in AdaBelief is different from Adam (typically eps_adabelief = eps_adam*eps_adam) <br>      >( eps of Adam in Tensorflow is 1e-7  in PyTorch is 1e-8  need to consider this when use AdaBelief in Tensorflow) <br>            >> If SGD is better than Adam   ->  Set a large eps (1e-8) in AdaBelief-pytorch (1e-7 in Tensorflow )<br>      >> If SGD is worse than Adam   ->  Set a small eps (1e-16) in AdaBelief-pytorch (1e-14 in Tensorflow  rectify=True often helps) <br>            >> If AdamW is better than Adam   ->   Turn on ‚Äúweight_decouple‚Äù  in AdaBelief-pytorch (this is on in adabelief-tf==0.1.0 and cannot shut down). <br>      >> Note that default weight decay is very different for Adam and AdamW  you might need to consider this when using AdaBelief with and without decoupled weight decay. <br>  * Check ALL hyper-parameters. Refer to our github page for a list of recommended hyper-parameters   We have released adabelief-pytorch==0.2.0 and adabelief-tf==0.2.0. Please use the latest version from pip. Source code is available under folder pypi_packages/adabelief_pytorch0.2.0 and pypi_packages/adabelief_tf0.2.0.   Update Plan   SN-GAN https://github.com/juntang-zhuang/SNGAN-AdaBelief <br>  Transformer (PyTorch 1.1) https://github.com/juntang-zhuang/transformer-adabelief <br>  Transformer (PyTorch 1.6) https://github.com/juntang-zhuang/fairseq-adabelief <br>   Object detection (by yuanwei2019) https://github.com/yuanwei2019/EAdam-optimizer (Note that this version uses adabelief-pytorch==0.0.5  and the default hyper-parameters is different from adabelief-pytorch==0.1.0. Please check your version of adabelief  and whether you specify all hyper-parameters  or does the default is what you want.) <br>   |   Version| epsilon | weight_decouple | rectify     |    The default value is updated  please check if you specify these arguments or use the default when upgrade from version 0.0.1 to higher.:  |   Version| epsilon | weight_decouple | rectify     |    Ôºà Results in the paper are all generated using the PyTorch implementation in adabelief-pytorch package  which is the ONLY package that I have extensively tested for now.) <br>  Please install latest version (0.2.0)  previous version (0.0.5) uses different default arguments.  pip install adabelief-pytorch==0.2.0   pip install ranger-adabelief==0.1.0   pip install adabelief-tf==0.2.0   <del> The AMSGrad implmentation might be problematic  see discusssion https://github.com/juntang-zhuang/Adabelief-Optimizer/issues/32#issuecomment-742350592 </del>   Identify the problem of Transformer with PyTorch 1.4  to be an old version fairseq is incompatible with new version PyTorch  works fine with latest fairseq. <br> Code on Transformer to work with PyTorch 1.6 is at: https://github.com/juntang-zhuang/fairseq-adabelief <br>    Code for transformer to work with PyTorch 1.1 and CUDA9.0 is at: https://github.com/juntang-zhuang/transformer-adabelief   Released adabelief-pytorch==0.2.0. Fix the error with coupled weight decay in adabelief-pytorch==0.1.0  fix the amsgrad update in adabelief-pytorch==0.1.0. Add options to disable the message printing  by specify print_change_log=False when initiating the optimizer.   <p align=""center""> <img src=""./imgs/Beale2.gif"" width=""80%""/> </p>   """;General;https://github.com/juntang-zhuang/Adabelief-Optimizer
"""Update: 07.11.2020.   """;General;https://github.com/scrayish/ML_NLP
"""Update: 07.11.2020.   """;Natural Language Processing;https://github.com/scrayish/ML_NLP
"""""";General;https://github.com/xiangli13/MoCo
"""Manual: https://github.com/AlexeyAB/darknet/wiki   Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/  Pytorch - Scaled-YOLOv4: https://github.com/WongKinYiu/ScaledYOLOv4  TensorFlow: pip install yolov4 YOLOv4 on TensorFlow 2.0 / TFlite / Andriod: https://github.com/hunglc007/tensorflow-yolov4-tflite   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example   PyTorch > ONNX:    TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   Deepstream 5.0 / TensorRT for YOLOv4 https://github.com/NVIDIA-AI-IOT/yolov4_deepstream or https://github.com/marcoslucianops/DeepStream-Yolo   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like CUDA  cudnn  ZED and build against those. It will also create a shared object library file to use darknet for code development.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   You also need to specify for which graphics card the code is generated. This is done by setting ARCH=. If you use a never version than CUDA 11 you further need to edit line 20 from Makefile and remove -gencode arch=compute_30 code=sm_30 \ as Kepler GPU support was dropped in CUDA 11. You can also drop the general ARCH= and just uncomment ARCH= for your graphics card.   * MSVS: https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community  * Cmake GUI: Windows win64-x64 Installerhttps://cmake.org/download/   find the executable file darknet.exe in the output path to the binaries you specified  This is the recommended approach to build Darknet on Windows.  Install Visual Studio 2017 or 2019. In case you need to download it  please go here: Visual Studio Community  Install CUDA (at least v10.0) enabling VS Integration during installation.   PS Code\&gt;              git clone https://github.com/microsoft/vcpkg  PS Code\&gt;              cd vcpkg   PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base cuda cudnn]:x64-windows for a quicker install of dependencies  PS Code\vcpkg&gt;         cd ..  PS Code\&gt;              git clone https://github.com/AlexeyAB/darknet  PS Code\&gt;              cd darknet   Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   in Python: https://github.com/tzutalin/labelImg  in Python: https://github.com/Cartucho/OpenLabeling   in JavaScript: https://github.com/opencv/cvat   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux   * using `build.sh` or   * build `darknet` using `cmake` or   * set `LIBSO=1` in the `Makefile` and do `make` * on Windows   * using `build.ps1` or   * build `darknet` using `cmake` or   * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs:  * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h   * Python examples using the C API:     * https://github.com/AlexeyAB/darknet/blob/master/darknet.py     * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py  * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp   * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp  ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`      * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)  `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42)  ```cpp struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false);         std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/6-dl/darknet_wpb
"""``` #: Train and test CIFAR 10 with mixup. python main_cifar10.py --mixup --exp='cifar10_nomixup' #: Train and test CIFAR 10 without mixup. python main_cifar10.py --exp='cifar10_nomixup' #: Train and test CIFAR 100 with mixup. python main_cifar100.py --mixup --exp='cifar100_mixup' #: Train and test CIFAR 100 without mixup. python main_cifar100.py --exp='cifar100_nomixup' ```  """;Computer Vision;https://github.com/leehomyc/mixup_pytorch
"""The setup assumes that you have an Nvidia GPU in your system. However  it should be possible to run the code without a  compatible GPU  by adjusting the relevant packages (pytorch and detectron2). As of now  detectron2 does not officially  support Windows. However  there have been reports that it can be run on Windows with some tweaks (see this [repository](https://github.com/ivanpp/detectron2) and  the accompanying  [tutorial](https://ivanpp.cc/detectron2-walkthrough-windows/)).   git clone https://github.com/maxfrei750/FibeR-CNN.git   cd FibeR-CNN   Install conda.   git clone https://github.com/maxfrei750/FibeR-CNN.git   cd FibeR-CNN  Create a new conda environment:  conda env create --file environment.yaml  Activate the new conda environment:   Depending on your use case  the following scripts are good starting points:       `demo.py`       `train_model.py`       `evaluate_model.py`    """;Computer Vision;https://github.com/maxfrei750/FibeR-CNN
"""Manual: https://github.com/AlexeyAB/darknet/wiki   Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example   PyTorch > ONNX:    TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like CUDA  cudnn  ZED and build against those. It will also create a shared object library file to use darknet for code development.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows.  Install Visual Studio 2017 or 2019. In case you need to download it  please go here: Visual Studio Community  Install CUDA (at least v10.0) enabling VS Integration during installation.   PS Code\&gt;              git clone https://github.com/microsoft/vcpkg  PS Code\&gt;              cd vcpkg   PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base cuda cudnn]:x64-windows for a quicker install of dependencies  PS Code\vcpkg&gt;         cd ..  PS Code\&gt;              git clone https://github.com/AlexeyAB/darknet  PS Code\&gt;              cd darknet   Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   in Python: https://github.com/tzutalin/labelImg  in Python: https://github.com/Cartucho/OpenLabeling   in JavaScript: https://github.com/opencv/cvat   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux   * using `build.sh` or   * build `darknet` using `cmake` or   * set `LIBSO=1` in the `Makefile` and do `make` * on Windows   * using `build.ps1` or   * build `darknet` using `cmake` or   * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs:  * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h   * Python examples using the C API:     * https://github.com/AlexeyAB/darknet/blob/master/darknet.py     * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py  * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp   * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp  ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`      * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)  `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42)  ```cpp struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false);         std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/AcramBousa/darknet
"""""";Computer Vision;https://github.com/soumik12345/Vision-Transformer
"""``` Layer (type)                 Output Shape              Param #:    ================================================================= input_word_ids (InputLayer)  [(None  1500)]            0          _________________________________________________________________ tf_bert_model_1 (TFBertModel ((None  1500  768)  (None 109482240))  _________________________________________________________________ tf_op_layer_strided_slice_1  [(None  768)]             0          _________________________________________________________________ dense_1 (Dense)              (None  16)                12304      ================================================================= Total params: 109 494 544 Trainable params: 109 494 544 Non-trainable params: 0 ```   tweepy for connecting the API with Python (https://pypi.org/project/tweepy/)   If you wish to contribute to our model  you can take a look at our notebook  and provide suggestions or comments.    [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/10Dj-ySjfZVqOWg25ywmPsdrnk9XJoFP-?usp=sharing)     **Landing Page**:  ![h1](https://user-images.githubusercontent.com/51776663/98763145-3f581f00-238e-11eb-9c15-e59c3d898e7a.png)  **A brief description of personality types**:  ![h2](https://user-images.githubusercontent.com/51776663/98763146-42eba600-238e-11eb-924e-2c45792281bb.png)   **Try it Out**:  Head over to the `Get Started` section to put it your Twitter Handle and press `Submit`. The model should take approx. 15 sec to return your predicted personality type on the screen as follows: ![Homepage](https://user-images.githubusercontent.com/51776663/98763149-441cd300-238e-11eb-983d-8b7d3ad5e877.png)   **Head over to the Dashboard**:  Click on `Go to Dashboard` to get detailed personality analysis along with career suggestions.  ![d](https://user-images.githubusercontent.com/51776663/98763154-467f2d00-238e-11eb-9e08-75589b52a507.png)  **Compare personality types!**:  Now you can also compare your personality type against that of your followers and friends!  ![d2](https://user-images.githubusercontent.com/51776663/98763157-4848f080-238e-11eb-9887-c463a77045c8.png)      """;Natural Language Processing;https://github.com/MLH-Fellowship/Social-BERTerfly
"""Note that the building scripts only apply to specific OS and software (Pytorch  OpenNMT  transformers  etc.) versions. Please adjust them according to your needs.   Excellent CPU / GPU performance.   | pytorch (CPU/GPU) | Medium/Medium | No | Yes | Easy |   BERT [Python] [C++]  ALBERT [Python]  Roberta [Python]  Transformer Decoder [Python]  GPT2 [Python]   git clone https://github.com/Tencent/TurboTransformers --recursive   sh tools/build_docker_cpu.sh   env BUILD_TYPE=dev sh tools/build_docker_cpu.sh   cd /workspace   cd /workspace  mkdir -p build && cd build   make -j 4  pip install find . -name *whl   cd benchmark  bash run_benchmark.sh  4. Install conda packages in docker (optional)  sh tool/build_conda_package.sh  : The conda package will be in /workspace/dist/*.tar.bz2  : When using turbo_transformers in other environments outside this container: conda install your_root_path/dist/*.tar.bz2   git clone https://github.com/Tencent/TurboTransformers --recursive   : You can modify the environment variables in the script to specify the cuda version and operating system version  sh tools/build_docker_gpu.sh $PWD   : for example: nvidia-docker run --gpus all --net=host --rm -it -v $PWD:/workspace -v /etc/passwd:/etc/passwd --name=turbo_gpu_env thufeifeibear:0.1.1-cuda9.0-ubuntu16.04-gpu-dev   cd /workspace  sh tools/build_and_run_unittests.sh $PWD -DWITH_GPU=ON   cd benchmark  bash gpu_run_benchmark.sh   Tensor Core  can accelerate computing on GPU. It is disabled by default in TurboTransformers. If you want to turn it on  before compiling code  set option WITH_MODULE_BENCHMAKR ON in CMakeLists.txt   zero-padding is required to make all the requests have the same length.   Download PyTorch version to 1.1.0 will improve Turbo's Performance.   July 2020 v0.3.1  TurboTransformers added support for ALbert  Roberta on CPU/GPU.  June 2020 v0.3.0  TurboTransformers added support for Transformer Decoder on CPU/GPU.   TurboTransformers provides C++ / python API interfaces. We hope to do our best to adapt to a variety of online environments to reduce the difficulty of development for users.    """;Natural Language Processing;https://github.com/Tencent/TurboTransformers
"""We build a plant disease diagnosis system on Android  by implementing a deep convolutional neural network with Tensorflow to detect disease from various plant leave images.   Generally  due to the size limitation of the dataset  we adopt the transefer learning in this system. Specifically  we retrain the MobileNets [[1]](https://arxiv.org/pdf/1704.04861.pdf)  which is first trained on ImageNet dataset  on the plant disease datasets. Finally  we port the trained model to Android.   """;General;https://github.com/abhimangalms/PlantDoctor
"""We build a plant disease diagnosis system on Android  by implementing a deep convolutional neural network with Tensorflow to detect disease from various plant leave images.   Generally  due to the size limitation of the dataset  we adopt the transefer learning in this system. Specifically  we retrain the MobileNets [[1]](https://arxiv.org/pdf/1704.04861.pdf)  which is first trained on ImageNet dataset  on the plant disease datasets. Finally  we port the trained model to Android.   """;Computer Vision;https://github.com/abhimangalms/PlantDoctor
"""```  #: Required: Sampling conda create --name jukebox python=3.7.5 conda activate jukebox conda install pytorch=1.4 torchvision=0.5 cudatoolkit=10.0 -c pytorch pip install mpi4py==3.0.3 git clone https://github.com/openai/jukebox.git cd jukebox pip install -r requirements.txt pip install -e .  #: Required: Training conda install av=7.0.01 -c conda-forge  pip install ./tensorboardX   #: Optional: Apex for faster training with fused_adam conda install pytorch=1.1 torchvision=0.3 cudatoolkit=10.0 -c pytorch pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./apex ```   Here  {audio_files_dir} is the directory in which you can put the audio files for your dataset  and {ngpus} is number of GPU's you want to use to train.    To train the upsampler  we can run   """;Audio;https://github.com/Broccaloo/Jukebox_mod
"""""";Natural Language Processing;https://github.com/rheem-ecosystem/rheem-benchmark
"""```python import torch from crossvit import CrossViT  img = torch.ones([1  3  224  224])      model = CrossViT(image_size = 224  channels = 3  num_classes = 100) out = model(img)  print(""Shape of out :""  out.shape)      #: [B  num_classes]   ```   """;General;https://github.com/rishikksh20/CrossViT-pytorch
"""```python import torch from crossvit import CrossViT  img = torch.ones([1  3  224  224])      model = CrossViT(image_size = 224  channels = 3  num_classes = 100) out = model(img)  print(""Shape of out :""  out.shape)      #: [B  num_classes]   ```   """;Computer Vision;https://github.com/rishikksh20/CrossViT-pytorch
"""bash ./checkpoints/download.sh      bash ./scripts/search_mobilenet_0.5flops.sh   bash ./scripts/export_mobilenet_0.5flops.sh   bash ./scripts/finetune_mobilenet_0.5flops.sh   """;Computer Vision;https://github.com/mit-han-lab/amc
"""bash ./checkpoints/download.sh      bash ./scripts/search_mobilenet_0.5flops.sh   bash ./scripts/export_mobilenet_0.5flops.sh   bash ./scripts/finetune_mobilenet_0.5flops.sh   """;General;https://github.com/mit-han-lab/amc
"""Our project will process videos that contain human faces and return video with all facial features removed  either by performing a blur with randomized parameters  or by omitting facial pixels all together. We will likely do this by training a convolution neural network with a dataset used for video facial recognition  such as ‚ÄòYoutube Faces with Facial Keypoints‚Äô found here.   Existing video editing software has blurring functionality  but the user often has to select the features  and it‚Äôs unclear whether deblurring could reveal the identity after-the-fact. There are a few papers and similar projects available online that have demonstrated such work  such as this research paper  the following two articles  and the work of Terrance Boult and Walter Schierer.  If time and project complexity allow  an additional portion of the project could be examining feasibility of an on-device-algorithm that could be used on a camera so there was no back-door to deanonymize the data.    We hope to provide side-by-side video of before and after the algorithm runs on a variety of scenes containing people. It would be cool to implement it so that we could run it on live video  but achieving this level of efficiency with our methods may not be feasible.   """;General;https://github.com/johngear/eecs504
"""Our project will process videos that contain human faces and return video with all facial features removed  either by performing a blur with randomized parameters  or by omitting facial pixels all together. We will likely do this by training a convolution neural network with a dataset used for video facial recognition  such as ‚ÄòYoutube Faces with Facial Keypoints‚Äô found here.   Existing video editing software has blurring functionality  but the user often has to select the features  and it‚Äôs unclear whether deblurring could reveal the identity after-the-fact. There are a few papers and similar projects available online that have demonstrated such work  such as this research paper  the following two articles  and the work of Terrance Boult and Walter Schierer.  If time and project complexity allow  an additional portion of the project could be examining feasibility of an on-device-algorithm that could be used on a camera so there was no back-door to deanonymize the data.    We hope to provide side-by-side video of before and after the algorithm runs on a variety of scenes containing people. It would be cool to implement it so that we could run it on live video  but achieving this level of efficiency with our methods may not be feasible.   """;Computer Vision;https://github.com/johngear/eecs504
"""For the experiments  the following losses are also implemented:   Training LeNet on MNIST using cross-entropy loss and no label corruption: ``` python3 train.py dataset=mnist model=lenet loss=ce dataset.train.corrupt_prob=0.0 ```  Training a ResNet-50 on CIFAR-10 using the partially Huberised cross-entropy loss (PHuber-CE) with œÑ=2  and label corruption probability œÅ of 0.2:  ``` python3 train.py dataset=cifar10 model=resnet50 loss=phuber_ce loss.tau=2 dataset.train.corrupt_prob=0.2 ```  Training a ResNet-50 on CIFAR-100 using the Generalized Cross Entropy loss (GCE) and label corruption probability œÅ of 0.6  with [mixed precision](https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/):  ``` python3 train.py dataset=cifar100 model=resnet50 loss=gce dataset.train.corrupt_prob=0.6 mixed_precision=true ```   Training LeNet on MNIST using cross-entropy loss  and varying label corruption probability œÅ (0.0  0.2  0.4 and 0.6). This uses [Hydra's multi-run flag](https://hydra.cc/docs/tutorials/basic/running_your_app/multi-run) for parameter sweeps:  ``` python3 train.py --multirun dataset=mnist model=lenet loss=ce dataset.train.corrupt_prob=0.0 0.2 0.4 0.6 ```   """;General;https://github.com/dmizr/phuber
"""For the experiments  the following losses are also implemented:   Training LeNet on MNIST using cross-entropy loss and no label corruption: ``` python3 train.py dataset=mnist model=lenet loss=ce dataset.train.corrupt_prob=0.0 ```  Training a ResNet-50 on CIFAR-10 using the partially Huberised cross-entropy loss (PHuber-CE) with œÑ=2  and label corruption probability œÅ of 0.2:  ``` python3 train.py dataset=cifar10 model=resnet50 loss=phuber_ce loss.tau=2 dataset.train.corrupt_prob=0.2 ```  Training a ResNet-50 on CIFAR-100 using the Generalized Cross Entropy loss (GCE) and label corruption probability œÅ of 0.6  with [mixed precision](https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/):  ``` python3 train.py dataset=cifar100 model=resnet50 loss=gce dataset.train.corrupt_prob=0.6 mixed_precision=true ```   Training LeNet on MNIST using cross-entropy loss  and varying label corruption probability œÅ (0.0  0.2  0.4 and 0.6). This uses [Hydra's multi-run flag](https://hydra.cc/docs/tutorials/basic/running_your_app/multi-run) for parameter sweeps:  ``` python3 train.py --multirun dataset=mnist model=lenet loss=ce dataset.train.corrupt_prob=0.0 0.2 0.4 0.6 ```   """;Computer Vision;https://github.com/dmizr/phuber
"""Install via pip: ```bash pip install efficientnet_pytorch ```  Or install from source: ```bash git clone https://github.com/lukemelas/EfficientNet-PyTorch cd EfficientNet-Pytorch pip install -e . ```   Install with pip install efficientnet_pytorch and load a pretrained EfficientNet with:   <img src=""https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnetv2-image.png"" width=""100%"" />   Upgrade the pip package with pip install --upgrade efficientnet-pytorch   Also: fixed a CUDA/CPU bug (#32)   At the moment  you can easily:   If you have any feature requests or questions  feel free to leave them as GitHub issues!   |    Name         |*   Below is a simple  complete example. It may also be found as a jupyter notebook in `examples/simple` or as a [Colab Notebook](https://colab.research.google.com/drive/1Jw28xZ1NJq4Cja4jLe6tJ6_F5lCzElb4).  We assume that in your current directory  there is a `img.jpg` file and a `labels_map.txt` file (ImageNet class names). These are both included in `examples/simple`.  ```python import json from PIL import Image import torch from torchvision import transforms  from efficientnet_pytorch import EfficientNet model = EfficientNet.from_pretrained('efficientnet-b0')  #: Preprocess image tfms = transforms.Compose([transforms.Resize(224)  transforms.ToTensor()      transforms.Normalize([0.485  0.456  0.406]  [0.229  0.224  0.225]) ]) img = tfms(Image.open('img.jpg')).unsqueeze(0) print(img.shape) #: torch.Size([1  3  224  224])  #: Load ImageNet class names labels_map = json.load(open('labels_map.txt')) labels_map = [labels_map[str(i)] for i in range(1000)]  #: Classify model.eval() with torch.no_grad():     outputs = model(img)  #: Print predictions print('-----') for idx in torch.topk(outputs  k=5).indices.squeeze(0).tolist():     prob = torch.softmax(outputs  dim=1)[0  idx].item()     print('{label:<75} ({p:.2f}%)'.format(label=labels_map[idx]  p=prob*100)) ```   You can easily extract features with `model.extract_features`: ```python from efficientnet_pytorch import EfficientNet model = EfficientNet.from_pretrained('efficientnet-b0')  #: ... image preprocessing as in the classification example ... print(img.shape) #: torch.Size([1  3  224  224])  features = model.extract_features(img) print(features.shape) #: torch.Size([1  1280  7  7]) ```   Exporting to ONNX for deploying to production is now simple: ```python import torch from efficientnet_pytorch import EfficientNet  model = EfficientNet.from_pretrained('efficientnet-b1') dummy_input = torch.randn(10  3  240  240)  model.set_swish(memory_efficient=False) torch.onnx.export(model  dummy_input  ""test-b1.onnx""  verbose=True) ```  [Here](https://colab.research.google.com/drive/1rOAEXeXHaA8uo3aG2YcFDHItlRJMV0VP) is a Colab example.    """;Computer Vision;https://github.com/lukemelas/EfficientNet-PyTorch
"""<a href=""https://pypi.org/project/transformer-implementations/"">PyPi</a>  ```bash $ pip install transformer-implementations ```  or  ```bash python setup.py build python setup.py install ```           <img alt=""PyPi Version"" src=""https://img.shields.io/pypi/v/transformer-implementations"">           <img alt=""PyPi Downloads"" src=""https://img.shields.io/pypi/dm/transformer-implementations"">           <img alt=""Package Status"" src=""https://img.shields.io/pypi/status/transformer-implementations"">   In <a href=""https://github.com/UdbhavPrasad072300/Transformer-Implementations/blob/main/notebooks/"">notebooks</a> directory there is a notebook on how to use each of these models for their intented use; such as image classification for Vision Transformer (ViT) and others. Check them out!  ```python from transformer_package.models import ViT  image_size = 28 #: Model Parameters channel_size = 1 patch_size = 7 embed_size = 512 num_heads = 8 classes = 10 num_layers = 3 hidden_size = 256 dropout = 0.2  model = ViT(image_size               channel_size               patch_size               embed_size               num_heads               classes               num_layers               hidden_size               dropout=dropout).to(DEVICE)              prediction = model(image_tensor) ```   """;Computer Vision;https://github.com/UdbhavPrasad072300/Transformer-Implementations
"""Lyra is built using Google's build system  Bazel. Install it following these [instructions](https://docs.bazel.build/versions/master/install.html). Bazel verson 4.0.0 is required  and some Linux distributions may make an older version available in their application repositories  so make sure you are using the required version or newer. The latest version can be downloaded via [Github](https://github.com/bazelbuild/bazel/releases).  Lyra can be built from linux using bazel for an arm android target  or a linux target.  The android target is optimized for realtime performance.  The linux target is typically used for development and debugging.   You can build the cc_binaries with the default config.  encoder_main is an   Similarly  you can build decoder_main and use it on the output of encoder_main   bazel build android_example:lyra_android_example --config=android_arm64 --copt=-DBENCHMARK  adb install bazel-bin/android_example/lyra_android_example.apk   If you press 'Benchmark'  you should see something like the following in logcat   to create a .so that you can use in your own build system. Or you can use it   You can build the example cc_binary targets with:   Given a LyraEncoder  any audio stream can be compressed using the Encode   using the following interface:   """;Sequential;https://github.com/google/lyra
"""You can download datasets of the competition from the following link:   or you can install using pip by running: pip install pytorch-tabnet   Pytorch Implementation: https://github.com/dreamquark-ai/tabnet   kaggle notebook:https://www.kaggle.com/chriscc/kubi-pytorch-moa-transfer                 - https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids   sklearn implementation: https://scikit-learn.org/stable/modules/permutation_importance.html   """;General;https://github.com/danleiQ/Mechanisms-of-Action-Classification
"""This is a classifier to predict label of hand drawn doodle images in real time. The idea is based on [QuickDraw](https://quickdraw.withgoogle.com/#) by Google. The [dataset](https://github.com/googlecreativelab/quickdraw-dataset) they provide contains 50 million images across 345 categories! I am using a subset of 50 categories for my model because of limited resources but one can use the same code with small tweaks to train on all 345 categories.  I built and trained the model in Pytorch and converted it to onnx format to use it in the browser. Initially my plan was to perform the classification on the backend. After drawing  the user would press a button and the request would be sent to the server for classification. That is how I built the app. However  it was very expensive on the server because for every image there would a request. Hence I decided to move the classification on the frontend. Also it is a lot more fun to see the model try to classify the image in real time :grin:.  It took me a very long time to train and tweak the model to obtain a good accuracy particularly because the dataset was huge even though I was using only a subset of it and training the model on the GPU. How someone draws a certain object varies a lot. It is all based on imagination and perception of that person about that object. Hence it was necessary to use lots of images per category to capture maximum variations.   For the record  88% was the average test accuracy(averaged out across the classes). Had it been allowed to train for longer  I believe it could have crossed 90% mark but I had already spent days on training it so I let it go. Yes days! Colab has a limit on the usage of GPUs after which the runtime gets disconnected. So I had to train the model for some hours every day for about 2-3 days to get good accuracy. Then I would make tweaks to the model and restart the process.     Live App: https://doodleai.herokuapp.com   ![ezgif com-gif-maker](https://user-images.githubusercontent.com/44807945/104150519-14c32a80-5400-11eb-99f9-949163feda34.gif)  I am sure  you can draw better doodles :wink:.  """;Computer Vision;https://github.com/Prateek93a/DoodleAI
"""  - Name: Inception v3       Weights: https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth    All fields except for Name are optional. You can add any fields you like  but the ones above have a    Name: RexNet   Check out our [official documentation](https://model-index.readthedocs.io/en/latest/) on how to get started.    """;Computer Vision;https://github.com/paperswithcode/model-index
"""""";Sequential;https://github.com/anandaswarup/TTS
"""""";General;https://github.com/anandaswarup/TTS
"""This code has been tested on Ubuntu 18.04 with a Nvidia GeForce GTX Titan XP GPU  CUDA Version 11.0  Python 3.6.9  and PyTorch 1.3.1.   """;General;https://github.com/JeongHyunJin/Jeong2020_SolarFarsideMagnetograms
"""This code has been tested on Ubuntu 18.04 with a Nvidia GeForce GTX Titan XP GPU  CUDA Version 11.0  Python 3.6.9  and PyTorch 1.3.1.   """;Computer Vision;https://github.com/JeongHyunJin/Jeong2020_SolarFarsideMagnetograms
"""""";Computer Vision;https://github.com/kdexd/virtex
"""pip install madgrad   The madgrad.py file containing the optimizer can be directly dropped into any PyTorch project if you don't want to install via pip. If you are using fairseq  you need the acompanying fairseq_madgrad.py file as well.   You may need to use a lower weight decay than you are accustomed to. Often 0.   """;General;https://github.com/facebookresearch/madgrad
"""""";Computer Vision;https://github.com/Chubbyman2/GAN_Waifu_Generator
"""Check the [file here for example outputs](https://github.com/akashe/Python-Code-Generation/blob/main/data/example_output.txt) for better formatting. Refer [this file](https://github.com/akashe/Python-Code-Generation/blob/main/Conala_with_original_data_with_python_embeddings.ipynb) for code.   """;General;https://github.com/akashe/Python-Code-Generation
"""Check the [file here for example outputs](https://github.com/akashe/Python-Code-Generation/blob/main/data/example_output.txt) for better formatting. Refer [this file](https://github.com/akashe/Python-Code-Generation/blob/main/Conala_with_original_data_with_python_embeddings.ipynb) for code.   """;Natural Language Processing;https://github.com/akashe/Python-Code-Generation
"""Python Libraries: NLTK<br>  MOSES: https://github.com/moses-smt/mosesdecoder<br>   We recommend you to use the latest version of Theano.<br>   """;Natural Language Processing;https://github.com/nyu-dl/dl4mt-cdec
"""Wheels (precompiled binary packages) are available for Linux (x86_64). Package names are different depending on your CUDA Toolkit version (CUDA Toolkit version is shown in `nvcc --version`).  | CUDA Toolkit version | Installation command        | |----------------------|-----------------------------| | >= v10.2             | `pip install bagua-cuda102` | | >= v11.1             | `pip install bagua-cuda111` | | >= v11.3             | `pip install bagua-cuda113` |  Add `--pre` to `pip install` commands to install pre-release (development) versions. See [Bagua tutorials](https://tutorials.baguasys.com/getting-started/) for quick start guide and more installation options.   Bagua Main Git Repo   Thanks to the [Amazon Machine Images (AMI)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html)  we can provide users an easy way to deploy and run Bagua on AWS EC2 clusters with flexible size of machines and a wide range of GPU types. Users can find our pre-installed Bagua image on EC2 by a unique AMI-ID that we publish here.   | Bagua version  | AMI ID |  Region | |---|---|---| | 0.6.3 | ami-0e719d0e3e42b397e | us-east-1 |  To manage the EC2 cluster more efficiently  we use [Starcluster](http://star.mit.edu/cluster/) as a toolkit to manipulate the cluster. In the `config` file of Starcluster  there are a few configurations that need to be set up by users  including AWS credentials  cluster settings  etc. More information regarding the Starcluster configuration can be found in this [tutorial](http://star.mit.edu/cluster/docs/latest/quickstart.html). Note that AMI is a regional resource  so you need to specify the AMI ID and its corresponding EC2 region at the same time.  For example  we create a EC2 cluster with 4 machines (`p3.16xlarge`)  each of which has 8 V100 GPUs. The cluster is based on the Bagua AMI we pre-installed in `us-east-1` region. Then the `config` file of Starcluster would be:  ```yaml #: region of EC2 instances  here we choose us_east_1 AWS_REGION_NAME = us-east-1 AWS_REGION_HOST = ec2.us-east-1.amazonaws.com #: AMI ID of Bagua NODE_IMAGE_ID = ami-0e719d0e3e42b397e #: number of instances CLUSTER_SIZE = 4 #: instance type NODE_INSTANCE_TYPE = p3.16xlarge ```  With above setup  we created two identical clusters to benchmark a synthesized image classification task over Bagua and Horovod  respectively. Here is the screen recording video of this experiment.   <p align=""center"">     <a href=""https://youtu.be/G8o5HVYZJvs""><img src=""https://user-images.githubusercontent.com/18649508/136463585-ba911d20-9088-48b7-ab32-fc3e465c31b8.png"" width=""600""/></a> </p>   """;General;https://github.com/BaguaSys/bagua
"""""";Computer Vision;https://github.com/ayushmankumar7/SegNet---Tensorflow-2
"""""";Computer Vision;https://github.com/sirius-image-inpainting/Free-Form-Image-Inpainting-With-Gated-Convolution
"""""";General;https://github.com/sirius-image-inpainting/Free-Form-Image-Inpainting-With-Gated-Convolution
"""- `main.lua` (~30 lines) - loads all other files  starts training. - `opts.lua` (~50 lines) - all the command-line options and description - `data.lua` (~60 lines) - contains the logic to create K threads for parallel data-loading. - `donkey.lua` (~200 lines) - contains the data-loading logic and details. It is run by each data-loader thread. random image cropping  generating 10-crops etc. are in here. - `model.lua` (~80 lines) - creates AlexNet model and criterion - `train.lua` (~190 lines) - logic for training the network. we hard-code a learning rate + weight decay schedule that produces good results. - `test.lua` (~120 lines) - logic for testing the network on validation set (including calculating top-1 and top-5 errors) - `dataset.lua` (~430 lines) - a general purpose data loader  mostly derived from [here: imagenetloader.torch](https://github.com/soumith/imagenetloader.torch). That repo has docs and more examples of using this loader.  To do this  download ILSVRC2012_img_train.tar ILSVRC2012_img_val.tar and use the following commands:   mkdir train && mv ILSVRC2012_img_train.tar train/ && cd train  tar -xvf ILSVRC2012_img_train.tar && rm -f ILSVRC2012_img_train.tar  find . -name ""*.tar"" | while read NAME ; do mkdir -p ""${NAME%.tar}""; tar -xvf ""${NAME}"" -C ""${NAME%.tar}""; rm -f ""${NAME}""; done   cd ../ && mkdir val && mv ILSVRC2012_img_val.tar val/ && cd val && tar -xvf ILSVRC2012_img_val.tar  wget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash   Now you are all set!   """;Computer Vision;https://github.com/soumith/imagenet-multiGPU.torch
"""Notebook based on https://github.com/lucidrains/TimeSformer-pytorch repository.    """;Computer Vision;https://github.com/davide-coccomini/TimeSformer-Video-Classification
"""""";Computer Vision;https://github.com/selimseker/logogram-language-generator
"""""";General;https://github.com/selimseker/logogram-language-generator
"""üëâ https://portrait-me.herokuapp.com/   - COCO - https://cocodataset.org/#home   """;Computer Vision;https://github.com/G0rav/Automatic_Background_Removal
"""We provided following boundaries in folder `boundaries/`. The boundaries can be more accurate if stronger attribute predictor is used.  - ProgressiveGAN model trained on CelebA-HQ dataset:   - Single boundary:     - `pggan_celebahq_pose_boundary.npy`: Pose.     - `pggan_celebahq_smile_boundary.npy`: Smile (expression).     - `pggan_celebahq_age_boundary.npy`: Age.     - `pggan_celebahq_gender_boundary.npy`: Gender.     - `pggan_celebahq_eyeglasses_boundary.npy`: Eyeglasses.     - `pggan_celebahq_quality_boundary.npy`: Image quality.   - Conditional boundary:     - `pggan_celebahq_age_c_gender_boundary.npy`: Age (conditioned on gender).     - `pggan_celebahq_age_c_eyeglasses_boundary.npy`: Age (conditioned on eyeglasses).     - `pggan_celebahq_age_c_gender_eyeglasses_boundary.npy`: Age (conditioned on gender and eyeglasses).     - `pggan_celebahq_gender_c_age_boundary.npy`: Gender (conditioned on age).     - `pggan_celebahq_gender_c_eyeglasses_boundary.npy`: Gender (conditioned on eyeglasses).     - `pggan_celebahq_gender_c_age_eyeglasses_boundary.npy`: Gender (conditioned on age and eyeglasses).     - `pggan_celebahq_eyeglasses_c_age_boundary.npy`: Eyeglasses (conditioned on age).     - `pggan_celebahq_eyeglasses_c_gender_boundary.npy`: Eyeglasses (conditioned on gender).     - `pggan_celebahq_eyeglasses_c_age_gender_boundary.npy`: Eyeglasses (conditioned on age and gender). - StyleGAN model trained on CelebA-HQ dataset:   - Single boundary in $\mathcal{Z}$ space:     - `stylegan_celebahq_pose_boundary.npy`: Pose.     - `stylegan_celebahq_smile_boundary.npy`: Smile (expression).     - `stylegan_celebahq_age_boundary.npy`: Age.     - `stylegan_celebahq_gender_boundary.npy`: Gender.     - `stylegan_celebahq_eyeglasses_boundary.npy`: Eyeglasses.   - Single boundary in $\mathcal{W}$ space:     - `stylegan_celebahq_pose_w_boundary.npy`: Pose.     - `stylegan_celebahq_smile_w_boundary.npy`: Smile (expression).     - `stylegan_celebahq_age_w_boundary.npy`: Age.     - `stylegan_celebahq_gender_w_boundary.npy`: Gender.     - `stylegan_celebahq_eyeglasses_w_boundary.npy`: Eyeglasses.  - StyleGAN model trained on FF-HQ dataset:   - Single boundary in $\mathcal{Z}$ space:     - `stylegan_ffhq_pose_boundary.npy`: Pose.     - `stylegan_ffhq_smile_boundary.npy`: Smile (expression).     - `stylegan_ffhq_age_boundary.npy`: Age.     - `stylegan_ffhq_gender_boundary.npy`: Gender.     - `stylegan_ffhq_eyeglasses_boundary.npy`: Eyeglasses.   - Conditional boundary in $\mathcal{Z}$ space:     - `stylegan_ffhq_age_c_gender_boundary.npy`: Age (conditioned on gender).     - `stylegan_ffhq_age_c_eyeglasses_boundary.npy`: Age (conditioned on eyeglasses).     - `stylegan_ffhq_eyeglasses_c_age_boundary.npy`: Eyeglasses (conditioned on age).     - `stylegan_ffhq_eyeglasses_c_gender_boundary.npy`: Eyeglasses (conditioned on gender).   - Single boundary in $\mathcal{W}$ space:     - `stylegan_ffhq_pose_w_boundary.npy`: Pose.     - `stylegan_ffhq_smile_w_boundary.npy`: Smile (expression).     - `stylegan_ffhq_age_w_boundary.npy`: Age.     - `stylegan_ffhq_gender_w_boundary.npy`: Gender.     - `stylegan_ffhq_eyeglasses_w_boundary.npy`: Eyeglasses.   ```bash NUM=10000 python generate_data.py -m pggan_celebahq -o data/pggan_celebahq -n ""$NUM"" ```   build(): Build a pytorch module.   Pick up a model  pick up a boundary  pick up a latent code  and then EDIT!  ```bash #: Before running the following code  please first download #: the pre-trained ProgressiveGAN model on CelebA-HQ dataset  #: and then place it under the folder "".models/pretrain/"". LATENT_CODE_NUM=10 python edit.py \     -m pggan_celebahq \     -b boundaries/pggan_celebahq_smile_boundary.npy \     -n ""$LATENT_CODE_NUM"" \     -o results/pggan_celebahq_smile_editing ```   We take ProgressiveGAN model trained on CelebA-HQ dataset as an instance.   """;Computer Vision;https://github.com/genforce/interfacegan
"""First download the code by git clone this repo: ```bash git clone https://github.com/galprz/brain-tumor-segemntation ``` Then use conda to install dependencies and setup the environment  ```bash conda end update -f environment.yml conda activate brain-tumor-segmentation ```  We tested those models` performance with the dice metric on the brain_tumor_dataset (https://figshare.com/articles/brain_tumor_dataset/1512427).   """;Computer Vision;https://github.com/galprz/brain-tumor-segmentation
"""To install NPM dependencies: ``` npm install ``` To install all python dependencies: ``` pip3 install -r requirements.txt ``` To run the web app: ``` python3 explore.py ``` Make sure to download the [word vectors](http://master2-bigdata.polytechnique.fr/FrenchLinguisticResources/resources) you're interseting in testing under '../word2vec/dascim2.bin'  We introduce the following resources:<br>   BARThez github link: https://github.com/moussaKam/BARThez    """;Sequential;https://github.com/hadi-abdine/FrenchWordEmbeddingsDemo
"""You can install fastT5 from PyPI:  ```python  pip install fastt5 ```  If you want to build from source:  ```python git clone https://github.com/Ki6an/fastT5 cd fastT5 pip3 install -e . ```   If you are unable to call $ transformers-cli login or prefer to use your API Key  found at https://huggingface.co/settings/token (or https://huggingface.co/organizations/ORG_NAME/settings/token for organizations)  you can pass that as a string to set_auth_token. Avoid hard-coding your API key into code by setting the environment variable HF_API_KEY=&lt;redacted&gt;  and then in code:   auth_token = os.environ.get(""HF_API_KEY"")   The `export_and_get_onnx_model()` method exports the given pretrained T5 model to onnx  quantizes it and runs it on the onnxruntime with default settings. The returned model from this method supports the `generate()` method of huggingface.  > If you don't wish to quantize the model then use `quantized=False` in the method.  ```python from fastT5 import export_and_get_onnx_model from transformers import AutoTokenizer  model_name = 't5-small' model = export_and_get_onnx_model(model_name)  tokenizer = AutoTokenizer.from_pretrained(model_name) t_input = ""translate English to French: The universe is a dark forest."" token = tokenizer(t_input  return_tensors='pt')  tokens = model.generate(input_ids=token['input_ids']                 attention_mask=token['attention_mask']                 num_beams=2)  output = tokenizer.decode(tokens.squeeze()  skip_special_tokens=True) print(output) ```  > to run the already exported model use `get_onnx_model()`  you can customize the whole pipeline as shown in the below code example:  ```python from fastT5 import (OnnxT5  get_onnx_runtime_sessions                      generate_onnx_representation  quantize) from transformers import AutoTokenizer  model_or_model_path = 't5-small'  #: Step 1. convert huggingfaces t5 model to onnx onnx_model_paths = generate_onnx_representation(model_or_model_path)  #: Step 2. (recommended) quantize the converted model for fast inference and to reduce model size. quant_model_paths = quantize(onnx_model_paths)  #: step 3. setup onnx runtime model_sessions = get_onnx_runtime_sessions(quant_model_paths)  #: step 4. get the onnx model model = OnnxT5(model_or_model_path  model_sessions)                        ... ```   - Contact me at kiranr8k@gmail.com - If appropriate  [open an issue](https://github.com/Ki6an/fastT5/issues/new/choose) on GitHub   """;Natural Language Processing;https://github.com/Ki6an/fastT5
"""This repository is an academical work on a new subject introduced by google researchers called:  Transformer for Image classification at scale. We worked at Georgia Tech Lorraine with the DREAM research team  a robotic laboratory  in in order to test this new image classification technique on a diatom dataset.  This technique called Vision Transformer was published in the folowing paper:  [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929).  Overview of the model given by Google: we split an image into fixed-size patches  linearly embed each of them  add position embeddings  and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification  we use the standard approach of adding an extra learnable ""classification token"" to the sequence.   Make sure you have `Python>=3.6` installed on your machine.  ‚Üí Install venv package: ``` apt-get install python3-venv ``` ‚Üí Create jax-ViT venv:   ``` python3 -m venv venv/jax-ViT ``` ‚Üí Activate venv:  ``` source /venv/jax-ViT/bin/activate ``` ‚Üí Upgrade pip before installing required package:  ``` python -m pip install --upgrade pip ``` ‚Üí Install required package for jax-ViT into the venv: ```  pip3 install -r vit_jax/requirements.txt ``` ‚Üí Install jax-GPU version:  ``` pip install --upgrade jax jaxlib==0.1.61+cuda110 -f https://storage.googleapis.com/jax-releases/jax_releases.html ``` (‚Äúcuda110‚Äù ‚Üí means cuda v.11.0: change this according to the cuda version in your computer) ‚Üí Clone Github code:  ``` git clone https://github.com/Thanusan19/Vision_Transformer.git ```  For more details on Jax  please check the [Jax GitHub repository](https://github.com/google/jax) Note that installation instructions for GPU differs slightly from the instructions for CPU.    You can find all these models in the following storage bucket:   Download one of the pre-trained model with the following command:   We ended having multiple branches depending on the use. The first one corresponds to our initial ViT implementation changes  and is capable of training on the diatom dataset and has data augmentation capabilities. The second one is ""cnn_model"" branch  which was used to test simple convolution and PCA based feature extractor. Finally the third one is ""resnet_vit"" branch which was used to test the resnet50 model as feature extractor.  Here are the general settings to check in the various implementations: - First make sure that `FINE_TUNE = True` in order to do fine-tuning - Set the following parameter in order to enable inference: `INFERENCE = True`. Also set the checkpoint's filepath  in: `params = checkpoint.load('../models/model_diatom_final_checkpoints.npz')` - If you want to train without the fine-tuned weights  use: `LOAD_FINE_TUNNED_CHECKPOINTS = False`. Also set the checkpoint's filepath: `checkpoints_file_path = ""../models/model_diatom_checkpoint_step_6000_with_data_aug.npz""` - Test a saved checkpoint accuracy by setting the following parameter: `CHECKPOINTS_TEST = True`. Also set the checkpoint's filepath: `  checkpoints_file_path = ""../models/model_diatom_final_checkpoints.npz""` - Choose the ViT model to train on with the `model` parameter. The basic model to use could be `model = 'ViT-B_16'`. See branch specific instructions for more details.  - Choose the dataset to load  e.g: `DATASET = 2`. Choose the dataset between:   - `0` for CIFAR-10    - `1` for dog and cats    - `2` for the diatom dataset. - Set the batch size and epochs according to your training resources and needs. We recommend the following parameters:   ```python   epochs = 100   batch_size = 256   warmup_steps = 5   decay_type = 'cosine'   grad_norm_clip = 1   accum_steps = 64  #: 64--> GPU3  #:8--> TPU   base_lr = 0.03 #: base learning rate   ``` - If you want to use data augmentation during training  change the `doDataAugmentation` parameter inside the corresponding call to the python data generator `MyDogsCats()`: `doDataAugmentation=True`. We recommend not using data augmentation on the train and validation sets.  Here is an example of parameters we recommend to use if you want to do fine-tuning with untrained fine-tuning weights  on the diatom dataset: ```python INFERENCE = False FINE_TUNE = True LOAD_FINE_TUNNED_CHECKPOINTS = False CHECKPOINTS_TEST = False DATASET = 2 #:to load diatom dataset batch_size = 256 #:can be set to 512 for no data augmentation and simple ViT model fine-tuning epochs = 100 warmup_steps = 5 decay_type = 'cosine' grad_norm_clip = 1 accum_steps = 64  #: 64--> GPU3  #:8--> TPU base_lr = 0.03 #:base learning rate ```  Once you have checked the specific recommendation for the specific branch  you can launch the training using: ``` cd vit_jax/ python vit_jax.py ``` __NB:__ Make sure you have activated the virtual environment before launching. (e.g. `source /venv/jax-ViT/bin/activate`)   """;Computer Vision;https://github.com/Thanusan19/Vision_Transformer
"""""";Computer Vision;https://github.com/YanYan0716/vision_transform
"""""";Audio;https://github.com/acids-ircam/diffusion_models
"""To practice what you've learned  a good idea would be to spend an hour on 3 of the following (3-hours total  you could through them all if you want) and then write a blog post about what you've learned.  * For an overview of the different problems within NLP and how to solve them read through:    * [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)   * [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e) * Go through [MIT's Recurrent Neural Networks lecture](https://youtu.be/SEnXr6v2ifU). This will be one of the greatest additions to what's happening behind the RNN model's you've been building. * Read through the [word embeddings page on the TensorFlow website](https://www.tensorflow.org/tutorials/text/word_embeddings). Embeddings are such a large part of NLP. We've covered them throughout this notebook but extra practice would be well worth it. A good exercise would be to write out all the code in the guide in a new notebook.  * For more on RNN's in TensorFlow  read and reproduce [the TensorFlow RNN guide](https://www.tensorflow.org/guide/keras/rnn). We've covered many of the concepts in this guide  but it's worth writing the code again for yourself. * Text data doesn't always come in a nice package like the data we've downloaded. So if you're after more on preparing different text sources for being with your TensorFlow deep learning models  it's worth checking out the following:   * [TensorFlow text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text).   * [Reading text files with Python](https://realpython.com/read-write-files-python/) by Real Python. * This notebook has focused on writing NLP code. For a mathematically rich overview of how NLP with Deep Learning happens  read [Standford's Natural Language Processing with Deep Learning lecture notes Part 1](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).     * For an even deeper dive  you could even do the whole [CS224n](http://web.stanford.edu/class/cs224n/) (Natural Language Processing with Deep Learning) course.  * Great blog posts to read:   * Andrei Karpathy's [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) dives into generating Shakespeare text with RNNs.   * [Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) by Mauro Di Pietro. An overview of different techniques for turning text into numbers and then classifying it.   * [What are word embeddings?](https://machinelearningmastery.com/what-are-word-embeddings/) by Machine Learning Mastery. * Other topics worth looking into:   * [Attention mechanisms](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/). These are a foundational component of the transformer architecture and also often add improvments to deep NLP models.   * [Transformer architectures](http://jalammar.github.io/illustrated-transformer/). This model architecture has recently taken the NLP world by storm  achieving state of the art on many benchmarks. However  it does take a little more processing to get off the ground  the [HuggingFace Models (formerly HuggingFace Transformers) library](https://huggingface.co/models/) is probably your best quick start.  ---   1. Rebuild  compile and train `model_1`  `model_2` and `model_5` using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) instead of the Functional API. 2. Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data? 3. Try fine-tuning the TF Hub Universal Sentence Encoder model by setting `training=True` when instantiating it as a Keras layer.  ``` #: We can use this encoding layer in place of our text_vectorizer and embedding layer sentence_encoder_layer = hub.KerasLayer(""https://tfhub.dev/google/universal-sentence-encoder/4""                                          input_shape=[]                                          dtype=tf.string                                          trainable=True) #: turn training on to fine-tune the TensorFlow Hub model ``` 4. Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the `sample_submission.csv` file from Kaggle (see the Files tab in Colab for what the `sample_submission.csv` file looks like). Once you've done this  [make a submission to the Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/data)  how did your model perform? 5. Combine the ensemble predictions using the majority vote (mode)  how does this perform compare to averaging the prediction probabilities of each model? 6. Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels.   * 02 Dec 2021 - Added fix for TensorFlow 2.7.0+ for notebook 02  [see discussion for more](https://github.com/mrdbourke/tensorflow-deep-learning/discussions/278) * 11 Nov 2021 - Added fix for TensorFlow 2.7.0+ for notebook 01   [see discussion for more](https://github.com/mrdbourke/tensorflow-deep-learning/discussions/256) * 14 Aug 2021 - Added a [discussion with TensorFlow 2.6 updates and EfficientNetV2 notes](https://github.com/mrdbourke/tensorflow-deep-learning/discussions/166)    Note: You can get all of the notebook code created during the videos in the video_notebooks directory.   It is taught with the following mantra:   If you're training for longer  you probably want to reduce the learning rate as you go... the closer you get to the bottom of the hill  the smaller steps you want to take. Imagine it like finding a coin at the bottom of your couch. In the beginning your arm movements are going to be large and the closer you get  the smaller your movements become.   Jordan Kern  watching these will take you from 0 to 1 with time series problems:    If you'd like some extra materials to go through to further your skills with TensorFlow and deep learning in general or to prepare more for the exam  I'd highly recommend the following:   12 May 2021 - all videos for 09 have now been released on Udemy & ZTM!!! enjoy build SkimLit üìÑüî•   To prevent the course from being 100+ hours (deep learning is a broad field)  various external resources for different sections are recommended to puruse under your own discrestion.  You can find solutions to the exercises in [`extras/solutions/`](https://github.com/mrdbourke/tensorflow-deep-learning/tree/main/extras/solutions)  there's a notebook per set of exercises (one for 00  01  02... etc). Thank you to [Ashik Shafi](https://github.com/ashikshafi08) for all of the efforts creating these.  ---   1. Create a vector  scalar  matrix and tensor with values of your choosing using `tf.constant()`. 2. Find the shape  rank and size of the tensors you created in 1. 3. Create two tensors containing random values between 0 and 1 with shape `[5  300]`. 4. Multiply the two tensors you created in 3 using matrix multiplication. 5. Multiply the two tensors you created in 3 using dot product. 6. Create a tensor with random values between 0 and 1 with shape `[224  224  3]`. 7. Find the min and max values of the tensor you created in 6 along the first axis. 8. Created a tensor with random values of shape `[1  224  224  3]` then squeeze it to change the shape to `[224  224  3]`. 9. Create a tensor with shape `[10]` using your own choice of values  then find the index which has the maximum value. 10. One-hot encode the tensor you created in 9.   1. Create your own regression dataset (or make the one we created in ""Create data to view and fit"" bigger) and build fit a model to it. 2. Try building a neural network with 4 Dense layers and fitting it to your own regression dataset  how does it perform? 3. Try and improve the results we got on the insurance dataset  some things you might want to try include:   * Building a larger model (how does one with 4 dense layers go?).   * Increasing the number of units in each layer.   * Lookup the documentation of [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) and find out what the first parameter is  what happens if you increase it by 10x?   * What happens if you train for longer (say 300 epochs instead of 200)?  4. Import the [Boston pricing dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing/load_data) from TensorFlow [`tf.keras.datasets`](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) and model it.   1. Play with neural networks in the [TensorFlow Playground](https://playground.tensorflow.org/) for 10-minutes. Especially try different values of the learning  what happens when you decrease it? What happens when you increase it? 2. Replicate the model pictured in the [TensorFlow Playground diagram](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=6 6 6 6 6&seed=0.51287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&discretize_hide=true&regularizationRate_hide=true&percTrainData_hide=true&dataset_hide=true&problem_hide=true&noise_hide=true&batchSize_hide=true) below using TensorFlow code. Compile it using the Adam optimizer  binary crossentropy loss and accuracy metric. Once it's compiled check a summary of the model. ![tensorflow playground example neural network](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-tensorflow-playground-replication-exercise.png) *Try this network out for yourself on the [TensorFlow Playground website](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=6 6 6 6 6&seed=0.51287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&discretize_hide=true&regularizationRate_hide=true&percTrainData_hide=true&dataset_hide=true&problem_hide=true&noise_hide=true&batchSize_hide=true). Hint: there are 5 hidden layers but the output layer isn't pictured  you'll have to decide what the output layer should be based on the input data.* 3. Create a classification dataset using Scikit-Learn's [`make_moons()`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function  visualize it and then build a model to fit it at over 85% accuracy. 4. Train a model to get 88%+ accuracy on the fashion MNIST test set. Plot a confusion matrix to see the results after. 5. Recreate [TensorFlow's](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function) in your own code. Make sure it can accept a tensor and return that tensor after having the softmax function applied to it. 6. Create a function (or write code) to visualize multiple image predictions for the fashion MNIST at the same time. Plot at least three different images and their prediciton labels at the same time. Hint: see the [classifcation tutorial in the TensorFlow documentation](https://www.tensorflow.org/tutorials/keras/classification) for ideas. 7. Make a function to show an image of a certain class of the fashion MNIST dataset and make a prediction on it. For example  plot 3 images of the `T-shirt` class with their predictions.   1. Spend 20-minutes reading and interacting with the [CNN explainer website](https://poloclub.github.io/cnn-explainer/).   * What are the key terms? e.g. explain convolution in your own words  pooling in your own words 2. Play around with the ""understanding hyperparameters"" section in the [CNN explainer](https://poloclub.github.io/cnn-explainer/) website for 10-minutes.   * What is the kernel size?   * What is the stride?    * How could you adjust each of these in TensorFlow code? 3. Take 10 photos of two different things and build your own CNN image classifier using the techniques we've built here. 4. Find an ideal learning rate for a simple convolutional neural network model on your the 10 class dataset.   1. Build and fit a model using the same data we have here but with the MobileNetV2 architecture feature extraction ([`mobilenet_v2_100_224/feature_vector`](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4)) from TensorFlow Hub  how does it perform compared to our other models? 2. Name 3 different image classification models on TensorFlow Hub that we haven't used. 3. Build a model to classify images of two different things you've taken photos of.   * You can use any feature extraction layer from TensorFlow Hub you like for this.   * You should aim to have at least 10 images of each class  for example to build a fridge versus oven classifier  you'll want 10 images of fridges and 10 images of ovens. 4. What is the current best performing model on ImageNet?   * Hint: you might want to check [sotabench.com](https://www.sotabench.com) for this.   1. Use feature-extraction to train a transfer learning model on 10% of the Food Vision data for 10 epochs using [`tf.keras.applications.EfficientNetB0`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB0) as the base model. Use the [`ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) callback to save the weights to file. 2. Fine-tune the last 20 layers of the base model you trained in 2 for another 10 epochs. How did it go? 3. Fine-tune the last 30 layers of the base model you trained in 2 for another 10 epochs. How did it go? 4. Write a function to visualize an image from any dataset (train or test file) and any class (e.g. ""steak""  ""pizza""... etc)  visualize it and make a prediction on it using a trained model.   1. Take 3 of your own photos of food and use the trained model to make predictions on them  share your predictions with the other students in Discord and show off your Food Vision model üçîüëÅ. 2. Train a feature-extraction transfer learning model for 10 epochs on the same data and compare its performance versus a model which used feature extraction for 5 epochs and fine-tuning for 5 epochs (like we've used in this notebook). Which method is better? 3. Recreate the first model (the feature extraction model) with [`mixed_precision`](https://www.tensorflow.org/guide/mixed_precision) turned on.    * Does it make the model train faster?    * Does it effect the accuracy or performance of our model?    * What's the advatanges of using `mixed_precision` training?   **Note:** The chief exercise for Milestone Project 1 is to finish the ""TODO"" sections in the [Milestone Project 1 Template notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/TEMPLATE_07_food_vision_milestone_project_1.ipynb). After doing so  move onto the following.  1. Use the same evaluation techniques on the large-scale Food Vision model as you did in the previous notebook ([Transfer Learning Part 3: Scaling up](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/06_transfer_learning_in_tensorflow_part_3_scaling_up.ipynb)). More specifically  it would be good to see:   * A confusion matrix between all of the model's predictions and true labels.   * A graph showing the f1-scores of each class.   * A visualization of the model making predictions on various images and comparing the predictions to the ground truth.     * For example  plot a sample image from the test dataset and have the title of the plot show the prediction  the prediction probability and the ground truth label.    * **Note:** To compare predicted labels to test labels  it might be a good idea when loading the test data to set `shuffle=False` (so the ordering of test data is preserved alongside the order of predicted labels). 2. Take 3 of your own photos of food and use the Food Vision model to make predictions on them. How does it go? Share your images/predictions with the other students. 3. Retrain the model (feature extraction and fine-tuning) we trained in this notebook  except this time use [`EfficientNetB4`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB4) as the base model instead of `EfficientNetB0`. Do you notice an improvement in performance? Does it take longer to train? Are there any tradeoffs to consider? 4. Name one important benefit of mixed precision training  how does this benefit take place?   1. Rebuild  compile and train `model_1`  `model_2` and `model_5` using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) instead of the Functional API. 2. Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data? 3. Try fine-tuning the TF Hub Universal Sentence Encoder model by setting `training=True` when instantiating it as a Keras layer.  ``` #: We can use this encoding layer in place of our text_vectorizer and embedding layer sentence_encoder_layer = hub.KerasLayer(""https://tfhub.dev/google/universal-sentence-encoder/4""                                          input_shape=[]                                          dtype=tf.string                                          trainable=True) #: turn training on to fine-tune the TensorFlow Hub model ``` 4. Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the `sample_submission.csv` file from Kaggle (see the Files tab in Colab for what the `sample_submission.csv` file looks like). Once you've done this  [make a submission to the Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/data)  how did your model perform? 5. Combine the ensemble predictions using the majority vote (mode)  how does this perform compare to averaging the prediction probabilities of each model? 6. Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels.   1. Train `model_5` on all of the data in the training dataset for as many epochs until it stops improving. Since this might take a while  you might want to use:   * [`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to save the model's best weights only.   * [`tf.keras.callbacks.EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) to stop the model from training once the validation loss has stopped improving for ~3 epochs. 2. Checkout the [Keras guide on using pretrained GloVe embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/). Can you get this working with one of our models?   * Hint: You'll want to incorporate it with a custom token [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.   * It's up to you whether or not you fine-tune the GloVe embeddings or leave them frozen. 3. Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained  embedding for the [TensorFlow Hub BERT PubMed expert](https://tfhub.dev/google/experts/bert/pubmed/2) (a language model pretrained on PubMed texts) pretrained embedding. Does this effect results?   * Note: Using the BERT PubMed expert pretrained embedding requires an extra preprocessing step for sequences (as detailed in the [TensorFlow Hub guide](https://tfhub.dev/google/experts/bert/pubmed/2)).   * Does the BERT model beat the results mentioned in this paper? https://arxiv.org/pdf/1710.06071.pdf  4. What happens if you were to merge our `line_number` and `total_lines` features for each sequence? For example  created a `X_of_Y` feature instead? Does this effect model performance?   * Another example: `line_number=1` and `total_lines=11` turns into `line_of_X=1_of_11`. 5. Write a function (or series of functions) to take a sample abstract string  preprocess it (in the same way our model has been trained)  make a prediction on each sequence in the abstract and return the abstract in the format:   * `PREDICTED_LABEL`: `SEQUENCE`   * `PREDICTED_LABEL`: `SEQUENCE`   * `PREDICTED_LABEL`: `SEQUENCE`   * `PREDICTED_LABEL`: `SEQUENCE`   * ...     * You can find your own unstrcutured RCT abstract from PubMed or try this one from: [*Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection*](https://pubmed.ncbi.nlm.nih.gov/22244707/).   1. Does scaling the data help for univariate/multivariate data? (e.g. getting all of the values between 0 & 1)    * Try doing this for a univariate model (e.g. `model_1`) and a multivariate model (e.g. `model_6`) and see if it effects model training or evaluation results. 2. Get the most up to date data on Bitcoin  train a model & see how it goes (our data goes up to May 18 2021).   * You can download the Bitcoin historical data for free from [coindesk.com/price/bitcoin](https://www.coindesk.com/price/bitcoin) and clicking ""Export Data"" -> ""CSV"". 3. For most of our models we used `WINDOW_SIZE=7`  but is there a better window size?   * Setup a series of experiments to find whether or not there's a better window size.   * For example  you might train 10 different models with `HORIZON=1` but with window sizes ranging from 2-12. 4. Create a windowed dataset just like the ones we used for `model_1` using [`tf.keras.preprocessing.timeseries_dataset_from_array()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array) and retrain `model_1` using the recreated dataset. 5. For our multivariate modelling experiment  we added the Bitcoin block reward size as an extra feature to make our time series multivariate.    * Are there any other features you think you could add?    * If so  try it out  how do these affect the model? 6. Make prediction intervals for future forecasts. To do so  one way would be to train an ensemble model on all of the data  make future forecasts with it and calculate the prediction intervals of the ensemble just like we did for `model_8`. 7. For future predictions  try to make a prediction  retrain a model on the predictions  make a prediction  retrain a model  make a prediction  retrain a model  make a prediction (retrain a model each time a new prediction is made). Plot the results  how do they look compared to the future predictions where a model wasn't retrained for every forecast (`model_9`)? 8. Throughout this notebook  we've only tried algorithms we've handcrafted ourselves. But it's worth seeing how a purpose built forecasting algorithm goes.    * Try out one of the extra algorithms listed in the modelling experiments part such as:     * [Facebook's Kats library](https://github.com/facebookresearch/Kats) - there are many models in here  remember the machine learning practioner's motto: experiment  experiment  experiment.     * [LinkedIn's Greykite library](https://github.com/linkedin/greykite)   **Preparing your brain** 1. Read through the [TensorFlow Developer Certificate Candidate Handbook](https://www.tensorflow.org/extras/cert/TF_Certificate_Candidate_Handbook.pdf). 2. Go through the Skills checklist section of the TensorFlow Developer Certification Candidate Handbook and create a notebook which covers all of the skills required  write code for each of these (this notebook can be used as a point of reference during the exam).  ![mapping the TensorFlow Developer handbook to code in a notebook](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/11-map-the-skills-checklist-to-a-notebook.png) *Example of mapping the Skills checklist section of the TensorFlow Developer Certification Candidate handbook to a notebook.*  **Prearing your computer** 1. Go through the [PyCharm quick start](https://www.jetbrains.com/pycharm/learning-center/) tutorials to make sure you're familiar with PyCharm (the exam uses PyCharm  you can download the free version). 2. Read through and follow the suggested steps in the [setting up for the TensorFlow Developer Certificate Exam guide](https://www.tensorflow.org/extras/cert/Setting_Up_TF_Developer_Certificate_Exam.pdf). 3. After going through (2)  go into PyCharm and make sure you can train a model in TensorFlow. The model and dataset in the example `image_classification_test.py` [script on GitHub](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_classification_test.py) should be enough. If you can train and save the model in under 5-10 minutes  your computer will be powerful enough to train the models in the exam.     - Make sure you've got experience running models locally in PyCharm before taking the exam. Google Colab (what we used through the course) is a little different to PyCharm.  ![before taking the TensorFlow Developer certification exam  make sure you can run TensorFlow code in PyCharm on your local machine](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/11-getting-example-script-to-run-in-pycharm.png) *Before taking the exam make sure you can run TensorFlow code on your local machine in PyCharm. If the [example `image_class_test.py` script](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_classification_test.py) can run completely in under 5-10 minutes on your local machine  your local machine can handle the exam (if not  you can use Google Colab to train  save and download models to submit for the exam).*   * [Neural Networks and Deep Learning Book](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen - If the Zero to Mastery TensorFlow for Deep Learning course is top down  this book is bottom up. A fantastic resource to sandwich your knowledge.  * [Deeplearning.AI specializations](https://www.deeplearning.ai) - This course focuses on code-first  the deeplearning.ai specializations will teach you what's going on behind the code. * [Hands-on Machine Learning with Scikit-Learn  Keras and TensorFlow Book](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) (especially the 2nd half) - Many of the materials in this course were inspired by and guided by the pages of this beautiful text book. * [Full Stack Deep Learning](https://fullstackdeeplearning.com) - Learn how to turn your models into machine learning-powered applications. * [Made with ML MLOps materials](https://madewithml.com/#mlops) - Similar to Full Stack Deep Learning but comprised into many small lessons around all the pieces of the puzzle (data collection  labelling  deployment and more) required to build a full-stack machine learning-powered application. * [fast.ai Curriculum](https://www.fast.ai) - One of the best (and free) AI/deep learning courses online. Enough said. * [""How does a beginner data scientist like me gain experience?""](https://www.mrdbourke.com/how-can-a-beginner-data-scientist-like-me-gain-experience/) by Daniel Bourke - Read this on how to get experience for a job after studying online/at unveristy (start the job before you have it).   """;General;https://github.com/mrdbourke/tensorflow-deep-learning
"""To practice what you've learned  a good idea would be to spend an hour on 3 of the following (3-hours total  you could through them all if you want) and then write a blog post about what you've learned.  * For an overview of the different problems within NLP and how to solve them read through:    * [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)   * [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e) * Go through [MIT's Recurrent Neural Networks lecture](https://youtu.be/SEnXr6v2ifU). This will be one of the greatest additions to what's happening behind the RNN model's you've been building. * Read through the [word embeddings page on the TensorFlow website](https://www.tensorflow.org/tutorials/text/word_embeddings). Embeddings are such a large part of NLP. We've covered them throughout this notebook but extra practice would be well worth it. A good exercise would be to write out all the code in the guide in a new notebook.  * For more on RNN's in TensorFlow  read and reproduce [the TensorFlow RNN guide](https://www.tensorflow.org/guide/keras/rnn). We've covered many of the concepts in this guide  but it's worth writing the code again for yourself. * Text data doesn't always come in a nice package like the data we've downloaded. So if you're after more on preparing different text sources for being with your TensorFlow deep learning models  it's worth checking out the following:   * [TensorFlow text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text).   * [Reading text files with Python](https://realpython.com/read-write-files-python/) by Real Python. * This notebook has focused on writing NLP code. For a mathematically rich overview of how NLP with Deep Learning happens  read [Standford's Natural Language Processing with Deep Learning lecture notes Part 1](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).     * For an even deeper dive  you could even do the whole [CS224n](http://web.stanford.edu/class/cs224n/) (Natural Language Processing with Deep Learning) course.  * Great blog posts to read:   * Andrei Karpathy's [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) dives into generating Shakespeare text with RNNs.   * [Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) by Mauro Di Pietro. An overview of different techniques for turning text into numbers and then classifying it.   * [What are word embeddings?](https://machinelearningmastery.com/what-are-word-embeddings/) by Machine Learning Mastery. * Other topics worth looking into:   * [Attention mechanisms](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/). These are a foundational component of the transformer architecture and also often add improvments to deep NLP models.   * [Transformer architectures](http://jalammar.github.io/illustrated-transformer/). This model architecture has recently taken the NLP world by storm  achieving state of the art on many benchmarks. However  it does take a little more processing to get off the ground  the [HuggingFace Models (formerly HuggingFace Transformers) library](https://huggingface.co/models/) is probably your best quick start.  ---   1. Rebuild  compile and train `model_1`  `model_2` and `model_5` using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) instead of the Functional API. 2. Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data? 3. Try fine-tuning the TF Hub Universal Sentence Encoder model by setting `training=True` when instantiating it as a Keras layer.  ``` #: We can use this encoding layer in place of our text_vectorizer and embedding layer sentence_encoder_layer = hub.KerasLayer(""https://tfhub.dev/google/universal-sentence-encoder/4""                                          input_shape=[]                                          dtype=tf.string                                          trainable=True) #: turn training on to fine-tune the TensorFlow Hub model ``` 4. Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the `sample_submission.csv` file from Kaggle (see the Files tab in Colab for what the `sample_submission.csv` file looks like). Once you've done this  [make a submission to the Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/data)  how did your model perform? 5. Combine the ensemble predictions using the majority vote (mode)  how does this perform compare to averaging the prediction probabilities of each model? 6. Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels.   * 02 Dec 2021 - Added fix for TensorFlow 2.7.0+ for notebook 02  [see discussion for more](https://github.com/mrdbourke/tensorflow-deep-learning/discussions/278) * 11 Nov 2021 - Added fix for TensorFlow 2.7.0+ for notebook 01   [see discussion for more](https://github.com/mrdbourke/tensorflow-deep-learning/discussions/256) * 14 Aug 2021 - Added a [discussion with TensorFlow 2.6 updates and EfficientNetV2 notes](https://github.com/mrdbourke/tensorflow-deep-learning/discussions/166)    Note: You can get all of the notebook code created during the videos in the video_notebooks directory.   It is taught with the following mantra:   If you're training for longer  you probably want to reduce the learning rate as you go... the closer you get to the bottom of the hill  the smaller steps you want to take. Imagine it like finding a coin at the bottom of your couch. In the beginning your arm movements are going to be large and the closer you get  the smaller your movements become.   Jordan Kern  watching these will take you from 0 to 1 with time series problems:    If you'd like some extra materials to go through to further your skills with TensorFlow and deep learning in general or to prepare more for the exam  I'd highly recommend the following:   12 May 2021 - all videos for 09 have now been released on Udemy & ZTM!!! enjoy build SkimLit üìÑüî•   To prevent the course from being 100+ hours (deep learning is a broad field)  various external resources for different sections are recommended to puruse under your own discrestion.  You can find solutions to the exercises in [`extras/solutions/`](https://github.com/mrdbourke/tensorflow-deep-learning/tree/main/extras/solutions)  there's a notebook per set of exercises (one for 00  01  02... etc). Thank you to [Ashik Shafi](https://github.com/ashikshafi08) for all of the efforts creating these.  ---   1. Create a vector  scalar  matrix and tensor with values of your choosing using `tf.constant()`. 2. Find the shape  rank and size of the tensors you created in 1. 3. Create two tensors containing random values between 0 and 1 with shape `[5  300]`. 4. Multiply the two tensors you created in 3 using matrix multiplication. 5. Multiply the two tensors you created in 3 using dot product. 6. Create a tensor with random values between 0 and 1 with shape `[224  224  3]`. 7. Find the min and max values of the tensor you created in 6 along the first axis. 8. Created a tensor with random values of shape `[1  224  224  3]` then squeeze it to change the shape to `[224  224  3]`. 9. Create a tensor with shape `[10]` using your own choice of values  then find the index which has the maximum value. 10. One-hot encode the tensor you created in 9.   1. Create your own regression dataset (or make the one we created in ""Create data to view and fit"" bigger) and build fit a model to it. 2. Try building a neural network with 4 Dense layers and fitting it to your own regression dataset  how does it perform? 3. Try and improve the results we got on the insurance dataset  some things you might want to try include:   * Building a larger model (how does one with 4 dense layers go?).   * Increasing the number of units in each layer.   * Lookup the documentation of [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) and find out what the first parameter is  what happens if you increase it by 10x?   * What happens if you train for longer (say 300 epochs instead of 200)?  4. Import the [Boston pricing dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing/load_data) from TensorFlow [`tf.keras.datasets`](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) and model it.   1. Play with neural networks in the [TensorFlow Playground](https://playground.tensorflow.org/) for 10-minutes. Especially try different values of the learning  what happens when you decrease it? What happens when you increase it? 2. Replicate the model pictured in the [TensorFlow Playground diagram](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=6 6 6 6 6&seed=0.51287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&discretize_hide=true&regularizationRate_hide=true&percTrainData_hide=true&dataset_hide=true&problem_hide=true&noise_hide=true&batchSize_hide=true) below using TensorFlow code. Compile it using the Adam optimizer  binary crossentropy loss and accuracy metric. Once it's compiled check a summary of the model. ![tensorflow playground example neural network](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-tensorflow-playground-replication-exercise.png) *Try this network out for yourself on the [TensorFlow Playground website](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=6 6 6 6 6&seed=0.51287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&discretize_hide=true&regularizationRate_hide=true&percTrainData_hide=true&dataset_hide=true&problem_hide=true&noise_hide=true&batchSize_hide=true). Hint: there are 5 hidden layers but the output layer isn't pictured  you'll have to decide what the output layer should be based on the input data.* 3. Create a classification dataset using Scikit-Learn's [`make_moons()`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function  visualize it and then build a model to fit it at over 85% accuracy. 4. Train a model to get 88%+ accuracy on the fashion MNIST test set. Plot a confusion matrix to see the results after. 5. Recreate [TensorFlow's](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function) in your own code. Make sure it can accept a tensor and return that tensor after having the softmax function applied to it. 6. Create a function (or write code) to visualize multiple image predictions for the fashion MNIST at the same time. Plot at least three different images and their prediciton labels at the same time. Hint: see the [classifcation tutorial in the TensorFlow documentation](https://www.tensorflow.org/tutorials/keras/classification) for ideas. 7. Make a function to show an image of a certain class of the fashion MNIST dataset and make a prediction on it. For example  plot 3 images of the `T-shirt` class with their predictions.   1. Spend 20-minutes reading and interacting with the [CNN explainer website](https://poloclub.github.io/cnn-explainer/).   * What are the key terms? e.g. explain convolution in your own words  pooling in your own words 2. Play around with the ""understanding hyperparameters"" section in the [CNN explainer](https://poloclub.github.io/cnn-explainer/) website for 10-minutes.   * What is the kernel size?   * What is the stride?    * How could you adjust each of these in TensorFlow code? 3. Take 10 photos of two different things and build your own CNN image classifier using the techniques we've built here. 4. Find an ideal learning rate for a simple convolutional neural network model on your the 10 class dataset.   1. Build and fit a model using the same data we have here but with the MobileNetV2 architecture feature extraction ([`mobilenet_v2_100_224/feature_vector`](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4)) from TensorFlow Hub  how does it perform compared to our other models? 2. Name 3 different image classification models on TensorFlow Hub that we haven't used. 3. Build a model to classify images of two different things you've taken photos of.   * You can use any feature extraction layer from TensorFlow Hub you like for this.   * You should aim to have at least 10 images of each class  for example to build a fridge versus oven classifier  you'll want 10 images of fridges and 10 images of ovens. 4. What is the current best performing model on ImageNet?   * Hint: you might want to check [sotabench.com](https://www.sotabench.com) for this.   1. Use feature-extraction to train a transfer learning model on 10% of the Food Vision data for 10 epochs using [`tf.keras.applications.EfficientNetB0`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB0) as the base model. Use the [`ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) callback to save the weights to file. 2. Fine-tune the last 20 layers of the base model you trained in 2 for another 10 epochs. How did it go? 3. Fine-tune the last 30 layers of the base model you trained in 2 for another 10 epochs. How did it go? 4. Write a function to visualize an image from any dataset (train or test file) and any class (e.g. ""steak""  ""pizza""... etc)  visualize it and make a prediction on it using a trained model.   1. Take 3 of your own photos of food and use the trained model to make predictions on them  share your predictions with the other students in Discord and show off your Food Vision model üçîüëÅ. 2. Train a feature-extraction transfer learning model for 10 epochs on the same data and compare its performance versus a model which used feature extraction for 5 epochs and fine-tuning for 5 epochs (like we've used in this notebook). Which method is better? 3. Recreate the first model (the feature extraction model) with [`mixed_precision`](https://www.tensorflow.org/guide/mixed_precision) turned on.    * Does it make the model train faster?    * Does it effect the accuracy or performance of our model?    * What's the advatanges of using `mixed_precision` training?   **Note:** The chief exercise for Milestone Project 1 is to finish the ""TODO"" sections in the [Milestone Project 1 Template notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/TEMPLATE_07_food_vision_milestone_project_1.ipynb). After doing so  move onto the following.  1. Use the same evaluation techniques on the large-scale Food Vision model as you did in the previous notebook ([Transfer Learning Part 3: Scaling up](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/06_transfer_learning_in_tensorflow_part_3_scaling_up.ipynb)). More specifically  it would be good to see:   * A confusion matrix between all of the model's predictions and true labels.   * A graph showing the f1-scores of each class.   * A visualization of the model making predictions on various images and comparing the predictions to the ground truth.     * For example  plot a sample image from the test dataset and have the title of the plot show the prediction  the prediction probability and the ground truth label.    * **Note:** To compare predicted labels to test labels  it might be a good idea when loading the test data to set `shuffle=False` (so the ordering of test data is preserved alongside the order of predicted labels). 2. Take 3 of your own photos of food and use the Food Vision model to make predictions on them. How does it go? Share your images/predictions with the other students. 3. Retrain the model (feature extraction and fine-tuning) we trained in this notebook  except this time use [`EfficientNetB4`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB4) as the base model instead of `EfficientNetB0`. Do you notice an improvement in performance? Does it take longer to train? Are there any tradeoffs to consider? 4. Name one important benefit of mixed precision training  how does this benefit take place?   1. Rebuild  compile and train `model_1`  `model_2` and `model_5` using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) instead of the Functional API. 2. Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data? 3. Try fine-tuning the TF Hub Universal Sentence Encoder model by setting `training=True` when instantiating it as a Keras layer.  ``` #: We can use this encoding layer in place of our text_vectorizer and embedding layer sentence_encoder_layer = hub.KerasLayer(""https://tfhub.dev/google/universal-sentence-encoder/4""                                          input_shape=[]                                          dtype=tf.string                                          trainable=True) #: turn training on to fine-tune the TensorFlow Hub model ``` 4. Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the `sample_submission.csv` file from Kaggle (see the Files tab in Colab for what the `sample_submission.csv` file looks like). Once you've done this  [make a submission to the Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/data)  how did your model perform? 5. Combine the ensemble predictions using the majority vote (mode)  how does this perform compare to averaging the prediction probabilities of each model? 6. Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels.   1. Train `model_5` on all of the data in the training dataset for as many epochs until it stops improving. Since this might take a while  you might want to use:   * [`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to save the model's best weights only.   * [`tf.keras.callbacks.EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) to stop the model from training once the validation loss has stopped improving for ~3 epochs. 2. Checkout the [Keras guide on using pretrained GloVe embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/). Can you get this working with one of our models?   * Hint: You'll want to incorporate it with a custom token [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.   * It's up to you whether or not you fine-tune the GloVe embeddings or leave them frozen. 3. Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained  embedding for the [TensorFlow Hub BERT PubMed expert](https://tfhub.dev/google/experts/bert/pubmed/2) (a language model pretrained on PubMed texts) pretrained embedding. Does this effect results?   * Note: Using the BERT PubMed expert pretrained embedding requires an extra preprocessing step for sequences (as detailed in the [TensorFlow Hub guide](https://tfhub.dev/google/experts/bert/pubmed/2)).   * Does the BERT model beat the results mentioned in this paper? https://arxiv.org/pdf/1710.06071.pdf  4. What happens if you were to merge our `line_number` and `total_lines` features for each sequence? For example  created a `X_of_Y` feature instead? Does this effect model performance?   * Another example: `line_number=1` and `total_lines=11` turns into `line_of_X=1_of_11`. 5. Write a function (or series of functions) to take a sample abstract string  preprocess it (in the same way our model has been trained)  make a prediction on each sequence in the abstract and return the abstract in the format:   * `PREDICTED_LABEL`: `SEQUENCE`   * `PREDICTED_LABEL`: `SEQUENCE`   * `PREDICTED_LABEL`: `SEQUENCE`   * `PREDICTED_LABEL`: `SEQUENCE`   * ...     * You can find your own unstrcutured RCT abstract from PubMed or try this one from: [*Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection*](https://pubmed.ncbi.nlm.nih.gov/22244707/).   1. Does scaling the data help for univariate/multivariate data? (e.g. getting all of the values between 0 & 1)    * Try doing this for a univariate model (e.g. `model_1`) and a multivariate model (e.g. `model_6`) and see if it effects model training or evaluation results. 2. Get the most up to date data on Bitcoin  train a model & see how it goes (our data goes up to May 18 2021).   * You can download the Bitcoin historical data for free from [coindesk.com/price/bitcoin](https://www.coindesk.com/price/bitcoin) and clicking ""Export Data"" -> ""CSV"". 3. For most of our models we used `WINDOW_SIZE=7`  but is there a better window size?   * Setup a series of experiments to find whether or not there's a better window size.   * For example  you might train 10 different models with `HORIZON=1` but with window sizes ranging from 2-12. 4. Create a windowed dataset just like the ones we used for `model_1` using [`tf.keras.preprocessing.timeseries_dataset_from_array()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array) and retrain `model_1` using the recreated dataset. 5. For our multivariate modelling experiment  we added the Bitcoin block reward size as an extra feature to make our time series multivariate.    * Are there any other features you think you could add?    * If so  try it out  how do these affect the model? 6. Make prediction intervals for future forecasts. To do so  one way would be to train an ensemble model on all of the data  make future forecasts with it and calculate the prediction intervals of the ensemble just like we did for `model_8`. 7. For future predictions  try to make a prediction  retrain a model on the predictions  make a prediction  retrain a model  make a prediction  retrain a model  make a prediction (retrain a model each time a new prediction is made). Plot the results  how do they look compared to the future predictions where a model wasn't retrained for every forecast (`model_9`)? 8. Throughout this notebook  we've only tried algorithms we've handcrafted ourselves. But it's worth seeing how a purpose built forecasting algorithm goes.    * Try out one of the extra algorithms listed in the modelling experiments part such as:     * [Facebook's Kats library](https://github.com/facebookresearch/Kats) - there are many models in here  remember the machine learning practioner's motto: experiment  experiment  experiment.     * [LinkedIn's Greykite library](https://github.com/linkedin/greykite)   **Preparing your brain** 1. Read through the [TensorFlow Developer Certificate Candidate Handbook](https://www.tensorflow.org/extras/cert/TF_Certificate_Candidate_Handbook.pdf). 2. Go through the Skills checklist section of the TensorFlow Developer Certification Candidate Handbook and create a notebook which covers all of the skills required  write code for each of these (this notebook can be used as a point of reference during the exam).  ![mapping the TensorFlow Developer handbook to code in a notebook](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/11-map-the-skills-checklist-to-a-notebook.png) *Example of mapping the Skills checklist section of the TensorFlow Developer Certification Candidate handbook to a notebook.*  **Prearing your computer** 1. Go through the [PyCharm quick start](https://www.jetbrains.com/pycharm/learning-center/) tutorials to make sure you're familiar with PyCharm (the exam uses PyCharm  you can download the free version). 2. Read through and follow the suggested steps in the [setting up for the TensorFlow Developer Certificate Exam guide](https://www.tensorflow.org/extras/cert/Setting_Up_TF_Developer_Certificate_Exam.pdf). 3. After going through (2)  go into PyCharm and make sure you can train a model in TensorFlow. The model and dataset in the example `image_classification_test.py` [script on GitHub](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_classification_test.py) should be enough. If you can train and save the model in under 5-10 minutes  your computer will be powerful enough to train the models in the exam.     - Make sure you've got experience running models locally in PyCharm before taking the exam. Google Colab (what we used through the course) is a little different to PyCharm.  ![before taking the TensorFlow Developer certification exam  make sure you can run TensorFlow code in PyCharm on your local machine](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/11-getting-example-script-to-run-in-pycharm.png) *Before taking the exam make sure you can run TensorFlow code on your local machine in PyCharm. If the [example `image_class_test.py` script](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_classification_test.py) can run completely in under 5-10 minutes on your local machine  your local machine can handle the exam (if not  you can use Google Colab to train  save and download models to submit for the exam).*   * [Neural Networks and Deep Learning Book](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen - If the Zero to Mastery TensorFlow for Deep Learning course is top down  this book is bottom up. A fantastic resource to sandwich your knowledge.  * [Deeplearning.AI specializations](https://www.deeplearning.ai) - This course focuses on code-first  the deeplearning.ai specializations will teach you what's going on behind the code. * [Hands-on Machine Learning with Scikit-Learn  Keras and TensorFlow Book](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) (especially the 2nd half) - Many of the materials in this course were inspired by and guided by the pages of this beautiful text book. * [Full Stack Deep Learning](https://fullstackdeeplearning.com) - Learn how to turn your models into machine learning-powered applications. * [Made with ML MLOps materials](https://madewithml.com/#mlops) - Similar to Full Stack Deep Learning but comprised into many small lessons around all the pieces of the puzzle (data collection  labelling  deployment and more) required to build a full-stack machine learning-powered application. * [fast.ai Curriculum](https://www.fast.ai) - One of the best (and free) AI/deep learning courses online. Enough said. * [""How does a beginner data scientist like me gain experience?""](https://www.mrdbourke.com/how-can-a-beginner-data-scientist-like-me-gain-experience/) by Daniel Bourke - Read this on how to get experience for a job after studying online/at unveristy (start the job before you have it).   """;Natural Language Processing;https://github.com/mrdbourke/tensorflow-deep-learning
"""To install  run the following command: ``` conda env create --file environment.yml --prefix ./env conda activate ./env ```  Note: the tensorboard requirement is crucial  because otherwise upfirdn2d will not compile for some magical reason. The repo should work both on Linux/MacOS and Windows machines. However  on Windows  there might arise difficulties with installing some requirements: please see [#3](https://github.com/universome/alis/issues/3) to troubleshoot. Also  since the current repo is heavily based on StyleGAN2-ADA  it might be helpful to check [the original installation requirements](https://github.com/NVlabs/stylegan2-ada-pytorch#requirements).    We will release a filtered version soon.   """;Computer Vision;https://github.com/universome/alis
"""Clone the current repository and required submodules:  ```bash git clone https://github.com/GT-RIPL/robo-vln cd robo-vln    export robovln_rootdir=$PWD      git submodule init  git submodule update ```   """;Reinforcement Learning;https://github.com/GT-RIPL/robo-vln
"""""";Computer Vision;https://github.com/ludics/ViT-Retri
"""* Install tesnorflow (skip this step if it's already installed test environment:tensorflow 2.4.0) * Install dependencies:  `pip install -r requirements.txt`   git clone https://github.com/wangermeng2021/FastClassification.git    cd FastClassification   """;Computer Vision;https://github.com/wangermeng2021/FastClassification
"""""";Computer Vision;https://github.com/houstonsantos/CassavaLeafDisease
"""``` python   You should install ü§ó Transformers in a virtual environment. If you're unfamiliar with Python virtual environments  check out the user guide.  First  create a virtual environment with the version of Python you're going to use and activate it.  Then  you will need to install at least one of Flax  PyTorch or TensorFlow.  Please refer to TensorFlow installation page  PyTorch installation page and/or Flax installation page regarding the specific install command for your platform.  When one of those backends has been installed  ü§ó Transformers can be installed using pip as follows:  pip install transformers  If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release  you must install the library from source.  Since Transformers version v4.0.0  we now have a conda channel: huggingface.  ü§ó Transformers can be installed using conda as follows:   conda install -c huggingface transformers  Follow the installation pages of Flax  PyTorch or TensorFlow to see how to install them with conda.   You can test most of our models directly on their pages from the [model hub](https://huggingface.co/models). We also offer [private model hosting  versioning  & an inference API](https://huggingface.co/pricing) for public and private models.  Here are a few examples:   In Natural Language Processing: - [Masked word completion with BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France) - [Name Entity Recognition with Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city) - [Text generation with GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+) - [Natural Language Inference with RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal) - [Summarization with BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct) - [Question answering with DistilBERT](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species) - [Translation with T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)  In Computer Vision: - [Image classification with ViT](https://huggingface.co/google/vit-base-patch16-224) - [Object Detection with DETR](https://huggingface.co/facebook/detr-resnet-50) - [Image Segmentation with DETR](https://huggingface.co/facebook/detr-resnet-50-panoptic)  In Audio: - [Automatic Speech Recognition with Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h) - [Keyword Spotting with Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)  **[Write With Transformer](https://transformer.huggingface.co)**  built by the Hugging Face team  is the official demo of this repo‚Äôs text generation capabilities.   1. Easy-to-use state-of-the-art models:     - High performance on natural language understanding & generation  computer vision  and audio tasks.     - Low barrier to entry for educators and practitioners.     - Few user-facing abstractions with just three classes to learn.     - A unified API for using all our pretrained models.  1. Lower compute costs  smaller carbon footprint:     - Researchers can share trained models instead of always retraining.     - Practitioners can reduce compute time and production costs.     - Dozens of architectures with over 20 000 pretrained models  some in more than 100 languages.  1. Choose the right framework for every part of a model's lifetime:     - Train state-of-the-art models in 3 lines of code.     - Move a single model between TF2.0/PyTorch/JAX frameworks at will.     - Seamlessly pick the right framework for training  evaluation and production.  1. Easily customize a model or an example to your needs:     - We provide examples for each architecture to reproduce the results published by its original authors.     - Model internals are exposed as consistently as possible.     - Model files can be used independently of the library for quick experiments.   - This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose  so that researchers can quickly iterate on each of the models without diving into additional abstractions/files. - The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops  you should use another library. - While we strive to present as many use cases as possible  the scripts in our [examples folder](https://github.com/huggingface/transformers/tree/master/examples) are just that: examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.   """;Computer Vision;https://github.com/huggingface/transformers
"""**Refined Vision Transformer** is initially described in [arxiv](https://arxiv.org/abs/2106.03714)  which observes vision transformers require much more datafor model pre-training. Most of recent works thus are dedicated to designing morecomplex architectures or training methods to address the data-efficiency issue ofViTs. However  few of them explore improving the self-attention mechanism  akey factor distinguishing ViTs from CNNs.  Different from existing works  weintroduce a conceptually simple scheme  calledrefiner  to directly refine the self-attention maps of ViTs.  Specifically  refiner exploresattention expansionthatprojects the multi-head attention maps to a higher-dimensional space to promotetheir diversity.  Further  refiner applies convolutions to augment local patternsof the attention maps  which we show is equivalent to adistributed local atten-tion‚Äîfeatures are aggregated locally with learnable kernels and then globallyaggregated with self-attention.  Extensive experiments demonstrate that refinerworks surprisingly well. Significantly  it enables ViTs to achieve 86% top-1 classifi-cation accuracy on ImageNet with only 81M parameters.  <p align=""center""> <img src=""https://github.com/zhoudaquan/Refiner_ViT/blob/master/figures/overall_flow.png"" | width=500> </p>  Please run git clone with --recursive to clone timm as submodule and install it with ` cd pytorch-image-models && pip install -e ./`    bash run.sh scripts/refiner_s.yaml   """;Computer Vision;https://github.com/zhoudaquan/Refiner_ViT
"""To run the code you need to set up a conda environment with the environment.yml file.  First  ensure you have conda installed. Then  if you intend to run the model on GPU  do:  conda env create -f environment.yml  We don't recommend running the code on CPU as it will be very slow to train models. If you still intend to  you will need to modify tensorflow-gpu by tensorflow in the environment.yml file.  Once the command runs without errors  you should have a new environment available  called ncl  that you can activate with :  conda activate ncl   git clone https://github.com/YerevaNN/mimic3-benchmarks/  cd mimic3-benchmarks/   As it name indicates  this script train and evaluates NCL(n_w) method over 20 seeds on the MIMIC-III benchmark dataset. One important thing to note is that we base our pipeline on gin-config files (https://github.com/google/gin-config). You can run it the following way :   """;General;https://github.com/ratschlab/ncl
"""The following digits show isometry shifted digits (note the reflection at the cut):   """;Computer Vision;https://github.com/mauriceweiler/MobiusCNNs
"""To extract and run the network in Code::Blocks <br/> $ mkdir *MyDir* <br/> $ cd *MyDir* <br/> $ wget https://github.com/Qengineering/YoloV2-ncnn-Raspberry-Pi-4/archive/refs/heads/main.zip <br/> $ unzip -j master.zip <br/> Remove master.zip  LICENSE and README.md as they are no longer needed. <br/>  $ rm master.zip <br/> $ rm LICENSE <br/> $ rm README.md <br/> <br/> Your *MyDir* folder must now look like this: <br/>  parking.jpg <br/> busstop.jpg <br/> YoloV2.cpb <br/> yoloV2.cpp <br/> mobilenet_yolo.bin <br/> mobilenet_yolo.param <br/>  ------------   """;Computer Vision;https://github.com/Qengineering/YoloV2-ncnn-Raspberry-Pi-4
"""In this implementation example  the original hyper-parameters specified by the original paper are set. Feel free to play with other  hyper-parameters:  ```python from torchvision.models import resnet18  model = resnet18()  learner = BYOL(model)  opt = torch.optim.Adam(learner.parameters()  lr=3e-4)  criterion = NormalizedMSELoss()  def sample_unlabelled_images():     return torch.randn(20  3  256  256)  for _ in range(100):     images1 = sample_unlabelled_images()     images2 = sample_unlabelled_images() * 0.9     v1_on  v2_tar  v2_on  v1_tar = learner(images1  images2)     loss = criterion(v1_on  v2_tar  v2_on  v1_tar)     opt.zero_grad()     loss.backward()     opt.step()     learner.update_target_network()     print(loss) ```     """;General;https://github.com/SaeedShurrab/Simple-BYOL
"""""";Computer Vision;https://github.com/keillernogueira/exploit-cnn-rs
"""This project is dedicated to the investigation of methods for predicting meaningful events in footage of car racing. This repository is focused on the exploration of **collision detection** but contains a tool for the classification of  as well. During the work on this project we've also developed a **monadic pipeline** library [mPyPl](https://github.com/shwars/mPyPl) to simplify tasks of data processing and creating complex data pipelines.    Due to the small amount of data in this problem; we could not rely on neural networks to learn representations as part of the training process. Instead; we needed to design bespoke features  crafted with domain knowledge. After series of experiments  we've created a model based on features obtained using three different approaches:   * Dense Optical Flow * VGG16 embeddings  * A special kind of Optical Flow - [Focused Optical Flow](#focused-optical-flow).     ‚ÑπÔ∏è *After the release of mPyPl as a independent [pip-installable framework](https://pypi.org/project/mPyPl/)  some experimental notebooks in the ```notebooks``` folder have not been updated  but may contain interesting things to explore.*   * opencv-python   To successfully run the collision recognition examples  you need to install all the requirements using   pip install -r requirements.txt    git clone https://github.com/fizyr/keras-retinanet.git   To run the scene detection example  you need to have installed:   * Windows 10 - Build 17738 or higher  * Windows SDK - Build 17738 or higher   * The **training** process is described in the ```combined-training.ipynb``` notebook.  * To run the **inference** process use the ```video-pipeline.ipynb``` notebook. * All the working notebooks with our **experiments** are in ```notebooks``` folder (even though some notebooks are outdated  they contain interesting ideas). * The ```research``` folder contains our main **python modules**.   * The ```utils``` folder contains different **useful things** (e.g. visualization tools).      """;Computer Vision;https://github.com/sulasen/race-events-recognition-1
"""This project is dedicated to the investigation of methods for predicting meaningful events in footage of car racing. This repository is focused on the exploration of **collision detection** but contains a tool for the classification of  as well. During the work on this project we've also developed a **monadic pipeline** library [mPyPl](https://github.com/shwars/mPyPl) to simplify tasks of data processing and creating complex data pipelines.    Due to the small amount of data in this problem; we could not rely on neural networks to learn representations as part of the training process. Instead; we needed to design bespoke features  crafted with domain knowledge. After series of experiments  we've created a model based on features obtained using three different approaches:   * Dense Optical Flow * VGG16 embeddings  * A special kind of Optical Flow - [Focused Optical Flow](#focused-optical-flow).     ‚ÑπÔ∏è *After the release of mPyPl as a independent [pip-installable framework](https://pypi.org/project/mPyPl/)  some experimental notebooks in the ```notebooks``` folder have not been updated  but may contain interesting things to explore.*   * opencv-python   To successfully run the collision recognition examples  you need to install all the requirements using   pip install -r requirements.txt    git clone https://github.com/fizyr/keras-retinanet.git   To run the scene detection example  you need to have installed:   * Windows 10 - Build 17738 or higher  * Windows SDK - Build 17738 or higher   * The **training** process is described in the ```combined-training.ipynb``` notebook.  * To run the **inference** process use the ```video-pipeline.ipynb``` notebook. * All the working notebooks with our **experiments** are in ```notebooks``` folder (even though some notebooks are outdated  they contain interesting ideas). * The ```research``` folder contains our main **python modules**.   * The ```utils``` folder contains different **useful things** (e.g. visualization tools).      """;General;https://github.com/sulasen/race-events-recognition-1
""" ``` git clone https://github.com/AutodeskAILab/BRepNet.git cd BRepNet conda env create -f environment.yml conda activate brepnet ```  For GPU training you will need to change the pytorch install to include your cuda version.  i.e. for cuda 11.1 ``` conda install pytorch cudatoolkit=11.1 -c pytorch -c conda-forge ``` or for cuda 11.0 ``` conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=11.0 -c pytorch ```  For training with multiple workers you may hit errors of the form `OSError: [Errno 24] Too many open files`.  In this case you need to increase the number of available file handles on the machine using  ``` ulimit -Sn 10000 ``` I find I need to set the limit to 10000 for 10 worker threads.   cd BRepNet/  python -m pipeline.quickstart --dataset_dir /path/to/where_you_keep_data/s2.0.0 --num_workers 5   You are then ready to train the model.  The quickstart script should exit telling you a default command to use which should be something like  python -m train.train \   python -m train.train --help   train/reproduce_paper_results.sh /path/to/where_you_keep_data/s2.0.0   cd BRepNet   """;Computer Vision;https://github.com/AutodeskAILab/BRepNet
"""Machine translation is a natural language processing task that aims to translate natural languages using computers automatically. Recent several years have witnessed the rapid development of end-to-end neural machine translation  which has become the new mainstream method in practical MT systems.  THUMT is an open-source toolkit for neural machine translation developed by [the Natural Language Processing Group at Tsinghua University](http://nlp.csai.tsinghua.edu.cn/site2/index.php?lang=en).    """;Natural Language Processing;https://github.com/insigh/THUMT
"""A general YOLOv4/v3/v2 object detection pipeline inherited from [keras-yolo3-Mobilenet](https://github.com/Adamdad/keras-YOLOv3-mobilenet)/[keras-yolo3](https://github.com/qqwweee/keras-yolo3) and [YAD2K](https://github.com/allanzelener/YAD2K). Implement with tf.keras  including data collection/annotation  model training/tuning  model evaluation and on device deployment. Support different architecture and different technologies:      #: cd tools/dataset_converter/ && python voc_annotation.py -h                              both      You can merge these train &amp; val annotation file as your need. For example  following cmd will creat 07/12 combined trainval dataset:      #: cd tools/dataset_converter/ && python coco_annotation.py -h   If you want to download PascalVOC or COCO dataset  refer to Dockerfile for cmd     --gpu_num GPU_NUM     Number of GPU to use  default=1   You can also use Tensorboard to monitor the loss trend during train:   If you're evaluating with MSCOCO dataset  you can further use pycoco_eval.py with the generated txt detection result and COCO GT annotation to get official COCO AP with pycocotools:   The test environment is   Python 3.6.8   1. Install requirements on Ubuntu 16.04/18.04:  ``` #: apt install python3-opencv #: pip install Cython #: pip install -r requirements.txt ```  2. Download Related Darknet/YOLOv2/v3/v4 weights from [YOLO website](http://pjreddie.com/darknet/yolo/) and [AlexeyAB/darknet](https://github.com/AlexeyAB/darknet). 3. Convert the Darknet YOLO model to a Keras model. 4. Run YOLO detection on your image or video  default using Tiny YOLOv3 model.  ``` #: wget -O weights/darknet53.conv.74.weights https://pjreddie.com/media/files/darknet53.conv.74 #: wget -O weights/darknet19_448.conv.23.weights https://pjreddie.com/media/files/darknet19_448.conv.23 #: wget -O weights/yolov3.weights https://pjreddie.com/media/files/yolov3.weights #: wget -O weights/yolov3-tiny.weights https://pjreddie.com/media/files/yolov3-tiny.weights #: wget -O weights/yolov3-spp.weights https://pjreddie.com/media/files/yolov3-spp.weights #: wget -O weights/yolov2.weights http://pjreddie.com/media/files/yolo.weights #: wget -O weights/yolov2-voc.weights http://pjreddie.com/media/files/yolo-voc.weights #: wget -O weights/yolov2-tiny.weights https://pjreddie.com/media/files/yolov2-tiny.weights #: wget -O weights/yolov2-tiny-voc.weights https://pjreddie.com/media/files/yolov2-tiny-voc.weights  #:#:#: manually download csdarknet53-omega_final.weights from https://drive.google.com/open?id=18jCwaL4SJ-jOvXrZNGHJ5yz44g9zi8Hm #: wget -O weights/yolov4.weights https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights  #: python tools/model_converter/convert.py cfg/yolov3.cfg weights/yolov3.weights weights/yolov3.h5 #: python tools/model_converter/convert.py cfg/yolov3-tiny.cfg weights/yolov3-tiny.weights weights/yolov3-tiny.h5 #: python tools/model_converter/convert.py cfg/yolov3-spp.cfg weights/yolov3-spp.weights weights/yolov3-spp.h5 #: python tools/model_converter/convert.py cfg/yolov2.cfg weights/yolov2.weights weights/yolov2.h5 #: python tools/model_converter/convert.py cfg/yolov2-voc.cfg weights/yolov2-voc.weights weights/yolov2-voc.h5 #: python tools/model_converter/convert.py cfg/yolov2-tiny.cfg weights/yolov2-tiny.weights weights/yolov2-tiny.h5 #: python tools/model_converter/convert.py cfg/yolov2-tiny-voc.cfg weights/yolov2-tiny-voc.weights weights/yolov2-tiny-voc.h5 #: python tools/model_converter/convert.py cfg/darknet53.cfg weights/darknet53.conv.74.weights weights/darknet53.h5 #: python tools/model_converter/convert.py cfg/darknet19_448_body.cfg weights/darknet19_448.conv.23.weights weights/darknet19.h5  #: python tools/model_converter/convert.py cfg/csdarknet53-omega.cfg weights/csdarknet53-omega_final.weights weights/cspdarknet53.h5  #:#:#: make sure to reorder output tensors for YOLOv4 cfg and weights file #: python tools/model_converter/convert.py --yolo4_reorder cfg/yolov4.cfg weights/yolov4.weights weights/yolov4.h5  #:#:#: Scaled YOLOv4 #:#:#: manually download yolov4-csp.weights from https://drive.google.com/file/d/1NQwz47cW0NUgy7L3_xOKaNEfLoQuq3EL/view?usp=sharing #: python tools/model_converter/convert.py --yolo4_reorder cfg/yolov4-csp_fixed.cfg weights/yolov4-csp.weights weights/scaled-yolov4-csp.h5  #:#:#: Yolo-Fastest #: wget -O weights/yolo-fastest.weights https://github.com/dog-qiuqiu/Yolo-Fastest/raw/master/ModelZoo/yolo-fastest-1.0_coco/yolo-fastest.weights #: wget -O weights/yolo-fastest-xl.weights https://github.com/dog-qiuqiu/Yolo-Fastest/raw/master/ModelZoo/yolo-fastest-1.0_coco/yolo-fastest-xl.weights  #: python tools/model_converter/convert.py cfg/yolo-fastest.cfg weights/yolo-fastest.weights weights/yolo-fastest.h5 #: python tools/model_converter/convert.py cfg/yolo-fastest-xl.cfg weights/yolo-fastest-xl.weights weights/yolo-fastest-xl.h5   #: python yolo.py --image #: python yolo.py --input=<your video file> ``` For other model  just do in a similar way  but specify different model type  weights path and anchor path with `--model_type`  `--weights_path` and `--anchors_path`.  Image detection sample:  <p align=""center"">   <img src=""assets/dog_inference.jpg"">   <img src=""assets/kite_inference.jpg""> </p>   1. [yolo.py](https://github.com/david8862/keras-YOLOv3-model-set/blob/master/yolo.py) > * Demo script for trained model  image detection mode ``` #: python yolo.py --model_type=yolo3_mobilenet_lite --weights_path=model.h5 --anchors_path=configs/yolo3_anchors.txt --classes_path=configs/voc_classes.txt --model_input_shape=416x416 --image ``` video detection mode ``` #: python yolo.py --model_type=yolo3_mobilenet_lite --weights_path=model.h5 --anchors_path=configs/yolo3_anchors.txt --classes_path=configs/voc_classes.txt --model_input_shape=416x416 --input=test.mp4 ``` For video detection mode  you can use ""input=0"" to capture live video from web camera and ""output=<video name>"" to dump out detection result to another video   """;Computer Vision;https://github.com/david8862/keras-YOLOv3-model-set
"""1) Download the model weights and place them in the `weights` folder:   Monodepth: - [dpt_hybrid-midas-501f0c75.pt](https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-midas-501f0c75.pt)  [Mirror](https://drive.google.com/file/d/1dgcJEYYw1F8qirXhZxgNK8dWWz_8gZBD/view?usp=sharing) - [dpt_large-midas-2f21e586.pt](https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt)  [Mirror](https://drive.google.com/file/d/1vnuhoMc6caF-buQQ4hK0CeiMk9SjwB-G/view?usp=sharing)  Segmentation:  - [dpt_hybrid-ade20k-53898607.pt](https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-ade20k-53898607.pt)  [Mirror](https://drive.google.com/file/d/1zKIAMbltJ3kpGLMh6wjsq65_k5XQ7_9m/view?usp=sharing)  - [dpt_large-ade20k-b12dca68.pt](https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-ade20k-b12dca68.pt)  [Mirror](https://drive.google.com/file/d/1foDpUM7CdS8Zl6GPdkrJaAOjskb7hHe-/view?usp=sharing)    2) Set up dependencies:       ```shell     pip install -r requirements.txt     ```     The code was tested with Python 3.7  PyTorch 1.8.0  OpenCV 4.5.1  and timm 0.4.5   1) Place one or more input images in the folder `input`.  2) Run a monocular depth estimation model:      ```shell     python run_monodepth.py     ```      Or run a semantic segmentation model:      ```shell     python run_segmentation.py     ```  3) The results are written to the folder `output_monodepth` and `output_semseg`  respectively.  Use the flag `-t` to switch between different models. Possible options are `dpt_hybrid` (default) and `dpt_large`.   **Additional models:**  - Monodepth finetuned on KITTI: [dpt_hybrid_kitti-cb926ef4.pt](https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid_kitti-cb926ef4.pt) [Mirror](https://drive.google.com/file/d/1-oJpORoJEdxj4LTV-Pc17iB-smp-khcX/view?usp=sharing) - Monodepth finetuned on NYUv2: [dpt_hybrid_nyu-2ce69ec7.pt](https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid_nyu-2ce69ec7.pt) [Mirror](https\://drive.google.com/file/d/1NjiFw1Z9lUAfTPZu4uQ9gourVwvmd58O/view?usp=sharing)  Run with   ```shell python run_monodepth -t [dpt_hybrid_kitti|dpt_hybrid_nyu]  ```   """;Computer Vision;https://github.com/isl-org/DPT
"""1. Jupyter Notebook   | stevensmiley1989_MoNuSAC_Entire_Code_5_15_2021.ipynb | My Jupyter notebook. |   """;Computer Vision;https://github.com/stevensmiley1989/MoNuSAC_Grand_Challenge_S4xUNet
"""LinkÔºöhttps://pan.baidu.com/s/1u12MAVgoIke6HobJLfbebg    """;Computer Vision;https://github.com/FenHua/Robust_Logo_Detection
"""Manufacturing is becoming automated on a broad scale. The technology enables manufacturers to affordably boost their throughput  improve quality and become nimbler as they respond to customer demands. Automation is a revolution in manufacturing quality control. It allows the companies to set certain bars or criteria for the products being manufactured. Then it also aids in real-time tracking of the manufacturing process through machine vision cameras and/or recordings.   The core deliverable for this project is building deep learning image classification models which can automate the process of inspection for casting defects. I have produced a rest endpoint which can accept a cast image and subsequently run a tuned model to classify if the cast is acceptable or not.    As part of this project  I have built the computer vision models in 3 different ways addressing different personas  because not all companies will have a resolute data science team.   1.	Using Keras Tensorflow model (convolution2d) what a trained team of data scientists would do. 2.	Using Azure Machine Learning Designer (designer) which enables AI engineers and Data scientists use a drag and drop prebuilt model DenseNet (densenet).  3.	Using Azure custom vision (custom-vision) which democratizes the process of building the computer vision model with little to no training.   The work that will be subsequently done as part of this paper will have at the very least embody the following principles (ai/responsible-ai  n.d.):   ‚Ä¢	Fair - AI must maximize efficiencies without destroying dignity and guard against bias.   ‚Ä¢	Accountable - AI must have algorithmic accountability.   ‚Ä¢	Transparent - AI systems must be transparent and understandable.   ‚Ä¢	Ethical - AI must assist humanity and be designed for intelligent privacy.     """;General;https://github.com/RajdeepBiswas/Manufacturing-Quality-Inspection
"""Manufacturing is becoming automated on a broad scale. The technology enables manufacturers to affordably boost their throughput  improve quality and become nimbler as they respond to customer demands. Automation is a revolution in manufacturing quality control. It allows the companies to set certain bars or criteria for the products being manufactured. Then it also aids in real-time tracking of the manufacturing process through machine vision cameras and/or recordings.   The core deliverable for this project is building deep learning image classification models which can automate the process of inspection for casting defects. I have produced a rest endpoint which can accept a cast image and subsequently run a tuned model to classify if the cast is acceptable or not.    As part of this project  I have built the computer vision models in 3 different ways addressing different personas  because not all companies will have a resolute data science team.   1.	Using Keras Tensorflow model (convolution2d) what a trained team of data scientists would do. 2.	Using Azure Machine Learning Designer (designer) which enables AI engineers and Data scientists use a drag and drop prebuilt model DenseNet (densenet).  3.	Using Azure custom vision (custom-vision) which democratizes the process of building the computer vision model with little to no training.   The work that will be subsequently done as part of this paper will have at the very least embody the following principles (ai/responsible-ai  n.d.):   ‚Ä¢	Fair - AI must maximize efficiencies without destroying dignity and guard against bias.   ‚Ä¢	Accountable - AI must have algorithmic accountability.   ‚Ä¢	Transparent - AI systems must be transparent and understandable.   ‚Ä¢	Ethical - AI must assist humanity and be designed for intelligent privacy.     """;Computer Vision;https://github.com/RajdeepBiswas/Manufacturing-Quality-Inspection
"""NEZHA-PyTorch is the PyTorch version of NEZHA.   """;General;https://github.com/huawei-noah/Pretrained-Language-Model
"""You can run a training session on 30 Monet images using the following Jupiter notebook:   """;Computer Vision;https://github.com/uzielroy/CUT
"""You can run a training session on 30 Monet images using the following Jupiter notebook:   """;General;https://github.com/uzielroy/CUT
"""1. Run `pip install -r requirements.txt` to install the necessary dependencies. 2. Run `python launch_script.py` to start training the Chess Engine.   python-chess - For handling the chess environment and gameplay.   """;Reinforcement Learning;https://github.com/saikrishna-1996/deep_pepper_chess
"""All unit tests in baselines can be run using pytest runner: ``` pip install pytest pytest ```   - Clone the repo and cd into it:     ```bash     git clone https://github.com/openai/baselines.git     cd baselines     ``` - If you don't have TensorFlow installed already  install your favourite flavor of TensorFlow. In most cases  you may use     ```bash      pip install tensorflow-gpu==1.14 #: if you have a CUDA-compatible gpu and proper drivers     ```     or      ```bash     pip install tensorflow==1.14     ```     to install Tensorflow 1.14  which is the latest version of Tensorflow supported by the master branch. Refer to [TensorFlow installation guide](https://www.tensorflow.org/install/)     for more details.   - Install baselines package     ```bash     pip install -e .     ```   sudo apt-get update &amp;&amp; sudo apt-get install cmake libopenmpi-dev python3-dev zlib1g-dev  Installation of system packages on Mac requires Homebrew. With Homebrew installed  run the following:  brew install cmake openmpi  From the general python package sanity perspective  it is a good idea to use virtual environments (virtualenvs) to make sure packages from different projects do not interfere with each other. You can install virtualenv (which is itself a pip package) via  pip install virtualenv   To create a virtualenv called venv with python3  one runs   virtualenv /path/to/venv --python=python3  To activate a virtualenv:    python -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v4 --num_timesteps=2e7 --save_path=~/models/pong_20M_ppo2   python -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v4 --num_timesteps=0 --load_path=~/models/pong_20M_ppo2 --play   The directory can be changed with the --log_path command-line option.  python -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v4 --num_timesteps=2e7 --save_path=~/models/pong_20M_ppo2 --log_path=~/logs/Pong/   Another way the temp directory can be changed is through the use of the $OPENAI_LOGDIR environment variable.   For instance  to train a fully-connected network controlling MuJoCo humanoid using PPO2 for 20M timesteps ```bash python -m baselines.run --alg=ppo2 --env=Humanoid-v2 --network=mlp --num_timesteps=2e7 ``` Note that for mujoco environments fully-connected network is default  so we can omit `--network=mlp` The hyperparameters for both network and the learning algorithm can be controlled via the command line  for instance: ```bash python -m baselines.run --alg=ppo2 --env=Humanoid-v2 --network=mlp --num_timesteps=2e7 --ent_coef=0.1 --num_hidden=32 --num_layers=3 --value_network=copy ``` will set entropy coefficient to 0.1  and construct fully connected network with 3 layers with 32 hidden units in each  and create a separate network for value function estimation (so that its parameters are not shared with the policy network  but the structure is the same)  See docstrings in [common/models.py](baselines/common/models.py) for description of network parameters for each type of model  and  docstring for [baselines/ppo2/ppo2.py/learn()](baselines/ppo2/ppo2.py#L152) for the description of the ppo2 hyperparameters.    DQN with Atari is at this point a classics of benchmarks. To run the baselines implementation of DQN on Atari Pong: ``` python -m baselines.run --alg=deepq --env=PongNoFrameskip-v4 --num_timesteps=1e6 ```   """;Reinforcement Learning;https://github.com/openai/baselines
"""<a href=""https://pypi.org/project/transformer-implementations/"">PyPi</a>  ```bash $ pip install transformer-implementations ```  or  ```bash python setup.py build python setup.py install ```           <img alt=""PyPi Version"" src=""https://img.shields.io/pypi/v/transformer-implementations"">           <img alt=""PyPi Downloads"" src=""https://img.shields.io/pypi/dm/transformer-implementations"">           <img alt=""Package Status"" src=""https://img.shields.io/pypi/status/transformer-implementations"">   In <a href=""https://github.com/UdbhavPrasad072300/Transformer-Implementations/blob/main/notebooks/"">notebooks</a> directory there is a notebook on how to use each of these models for their intented use; such as image classification for Vision Transformer (ViT) and others. Check them out!  ```python from transformer_package.models import ViT  image_size = 28 #: Model Parameters channel_size = 1 patch_size = 7 embed_size = 512 num_heads = 8 classes = 10 num_layers = 3 hidden_size = 256 dropout = 0.2  model = ViT(image_size               channel_size               patch_size               embed_size               num_heads               classes               num_layers               hidden_size               dropout=dropout).to(DEVICE)              prediction = model(image_tensor) ```   """;General;https://github.com/UdbhavPrasad072300/Transformer-Implementations
"""<a href=""https://pypi.org/project/transformer-implementations/"">PyPi</a>  ```bash $ pip install transformer-implementations ```  or  ```bash python setup.py build python setup.py install ```           <img alt=""PyPi Version"" src=""https://img.shields.io/pypi/v/transformer-implementations"">           <img alt=""PyPi Downloads"" src=""https://img.shields.io/pypi/dm/transformer-implementations"">           <img alt=""Package Status"" src=""https://img.shields.io/pypi/status/transformer-implementations"">   In <a href=""https://github.com/UdbhavPrasad072300/Transformer-Implementations/blob/main/notebooks/"">notebooks</a> directory there is a notebook on how to use each of these models for their intented use; such as image classification for Vision Transformer (ViT) and others. Check them out!  ```python from transformer_package.models import ViT  image_size = 28 #: Model Parameters channel_size = 1 patch_size = 7 embed_size = 512 num_heads = 8 classes = 10 num_layers = 3 hidden_size = 256 dropout = 0.2  model = ViT(image_size               channel_size               patch_size               embed_size               num_heads               classes               num_layers               hidden_size               dropout=dropout).to(DEVICE)              prediction = model(image_tensor) ```   """;Natural Language Processing;https://github.com/UdbhavPrasad072300/Transformer-Implementations
"""‚îú‚îÄ‚îÄ environment.yml   Define packages/dependencies required for the environment to be installed and run.   """;Reinforcement Learning;https://github.com/ianlimle/ItsMeMario
"""JDet is an object detection benchmark based on [Jittor](https://github.com/Jittor/jittor)  and mainly focus on aerial image object detection (oriented object detection).   <!-- **Features** - Automatic compilation. Our framwork is based on Jittor  which means we don't need to Manual compilation for these code with CUDA and C++. -  -->  <!-- Framework details are avaliable in the [framework.md](docs/framework.md) -->  JDet environment requirements:  * System: **Linux**(e.g. Ubuntu/CentOS/Arch)  **macOS**  or **Windows Subsystem of Linux (WSL)** * Python version >= 3.7 * CPU compiler (require at least one of the following)     * g++ (>=5.4.0)     * clang (>=8.0) * GPU compiler (optional)     * nvcc (>=10.0 for g++ or >=10.2 for clang) * GPU library: cudnn-dev (recommend tar file installation  [reference link](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux-tar))  **Step 1: Install the requirements** ```shell git clone https://github.com/Jittor/JDet cd JDet python -m pip install -r requirements.txt ``` If you have any installation problems for Jittor  please refer to [Jittor](https://github.com/Jittor/jittor)  **Step 2: Install JDet**   ```shell cd JDet #: suggest this  python setup.py develop #: or python setup.py install ``` If you don't have permission for install please add ```--user```.  Or use ```PYTHONPATH```:  You can add ```export PYTHONPATH=$PYTHONPATH:{you_own_path}/JDet/python``` into ```.bashrc```  and run ```shell source .bashrc ```   You can also build your own dataset by convert your datas to DOTA format.   mkdir $PROJECT_PATH$  cd $PROJECT_PATH$   mkdir configs   :heavy_check_mark: COCO   """;Computer Vision;https://github.com/Jittor/JDet
"""JDet is an object detection benchmark based on [Jittor](https://github.com/Jittor/jittor)  and mainly focus on aerial image object detection (oriented object detection).   <!-- **Features** - Automatic compilation. Our framwork is based on Jittor  which means we don't need to Manual compilation for these code with CUDA and C++. -  -->  <!-- Framework details are avaliable in the [framework.md](docs/framework.md) -->  JDet environment requirements:  * System: **Linux**(e.g. Ubuntu/CentOS/Arch)  **macOS**  or **Windows Subsystem of Linux (WSL)** * Python version >= 3.7 * CPU compiler (require at least one of the following)     * g++ (>=5.4.0)     * clang (>=8.0) * GPU compiler (optional)     * nvcc (>=10.0 for g++ or >=10.2 for clang) * GPU library: cudnn-dev (recommend tar file installation  [reference link](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux-tar))  **Step 1: Install the requirements** ```shell git clone https://github.com/Jittor/JDet cd JDet python -m pip install -r requirements.txt ``` If you have any installation problems for Jittor  please refer to [Jittor](https://github.com/Jittor/jittor)  **Step 2: Install JDet**   ```shell cd JDet #: suggest this  python setup.py develop #: or python setup.py install ``` If you don't have permission for install please add ```--user```.  Or use ```PYTHONPATH```:  You can add ```export PYTHONPATH=$PYTHONPATH:{you_own_path}/JDet/python``` into ```.bashrc```  and run ```shell source .bashrc ```   You can also build your own dataset by convert your datas to DOTA format.   mkdir $PROJECT_PATH$  cd $PROJECT_PATH$   mkdir configs   :heavy_check_mark: COCO   """;General;https://github.com/Jittor/JDet
"""""";Computer Vision;https://github.com/houssemjebari/Fruit-Detection
"""""";General;https://github.com/sidwa/ae_thesis
"""installing anything  and goes through the following experiments / SIREN properties:   Some of the experiments were run using the BSD500 datast  which you can download here.   If you want to reproduce all the results (including the baselines) shown in the paper  the videos  point clouds  and  audio files can be found [here](https://drive.google.com/drive/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K?usp=sharing).  You can then set up a conda environment with all dependencies like so: ``` conda env create -f environment.yml conda activate siren ```   """;General;https://github.com/TalFurman/Implict_neural_representation_of_images
"""* [PyTorch](http://pytorch.org/) version >= 1.5.0 * Python version >= 3.6 * For training new models  you'll also need an NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl) * **To install fairseq** and develop locally:  ``` bash git clone https://github.com/pytorch/fairseq cd fairseq pip install --editable ./  #: on MacOS: #: CFLAGS=""-stdlib=libc++"" pip install --editable ./  #: to install the latest stable release (0.10.x) #: pip install fairseq ```  * **For faster training** install NVIDIA's [apex](https://github.com/NVIDIA/apex) library:  ``` bash git clone https://github.com/NVIDIA/apex cd apex pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" \   --global-option=""--deprecated_fused_adam"" --global-option=""--xentropy"" \   --global-option=""--fast_multihead_attn"" ./ ```  * **For large datasets** install [PyArrow](https://arrow.apache.org/docs/python/install.html#using-pip): `pip install pyarrow` * If you use Docker make sure to increase the shared memory size either with `--ipc=host` or `--shm-size`  as command line options to `nvidia-docker run` .   * May 2020: [Follow fairseq on Twitter](https://twitter.com/fairseq)   * December 2019: [fairseq 0.9.0 released](https://github.com/pytorch/fairseq/releases/tag/v0.9.0)   mixed precision training (trains faster with less GPU memory on NVIDIA tensor cores)   ``` python   Twitter: https://twitter.com/fairseq   The [full documentation](https://fairseq.readthedocs.io/) contains instructions for getting started  training new models and extending fairseq with new model types and tasks.   We provide pre-trained models and pre-processed  binarized test sets for several tasks listed below  as well as example training and evaluation commands.  * [Translation](examples/translation/README.md): convolutional and transformer models are available * [Language Modeling](examples/language_model/README.md): convolutional and transformer models are available  We also have more detailed READMEs to reproduce results from specific papers:  * [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale (Babu et al.  2021)](examples/wav2vec/xlsr/README.md) * [Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al.  2020)](examples/criss/README.md) * [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al.  2020)](examples/wav2vec/README.md) * [Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al.  2020)](examples/unsupervised_quality_estimation/README.md) * [Training with Quantization Noise for Extreme Model Compression ({Fan*  Stock*} et al.  2020)](examples/quant_noise/README.md) * [Neural Machine Translation with Byte-Level Subwords (Wang et al.  2020)](examples/byte_level_bpe/README.md) * [Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at.  2020)](examples/mbart/README.md) * [Reducing Transformer Depth on Demand with Structured Dropout (Fan et al.  2019)](examples/layerdrop/README.md) * [Jointly Learning to Align and Translate with Transformer Models (Garg et al.  2019)](examples/joint_alignment_translation/README.md) * [Levenshtein Transformer (Gu et al.  2019)](examples/nonautoregressive_translation/README.md) * [Facebook FAIR's WMT19 News Translation Task Submission (Ng et al.  2019)](examples/wmt19/README.md) * [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al.  2019)](examples/roberta/README.md) * [wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al.  2019)](examples/wav2vec/README.md) * [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al.  2019)](examples/translation_moe/README.md) * [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al.  2019)](examples/pay_less_attention_paper/README.md) * [Understanding Back-Translation at Scale (Edunov et al.  2018)](examples/backtranslation/README.md) * [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al.  2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel) * [Hierarchical Neural Story Generation (Fan et al.  2018)](examples/stories/README.md) * [Scaling Neural Machine Translation (Ott et al.  2018)](examples/scaling_nmt/README.md) * [Convolutional Sequence to Sequence Learning (Gehring et al.  2017)](examples/conv_seq2seq/README.md) * [Language Modeling with Gated Convolutional Networks (Dauphin et al.  2017)](examples/language_model/README.conv.md)   """;Audio;https://github.com/pytorch/fairseq
"""If you want to train your own model  please prepare a parallel linguistics corpus  like corpus in WMT. A GPU with 12GB memory will be helpful. You could run bash train.sh or follow these steps.   """;General;https://github.com/SwordYork/DCNMT
"""To download and preprocess the data  run:  ```bash cd examples/language_model/ bash prepare-wikitext-103.sh cd ../..   TEXT=examples/language_model/wikitext-103 fairseq-preprocess \     --only-source \     --trainpref $TEXT/wiki.train.tokens \     --validpref $TEXT/wiki.valid.tokens \     --testpref $TEXT/wiki.test.tokens \     --destdir data-bin/wikitext-103 \     --workers 20 ```   This repository is a fork of the [Fairseq](https://github.com/pytorch/fairseq) repository and so has the same requirements.   Once you've installed the dependencies  you can install this repository by running:  ```bash pip install --editable . ```   Rename the file you downloaded to checkpoint_best.pt if you'd like to follow the directions below.   """;General;https://github.com/ofirpress/attention_with_linear_biases
"""Use the package manager [pip](https://pip.pypa.io/en/stable/) to install the package in the following way:  ```bash pip install vanilla-transformer-jax ```   To use the entire Transformer model (encoder and decoder)  you can use the following way:  ```python from jax import random from vtransformer import Transformer #: imports Transformer class  model = Transformer() #: model hyperparameters can be tuned  otherwise defualts mentioned in paper shall be used  prng = random.PRNGKey(42)  example_input_src = jax.random.randint(prng  (3 4)  minval=0  maxval=10000) example_input_trg = jax.random.randint(prng  (3 5)  minval=0  maxval=10000) mask = jax.array([1  1  1  0  0])  init = model.init(prng  example_input_src  example_input_trg  mask) #:initializing the params of model  output = model.apply(init  example_input_src  example_input_trg  mask) #: getting output ```  To use Encoder and Decoder seperately  you can do so in the following way:  ```python encoding = model.encoder(init  example_input_src)  #:using only the encoder decoding = model.decoder(init  example_input_trg  encoding  mask) #:using only the decoder ```    """;General;https://github.com/Bhavnicksm/vanilla-transformer-jax
"""Use the package manager [pip](https://pip.pypa.io/en/stable/) to install the package in the following way:  ```bash pip install vanilla-transformer-jax ```   To use the entire Transformer model (encoder and decoder)  you can use the following way:  ```python from jax import random from vtransformer import Transformer #: imports Transformer class  model = Transformer() #: model hyperparameters can be tuned  otherwise defualts mentioned in paper shall be used  prng = random.PRNGKey(42)  example_input_src = jax.random.randint(prng  (3 4)  minval=0  maxval=10000) example_input_trg = jax.random.randint(prng  (3 5)  minval=0  maxval=10000) mask = jax.array([1  1  1  0  0])  init = model.init(prng  example_input_src  example_input_trg  mask) #:initializing the params of model  output = model.apply(init  example_input_src  example_input_trg  mask) #: getting output ```  To use Encoder and Decoder seperately  you can do so in the following way:  ```python encoding = model.encoder(init  example_input_src)  #:using only the encoder decoding = model.decoder(init  example_input_trg  encoding  mask) #:using only the decoder ```    """;Natural Language Processing;https://github.com/Bhavnicksm/vanilla-transformer-jax
"""""";Natural Language Processing;https://github.com/akkarimi/aeda_nlp
"""""";Reinforcement Learning;https://github.com/Epsilon10/MARL
"""Install with:  pip install ""git+https://github.com/andrewlstewart/StereoNet_PyTorch""       https://github.com/zhixuanli/StereoNet/issues/12#issuecomment-508327106   """;General;https://github.com/andrewlstewart/StereoNet_PyTorch
"""We provide code and training configurations of VoTr-SSD/TSD on the KITTI and Waymo Open dataset. Checkpoints will not be released.    **Important Notes**: VoTr generally requires quite a long time (more than 60 epochs on Waymo) to converge  and a large GPU memory (32Gb) is needed for reproduction. Please strictly follow the instructions and train with sufficient number of epochs. If you don't have a 32G GPU  you can decrease the attention SIZE parameters in yaml files  but this may possibly harm the performance.    a. Clone this repository. ```shell git clone https://github.com/PointsCoder/VOTR.git ```  b. Install the dependent libraries as follows:  * Install the dependent python libraries:  ``` pip install -r requirements.txt  ```  * Install the SparseConv library  we use the implementation from [`[spconv]`](https://github.com/traveller59/spconv).      * If you use PyTorch 1.1  then make sure you install the `spconv v1.0` with ([commit 8da6f96](https://github.com/traveller59/spconv/tree/8da6f967fb9a054d8870c3515b1b44eca2103634)) instead of the latest one.     * If you use PyTorch 1.3+  then you need to install the `spconv v1.2`. As mentioned by the author of [`spconv`](https://github.com/traveller59/spconv)  you need to use their docker if you use PyTorch 1.4+.   c. Compile CUDA operators by running the following command: ```shell python setup.py develop ```   CUDA_VISIBLE_DEVICES=0 1 2 3 4 5 6 7 sh scripts/dist_train.sh 8 --cfg_file cfgs/waymo_models/votr_tsd.yaml   """;Computer Vision;https://github.com/PointsCoder/VOTR
"""We provide code and training configurations of VoTr-SSD/TSD on the KITTI and Waymo Open dataset. Checkpoints will not be released.    **Important Notes**: VoTr generally requires quite a long time (more than 60 epochs on Waymo) to converge  and a large GPU memory (32Gb) is needed for reproduction. Please strictly follow the instructions and train with sufficient number of epochs. If you don't have a 32G GPU  you can decrease the attention SIZE parameters in yaml files  but this may possibly harm the performance.    a. Clone this repository. ```shell git clone https://github.com/PointsCoder/VOTR.git ```  b. Install the dependent libraries as follows:  * Install the dependent python libraries:  ``` pip install -r requirements.txt  ```  * Install the SparseConv library  we use the implementation from [`[spconv]`](https://github.com/traveller59/spconv).      * If you use PyTorch 1.1  then make sure you install the `spconv v1.0` with ([commit 8da6f96](https://github.com/traveller59/spconv/tree/8da6f967fb9a054d8870c3515b1b44eca2103634)) instead of the latest one.     * If you use PyTorch 1.3+  then you need to install the `spconv v1.2`. As mentioned by the author of [`spconv`](https://github.com/traveller59/spconv)  you need to use their docker if you use PyTorch 1.4+.   c. Compile CUDA operators by running the following command: ```shell python setup.py develop ```   CUDA_VISIBLE_DEVICES=0 1 2 3 4 5 6 7 sh scripts/dist_train.sh 8 --cfg_file cfgs/waymo_models/votr_tsd.yaml   """;General;https://github.com/PointsCoder/VOTR
"""This work is done in python. you will require pip for installation. Create a virtual environment and in the virtual environment install the dependencies from the requirements.txt.   ``` pip install -r requirements.txt ```   """;Computer Vision;https://github.com/darrishabh/coviprox
"""- Dont sample from a Uniform distribution - When doing interpolations  do the interpolation via a great circle  rather than a straight line from point A to point B - Tom White's [Sampling Generative Networks](https://arxiv.org/abs/1609.04468) ref code https://github.com/dribnet/plat has more details   - Label Smoothing  i.e. if you have two target labels: Real=1 and Fake=0  then for each incoming sample  if it is real  then replace the label with a random number between 0.7 and 1.2  and if it is a fake sample  replace it with 0.0 and 0.3 (for example).   - Salimans et. al. 2016 - make the labels the noisy for the discriminator: occasionally flip the labels when training the discriminator - fake_labels = `tf.random.uniform(shape=[25  1]  minval=0  maxval=0.3  dtype=tf.float32)` - real_labels = `tf.random.uniform(shape=[25  1]  minval=0.7  maxval=1.2  dtype=tf.float32)`   - Experience Replay   - Keep a replay buffer of past generations and occassionally show them   - Keep checkpoints from the past of G and D and occassionaly swap them out for a few iterations - All stability tricks that work for deep deterministic policy gradients   - optim.Adam rules!   - See Radford et. al. 2015 - Use SGD for discriminator and ADAM for generator   - Provide noise in the form of dropout (50%). - Apply on several layers of our generator at both training and test time - https://arxiv.org/pdf/1611.07004v1.pdf  """;General;https://github.com/surajkarki66/Generative-Model-Deep-Learning
"""tar -xf annotations.tar.gz   requirements.txt - Used to install dependencies.   """;Computer Vision;https://github.com/adityarajsahu/UNet-Implementation
""": To get the ResNet18 use   : To get the ResNet34 use   : To get the ResNet50 use   : To get the ResNet101 use   To get the ResNet152 use   """;General;https://github.com/iArunava/ResNet
""": To get the ResNet18 use   : To get the ResNet34 use   : To get the ResNet50 use   : To get the ResNet101 use   To get the ResNet152 use   """;Computer Vision;https://github.com/iArunava/ResNet
"""""";Computer Vision;https://github.com/GitHberChen/FCN-Pytorch
"""IMPORTANT NOTE:  PySC2 2.0.1 must use game client v4.1.2 or above   Openmind's python wrapper component: https://github.com/deepmind/pysc2   """;Reinforcement Learning;https://github.com/sc2crazy/StarCrackRL
"""""";Computer Vision;https://github.com/Stick-To/Octconv-ResNet-tensorflow
"""""";Computer Vision;https://github.com/clemkoa/faster-rcnn
"""ËØ•Á≥ªÁªüÂÆûÁé∞‰∫ÜÂü∫‰∫éÊ∑±Â∫¶Ê°ÜÊû∂ÁöÑËØ≠Èü≥ËØÜÂà´‰∏≠ÁöÑÂ£∞Â≠¶Ê®°ÂûãÂíåËØ≠Ë®ÄÊ®°ÂûãÂª∫Ê®°ÔºåÂÖ∂‰∏≠Â£∞Â≠¶Ê®°ÂûãÂåÖÊã¨CNN-CTC„ÄÅGRU-CTC„ÄÅCNN-RNN-CTCÔºåËØ≠Ë®ÄÊ®°ÂûãÂåÖÂê´[transformer](https://jalammar.github.io/illustrated-transformer/)„ÄÅ[CBHG](https://github.com/crownpku/Somiao-Pinyin)ÔºåÊï∞ÊçÆÈõÜÂåÖÂê´stc„ÄÅprimewords„ÄÅAishell„ÄÅthchs30Âõõ‰∏™Êï∞ÊçÆÈõÜ„ÄÇ  Êú¨Á≥ªÁªüÊõ¥Êï¥‰Ωì‰ªãÁªçÔºöhttps://blog.csdn.net/chinatelecom08/article/details/82557715  Êú¨È°πÁõÆÁé∞Â∑≤ËÆ≠ÁªÉ‰∏Ä‰∏™Ëø∑‰Ω†ÁöÑËØ≠Èü≥ËØÜÂà´Á≥ªÁªüÔºåÂ∞ÜÈ°πÁõÆ‰∏ãËΩΩÂà∞Êú¨Âú∞‰∏äÔºå‰∏ãËΩΩ[thchsÊï∞ÊçÆÈõÜ](http://www.openslr.org/resources/18/data_thchs30.tgz)Âπ∂Ëß£ÂéãËá≥dataÔºåËøêË°å`test.py`Ôºå‰∏çÂá∫ÊÑèÂ§ñËÉΩÂ§üËøõË°åËØÜÂà´ÔºåÁªìÊûúÂ¶Ç‰∏ãÔºö       the  0 th example.     ÊñáÊú¨ÁªìÊûúÔºö lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     ÂéüÊñáÁªìÊûúÔºö lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     ÂéüÊñáÊ±âÂ≠óÔºö ÁªøÊòØÈò≥Êò•ÁÉüÊôØÂ§ßÂùóÊñáÁ´†ÁöÑÂ∫ïËâ≤ÂõõÊúàÁöÑÊûóÂ≥¶Êõ¥ÊòØÁªøÂæóÈ≤úÊ¥ªÁßÄÂ™öËØóÊÑèÁõéÁÑ∂     ËØÜÂà´ÁªìÊûúÔºö ÁªøÊòØÈò≥Êò•ÁÉüÊôØÂ§ßÂùóÊñáÁ´†ÁöÑÂ∫ïËâ≤ÂõõÊúàÁöÑÊûóÂ≥¶Êõ¥ÊòØÁªøÂæóÈ≤úÊ¥ªÁßÄÂ™öËØóÊÑèÁõéÁÑ∂  Ëã•Ëá™Â∑±Âª∫Á´ãÊ®°ÂûãÂàôÈúÄË¶ÅÂà†Èô§Áé∞ÊúâÊ®°ÂûãÔºåÈáçÊñ∞ÈÖçÁΩÆÂèÇÊï∞ËÆ≠ÁªÉÔºåÂÖ∑‰ΩìÂÆûÁé∞ÊµÅÁ®ãÂèÇËÄÉÊú¨È°µÊúÄÂêé„ÄÇ   |Name | train | dev | test   get source list...   make am vocab...   make lm pinyin vocab...   make lm hanzi vocab...   get source list...   make am vocab...   make lm pinyin vocab...   make lm hanzi vocab...   ÊàëÁöÑgithub: https://github.com/audier   """;General;https://github.com/yumoh/speech-keras
"""ËØ•Á≥ªÁªüÂÆûÁé∞‰∫ÜÂü∫‰∫éÊ∑±Â∫¶Ê°ÜÊû∂ÁöÑËØ≠Èü≥ËØÜÂà´‰∏≠ÁöÑÂ£∞Â≠¶Ê®°ÂûãÂíåËØ≠Ë®ÄÊ®°ÂûãÂª∫Ê®°ÔºåÂÖ∂‰∏≠Â£∞Â≠¶Ê®°ÂûãÂåÖÊã¨CNN-CTC„ÄÅGRU-CTC„ÄÅCNN-RNN-CTCÔºåËØ≠Ë®ÄÊ®°ÂûãÂåÖÂê´[transformer](https://jalammar.github.io/illustrated-transformer/)„ÄÅ[CBHG](https://github.com/crownpku/Somiao-Pinyin)ÔºåÊï∞ÊçÆÈõÜÂåÖÂê´stc„ÄÅprimewords„ÄÅAishell„ÄÅthchs30Âõõ‰∏™Êï∞ÊçÆÈõÜ„ÄÇ  Êú¨Á≥ªÁªüÊõ¥Êï¥‰Ωì‰ªãÁªçÔºöhttps://blog.csdn.net/chinatelecom08/article/details/82557715  Êú¨È°πÁõÆÁé∞Â∑≤ËÆ≠ÁªÉ‰∏Ä‰∏™Ëø∑‰Ω†ÁöÑËØ≠Èü≥ËØÜÂà´Á≥ªÁªüÔºåÂ∞ÜÈ°πÁõÆ‰∏ãËΩΩÂà∞Êú¨Âú∞‰∏äÔºå‰∏ãËΩΩ[thchsÊï∞ÊçÆÈõÜ](http://www.openslr.org/resources/18/data_thchs30.tgz)Âπ∂Ëß£ÂéãËá≥dataÔºåËøêË°å`test.py`Ôºå‰∏çÂá∫ÊÑèÂ§ñËÉΩÂ§üËøõË°åËØÜÂà´ÔºåÁªìÊûúÂ¶Ç‰∏ãÔºö       the  0 th example.     ÊñáÊú¨ÁªìÊûúÔºö lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     ÂéüÊñáÁªìÊûúÔºö lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2     ÂéüÊñáÊ±âÂ≠óÔºö ÁªøÊòØÈò≥Êò•ÁÉüÊôØÂ§ßÂùóÊñáÁ´†ÁöÑÂ∫ïËâ≤ÂõõÊúàÁöÑÊûóÂ≥¶Êõ¥ÊòØÁªøÂæóÈ≤úÊ¥ªÁßÄÂ™öËØóÊÑèÁõéÁÑ∂     ËØÜÂà´ÁªìÊûúÔºö ÁªøÊòØÈò≥Êò•ÁÉüÊôØÂ§ßÂùóÊñáÁ´†ÁöÑÂ∫ïËâ≤ÂõõÊúàÁöÑÊûóÂ≥¶Êõ¥ÊòØÁªøÂæóÈ≤úÊ¥ªÁßÄÂ™öËØóÊÑèÁõéÁÑ∂  Ëã•Ëá™Â∑±Âª∫Á´ãÊ®°ÂûãÂàôÈúÄË¶ÅÂà†Èô§Áé∞ÊúâÊ®°ÂûãÔºåÈáçÊñ∞ÈÖçÁΩÆÂèÇÊï∞ËÆ≠ÁªÉÔºåÂÖ∑‰ΩìÂÆûÁé∞ÊµÅÁ®ãÂèÇËÄÉÊú¨È°µÊúÄÂêé„ÄÇ   |Name | train | dev | test   get source list...   make am vocab...   make lm pinyin vocab...   make lm hanzi vocab...   get source list...   make am vocab...   make lm pinyin vocab...   make lm hanzi vocab...   ÊàëÁöÑgithub: https://github.com/audier   """;Natural Language Processing;https://github.com/yumoh/speech-keras
"""""";Computer Vision;https://github.com/arnavdodiedo/dog2sketch
"""""";General;https://github.com/arnavdodiedo/dog2sketch
"""~~GPU Í∏∞Îä• Ï∂îÍ∞Ä~~   """;Computer Vision;https://github.com/Aruie/Aru_StyleGAN
"""""";General;https://github.com/ikostrikov/pytorch-rl
"""In order to download both datasets with the script  just type the following command in a terminal:  ./download_dataset.sh path_to_folder   """;Computer Vision;https://github.com/susomena/DeepSlowMotion
"""1. Download 1-Billion Word Dataset - [Link](http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz)  The Torch Data Format loads the entire dataset at once  so it requires at least 32 GB of memory. The original format partitions the dataset into smaller chunks  but it runs slower.   1. Download Google Billion Word Dataset for Torch - [Link](http://lisaweb.iro.umontreal.ca/transfert/lisa/users/leonardn/billionwords.tar.gz) 2. Run ""process_gbw.py"" on the ""train_data.th7"" file to create the ""train_data.sid"" file 3. Install Cython framework and build Log_Uniform Sampler 4. Convert Torch data tensors to PyTorch tensor format (Requires Pytorch v0.4.1)  I leverage the GBW data preprocessed for the Torch framework. (See [Torch GBW](http://torch.ch/blog/2016/07/25/nce.html)) Each data tensor contains all the words in data partition. The ""train_data.sid"" file marks the start and end positions for each independent sentence. The preprocessing step and ""train_data.sid"" file speeds up loading the massive training data.   * Data Tensors - (test_data  valid_data  train_data  train_small  train_tiny) - (#words x 2) matrix - (sentence id  word id) * Sentence ID Tensor - (#sentences x 2) matrix - (start position  sentence length)   """;General;https://github.com/rdspring1/PyTorch_GBW_LM
"""""";Computer Vision;https://github.com/shenshutao/image_segmentation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/pengshuyuan/Bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/pengshuyuan/Bert
""": gets you on average ~99.97% speed up (on cpu)   To create a model with train architecture   ```python from RepVGG import create_RepVGG_A0  train_model = create_RepVGG_A0(deploy = False) ``` To convert this model to inference / deploy architecture  ```python from RepVGG import convert  deploy_model = convert(model = train_model) ```  To create a model with inference / deploy architecture   ```python from RepVGG import create_RepVGG_A0  model = create_RepVGG_A0(deploy = True) ``` To load pretrained weights  ```python from RepVGG import create_RepVGG_A0  convert import objax  model = create_RepVGG_A0(deploy = False) objax.io.load_var_collection(""path/to/RepVGG-A0-Train.npz""  model.vars()) #: do what you want with your train model deploy_model = convert(model  save_path='RepVGG-A0-deploy.npz') #: do what you want with your deploy model ```  """;Computer Vision;https://github.com/benjaminjellis/Objax-RepVGG
"""    brew install boost cmake           sudo apt install libboost-dev libboost-program-options-dev libboost-filesystem-dev opencl-headers ocl-icd-libopencl1 ocl-icd-opencl-dev zlib1g-dev       You need a PC with a GPU  i.e. a discrete graphics card made by NVIDIA or AMD    Follow the instructions below to compile the leelaz binary  then go into   git clone https://github.com/gcp/leela-zero  cd leela-zero  git submodule update --init --recursive  git clone https://github.com/gcp/leela-zero  cd leela-zero  git submodule update --init --recursive  git clone https://github.com/gcp/leela-zero  cd leela-zero  git submodule update --init --recursive  cd msvc   to the Visual Studio version you have.   This requires a working installation of TensorFlow 1.4 or later:   [ ] Implement GPU batching.   [ ] CUDA specific version using cuDNN or cuBLAS.            mkdir build && cd build     cmake ..     cmake --build .     ./tests     curl -O https://zero.sjeng.org/best-network     ./leelaz --weights best-network            mkdir build && cd build     cmake ..     cmake --build .     ./tests     curl -O https://zero.sjeng.org/best-network     ./leelaz --weights best-network        The engine supports the [GTP protocol  version 2](https://www.lysator.liu.se/~gunnar/gtp/gtp2-spec-draft2/gtp2-spec.html).  Leela Zero is not meant to be used directly. You need a graphical interface for it  which will interface with Leela Zero through the GTP protocol.  [Lizzie](https://github.com/featurecat/lizzie/releases) is a client specifically for Leela Zero which shows live search probilities  a win rate graph  and has an automatic game analysis mode. Has binaries for Windows  Mac  and Linux.  [Sabaki](http://sabaki.yichuanshen.de/) is a very nice looking GUI with GTP 2 capability.  [LeelaSabaki](https://github.com/SabakiHQ/LeelaSabaki) is modified to show variations and winning statistics in the game tree  as well as a heatmap on the game board.  A lot of go software can interface to an engine via GTP  so look around.  Add the --gtp commandline option on the engine command line to enable Leela Zero's GTP support. You will need a weights file  specify that with the -w option.  All required commands are supported  as well as the tournament subset  and ""loadsgf"". The full set can be seen with ""list_commands"". The time control can be specified over GTP via the time\_settings command. The kgs-time\_settings extension is also supported. These have to be supplied by the GTP 2 interface  not via the command line!   At the end of the game  you can send Leela Zero a ""dump\_training"" command  followed by the winner of the game (either ""white"" or ""black"") and a filename  e.g:      dump_training white train.txt  This will save (append) the training data to disk  in the format described below  and compressed with gzip.  Training data is reset on a new game.   """;Reinforcement Learning;https://github.com/fantianwen/leela13_training
"""Run the shell script. ``` bash run_cifar.sh ``` To use Weight Vector Normalization (WVN)  use --WVN flag. (It is already in the script.)   """;Computer Vision;https://github.com/feidfoe/AdjustBnd4Imbalance
"""Run the shell script. ``` bash run_cifar.sh ``` To use Weight Vector Normalization (WVN)  use --WVN flag. (It is already in the script.)   """;General;https://github.com/feidfoe/AdjustBnd4Imbalance
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.   ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER_ORIG.py		    #: original code without doc_stride |____ BERT_NER_STRIDE.py		    #: main code with doc_stride |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    * do_lower_case=False  * num_train_epochs=4.0 * crf=False    ``` accuracy:  98.15%; precision:  90.61%; recall:  88.85%; FB1:  89.72               LOC: precision:  91.93%; recall:  91.79%; FB1:  91.86  1387              MISC: precision:  83.83%; recall:  78.43%; FB1:  81.04  668               ORG: precision:  87.83%; recall:  85.18%; FB1:  86.48  1191               PER: precision:  95.19%; recall:  94.83%; FB1:  95.01  1311 ```  ``` bash run_ner.sh ```   """;Natural Language Processing;https://github.com/anupamsingh610/bert_ner_stride
"""""";General;https://github.com/xwu6614555/MobileNetV3-Mxnet
"""""";Computer Vision;https://github.com/xwu6614555/MobileNetV3-Mxnet
"""Just another implementation with Tensorflow for paper `Spatial Transformer Netrworks`.    """;Computer Vision;https://github.com/Legend94rz/spatial-transformer
"""This repo uses [*PoolNet* (cvpr19)](https://arxiv.org/abs/1904.09569) as the baseline method for Salient Object Detection .   [Res2Net](https://github.com/gasvn/Res2Net) is a powerful backbone architecture that can be easily implemented into state-of-the-art models by replacing the bottleneck with Res2Net module. More detail can be found on [ ""Res2Net: A New Multi-scale Backbone Architecture""](https://arxiv.org/pdf/1904.01169.pdf)   cd Res2Net_PoolNet/   The pretrained models for SOD using Res2Net is now available on ONEDRIVE.   """;Computer Vision;https://github.com/Res2Net/Res2Net-PoolNet
"""""";Computer Vision;https://github.com/gvtulder/elasticdeform
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to [arXiv paper](http://arxiv.org/abs/1512.02325).  This repository contains a TensorFlow re-implementation of the original [Caffe code](https://github.com/weiliu89/caffe/tree/ssd). At present  it only implements VGG-based SSD networks (with 300 and 512 inputs)  but the architecture of the project is modular  and should make easy the implementation and training of other SSD variants (ResNet or Inception based for instance). Present TF checkpoints have been directly converted from SSD Caffe models.  The organisation is inspired by the [TF-Slim models](https://github.com/tensorflow/models/blob/master/research/inception/inception/slim/README.md) repository containing the implementation of popular architectures (ResNet  Inception and VGG).   The SSD [SSD image detection](./ssd_image_detection.py) contains a minimal example of the SSD TensorFlow pipeline. Shortly  the detection is made of two main steps: running the SSD network on the image and post-processing the output using common algorithms.  To run the [SSD image detection](./ssd_image_detection.py) you first have to unzip the checkpoint files in ./checkpoint:  ``` unzip ssd_300_vgg.ckpt.zip ```  Now run this:  ``` python ssd_image_detection.py ```  """;Computer Vision;https://github.com/amirmohammadii/SSD-object-detection
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/google-research/bert
"""""";Natural Language Processing;https://github.com/adaisti/fin-eval
"""Alternatively  if you prefer to install dependencies with pip  please follow the instructions below:  ```virtualenv -p [PATH to python3.7 binary] hgcn```  ```source hgcn/bin/activate```  ```pip install -r requirements.txt```   If you don't have conda installed  please install it following the instructions [here](https://conda.io/projects/conda/en/latest/user-guide/install/index.html).  ```git clone https://github.com/HazyResearch/hgcn```  ```cd hgcn```  ```conda env create -f environment.yml```   source set_env.sh   We provide examples of training commands used to train HGCN and other graph embedding models for link prediction and node classification. In the examples below  we used a fixed random seed set to 1234 for reproducibility purposes. Note that results might slightly vary based on the machine used. To reproduce results in the paper  run each commad for 10 random seeds and average the results.   """;Graphs;https://github.com/HazyResearch/hgcn
"""""";General;https://github.com/tanmaylaud/Patient_Conversation_Classifier_FastAI
"""""";Natural Language Processing;https://github.com/tanmaylaud/Patient_Conversation_Classifier_FastAI
"""You need to:  Download/acquire the datsets   Dataset preparation instructions can be found here.   sh run_cityscapes_experiments.sh <run> <split_rng_seed>   sh run_cityscapes_experiments.sh 02 23456  sh run_cityscapes_experiments.sh 03 34567  sh run_cityscapes_experiments.sh 04 45678  sh run_cityscapes_experiments.sh 05 56789   sh run_pascal_aug_experiments.sh <n_supervised> <n_supervised_txt>   sh run_pascal_aug_deeplab3plus_experiments.sh <n_supervised> <n_supervised_txt>   sh run_isic2017_experiments.sh <run> <split_rng_seed>   sh run_isic2017_experiments.sh 02 23456  sh run_isic2017_experiments.sh 07 78901  sh run_isic2017_experiments.sh 08 89012  sh run_isic2017_experiments.sh 09 90123   Note that running the second notebook requires that you generate some data files using the   You can re-create the toy 2D experiments by running the run_toy2d_experiments.sh shell script:   sh run_toy2d_experiments.sh <run>   """;Computer Vision;https://github.com/Britefury/cutmix-semisup-seg
"""- Dont sample from a Uniform distribution - When doing interpolations  do the interpolation via a great circle  rather than a straight line from point A to point B - Tom White's [Sampling Generative Networks](https://arxiv.org/abs/1609.04468) ref code https://github.com/dribnet/plat has more details   - Label Smoothing  i.e. if you have two target labels: Real=1 and Fake=0  then for each incoming sample  if it is real  then replace the label with a random number between 0.7 and 1.2  and if it is a fake sample  replace it with 0.0 and 0.3 (for example).   - Salimans et. al. 2016 - make the labels the noisy for the discriminator: occasionally flip the labels when training the discriminator - fake_labels = `tf.random.uniform(shape=[25  1]  minval=0  maxval=0.3  dtype=tf.float32)` - real_labels = `tf.random.uniform(shape=[25  1]  minval=0.7  maxval=1.2  dtype=tf.float32)`   - Experience Replay   - Keep a replay buffer of past generations and occassionally show them   - Keep checkpoints from the past of G and D and occassionaly swap them out for a few iterations - All stability tricks that work for deep deterministic policy gradients   - optim.Adam rules!   - See Radford et. al. 2015 - Use SGD for discriminator and ADAM for generator   - Provide noise in the form of dropout (50%). - Apply on several layers of our generator at both training and test time - https://arxiv.org/pdf/1611.07004v1.pdf  """;Computer Vision;https://github.com/surajkarki66/Generative-Model-Deep-Learning
"""Assuming that you have installed the contents of the requirements.txt file into your preferred virtual environment  proceed with these instructions in a bash shell  cd Analogies/   """;Natural Language Processing;https://github.com/AaruranLog/Analogies
"""Getting the source code   $ wget https://github.com/facebookresearch/fastText/archive/v0.2.0.zip   $ cd fastText-0.2.0  $ make   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ mkdir build &amp;&amp; cd build &amp;&amp; cmake ..  $ make &amp;&amp; make install   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ pip install .   You can also quantize a supervised model to reduce its memory usage with the following command:   You can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.  There is also the master branch that contains all of our most recent work  but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.   This library has two main use cases: word representation learning and text classification. These were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).   """;Natural Language Processing;https://github.com/FengJiaChunFromSYSU/fastText
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/RenXiangyuan/tf_bert
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/RenXiangyuan/tf_bert
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.      ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    * do_lower_case=False  * num_train_epochs=4.0 * crf=False    ``` accuracy:  98.15%; precision:  90.61%; recall:  88.85%; FB1:  89.72               LOC: precision:  91.93%; recall:  91.79%; FB1:  91.86  1387              MISC: precision:  83.83%; recall:  78.43%; FB1:  81.04  668               ORG: precision:  87.83%; recall:  85.18%; FB1:  86.48  1191               PER: precision:  95.19%; recall:  94.83%; FB1:  95.01  1311 ```  ``` bash run_ner.sh ```   """;Natural Language Processing;https://github.com/crx934080895/Bert-CRF_New2
"""""";Computer Vision;https://github.com/matanrein/Understanding-Clouds-from-Satellite-Images
"""We make use of some pretrained models  that can be downloaded [here](https://drive.google.com/file/d/1TA-UWYVDkCkNPOy1INjUU9321s-HA6RF/view?usp=sharing). They are a subset of the [models](https://drive.google.com/file/d/1aXTmN2AyNLdZ8zOeyLzpVbRHZRZD0fW0/view?usp=sharing) provided with the code of the original paper. They need to be unzipped and put in the `./pretrained` folder  in the root directory of the repo.  The dataset ([CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html)) is automatically downloaded via `torchvision.datasets` when first running the experiment  and will be saved in the `data/` folder (more info [here](https://pytorch.org/docs/stable/torchvision/datasets.html#cifar)).  The paper is implemented and tested using Python 3.7. Dependencies are listed in [requirements.txt](requirements.txt).  For the moment  it is possible to run the experiment using [VGG nets](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) and [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) as reference models and [GDAS](https://arxiv.org/pdf/1910.04465.pdf)  [WRN](https://arxiv.org/pdf/1605.07146.pdf) and [PyramidNet](https://arxiv.org/pdf/1610.02915.pdf) as victim models.  In order to test our implemenation  install the dependencies with `pip3 install --user --requirement requirements.txt`  and run the following command:  ```bash python run.py ```  This will run the experiment on line 5 of table II of our report  with the following settings:  - Reference models: AlexNet+VGGs - Victim model: GDAS - Number of images: 1000 - Maximum queries per image: 10000 - 0 seed    And hyperparameters:  - eta_g = 0.1 - eta = 1/255 - delta = 0.1 - tau = 1.0 - epsilon = 8/255  N.B.: it takes 7 hours 45 minutes to run on a Google Cloud Platform n1-highmem-8 virtual machine  with 8 vCPU  52 GB memory and an Nvidia Tesla T4.  Moreover  the following settings can be used to customize the experiment:  ```bash usage: run.py [-h] [-ds {Dataset.CIFAR_10}]                      [--reference-models {vgg11_bn vgg13_bn vgg16_bn vgg19_bn AlexNet_bn} [{vgg11_bn vgg13_bn vgg16_bn vgg19_bn AlexNet_bn} ...]]                      [--victim-model {gdas wrn pyramidnet}]                      [--loss {ExperimentLoss.CROSS_ENTROPY ExperimentLoss.NEG_LL}]                      [--tau TAU] [--epsilon EPSILON] [--delta DELTA]                      [--eta ETA] [--eta_g ETA_G] [--n-images N_IMAGES]                      [--image-limit IMAGE_LIMIT]                      [--compare-gradients COMPARE_GRADIENTS]                      [--check-success CHECK_SUCCESS]                      [--show-images SHOW_IMAGES] [--seed SEED]  optional arguments:   -h  --help            show this help message and exit   -ds {Dataset.CIFAR_10}  --dataset {Dataset.CIFAR_10}                         The dataset to be used.   --reference-models {vgg11_bn vgg13_bn vgg16_bn vgg19_bn AlexNet_bn} [{vgg11_bn vgg13_bn vgg16_bn vgg19_bn AlexNet_bn} ...]                         The reference models to be used.   --victim-model {gdas wrn pyramidnet}                         The model to be attacked.   --loss {ExperimentLoss.CROSS_ENTROPY ExperimentLoss.NEG_LL}                         The loss function to be used   --tau TAU             Bandit exploration.   --epsilon EPSILON     The norm budget.   --delta DELTA         Finite difference probe.   --eta ETA             Image learning rate.   --eta_g ETA_G         OCO learning rate.   --n-images N_IMAGES   The number of images on which the attack has to be run   --image-limit IMAGE_LIMIT                         Limit of iterations to be done for each image   --compare-gradients COMPARE_GRADIENTS                         Whether the program should output a comparison between                         the estimated and the true gradients.   --check-success CHECK_SUCCESS                         Whether the attack on each image should stop if it has                         been successful.   --show-images SHOW_IMAGES                         Whether each image to be attacked  and its                         corresponding adversarial examples should be shown   --seed SEED           The random seed with which the experiment should be                         run  to be used for reproducibility purposes. ```  In order to run an experiment on 100 images in which the loss of the true model and the cosine similarity between the estimated and true gradient  for all 5000 iterations per image  regardless of the success of the attack (i.e. the one used for figures 1 and 2 of our report)  you should run  ```bash python3 run.py --check-success=False --n-images=100 --compare-gradients=True ```  N.B.: it takes around 20 hours to run the experiment on the aforementioned machine.  The experiment results are saved in the `outputs/` folder  in a file named `YYYY-MM-DD.HH-MM.npy` a dictionary exported with `numpy.save()`. The format of the dictionary is:  ```python experiment_info = {     'experiment_baseline': {         'victim_model': victim_model_name          'reference_model_names': reference_model_names          'dataset': dataset     }      'hyperparameters': {         'tau': tau          'epsilon': epsilon          'delta': delta          'eta': eta          'eta_g': eta_g     }      'settings': {         'n_images': n_images          'image_limit': image_limit          'compare_gradients': compare_gradients          'gpu': #: If the GPU has been used for the experiment          'seed': seed     }      'results': {         'queries': #: The number of queries run         'total_time' #: The time it took to run the experiment         #: The following are present only if compare_gradients == True         'gradient_products': #: The cosine similarities for each image         'true_gradient_norms': #: The norms of the true gradients for each image         'estimated_gradient_norms': #: The norms of the estimated gradients for each image         'true_losses': #: The true losses each iteration         'common_signs': #: The percentages of common signs between true and est gradients         'subs_common_signs': #: The percentages of common signs between subsequent gradients } ```  The file can be imported in Python using `np.load(output_path  allow_pickle=True).item()`.   """;General;https://github.com/epfl-ml-reproducers/subspace-attack-reproduction
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example   PyTorch > ONNX:    TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/Pepitaw/darknet
"""For details about setting up TPUs on GCP  please see the [t5 documentation][t5_setting-up-tpus-on-gcp].  In order to run training or eval on Cloud TPUs  you must set up the following variables based on your project  zone and  GCS bucket appropriately.  ```sh export PROJECT=your_project_name export ZONE=your_project_zone export BUCKET=yourbucket export TPU_NAME=t5-tpu export BASE_DIR=gs://yourbucket/ export MODELS_DIR_NAME=your_models_dir_name export DATA_DIR_NAME=your_data_dir export DATA_RAW_DIR_NAME=your_data_raw_dir_name ```   To install the CAET5 package  clone the github repo and run:  ```sh pip install . ```   In order to compute attribute transfer accuracy and perplexity  you need to store pre-trained parametric models. CAET5 currently supports BERT classification models fine-tuned on attribute classification and GPT2 language models  by  default stored in gs://yourbucket/[metric]\_binaries/[architecture]\_[metric]\_[mixture_or_task_name].pt where [metric] is  ""acc"" or ""ppl""  and [architecture] is ""bert"" or ""gpt2"".   You may either use a new or pre-existing `Task_ll`  or you may load examples from ""raw"" text files  each containing  single attribute examples.   use the following command:   The easiest way to try out CAET5 is with a free TPU on [Colab][colab].  Below we provide examples for how to fine-tune  evaluate and infer from a model from the model API. You can use these  instructions to reproduce our results  fine-tune one of T5's released checkpoints with your own data and/or  hyperparameters.   """;Natural Language Processing;https://github.com/LeoLaugier/conditional-auto-encoder-text-to-text-transfer-transformer
"""What is a model? What is their role? I think a good model needs to have a good [Inductive Bias](https://en.wikipedia.org/wiki/Inductive_bias)  which means it has good generalization capability to unseen example during training.  The difference between the Neural Network method of learning and other learning paradigm is that the Neural Network method learns from data by making a good representation of that data. On the contrary  many other methods learn by features that are manually selected by humans.  The Transformer model is one of the most popular representation generators of Neural Network methods of learning. Because of its general representation processing mechanism such as Attention-based learning  many recent advancements of deep learning rely on it.  So what actually Transformers do? What modules comprise Transformers? What are their implications? This is a natural question of mine as a beginner.   """;General;https://github.com/hiun/learning-transformers
"""What is a model? What is their role? I think a good model needs to have a good [Inductive Bias](https://en.wikipedia.org/wiki/Inductive_bias)  which means it has good generalization capability to unseen example during training.  The difference between the Neural Network method of learning and other learning paradigm is that the Neural Network method learns from data by making a good representation of that data. On the contrary  many other methods learn by features that are manually selected by humans.  The Transformer model is one of the most popular representation generators of Neural Network methods of learning. Because of its general representation processing mechanism such as Attention-based learning  many recent advancements of deep learning rely on it.  So what actually Transformers do? What modules comprise Transformers? What are their implications? This is a natural question of mine as a beginner.   """;Natural Language Processing;https://github.com/hiun/learning-transformers
"""pip install -r requirements.txt   Python 3  PyTorch >= 1.0   The example adapts a CIFAR-10 classifier to image corruptions on CIFAR-10-C. The purpose of the example is explanation  not reproduction: exact details of the model architecture  optimization settings  etc. may differ from the paper. That said  the results should be representative  so do give it a try and experiment!  This example compares a baseline without adaptation (source)  test-time normalization for updating feature statistics during testing (norm)  and our method for entropy minimization during testing (tent). The dataset is [CIFAR-10-C](https://github.com/hendrycks/robustness/)  with 15 types and 5 levels of corruption.   See [Fighting Gradients with Gradients: Dynamic Defenses against Adversarial Attacks](https://arxiv.org/abs/2105.08714) for more details on [dent](https://github.com/DequanWang/dent).   """;Computer Vision;https://github.com/DequanWang/tent
"""""";Computer Vision;https://github.com/MasoumehVahedi/Pix2Pix-GAN-model
"""""";General;https://github.com/MasoumehVahedi/Pix2Pix-GAN-model
"""```bash pip install --upgrade pip  pip install -r requirements.txt  #: Installs the wheel compatible with Cuda 11 and cudnn 8. pip install ""jax[cuda111]<=0.21.1"" -f https://storage.googleapis.com/jax-releases/jax_releases.html ```  Also  see other configurations for CUDA [here](https://github.com/google/jax#pip-installation-gpu-cuda).   For a PyTorch reimplementation see https://github.com/rail-berkeley/rlkit/tree/master/examples/iql   """;Reinforcement Learning;https://github.com/ikostrikov/implicit_q_learning
"""""";Computer Vision;https://github.com/clairehester/face-mask-detector
"""""";General;https://github.com/clairehester/face-mask-detector
"""bash scripts/pretrain/small.sh 0 1 2 3   : bash scripts/pretrain/small.sh 0 1 2 3 --debug       #: Only run a few steps per epoch.   bash scripts/pretrain/large.sh 0 1 2 3   bash scripts/pretrain/large_frame256cont.sh 0 1 2 3   bash scripts/finetune/small_ssv2.sh 0 1 2 3   bash scripts/finetune/small_diving48.sh 0 1 2 3   bash scripts/finetune/small_ucf101.sh 0 1 2 3   bash scripts/finetune/small_hmdb51.sh 0 1 2 3  Following ViT    bash scripts/finetune/small_ssv2.sh 0 1 2 3 --different-shape --clip-len 10 --bs-per-gpu 4   bash scripts/finetune/small_ssv2.sh 0 1 2 3 --different-shape --clip-len 10 --frame-rate 4 --bs-per-gpu 4   bash scripts/finetune/large_frame128_ucf101.sh 0 1 2 3   bash scripts/finetune/large_frame256_ucf101.sh 0 1 2 3   """;Natural Language Processing;https://github.com/airsplay/vimpac
"""We propose ResRep  a novel method for lossless channel pruning (a.k.a. filter pruning)  which aims to slim down a convolutional neural network (CNN) by reducing the width (number of output channels) of convolutional layers. Inspired by the neurobiology research about the independence of remembering and forgetting  we propose to re-parameterize a CNN into the remembering parts and forgetting parts  where the former learn to maintain the performance and the latter learn for efficiency. By training the re-parameterized model using regular SGD on the former but a novel update rule with penalty gradients on the latter  we realize structured sparsity  enabling us to equivalently convert the re-parameterized model into the original architecture with narrower layers. Such a methodology distinguishes ResRep from the traditional learning-based pruning paradigm that applies a penalty on parameters to produce structured sparsity  which may suppress the parameters essential for the remembering. Our method slims down a standard ResNet-50 with 76.15% accuracy on ImageNet to a narrower one with only 45% FLOPs and no accuracy drop  which is the first to achieve lossless pruning with such a high compression ratio  to the best of our knowledge.   1. We used torch==1.3.0  torchvision==0.4.1  CUDA==10.2  NVIDIA driver version==440.82  tensorboard==1.11.0 on a machine with eight 2080Ti GPUs.    3. If you get any errors regarding tensorboard or tensorflow  you may simply delete the code related to tensorboard or SummaryWriter.   2. If some layers must be pruned following others  do that correctly.   """;Computer Vision;https://github.com/DingXiaoH/ResRep
"""[rife-ncnn-vulkan](https://github.com/nihui/rife-ncnn-vulkan) is nihui's ncnn implementation of Real-World Super-Resolution via Kernel Estimation and Noise Injection super resolution.  rife-ncnn-vulkan-python wraps [rife-ncnn-vulkan project](https://github.com/nihui/rife-ncnn-vulkan) by SWIG to make it easier to integrate rife-ncnn-vulkan with existing python projects.   First  you have to install python  python development package (Python native development libs in Visual Studio)  vulkan SDK and SWIG on your platform. And then  there are two ways to build it:  - Use setuptools to build and install into python package directly. (Currently in developing)   git clone https://github.com/ArchieMeng/rife-ncnn-vulkan-python.git  cd rife-ncnn-vulkan-python  git submodule update --init --recursive  cmake -B build src  cd build   Install visual studio and open the project directory  and build. Job done.  The only problem on Windows is that  you cannot use CMake for Windows GUI to generate the Visual Studio solution file and build it. This will make the lib crash on loading.  One way is using Visual Studio to open the project as directory  and build it from Visual Studio.    https://github.com/nothings/stb for decoding and encoding image on Linux / MacOS  https://github.com/tronkko/dirent for listing files in directory on Windows   ```shell python setup.py install ```   ```python from rife_ncnn_vulkan_python import Rife from PIL import Image  with Image.open(""input0.png"") as image0:     with Image.open(""input1.png"") as image1:       rife = Rife(gpuid=0)       image = rife.process(image0  image1)       image.save(""output.png"") ```  If you encounter a crash or error  try upgrading your GPU driver:  - Intel: https://downloadcenter.intel.com/product/80939/Graphics-Drivers - AMD: https://www.amd.com/en/support - NVIDIA: https://www.nvidia.com/Download/index.aspx   """;Computer Vision;https://github.com/media2x/rife-ncnn-vulkan-python
"""Paddle version of Paper‚ÄúPointRend: Image Segmentation as Rendering(CVPR2020)‚Äù.  This project uses Baidu's paddlepaddle framework to reproduce the CVPR2020 paper's model PointRend. **Note: only the semantic segmentation experiment of Semantic FPN + PointRend on the cityscapes dataset is done here  excluding the instance segmentation experiment of Maskrcnn + Pointrend. The correctness of PointRend based on paste reproduction is verified.**  The project relies on the paddleseg tool.  **PointRend With Seg Architecture:**  ![PointRend](./images/pointrend.png)  ![PointRend Result](./images/pointrendresult.png)   **Paper:** [PointRend: Image Segmentation as Rendering](https://arxiv.org/abs/1912.08193)   4 Environment   Step1: Install  Step1: Clone   One GPU Training   which can achieved by use PaddleSeg's create_dataset_list.py(need to clone PaddleSeg from PaddleSeg's git repo firstly):   Hardwares: XPU  GPU  CPU   pip install -r requirements.txt  ``` bash  : clone this repo(Note: maybe need to checout branch after git clone)  git clone git@github.com:CuberrChen/PointRend-Paddle.git   ``` bash   ``` bash   | Framework Version | Paddle 2.0.2 |   | Supported Hardwares | XPU GPU CPU |  | Download Links | PointRendFPN: codeÔºö33h7|   The project is developed based on Paddleseg. Except that `train.py` is modified  other `val.py` and `predict.py` are the same as Paddleseg. The model and user-defined loss function definitions are located in the `paddleseg/models` directory.   The Pre-trained model is used to test the image  For specific use  please refer to [Paddleseg doc](https://paddleseg.readthedocs.io/zh_CN/release-2.1/index.html)  The use example is as follows: ```bash #: Use Pre-trained Models to Infer python predict.py \        --config configs/pointrendfpn/pointrend_resnet101_os8_cityscapes_512√ó1024_80k.yml \        --model_path output/best_model/model.pdparams \        --image_path data/xxx/JPEGImages/0003.jpg \        --save_dir output/result ```   """;Computer Vision;https://github.com/CuberrChen/PointRend-Paddle
"""pip install -r requirements.txt ÂÆâË£ÖÊâÄÈúÄÂÆâË£ÖÂåÖ   """;Computer Vision;https://github.com/Shun14/enet
"""- To install  `cd` into the root directory and type `pip install -e .`  - Known dependencies: Python (3.5.4+)  OpenAI gym (0.10.5)  tensorflow (1.14.0)  Install my implementation of [Multi-Agent Particle Environments (MPE)] included in this repository. (https://github.com/openai/multiagent-particle-envs)  given in the repository - `cd` into multiagent-particle-envs and type `pip install -e .`  Install my implementation of [Traffic Junction] included in this repository. (https://github.com/IC3Net/IC3Net/tree/master/ic3net-envs)  given in the repository - `cd` into ic3net-envs and type `python setup.py develop`   - Cooperative Navigation with 6 SARNet Agents: `python train.py --policy-grad maddpg --env-type mpe --scenario simple_spread_6 --num_adversaries 6 --key-units 32 --value-units 32 --query-units 32 --len-traj-update 10 --td3 --PER-sampling --encoder-model LSTM --max-episode-len 100`  - Traffic Junction with 6 SARNet Agents: `python train.py --env-type ic3net --scenario traffic_junction --policy-grad reinforce --num-adversaries 6 --adv-test SARNET --gpu-device 0 --exp-name SAR-TJ6-NoCurrLr --max-episode-len 20 --num-env 50 --dim 6 --add_rate_min 0.3 --add_rate_max 0.3 --curr_start 250 --curr_end 1250 --num-episodes 500000 --batch-size 500 --difficulty easy --vision 0 --batch-size 500`   """;Reinforcement Learning;https://github.com/caslab-vt/SARNet
"""""";Computer Vision;https://github.com/tahasamavati/Cycle-GAN-Tensorflow
"""""";General;https://github.com/tahasamavati/Cycle-GAN-Tensorflow
"""Chat-bots are becoming more and more useful in various simple professional tasks as they get more and more able to capture the essence of communicating with people. Still the development of good chat-bots that will answer more complicated questions  in general subjects  is a growing domain of research.   The goal of this project is to create a chat-bot able to answer python related questions.  Our project started with the main idea being that a programming assistant would be a much needed help by many people working or studying computer science. Although it sounds simple it soon proved to be a difficult task. The main challenge is that the model has to extract a technical correlation between Questions and Answers in order to be able to communicate effectively. The model that we used in order to achieve our goal is a recurrent sequence-to-sequence model. The main steps that we followed are described bellow.  - We found  downloaded  and processed data taken from stack overflow concerning questions that contained at least one python tag.[7] - Implement a sequence-to-sequence model. - Jointly train encoder and decoder models using mini-batches - Used greedy-search decoding - Interact with trained chatbot   Now it is time to prepare our data to be fed in to the model. For this reason the following steps are followed:  - Create torch tensors of data. - Create tensors of shape (max_length  batch_size) in order to help train using mini-batches instead of 1 sentence at a time.   - Zero pad tensors to fit the maximum sentence length. - Create tensors of length for each sentence in the batch. - Create mask tensors with a value of  1 if token is not a PAD_Token else value is 0.      Keep all questions with at least one answer.   Iteratively decode one word token at a time:    """;General;https://github.com/vGkatsis/Chat_Bot_DL
"""Chat-bots are becoming more and more useful in various simple professional tasks as they get more and more able to capture the essence of communicating with people. Still the development of good chat-bots that will answer more complicated questions  in general subjects  is a growing domain of research.   The goal of this project is to create a chat-bot able to answer python related questions.  Our project started with the main idea being that a programming assistant would be a much needed help by many people working or studying computer science. Although it sounds simple it soon proved to be a difficult task. The main challenge is that the model has to extract a technical correlation between Questions and Answers in order to be able to communicate effectively. The model that we used in order to achieve our goal is a recurrent sequence-to-sequence model. The main steps that we followed are described bellow.  - We found  downloaded  and processed data taken from stack overflow concerning questions that contained at least one python tag.[7] - Implement a sequence-to-sequence model. - Jointly train encoder and decoder models using mini-batches - Used greedy-search decoding - Interact with trained chatbot   Now it is time to prepare our data to be fed in to the model. For this reason the following steps are followed:  - Create torch tensors of data. - Create tensors of shape (max_length  batch_size) in order to help train using mini-batches instead of 1 sentence at a time.   - Zero pad tensors to fit the maximum sentence length. - Create tensors of length for each sentence in the batch. - Create mask tensors with a value of  1 if token is not a PAD_Token else value is 0.      Keep all questions with at least one answer.   Iteratively decode one word token at a time:    """;Sequential;https://github.com/vGkatsis/Chat_Bot_DL
"""Chat-bots are becoming more and more useful in various simple professional tasks as they get more and more able to capture the essence of communicating with people. Still the development of good chat-bots that will answer more complicated questions  in general subjects  is a growing domain of research.   The goal of this project is to create a chat-bot able to answer python related questions.  Our project started with the main idea being that a programming assistant would be a much needed help by many people working or studying computer science. Although it sounds simple it soon proved to be a difficult task. The main challenge is that the model has to extract a technical correlation between Questions and Answers in order to be able to communicate effectively. The model that we used in order to achieve our goal is a recurrent sequence-to-sequence model. The main steps that we followed are described bellow.  - We found  downloaded  and processed data taken from stack overflow concerning questions that contained at least one python tag.[7] - Implement a sequence-to-sequence model. - Jointly train encoder and decoder models using mini-batches - Used greedy-search decoding - Interact with trained chatbot   Now it is time to prepare our data to be fed in to the model. For this reason the following steps are followed:  - Create torch tensors of data. - Create tensors of shape (max_length  batch_size) in order to help train using mini-batches instead of 1 sentence at a time.   - Zero pad tensors to fit the maximum sentence length. - Create tensors of length for each sentence in the batch. - Create mask tensors with a value of  1 if token is not a PAD_Token else value is 0.      Keep all questions with at least one answer.   Iteratively decode one word token at a time:    """;Natural Language Processing;https://github.com/vGkatsis/Chat_Bot_DL
"""See [INSTALL.md](INSTALL.md).   The designed architecture follows this guide [PyTorch-Project-Template](https://github.com/L1aoXingyu/PyTorch-Project-Template)  you can check each folder's purpose by yourself.  See [GETTING_STARTED.md](GETTING_STARTED.md).  Learn more at out [documentation](https://fast-reid.readthedocs.io/). And see [projects/](projects) for some projects that are build on top of fastreid.   """;General;https://github.com/JDAI-CV/fast-reid
"""""";Natural Language Processing;https://github.com/stefan-it/europeana-bert
"""""";General;https://github.com/stefan-it/europeana-bert
"""""";Sequential;https://github.com/stefan-it/europeana-bert
"""Required python packages are listed in `requirements.txt`. All dependencies can be installed using pip ``` pip install -r requirements.txt ``` or using conda  ``` conda install --file requirements.txt ```   By default the following configuration is run:   * --device &lt;cuda / cpu&gt;: Specify whether training should be run on GPU (if available) or CPU   where --run-path specifies the path at which the run to be evaluated is saved. Alternatively  one can also check all    """;General;https://github.com/fbuchert/fixmatch-pytorch
"""To setup an environment for running our experiments  first install [conda](https://www.anaconda.com/products/individual). Then  run the following:  ```bash $ conda env create -f environment.yml $ conda activate scientific-exaggeration-detection ```  **NOTE**: We use wandb for logging. If you do not have wandb and only wish to use it for logging  run:  ```bash $ wandb offline ```   press_release_conclusion: The conclusion sentence from the press release   $ cd data  $ bash download_strength_data.sh   """;Natural Language Processing;https://github.com/copenlu/scientific-exaggeration-detection
"""* [Neural Machine Translation with Apollo Optimizer (Ma  2020)](https://github.com/XuezheMax/fairseq-apollo/tree/master/examples/apollo)  * [Luna: Linear Unified Nested Attention (Ma et al.  2021)](https://github.com/XuezheMax/fairseq-apollo/tree/master/examples/luna)    """;General;https://github.com/XuezheMax/fairseq-apollo
"""Parameters related to training and evaluation can be set in `train.py`  as follows:  |  Parameters   | default  | description | other | |  ----  |  ----  |  ----  |  ----  | | config| None  Mandatory| Configuration file path || | --eval| None  Optional| Evaluate after an epoch |If you don't select this  you might have trouble finding the best_model| | --fp16| None  Optional| Semi-precision training |If this option is not selected  32GB of video memory may not be sufficient| | --resume| None  Optional | Recovery training |For example: --resume output/yolov2_voc/66|   Note: Make sure the best_model.pdparams file is in the output directory. ```bash python3 eval.py -c configs/yolov4/yolov4_coco_test.yml ``` Zip the bbox. Json file generated in the live directory and send it to the evaluation server   - https://github.com/AlexeyAB/darknet   - notebookÔºöhttps://aistudio.baidu.com/aistudio/projectdetail/2479219   COCO Dataset   : clone this repo  git clone https://github.com/nuaaceieyty/Paddle-YOLOv4.git  cd Paddle-YOLOv4   **Install dependencies**bash  pip install -r requestments.txt   Put the images to be tested in the data directory  run the following command  and save the output images in the Output directory; If there is a GPU in the machine environment  delete -o use_gpu=False from the command   ‚îÇ  requirements.txt                  | Framework version | Paddle 2.1.2 |   | Support hardware | GPU„ÄÅCPU |   """;Computer Vision;https://github.com/nuaaceieyty/Paddle-YOLOv4
"""""";Audio;https://github.com/anandaswarup/TTS
"""""";Computer Vision;https://github.com/Kartik-Aggarwal/Real-Time-Traffic-Sign-Detection
"""The code reproduces the qualitative and quantitative experiments in the paper. The required dependencies are listed in dependencies.txt. Note that the INTERACTION dataset for the GL intersection has to be downloaded from: https://interaction-dataset.com/ and placed into the data directory. Then  a directory: data/INTERACTION-Dataset-DR-v1_1 should exist.  To run the experiments  please run the following files:   """;Computer Vision;https://github.com/sisl/MultiAgentVariationalOcclusionInference
"""The code reproduces the qualitative and quantitative experiments in the paper. The required dependencies are listed in dependencies.txt. Note that the INTERACTION dataset for the GL intersection has to be downloaded from: https://interaction-dataset.com/ and placed into the data directory. Then  a directory: data/INTERACTION-Dataset-DR-v1_1 should exist.  To run the experiments  please run the following files:   """;General;https://github.com/sisl/MultiAgentVariationalOcclusionInference
"""""";General;https://github.com/godhj93/DenseNet
"""""";Computer Vision;https://github.com/godhj93/DenseNet
"""NOTE: You can directly open the code in Gihub Codespaces on the web to run them without downloading! Also  try github.dev.   Want to quickly learn transfer learningÔºüÊÉ≥Â∞ΩÂø´ÂÖ•Èó®ËøÅÁßªÂ≠¶‰π†ÔºüÁúã‰∏ãÈù¢ÁöÑÊïôÁ®ã„ÄÇ  - Books ‰π¶Á±ç   - **„ÄäËøÅÁßªÂ≠¶‰π†„ÄãÔºàÊù®Âº∫Ôºâ** [[Buy](https://item.jd.com/12930984.html)] [[English version](https://www.cambridge.org/core/books/transfer-learning/CCFFAFE3CDBC245047F1DEC71D9EF3C7)]   - **„ÄäËøÅÁßªÂ≠¶‰π†ÂØºËÆ∫„Äã(ÁéãÊôã‰∏ú„ÄÅÈôàÁõäÂº∫Ëëó)** [[Homepage](http://jd92.wang/tlbook)] [[Buy](https://item.jd.com/13283188.html)]  - Blogs ÂçöÂÆ¢   - [Zhihu blogs - Áü•‰πé‰∏ìÊ†è„ÄäÂ∞èÁéãÁà±ËøÅÁßª„ÄãÁ≥ªÂàóÊñáÁ´†](https://zhuanlan.zhihu.com/p/130244395) 	 - Video tutorials ËßÜÈ¢ëÊïôÁ®ã    - [Recent advance of transfer learning - 2021Âπ¥ÊúÄÊñ∞ËøÅÁßªÂ≠¶‰π†ÂèëÂ±ïÁé∞Áä∂Êé¢ËÆ®](https://www.bilibili.com/video/BV1N5411T7Sb)   - [Definitions of transfer learning area - ËøÅÁßªÂ≠¶‰π†È¢ÜÂüüÂêçËØçËß£Èáä](https://www.bilibili.com/video/BV1fu411o7BW) [[Article](https://zhuanlan.zhihu.com/p/428097044)]   - [Domain generalization - ËøÅÁßªÂ≠¶‰π†Êñ∞ÂÖ¥Á†îÁ©∂ÊñπÂêëÈ¢ÜÂüüÊ≥õÂåñ](https://www.bilibili.com/video/BV1ro4y1S7dd/)      - [Domain adaptation - ËøÅÁßªÂ≠¶‰π†‰∏≠ÁöÑÈ¢ÜÂüüËá™ÈÄÇÂ∫îÊñπÊ≥ï(‰∏≠Êñá)](https://www.bilibili.com/video/BV1T7411R75a/)    - [Transfer learning by Hung-yi Lee @ NTU - Âè∞ÊπæÂ§ßÂ≠¶ÊùéÂÆèÊØÖÁöÑËßÜÈ¢ëËÆ≤Ëß£(‰∏≠ÊñáËßÜÈ¢ë)](https://www.youtube.com/watch?v=qD6iD4TFsdQ)  - Brief introduction and slides ÁÆÄ‰ªã‰∏épptËµÑÊñô    - [Recent advance of transfer learning](http://jd92.wang/assets/files/l15_jiqizhixin.pdf)      - [Domain generalization survey](http://jd92.wang/assets/files/DGSurvey-ppt.pdf)      - [Brief introduction in Chinese](https://github.com/jindongwang/transferlearning/blob/master/doc/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B.md) 	- [PPT (English)](http://jd92.wang/assets/files/l03_transferlearning.pdf) | [PPT (‰∏≠Êñá)](http://jd92.wang/assets/files/l08_tl_zh.pdf) 	   - ËøÅÁßªÂ≠¶‰π†‰∏≠ÁöÑÈ¢ÜÂüüËá™ÈÄÇÂ∫îÊñπÊ≥ï Domain adaptation: [PDF](http://jd92.wang/assets/files/l12_da.pdf) ÔΩú [Video on Bilibili](https://www.bilibili.com/video/BV1T7411R75a/) | [Video on Youtube](https://www.youtube.com/watch?v=RbIsHNtluwQ&t=22s) 	   - Tutorial on transfer learning by Qiang Yang: [IJCAI'13](http://ijcai13.org/files/tutorial_slides/td2.pdf) | [2016 version](http://kddchina.org/file/IntroTL2016.pdf)  - Talk is cheap  show me the code Âä®ÊâãÊïôÁ®ã„ÄÅ‰ª£Á†Å„ÄÅÊï∞ÊçÆ    - [PytorchÂÆòÊñπËøÅÁßªÂ≠¶‰π†Á§∫ÊÑè‰ª£Á†Å](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) 	- [PytorchÁöÑfinetune Fine-tune based on Alexnet and Resnet](https://github.com/jindongwang/transferlearning/tree/master/code/AlexNet_ResNet) 	- [Áî®PytorchËøõË°åÊ∑±Â∫¶ÁâπÂæÅÊèêÂèñ](https://github.com/jindongwang/transferlearning/tree/master/code/feature_extractor) 	- [Êõ¥Â§ö More...](https://github.com/jindongwang/transferlearning/tree/master/code)  - [Transfer Learning Scholars and Labs - ËøÅÁßªÂ≠¶‰π†È¢ÜÂüüÁöÑËëóÂêçÂ≠¶ËÄÖ„ÄÅ‰ª£Ë°®Â∑•‰ΩúÂèäÂÆûÈ™åÂÆ§‰ªãÁªç](https://github.com/jindongwang/transferlearning/blob/master/doc/scholar_TL.md)  - [Negative transfer - Ë¥üËøÅÁßª](https://www.zhihu.com/question/66492194/answer/242870418)  - - -   """;Sequential;https://github.com/jindongwang/transferlearning
"""Source: https://github.com/bplank/2019-ma-notebook   """;General;https://github.com/bplank/teaching-dl4nlp
"""Source: https://github.com/bplank/2019-ma-notebook   """;Natural Language Processing;https://github.com/bplank/teaching-dl4nlp
"""To collect the validation set  repeat both sets of steps above  except using the directory `data/raw_sim_data/validation` instead rather than `data/raw_sim_data/train`.   1. Run QuadSim 2. Click the `DL Training` button 3. Set patrol points  path points  and spawn points. **TODO** add link to data collection doc 3. With the simulator running  press ""r"" to begin recording. 4. In the file selection menu navigate to the `data/raw_sim_data/train/run1` directory 5. **optional** to speed up data collection  press ""9"" (1-9 will slow down collection speed) 6. When you have finished collecting data  hit ""r"" to stop recording. 7. To reset the simulator  hit ""`<esc>`"" 8. To collect multiple runs create directories `data/raw_sim_data/train/run2`  `data/raw_sim_data/train/run3` and repeat the above steps.    **Clone the repository** ``` $ git clone https://github.com/udacity/RoboND-DeepLearning.git ```  **Download the data**  Save the following three files into the data folder of the cloned repository.   [Training Data](https://s3-us-west-1.amazonaws.com/udacity-robotics/Deep+Learning+Data/Lab/train.zip)   [Validation Data](https://s3-us-west-1.amazonaws.com/udacity-robotics/Deep+Learning+Data/Lab/validation.zip)  [Sample Evaluation Data](https://s3-us-west-1.amazonaws.com/udacity-robotics/Deep+Learning+Data/Project/sample_evaluation_data.zip)  We used above data training and validation for train weight for FCN.  **Download the QuadSim binary**  To interface your neural net with the QuadSim simulator  you must use a version QuadSim that has been custom tailored for this project. The previous version that you might have used for the Controls lab will not work.  The simulator binary can be downloaded [here](https://github.com/udacity/RoboND-DeepLearning/releases/latest)   Windows 8.1 64bit  Python 3.x   NumPy 1.11  SciPy 0.17.0   * Latest NVIDIA Driver 398.11  * CUDA v9.0   * https://iamaaditya.github.io/2016/03/one-by-one-convolution/   ``` 0.7365470852017937 ```  """;Computer Vision;https://github.com/oktantod/RoboND-DeepLearning-Project
"""This project is based on Python 3  [Tensorflow](https://www.tensorflow.org)  and the [OpenAI Gym environments](https://gym.openai.com). It's been tested on various Atari environments  although the basic algorithm can easily be applied to other scenarios.  To install the python requirements  run `pip3 install -r requirements.txt` (although you may want to create a [virtual environment](https://docs.python.org/3/tutorial/venv.html) first). The video recorder also requires [ffmepg](https://ffmpeg.org) which must be installed separately.  To run an environment  use e.g.      python3 atari_ppo.py --logdir=./logdata --pfile=../example-pong-params.json  With the example parameters  the agent should be able to win a perfect game of Pong in about 2 million frames  which closely matches the results from the OpenAI baseline implementation. Other environments can be used by modifying the parameters file. To view the training progress  use tensorboard:      tensorboard --logdir=./logdata    ![a game of pong](./pong.gif) ![a game of space invaders](./space_invaders.gif)    """;Reinforcement Learning;https://github.com/clwainwright/proximal_policy_optimization
"""1. Clone the py-RFCN-priv repository     ```Shell     git clone https://github.com/soeaver/py-RFCN-priv     ```     We'll call the directory that you cloned py-RFCN-priv into `PRIV_ROOT`  2. Build the Cython modules     ```Shell     cd $PRIV_ROOT/lib     make     ```      3. Build Caffe and pycaffe     ```Shell     cd $RFCN_ROOT/caffe-priv     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html          #: cp Makefile.config.example Makefile.config     #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make all -j && make pycaffe -j    ```            **Note:** Caffe *must* be built with support for Python layers!     ```make     #: In your Makefile.config  make sure to have this line uncommented     WITH_PYTHON_LAYER := 1     #: Unrelatedly  it's also recommended that you use CUDNN     USE_CUDNN := 1     #: NCCL (https://github.com/NVIDIA/nccl) is necessary for multi-GPU training with python layer     USE_NCCL := 1     ```         **How to install nccl**     ```     git clone https://github.com/NVIDIA/nccl.git     cd nccl     sudo make install -j     sudo ldconfig     ```         """;Computer Vision;https://github.com/soeaver/py-RFCN-priv
"""If the test file has sentiment labels  just run the following command:   And then  run the follow code to get the transferred review:bash   1. Run the http server to allow the js. script. ```bash python3 -m web/run_server.sh & ``` 2. Visit web/demo.htm to watch the demo.  ***************************************************************   """;General;https://github.com/sy-sunmoon/Clever-Commenter-Let-s-Try-More-Apps
"""Traditional [WGAN](https://arxiv.org/abs/1701.07875) uses an approximation of the Wasserstein metric to opimize the generator. This Wasserstein metric in turn depends upon an underlying metric on _images_ which is taken to be the <img src=""https://latex.codecogs.com/svg.latex?%5Cell%5E2""> norm  <img src=""https://latex.codecogs.com/svg.latex?%5C%7Cx%5C%7C_%7B2%7D%20%3D%20%5Cleft%28%20%5Csum_%7Bi%3D1%7D%5En%20x_i%5E2%20%5Cright%29%5E%7B1/2%7D"">  The article extends the theory of [WGAN-GP](https://arxiv.org/abs/1704.00028) to any [Banach space](https://en.wikipedia.org/wiki/Banach_space)  while this code can be used to train WGAN over any [Sobolev space](https://en.wikipedia.org/wiki/Sobolev_space) <img src=""https://latex.codecogs.com/svg.latex?W%5E%7Bs%2C%20p%7D""> with norm  <img src=""https://latex.codecogs.com/svg.latex?%5C%7Cf%5C%7C_%7BW%5E%7Bs%2C%20p%7D%7D%20%3D%20%5Cleft%28%20%5Cint_%7B%5COmega%7D%20%5Cleft%28%20%5Cmathcal%7BF%7D%5E%7B-1%7D%20%5Cleft%5B%20%281%20&plus;%20%7C%5Cxi%7C%5E2%29%5E%7Bs/2%7D%20%5Cmathcal%7BF%7D%20f%20%5Cright%5D%28x%29%20%5Cright%29%5Ep%20dx%20%5Cright%29%5E%7B1/p%7D"">  The parameters _p_ can be used to control the focus on outliers  with high _p_ indicating a strong focus on the worst offenders. _s_ can be used to control focus on small/large scale behaviour  where negative _s_ indicates focus on large scales  while positive _s_ indicates focus on small scales (e.g. edges).   The code has some dependencies that can be easily installed  $ pip install https://github.com/adler-j/tensordata/archive/master.zip  $ pip install https://github.com/adler-j/adler/archive/master.zip   """;General;https://github.com/adler-j/bwgan
"""Traditional [WGAN](https://arxiv.org/abs/1701.07875) uses an approximation of the Wasserstein metric to opimize the generator. This Wasserstein metric in turn depends upon an underlying metric on _images_ which is taken to be the <img src=""https://latex.codecogs.com/svg.latex?%5Cell%5E2""> norm  <img src=""https://latex.codecogs.com/svg.latex?%5C%7Cx%5C%7C_%7B2%7D%20%3D%20%5Cleft%28%20%5Csum_%7Bi%3D1%7D%5En%20x_i%5E2%20%5Cright%29%5E%7B1/2%7D"">  The article extends the theory of [WGAN-GP](https://arxiv.org/abs/1704.00028) to any [Banach space](https://en.wikipedia.org/wiki/Banach_space)  while this code can be used to train WGAN over any [Sobolev space](https://en.wikipedia.org/wiki/Sobolev_space) <img src=""https://latex.codecogs.com/svg.latex?W%5E%7Bs%2C%20p%7D""> with norm  <img src=""https://latex.codecogs.com/svg.latex?%5C%7Cf%5C%7C_%7BW%5E%7Bs%2C%20p%7D%7D%20%3D%20%5Cleft%28%20%5Cint_%7B%5COmega%7D%20%5Cleft%28%20%5Cmathcal%7BF%7D%5E%7B-1%7D%20%5Cleft%5B%20%281%20&plus;%20%7C%5Cxi%7C%5E2%29%5E%7Bs/2%7D%20%5Cmathcal%7BF%7D%20f%20%5Cright%5D%28x%29%20%5Cright%29%5Ep%20dx%20%5Cright%29%5E%7B1/p%7D"">  The parameters _p_ can be used to control the focus on outliers  with high _p_ indicating a strong focus on the worst offenders. _s_ can be used to control focus on small/large scale behaviour  where negative _s_ indicates focus on large scales  while positive _s_ indicates focus on small scales (e.g. edges).   The code has some dependencies that can be easily installed  $ pip install https://github.com/adler-j/tensordata/archive/master.zip  $ pip install https://github.com/adler-j/adler/archive/master.zip   """;Computer Vision;https://github.com/adler-j/bwgan
"""`python preprocessing.py --in_dir ljspeech --out_dir DATASETS/ljspeech`   """;Audio;https://github.com/rickyHong/ClariNet-WaveNet-repl
"""`python preprocessing.py --in_dir ljspeech --out_dir DATASETS/ljspeech`   """;Sequential;https://github.com/rickyHong/ClariNet-WaveNet-repl
"""""";Reinforcement Learning;https://github.com/ailab-pku/rl-framework
"""1. Carefully follow the instructions of the offical implementation for Deformable Convolutional Networks based on MXNet [here](https://github.com/msracver/Deformable-ConvNets).   At the end of this step:   * The Deformable ConvNets repo should have been downloaded;  * MXNet should have been downloaded and properly compiled;  * The R-FCN demo should be able to run by `python ./rfcn/demo.py`.  Note:   For installing MXnet on NVIDIA Jetson TX2  if the above [installation instruction](https://github.com/msracver/Deformable-ConvNets) doesn't work  you can try following these steps (The MXnet on Jetson TX2 installation has been verified by [Koray](https://www.linkedin.com/in/koray-ozcan-b003903b/)):  a. MxNet is only compatible with opencv-python >= 3.2.0. Please first upgrade the OpenCV libraries for Jetson platform as described in [here](http://www.jetsonhacks.com/2017/04/05/build-opencv-nvidia-jetson-tx2/)  b. Then install MXnet libraries for Jetson TX2 as described [here](http://mxnet.io/get_started/install.html) (Devices - NVIDIA Jetson TX2).    2. clone this repo into the root/parent directory of Deformable-ConvNets:  `cd path/to/Deformable-ConvNets/..`  `git clone https://github.com/wkelongws/RDFCN_UADETRAC_AICITY`  3. implement the contribution codes in this repo to the Deformable-ConvNets folder:  `cd RDFCN_UADETRAC_AICITY`   `python setup.py`   Ubuntu 16.04  NVIDIA TITIAN X GPU  Python 2.7 (both training and inference).    Finish the Installation   cd RDFCN_UADETRAC_AICITY   cd Deformable-ConvNets   cd Deformable-ConvNets   1. Download data from UADETRAC and AICity into `data/` folder  2. Create configuration file  3. `cd path/to/Deformable-ConvNets`  4. Model training and Inference  * To train model from scratch (for both AICITY and UADETRAC)  run:  `python experiments/rfcn/rfcn_end2end_train_Shuo_UADETRAC.py --cfg path/to/your/configuration/file`  Note: For AICITY  based on our experience ""training from scratch"" doesn't yield good model. Please refer to ""Transfer Learning"" section for other solution.  * To detect vechiles from the test dataset  run:  `python experiments/rfcn/rfcn_Inference_Shuo_UADETRAC.py --cfg path/to/your/configuration/file`  `python experiments/rfcn/rfcn_Inference_Shuo_AICity.py --cfg path/to/your/configuration/file`  Two sample configuration files  one for UADETRAC and one for AICity  have been added to `experiments/rfcn/cfgs/`  The weights of submitted model (and the corresponding configuration file) on aic1080 can be downloaded [here](https://1drv.ms/u/s!AmGvrNE6VIsKjUevX-7tNAhS_5AF).  The weights of submitted model (and the corresponding configuration file) on aic540 can be downloaded [here](https://1drv.ms/u/s!AmGvrNE6VIsKjUmeEitETomFbCXQ).  The weights of submitted model (and the corresponding configuration file) on aic480 can be downloaded [here](https://1drv.ms/u/s!AmGvrNE6VIsKjUiF1J2qP4TeNsRc).  The weights of submitted model (and the corresponding configuration file) on UADETRAC can be downloaded [here](https://1drv.ms/u/s!AmGvrNE6VIsKjVK3j5kV6BfDghAU).   """;Computer Vision;https://github.com/NVIDIAAICITYCHALLENGE/AICity_Team6_ISU
"""""";General;https://github.com/clrrrr/promp_plus
"""To run the models you have to download the repo  install the requirements  and extract the datasets.  First  let's create a python environment: ```bash mkdir envs cd envs python -m venv bgnn_env source bgnn_env/bin/activate cd .. ``` --- Second  let's download the code and install requirements ```bash git clone https://github.com/nd7141/bgnn.git  cd bgnn unzip datasets.zip make install ``` --- Next we need to install a proper version of [PyTorch](https://pytorch.org/) and [DGL](https://www.dgl.ai/)  depending on the cuda version of your machine. We strongly encourage to use GPU-supported versions of DGL (the speed up in training can be 100x).  First  determine your cuda version with `nvcc --version`.  Then  check installation instructions for [pytorch](https://pytorch.org/get-started/locally/). For example for cuda version 9.2  install it as follows: ```bash pip install torch==1.7.1+cu92 torchvision==0.8.2+cu92 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html ```  If you don't have GPU  use the following:  ```bash pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html ``` --- Similarly  you need to install [DGL library](https://docs.dgl.ai/en/0.4.x/install/).  For example  cuda==9.2:  ```bash pip install dgl-cu92 ```  For cpu version of DGL:  ```bash pip install dgl ```  Tested versions of `torch` and `dgl` are: * torch==1.7.1+cu92 * dgl_cu92==0.5.3   """;General;https://github.com/nd7141/bgnn
"""`myolo` - the main implementation of Mask-YOLO. [model.py](https://github.com/jianing-sun/Mask-YOLO/blob/master/myolo/model.py) is the model instantiation.   `example` - including three training examples with inference: **Shapes** dataset is randomly generated by [dataset_shapes.py](https://github.com/jianing-sun/Mask-YOLO/blob/master/example/shapes/dataset_shapes.py). **Rice** and **Food** are small datasets I hand-annotated by [VGG Image Annotator (VIA)](http://www.robots.ox.ac.uk/~vgg/software/via/)  and can be downloaded from https://drive.google.com/file/d/1druK4Kgx5AhfchClU2aq5kf7UVoDtkvu/view.   """;Computer Vision;https://github.com/jianing-sun/Mask-YOLO
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   For R-FCN/Faster R-CNN\: 1. Please download COCO and VOC 2007+2012 datasets  and make sure it looks like this:  	``` 	./data/coco/ 	./data/VOCdevkit/VOC2007/ 	./data/VOCdevkit/VOC2012/ 	```  2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  For DeepLab\: 1. Please download Cityscapes and VOC 2012 datasets and make sure it looks like this:  	``` 	./data/cityscapes/ 	./data/VOCdevkit/VOC2012/ 	``` 2. Please download argumented VOC 2012 annotations/image lists  and put the argumented annotations and the argumented train/val lists into:  	``` 	./data/VOCdevkit/VOC2012/SegmentationClass/ 	./data/VOCdevkit/VOC2012/ImageSets/Main/ 	```      Respectively.     2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  1. Clone the Deformable ConvNets repository  and we'll call the directory that you cloned Deformable-ConvNets as ${DCN_ROOT}. ``` git clone https://github.com/msracver/Deformable-ConvNets.git ```  2. For Windows users  run ``cmd .\init.bat``. For Linux user  run `sh ./init.sh`. The scripts will build cython module automatically and create some folders.  3. Install MXNet: 	 	**Note: The MXNet's Custom Op cannot execute parallelly using multi-gpus after this [PR](https://github.com/apache/incubator-mxnet/pull/6928). We strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.5).**  	***Quick start***  	3.1 Install MXNet and all dependencies by  	``` 	pip install -r requirements.txt 	``` 	If there is no other error message  MXNet should be installed successfully.  	 	***Build from source (alternative way)***  	3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by 	``` 	git clone --recursive https://github.com/dmlc/mxnet.git 	git checkout 998378a 	git submodule update 	#: if it's the first time to checkout  just use: git submodule update --init --recursive 	``` 	3.3 Compile MXNet 	``` 	cd ${MXNET_ROOT} 	make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1 	``` 	3.4 Install the MXNet Python binding by 	 	***Note: If you will actively switch between different versions of MXNet  please follow 3.5 instead of 3.4*** 	``` 	cd python 	sudo python setup.py install 	``` 	3.5 For advanced users  you may put your Python packge into `./external/mxnet/$(YOUR_MXNET_PACKAGE)`  and modify `MXNET_VERSION` in `./experiments/rfcn/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`. Thus you can switch among different versions of MXNet quickly.  4. For Deeplab  we use the argumented VOC 2012 dataset. The argumented annotations are provided by [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html) dataset. For convenience  we provide the converted PNG annotations and the lists of train/val images  please download them from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMRhVImMI1jRrsxDg).   We provide trained deformable convnet models  including the deformable R-FCN & Faster R-CNN models trained on COCO trainval  and the deformable DeepLab model trained on CityScapes train.  1. To use the demo with our pre-trained deformable models  please download manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMSjehIcCgAhvEAHw) or [BaiduYun](https://pan.baidu.com/s/1dFlPFED)  and put it under folder `model/`.  	Make sure it looks like this: 	``` 	./model/rfcn_dcn_coco-0000.params 	./model/rfcn_coco-0000.params 	./model/fpn_dcn_coco-0000.params 	./model/fpn_coco-0000.params 	./model/rcnn_dcn_coco-0000.params 	./model/rcnn_coco-0000.params 	./model/deeplab_dcn_cityscapes-0000.params 	./model/deeplab_cityscapes-0000.params 	./model/deform_conv-0000.params 	./model/deform_psroi-0000.params 	``` 2. To run the R-FCN demo  run 	``` 	python ./rfcn/demo.py 	``` 	By default it will run Deformable R-FCN and gives several prediction results  to run R-FCN  use 	``` 	python ./rfcn/demo.py --rfcn_only 	``` 3. To run the DeepLab demo  run 	``` 	python ./deeplab/demo.py 	``` 	By default it will run Deformable Deeplab and gives several prediction results  to run DeepLab  use 	``` 	python ./deeplab/demo.py --deeplab_only 	``` 4. To visualize the offset of deformable convolution and deformable psroipooling  run 	``` 	python ./rfcn/deform_conv_demo.py 	python ./rfcn/deform_psroi_demo.py 	```    1. All of our experiment settings (GPU #  dataset  etc.) are kept in yaml config files at folder `./experiments/rfcn/cfgs`  `./experiments/faster_rcnn/cfgs` and `./experiments/deeplab/cfgs/`. 2. Eight config files have been provided so far  namely  R-FCN for COCO/VOC  Deformable R-FCN for COCO/VOC  Faster R-CNN(2fc) for COCO/VOC  Deformable Faster R-CNN(2fc) for COCO/VOC  Deeplab for Cityscapes/VOC and Deformable Deeplab for Cityscapes/VOC  respectively. We use 8 and 4 GPUs to train models on COCO and on VOC for R-FCN  respectively. For deeplab  we use 4 GPUs for all experiments.  3. To perform experiments  run the python scripts with the corresponding config file as input. For example  to train and test deformable convnets on COCO with ResNet-v1-101  use the following command     ```     python experiments\rfcn\rfcn_end2end_train_test.py --cfg experiments\rfcn\cfgs\resnet_v1_101_coco_trainval_rfcn_dcn_end2end_ohem.yaml     ```     A cache folder would be created automatically to save the model and the log under `output/rfcn_dcn_coco/`. 4. Please find more details in config files and in our code.   """;Computer Vision;https://github.com/xiaoyongzhu/Deformable-ConvNets
"""""";Computer Vision;https://github.com/16xccheng/keras-unet
"""TorchNLP requires a minimum of Python 3.5 and PyTorch 0.4.0 to run. Check [Pytorch](http://pytorch.org/) for the installation steps.  Clone this repository and install other dependencies like TorchText: ``` pip install -r requirements.txt ``` Go to the root of the project and check for integrity with PyTest: ``` pytest ``` Install this project: ``` python setup.py ```   TorchNLP is designed to be used inside the python interpreter to make it easier to experiment without typing cumbersome command line arguments.   **NER Task**  The NER task can be run on any dataset that confirms to the [CoNLL 2003](https://www.clips.uantwerpen.be/conll2003/ner/) format. To use the CoNLL 2003 NER dataset place the dataset files in the following directory structure within your workspace root: ``` .data   |   |---conll2003           |           |---eng.train.txt           |---eng.testa.txt           |---eng.testb.txt ``` `eng.testa.txt` is used the validation dataset and `eng.testb.txt` is used as the test dataset.  Start the NER module in the python shell which sets up the imports: ``` python -i -m torchnlp.ner ``` ``` Task: Named Entity Recognition  Available models: ------------------- TransformerTagger      Sequence tagger using the Transformer network (https://arxiv.org/pdf/1706.03762.pdf)     Specifically it uses the Encoder module. For character embeddings (per word) it uses     the same Encoder module above which an additive (Bahdanau) self-attention layer is added  BiLSTMTagger      Sequence tagger using bidirectional LSTM. For character embeddings per word     uses (unidirectional) LSTM   Available datasets: -------------------     conll2003: Conll 2003 (Parser only. You must place the files)  >>> ```  Train the [Transformer](https://arxiv.org/abs/1706.03762) model on the CoNLL 2003 dataset: ``` >>> train('ner-conll2003'  TransformerTagger  conll2003) ``` The first argument is the task name. You need to use the same task name during evaluation and inference. By default the train function will use the F1 metric with a window of 5 epochs to perform early stopping. To change the early stopping criteria set the `PREFS` global variable as follows: ``` >>> PREFS.early_stopping='lowest_3_loss' ``` This will now use validation loss as the stopping criteria with a window of 3 epochs. The model files are saved under *taskname-modelname* directory. In this case it is *ner-conll2003-TransformerTagger*  Evaluate the trained model on the *testb* dataset split: ``` >>> evaluate('ner-conll2003'  TransformerTagger  conll2003  'test') ``` It will display metrics like accuracy  sequence accuracy  F1 etc  Run the trained model interactively for the ner task: ``` >>> interactive('ner-conll2003'  TransformerTagger) ... Ctrl+C to quit > Tom went to New York I-PER O O I-LOC I-LOC ``` You can similarly train the bidirectional LSTM CRF model by using the `BiLSTMTagger` class. Customizing hyperparameters is quite straight forward. Let's look at the hyperparameters for `TransformerTagger`: ``` >>> h2 = hparams_transformer_ner() >>> h2  Hyperparameters:  filter_size=128  optimizer_adam_beta2=0.98  learning_rate=0.2  learning_rate_warmup_steps=500  input_dropout=0.2  embedding_size_char=16  dropout=0.2  hidden_size=128  optimizer_adam_beta1=0.9  embedding_size_word=300  max_length=256  attention_dropout=0.2  relu_dropout=0.2  batch_size=100  num_hidden_layers=1  attention_value_channels=0  attention_key_channels=0  use_crf=True  embedding_size_tags=100  learning_rate_decay=noam_step  embedding_size_char_per_word=100  num_heads=4  filter_size_char=64  ```  Now let's disable the CRF layer:  ```  >>> h2.update(use_crf=False)  Hyperparameters:  filter_size=128  optimizer_adam_beta2=0.98  learning_rate=0.2  learning_rate_warmup_steps=500  input_dropout=0.2  embedding_size_char=16  dropout=0.2  hidden_size=128  optimizer_adam_beta1=0.9  embedding_size_word=300  max_length=256  attention_dropout=0.2  relu_dropout=0.2  batch_size=100  num_hidden_layers=1  attention_value_channels=0  attention_key_channels=0  use_crf=False  embedding_size_tags=100  learning_rate_decay=noam_step  embedding_size_char_per_word=100  num_heads=4  filter_size_char=64  ```  Use it to re-train the model:  ```  >>> train('ner-conll2003-nocrf'  TransformerTagger  conll2003  hparams=h2)  ```  Along with the model the hyperparameters are also saved so there is no need to pass the `HParams` object during evaluation. Also note that by default it will not overwrite any existing model directories (will rename instead). To change that behavior set the PREFS variable:  ```  >>> PREFS.overwrite_model_dir = True  ```  The `PREFS` variable is automatically persisted in `prefs.json`    **Chunking Task**    The [CoNLL 2000](https://www.clips.uantwerpen.be/conll2000/chunking/) dataset is available for the Chunking task. The dataset is automatically downloaded from the public repository so you don't need to manually download it.  Start the Chunking task: ``` python -i -m torchnlp.chunk ``` Train the [Transformer](https://arxiv.org/abs/1706.03762) model: ``` >>> train('chunk-conll2000'  TransformerTagger  conll2000) ``` There is no validation partition provided in the repository hence 10% of the training set is used for validation.  Evaluate the model on the test set: ``` >>> evaluate('chunk-conll2000'  TransformerTagger  conll2000  'test') ```     The `transformer.Encoder`  `transformer.Decoder` and `CRF` modules can be independently imported as they only depend on PyTorch:  ```  from torchnlp.modules.transformer import Encoder  from torchnlp.modules.transformer import Decoder  from torchnlp.modules.crf import CRF  ``` Please refer to the comments within the source code for more details on the usage  """;General;https://github.com/kolloldas/torchnlp
"""TorchNLP requires a minimum of Python 3.5 and PyTorch 0.4.0 to run. Check [Pytorch](http://pytorch.org/) for the installation steps.  Clone this repository and install other dependencies like TorchText: ``` pip install -r requirements.txt ``` Go to the root of the project and check for integrity with PyTest: ``` pytest ``` Install this project: ``` python setup.py ```   TorchNLP is designed to be used inside the python interpreter to make it easier to experiment without typing cumbersome command line arguments.   **NER Task**  The NER task can be run on any dataset that confirms to the [CoNLL 2003](https://www.clips.uantwerpen.be/conll2003/ner/) format. To use the CoNLL 2003 NER dataset place the dataset files in the following directory structure within your workspace root: ``` .data   |   |---conll2003           |           |---eng.train.txt           |---eng.testa.txt           |---eng.testb.txt ``` `eng.testa.txt` is used the validation dataset and `eng.testb.txt` is used as the test dataset.  Start the NER module in the python shell which sets up the imports: ``` python -i -m torchnlp.ner ``` ``` Task: Named Entity Recognition  Available models: ------------------- TransformerTagger      Sequence tagger using the Transformer network (https://arxiv.org/pdf/1706.03762.pdf)     Specifically it uses the Encoder module. For character embeddings (per word) it uses     the same Encoder module above which an additive (Bahdanau) self-attention layer is added  BiLSTMTagger      Sequence tagger using bidirectional LSTM. For character embeddings per word     uses (unidirectional) LSTM   Available datasets: -------------------     conll2003: Conll 2003 (Parser only. You must place the files)  >>> ```  Train the [Transformer](https://arxiv.org/abs/1706.03762) model on the CoNLL 2003 dataset: ``` >>> train('ner-conll2003'  TransformerTagger  conll2003) ``` The first argument is the task name. You need to use the same task name during evaluation and inference. By default the train function will use the F1 metric with a window of 5 epochs to perform early stopping. To change the early stopping criteria set the `PREFS` global variable as follows: ``` >>> PREFS.early_stopping='lowest_3_loss' ``` This will now use validation loss as the stopping criteria with a window of 3 epochs. The model files are saved under *taskname-modelname* directory. In this case it is *ner-conll2003-TransformerTagger*  Evaluate the trained model on the *testb* dataset split: ``` >>> evaluate('ner-conll2003'  TransformerTagger  conll2003  'test') ``` It will display metrics like accuracy  sequence accuracy  F1 etc  Run the trained model interactively for the ner task: ``` >>> interactive('ner-conll2003'  TransformerTagger) ... Ctrl+C to quit > Tom went to New York I-PER O O I-LOC I-LOC ``` You can similarly train the bidirectional LSTM CRF model by using the `BiLSTMTagger` class. Customizing hyperparameters is quite straight forward. Let's look at the hyperparameters for `TransformerTagger`: ``` >>> h2 = hparams_transformer_ner() >>> h2  Hyperparameters:  filter_size=128  optimizer_adam_beta2=0.98  learning_rate=0.2  learning_rate_warmup_steps=500  input_dropout=0.2  embedding_size_char=16  dropout=0.2  hidden_size=128  optimizer_adam_beta1=0.9  embedding_size_word=300  max_length=256  attention_dropout=0.2  relu_dropout=0.2  batch_size=100  num_hidden_layers=1  attention_value_channels=0  attention_key_channels=0  use_crf=True  embedding_size_tags=100  learning_rate_decay=noam_step  embedding_size_char_per_word=100  num_heads=4  filter_size_char=64  ```  Now let's disable the CRF layer:  ```  >>> h2.update(use_crf=False)  Hyperparameters:  filter_size=128  optimizer_adam_beta2=0.98  learning_rate=0.2  learning_rate_warmup_steps=500  input_dropout=0.2  embedding_size_char=16  dropout=0.2  hidden_size=128  optimizer_adam_beta1=0.9  embedding_size_word=300  max_length=256  attention_dropout=0.2  relu_dropout=0.2  batch_size=100  num_hidden_layers=1  attention_value_channels=0  attention_key_channels=0  use_crf=False  embedding_size_tags=100  learning_rate_decay=noam_step  embedding_size_char_per_word=100  num_heads=4  filter_size_char=64  ```  Use it to re-train the model:  ```  >>> train('ner-conll2003-nocrf'  TransformerTagger  conll2003  hparams=h2)  ```  Along with the model the hyperparameters are also saved so there is no need to pass the `HParams` object during evaluation. Also note that by default it will not overwrite any existing model directories (will rename instead). To change that behavior set the PREFS variable:  ```  >>> PREFS.overwrite_model_dir = True  ```  The `PREFS` variable is automatically persisted in `prefs.json`    **Chunking Task**    The [CoNLL 2000](https://www.clips.uantwerpen.be/conll2000/chunking/) dataset is available for the Chunking task. The dataset is automatically downloaded from the public repository so you don't need to manually download it.  Start the Chunking task: ``` python -i -m torchnlp.chunk ``` Train the [Transformer](https://arxiv.org/abs/1706.03762) model: ``` >>> train('chunk-conll2000'  TransformerTagger  conll2000) ``` There is no validation partition provided in the repository hence 10% of the training set is used for validation.  Evaluate the model on the test set: ``` >>> evaluate('chunk-conll2000'  TransformerTagger  conll2000  'test') ```     The `transformer.Encoder`  `transformer.Decoder` and `CRF` modules can be independently imported as they only depend on PyTorch:  ```  from torchnlp.modules.transformer import Encoder  from torchnlp.modules.transformer import Decoder  from torchnlp.modules.crf import CRF  ``` Please refer to the comments within the source code for more details on the usage  """;Natural Language Processing;https://github.com/kolloldas/torchnlp
"""As is  PySC2 branch version 7a04e74 is installed via anaconda on Ubuntu 16.04.  Starcraft II client is 3.16.1 on Ubuntu.  Branch has been modified to support the learning Agent.  This is a work in progress and the AI can be started by way of calling ``` python deepmind_api_modified/pysc2/bin/agent.py ``` This hasn't been tested on other systems and may not work.  Updates will come relatively frequently as this is updated.  **What's Your Project**  Starcraft II Asynchronous Advantage Actor Critic reinforcement learner for PySCII API version 1.1  **What's The Problem?**  Given the complexity of the game (refer to the bottom section: Introduction To The Game for an overall description of the game)  it represents a testbed for testing reinforcement learning artificial intelligence  and given the complexity  it is expected that a successful AI will be revolutionary.  In particular  a well-known name in AI and especially reinforcement learning  DeepMind is taking a forefront position by developing APIs to give a computer agent access to play the game.  So far  progress by DeepMind has been tight-lipped with few publications apart from the initial statement of using Starcraft II as a testbed for developing AI  but the APIs have been released to the public and projects have been encouraged.    I'm very excited to delve deeper into the world of reinforcement learning  commonly regarded to be the possible holy grail of AI in terms of having the potential to be a general learner  and to possibly make a 'best in the world' player and do the same thing that AI has done to games like Chess and Go.    The beauty of the reinforcement learner problem is that a good reinforcement learner should be able to learn the game without any programmer domain knowledge and because of that be able to teach itself without any domain specific hacks and also find it's own novel solutions to problems that humans haven't thought of yet.    **How Did You Do It?**  The starting point of this project is not gathering data  unlike many other data-science projects.  Since reinforcement learners can learn from their past actions  and the API at this time did not support loading in of replays for imitation learning  despite the fact that large replay datasets did exist  I was unable to make use of those and had to focus on pure reinforcement learning.  The starting point was learning the software stack that I had to use  ie. the API.  APIs exist for C++  Javascript  and Python.  My only known language is Python so choosing an API was simple.  Due to the nature of the game development  there was an initial API built so that the game could process bot inputs built on Google's Protocool Buffers extensible mechanisms.  This is the C++ API and all others were built on top of this.  This led to a less than neat design to the code  and such that it was and still is in very active development  an understanding of the code took some time.  Fortunately  there was a major resource in the form of Steven Brown's tutorials in which he took the Python API and built a deep Q-learning agent in a limited action pool.  This is in part  a basis of the action pool the current iteration of my bot uses.    The project itself had a time duration of two weeks  and I had initially planned on spending one of the two weeks building out a robust action pool for the bot to work off.  Unfortunately  after the week was mostly spent  I was struggling with some of the technical limitations of the current version of the API  the largest issue being how data is passed to the bot.  The bot sees the landscape from a top down view with about 23 layers of arrays that each contain a different 'slice' of data.  This is useful for many applications; however  individual unit IDs were not implemented to be used by the agent at this time  and so to give orders to the economy based SCVs that have the ability to either mine Mineral Crystals or Vespene Gas (both being essential resources to buy buildings and units in the game)  while it was relatively simple to keep SCVs mining the more essential of the resources  Mineral Crystals  by randomly selecting pixels identified as SCVs and sending them to Mineral Crystals  if Vespene Geysers were allowed to be mined  all of the SCVs would eventually be clumped on the Vespene Geysers  either leading to game crashes  or all the SCVs on the Geyser and none on Minerals.  A solution could possibly be creating a machine vision agent whose sole purpose is to tag individual units  but this task would be an entire project in itself.  In the end  given that Minerals are the more essential resource (All the most basic  essential units and buildings use only Minerals)  I simply expanded the action pool to allow more basic things to be built so that I could focus on the agent (the brain and learning component) itself.    In Steven Brown's tutorial  Deep Q-Learning was implemented from Morvan Zhou (https://github.com/MorvanZhou).  I won't touch on Q-Learning or Deep Q for the sake of brevity  however feel free to check out Morvan's Github  it is a collection of very thorough ANN tutorials.  I needed to replace this agent.  In the past year  DeepMind has published a very interesting Reinforcement Learning Agent called Asynchronous-Advantage-Actor-Critic  aka A3C.  For more information on A3C  please refer to this paper by Deepmind.  [https://arxiv.org/pdf/1602.01783.pdf]: https://arxiv.org/pdf/1602.01783.pdf  Most of the work revolved around several major parts: Iniitalizing a global agent  starting up in tandem multiple PySC2 enviroments with environment agents and A2C agents for the environment agent  linking everything together as a properly threaded package (PySC2 allows multiple agents to run at once  but not talk to each other)  and tuning hyperparameters.  The restrictions I placed on the bot to minimise parameters were such: player race limitation was set to Terran  enemy difficulty was set to easiest  enemy race was set to Terran  map was set to Simple64 (a small  basic map)  action pool was very limited  parameters passed in to the bot itself were limited  bot memory was limited to 8 prior actions.  Tensorflow+Keras was utilized on the GPU and 8 agent threads were run simultaneously.    The structure of the API is such that an instance of Starcraft II is started  an agent is initiated in an executive loop that executes tasks in a looping order  and a decision making brain.  These task checks are linked to a decision making brain that determines which action to make from memory states given by the executive loop  and these decisions are then fed back into the actual exectuive loop where the agent carries out the actions and the states are saved as a sequence of the last n states.  This executive process continues until an end condition is reached at which point the outcome is saved in terms of a win  loss  or tie (1  -1  or 0 respectively).  The score is backpropogated and the weights and biases are updated and the decision making brain is saved as a model.  The environment is restarted and the model is loaded in again.   Due to the brain needing to be accessed by all simultaneous executive agents in their own environments  a single global brain is created and 8 threaded environment and executive agents are started afterwards  each executive agent also having individual memories that are passed into the brain that effect the decisions the brain makes.  The brain will sometimes take actions made by an executive agent in one environment and randomly apply those actions to another agent in a different environment.  The brain itself takes in an agent's environmental factors and memory and makes judgements on best actions represented as probabilities  eg. in situation A  with these X actions available to itself  Action X(5) from X(i:n) may be the best action available to making it to a win-state.    **Lessons Learned From the Data**  There were three outcomes recorded at the end of every game: win  loss  or tie.  Each of these outcomes had a reward attached: 1  -1  or 0 respectively.  The initial runs of roughly 3 000 games displayed what seemed to be a very cyclical process.  Only at a single point did the AI reach a point where it won consistently  with an 18 win streak  only to fall immediately afterwards to below a 10% win-rate again.  Given the results  this looked to me like an issue with an overly high learning-rate parameter which caused the bot to reach an optimal strategy at one point  but because of the nature of the policy descent  very similar to gradient descent  it overshot and was pushed away from the optimal strategy.    After reducing the learning-rate by a factor of 10 and re-running the agent  I found that the observed speed at which the bot improved increased.  Due to the random nature of the starting policies  I can't say that this is the reason it reached an optimal strategy more quickly  but the agent has shown that once it did reach an optimal strategy  it has stayed there more consistently  although there are many patterns in the data that I cannot explain  such as the large increase in loss-rate before a major upswing in win-rate  then the proceeding increase in tie-rate before another major upswing in win-rate.  However  as a proof of concept for myself  this has more than proved itself adequate.  Refer to Fig(a) the initial results  and Fig(b) the results with the updated lower learning rate.  Also  refer to the videos below for what an untrained versus trained agent look like in the current action pool.  **Next Steps**  Version 2 of the API was released around the same time I was concluding the initial run of the project  and this has many substantial additions  such as the ability to load replays and let the bot use imitation learning to improve as well as the inclusion of unit IDs which are needed to populate a more robust action pool that the agent can then explore.  The next steps are to port the agent over to the newest version of the API and populate the larger action pool  at which point it becomes potentially possible that a significantly better than human player can be created.    **Introduction To The Game**  Starcraft II is a very complex multiplayer game in the RTS format (in a 1v1 format played in real-time  players try to destroy all the buildings of the opposing player while controlling large armies from a top-down view).  As an introduction to Starcraft II  there are three main 'categories' that need to be balanced to master the game: economy  tech  and army.  Players have the choice to advance in each category at any point in the game.  Investing in economy gains more income and creates a bigger potential to invest in tech or army at a later point  but it leaves a player weak if the enemy chooses to attack early.  Vice versa then  investing in army creates a lower potential to invest in tech or economy at a later point in the game  but creates a higher damage/kill potential on the enemy.  Investing in tech gains a higher potential for creating tech units but has lower potential to invest in economy or army at a later point in the game.  Tech units are special units that have many different traits ranging from being more cost efficient versus lower tech units  to having special abilities or added utility; eg. the ability to make a unit that can go invisible  or an ability added to already existing units that makes them faster or traverse different kinds of terrain better.    There are 3 unique races in the game that vary from each other in how they operate  creating 6 unique matchups.  In addition  players play on different maps that vary in terrain (air-space  ledges  ramps  dead-space  geometry  and various other terrain features).  All units in the game have properties that make them either 'soft counters' or 'hard counters' versus each other.  All these factors lead to compositions and a changing meta-game.    The players in the game also exist under unknown conditions to each other unless they have vision of the other player (which their units provide in a circle around them of limited range).  The game is also constantly being rebalanced with unit properties changing from season to season.  In addition to all of the fore-mentioned features  because the game in real-time  a player who thinks and moves faster and more accurately will be rewarded.  All of these factors lead to an incredibly complex game with a number of unique game states that are essentially infinite.     """;Reinforcement Learning;https://github.com/cdesilv1/sc2_ai_cdes
"""As is  PySC2 branch version 7a04e74 is installed via anaconda on Ubuntu 16.04.  Starcraft II client is 3.16.1 on Ubuntu.  Branch has been modified to support the learning Agent.  This is a work in progress and the AI can be started by way of calling ``` python deepmind_api_modified/pysc2/bin/agent.py ``` This hasn't been tested on other systems and may not work.  Updates will come relatively frequently as this is updated.  **What's Your Project**  Starcraft II Asynchronous Advantage Actor Critic reinforcement learner for PySCII API version 1.1  **What's The Problem?**  Given the complexity of the game (refer to the bottom section: Introduction To The Game for an overall description of the game)  it represents a testbed for testing reinforcement learning artificial intelligence  and given the complexity  it is expected that a successful AI will be revolutionary.  In particular  a well-known name in AI and especially reinforcement learning  DeepMind is taking a forefront position by developing APIs to give a computer agent access to play the game.  So far  progress by DeepMind has been tight-lipped with few publications apart from the initial statement of using Starcraft II as a testbed for developing AI  but the APIs have been released to the public and projects have been encouraged.    I'm very excited to delve deeper into the world of reinforcement learning  commonly regarded to be the possible holy grail of AI in terms of having the potential to be a general learner  and to possibly make a 'best in the world' player and do the same thing that AI has done to games like Chess and Go.    The beauty of the reinforcement learner problem is that a good reinforcement learner should be able to learn the game without any programmer domain knowledge and because of that be able to teach itself without any domain specific hacks and also find it's own novel solutions to problems that humans haven't thought of yet.    **How Did You Do It?**  The starting point of this project is not gathering data  unlike many other data-science projects.  Since reinforcement learners can learn from their past actions  and the API at this time did not support loading in of replays for imitation learning  despite the fact that large replay datasets did exist  I was unable to make use of those and had to focus on pure reinforcement learning.  The starting point was learning the software stack that I had to use  ie. the API.  APIs exist for C++  Javascript  and Python.  My only known language is Python so choosing an API was simple.  Due to the nature of the game development  there was an initial API built so that the game could process bot inputs built on Google's Protocool Buffers extensible mechanisms.  This is the C++ API and all others were built on top of this.  This led to a less than neat design to the code  and such that it was and still is in very active development  an understanding of the code took some time.  Fortunately  there was a major resource in the form of Steven Brown's tutorials in which he took the Python API and built a deep Q-learning agent in a limited action pool.  This is in part  a basis of the action pool the current iteration of my bot uses.    The project itself had a time duration of two weeks  and I had initially planned on spending one of the two weeks building out a robust action pool for the bot to work off.  Unfortunately  after the week was mostly spent  I was struggling with some of the technical limitations of the current version of the API  the largest issue being how data is passed to the bot.  The bot sees the landscape from a top down view with about 23 layers of arrays that each contain a different 'slice' of data.  This is useful for many applications; however  individual unit IDs were not implemented to be used by the agent at this time  and so to give orders to the economy based SCVs that have the ability to either mine Mineral Crystals or Vespene Gas (both being essential resources to buy buildings and units in the game)  while it was relatively simple to keep SCVs mining the more essential of the resources  Mineral Crystals  by randomly selecting pixels identified as SCVs and sending them to Mineral Crystals  if Vespene Geysers were allowed to be mined  all of the SCVs would eventually be clumped on the Vespene Geysers  either leading to game crashes  or all the SCVs on the Geyser and none on Minerals.  A solution could possibly be creating a machine vision agent whose sole purpose is to tag individual units  but this task would be an entire project in itself.  In the end  given that Minerals are the more essential resource (All the most basic  essential units and buildings use only Minerals)  I simply expanded the action pool to allow more basic things to be built so that I could focus on the agent (the brain and learning component) itself.    In Steven Brown's tutorial  Deep Q-Learning was implemented from Morvan Zhou (https://github.com/MorvanZhou).  I won't touch on Q-Learning or Deep Q for the sake of brevity  however feel free to check out Morvan's Github  it is a collection of very thorough ANN tutorials.  I needed to replace this agent.  In the past year  DeepMind has published a very interesting Reinforcement Learning Agent called Asynchronous-Advantage-Actor-Critic  aka A3C.  For more information on A3C  please refer to this paper by Deepmind.  [https://arxiv.org/pdf/1602.01783.pdf]: https://arxiv.org/pdf/1602.01783.pdf  Most of the work revolved around several major parts: Iniitalizing a global agent  starting up in tandem multiple PySC2 enviroments with environment agents and A2C agents for the environment agent  linking everything together as a properly threaded package (PySC2 allows multiple agents to run at once  but not talk to each other)  and tuning hyperparameters.  The restrictions I placed on the bot to minimise parameters were such: player race limitation was set to Terran  enemy difficulty was set to easiest  enemy race was set to Terran  map was set to Simple64 (a small  basic map)  action pool was very limited  parameters passed in to the bot itself were limited  bot memory was limited to 8 prior actions.  Tensorflow+Keras was utilized on the GPU and 8 agent threads were run simultaneously.    The structure of the API is such that an instance of Starcraft II is started  an agent is initiated in an executive loop that executes tasks in a looping order  and a decision making brain.  These task checks are linked to a decision making brain that determines which action to make from memory states given by the executive loop  and these decisions are then fed back into the actual exectuive loop where the agent carries out the actions and the states are saved as a sequence of the last n states.  This executive process continues until an end condition is reached at which point the outcome is saved in terms of a win  loss  or tie (1  -1  or 0 respectively).  The score is backpropogated and the weights and biases are updated and the decision making brain is saved as a model.  The environment is restarted and the model is loaded in again.   Due to the brain needing to be accessed by all simultaneous executive agents in their own environments  a single global brain is created and 8 threaded environment and executive agents are started afterwards  each executive agent also having individual memories that are passed into the brain that effect the decisions the brain makes.  The brain will sometimes take actions made by an executive agent in one environment and randomly apply those actions to another agent in a different environment.  The brain itself takes in an agent's environmental factors and memory and makes judgements on best actions represented as probabilities  eg. in situation A  with these X actions available to itself  Action X(5) from X(i:n) may be the best action available to making it to a win-state.    **Lessons Learned From the Data**  There were three outcomes recorded at the end of every game: win  loss  or tie.  Each of these outcomes had a reward attached: 1  -1  or 0 respectively.  The initial runs of roughly 3 000 games displayed what seemed to be a very cyclical process.  Only at a single point did the AI reach a point where it won consistently  with an 18 win streak  only to fall immediately afterwards to below a 10% win-rate again.  Given the results  this looked to me like an issue with an overly high learning-rate parameter which caused the bot to reach an optimal strategy at one point  but because of the nature of the policy descent  very similar to gradient descent  it overshot and was pushed away from the optimal strategy.    After reducing the learning-rate by a factor of 10 and re-running the agent  I found that the observed speed at which the bot improved increased.  Due to the random nature of the starting policies  I can't say that this is the reason it reached an optimal strategy more quickly  but the agent has shown that once it did reach an optimal strategy  it has stayed there more consistently  although there are many patterns in the data that I cannot explain  such as the large increase in loss-rate before a major upswing in win-rate  then the proceeding increase in tie-rate before another major upswing in win-rate.  However  as a proof of concept for myself  this has more than proved itself adequate.  Refer to Fig(a) the initial results  and Fig(b) the results with the updated lower learning rate.  Also  refer to the videos below for what an untrained versus trained agent look like in the current action pool.  **Next Steps**  Version 2 of the API was released around the same time I was concluding the initial run of the project  and this has many substantial additions  such as the ability to load replays and let the bot use imitation learning to improve as well as the inclusion of unit IDs which are needed to populate a more robust action pool that the agent can then explore.  The next steps are to port the agent over to the newest version of the API and populate the larger action pool  at which point it becomes potentially possible that a significantly better than human player can be created.    **Introduction To The Game**  Starcraft II is a very complex multiplayer game in the RTS format (in a 1v1 format played in real-time  players try to destroy all the buildings of the opposing player while controlling large armies from a top-down view).  As an introduction to Starcraft II  there are three main 'categories' that need to be balanced to master the game: economy  tech  and army.  Players have the choice to advance in each category at any point in the game.  Investing in economy gains more income and creates a bigger potential to invest in tech or army at a later point  but it leaves a player weak if the enemy chooses to attack early.  Vice versa then  investing in army creates a lower potential to invest in tech or economy at a later point in the game  but creates a higher damage/kill potential on the enemy.  Investing in tech gains a higher potential for creating tech units but has lower potential to invest in economy or army at a later point in the game.  Tech units are special units that have many different traits ranging from being more cost efficient versus lower tech units  to having special abilities or added utility; eg. the ability to make a unit that can go invisible  or an ability added to already existing units that makes them faster or traverse different kinds of terrain better.    There are 3 unique races in the game that vary from each other in how they operate  creating 6 unique matchups.  In addition  players play on different maps that vary in terrain (air-space  ledges  ramps  dead-space  geometry  and various other terrain features).  All units in the game have properties that make them either 'soft counters' or 'hard counters' versus each other.  All these factors lead to compositions and a changing meta-game.    The players in the game also exist under unknown conditions to each other unless they have vision of the other player (which their units provide in a circle around them of limited range).  The game is also constantly being rebalanced with unit properties changing from season to season.  In addition to all of the fore-mentioned features  because the game in real-time  a player who thinks and moves faster and more accurately will be rewarded.  All of these factors lead to an incredibly complex game with a number of unique game states that are essentially infinite.     """;General;https://github.com/cdesilv1/sc2_ai_cdes
"""""";General;https://github.com/Epiphqny/Colorization
"""""";Reinforcement Learning;https://github.com/krasing/DRLearningContinuousControl
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   For R-FCN/Faster R-CNN\: 1. Please download COCO and VOC 2007+2012 datasets  and make sure it looks like this:  	``` 	./data/coco/ 	./data/VOCdevkit/VOC2007/ 	./data/VOCdevkit/VOC2012/ 	```  2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  For DeepLab\: 1. Please download Cityscapes and VOC 2012 datasets and make sure it looks like this:  	``` 	./data/cityscapes/ 	./data/VOCdevkit/VOC2012/ 	``` 2. Please download argumented VOC 2012 annotations/image lists  and put the argumented annotations and the argumented train/val lists into:  	``` 	./data/VOCdevkit/VOC2012/SegmentationClass/ 	./data/VOCdevkit/VOC2012/ImageSets/Main/ 	```      Respectively.     2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  1. Clone the Deformable ConvNets repository  and we'll call the directory that you cloned Deformable-ConvNets as ${DCN_ROOT}. ``` git clone https://github.com/msracver/Deformable-ConvNets.git ```  2. For Windows users  run ``cmd .\init.bat``. For Linux user  run `sh ./init.sh`. The scripts will build cython module automatically and create some folders.  3. Install MXNet: 	 	**Note: The MXNet's Custom Op cannot execute parallelly using multi-gpus after this [PR](https://github.com/apache/incubator-mxnet/pull/6928). We strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.5).**  	***Quick start***  	3.1 Install MXNet and all dependencies by  	``` 	pip install -r requirements.txt 	``` 	If there is no other error message  MXNet should be installed successfully.  	 	***Build from source (alternative way)***  	3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by 	``` 	git clone --recursive https://github.com/dmlc/mxnet.git 	git checkout 998378a 	git submodule update 	#: if it's the first time to checkout  just use: git submodule update --init --recursive 	``` 	3.3 Compile MXNet 	``` 	cd ${MXNET_ROOT} 	make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1 	``` 	3.4 Install the MXNet Python binding by 	 	***Note: If you will actively switch between different versions of MXNet  please follow 3.5 instead of 3.4*** 	``` 	cd python 	sudo python setup.py install 	``` 	3.5 For advanced users  you may put your Python packge into `./external/mxnet/$(YOUR_MXNET_PACKAGE)`  and modify `MXNET_VERSION` in `./experiments/rfcn/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`. Thus you can switch among different versions of MXNet quickly.  4. For Deeplab  we use the argumented VOC 2012 dataset. The argumented annotations are provided by [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html) dataset. For convenience  we provide the converted PNG annotations and the lists of train/val images  please download them from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMRhVImMI1jRrsxDg).   We provide trained deformable convnet models  including the deformable R-FCN & Faster R-CNN models trained on COCO trainval  and the deformable DeepLab model trained on CityScapes train.  1. To use the demo with our pre-trained deformable models  please download manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMSjehIcCgAhvEAHw) or [BaiduYun](https://pan.baidu.com/s/1dFlPFED)  and put it under folder `model/`.  	Make sure it looks like this: 	``` 	./model/rfcn_dcn_coco-0000.params 	./model/rfcn_coco-0000.params 	./model/fpn_dcn_coco-0000.params 	./model/fpn_coco-0000.params 	./model/rcnn_dcn_coco-0000.params 	./model/rcnn_coco-0000.params 	./model/deeplab_dcn_cityscapes-0000.params 	./model/deeplab_cityscapes-0000.params 	./model/deform_conv-0000.params 	./model/deform_psroi-0000.params 	``` 2. To run the R-FCN demo  run 	``` 	python ./rfcn/demo.py 	``` 	By default it will run Deformable R-FCN and gives several prediction results  to run R-FCN  use 	``` 	python ./rfcn/demo.py --rfcn_only 	``` 3. To run the DeepLab demo  run 	``` 	python ./deeplab/demo.py 	``` 	By default it will run Deformable Deeplab and gives several prediction results  to run DeepLab  use 	``` 	python ./deeplab/demo.py --deeplab_only 	``` 4. To visualize the offset of deformable convolution and deformable psroipooling  run 	``` 	python ./rfcn/deform_conv_demo.py 	python ./rfcn/deform_psroi_demo.py 	```    1. All of our experiment settings (GPU #  dataset  etc.) are kept in yaml config files at folder `./experiments/rfcn/cfgs`  `./experiments/faster_rcnn/cfgs` and `./experiments/deeplab/cfgs/`. 2. Eight config files have been provided so far  namely  R-FCN for COCO/VOC  Deformable R-FCN for COCO/VOC  Faster R-CNN(2fc) for COCO/VOC  Deformable Faster R-CNN(2fc) for COCO/VOC  Deeplab for Cityscapes/VOC and Deformable Deeplab for Cityscapes/VOC  respectively. We use 8 and 4 GPUs to train models on COCO and on VOC for R-FCN  respectively. For deeplab  we use 4 GPUs for all experiments.  3. To perform experiments  run the python scripts with the corresponding config file as input. For example  to train and test deformable convnets on COCO with ResNet-v1-101  use the following command     ```     python experiments\rfcn\rfcn_end2end_train_test.py --cfg experiments\rfcn\cfgs\resnet_v1_101_coco_trainval_rfcn_dcn_end2end_ohem.yaml     ```     A cache folder would be created automatically to save the model and the log under `output/rfcn_dcn_coco/`. 4. Please find more details in config files and in our code.   """;Computer Vision;https://github.com/qilei123/DEEPLAB_4_RETINAIMG
"""Create a directory `./data/imagenet` under the root of this repository. This directory should contain:  * `train.txt`  * `train/`  * `val.txt`  * `val/`  The `*.txt` files are lists of labeled images as used in Caffe. See the [Caffe ImageNet tutorial](http://caffe.berkeleyvision.org/gathered/examples/imagenet.html) (specifically the `get_ilsvrc_aux.sh` script) to download them  or prepare them yourself as follows. `train.txt` lists image paths relative to `./data/imagenet/train` and integer labels (`val.txt` is analogous):      n01440764/n01440764_10026.JPEG 0     n01440764/n01440764_10027.JPEG 0     n01440764/n01440764_10029.JPEG 0     n01440764/n01440764_10040.JPEG 0     [...]     n15075141/n15075141_9933.JPEG 999     n15075141/n15075141_9942.JPEG 999     n15075141/n15075141_999.JPEG 999     n15075141/n15075141_9993.JPEG 999  Relative to the root of this repository  the first image listed above should be located at `./data/imagenet/train/n01440764/n01440764_10026.JPEG`.   Create a directory `./data/mnist` under the root of this repository. This directory should contain the MNIST data files (or symlinks to them) with these names:      t10k-images.idx3-ubyte     t10k-labels.idx1-ubyte     train-images.idx3-ubyte     train-labels.idx1-ubyte  The `train_mnist.sh` script trains a ""permutation-invariant"" BiGAN (by default) on the MNIST dataset. MNIST training takes about 30 minutes on a Titan X GPU (400 epochs at ~3.3 seconds per epoch).   You should see output like the following:   For the (joint) latent regressor baselines  change the OBJECTIVE=... setting appropriately (see MNIST instructions above).   You should see output like the following:   To download and install these weights at the locations assumed in eval_model.sh (see below)  do the following from the root of this repository:   for classification experiments  use philkr's ""future"" version of Caffe linked from voc-classification (see below)   To run eval_model.sh yourself  follow these steps:   Modify the variables (CAFFE_DIR  MAGIC_DIR  CLASS_DIR) near the top of eval_model.sh specifying the paths where you installed these packages.       python resize_imageset.py -r -j 4 ${SIZE} ./data/imagenet ./data/imagenet${SIZE}  With an argument of `--raw_size 72` (for example)  `train_gan.py` will automatically check if the presized image directory `./data/imagenet72` exists before falling back to `./data/imagenet`.   """;General;https://github.com/jeffdonahue/bigan
"""""";Computer Vision;https://github.com/renato145/stn
"""This code is simple enough to just hack inline  not ""used""  but current API looks something like:  ```python  #: you're on your own to define a class that returns individual examples as PyTorch LongTensors from torch.utils.data import Dataset train_dataset = MyDataset(...) test_dataset = MyDataset(...)  #: construct a GPT model from mingpt.model import GPT  GPTConfig mconf = GPTConfig(vocab_size  block_size  n_layer=12  n_head=12  n_embd=768) #: a GPT-1 model = GPT(mconf)  #: construct a trainer from mingpt.trainer import Trainer  TrainerConfig tconf = TrainerConfig(max_epochs=10  batch_size=256) trainer = Trainer(model  train_dataset  test_dataset  tconf) trainer.train() #: (... enjoy the show for a while... )  #: sample from the model (the [None  ...] and [0] are to push/pop a needed dummy batch dimension) from mingpt.utils import sample x = torch.tensor([1  2  3]  dtype=torch.long)[None  ...] #: context conditioning y = sample(model  x  steps=30  temperature=1.0  sample=True  top_k=5)[0] print(y) #: our model filled in the integer sequence with 30 additional likely integers ```   """;General;https://github.com/karynaur/Attention-Free-minGPT
"""Before using pykg2vec  we recommend users to have the following libraries installed: * python >=3.7 (recommended) * pytorch>= 1.5  Quick Guide for Anaconda users:  * Setup a Virtual Environment: we encourage you to use anaconda to work with pykg2vec: ```bash (base) $ conda create --name pykg2vec python=3.7 (base) $ conda activate pykg2vec ``` * Setup Pytorch: we encourage to use pytorch with GPU support for good training performance. However  a CPU version also runs. The following sample commands are for setting up pytorch:  ```bash #: if you have a GPU with CUDA 10.1 installed (pykg2vec) $ conda install pytorch torchvision cudatoolkit=10.1 -c pytorch #: or cpu-only (pykg2vec) $ conda install pytorch torchvision cpuonly -c pytorch ```  * Setup Pykg2vec: ```bash (pykg2vec) $ git clone https://github.com/Sujit-O/pykg2vec.git (pykg2vec) $ cd pykg2vec (pykg2vec) $ python setup.py install ```  For beginners  these papers  [A Review of Relational Machine Learning for Knowledge Graphs](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7358050)  [Knowledge Graph Embedding: A Survey of Approaches and Applications](https://ieeexplore.ieee.org/document/8047276)  and [An overview of embedding models of entities and relationships for knowledge base completion](https://arxiv.org/abs/1703.08098) can be good starting points!   With pykg2vec command-line interface  you can 1. Run a single algorithm with various models and datasets (customized dataset also supported).     ```     #: Check all tunnable parameters.     (pykg2vec) $ pykg2vec-train -h      #: Train TransE on FB15k benchmark dataset.     (pykg2vec) $ pykg2vec-train -mn TransE      #: Train using different KGE methods.     (pykg2vec) $ pykg2vec-train -mn [TransE|TransD|TransH|TransG|TransM|TransR|Complex|ComplexN3|                         CP|RotatE|Analogy|DistMult|KG2E|KG2E_EL|NTN|Rescal|SLM|SME|SME_BL|HoLE|                         ConvE|ConvKB|Proje_pointwise|MuRP|QuatE|OctonionE|InteractE|HypER]      #: For KGE using projection-based loss function  use more processes for batch generation.     (pykg2vec) $ pykg2vec-train -mn [ConvE|ConvKB|Proje_pointwise] -npg [the number of processes  4 or 6]      #: Train TransE model using different benchmark datasets.     (pykg2vec) $ pykg2vec-train -mn TransE -ds [fb15k|wn18|wn18_rr|yago3_10|fb15k_237|ks|nations|umls|dl50a|nell_955]      #: Train TransE model using your own hyperparameters.     (pykg2vec) $ pykg2vec-train -exp True -mn TransE -ds fb15k -hpf ./examples/custom_hp.yaml      #: Use your own dataset     (pykg2vec) $ pykg2vec-train -mn TransE -ds [name] -dsp [path to the custom dataset]     ``` 2. Tune a single algorithm.     ```     #: Tune TransE using the benchmark dataset.     (pykg2vec) $ pykg2vec-tune -mn [TransE] -ds [dataset name]      #: Tune TransE with your own search space     (pykg2vec) $ pykg2vec-tune -exp True -mn TransE -ds fb15k -ssf ./examples/custom_ss.yaml     ``` 3. Perform Inference Tasks (more advanced).     ```     #: Train a model and perform inference tasks.     (pykg2vec) $ pykg2vec-infer -mn TransE      #: Perform inference tasks over a pretrained model.     (pykg2vec) $ pykg2vec-infer -mn TransE -ld [path to the pretrained model]     ``` \* NB: On Windows  use `pykg2vec-train.exe`  `pykg2vec-tune.exe` and `pykg2vec-infer.exe` instead.  For more usage of pykg2vec APIs  please check the [programming examples](https://pykg2vec.readthedocs.io/en/latest/auto_examples/index.html).   """;Graphs;https://github.com/Sujit-O/pykg2vec
"""""";General;https://github.com/ibatra/nlm
"""""";Sequential;https://github.com/ibatra/nlm
"""""";Natural Language Processing;https://github.com/ibatra/nlm
"""TensorFlow version == 1.4   """;General;https://github.com/SeonbeomKim/TensorFlow-MemN2N
"""""";Graphs;https://github.com/YunseobShin/wiki_GAT
"""Use the fastai (https://www.fast.ai/)framework to implement transfer learning for text classification.  The language model was trained on a corpus named Wikitext-103.(https://openreview.net/pdf?id=Byj72udxe)  If you want to know some detailed information  please refer to Jeremy Howard‚Äôs paper.(https://arxiv.org/abs/1801.06146v5)  fasiai Github https://github.com/fastai/fastai  ÔºàNote: It is highly recommended to use the conda method to install.Ôºâ  The tutorial below the official github course folder is fastai version is 0.7. I am using the address of the corresponding tutorial for 1.0.0 should be:  https://github.com/fastai/course-v3  video:  https://www.usfca.edu/data-institute/certificates/deep-learning-part-two """;General;https://github.com/SkullFang/ULMFIT_NLP_Classification
"""Use the fastai (https://www.fast.ai/)framework to implement transfer learning for text classification.  The language model was trained on a corpus named Wikitext-103.(https://openreview.net/pdf?id=Byj72udxe)  If you want to know some detailed information  please refer to Jeremy Howard‚Äôs paper.(https://arxiv.org/abs/1801.06146v5)  fasiai Github https://github.com/fastai/fastai  ÔºàNote: It is highly recommended to use the conda method to install.Ôºâ  The tutorial below the official github course folder is fastai version is 0.7. I am using the address of the corresponding tutorial for 1.0.0 should be:  https://github.com/fastai/course-v3  video:  https://www.usfca.edu/data-institute/certificates/deep-learning-part-two """;Natural Language Processing;https://github.com/SkullFang/ULMFIT_NLP_Classification
"""""";Computer Vision;https://github.com/violin0847/crowdcounting
"""  pytorch implement for PointNet(point cloud segmentation)    - train.py for model training  - evaluate.py for finally test  - IOU.py for calculating average IOU to estimate model performance  """;Computer Vision;https://github.com/PaParaZz1/PointNet
"""""";General;https://github.com/tkuanlun350/Kaggle_Ship_Detection_2018
"""The *Show and Tell* model is a deep neural network that learns how to describe the content of images. For example:  ![Example captions](g3doc/example_captions.jpg)   To train the model you will need to provide training data in native TFRecord format. The TFRecord format consists of a set of sharded files containing serialized `tf.SequenceExample` protocol buffers. Each `tf.SequenceExample` proto contains an image (JPEG format)  a caption and metadata such as the image id.  Each caption is a list of words. During preprocessing  a dictionary is created that assigns each word in the vocabulary to an integer-valued id. Each caption is encoded as a list of integer word ids in the `tf.SequenceExample` protos.  We have provided a script to download and preprocess the [MSCOCO](http://mscoco.org/) image captioning data set into this format. Downloading and preprocessing the data may take several hours depending on your network and computer speed. Please be patient.  Before running the script  ensure that your hard disk has at least 150GB of available space for storing the downloaded and processed data.  ```shell #: Location to save the MSCOCO data. MSCOCO_DIR=""${HOME}/im2txt/data/mscoco""  #: Build the preprocessing script. cd research/im2txt bazel build //im2txt:download_and_preprocess_mscoco  #: Run the preprocessing script. bazel-bin/im2txt/download_and_preprocess_mscoco ""${MSCOCO_DIR}"" ```  The final line of the output should read:  ``` 2016-09-01 16:47:47.296630: Finished processing all 20267 image-caption pairs in data set 'test'. ```  When the script finishes you will find 256 training  4 validation and 8 testing files in `DATA_DIR`. The files will match the patterns `train-?????-of-00256`  `val-?????-of-00004` and `test-?????-of-00008`  respectively.   First ensure that you have installed the following required packages:  * **Bazel** ([instructions](http://bazel.io/docs/install.html)) * **TensorFlow** 1.0 or greater ([instructions](https://www.tensorflow.org/install/)) * **NumPy** ([instructions](http://www.scipy.org/install.html)) * **Natural Language Toolkit (NLTK)**:     * First install NLTK ([instructions](http://www.nltk.org/install.html))     * Then install the NLTK data package ""punkt"" ([instructions](http://www.nltk.org/data.html)) * **Unzip**  Install Required Packages   Download the Inception v3 Checkpoint   cd research/im2txt   Note that you may run out of memory if you run the evaluation script on the same  GPU as the training script. You can run the command   : Ignore GPU devices (only necessary if your GPU is currently memory   : you will need to pass the checkpoint path explicitly.   cd research/im2txt   : Ignore GPU devices (only necessary if your GPU is currently memory   A TensorFlow implementation of the image-to-text model described in the paper:  ""Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge.""  Oriol Vinyals  Alexander Toshev  Samy Bengio  Dumitru Erhan.  *IEEE transactions on pattern analysis and machine intelligence (2016).*  Full text available at: http://arxiv.org/abs/1609.06647   """;Computer Vision;https://github.com/brandontrabucco/im2txt_match
"""```bash python setup.py install ```   You can choose between the following models:    ```bash python train.py ```   """;Graphs;https://github.com/tkipf/gae
"""We are given satellite images (more accurately sections of satellite images)  which might contain ships or other waterborne vehicles. The goal is to segment the images to the ""ship""/""no-ship"" classes (label each pixel using these classes). The images might contain multiple ships  they can be placed close to each other (yet should be detected as separate ships)  they can be located in ports  can be moving or stationary  etc. The pictures might show inland areas the sea without ships  can be cloudy or foggy  lighting conditions can vary.  The training data is given as images and masks for the ships (in a run length encoded format). If an image contains multiple ships  each ship has a separate record  mask.    The training data is analysed and visualised in  the [ShipDetectionDataPrep.ipynb](ShipDetectionDataPrep.ipynb) Jupyter Notebook. The script can be executed if a few images are included in the train_img folder. Whithout this it can't show examples for different scenarios apperaring in the dataset.    One prediction the network made can be seen on the picture below.    """;Computer Vision;https://github.com/kaland313/vitmav45-ShipSeakers
"""""";Reinforcement Learning;https://github.com/juliusfrost/Research-Paper-Implementations
"""We recommend using virtual environment. Installing instructions can be found in the following link: https://www.tensorflow.org/install/pip  After the virtual environment activation  we have to install the required packages: ``` pip install -r requirements.txt ``` Make sure the current directory is the repository main directory.   Both can be automatically downloaded by torchvision.   Make sure the current directory is the cnn directory.   Make sure the current directory is the cnn directory.   """;General;https://github.com/yochaiz/darts-UNIQ
"""iterations on the above graph). Like their paper  we start at a   To test these strategies  we repeat the above protocol using the   - You need at least CUDA 7.0 and CuDNN v4. - Install Torch. - Install the Torch CUDNN V4 library: `git clone https://github.com/soumith/cudnn.torch; cd cudnn; git co R4; luarocks make` This will give you `cudnn.SpatialBatchNormalization`  which helps save quite a lot of memory. - Install nninit: `luarocks install nninit`. - Download   [CIFAR 10](http://torch7.s3-website-us-east-1.amazonaws.com/data/cifar-10-torch.tar.gz).   Use `--dataRoot <cifar>` to specify the location of the extracted CIFAR 10 folder. - Run `train-cifar.lua`.   """;General;https://github.com/gcr/torch-residual-networks
"""iterations on the above graph). Like their paper  we start at a   To test these strategies  we repeat the above protocol using the   - You need at least CUDA 7.0 and CuDNN v4. - Install Torch. - Install the Torch CUDNN V4 library: `git clone https://github.com/soumith/cudnn.torch; cd cudnn; git co R4; luarocks make` This will give you `cudnn.SpatialBatchNormalization`  which helps save quite a lot of memory. - Install nninit: `luarocks install nninit`. - Download   [CIFAR 10](http://torch7.s3-website-us-east-1.amazonaws.com/data/cifar-10-torch.tar.gz).   Use `--dataRoot <cifar>` to specify the location of the extracted CIFAR 10 folder. - Run `train-cifar.lua`.   """;Computer Vision;https://github.com/gcr/torch-residual-networks
"""""";General;https://github.com/Edmonton-School-of-AI/ml5-Simple-Image-Classification
"""""";Computer Vision;https://github.com/Edmonton-School-of-AI/ml5-Simple-Image-Classification
"""You can tune the following parameters:   """;General;https://github.com/b-etienne/Seq2seq-PyTorch
"""Generative Adversarial Networks (GANs) are one of the most popular (and coolest) Machine Learning algorithms developed in recent times. They belong to a set of algorithms called generative models  which are widely used for unupervised learning tasks which aim to learn the uderlying structure of the given data. As the name suggests GANs allow you to generate new unseen data that mimic the actual given real data. However  GANs pose problems in training and require carefullly tuned hyperparameters.This paper aims to solve this problem.  DCGAN is one of the most popular and succesful network design for GAN. It mainly composes of convolution layers  without max pooling or fully connected layers. It uses strided convolutions and transposed convolutions  for the downsampling and the upsampling respectively.  **Generator architecture of DCGAN** <p align=""center""> <img src=""images/Generator.png"" title=""DCGAN Generator"" alt=""DCGAN Generator""> </p>  **Network Design of DCGAN:** * Replace all pooling layers with strided convolutions. * Remove all fully connected layers. * Use transposed convolutions for upsampling. * Use Batch Normalization after every layer except after the output layer of the generator and the input layer of the discriminator. * Use ReLU non-linearity for each layer in the generator except for output layer use tanh. * Use Leaky-ReLU non-linearity for each layer of the disciminator excpet for output layer use sigmoid.   """;Computer Vision;https://github.com/Natsu6767/DCGAN-PyTorch
"""**Note : Use Python 3.6 or newer**  """;Computer Vision;https://github.com/ljllili23/unet_baseline
"""Ôºà1Ôºâ„ÄÅÂ¢ûÂä†  message LayerParameter { optional GhmcLossParameter ghmc_loss_param = 160;}         optional uint32 m = 1 [default = 30];                   name: ""ghmcloss""   """;General;https://github.com/xialuxi/GHMLoss-caffe
"""""";Computer Vision;https://github.com/YanWei123/PointNet-encoder-and-FoldingNet-decoder-add-quantization-change-latent-code-size-from-512-to-1024
"""git clone https://github.com/kinimod23/NMT_Project.git  cd ~/NMT_Project/NMT_environment/shell_scripts  Set up the NMT environment:  bash sockeye_wmt_env.sh   bash sockeye_wmt_prep.sh   cd ~/NMT_Project/NLR_pre-training/glove   git init .  git remote add -t \* -f origin http://github.com/stanfordnlp/glove  git checkout master   cd ~/NMT_Project/NMT_environment/shell_scripts  bash sockeye_wmt_create.small.embs.sh   bash sockeye_wmt_train_basel.sh   bash sockeye_wmt_train_small.prembs.sh model_wmt17_small.glove   cd ~/NMT_Project/NMT_environment/shell_scripts  bash sockeye_wmt_create.large.embs.sh   bash sockeye_wmt_train_large.prembs.sh model_wmt17_large.glove   cd ~/NMT_Project/NMT_environment/shell_scripts  bash sockeye_wmt_eval.sh model_wmt17_basel  bash sockeye_wmt_eval.sh model_wmt17_small.glove  bash sockeye_wmt_eval.sh model_wmt17_large.glove   bash sockeye_wmt_prembs.recheck.sh model_wmt17_small.glove &amp;&amp; exit  bash sockeye_wmt_prembs.recheck.sh model_wmt17_large.glove &amp;&amp; exit   mkdir ~/Desktop/recheck_embs  cd ~/Desktop/recheck_embs  wget https://raw.githubusercontent.com/kinimod23/NMT_Project/master/NMT_environment/tools/recheck_embs.sh   bash recheck_embs.sh model_wmt17_basel  bash recheck_embs.sh model_wmt17_large.glove   cd ~/NMT_Project/Signifikanztests       cd ~/NMT_Project/NMT_environment/shell_scripts     bash sockeye_wmt_prep_add.data  Train glove embeddings with previously generated additional BPE training data:      cd ~/NMT_Project/NLR_pre-training/glove      &nbsp;   Natural language Representations (NLRs) might ignore key features of distributional semantics! A new NLR model is typically evaluated across several tasks  and is considered an improvement if it achieves better accuracy than its predecessors. However  different applications rely on different aspects of word embeddings  and good performance in one application does not necessarily imply equally good performance on another.   WMT 2017 Translation Task http://data.statmt.org/wmt17/translation-task/preprocessed/de-en/  ------------------------------------------------------------------------------------------  """;Natural Language Processing;https://github.com/kinimod23/NMT_Project
"""``` $ git clone https://github.com/kuc2477/pytorch-vae && cd pytorch-vae $ pip install -r requirements.txt ```    ``` $ ./main.py --help $ usage: VAE PyTorch implementation [-h] [--dataset {mnist cifar10 cifar100}]                                   [--kernel-num KERNEL_NUM] [--z-size Z_SIZE]                                   [--epochs EPOCHS] [--batch-size BATCH_SIZE]                                   [--sample-size SAMPLE_SIZE] [--lr LR]                                   [--weight-decay WEIGHT_DECAY]                                   [--loss-log-interval LOSS_LOG_INTERVAL]                                   [--image-log-interval IMAGE_LOG_INTERVAL]                                   [--resume] [--checkpoint-dir CHECKPOINT_DIR]                                   [--sample-dir SAMPLE_DIR] [--no-gpus]                                   (--test | --train)  optional arguments:   -h  --help            show this help message and exit   --dataset {mnist cifar10 cifar100}   --kernel-num KERNEL_NUM   --z-size Z_SIZE   --epochs EPOCHS   --batch-size BATCH_SIZE   --sample-size SAMPLE_SIZE   --lr LR   --weight-decay WEIGHT_DECAY   --loss-log-interval LOSS_LOG_INTERVAL   --image-log-interval IMAGE_LOG_INTERVAL   --resume   --checkpoint-dir CHECKPOINT_DIR   --sample-dir SAMPLE_DIR   --no-gpus   --test   --train ```   """;Computer Vision;https://github.com/kuc2477/pytorch-vae
"""``` $ git clone https://github.com/kuc2477/pytorch-vae && cd pytorch-vae $ pip install -r requirements.txt ```    ``` $ ./main.py --help $ usage: VAE PyTorch implementation [-h] [--dataset {mnist cifar10 cifar100}]                                   [--kernel-num KERNEL_NUM] [--z-size Z_SIZE]                                   [--epochs EPOCHS] [--batch-size BATCH_SIZE]                                   [--sample-size SAMPLE_SIZE] [--lr LR]                                   [--weight-decay WEIGHT_DECAY]                                   [--loss-log-interval LOSS_LOG_INTERVAL]                                   [--image-log-interval IMAGE_LOG_INTERVAL]                                   [--resume] [--checkpoint-dir CHECKPOINT_DIR]                                   [--sample-dir SAMPLE_DIR] [--no-gpus]                                   (--test | --train)  optional arguments:   -h  --help            show this help message and exit   --dataset {mnist cifar10 cifar100}   --kernel-num KERNEL_NUM   --z-size Z_SIZE   --epochs EPOCHS   --batch-size BATCH_SIZE   --sample-size SAMPLE_SIZE   --lr LR   --weight-decay WEIGHT_DECAY   --loss-log-interval LOSS_LOG_INTERVAL   --image-log-interval IMAGE_LOG_INTERVAL   --resume   --checkpoint-dir CHECKPOINT_DIR   --sample-dir SAMPLE_DIR   --no-gpus   --test   --train ```   """;General;https://github.com/kuc2477/pytorch-vae
"""""";Reinforcement Learning;https://github.com/fshamshirdar/pytorch-rdpg
"""This repo trains a Reinforcement Learning Neural Network so that it's able to play Pong from raw pixel input.  I've written up a [blog post](https://medium.com/@omkarv/intro-to-reinforcement-learning-pong-92a94aa0f84d) which walks through the code here and the basic principles of Reinforcement Learning  with Pong as the guiding example.  It is largely based on [a Gist by Andrej Karpathy](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)  which in turn is based on the [Playing Atari with Deep Reinforcement Learning paper by Mnih et al.](https://arxiv.org/abs/1312.5602)  This script uses the [Open AI Gym environments](https://github.com/openai/gym) in order to run the Atari emulator and environments  and currently uses no external ML framework & only numpy.   The instructions below are for Mac OS & assume you have Homebrew installed.  * You'll need to run the code with Python 2.7 - I recommend the use of `conda` to manage python environments * Install Open AI Gym `brew install gym` * Install Cmake `brew install cmake` * Install ffmpeg `brew install ffmpeg` - Required for monitoring / videos   """;Reinforcement Learning;https://github.com/omkarv/pong-from-pixels
"""""";Computer Vision;https://github.com/lyeoni/pytorch-mnist-VAE
"""""";General;https://github.com/lyeoni/pytorch-mnist-VAE
"""Note: Make sure your dataset follows the same structure as shown above  as these scripts are hard-coded to this structure.   Below you can see a VGG diagram representing the 16 layers (more details here):   Here is an overview on how to do automated image analysis of photoquadrats to estimate abundance (i.e.  cover). More detailed steps will be detailed in documents within the repository and referred to in this summary.   1. __Configure the machine:__ Prepare either your local machine or the AWS instance by installing Caffe and other dependencies needed to execute this code. Please refer to Caffe installation instructions [here](http://caffe.berkeleyvision.org/installation.html). We have also created a configurations guide for the AWS instance using a Bitfusion imafe. (Details in  `config.md` ) 2. __Download data:__ Here we are using the data from the publication above as an example. However  you can use your own dataset. If this is the case  follow the data structure of our data repository. You can download our dataset from [here](https://nextcloud.qriscloud.org.au/index.php/s/YMgU7ZpdxSjPwpu).  >__Note:__ The data provided includes the entire dataset for all the global regions analysed in this paper. For each region  or country within a region  we trained and tested a different Deep Learning network.   Once the AWS instance and data have been configured  the Jupyter Notebooks  in the `notebooks` folder  will provide a guide and an example to run the source code from this repository. Here is a brief description of each notebook to automatically extract cover data from your images:  1. __Train Deep Learning Nets (`train_nets.pynb`):__   Using the training images and annotations  this step will perform a number of iterations to fine-tune the base Net `model_zoo/VGG_ILSVRC_16_layers.caffemodel`. This base Net is a VGG-16 network initialized or trained with the ImageNet dataset (1M images and 1K classification units or labels). For more details about this Net  please refer to the [Caffe Model Zoo documentation](https://github.com/BVLC/caffe/wiki/Model-Zoo) and the developers [arxiv paper](http://arxiv.org/pdf/1409.1556). This code will fine-tune the base Net using the training images and annotations provided. In the process of training the net  the last layer of the base net gets replaces by the classifciation units of our data (labels) and the hyperparameters of the network will be adjusted by back-propagation. During the training  two more hyperparameters are recommended calibrate (e.g.  number of iterations  base Learning Rate  Image Scale). to calibrate such parameters  the code will train  multiple nets using a range of parameter values. This step is refered here as ""Experiments""  and the resulting nets from these experiments will then be screened to select the best network.    2. __Screen and select best Net configuration on their performance (`compare_nets.pynb`):__   All Nets produced in the step above  using different configuration parameters  will be contrasted in this step. Different performance metrics will be produced based on the net classification on manually classified images.  Once the best Net is selected  the final step is to evaluate the performance of the Net by using the error metric described in the publication. This error is a measure of the absolute difference between automated and manual estimations of cover by the machine and expert human  respectively. For this work  we are no longer looking at the accuracy of the machine at the point level (as determined by the confusion matrix)  but rather the precision of the automated estimations of benthic cover for each label within a sample unit. In our case  a sample unit is a 50m transect containing a number of images. To evaluate the performance error  a set of transects are set aside  called ""test-cells"". The images and manual annotations from these test-cells are stored in the test folder  as downloaded from our dataset. The predicted cover estimations will then be contrasted with the manual cover estimations on the test images. 3. __Classify new images (`deploy_net.pynb`):__  The selected Net will be used to predict labels on a given number of random points from a specified set of images that are contained in a folder called `data`. After running the classifier on the new images  the scripts will produce a folder called `coverages` that will contain a text file for every new image with the location and classification of each point. Use these files to estimate benthic coverage as described in the manuscript associated to this repository   """;Computer Vision;https://github.com/mgonzalezrivero/reef_learning
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M and VGG2 datasets  which were already packed in the MxNet binary format. The network backbones include ResNet  InceptionResNet_v2  DenseNet  DPN and MobileNet. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss. * loss-type=0:  Softmax * loss-type=1:  SphereFace * loss-type=2:  CosineFace * loss-type=4:  ArcFace * loss-type=5:  Combined Margin * loss-type=12: TripletLoss  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu80   git clone --recursive https://github.com/deepinsight/insightface.git   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/chenggongliang/arcface
"""    $ git clone https://github.com/VitoRazor/Gan_Architecture.git     $ cd Gan_Architecture-master/     $ pip install keras   """;Computer Vision;https://github.com/VitoRazor/Gan_Architecture
"""""";Computer Vision;https://github.com/singhsidhukuldeep/Generative-Adversarial-Network-GAN
"""Then  start train with   ContentLoss. Inlcuded VGG/saveVGG19.sh to build VGG loss.  luarocks install loadcaffe  Download VGG : cd VGG; ./saveVGG19  luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec    """;General;https://github.com/junhocho/SRGAN
"""Then  start train with   ContentLoss. Inlcuded VGG/saveVGG19.sh to build VGG loss.  luarocks install loadcaffe  Download VGG : cd VGG; ./saveVGG19  luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec    """;Computer Vision;https://github.com/junhocho/SRGAN
"""""";General;https://github.com/liqiang2018/Vehicle_delector
"""""";Computer Vision;https://github.com/liqiang2018/Vehicle_delector
"""1. Clone the repository     ```shell    git clone https://github.com/YudeWang/UNet-Satellite-Image-Segmentation.git    ```  2. Install PyDenseCRF     You can follow the install instruction of [PyDenseCRF](https://github.com/lucasb-eyer/pydensecrf)     If you **do not have the permission of sudo**  you can download the source code by:     ```shell    git clone https://github.com/lucasb-eyer/pydensecrf.git    ```     Follow the instruction and install:     ```shell    cd pydensecrf-master    python setup.py install    ```  3. Download dataset and model     You can download 2017 CCF BDCI remote sensing challenge dataset and our pre-trained model from [here](https://drive.google.com/file/d/1FMRMe4qSI-JS6AzrO8kASO3BfHOLoUfM/view). Please unzip package in this repository folder and change the ckpt file name to **UNet_ResNet_itr100000.ckpt**(I used to call it FPN  while the structure of network is symmetrical and then rename it).    This project implement by gpu version of tensorflow1.3. Therefore a Nvidia GPU is needed.   python train.py --gpu=0   python test.py --gpu=0   """;General;https://github.com/YudeWang/UNet-Satellite-Image-Segmentation
"""1. Clone the repository     ```shell    git clone https://github.com/YudeWang/UNet-Satellite-Image-Segmentation.git    ```  2. Install PyDenseCRF     You can follow the install instruction of [PyDenseCRF](https://github.com/lucasb-eyer/pydensecrf)     If you **do not have the permission of sudo**  you can download the source code by:     ```shell    git clone https://github.com/lucasb-eyer/pydensecrf.git    ```     Follow the instruction and install:     ```shell    cd pydensecrf-master    python setup.py install    ```  3. Download dataset and model     You can download 2017 CCF BDCI remote sensing challenge dataset and our pre-trained model from [here](https://drive.google.com/file/d/1FMRMe4qSI-JS6AzrO8kASO3BfHOLoUfM/view). Please unzip package in this repository folder and change the ckpt file name to **UNet_ResNet_itr100000.ckpt**(I used to call it FPN  while the structure of network is symmetrical and then rename it).    This project implement by gpu version of tensorflow1.3. Therefore a Nvidia GPU is needed.   python train.py --gpu=0   python test.py --gpu=0   """;Computer Vision;https://github.com/YudeWang/UNet-Satellite-Image-Segmentation
"""The repo has been forked initially from https://github.com/Diego999/pyGAT by   """;Graphs;https://github.com/weiyangfb/PyTorchSparseGAT
""" ![sn](./assests/sn.png)      ```bash  > python main.py --dataset mnist --sn True  ```      ```python     w = tf.get_variable(""kernel""  shape=[kernel  kernel  x.get_shape()[-1]  channels])     b = tf.get_variable(""bias""  [channels]  initializer=tf.constant_initializer(0.0))      x = tf.nn.conv2d(input=x  filter=spectral_norm(w)  strides=[1  stride  stride  1]) + b  ```     """;Computer Vision;https://github.com/taki0112/Spectral_Normalization-Tensorflow
""" ![sn](./assests/sn.png)      ```bash  > python main.py --dataset mnist --sn True  ```      ```python     w = tf.get_variable(""kernel""  shape=[kernel  kernel  x.get_shape()[-1]  channels])     b = tf.get_variable(""bias""  [channels]  initializer=tf.constant_initializer(0.0))      x = tf.nn.conv2d(input=x  filter=spectral_norm(w)  strides=[1  stride  stride  1]) + b  ```     """;General;https://github.com/taki0112/Spectral_Normalization-Tensorflow
"""""";General;https://github.com/AirBernard/DenseNet-by-Pytorch
"""""";Computer Vision;https://github.com/AirBernard/DenseNet-by-Pytorch
"""tinymind ‰ΩøÁî®ËØ¥ÊòéÔºöhttps://gitee.com/ai100/quiz-w7-doc   """;General;https://github.com/0492wzl/tensorflow_slim_densenet
"""tinymind ‰ΩøÁî®ËØ¥ÊòéÔºöhttps://gitee.com/ai100/quiz-w7-doc   """;Computer Vision;https://github.com/0492wzl/tensorflow_slim_densenet
"""""";Computer Vision;https://github.com/Lxrd-AJ/Advanced_ML
"""python: 3.x  Pytorch: 0.4+  For 5-way 1-shot exp.  it allocates nearly 6GB GPU memory.   """;General;https://github.com/dragen1860/MAML-Pytorch
"""```python TRAIN = ""train/"" TEST = ""test/""   #: Load ""X"" (the neural network's training and testing inputs)  def load_X(X_signals_paths):     X_signals = []      for signal_type_path in X_signals_paths:         file = open(signal_type_path  'r')         #: Read dataset from disk  dealing with text files' syntax         X_signals.append(             [np.array(serie  dtype=np.float32) for serie in [                 row.replace('  '  ' ').strip().split(' ') for row in file             ]]         )         file.close()      return np.transpose(np.array(X_signals)  (1  2  0))  X_train_signals_paths = [     DATASET_PATH + TRAIN + ""Inertial Signals/"" + signal + ""train.txt"" for signal in INPUT_SIGNAL_TYPES ] X_test_signals_paths = [     DATASET_PATH + TEST + ""Inertial Signals/"" + signal + ""test.txt"" for signal in INPUT_SIGNAL_TYPES ]  X_train = load_X(X_train_signals_paths) X_test = load_X(X_test_signals_paths)   #: Load ""y"" (the neural network's training and testing outputs)  def load_y(y_path):     file = open(y_path  'r')     #: Read dataset from disk  dealing with text file's syntax     y_ = np.array(         [elem for elem in [             row.replace('  '  ' ').strip().split(' ') for row in file         ]]          dtype=np.int32     )     file.close()      #: Substract 1 to each output class for friendly 0-based indexing     return y_ - 1  y_train_path = DATASET_PATH + TRAIN + ""y_train.txt"" y_test_path = DATASET_PATH + TEST + ""y_test.txt""  y_train = load_y(y_train_path) y_test = load_y(y_test_path)  ```   #: Get LSTM cell output   Instructions for updating:   matplotlib.rc('font'  **font)   ```python #: Note: Linux bash commands start with a ""!"" inside those ""ipython notebook"" cells  DATA_PATH = ""data/""  !pwd && ls os.chdir(DATA_PATH) !pwd && ls  !python download_dataset.py  !pwd && ls os.chdir("".."") !pwd && ls  DATASET_PATH = DATA_PATH + ""UCI HAR Dataset/"" print(""\n"" + ""Dataset is now located at: "" + DATASET_PATH)  ```      /home/ubuntu/pynb/LSTM-Human-Activity-Recognition     data	 LSTM_files  LSTM_OLD.ipynb  README.md     LICENSE  LSTM.ipynb  lstm.py	     screenlog.0     /home/ubuntu/pynb/LSTM-Human-Activity-Recognition/data     download_dataset.py  source.txt      Downloading...     --2017-05-24 01:49:53--  https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip     Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249     Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.     HTTP request sent  awaiting response... 200 OK     Length: 60999314 (58M) [application/zip]     Saving to: ‚ÄòUCI HAR Dataset.zip‚Äô      100%[======================================>] 60 999 314  1.69MB/s   in 38s          2017-05-24 01:50:31 (1.55 MB/s) - ‚ÄòUCI HAR Dataset.zip‚Äô saved [60999314/60999314]      Downloading done.      Extracting...     Extracting successfully done to /home/ubuntu/pynb/LSTM-Human-Activity-Recognition/data/UCI HAR Dataset.     /home/ubuntu/pynb/LSTM-Human-Activity-Recognition/data     download_dataset.py  __MACOSX  source.txt  UCI HAR Dataset  UCI HAR Dataset.zip     /home/ubuntu/pynb/LSTM-Human-Activity-Recognition     data	 LSTM_files  LSTM_OLD.ipynb  README.md     LICENSE  LSTM.ipynb  lstm.py	     screenlog.0      Dataset is now located at: data/UCI HAR Dataset/    ```python  #: Graph input/output x = tf.placeholder(tf.float32  [None  n_steps  n_input]) y = tf.placeholder(tf.float32  [None  n_classes])  #: Graph weights weights = {     'hidden': tf.Variable(tf.random_normal([n_input  n_hidden]))  #: Hidden layer weights     'out': tf.Variable(tf.random_normal([n_hidden  n_classes]  mean=1.0)) } biases = {     'hidden': tf.Variable(tf.random_normal([n_hidden]))      'out': tf.Variable(tf.random_normal([n_classes])) }  pred = LSTM_RNN(x  weights  biases)  #: Loss  optimizer and evaluation l2 = lambda_loss_amount * sum(     tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables() ) #: L2 loss prevents this overkill neural network to overfit the data cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y  logits=pred)) + l2 #: Softmax loss optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) #: Adam Optimizer  correct_pred = tf.equal(tf.argmax(pred 1)  tf.argmax(y 1)) accuracy = tf.reduce_mean(tf.cast(correct_pred  tf.float32))  ```   """;Natural Language Processing;https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition
"""""";Graphs;https://github.com/PetarV-/DGI
"""""";Computer Vision;https://github.com/ajinkyakhoche/Object-Detection-Project
"""The ""unbatched"" version should not be used anymore. If you've downloaded this code previously  please update it immediately to  the new version. The old version included a bug!  If you're looking for a pytorch implementation we recommend https://github.com/mseitzer/pytorch-fid  Requirements: TF 1.1+  Python 3.x   Improved WGAN (WGAN-GP) implementation forked from https://github.com/igul222/improved_wgan_training   """;General;https://github.com/bioinf-jku/TTUR
"""Once in the tmux session  you can see all your windows with ctrl-b w.   environments with low latency.  Alternatively  you can run the   stderr.  If you run both the agent and the environment on nearby   (you can connect to this view via note above)   ``` conda create --name universe-starter-agent python=3.5 source activate universe-starter-agent  brew install tmux htop      #: On Linux use sudo apt-get install -y tmux htop  pip install gym[atari] pip install universe pip install six pip install tensorflow conda install -y -c https://conda.binstar.org/menpo opencv3 conda install -y numpy conda install -y scipy ```   Add the following to your `.bashrc` so that you'll have the correct environment when the `train.py` script spawns new bash shells ```source activate universe-starter-agent```   """;Reinforcement Learning;https://github.com/vladfi1/universe-starter-agent
"""Once in the tmux session  you can see all your windows with ctrl-b w.   environments with low latency.  Alternatively  you can run the   stderr.  If you run both the agent and the environment on nearby   (you can connect to this view via note above)   ``` conda create --name universe-starter-agent python=3.5 source activate universe-starter-agent  brew install tmux htop      #: On Linux use sudo apt-get install -y tmux htop  pip install gym[atari] pip install universe pip install six pip install tensorflow conda install -y -c https://conda.binstar.org/menpo opencv3 conda install -y numpy conda install -y scipy ```   Add the following to your `.bashrc` so that you'll have the correct environment when the `train.py` script spawns new bash shells ```source activate universe-starter-agent```   """;General;https://github.com/vladfi1/universe-starter-agent
"""(currently working on t2) `python srez_main.py --dataset_input /home/enhaog/GANCS/srez/dataset_MRI/phantom --batch_size 8 --run train --summary_period 123 --sample_size 256 --train_time 10  --train_dir train_save_all --R_factor 4 --R_alpha 3 --R_seed 0`                (currently working on t2 for DCE) `python srez_main.py --run train --dataset_input /home/enhaog/GANCS/srez/dataset_MRI/abdominal_DCE --sample_size 200 --sample_size_y 100 --sampling_pattern /home/enhaog/GANCS/srez/dataset_MRI/sampling_pattern_DCE/mask_2dvardesnity_radiaview_4fold.mat --batch_size 4  --summary_period 125 --sample_test 32 --sample_train 10000 --train_time 200  --train_dir train_DCE_test `   """;General;https://github.com/gongenhao/GANCS
"""(currently working on t2) `python srez_main.py --dataset_input /home/enhaog/GANCS/srez/dataset_MRI/phantom --batch_size 8 --run train --summary_period 123 --sample_size 256 --train_time 10  --train_dir train_save_all --R_factor 4 --R_alpha 3 --R_seed 0`                (currently working on t2 for DCE) `python srez_main.py --run train --dataset_input /home/enhaog/GANCS/srez/dataset_MRI/abdominal_DCE --sample_size 200 --sample_size_y 100 --sampling_pattern /home/enhaog/GANCS/srez/dataset_MRI/sampling_pattern_DCE/mask_2dvardesnity_radiaview_4fold.mat --batch_size 4  --summary_period 125 --sample_test 32 --sample_train 10000 --train_time 200  --train_dir train_DCE_test `   """;Computer Vision;https://github.com/gongenhao/GANCS
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	``` 	 2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```   	 4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. Follow the next sections to download pre-computed object proposals and pre-trained ImageNet models   1. Clone the Fast R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/fast-rcnn.git   ```    2. We'll call the directory that you cloned Fast R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*        **Note 1:** If you didn't clone Fast R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `fast-rcnn` branch (or equivalent detached state). This will happen automatically *if you follow these instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```      4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```      5. Download pre-computed Fast R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_fast_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `fast_rcnn_models`. See `data/README.md` for details.   Requirements: hardware  Basic installation   1. Clone the Fast R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/fast-rcnn.git   ```    2. We'll call the directory that you cloned Fast R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*        **Note 1:** If you didn't clone Fast R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `fast-rcnn` branch (or equivalent detached state). This will happen automatically *if you follow these instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```      4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```      5. Download pre-computed Fast R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_fast_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `fast_rcnn_models`. See `data/README.md` for details.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  **Python**  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007. The object proposals are pre-computed in order to reduce installation requirements.  **Note:** If the demo crashes Caffe because your GPU doesn't have enough memory  try running the demo with a small network  e.g.  `./tools/demo.py --net caffenet` or with `--net vgg_cnn_m_1024`. Or run in CPU mode `./tools/demo.py --cpu`. Type `./tools/demo.py -h` for usage.  **MATLAB**  There's also a *basic* MATLAB demo  though it's missing some minor bells and whistles compared to the Python version. ```Shell cd $FRCN_ROOT/matlab matlab #: wait for matlab to start...  #: At the matlab prompt  run the script: >> fast_rcnn_demo ```  Fast R-CNN training is implemented in Python only  but test-time detection functionality also exists in MATLAB. See `matlab/fast_rcnn_demo.m` and `matlab/fast_rcnn_im_detect.m` for details.  **Computing object proposals**  The demo uses pre-computed selective search proposals computed with [this code](https://github.com/rbgirshick/rcnn/blob/master/selective_search/selective_search_boxes.m). If you'd like to compute proposals on your own images  there are many options. Here are some pointers; if you run into trouble using these resources please direct questions to the respective authors.  1. Selective Search: [original matlab code](http://disi.unitn.it/~uijlings/MyHomepage/index.php#page=projects1)  [python wrapper](https://github.com/sergeyk/selective_search_ijcv_with_python) 2. EdgeBoxes: [matlab code](https://github.com/pdollar/edges) 3. GOP and LPO: [python code](http://www.philkr.net/) 4. MCG: [matlab code](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/) 5. RIGOR: [matlab code](http://cpl.cc.gatech.edu/projects/RIGOR/)  Apologies if I've left your method off this list. Feel free to contact me and ask for it to be included.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	``` 	 2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```   	 4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. Follow the next sections to download pre-computed object proposals and pre-trained ImageNet models   **Train** a Fast R-CNN detector. For example  train a VGG16 network on VOC 2007 trainval:  ```Shell ./tools/train_net.py --gpu 0 --solver models/VGG16/solver.prototxt \ 	--weights data/imagenet_models/VGG16.v2.caffemodel ```  If you see this error  ``` EnvironmentError: MATLAB command 'matlab' not found. Please add 'matlab' to your PATH. ```  then you need to make sure the `matlab` binary is in your `$PATH`. MATLAB is currently required for PASCAL VOC evaluation.  **Test** a Fast R-CNN detector. For example  test the VGG 16 network on VOC 2007 test:  ```Shell ./tools/test_net.py --gpu 1 --def models/VGG16/test.prototxt \ 	--net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel ```  Test output is written underneath `$FRCN_ROOT/output`.  **Compress** a Fast R-CNN model using truncated SVD on the fully-connected layers:  ```Shell ./tools/compress_net.py --def models/VGG16/test.prototxt \ 	--def-svd models/VGG16/compressed/test.prototxt \     --net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel #: Test the model you just compressed ./tools/test_net.py --gpu 0 --def models/VGG16/compressed/test.prototxt \ 	--net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000_svd_fc6_1024_fc7_256.caffemodel ```   """;Computer Vision;https://github.com/sunhui1234/haha
"""``` git clone https://github.com/antoinecollas/transformer_neural_machine_translation cd transformer_neural_machine_translation conda env update conda activate transformer ```   """;General;https://github.com/antoinecollas/transformer_neural_machine_translation
"""``` git clone https://github.com/antoinecollas/transformer_neural_machine_translation cd transformer_neural_machine_translation conda env update conda activate transformer ```   """;Natural Language Processing;https://github.com/antoinecollas/transformer_neural_machine_translation
"""""";General;https://github.com/ppwwyyxx/moco.tensorflow
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;Computer Vision;https://github.com/EMsnap/RobotSorting
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;General;https://github.com/EMsnap/RobotSorting
"""* [Photoshop Quick Selection Tool](https://helpx.adobe.com/photoshop/using/making-quick-selections.html) * [GIMP Selection Tool](https://docs.gimp.org/en/gimp-tools-selection.html) * [GIMP G'MIC Interactive Foreground Extraction tool](http://gmic.eu/gimp.shtml)   This code is based on torch. It has been tested on Ubuntu 14.04 LTS.  Dependencies: * [Torch](https://github.com/torch/torch7) (with [matio-ffi](https://github.com/soumith/matio-ffi.torch) and [loadcaffe](https://github.com/szagoruyko/loadcaffe)) * [Matlab](https://www.mathworks.com/) or [Octave](https://www.gnu.org/software/octave/)  CUDA backend: * [CUDA](https://developer.nvidia.com/cuda-downloads) * [cudnn](https://developer.nvidia.com/cudnn)  Download VGG-19: ``` sh models/download_models.sh ```  Compile ``cuda_utils.cu`` (Adjust ``PREFIX`` and ``NVCC_PREFIX`` in ``makefile`` for your machine): ``` make clean && make ```   To generate all results (in ``examples/``) using the provided scripts  simply run ``` run('gen_laplacian/gen_laplacian.m') ``` in Matlab or Octave and then ``` python gen_all.py ``` in Python. The final output will be in ``examples/final_results/``.   1. Given input and style images with semantic segmentation masks  put them in ``examples/`` respectively. They will have the following filename form: ``examples/input/in<id>.png``  ``examples/style/tar<id>.png`` and ``examples/segmentation/in<id>.png``  ``examples/segmentation/tar<id>.png``; 2. Compute the matting Laplacian matrix using ``gen_laplacian/gen_laplacian.m`` in Matlab. The output matrix will have the following filename form: ``gen_laplacian/Input_Laplacian_3x3_1e-7_CSR<id>.mat``;   **Note: Please make sure that the content image resolution is consistent for Matting Laplacian computation in Matlab and style transfer in Torch  otherwise the result won't be correct.**  3. Run the following script to generate segmented intermediate result: ``` th neuralstyle_seg.lua -content_image <input> -style_image <style> -content_seg <inputMask> -style_seg <styleMask> -index <id> -serial <intermediate_folder> ``` 4. Run the following script to generate final result: ``` th deepmatting_seg.lua -content_image <input> -style_image <style> -content_seg <inputMask> -style_seg <styleMask> -index <id> -init_image <intermediate_folder/out<id>_t_1000.png> -serial <final_folder> -f_radius 15 -f_edge 0.01 ```  You can pass `-backend cudnn` and `-cudnn_autotune` to both Lua scripts (step 3. and 4.) to potentially improve speed and memory usage. `libcudnn.so` must be in your `LD_LIBRARY_PATH`. This requires [cudnn.torch](https://github.com/soumith/cudnn.torch).   Here are some results from our algorithm (from left to right are input  style and our output): <p align='center'>   <img src='examples/input/in3.png' height='194' width='290'/>   <img src='examples/style/tar3.png' height='194' width='290'/>   <img src='examples/refine_posterization/refine_3.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in4.png' height='194' width='290'/>   <img src='examples/style/tar4.png' height='194' width='290'/>   <img src='examples/refine_posterization/refine_4.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in13.png' height='194' width='290'/>   <img src='examples/style/tar13.png' height='194' width='290'/>   <img src='examples/refine_posterization/refine_13.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in9.png' height='194' width='290'/>   <img src='examples/style/tar9.png' height='194' width='290'/>   <img src='examples/refine_posterization/refine_9.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in20.png' height='194' width='290'/>   <img src='examples/style/tar20.png' height='194' width='290'/>   <img src='examples/refine_posterization/refine_20.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in1.png' height='194' width='290'/>   <img src='examples/style/tar1.png' height='194' width='290'/>   <img src='examples/refine_posterization/refine_1.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in39.png' height='194' width='290'/>   <img src='examples/style/tar39.png' height='194' width='290'/>   <img src='examples/refine_posterization/refine_39.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in57.png' height='194' width='290'/>   <img src='examples/style/tar57.png' height='194' width='290'/>   <img src='examples/refine_posterization/refine_57.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in47.png' height='194' width='290'/>   <img src='examples/style/tar47.png' height='194' width='290'/>   <img src='examples/refine_posterization/refine_47.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in58.png' height='194' width='290'/>   <img src='examples/style/tar58.png' height='194' width='290'/>   <img src='examples/refine_posterization/refine_58.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in51.png' height='194' width='290'/>   <img src='examples/style/tar51.png' height='194' width='290'/>   <img src='examples/refine_posterization/refine_51.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in7.png' height='194' width='290'/>   <img src='examples/style/tar7.png' height='194' width='290'/>   <img src='examples/refine_posterization/refine_7.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in23.png' width='290'/>   <img src='examples/input/in23.png' width='290'/>   <img src='examples/final_results/best23_t_1000.png' width='290'/> </p>  <p align='center'>   <img src='examples/input/in16.png' height='194' width='290'/>   <img src='examples/style/tar16.png' height='194' width='290'/>   <img src='examples/refine_posterization/refine_16.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in30.png' height='194' width='290'/>   <img src='examples/style/tar30.png' height='194' width='290'/>   <img src='examples/refine_posterization/refine_30.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in2.png' height='194' width='290'/>   <img src='examples/style/tar2.png' height='194' width='290'/>   <img src='examples/final_results/best2_t_1000.png' height='194' width='290'/> </p>  <p align='center'>   <img src='examples/input/in11.png'  width='290'/>   <img src='examples/style/tar11.png' width='290'/>   <img src='examples/refine_posterization/refine_11.png'  width='290'/> </p>    """;Computer Vision;https://github.com/purushothamgowthu/deep-photo-styletransfer
"""""";Audio;https://github.com/scpark20/universal-music-translation
"""""";Sequential;https://github.com/scpark20/universal-music-translation
"""""";General;https://github.com/thomlake/pytorch-attention
"""Install Pytorch-rl from Pypi (recommended):  pip install pytorch-policy   <img width=""160px"" height=""22px"" href=""https://github.com/pytorch/pytorch"" src=""https://pp.userapi.com/c847120/v847120960/82b4/xGBK9pXAkw8.jpg"">   PPO (https://github.com/ikostrikov/pytorch-a2c-ppo-acktr)   Super Mario Bros (Follow instructions to install gym-retro https://github.com/openai/retro)   """;Computer Vision;https://github.com/meg965/pytorch-rl
"""Install Pytorch-rl from Pypi (recommended):  pip install pytorch-policy   <img width=""160px"" height=""22px"" href=""https://github.com/pytorch/pytorch"" src=""https://pp.userapi.com/c847120/v847120960/82b4/xGBK9pXAkw8.jpg"">   PPO (https://github.com/ikostrikov/pytorch-a2c-ppo-acktr)   Super Mario Bros (Follow instructions to install gym-retro https://github.com/openai/retro)   """;General;https://github.com/meg965/pytorch-rl
"""Follow the steps in the installation documentation in [doc/installation.md](doc/installation.md).     """;General;https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin
"""You should prepare lmdb dataset   """;Computer Vision;https://github.com/rosinality/style-based-gan-pytorch
"""""";Computer Vision;https://github.com/DT42/squeezenet_demo
"""""";Computer Vision;https://github.com/zhangchi9/Airbus_Ship_Detection
"""the environment   Step 1:   First  please follow the instructions of the part dependencies on the Link below. https://github.com/udacity/deep-reinforcement-learning  Step 2:  Install the Unity-Banana-Environment for your operating system.    https://github.com/udacity/deep-reinforcement-learning/tree/master/p1_navigation  Step 3:    Copy the files of my Github-Project-Directory in your local project-directory.  Step 4:   Start your jupyter Notebook and call the ""navigation.ipynb""-File in the working directory. (If jupyter notebook is not available on your computer  install it:     https://jupyter.readthedocs.io/en/latest/install.html)  good luck ‚ò∫Ô∏è   """;Reinforcement Learning;https://github.com/NikolausBerl/Udacity_DRLN_Navigation_Project
"""***Note***: there is now a PyTorch version of this toolkit ([fairseq-py](https://github.com/pytorch/fairseq)) and new development efforts will focus on it. The Lua version is preserved here  but is provided without any support.  This is fairseq  a sequence-to-sequence learning toolkit for [Torch](http://torch.ch/) from Facebook AI Research tailored to Neural Machine Translation (NMT). It implements the convolutional NMT models proposed in [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) and [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344) as well as a standard LSTM-based model. It features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU. We provide pre-trained models for English to French  English to German and English to Romanian translation.  ![Model](fairseq.gif)   * A computer running macOS or Linux * For training new models  you'll also need a NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl) * A [Torch installation](http://torch.ch/docs/getting-started.html). For maximum speed  we recommend using LuaJIT and [Intel MKL](https://software.intel.com/en-us/intel-mkl). * A recent version [nn](https://github.com/torch/nn). The minimum required version is from May 5th  2017. A simple `luarocks install nn` is sufficient to update your locally installed version.  Install fairseq by cloning the GitHub repository and running ``` luarocks make rocks/fairseq-scm-1.rockspec ``` LuaRocks will fetch and build any additional dependencies that may be missing. In order to install the CPU-only version (which is only useful for translating new data with an existing model)  do ``` luarocks make rocks/fairseq-cpu-scm-1.rockspec ```  The LuaRocks installation provides a command-line tool that includes the following functionality: * `fairseq preprocess`: Data pre-processing: build vocabularies and binarize training data * `fairseq train`: Train a new model on one or multiple GPUs * `fairseq generate`: Translate pre-processed data with a trained model * `fairseq generate-lines`: Translate raw text with a trained model * `fairseq score`: BLEU scoring of generated translations against reference translations * `fairseq tofloat`: Convert a trained model to a CPU model * `fairseq optimize-fconv`: Optimize a fully convolutional model for generation. This can also be achieved by passing the `-fconvfast` flag to the generation scripts.   $ cd data/  $ bash prepare-iwslt14.sh  $ cd ..   $ mkdir -p trainings/blstm   $ mkdir -p trainings/fconv   $ mkdir -p trainings/convenc   Use the CUDA_VISIBLE_DEVICES environment variable to select specific GPUs or -ngpus to change the number of GPU devices that will be used.   | Timings: setup 0.1s (0.1%)  encoder 1.9s (1.4%)  decoder 108.9s (79.9%)  search_results 0.0s (0.0%)  search_prune 12.5s (9.2%)   """;Natural Language Processing;https://github.com/facebookresearch/fairseq
"""- Clone this repo: ```bash git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix cd pytorch-CycleGAN-and-pix2pix ```  - Install [PyTorch](http://pytorch.org) and 0.4+ and other dependencies (e.g.  torchvision  [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate)).   - For pip users  please type the command `pip install -r requirements.txt`.   - For Conda users  you can create a new Conda environment using `conda env create -f environment.yml`.   - For Docker users  we provide the pre-built Docker image and Dockerfile. Please refer to our [Docker](docs/docker.md) page.   - For Repl users  please click [![Run on Repl.it](https://repl.it/badge/github/junyanz/pytorch-CycleGAN-and-pix2pix)](https://repl.it/github/junyanz/pytorch-CycleGAN-and-pix2pix).   <a href=""https://github.com/yunjey/mnist-svhn-transfer"">[Minimal PyTorch]</a> (by yunjey)   <a href=""https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN"">[Mxnet]</a> (by Ldpe2G)    <a href=""https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Oneflow-Python/CycleGAN"">[OneFlow]</a> (by Ldpe2G)   <a href=""https://github.com/taey16/pix2pixBEGAN.pytorch"">[Pytorch]</a> (by taey16)   Test the model (bash ./scripts/test_pix2pix.sh):   You can download a pretrained model (e.g. horse2zebra) with the following script:  bash ./scripts/download_cyclegan_model.sh horse2zebra   bash ./datasets/download_cyclegan_dataset.sh horse2zebra   bash ./scripts/download_pix2pix_model.sh facades_label2photo   """;Computer Vision;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix
"""- Clone this repo: ```bash git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix cd pytorch-CycleGAN-and-pix2pix ```  - Install [PyTorch](http://pytorch.org) and 0.4+ and other dependencies (e.g.  torchvision  [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate)).   - For pip users  please type the command `pip install -r requirements.txt`.   - For Conda users  you can create a new Conda environment using `conda env create -f environment.yml`.   - For Docker users  we provide the pre-built Docker image and Dockerfile. Please refer to our [Docker](docs/docker.md) page.   - For Repl users  please click [![Run on Repl.it](https://repl.it/badge/github/junyanz/pytorch-CycleGAN-and-pix2pix)](https://repl.it/github/junyanz/pytorch-CycleGAN-and-pix2pix).   <a href=""https://github.com/yunjey/mnist-svhn-transfer"">[Minimal PyTorch]</a> (by yunjey)   <a href=""https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN"">[Mxnet]</a> (by Ldpe2G)    <a href=""https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Oneflow-Python/CycleGAN"">[OneFlow]</a> (by Ldpe2G)   <a href=""https://github.com/taey16/pix2pixBEGAN.pytorch"">[Pytorch]</a> (by taey16)   Test the model (bash ./scripts/test_pix2pix.sh):   You can download a pretrained model (e.g. horse2zebra) with the following script:  bash ./scripts/download_cyclegan_model.sh horse2zebra   bash ./datasets/download_cyclegan_dataset.sh horse2zebra   bash ./scripts/download_pix2pix_model.sh facades_label2photo   """;General;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix
"""See data/faster_rcnn_inception_resnet_v2_atrous_text.config for example configuration The parameter: second_stage_localization_loss_weight_oriented is the weight for the oriented bounding box prediction.   Changing the network configuration setting is easy. For example  to change the different aspect ratios of the anchors used  simply changing the grid_anchor_generator in the configuration file.   You do not need to use blaze build to build the code. Simply run the code from the root directory for fast experiment.   """;Computer Vision;https://github.com/dafanghe/Tensorflow_SceneText_Oriented_Box_Predictor
"""- Install tensorflow - Opencv   Using 'git clone https://github.com/yaxingwang/Transferring-GANs'  You will get new folder whose name is 'Transferring-GANs' in your current path  then  use 'cd Transferring-GANs' to enter the downloaded new folder   Download dataset or use your dataset.   """;General;https://github.com/DevashishJoshi/Transferring-GANs-FYP
"""- Install tensorflow - Opencv   Using 'git clone https://github.com/yaxingwang/Transferring-GANs'  You will get new folder whose name is 'Transferring-GANs' in your current path  then  use 'cd Transferring-GANs' to enter the downloaded new folder   Download dataset or use your dataset.   """;Computer Vision;https://github.com/DevashishJoshi/Transferring-GANs-FYP
"""Install the `stn` package using:  ``` pip3 install stn ```  Then  you can call the STN layer as follows:  ```python from stn import spatial_transformer_network as transformer  out = transformer(input_feature_map  theta  out_dims) ```  **Parameters**  - `input_feature_map`: the output of the layer preceding the localization network. If the STN layer is the first layer of the network  then this corresponds to the input images. Shape should be (B  H  W  C). - `theta`: this is the output of the localization network. Shape should be (B  6) - `out_dims`: desired (H  W) of the output feature map. Useful for upsampling or downsampling. If not specified  then output dimensions will be equal to `input_feature_map` dimensions.   """;Computer Vision;https://github.com/kevinzakka/spatial-transformer-network
"""""";Computer Vision;https://github.com/PuchatekwSzortach/voc_fcn
"""""";Computer Vision;https://github.com/wandb/awesome-dl-projects
"""This is the repo for the Tensorflow implementation of cellSTORM based on the conditional generative adversarial network (cGAN) implmented by pix2pix-tensoflow. In this case it learns a mapping from input images (i.e. degraded  noisy  compressed video-sequences of dSTORM blinking events) to output images (i.e. localization maps/center of possible blinking fluorophore)  The repository has also a scipt to export a given Graph trained on GPU to a cellphone. The cellSTORM localizer APP can be found in another repo.    pip install sk-video  pip install tifffile  pip install tensorflow-gpu  pip install opencv-python  pip install sk-video  Linux with Tensorflow GPU edition + cuDNN   cd ./pix2pix-tensorflow   : Change the directory where you'Ve downloaded the Repo  cd /home/useradmin/Dropbox/Dokumente/Promotion/PROJECTS/STORM/PYTHON/pix2pix-tensorflow   Most of the steps are equivalent to Deep-Storm by Nehme et al. 2018. Please also look into their project.   A detailed list with all dependecies will follow soon!  See also the `environmet.yml` file   """;Computer Vision;https://github.com/bionanoimaging/cellSTORM-Tensorflow
"""This is the repo for the Tensorflow implementation of cellSTORM based on the conditional generative adversarial network (cGAN) implmented by pix2pix-tensoflow. In this case it learns a mapping from input images (i.e. degraded  noisy  compressed video-sequences of dSTORM blinking events) to output images (i.e. localization maps/center of possible blinking fluorophore)  The repository has also a scipt to export a given Graph trained on GPU to a cellphone. The cellSTORM localizer APP can be found in another repo.    pip install sk-video  pip install tifffile  pip install tensorflow-gpu  pip install opencv-python  pip install sk-video  Linux with Tensorflow GPU edition + cuDNN   cd ./pix2pix-tensorflow   : Change the directory where you'Ve downloaded the Repo  cd /home/useradmin/Dropbox/Dokumente/Promotion/PROJECTS/STORM/PYTHON/pix2pix-tensorflow   Most of the steps are equivalent to Deep-Storm by Nehme et al. 2018. Please also look into their project.   A detailed list with all dependecies will follow soon!  See also the `environmet.yml` file   """;General;https://github.com/bionanoimaging/cellSTORM-Tensorflow
"""Karate Club can be installed with the following pip command.  $ pip install karateclub   $ pip install karateclub --upgrade   $ cd examples/whole_graph_embedding/   """;Graphs;https://github.com/benedekrozemberczki/karateclub
"""""";Computer Vision;https://github.com/nutintin/Robotic_SP2019
"""To run this organized notebook  you need the following packages: pytorch  PIL  cv2.   """;General;https://github.com/gaetandi/cheXpert
"""To run this organized notebook  you need the following packages: pytorch  PIL  cv2.   """;Computer Vision;https://github.com/gaetandi/cheXpert
"""cd src/hdf5_tools  bash folder_to_multisize_hdf5_cmds.sh 1 YOUR_PATH_TO_RAW_FFHQ_IMAGES   """;Computer Vision;https://github.com/pfnet-research/chainer-stylegan
"""The interval that you save snapshot. You can set this parameter like as trigger.   (without GPU)   (with GPU #:n)   [x] using GPU fot generating   """;Computer Vision;https://github.com/dhgrs/chainer-VQ-VAE
"""It utilised following artifacts for text classification.   implemented using pytorch   huggingface   | SciBERT | https://github.com/allenai/scibert |  |huggingface | https://huggingface.co/transformers/ |   """;Natural Language Processing;https://github.com/abhineet/sentence_classification_pubmed_scibert
"""| provenance | https://github.com/onnx/models/tree/master/models/face_recognition/ArcFace |   """;General;https://github.com/modelhub-ai/arc-face
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO** (https://pjreddie.com/darknet/yolo/) and to **Erik Lindernoren for the PyTorch implementation** this work is based on (https://github.com/eriklindernoren/PyTorch-YOLOv3).   This directory contains python software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information on Ultralytics projects please visit: https://www.ultralytics.com.   """;Computer Vision;https://github.com/guagen/yolov3-ultralytics-source
"""$ PROJ_DIR=~/shake_shake_chainer  #: assuming you clone this repo to your home directory  $ git clone https://github.com/motokimura/shake_shake_chainer.git $PROJ_DIR   $ bash build.sh   $ bash run.sh   $ bash exec.sh  : Now you should be inside the container already running. Start TensorBoard by following:   owruby's Pytorch implementation. [GitHub]   """;General;https://github.com/motokimura/shake_shake_chainer
"""Please clone the master branch and follow the instructions to run YellowFin on [ResNext](https://arxiv.org/abs/1611.05431) for CIFAR10 and [tied LSTM](https://arxiv.org/pdf/1611.01462.pdf) on Penn Treebank for language modeling. The models are adapted from [ResNext repo](https://github.com/kuangliu/pytorch-cifar) and [PyTorch example tied LSTM repo](https://github.com/pytorch/examples/tree/master/word_language_model) respectively. Thanks to the researchers for developing the models. **For more experiments on more convolutional and recurrent neural networks  please refer to our [Tensorflow implementation](https://github.com/JianGoForIt/YellowFin) of YellowFin**.  Note YellowFin is tested with PyTorch v0.2.0 for compatibility. It is tested under Python 2.7.   <!---For MXNet users  Github user [StargazerZhu](https://github.com/StargazerZhu) has already implemented a Theano version here: [YellowFin MXNet Repo](https://github.com/StargazerZhu/YellowFin_MXNet).--->  <!---For Theano users  Github user [botev](https://github.com/botev) has already implemented a Theano version here: [YellowFin Theano Repo](https://gist.github.com/botev/f8b32c00eafee222e47393f7f0747666).--->   """;General;https://github.com/JianGoForIt/YellowFin_Pytorch
"""""";Reinforcement Learning;https://github.com/plkmo/AlphaZero_Connect4
"""Generative Adversarial Networks (GANs) are one of the most popular (and coolest) Machine Learning algorithms developed in recent times. They belong to a set of algorithms called generative models  which are widely used for unupervised learning tasks which aim to learn the uderlying structure of the given data. As the name suggests GANs allow you to generate new unseen data that mimic the actual given real data. However  GANs pose problems in training and require carefullly tuned hyperparameters.This paper aims to solve this problem.  DCGAN is one of the most popular and succesful network design for GAN. It mainly composes of convolution layers  without max pooling or fully connected layers. It uses strided convolutions and transposed convolutions  for the downsampling and the upsampling respectively.  **Generator architecture of DCGAN** <p align=""center""> <img src=""images/Generator.png"" title=""DCGAN Generator"" alt=""DCGAN Generator""> </p>  **Network Design of DCGAN:** * Replace all pooling layers with strided convolutions. * Remove all fully connected layers. * Use transposed convolutions for upsampling. * Use Batch Normalization after every layer except after the output layer of the generator and the input layer of the discriminator. * Use ReLU non-linearity for each layer in the generator except for output layer use tanh. * Use Leaky-ReLU non-linearity for each layer of the disciminator excpet for output layer use sigmoid.   """;Computer Vision;https://github.com/davidhalladay/CE-compressive-sensing-on-generative-model-pytorch
"""Anaconda Python 3.5.3   - Execute ```python train_model.py``` to train the CPC model. - Execute ```python benchmark_model.py``` to train the MLP on top of the CPC encoder.   """;General;https://github.com/davidtellez/contrastive-predictive-coding
"""1. Download ffhq-dataset from [here.](https://github.com/NVlabs/ffhq-dataset)  2. Put images1024x1024 in ffhq_dataset and thumbnails128x128 in ffhq_dataset128.  like this  ``` ... ‚îÇ ‚îú‚îÄ‚îÄ ffhq_dataset ‚îÇ     ‚îú‚îÄ‚îÄ 00000.png ‚îÇ     ‚îú‚îÄ‚îÄ 00001.png ‚îÇ     ‚îú‚îÄ‚îÄ ... ‚îÇ     ‚îî‚îÄ‚îÄ 69999.png ‚îú‚îÄ‚îÄ ffhq_dataset128 ‚îÇ     ‚îú‚îÄ‚îÄ 00000.png ‚îÇ     ‚îú‚îÄ‚îÄ 00001.png ‚îÇ     ‚îú‚îÄ‚îÄ ... ‚îÇ     ‚îî‚îÄ‚îÄ 69999.png  ‚îú‚îÄ‚îÄ main.py ‚îú‚îÄ‚îÄ model.py ... ```  3. Train StyleGAN.  ``` python main.py ```  How long does it take to train using RTX 2070  ``` 64x64     1d00h 128x128   2d00h 256x256   3d18h 512x512   6d13h(estimated) 1024x1024 unknown ```  4. After training  inference can be performed.  to draw uncurated images  ``` python pred.py -m uc ```  <img src = 'examples/uc_ffhq.png' width=1280>  to draw truncation trick images  ``` python pred.py -m tt ```  <img src = 'examples/tt_ffhq.png' width=1280>  to draw style mixing images  ``` python pred.py -m sm ```  <img src = 'examples/sm_ffhq.png' width=1280>   """;Computer Vision;https://github.com/itsuki8914/stylegan-TensorFlow
"""Transformer model with Fixup (instead of layer normalization) is available. To run the experiments  you will need to download and install the fairseq library (the provided code was tested on an earlier version: https://github.com/pytorch/fairseq/tree/5d00e8eea2644611f397d05c6c8f15083388b8b4). You can then copy the files into corresponding folders.   """;General;https://github.com/hongyi-zhang/Fixup
"""Transformer model with Fixup (instead of layer normalization) is available. To run the experiments  you will need to download and install the fairseq library (the provided code was tested on an earlier version: https://github.com/pytorch/fairseq/tree/5d00e8eea2644611f397d05c6c8f15083388b8b4). You can then copy the files into corresponding folders.   """;Computer Vision;https://github.com/hongyi-zhang/Fixup
"""**Windows portable version**: Simply download and use the latest version from the [Releases](https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases) section.  Otherwise  check [doc/installation.md](doc/installation.md) for instructions on how to build OpenPose from source.     OS: Ubuntu (14  16)  Windows (8  10)  Mac OSX  Nvidia TX2.   CUDA (Nvidia GPU)  OpenCL (AMD GPU)  and CPU-only (no GPU) versions.   Most users do not need the OpenPose C++/Python API  but can simply use the OpenPose Demo:  - **OpenPose Demo**: To easily process images/video/webcam and display/save the results. See [doc/demo_overview.md](doc/demo_overview.md). E.g.  run OpenPose in a video with: ``` #: Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi ```  - **Calibration toolbox**: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See [doc/modules/calibration_module.md](doc/modules/calibration_module.md).  - **OpenPose C++ API**: If you want to read a specific input  and/or add your custom post-processing function  and/or implement your own display/saving  check the C++ API tutorial on [examples/tutorial_api_cpp/](examples/tutorial_api_cpp/) and [doc/library_introduction.md](doc/library_introduction.md). You can create your custom code on [examples/user_code/](examples/user_code/) and quickly compile it with CMake when compiling the whole OpenPose project. Quickly **add your custom code**: See [examples/user_code/README.md](examples/user_code/README.md) for further details.  - **OpenPose Python API**: Analogously to the C++ API  find the tutorial for the Python API on [examples/tutorial_api_python/](examples/tutorial_api_python/).  - **Adding an extra module**: Check [doc/library_add_new_module.md](./doc/library_add_new_module.md).  - **Standalone face or hand detector**:     - **Face** keypoint detection **without body** keypoint detection: If you want to speed it up (but also reduce amount of detected faces)  check the OpenCV-face-detector approach in [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     - **Use your own face/hand detector**: You can use the hand and/or face keypoint detectors with your own face or hand detectors  rather than using the body detector. E.g.  useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     """;General;https://github.com/lncarter/Openpose
"""1. Git clone this repository.  $git clone https://github.com/jacquelinelala/GFN.git  $cd GFN   (If you don't have access to MATLAB  we offer a validation dataset for testing. You can download it from GoogleDrive or Pan Baidu.)  folder = 'your_downloads_directory/GOPRO_Large'; #: You should replace the your_downloads_directory by your GOPRO_Large's directory.   You should accomplish the first two steps in Test on LR-GOPRO Validation before the following steps.   """;General;https://github.com/jacquelinelala/GFN
"""1. Git clone this repository.  $git clone https://github.com/jacquelinelala/GFN.git  $cd GFN   (If you don't have access to MATLAB  we offer a validation dataset for testing. You can download it from GoogleDrive or Pan Baidu.)  folder = 'your_downloads_directory/GOPRO_Large'; #: You should replace the your_downloads_directory by your GOPRO_Large's directory.   You should accomplish the first two steps in Test on LR-GOPRO Validation before the following steps.   """;Computer Vision;https://github.com/jacquelinelala/GFN
"""- Download MPII Dataset and put its images under `data/mpii/images` - The json `mpii_annotations.json` contains all of images' annotations including train and validation.   Note:    - Download pre-trained model from shared drive and put them under `trained_models`     BaiDu Pan:  [hg_s2_b1_mobile](https://pan.baidu.com/s/15NGJv1e-_5wqpu5NvJIifQ) and  [hg_s2_b1](https://pan.baidu.com/s/1Brjc9deRehnj7FhPV0UUOQ)     Google Drive: [hg_s2_b1_mobile](https://drive.google.com/open?id=12lbNv7jTQDZArf-lVaZ9yKj6Jr7qB1tQ) and    [hg_s2_b1](https://drive.google.com/open?id=12ioJONmse658qc9fgMpzSy2D_JCdkFVg)  - Run a quick demo to predict sample image ``` python demo.py --gpuID 0 --model_json ../../trained_models/hg_s2_b1/net_arch.json  --model_weights ../../trained_models/hg_s2_b1/weights_epoch89.h5  --conf_threshold 0.1 --input_image ../../images/sample.jpg ```   """;Computer Vision;https://github.com/yuanyuanli85/Stacked_Hourglass_Network_Keras
"""""";General;https://github.com/aadil-srivastava01/End-To-End-Memory-Networks
"""""";Computer Vision;https://github.com/Kaido0/Brain-Tissue-Segment-Keras
"""Here are the experiments used in the Jupyter notebook:   Python packages:       * (virtualenv installation with python 3)      * I installed the other packages while in the virtual environment.     * matplotlib    * numpy    * pandas     * scipy  Linux packages:   Also clone this repo. run.sh assumes that this repo's top directory is a sibling directory to the tensorflow directory containing bin/activate.  To run an experiment  edit run.sh and adjust any command-line flags. Then  from this repo's top directory run ./run.sh &lt;experiment_name&gt;  where <experiment_name> can be anything you like. This will create a new directory with that name under experiments/.  You'll have to download the videos from your VM to view them. See these instructions for connecting with gcloud or ssh if you're using GCE.  To execute the Jupyter notebook  set up port forwarding from your local machine (where your browser is) to the cloud machine (where you cloned this repo). I did this by following the above instructions for ssh and running   """;Reinforcement Learning;https://github.com/google/maddpg-replication
"""""";General;https://github.com/sheryl-ai/MetaPred
"""dataset from: https://github.com/PengKiKi/camvid       classifi_mode=""one""  **kargs)   Default: ""one""  which means one-hot encoding.       classifi_mode='one')       classifi_mode=""one"")       classifi_mode='one')       classifi_mode='one')       classifi_mode=""one""):       classifi_mode=""one""        classifi_mode=""one""            https://matplotlib.org/tutorials/colors/colors.html       classifi_mode=""one""        classifi_mode=""one""  thread_num=1):           https://matplotlib.org/tutorials/colors/colors.html       path        classifi_mode='one'            https://matplotlib.org/tutorials/colors/colors.html       classifi_mode='one')   A pandas.Series.       classifi_mode='one')  Get Dice coefficient table.   A pandas.Series.       classifi_mode='one'):   - confusion_mat: pandas.Dataframe  you can get this from create_confusion_mat().   1. Clone or download     - Use the command bellow in terminal to git clone:         ```git clone https://github.com/samson6460/tf2_Segmentation.git```      - Or just download whole files using the **[Code > Download ZIP]** button in the upper right corner.      2. Install dependent packages:      ```pip install -r requirements.txt```  3. Import tf2_Segmentation:    ```import tf2_Segmentation```    """;Computer Vision;https://github.com/samson6460/tf2_Segmentation
"""    $ git clone https://github.com/VitoRazor/Gan_Architecture.git     $ cd Gan_Architecture-master/     $ pip install keras   """;General;https://github.com/VitoRazor/Gan_Architecture
"""meningioma       	  | glioma		| 	pituitary tumor              :-------------------------:|:-------------------------:|:------------------------: ![](samples/sample1.png)  |  ![](samples/sample2.png)		| ![](samples/sample3.png) ![](samples/sample4.png)  |  ![](samples/sample5.png)		| ![](samples/sample6.png)  ![](samples/sample7.png)  |  ![](samples/sample8.png)		| ![](samples/sample9.png)    Here I will explain how to get the data and convert it into the usable form. You can run the train and run model using [notebook](https://github.com/adityajn105/brain-tumor-segmentation-unet/blob/master/brain-tumor-segmentation.ipynb).   I have used brain-tumor segment dataset which is available on the internet. You can run [download_data.sh](https://github.com/adityajn105/brain-tumor-segmentation-unet/blob/master/download_data.sh) shell script to download all data. It contains 3064 MRI images and 3064 masks.  > bash tumor-segmentation-unet/download_data.sh  After that run the following command to convert data in useable form. > python tumor-segmentation-unet/mat_to_numpy.py brain_tumor_dataset/    """;Computer Vision;https://github.com/adityajn105/brain-tumor-segmentation-unet
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.  git clone https://github.com/affinelayer/pix2pix-tensorflow.git  cd pix2pix-tensorflow   ```sh #: clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git cd pix2pix-tensorflow #: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir facades_train \   --max_epochs 200 \   --input_dir facades/train \   --which_direction BtoA #: test the model python pix2pix.py \   --mode test \   --output_dir facades_test \   --input_dir facades/val \   --checkpoint facades_train ```  The test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.  If you have Docker installed  you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:  ```sh #: train the model python tools/dockrun.py python pix2pix.py \       --mode train \       --output_dir facades_train \       --max_epochs 200 \       --input_dir facades/train \       --which_direction BtoA #: test the model python tools/dockrun.py python pix2pix.py \       --mode test \       --output_dir facades_test \       --input_dir facades/val \       --checkpoint facades_train ```   <img src=""docs/combine.png"" width=""900px""/>  ```sh #: Resize source images python tools/process.py \   --input_dir photos/original \   --operation resize \   --output_dir photos/resized #: Create images with blank centers python tools/process.py \   --input_dir photos/resized \   --operation blank \   --output_dir photos/blank #: Combine resized images with blanked images python tools/process.py \   --input_dir photos/resized \   --b_dir photos/blank \   --operation combine \   --output_dir photos/combined #: Split into train/val set python tools/split.py \   --dir photos/combined ```  The folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.   """;Computer Vision;https://github.com/JobQiu/hackrice
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.  git clone https://github.com/affinelayer/pix2pix-tensorflow.git  cd pix2pix-tensorflow   ```sh #: clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git cd pix2pix-tensorflow #: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir facades_train \   --max_epochs 200 \   --input_dir facades/train \   --which_direction BtoA #: test the model python pix2pix.py \   --mode test \   --output_dir facades_test \   --input_dir facades/val \   --checkpoint facades_train ```  The test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.  If you have Docker installed  you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:  ```sh #: train the model python tools/dockrun.py python pix2pix.py \       --mode train \       --output_dir facades_train \       --max_epochs 200 \       --input_dir facades/train \       --which_direction BtoA #: test the model python tools/dockrun.py python pix2pix.py \       --mode test \       --output_dir facades_test \       --input_dir facades/val \       --checkpoint facades_train ```   <img src=""docs/combine.png"" width=""900px""/>  ```sh #: Resize source images python tools/process.py \   --input_dir photos/original \   --operation resize \   --output_dir photos/resized #: Create images with blank centers python tools/process.py \   --input_dir photos/resized \   --operation blank \   --output_dir photos/blank #: Combine resized images with blanked images python tools/process.py \   --input_dir photos/resized \   --b_dir photos/blank \   --operation combine \   --output_dir photos/combined #: Split into train/val set python tools/split.py \   --dir photos/combined ```  The folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.   """;General;https://github.com/JobQiu/hackrice
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;General;https://github.com/zsl1996/mask
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/zsl1996/mask
"""*** __Epsilon decay:__ 0.9995  __Batch-size:__ 64  __Gamma:__ 0.75  __Prioritized fraction:__ 0.25     """;Reinforcement Learning;https://github.com/matthewsparr/Deep-Zork
"""""";Sequential;https://github.com/erickrf/autoencoder
"""The project is built using Python 2.7.14  Tensorflow 1.6.0 and Keras 2.1.6. Install these dependencies to get a development env running ``` sudo easy_install --upgrade pip sudo easy_install --upgrade six sudo pip install tensorflow sudo pip install keras pip install opencv-python pip install h5py pip install unidecode python -mpip install matplotlib ```  """;General;https://github.com/Pendulibrium/ai-visual-storytelling-seq2seq
"""put the images in the folder named ""data"". They are used for training.   crop.py helps to randomly crop pictures from  the folder you assigned.  example: ``` python crop.py pictures data ```         put the image in a folder named ""val"". They are used for validation.  example: ``` python crop.py for_valid val ```  like this ``` main.py pred.py data   ‚îú 000.jpg   ‚îú aaa.png   ...   ‚îî zzz.jpg val   ‚îú 111.jpg   ‚îú bbb.png   ...   ‚îî xxx.jpg test   ‚îú 222.jpg   ‚îú ccc.png   ...   ‚îî yyy.jpg  ```  To train ``` python main.py ```  To test ``` python pred.py test ```   upper left: black and white picture  upper right: given hints  under left: inference by pix2pix  under right: Ground Truth  <img src = 'baboons.png' >  <img src = 'paprikas.png' >   """;Computer Vision;https://github.com/itsuki8914/pix2pix-automaticColorization
"""put the images in the folder named ""data"". They are used for training.   crop.py helps to randomly crop pictures from  the folder you assigned.  example: ``` python crop.py pictures data ```         put the image in a folder named ""val"". They are used for validation.  example: ``` python crop.py for_valid val ```  like this ``` main.py pred.py data   ‚îú 000.jpg   ‚îú aaa.png   ...   ‚îî zzz.jpg val   ‚îú 111.jpg   ‚îú bbb.png   ...   ‚îî xxx.jpg test   ‚îú 222.jpg   ‚îú ccc.png   ...   ‚îî yyy.jpg  ```  To train ``` python main.py ```  To test ``` python pred.py test ```   upper left: black and white picture  upper right: given hints  under left: inference by pix2pix  under right: Ground Truth  <img src = 'baboons.png' >  <img src = 'paprikas.png' >   """;General;https://github.com/itsuki8914/pix2pix-automaticColorization
"""[chrome web store](https://chrome.google.com/webstore/detail/arxiv-clip/enkadffnndphdjnpjamejdjlcbkkbpmp)   """;Computer Vision;https://github.com/jojonki/arxiv-clip
"""To avoid any conflict with your existing Python setup  and to keep this project self-contained  it is suggested to work in a virtual environment with [`virtualenv`](http://docs.python-guide.org/en/latest/dev/virtualenvs/). To install `virtualenv`: ``` pip install --upgrade virtualenv ``` Create a virtual environment  activate it and install the requirements in [`requirements.txt`](requirements.txt). ``` virtualenv venv source venv/bin/activate pip install -r requirements.txt ```   You can use the [`main.py`](main.py) script in order to run reinforcement learning experiments with MAML. This script was tested with Python 3.5. Note that some environments may also work with Python 2.7 (all experiments besides MuJoCo-based environments). ``` python main.py --env-name HalfCheetahDir-v1 --num-workers 8 --fast-lr 0.1 --max-kl 0.01 --fast-batch-size 20 --meta-batch-size 40 --num-layers 2 --hidden-size 100 --num-batches 1000 --gamma 0.99 --tau 1.0 --cg-damping 1e-5 --ls-max-steps 15 --output-folder maml-halfcheetah-dir --device cuda ```   """;General;https://github.com/dragen1860/MAML-Pytorch-RL
"""""";Computer Vision;https://github.com/lamia482/octconv-pytorch
"""RetinaFace is a practical single-stage [SOTA](http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html) face detector which is initially described in [arXiv technical report](https://arxiv.org/abs/1905.00641)  ![demoimg1](https://github.com/deepinsight/insightface/blob/master/resources/11513D05.jpg)  ![demoimg2](https://github.com/deepinsight/insightface/blob/master/resources/widerfacevaltest.png)   mkdir ./Temp/train_data   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/1996scarlet/ArcFace-Multiplex-Recognition
"""Implementation: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix   """;Computer Vision;https://github.com/cvp19g2/cvp19g2
"""Implementation: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix   """;General;https://github.com/cvp19g2/cvp19g2
"""This folder contains the deploy files(include generator scripts) and pre-train models of resnet-v1  resnet-v2  inception-v3  inception-resnet-v2 and densenet(coming soon).  We didn't train any model from scratch  some of them are converted from other deep learning framworks (inception-v3 from [mxnet](https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-inception-v3.md)  inception-resnet-v2 from [tensorflow](https://github.com/tensorflow/models/blob/master/slim/nets/inception_resnet_v2.py))  some of them are converted from other modified caffe ([resnet-v2](https://github.com/yjxiong/caffe/tree/mem)). But to achieve the original performance  finetuning is performed on imagenet for several epochs.   The main contribution belongs to the authors and model trainers.   """;Computer Vision;https://github.com/GeekLiB/caffe-model
"""This folder contains the deploy files(include generator scripts) and pre-train models of resnet-v1  resnet-v2  inception-v3  inception-resnet-v2 and densenet(coming soon).  We didn't train any model from scratch  some of them are converted from other deep learning framworks (inception-v3 from [mxnet](https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-inception-v3.md)  inception-resnet-v2 from [tensorflow](https://github.com/tensorflow/models/blob/master/slim/nets/inception_resnet_v2.py))  some of them are converted from other modified caffe ([resnet-v2](https://github.com/yjxiong/caffe/tree/mem)). But to achieve the original performance  finetuning is performed on imagenet for several epochs.   The main contribution belongs to the authors and model trainers.   """;General;https://github.com/GeekLiB/caffe-model
"""""";Sequential;https://github.com/thomlake/pytorch-attention
"""""";General;https://github.com/Linear95/CLUB
"""""";General;https://github.com/muellerzr/CodeFest_2019
"""""";Natural Language Processing;https://github.com/muellerzr/CodeFest_2019
"""- Download the MNIST dataset from [here](http://yann.lecun.com/exdb/mnist/) and CelebA from [here](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). - Setup path in [`examples/gans.py`](examples/gans.py): `MNIST_PATH` is the directory to put MNIST dataset and `CELEBA_PATH` is the directory to put CelebA dataset. `SAVE_PATH` is the directory to save output images and trained model.   <!-- *Name* | *MNIST* |*CelebA* |   - GAN models are defined in [`src/nets/`](src/nets/). - The script [`example/gans.py`](example/gans.py) contains experiments of all the GAN models.   """;Computer Vision;https://github.com/conan7882/tf-gans
"""- Download the MNIST dataset from [here](http://yann.lecun.com/exdb/mnist/) and CelebA from [here](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). - Setup path in [`examples/gans.py`](examples/gans.py): `MNIST_PATH` is the directory to put MNIST dataset and `CELEBA_PATH` is the directory to put CelebA dataset. `SAVE_PATH` is the directory to save output images and trained model.   <!-- *Name* | *MNIST* |*CelebA* |   - GAN models are defined in [`src/nets/`](src/nets/). - The script [`example/gans.py`](example/gans.py) contains experiments of all the GAN models.   """;General;https://github.com/conan7882/tf-gans
"""The evaluation script can take in image or folder of images (recursively). Please look at the folder *`./data/samples`* in this repo to see an example input image folder. After evaluation  the script will generate a csv file  with the leftmost column is the image filename  and all other columns contain the confidence score for corresponding class.  The pretrained model can be downloaded either from [Dropbox](https://www.dropbox.com/s/pskw0fsjhnrlmyj/010.ckpt?dl=1) (wget-friendly) or  [Gdrive](https://drive.google.com/open?id=1znFK4xtXkNrcRBVF9NCUHHjKMMawgKf0).  ```bash #: clone this repo git clone https://github.com/johntd54/stanford_car cd stanford_car  #: create a new python 3.7 environment with conda conda create -n stanford_car_eval python=3.7 conda activate stanford_car_eval  mkdir ckpt wget https://www.dropbox.com/s/pskw0fsjhnrlmyj/010.ckpt?dl=1 -O ckpt/010.ckpt #: download the pre-trained model into folder `ckpt` #: or GDrive: https://drive.google.com/open?id=1znFK4xtXkNrcRBVF9NCUHHjKMMawgKf0  #: install supporting libraries pip install -r requirements.txt conda install pytorch torchvision cudatoolkit=10.0 -c pytorch #: or `pip install torch torchvision` if gpu is not available  #: take note of the evaluation folder and run the evaluation script #: the result will be stored at ./data/result.csv and ./data/confidence.csv #: remove the `--gpu` parameter on non-gpu machine python evaluate.py [image_folder] --ckpt [checkpoint_path] --gpu ```  The above step-by-step operation will output 2 files: - *`./data/confidence.csv`*: this file contains 197 columns  the first column is filename  the other 196 columns are confidence score for corresponding class. The class label can be obtained at *`./data/meta.json`*. - *`./data/result.csv`*: this file contains 3 columns  the first column is filename  the second column is the class index  and the third column is class label (looked up from `./data/meta.json`.   """;General;https://github.com/johntd54/stanford_car
"""The evaluation script can take in image or folder of images (recursively). Please look at the folder *`./data/samples`* in this repo to see an example input image folder. After evaluation  the script will generate a csv file  with the leftmost column is the image filename  and all other columns contain the confidence score for corresponding class.  The pretrained model can be downloaded either from [Dropbox](https://www.dropbox.com/s/pskw0fsjhnrlmyj/010.ckpt?dl=1) (wget-friendly) or  [Gdrive](https://drive.google.com/open?id=1znFK4xtXkNrcRBVF9NCUHHjKMMawgKf0).  ```bash #: clone this repo git clone https://github.com/johntd54/stanford_car cd stanford_car  #: create a new python 3.7 environment with conda conda create -n stanford_car_eval python=3.7 conda activate stanford_car_eval  mkdir ckpt wget https://www.dropbox.com/s/pskw0fsjhnrlmyj/010.ckpt?dl=1 -O ckpt/010.ckpt #: download the pre-trained model into folder `ckpt` #: or GDrive: https://drive.google.com/open?id=1znFK4xtXkNrcRBVF9NCUHHjKMMawgKf0  #: install supporting libraries pip install -r requirements.txt conda install pytorch torchvision cudatoolkit=10.0 -c pytorch #: or `pip install torch torchvision` if gpu is not available  #: take note of the evaluation folder and run the evaluation script #: the result will be stored at ./data/result.csv and ./data/confidence.csv #: remove the `--gpu` parameter on non-gpu machine python evaluate.py [image_folder] --ckpt [checkpoint_path] --gpu ```  The above step-by-step operation will output 2 files: - *`./data/confidence.csv`*: this file contains 197 columns  the first column is filename  the other 196 columns are confidence score for corresponding class. The class label can be obtained at *`./data/meta.json`*. - *`./data/result.csv`*: this file contains 3 columns  the first column is filename  the second column is the class index  and the third column is class label (looked up from `./data/meta.json`.   """;Computer Vision;https://github.com/johntd54/stanford_car
"""""";Computer Vision;https://github.com/shekkizh/FCN.tensorflow
"""Following DQN versions are included:   Its interesting to see that the add-ons have a negative impact for the super simple CartPole environment. Still the Dueling DDQN version performs clearly better than the standard DDQN version.   """;Reinforcement Learning;https://github.com/BY571/DQN-Atari-Agents
"""‚îÇ  requirements.txt   $ pip install -r requirements.txt   https://github.com/codemayq/chinese_chatbot_corpus    """;General;https://github.com/riversdie/chatbot
""" you need download pretrained ERNIE model  1. Download the  pretrained ERNIE model from [baiduPan](https://pan.baidu.com/s/1BQlwbc9PZjAoVB7Kfq_Ihg) {password: uwds} and place it into the `/pyernie/model/pretrain` directory.  2. prepare Chinese raw data(example news data)  you can modify the `io.data_transformer.py` to adapt your data.  3. Modify configuration information in `pyernie/config/basic_config.py`(the path of data ...).  4. run `fine_tune_ernie.py`.   """;Natural Language Processing;https://github.com/lonePatient/ERNIE-text-classification-pytorch
"""cd realtime-facereccpp  mkdir build  cd build   make -j4   """;General;https://github.com/thongdk8/realtime-facereccpp
"""""";General;https://github.com/zhangjiong724/spectral-RNN
"""```bash $ pip install sinkhorn_transformer ```   You can follow the instructions here to set it correctly https://github.com/lucidrains/product-key-memory#learning-rates   A Sinkhorn Transformer based language model  ```python import torch from sinkhorn_transformer import SinkhornTransformerLM  model = SinkhornTransformerLM(     num_tokens = 20000      dim = 1024      heads = 8      depth = 12      max_seq_len = 8192      bucket_size = 128         #: size of the buckets     causal = False            #: auto-regressive or not     n_sortcut = 2             #: use sortcut to reduce memory complexity to linear     n_top_buckets = 2         #: sort specified number of key/value buckets to one query bucket. paper is at 1  defaults to 2     ff_chunks = 10            #: feedforward chunking  from Reformer paper     reversible = True         #: make network reversible  from Reformer paper     emb_dropout = 0.1         #: embedding dropout     ff_dropout = 0.1          #: feedforward dropout     attn_dropout = 0.1        #: post attention dropout     attn_layer_dropout = 0.1  #: post attention layer dropout     layer_dropout = 0.1       #: add layer dropout  from 'Reducing Transformer Depth on Demand' paper     weight_tie = True         #: tie layer parameters  from Albert paper     emb_dim = 128             #: embedding factorization  from Albert paper     dim_head = 64             #: be able to fix the dimension of each head  making it independent of the embedding dimension and the number of heads     ff_glu = True             #: use GLU in feedforward  from paper 'GLU Variants Improve Transformer'     n_local_attn_heads = 2    #: replace N heads with local attention  suggested to work well from Routing Transformer paper     pkm_layers = (4 7)        #: specify layers to use product key memory. paper shows 1 or 2 modules near the middle of the transformer is best     pkm_num_keys = 128        #: defaults to 128  but can be increased to 256 or 512 as memory allows )  x = torch.randint(0  20000  (1  2048)) model(x) #: (1  2048  20000) ```  A plain Sinkhorn Transformer  layers of sinkhorn attention  ```python import torch from sinkhorn_transformer import SinkhornTransformer  model = SinkhornTransformer(     dim = 1024      heads = 8      depth = 12      bucket_size = 128 )  x = torch.randn(1  2048  1024) model(x) #: (1  2048  1024) ```  Sinkhorn Encoder / Decoder Transformer  ```python import torch from sinkhorn_transformer import SinkhornTransformerLM  DE_SEQ_LEN = 4096 EN_SEQ_LEN = 4096  enc = SinkhornTransformerLM(     num_tokens = 20000      dim = 512      depth = 6      heads = 8      bucket_size = 128      max_seq_len = DE_SEQ_LEN      reversible = True      return_embeddings = True ).cuda()  dec = SinkhornTransformerLM(     num_tokens = 20000      dim = 512      depth = 6      causal = True      bucket_size = 128      max_seq_len = EN_SEQ_LEN      receives_context = True      context_bucket_size = 128   #: context key / values can be bucketed differently     reversible = True ).cuda()  x = torch.randint(0  20000  (1  DE_SEQ_LEN)).cuda() y = torch.randint(0  20000  (1  EN_SEQ_LEN)).cuda()  x_mask = torch.ones_like(x).bool().cuda() y_mask = torch.ones_like(y).bool().cuda()  context = enc(x  input_mask=x_mask) dec(y  context=context  input_mask=y_mask  context_mask=x_mask) #: (1  4096  20000) ```   """;General;https://github.com/lucidrains/sinkhorn-transformer
"""‚Ä¢ https://github.com/matterport/Mask_RCNN    """;Computer Vision;https://github.com/DivJAth/DeepLearning5922
"""CenterNet is a framework for object detection with deep convolutional neural networks. You can use the code to train and evaluate a network for object detection on the MS-COCO dataset.  * It achieves state-of-the-art performance (an AP of 47.0%) on one of the most challenging dataset: MS-COCO.  * Our code is written in Python  based on [CornerNet](https://github.com/princeton-vl/CornerNet).  *More detailed descriptions of our approach and code will be made available soon.*  **If you encounter any problems in using our code  please contact Kaiwen Duan: kaiwen.duan@vipl.ict.ac.cn.**   ``` cd <CenterNet dir>/data/coco/PythonAPI make ```   Please first install [Anaconda](https://anaconda.org) and create an Anaconda environment using the provided package list. ``` conda create --name CenterNet --file conda_packagelist.txt ```  After you create the environment  activate it. ``` source activate CenterNet ```   python setup.py install --user   """;Computer Vision;https://github.com/takooctopus/CenterNet-Tako
"""""";Computer Vision;https://github.com/mingxingtan/mnasnet
"""""";General;https://github.com/mingxingtan/mnasnet
"""1. Install `pytorch`: https://pytorch.org/get-started/locally/ 2. For now  you will also need a tool to view notebooks. I use Jupyter. 3. 3. Dependencies: TBD  """;General;https://github.com/joshpc/StyledFontGAN
"""1. Install `pytorch`: https://pytorch.org/get-started/locally/ 2. For now  you will also need a tool to view notebooks. I use Jupyter. 3. 3. Dependencies: TBD  """;Computer Vision;https://github.com/joshpc/StyledFontGAN
"""In Neural Network Language Models(NNLM) with huge number of words in vocabulary  exhaustive activation functions such as Softmax are very slow.  This paper addresses shortcomings of Softmax. It consists of mainly two ideas 1. Representing words as low-dimensional feature vectors - to learn relation between words and contexts. 2. Clustering similar words in similar components(subtree) using the feature vectors.  Following is the summary of the Hierarchical Log-Bilinear Model. (If this explanation doesn't summarise the content please go to Section 4 in the Paper) * Initially start with a random binary tree. With words as leaf. * Use log-bilinear model to fit training data.    * Input will be context: w<sub>1</sub> w<sub>2</sub> ... w<sub>n-1</sub>.      * Each word w is represented by a feature vector r<sub>w<sub>1</sub></sub>. Say shape (1 100) each.     * So input at each forward pass will be (n-1  1  100)   * Hidden Layers apply matrix transformations  with weights C <p align='center'> <img src='https://github.com/AshwinDeshpande96/Hierarchical-Softmax/blob/master/Screenshot%202019-06-05%20at%208.38.05%20PM.png' width=150> </p>      * Output will be w<sub>n</sub>     * Will be a predicted feature vector r_hat     * So output shape at each forward pass will be (1 100)       * If there are 8 words in vocabulary (output classes)(Fig-1)         * Each of q<sub>i</sub> are multiplied with output r_hat and activated using sigmoid. Gives the probability of decision going to left subtree.          <p align='center'> P(d<sub>i</sub> = 1): sigmoid( r_hat * q<sub>i</sub> + b<sub>i</sub>) </p>                  * Each leaf is scored according to it's decision code. For example:            * leaf_5: P(d<sub>1</sub>=0  d<sub>3</sub>=1  d<sub>6</sub>=1)           * leaf_3: P(d<sub>1</sub>=1  d<sub>2</sub>=0  d<sub>5</sub>=1) * Fit the model with given data   * This is a teacher-forcing type of model  output at time step t is used at the next step t+1.   * This creates feature vectors r_hat depending on the context as we train. <p align='center'> <img src='https://github.com/AshwinDeshpande96/Hierarchical-Softmax/blob/master/tree.png'> </p>  Task specific feature vector perform well in every NLP task  because the tailored feature vector represent the training data well: [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)     <img src='https://github.com/AshwinDeshpande96/Hierarchical-Softmax/blob/master/matrix_decisions.gif' width=560>   """;Natural Language Processing;https://github.com/AshwinDeshpande96/Hierarchical-Softmax
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/icewing1996/bert_dep
"""![motivation of CCNet](https://user-images.githubusercontent.com/4509744/50546460-7df6ed00-0bed-11e9-9340-d026373b2cbe.png) Long-range dependencies can capture useful contextual information to benefit visual understanding problems. In this work  we propose a Criss-Cross Network (CCNet) for obtaining such important information through a more effective and efficient way. Concretely  for each pixel  our CCNet can harvest the contextual information of its surrounding pixels on the criss-cross path through a novel criss-cross attention module. By taking a further recurrent operation  each pixel can finally capture the long-range dependencies from all pixels. Overall  our CCNet is with the following merits:  - **GPU memory friendly**   - **High computational efficiency**  - **The state-of-the-art performance**    : Install Pytorch  $ conda install pytorch torchvision -c pytorch  : Install Apex  $ git clone https://github.com/NVIDIA/apex  $ cd apex   : Install Inplace-ABN  $ git clone https://github.com/mapillary/inplace_abn.git  $ cd inplace_abn  $ python setup.py install   ./run_local.sh YOUR_CS_PATH   """;Computer Vision;https://github.com/speedinghzl/CCNet
"""""";Computer Vision;https://github.com/gunooknam/Code_Study_Yolov3
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/wangshanmin/interface
"""**Python 3.6 or 3.7** is needed to run the toolbox.  * Install [PyTorch](https://pytorch.org/get-started/locally/) (>=1.1.0). * Install [ffmpeg](https://ffmpeg.org/download.html#get-packages). * Run `pip install -r requirements.txt` to install the remaining necessary packages.   14/02/21: This repo now runs on PyTorch instead of Tensorflow  thanks to the help of @bluefish. If you wish to run the tensorflow version instead  checkout commit 5425557.   20/08/19: I'm working on resemblyzer  an independent package for the voice encoder. You can use your trained encoder models from this repo with it.   Before you download any dataset  you can begin by testing your configuration with:   You can then try the toolbox:   """;Sequential;https://github.com/CorentinJ/Real-Time-Voice-Cloning
"""Run it on colab!   The hyperparameters are for a V100 GPU with 16 GB GPU memory. The 1b_lyrics  5b  and 5b_lyrics top-level priors take up 3.8 GB  10.3 GB  and 11.5 GB.  If you continue to have memory issues after this (or run into issues on your own home setup)  switch to the 1B model.   [PLAYLIST](https://www.youtube.com/playlist?list=PL8kGuiVdKeKhejC3rLR-t5JVweFHzwE9-)  Music is an art of time. It is formed by the colaboration of instruments -composed with many instruments collectively- harmonization of notes. So  music generation with deep neural networks strictly connected with this features of music. There are many models have been proposed so far for generating music. Some of them based on the structure of Recurrent Neural Networks or Generative Adversarial Networks or Variational Autoencoders.  In this work  we tackle the generating music with deep neural networks  especially with Vector Quantized Variational Autoencoders (Oord et al.  2017).   [This folder](https://github.com/inzva/music-generation/tree/main/jukebox/samples) contains our examples for music generation.    """;Audio;https://github.com/inzva/music-generation
"""To run the project  first clone the project and go into the folder  ``` git clone ... cd Listen-and-Translate/ ```  The following instructions will get you a copy of the project and running on your local machine for testing purposes.   """;General;https://github.com/CKRC24/Listen-and-Translate
"""""";Computer Vision;https://github.com/Cjiangbpcs/cjiang.github.io
"""Assuming you have Python (>= 3.6) and FFMPEG (>= 4.2) preinstalled  simply run:   pip install -r requirements.txt   """;Computer Vision;https://github.com/daniegr/EfficientPose
"""Â∞ÜÊ†∑Êú¨Êï¥ÁêÜÊàêÂ¶Ç‰∏ãÊ†ºÂºèÊåâÊñá‰ª∂Â§πÂ≠òÊîæÔºö  ``` MLDataloader load MTMC dataset as following directory tree. Make sur train-val directory tree keeps consistency.  data_root_path ‚îú‚îÄ‚îÄ task_A ‚îÇ   ‚îú‚îÄ‚îÄ train ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ class_1 ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ class_2 ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ class_3 ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ class_4 ‚îÇ   ‚îî‚îÄ‚îÄ val ‚îÇ       ‚îú‚îÄ‚îÄ class_1 ‚îÇ       ‚îú‚îÄ‚îÄ class_2 ‚îÇ       ‚îú‚îÄ‚îÄ class_3 ‚îÇ       ‚îî‚îÄ‚îÄ class_4 ‚îî‚îÄ‚îÄ task_B     ‚îú‚îÄ‚îÄ train     ‚îÇ   ‚îú‚îÄ‚îÄ class_1     ‚îÇ   ‚îú‚îÄ‚îÄ class_2     ‚îÇ   ‚îî‚îÄ‚îÄ class_3     ‚îî‚îÄ‚îÄ val         ‚îú‚îÄ‚îÄ class_1         ‚îú‚îÄ‚îÄ class_2         ‚îî‚îÄ‚îÄ class_3  ```   pytorch                   0.4.1           py36_cuda0.0_cudnn0.0_1    pytorch  torchvision               0.2.1                    py36_1    pytorch   $ bash bash_trainval_mtmc_resnet18_ft.sh   """;Computer Vision;https://github.com/cinastanbean/Pytorch-Multi-Task-Multi-class-Classification
"""Please prepare: [![Python Version](https://img.shields.io/badge/python-3.6 3.7-green.svg)](https://www.python.org/downloads/release/python-360/) [![Pytorch Version](https://img.shields.io/badge/pytorch-1.0 1.1-orange.svg)](https://pytorch.org/get-started/locally/)  Clone the repository and install the following additional packages:  ``` git clone https://github.com/hebo1221/RandWireNN.git pip install -r requirements.txt ```   Just ``` python run_RandWireNN.py ``` - If you want to change dataset  see run_RandWireNN.py  get_configuration(). MNIST CIFAR ImageNet available - You don't have to prepare a dataset. The code will automatically download it. - But if you have it already  set your dataset directory in RandWireNN_config.py  __C.DATASET_DIR - If you want to see a train-loss graph  see RandWireNN_config.py  __C.VISDOM - You can change the hyperparameters and dataset settings from *_config.py files. Look it up.    """;Computer Vision;https://github.com/hebo1221/RandWireNN
"""In this project we are trying to build a system which can parse clinical discharge notes and generate information about clinical events along with temporal information about their occurrence. In  phase 1 of the project we establish a foundation for that by treating clinical events and temporal information as individual clinical entities and extracting those by performing Named Entity Recognition (NER) using fine-tuned NCBI BERT model. In phase 2 of the project  we take inspiration from the paper KG-BERT to train a model using the word embedding representations generated from phase-1 fine-tuned model. We observe that the model do not perform well on test data. Thus we analyze the nature of the relationships and the model behavior to find out possible reasons of the poor performance.  Then we decide to fine-tune NCBI BERT for relation extraction task. For that we format the input in a specific way so that the model can perform sequence classification successfully using Transformer's attention patterns and BERT special tokens. We evaluate the model on test data and see significant improvement. We dockerise the solution along with the Flask application developed in phase-1 used to visualise the tagged data.   Though the fine-tuned NCBI-BERT model perform well on test data  there exists a major challenge related to usability of the model. So far the model is tested on relevant entity pairs for which TLINKs are available. The way of finding such relevant entity pairs from the available set of entities is yet to be found.  Another area with scope of improvement is to augment data using the nature of relationships to improve the model behavior. For example  if event A is known to happen before patient admission  it means it also occurs before all other events which occurred after admission. Also the input to BERT while fine-tuning can be improved. Our experiment can be treated as a baseline for future research and development in this area.                                           Image source(https://towardsdatascience.com/)   """;Natural Language Processing;https://github.com/ManasRMohanty/DS5500-capstone
"""Dependecies to run the notebook:  - scikitlearn pip install sklearn  - networkx pip install networkx  - grakel pip install grakel-dev   """;Graphs;https://github.com/FilippoMB/Benchmark_dataset_for_graph_classification
"""Python >= 3.6  PyTorch >= 1.1   Currently supports 256px (top/bottom hierarchical prior)  1. Stage 1 (VQ-VAE)  > python train_vqvae.py [DATASET PATH]  If you use FFHQ  I highly recommends to preprocess images. (resize and convert to jpeg)  2. Extract codes for stage 2 training  > python extract_code.py --ckpt checkpoint/[VQ-VAE CHECKPOINT] --name [LMDB NAME] [DATASET PATH]  3. Stage 2 (PixelSNAIL)  > python train_pixelsnail.py [LMDB NAME]  Maybe it is better to use larger PixelSNAIL model. Currently model size is reduced due to GPU constraints.   """;Computer Vision;https://github.com/rosinality/vq-vae-2-pytorch
"""| 2 | Federated Learning on MNIST using PyTorch + PySyft | Day 4 |   Link ::link:   https://github.com/gargarchit/PATE_Analysis   with Accuracy 92.363%   Link:link:: https://github.com/gargarchit/Federated-Learning-on-MNIST   :heavy_check_mark:Make a Repo Link: https://github.com/gargarchit/60DaysOfUdacity to manage my Daily Updates on #60daysofudacity   Link:link:: https://github.com/gargarchit/Federated-Learning-on-MNIST   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity   Link:link:: https://github.com/gargarchit/Federated-Learning-on-MNIST   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   Dataset Link: :link: https://www.kaggle.com/c/virtual-hack   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :bettertogether::dart:Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :link: https://secureprivataischolar.slack.com/archives/CKC6MNSRG/p1566486761374100   :bettertogether::dart:Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :bettertogether::dart:Updated: https://github.com/gargarchit/60DaysOfUdacity    :bettertogether::dart:Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart:Updated my #60daysofudacity posts :link: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   """;Computer Vision;https://github.com/gargarchit/60DaysOfUdacity
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/k5iogura/darknetAB
"""1.  Prepare the environment.  2.  Clone the repository.      3.  Type  `make`  to build necessary cxx libs.  4.  Download the pre-trained model and place it in *`./model/`*  5.  Download the IQIYI-VID Datasets from [IQIYI_VID](http://challenge.ai.iqiyi.com/detail?raceId=5afc36639689443e8f815f9e) and unzip them to `data/iqiyi_vid` directory.    Pre-trained models can be downloaded on BaiduCloud or GoogleDrive.  This repository has been tested under the following environment:  Python 2.7    1.  Detect faces on train+val dataset and test dataset respectively using ESSH model.  Model `model-r50-gg` is used to judge the quality of the detected faces. ``` python detect.py --model ./model/model-r50-gg/model 0 --output ./output/det_trainval --dataset ./data/iqiyi_vid --gpu 0 --stage trainval python detect.py --model ./model/model-r50-gg/model 0 --output ./output/det_test --dataset ./data/iqiyi_vid --gpu 0 --stage test ```  2. Extract features to the detected faces of train+val and test dataset respectively using `model-r100-gg` model. ``` python feature.py --model ./model/model-r100-gg/model 0 --input ./output/det_trainval --output ./output/feat_trainval  --gpu 0 python feature.py --model ./model/model-r100-gg/model 0 --input ./output/det_test --output ./output/feat_test --gpu 0 ``` 3.  Re-save the extracted face features for training the MLP network. ``` python genfeat.py --inputs ./output/feat_trainval --output ./output/trainval ``` 4. Train the MLP network for face ID recognition using train+val datasets. ``` python train_mlp.py --data ./output/trainval --prefix ./model/iqiyi --ckpt 1 --network r50 --lr 0.2 --per-batch-size 1024 ``` 5.  Predict face ID from features of the test dataset using the pre-trained MLP network. ``` python predict.py --model ./model/iqiyi 40 --gpu 0 --inputs ./output/feat_test --output ./output/pred_test ``` 6. Run ``python submit.py`` to generate the final submissions for IQIYI-VID Challenge.   """;General;https://github.com/deepinx/iqiyi-vid-challenge
"""Environment: Ubuntu 16.04    CUDA    Copy yolov3-voc.cfg in folder cfg  rename to animal.cfg  make the following changes:   P (Precision) ---> for one image in one class   The forth one is class name.  The fifth one is the path to save annots.pkl file.   """;Computer Vision;https://github.com/Pengchengzhi/Yolov3_Own-dataset
"""""";General;https://github.com/flrngel/cpc-tensorflow
"""PAU is implemented as a pytorch extension using CUDA 10.1. So all that is needed is to install the extension. This requires the cuda compiler and dev-tools  however the process is pretty straight forward:  in the folder /pau/cuda execute ~~~~ python3 setup.py install ~~~~ For this  you might need super user rights or work in a virtual environment.    	 """;General;https://github.com/ml-research/pau
"""ImageNet needs to be manually downloaded following the instructions here.   """;General;https://github.com/NivNayman/XNAS
"""Run 7-Synthetic_1D.ipynb (jupyter notebook)   Run 11-FD_sim_testingmodel_ypred.m   Run 12-Synthetic_2D.ipynb (jupyter notebook) - Part2    Run 20-Field_Application.ipynb (jupyter notebook)   Run 21-Synthetic_UQ.ipynb (jupyter notebook)   Run 22-Field_UQ.ipynb (jupyter notebook)   """;Computer Vision;https://github.com/zxleong/GPRNet
"""%<i></i> | ultralytics/yolov3 OR-NMS 5:52@416 (`pycocotools`) | darknet     ``` bash  git clone https://github.com/ultralytics/yolov3   git clone https://github.com/cocodataset/cocoapi && cd cocoapi/PythonAPI && make && cd ../.. && cp -r cocoapi/PythonAPI/pycocotools yolov3  cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/guxiaowei1/yolov3--detection-of-rmb-code-region
"""DeepLab is a state-of-art deep learning system for semantic image segmentation built on top of [Caffe](http://caffe.berkeleyvision.org).  It combines (1) *atrous convolution* to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks  (2) *atrous spatial pyramid pooling* to robustly segment objects at multiple scales with filters at multiple sampling rates and effective fields-of-views  and (3) densely connected conditional random fields (CRF) as post processing.  This distribution provides a publicly available implementation for the key model ingredients reported in our latest [arXiv paper](http://arxiv.org/abs/1606.00915). This version also supports the experiments (DeepLab v1) in our ICLR'15. You only need to modify the old prototxt files. For example  our proposed atrous convolution is called dilated convolution in CAFFE framework  and you need to change the convolution parameter ""hole"" to ""dilation"" (the usage is exactly the same). For the experiments in ICCV'15  there are some differences between our argmax and softmax_loss layers and Caffe's. Please refer to [DeepLabv1](https://bitbucket.org/deeplab/deeplab-public/) for details.  Please consult and consider citing the following papers:      @article{CP2016Deeplab        title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets  Atrous Convolution  and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        journal={arXiv:1606.00915}        year={2016}     }      @inproceedings{CY2016Attention        title={Attention to Scale: Scale-aware Semantic Image Segmentation}        author={Liang-Chieh Chen and Yi Yang and Jiang Wang and Wei Xu and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{CB2016Semantic        title={Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform}        author={Liang-Chieh Chen and Jonathan T Barron and George Papandreou and Kevin Murphy and Alan L Yuille}        booktitle={CVPR}        year={2016}     }      @inproceedings{PC2015Weak        title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation}        author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille}        booktitle={ICCV}        year={2015}     }      @inproceedings{CP2015Semantic        title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}        author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille}        booktitle={ICLR}        year={2015}     }   Note that if you use the densecrf implementation  please consult and cite the following paper:      @inproceedings{KrahenbuhlK11        title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}        author={Philipp Kr{\""{a}}henb{\""{u}}hl and Vladlen Koltun}        booktitle={NIPS}        year={2011}     }   1. The scripts we used for our experiments can be downloaded from this [link](https://ucla.box.com/s/4grlj8yoodv95936uybukjh5m0tdzvrf):     1. run_pascal.sh: the script for training/testing on the PASCAL VOC 2012 dataset. __Note__ You also need to download sub.sed script.     2. run_densecrf.sh and run_densecrf_grid_search.sh: the scripts we used for post-processing the DCNN computed results by DenseCRF. 2. The image list files used in our experiments can be downloaded from this [link](https://ucla.box.com/s/rd9z2xvwsfpksi7mi08i2xqrj7ab4keb):     * The zip file stores the list files for the PASCAL VOC 2012 dataset. 3. To use the mat_read_layer and mat_write_layer  please download and install [matio](http://sourceforge.net/projects/matio/files/matio/1.5.2/).   """;Computer Vision;https://github.com/open-cv/deeplab-v2
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`  2. Download VOC2007 and VOC2012 dataset. By default  we assume the data is stored in `$HOME/data/`   ```Shell   #: Download the data.   cd $HOME/data   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar   #: Extract the data.   tar -xvf VOCtrainval_11-May-2012.tar   tar -xvf VOCtrainval_06-Nov-2007.tar   tar -xvf VOCtest_06-Nov-2007.tar   ```  3. Create the LMDB file.   ```Shell   cd $CAFFE_ROOT   #: Create the trainval.txt  test.txt  and test_name_size.txt in data/VOC0712/   ./data/VOC0712/create_list.sh   #: You can modify the parameters in create_data.sh if needed.   #: It will create lmdb files for trainval and test with encoded original image:   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb   #: and make soft links at examples/VOC0712/   ./data/VOC0712/create_data.sh   ```   1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`   ```Shell   git clone https://github.com/weiliu89/caffe.git   cd caffe   git checkout ssd   ```  2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.   ```Shell   #: Modify Makefile.config according to your Caffe installation.   cp Makefile.config.example Makefile.config   make -j8   #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.   make py   make test -j8   #: (Optional)   make runtest -j8   ```   COCO<sup>[1]</sup>: SSD300*  SSD512*  07+12+COCO: SSD300*  SSD512*  07++12+COCO: SSD300*  SSD512*  COCO models:   """;Computer Vision;https://github.com/gmt710/caffe_gpu
"""""";General;https://github.com/sjyttkl/Transformer_learning
"""""";Natural Language Processing;https://github.com/sjyttkl/Transformer_learning
"""""";Computer Vision;https://github.com/geodekid/FCN
"""""";General;https://github.com/BingkAI-B21CAP0161/C-Mask-Machine-Learning
"""""";Computer Vision;https://github.com/BingkAI-B21CAP0161/C-Mask-Machine-Learning
"""""";General;https://github.com/lkfo415579/MT-Readling-List
"""| 2 | Federated Learning on MNIST using PyTorch + PySyft | Day 4 |   Link ::link:   https://github.com/gargarchit/PATE_Analysis   with Accuracy 92.363%   Link:link:: https://github.com/gargarchit/Federated-Learning-on-MNIST   :heavy_check_mark:Make a Repo Link: https://github.com/gargarchit/60DaysOfUdacity to manage my Daily Updates on #60daysofudacity   Link:link:: https://github.com/gargarchit/Federated-Learning-on-MNIST   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity   Link:link:: https://github.com/gargarchit/Federated-Learning-on-MNIST   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   Dataset Link: :link: https://www.kaggle.com/c/virtual-hack   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :dart: Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :bettertogether::dart:Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :link: https://secureprivataischolar.slack.com/archives/CKC6MNSRG/p1566486761374100   :bettertogether::dart:Updated: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   :bettertogether::dart:Updated: https://github.com/gargarchit/60DaysOfUdacity    :bettertogether::dart:Updated: https://github.com/gargarchit/60DaysOfUdacity    :dart:Updated my #60daysofudacity posts :link: https://github.com/gargarchit/60DaysOfUdacity :60daysofudacitybadge:   """;General;https://github.com/gargarchit/60DaysOfUdacity
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/SYangDong/bert-with-frozen-code
"""""";General;https://github.com/ktjonsson/keras-ArcFace
"""install pytorch0.4  install torchvision  install numpy  install scipy  install h5py  install opencv-python   """;Computer Vision;https://github.com/MandyMo/pytorch_HMR
"""""";Sequential;https://github.com/muellerzr/CodeFest_2019
"""""";Computer Vision;https://github.com/machinelearning-goettingen/EfficientNet-ModelScaling
"""""";Reinforcement Learning;https://github.com/neilsgp/RL-Algorithms
"""``` $ python main.py --block_type basic --depth 110 --outdir results ```   """;General;https://github.com/hysts/pytorch_resnet
"""``` $ python main.py --block_type basic --depth 110 --outdir results ```   """;Computer Vision;https://github.com/hysts/pytorch_resnet
"""Convolutional neural network is a key architure to recognize and classify the image to classes. However  variation of convolutional neural networks are very many and very vary their space/time usage for learning.   In this project  I compared the state-of-art convolutional neural networks and compared them based on **Performance** and **Resources**(space/time). Through this project  it helps to decide the model to solve problem based on problem size and limitation of resources.    --------   """;Computer Vision;https://github.com/wonjaek36/sodeep_final
"""Go into babi/data/extra_seq_tasks  run bash generate_10_fold_data.sh to   """;Graphs;https://github.com/yujiali/ggnn
"""```bash $ pip install reformer_pytorch ```   Since version 0.17.0  and some corrections to the reversible network  Reformer Pytorch is compatible with Microsoft's Deepspeed! If you have multiple local GPUs  you can follow the instructions / example <a href=""https://github.com/lucidrains/reformer-pytorch/tree/master/examples/enwik8_deepspeed"">here</a>.   You can follow the instructions here to set it correctly https://github.com/lucidrains/product-key-memory#learning-rates   Routing Transformer - https://github.com/lucidrains/routing-transformer  Sinkhorn Transformer - https://github.com/lucidrains/sinkhorn-transformer  Performer - https://github.com/lucidrains/performer-pytorch   A simple Reformer language model  ```python #: should fit in ~ 5gb - 8k tokens  import torch from reformer_pytorch import ReformerLM  model = ReformerLM(     num_tokens= 20000      dim = 1024      depth = 12      max_seq_len = 8192      heads = 8      lsh_dropout = 0.1      ff_dropout = 0.1      post_attn_dropout = 0.1      layer_dropout = 0.1   #: layer dropout from 'Reducing Transformer Depth on Demand' paper     causal = True         #: auto-regressive or not     bucket_size = 64      #: average size of qk per bucket  64 was recommended in paper     n_hashes = 4          #: 4 is permissible per author  8 is the best but slower     emb_dim = 128         #: embedding factorization for further memory savings     dim_head = 64         #: be able to fix the dimension of each head  making it independent of the embedding dimension and the number of heads     ff_chunks = 200       #: number of chunks for feedforward layer  make higher if there are memory issues     attn_chunks = 8       #: process lsh attention in chunks  only way for memory to fit when scaling to 16k tokens     num_mem_kv = 128        #: persistent learned memory key values  from all-attention paper     full_attn_thres = 1024  #: use full attention if context length is less than set value     reverse_thres = 1024    #: turn off reversibility for 2x speed for sequence lengths shorter or equal to the designated value     use_scale_norm = False   #: use scale norm from 'Transformers without tears' paper     use_rezero = False       #: remove normalization and use rezero from 'ReZero is All You Need'     one_value_head = False   #: use one set of values for all heads from 'One Write-Head Is All You Need'     weight_tie = False            #: tie parameters of each layer for no memory per additional depth     weight_tie_embedding = False  #: use token embedding for projection of output  some papers report better results     n_local_attn_heads = 2        #: many papers suggest mixing local attention heads aids specialization and improves on certain tasks     pkm_layers = (4 7)            #: specify layers to use product key memory. paper shows 1 or 2 modules near the middle of the transformer is best     pkm_num_keys = 128            #: defaults to 128  but can be increased to 256 or 512 as memory allows     use_full_attn = False    #: only turn on this flag to override and turn on full attention for all sequence lengths. for comparison with LSH to show that it is working ).cuda()  x = torch.randint(0  20000  (1  8192)).long().cuda() y = model(x) #: (1  8192  20000) ```  The Reformer (just a stack of reversible LSH attention)  ```python #: should fit in ~ 5gb - 8k embeddings  import torch from reformer_pytorch import Reformer  model = Reformer(     dim = 512      depth = 12      heads = 8      lsh_dropout = 0.1      causal = True ).cuda()  x = torch.randn(1  8192  512).cuda() y = model(x) #: (1  8192  512) ```  Self Attention with LSH  ```python import torch from reformer_pytorch import LSHSelfAttention  attn = LSHSelfAttention(     dim = 128      heads = 8      bucket_size = 64      n_hashes = 8      causal = False )  x = torch.randn(10  1024  128) y = attn(x) #: (10  1024  128) ```  LSH (locality sensitive hashing) Attention  ```python import torch from reformer_pytorch import LSHAttention  attn = LSHAttention(     bucket_size = 64      n_hashes = 16      causal = True )  qk = torch.randn(10  1024  128) v = torch.randn(10  1024  128)  out  attn  buckets = attn(qk  v) #: (10  1024  128) #: attn contains the unsorted attention weights  provided return_attn is set to True (costly otherwise) #: buckets will contain the bucket number (post-argmax) of each token of each batch ```   A full Reformer sequence ‚Üí sequence  say translation  ```python import torch from reformer_pytorch import ReformerLM  DE_SEQ_LEN = 4096 EN_SEQ_LEN = 4096  encoder = ReformerLM(     num_tokens = 20000      emb_dim = 128      dim = 1024      depth = 12      heads = 8      max_seq_len = DE_SEQ_LEN      fixed_position_emb = True      return_embeddings = True #: return output of last attention layer ).cuda()  decoder = ReformerLM(     num_tokens = 20000      emb_dim = 128      dim = 1024      depth = 12      heads = 8      max_seq_len = EN_SEQ_LEN      fixed_position_emb = True      causal = True ).cuda()  x  = torch.randint(0  20000  (1  DE_SEQ_LEN)).long().cuda() yi = torch.randint(0  20000  (1  EN_SEQ_LEN)).long().cuda()  enc_keys = encoder(x)               #: (1  4096  1024) yo = decoder(yi  keys = enc_keys)   #: (1  4096  20000) ```  A full Reformer image ‚Üí caption  ```python import torch from torch.nn import Sequential from torchvision import models from reformer_pytorch import Reformer  ReformerLM  resnet = models.resnet50(pretrained=True) resnet = Sequential(*list(resnet.children())[:-4])  SEQ_LEN = 4096  encoder = Reformer(     dim = 512      depth = 6      heads = 8      max_seq_len = 4096 )  decoder = ReformerLM(     num_tokens = 20000      dim = 512      depth = 6      heads = 8      max_seq_len = SEQ_LEN      causal = True )  x  = torch.randn(1  3  512  512) yi = torch.randint(0  20000  (1  SEQ_LEN)).long()  visual_emb = resnet(x) b  c  h  w = visual_emb.shape visual_emb = visual_emb.view(1  c  h * w).transpose(1  2) #: nchw to nte  enc_keys = encoder(visual_emb) yo = decoder(yi  keys = enc_keys) #: (1  4096  20000) ```   """;Natural Language Processing;https://github.com/lucidrains/reformer-pytorch
"""```bash $ pip install reformer_pytorch ```   Since version 0.17.0  and some corrections to the reversible network  Reformer Pytorch is compatible with Microsoft's Deepspeed! If you have multiple local GPUs  you can follow the instructions / example <a href=""https://github.com/lucidrains/reformer-pytorch/tree/master/examples/enwik8_deepspeed"">here</a>.   You can follow the instructions here to set it correctly https://github.com/lucidrains/product-key-memory#learning-rates   Routing Transformer - https://github.com/lucidrains/routing-transformer  Sinkhorn Transformer - https://github.com/lucidrains/sinkhorn-transformer  Performer - https://github.com/lucidrains/performer-pytorch   A simple Reformer language model  ```python #: should fit in ~ 5gb - 8k tokens  import torch from reformer_pytorch import ReformerLM  model = ReformerLM(     num_tokens= 20000      dim = 1024      depth = 12      max_seq_len = 8192      heads = 8      lsh_dropout = 0.1      ff_dropout = 0.1      post_attn_dropout = 0.1      layer_dropout = 0.1   #: layer dropout from 'Reducing Transformer Depth on Demand' paper     causal = True         #: auto-regressive or not     bucket_size = 64      #: average size of qk per bucket  64 was recommended in paper     n_hashes = 4          #: 4 is permissible per author  8 is the best but slower     emb_dim = 128         #: embedding factorization for further memory savings     dim_head = 64         #: be able to fix the dimension of each head  making it independent of the embedding dimension and the number of heads     ff_chunks = 200       #: number of chunks for feedforward layer  make higher if there are memory issues     attn_chunks = 8       #: process lsh attention in chunks  only way for memory to fit when scaling to 16k tokens     num_mem_kv = 128        #: persistent learned memory key values  from all-attention paper     full_attn_thres = 1024  #: use full attention if context length is less than set value     reverse_thres = 1024    #: turn off reversibility for 2x speed for sequence lengths shorter or equal to the designated value     use_scale_norm = False   #: use scale norm from 'Transformers without tears' paper     use_rezero = False       #: remove normalization and use rezero from 'ReZero is All You Need'     one_value_head = False   #: use one set of values for all heads from 'One Write-Head Is All You Need'     weight_tie = False            #: tie parameters of each layer for no memory per additional depth     weight_tie_embedding = False  #: use token embedding for projection of output  some papers report better results     n_local_attn_heads = 2        #: many papers suggest mixing local attention heads aids specialization and improves on certain tasks     pkm_layers = (4 7)            #: specify layers to use product key memory. paper shows 1 or 2 modules near the middle of the transformer is best     pkm_num_keys = 128            #: defaults to 128  but can be increased to 256 or 512 as memory allows     use_full_attn = False    #: only turn on this flag to override and turn on full attention for all sequence lengths. for comparison with LSH to show that it is working ).cuda()  x = torch.randint(0  20000  (1  8192)).long().cuda() y = model(x) #: (1  8192  20000) ```  The Reformer (just a stack of reversible LSH attention)  ```python #: should fit in ~ 5gb - 8k embeddings  import torch from reformer_pytorch import Reformer  model = Reformer(     dim = 512      depth = 12      heads = 8      lsh_dropout = 0.1      causal = True ).cuda()  x = torch.randn(1  8192  512).cuda() y = model(x) #: (1  8192  512) ```  Self Attention with LSH  ```python import torch from reformer_pytorch import LSHSelfAttention  attn = LSHSelfAttention(     dim = 128      heads = 8      bucket_size = 64      n_hashes = 8      causal = False )  x = torch.randn(10  1024  128) y = attn(x) #: (10  1024  128) ```  LSH (locality sensitive hashing) Attention  ```python import torch from reformer_pytorch import LSHAttention  attn = LSHAttention(     bucket_size = 64      n_hashes = 16      causal = True )  qk = torch.randn(10  1024  128) v = torch.randn(10  1024  128)  out  attn  buckets = attn(qk  v) #: (10  1024  128) #: attn contains the unsorted attention weights  provided return_attn is set to True (costly otherwise) #: buckets will contain the bucket number (post-argmax) of each token of each batch ```   A full Reformer sequence ‚Üí sequence  say translation  ```python import torch from reformer_pytorch import ReformerLM  DE_SEQ_LEN = 4096 EN_SEQ_LEN = 4096  encoder = ReformerLM(     num_tokens = 20000      emb_dim = 128      dim = 1024      depth = 12      heads = 8      max_seq_len = DE_SEQ_LEN      fixed_position_emb = True      return_embeddings = True #: return output of last attention layer ).cuda()  decoder = ReformerLM(     num_tokens = 20000      emb_dim = 128      dim = 1024      depth = 12      heads = 8      max_seq_len = EN_SEQ_LEN      fixed_position_emb = True      causal = True ).cuda()  x  = torch.randint(0  20000  (1  DE_SEQ_LEN)).long().cuda() yi = torch.randint(0  20000  (1  EN_SEQ_LEN)).long().cuda()  enc_keys = encoder(x)               #: (1  4096  1024) yo = decoder(yi  keys = enc_keys)   #: (1  4096  20000) ```  A full Reformer image ‚Üí caption  ```python import torch from torch.nn import Sequential from torchvision import models from reformer_pytorch import Reformer  ReformerLM  resnet = models.resnet50(pretrained=True) resnet = Sequential(*list(resnet.children())[:-4])  SEQ_LEN = 4096  encoder = Reformer(     dim = 512      depth = 6      heads = 8      max_seq_len = 4096 )  decoder = ReformerLM(     num_tokens = 20000      dim = 512      depth = 6      heads = 8      max_seq_len = SEQ_LEN      causal = True )  x  = torch.randn(1  3  512  512) yi = torch.randint(0  20000  (1  SEQ_LEN)).long()  visual_emb = resnet(x) b  c  h  w = visual_emb.shape visual_emb = visual_emb.view(1  c  h * w).transpose(1  2) #: nchw to nte  enc_keys = encoder(visual_emb) yo = decoder(yi  keys = enc_keys) #: (1  4096  20000) ```   """;General;https://github.com/lucidrains/reformer-pytorch
"""""";General;https://github.com/lessw2020/auto-adaptive-ai
"""""";Computer Vision;https://github.com/tsing-cv/EfficientNet-tensorflow-eager
"""- Preparing *.npz* files for Pytorch Inception metrics evaluation (cifar10 as an example): ``` python envs_repo/inception_pytorch/calculate_inception_moments.py --dataset C10 --data_root datasets ``` - Preparing *.tgz* files for Tensorflow Inception metrics evaluation - Preparing *.npz* files for Tensorflow FID metrics evaluation   bash ./scripts/operator_cifar10.sh   """;Computer Vision;https://github.com/AlephZr/CE-GAN
"""""";Computer Vision;https://github.com/VIRUS-ATTACK/DCGAN-GPU-IMPLEMENTATION
"""Run ```pip3 install git+https://github.com/dbaumgarten/FToDTF.git```   The programm is now installed system-wide. You can now import the package ftodtf in python3 and run the cli-command ```fasttext <optional args>```   """;Natural Language Processing;https://github.com/dbaumgarten/FToDTF
"""How to use each model is written in README in the each model. Basically you can training with  ``` $ pipenv run python -m $(MODEL_NAME)/train $(options) ```   """;Computer Vision;https://github.com/Hayashi-Yudai/ML_models
"""Run the below script.   Run the below script.   Run the below script.   Run the below script.   """;Graphs;https://github.com/pfnet-research/treewidth-prediction
"""""";Natural Language Processing;https://github.com/eaishwa/quora-ques-pair-similarity
"""""";General;https://github.com/blankWorld/GroupNorm-caffe
"""""";General;https://github.com/LeeHyeJin91/Wide_and_Deep
"""The jupyter [notebook](/notebooks/run_bbdl_experiment.ipynb) has provides a quick setup for running some experiments on the CIFAR-10 dataset   comparing MC Dropout to a standard CNN model.  ![PRCurve](/notebooks/class8.png)   [1] https://arxiv.org/abs/1506.02142  """;General;https://github.com/omacshane/basicbayesDL
"""""";Computer Vision;https://github.com/BathVisArtData/PeopleArt
""".. code:: sh      $ pip3 install -e .  or if you have the ``requirements.txt`` already installed (e.g. by your system's package manager)  .. code:: sh      $ pip3 install -e . --no-deps    One can now cut through the dendrogram tree at a certain height (sim   calc.cluster(...  min_csize=1)). One can now start to lower sim to   Contributions are welcome. To streamline the git log  consider using one of   .. _alexcnwy: https://github.com/alexcnwy   The package is designed as a library. Here is what you can do: Enter the python interactive mode or create a python file with the following code  .. code:: python      from imagecluster import calc as ic     from imagecluster import postproc as pp      # Create image database in memory. This helps to feed images to the NN model     # quickly.     ias = ic.image_arrays('pics/'  size=(224 224))      # Create Keras NN model.     model = ic.get_model()      # Feed images through the model and extract fingerprints (feature vectors).     fps = ic.fingerprints(ias  model)      # Optionally run a PCA on the fingerprints to compress the dimensions. Use a     # cumulative explained variance ratio of 0.95.     fps = ic.pca(fps  n_components=0.95)      # Run clustering on the fingerprints.  Select clusters with similarity index     # sim=0.5     clusters = ic.cluster(fps  sim=0.5)      # Create dirs with links to images. Dirs represent the clusters the images     # belong to.     pp.make_links(clusters  'pics/imagecluster/clusters')      # Plot images arranged in clusters.     pp.visualize(clusters  ias)  See also ``imagecluster.main.main()``. It does the same as the code above  but also saves/loads the image database and the fingerprints to/from disk  such that you can re-run the clustering and post-processing again without re-calculating fingerprints.  Example session:  .. code:: python      >>> from imagecluster import main     >>> main.main('pics/'  sim=0.5  vis=True)     no fingerprints database pics/imagecluster/fingerprints.pk found     create image array database pics/imagecluster/images.pk     pics/140301.jpg     pics/140601.jpg     pics/140101.jpg     pics/140400.jpg     pics/140801.jpg     [...]     running all images through NN model ...     pics/140301.jpg     pics/140503.jpg     pics/140601.jpg     pics/140901.jpg     pics/140101.jpg     [...]     clustering ...     #images : #clusters     2 : 7     3 : 1     #images in clusters total:  17     cluster dir: pics/imagecluster/clusters  If you run this again on the same directory  only the clustering (which is very fast) and the post-processing (links  visualization) will be repeated.  For this example  we use a very small subset of the `Holiday image dataset <holiday_>`_ (25 images (all named 140*.jpg) of 1491 total images in the dataset).  Have a look at the clusters (as dirs with symlinks to the relevant files):  .. code:: sh      $ tree pics/imagecluster/clusters/     pics/imagecluster/clusters/     ‚îú‚îÄ‚îÄ cluster_with_2     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ cluster_0     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 140100.jpg -> /path/to/pics/140100.jpg     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ 140101.jpg -> /path/to/pics/140101.jpg     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ cluster_1     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 140600.jpg -> /path/to/pics/140600.jpg     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ 140601.jpg -> /path/to/pics/140601.jpg     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ cluster_2     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 140400.jpg -> /path/to/pics/140400.jpg     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ 140401.jpg -> /path/to/pics/140401.jpg     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ cluster_3     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 140501.jpg -> /path/to/pics/140501.jpg     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ 140502.jpg -> /path/to/pics/140502.jpg     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ cluster_4     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 140000.jpg -> /path/to/pics/140000.jpg     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ 140001.jpg -> /path/to/pics/140001.jpg     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ cluster_5     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 140300.jpg -> /path/to/pics/140300.jpg     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ 140301.jpg -> /path/to/pics/140301.jpg     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ cluster_6     ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 140200.jpg -> /path/to/pics/140200.jpg     ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ 140201.jpg -> /path/to/pics/140201.jpg     ‚îî‚îÄ‚îÄ cluster_with_3         ‚îî‚îÄ‚îÄ cluster_0             ‚îú‚îÄ‚îÄ 140801.jpg -> /path/to/pics/140801.jpg             ‚îú‚îÄ‚îÄ 140802.jpg -> /path/to/pics/140802.jpg             ‚îî‚îÄ‚îÄ 140803.jpg -> /path/to/pics/140803.jpg  So there are some clusters with 2 images each  and one with 3 images. Lets look at the clusters:  .. image:: doc/clusters.png  Here is the result of using a larger subset of 292 images from the same dataset.  .. image:: doc/clusters_many.png   """;Computer Vision;https://github.com/leenaali1114/Hierarchical-Image-Clustering---Unsupervised-Learning
"""""";Computer Vision;https://github.com/MaybeShewill-CV/sfnet-tensorflow
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;General;https://github.com/worldlife123/maskrcnn-benchmark
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/worldlife123/maskrcnn-benchmark
"""Please check [INSTALL.md](INSTALL.md) (same as original FCOS) for installation instructions.    **Results**   Model | Total training mem (GB) | Multi-scale training | Testing time / im | AP (minival) | link  ---   |:---:|:---:|:---:|:---:|:---:| FCOS_R_50_FPN_1x | 29.3 | No | 71ms | 37.0 | [model](https://pan.baidu.com/s/1Xcbx7EfOGvwnexXAuovM0A) | FCOS_R_50_FPN_1x_center | 30.61 | No | 71ms | 37.8 | [model](https://pan.baidu.com/s/1Gs7AzmJRmeYhXUPDQZuSLA) | FCOS_R_50_FPN_1x_center_liou | 30.61 | No | 71ms | 38.1 | [model](https://pan.baidu.com/s/1HpYrkAsVXNvXRFTd06SGgA) | FCOS_R_50_FPN_1x_center_giou | 30.61 | No | 71ms | 38.2 | [model](https://pan.baidu.com/s/13_o6343Ikg4td01kVXxGSw) | FCOS_R_101_FPN_2x | 44.1 | Yes | 74ms | 41.4 | [model](https://pan.baidu.com/s/1u_5OD5NURYe1EYFWnohgEA) | FCOS_R_101_FPN_2x_center_giou | 44.1 | Yes | 74ms | 42.5 | [model](https://pan.baidu.com/s/1qhHM067ywwlEXfamaFq23g) |  [1] *1x and 2x mean the model is trained for 90K and 180K iterations  respectively.* \ [2] center means [center sample](fcos.pdf) is used in our training. \ [3] liou means the model use linear iou loss function. (1 - iou) \ [4] giou means the use giou loss function. (1 - giou)     Note that:   """;Computer Vision;https://github.com/yqyao/FCOS_PLUS
"""Neural models for essay scoring for TOEFL essays (https://catalog.ldc.upenn.edu/LDC2014T06) and ASAP essays (http://www.kaggle.com/c/asap-aes). The TensorFlow version is 1.12   Cuda 9.0  and CUDNN 7.1.4.   """;Natural Language Processing;https://github.com/Farahn/AES
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/junhahyung/bert_transfer
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/junhahyung/bert_transfer
"""""";Computer Vision;https://github.com/Afsaan/CNN-Architectures
"""""";Audio;https://github.com/nicobernasconi/specgan
"""""";General;https://github.com/nicobernasconi/specgan
"""""";Computer Vision;https://github.com/nicobernasconi/specgan
"""""";General;https://github.com/ifrit98/layer-glu
"""""";Sequential;https://github.com/ifrit98/layer-glu
"""""";Natural Language Processing;https://github.com/ifrit98/layer-glu
"""To start training on simple_crypto  with an MATD3 team of agents and an MADDPG adversary  use  ``` python train.py --scenario simple_speaker_listener --good-policy matd3 --adv-policy maddpg ```    """;General;https://github.com/JohannesAck/MATD3implementation
"""To start training on simple_crypto  with an MATD3 team of agents and an MADDPG adversary  use  ``` python train.py --scenario simple_speaker_listener --good-policy matd3 --adv-policy maddpg ```    """;Reinforcement Learning;https://github.com/JohannesAck/MATD3implementation
"""- Please refer to **[DatasetPreparation.md](docs/DatasetPreparation.md)** for more details. - The descriptions of currently supported datasets (`torch.utils.data.Dataset` classes) are in [Datasets.md](docs/Datasets.md).   - Python >= 3.7 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html)) - [PyTorch >= 1.3](https://pytorch.org/) - NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads)  1. Clone repo      ```bash     git clone https://github.com/xinntao/BasicSR.git     ```  1. Install dependent packages      ```bash     cd BasicSR     pip install -r requirements.txt     ```  1. Install BasicSR      Please run the following commands in the **BasicSR root path** to install BasicSR:<br>     (Make sure that your GCC version: gcc >= 5) <br>     If you do not need the cuda extensions: <br>     &emsp;[*dcn* for EDVR](basicsr/models/ops)<br>     &emsp;[*upfirdn2d* and *fused_act* for StyleGAN2](basicsr/models/ops)<br>     please add `--no_cuda_ext` when installing      ```bash     python setup.py develop --no_cuda_ext     ```      If you use the EDVR and StyleGAN2 model  the above cuda extensions are necessary.      ```bash     python setup.py develop     ```      You may also want to specify the CUDA paths:        ```bash       CUDA_HOME=/usr/local/cuda \       CUDNN_INCLUDE_DIR=/usr/local/cuda \       CUDNN_LIB_DIR=/usr/local/cuda \       python setup.py develop       ```  Note that BasicSR is only tested in Ubuntu  and may be not suitable for Windows. You may try [Windows WSL with CUDA supports](https://docs.microsoft.com/en-us/windows/win32/direct3d12/gpu-cuda-in-wsl) :-) (It is now only available for insider build with Fast ring).   Note that this version is not compatible with previous versions. If you want to use previous ones  please refer to the `old_version` branch.  ---   """;Computer Vision;https://github.com/xinntao/EDVR
"""""";Reinforcement Learning;https://github.com/lollcat/Soft-Actor-Critic
"""- To use your own data  you will have to specify the path to the folder containing this data (--root_dir). - Images have to be in nifti (.nii) format - You have to split your data into two folders: Training/Validation. Each folder will contain N sub-folders: N-1 subfolders that will contain each modality and GT  which contain the nifti files for the images and their corresponding ground truths. Then  for training  you only need to specify which subfolders you want to use in the command line. For example: ``` --modality_dirs T1 T2_FLAIR ``` - Image names should be the same across folders (e.g.  )  The current version includes HyperDenseNet for 2 and 3 modalities. To run either one or the other  you simply need to specify the number of modalities on the input arg numModal   """;Computer Vision;https://github.com/josedolz/HyperDenseNet_pytorch
"""Set up a generator and discriminator model  ```python from models import Generator  Discriminator generator = Generator(img_size=(32  32  1)  latent_dim=100  dim=16) discriminator = Discriminator(img_size=(32  32  1)  dim=16) ```  The generator and discriminator are built to automatically scale with image sizes  so you can easily use images from your own dataset.  Train the generator and discriminator with the WGAN-GP loss  ```python import torch #: Initialize optimizers G_optimizer = torch.optim.Adam(generator.parameters()  lr=1e-4  betas=(.9  .99)) D_optimizer = torch.optim.Adam(discriminator.parameters()  lr=1e-4  betas=(.9  .99))  #: Set up trainer from training import Trainer trainer = Trainer(generator  discriminator  G_optimizer  D_optimizer                    use_cuda=torch.cuda.is_available())  #: Train model for 200 epochs trainer.train(data_loader  epochs=200  save_training_gif=True) ```  This will train the models and generate a gif of the training progress.  Note that WGAN-GPs take a *long* time to converge. Even on MNIST it takes about 50 epochs to start seeing decent results. For more information and a full example on MNIST  check out `main.py`.   """;General;https://github.com/EmilienDupont/wgan-gp
"""Set up a generator and discriminator model  ```python from models import Generator  Discriminator generator = Generator(img_size=(32  32  1)  latent_dim=100  dim=16) discriminator = Discriminator(img_size=(32  32  1)  dim=16) ```  The generator and discriminator are built to automatically scale with image sizes  so you can easily use images from your own dataset.  Train the generator and discriminator with the WGAN-GP loss  ```python import torch #: Initialize optimizers G_optimizer = torch.optim.Adam(generator.parameters()  lr=1e-4  betas=(.9  .99)) D_optimizer = torch.optim.Adam(discriminator.parameters()  lr=1e-4  betas=(.9  .99))  #: Set up trainer from training import Trainer trainer = Trainer(generator  discriminator  G_optimizer  D_optimizer                    use_cuda=torch.cuda.is_available())  #: Train model for 200 epochs trainer.train(data_loader  epochs=200  save_training_gif=True) ```  This will train the models and generate a gif of the training progress.  Note that WGAN-GPs take a *long* time to converge. Even on MNIST it takes about 50 epochs to start seeing decent results. For more information and a full example on MNIST  check out `main.py`.   """;Computer Vision;https://github.com/EmilienDupont/wgan-gp
"""At this time  you need to download the pretrained models manually.  Since this project is based on pytorch  so you need the pytorch version of the BERT.    pytorch-pretrained-bert has changed name to pytorch-transformers. here is the github:   """;Natural Language Processing;https://github.com/darr/nerbert
"""""";Computer Vision;https://github.com/xuannianz/EfficientDet
"""Load weights that you want to auralise. I'm using this function `W = load_weights()` to load my keras model  it can be anything else. `W` is a list of weights for the convnet. (TODO: more details)  Then load source files  get STFT of it. I'm using `librosa`.  Then deconve it with `get_deconve_mask`.   """;Computer Vision;https://github.com/keunwoochoi/Auralisation
"""""";Computer Vision;https://github.com/kentaroy47/efficientdet.pytorch
"""The main implementation of this code is available in `umap.parametric_umap` in the [UMAP repository](https://github.com/lmcinnes/umap) (v0.5+). Most people reading this will want to use that code  and can ignore this repository.   The code in this repository is the 'messy' version. It has custom training loops which are a bit more verbose and customizable. It might be more useful for integrating UMAP into your custom models.   The code can be installed with `python setup.py develop`. Though  unless you're just trying to reproduce our results  you'll probably just want to pick through the notebooks and tfumap folder for the code relevant to your project.   In addition  we have a more verbose Colab notebook to walk you through the algorithm:  Parametric UMAP (verbose) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1lpdCy7HkC5TRI9LfUtIHBBW8oRO86Nvi?usp=sharing)    """;General;https://github.com/timsainb/ParametricUMAP_paper
"""""";Computer Vision;https://github.com/ifrit98/bengaliai
"""   1. Set config  2. Train the model  3. Evaluate using test dataset  """;Computer Vision;https://github.com/Wordbe/MobileNetV2
"""   1. Set config  2. Train the model  3. Evaluate using test dataset  """;General;https://github.com/Wordbe/MobileNetV2
"""To use the framework for creating a tf-records file: 1. Place your images and ground truth in folders called **Images** and **Labels**  respectively. 2. Specify the path to your data  the desired filename and the desired image size in [`make_tfrecords_dataset.py`](https://github.com/cgsaxner/UB_Segmentation/blob/master/make_tfrecords_dataset.py). 3. Run the script!  To use the framework for training: 1. Download the pre-trained model checkpoint you want to use from [TensorFlow-Slim](https://github.com/tensorflow/models/tree/master/research/slim#Pretrained) and place it in a **\Checkpoints** folder in your project repository. 2. Specify your paths in the top section of [`FCN_training.py`](https://github.com/cgsaxner/UB_Segmentation/blob/master/FCN_training.py) or [`ResNet_training.py`](https://github.com/cgsaxner/UB_Segmentation/blob/master/ResNet_training.py). 3. Run the script!  To use the framework for testing: 1. Specify your paths in the top section of [`FCN_testing.py`](https://github.com/cgsaxner/UB_Segmentation/blob/master/FCN_testing.py) or [`ResNet_testing.py`](https://github.com/cgsaxner/UB_Segmentation/blob/master/ResNet_testing.py). 2. Run the script!     """;Computer Vision;https://github.com/cgsaxner/UB_Segmentation
"""[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.   Getting the source code   $ wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip   $ cd fastText-0.1.0  $ make   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ mkdir build &amp;&amp; cd build &amp;&amp; cmake ..  $ make &amp;&amp; make install   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ pip install .   You can also quantize a supervised model to reduce its memory usage with the following command:   You can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.  There is also the master branch that contains all of our most recent work  but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.   This library has two main use cases: word representation learning and text classification. These were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).   """;Natural Language Processing;https://github.com/romik9999/fasttext-1925f09ed3
"""Today's deep learning models need to be trained on large-scale surveillance data sets. This means that for each data  there will be a corresponding label. In the popular ImageNet dataset  there are one million images with manual annotations  that is  1000 of each of the 1000 categories. Creating such a data set requires a lot of effort  and it may take a lot of people to spend months working on it. Assuming that you now have to create a data set of one million classes  you must label each frame in a total of 100 million frames of video data  which is basically impossible.  Ideally  we want to have a model that runs more like our brain. It requires only a few tags to understand many things in the real world. In the real world  I refer to classes that are object categories  action categories  environment categories  categories of objects  and many more.   """;Computer Vision;https://github.com/Yiming-Miao/Miniproject2
"""This is the official code of [high-resolution representations for ImageNet classification](https://arxiv.org/abs/1904.04514).  We augment the HRNet with a classification head shown in the figure below. First  the four-resolution feature maps are fed into a bottleneck and the number of output channels are increased to 128  256  512  and 1024  respectively. Then  we downsample the high-resolution representations by a 2-strided 3x3 convolution outputting 256 channels and add them to the representations of the second-high-resolution representations. This process is repeated two times to get 1024 channels over the small resolution. Last  we transform 1024 channels to 2048 channels through a 1x1 convolution  followed by a global average pooling operation. The output 2048-dimensional representation is fed into the classifier.  ![](figures/cls-hrnet.png)   You can follow the Pytorch implementation: https://github.com/pytorch/examples/tree/master/imagenet  The data should be under ./data/imagenet/images/.   1. Install PyTorch=0.4.1 following the [official instructions](https://pytorch.org/) 2. git clone https://github.com/HRNet/HRNet-Image-Classification 3. Install dependencies: pip install -r requirements.txt   """;Computer Vision;https://github.com/HRNet/HRNet-Image-Classification
"""<img src=https://github.com/stqc/Simple_CycleGAN_pytorch/blob/master/test.gif >    <img src=https://github.com/stqc/Simple_CycleGAN_pytorch/blob/master/ezgif.com-optimize.gif>  <h2> If you need my dataset for your testing contact me at the email below</h2>   """;Computer Vision;https://github.com/stqc/Simple_CycleGAN_pytorch
"""<img src=https://github.com/stqc/Simple_CycleGAN_pytorch/blob/master/test.gif >    <img src=https://github.com/stqc/Simple_CycleGAN_pytorch/blob/master/ezgif.com-optimize.gif>  <h2> If you need my dataset for your testing contact me at the email below</h2>   """;General;https://github.com/stqc/Simple_CycleGAN_pytorch
"""""";Natural Language Processing;https://github.com/piegu/language-models
"""""";General;https://github.com/XapaJIaMnu/transformative
"""""";Natural Language Processing;https://github.com/XapaJIaMnu/transformative
"""""";General;https://github.com/Sj-Amani/Practical_Behavioral_Cloning
"""""";Computer Vision;https://github.com/billlyzhaoyh/SegNetFromScratch
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/solapark/da_yolo
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/solapark/da_yolo
"""""";General;https://github.com/lyw1538272605/AnotherTenseNet
"""""";Computer Vision;https://github.com/lyw1538272605/AnotherTenseNet
"""```bash $ pip install sinkhorn_transformer ```   You can follow the instructions here to set it correctly https://github.com/lucidrains/product-key-memory#learning-rates   A Sinkhorn Transformer based language model  ```python import torch from sinkhorn_transformer import SinkhornTransformerLM  model = SinkhornTransformerLM(     num_tokens = 20000      dim = 1024      heads = 8      depth = 12      max_seq_len = 8192      bucket_size = 128         #: size of the buckets     causal = False            #: auto-regressive or not     n_sortcut = 2             #: use sortcut to reduce memory complexity to linear     n_top_buckets = 2         #: sort specified number of key/value buckets to one query bucket. paper is at 1  defaults to 2     ff_chunks = 10            #: feedforward chunking  from Reformer paper     reversible = True         #: make network reversible  from Reformer paper     emb_dropout = 0.1         #: embedding dropout     ff_dropout = 0.1          #: feedforward dropout     attn_dropout = 0.1        #: post attention dropout     attn_layer_dropout = 0.1  #: post attention layer dropout     layer_dropout = 0.1       #: add layer dropout  from 'Reducing Transformer Depth on Demand' paper     weight_tie = True         #: tie layer parameters  from Albert paper     emb_dim = 128             #: embedding factorization  from Albert paper     dim_head = 64             #: be able to fix the dimension of each head  making it independent of the embedding dimension and the number of heads     ff_glu = True             #: use GLU in feedforward  from paper 'GLU Variants Improve Transformer'     n_local_attn_heads = 2    #: replace N heads with local attention  suggested to work well from Routing Transformer paper     pkm_layers = (4 7)        #: specify layers to use product key memory. paper shows 1 or 2 modules near the middle of the transformer is best     pkm_num_keys = 128        #: defaults to 128  but can be increased to 256 or 512 as memory allows )  x = torch.randint(0  20000  (1  2048)) model(x) #: (1  2048  20000) ```  A plain Sinkhorn Transformer  layers of sinkhorn attention  ```python import torch from sinkhorn_transformer import SinkhornTransformer  model = SinkhornTransformer(     dim = 1024      heads = 8      depth = 12      bucket_size = 128 )  x = torch.randn(1  2048  1024) model(x) #: (1  2048  1024) ```  Sinkhorn Encoder / Decoder Transformer  ```python import torch from sinkhorn_transformer import SinkhornTransformerLM  DE_SEQ_LEN = 4096 EN_SEQ_LEN = 4096  enc = SinkhornTransformerLM(     num_tokens = 20000      dim = 512      depth = 6      heads = 8      bucket_size = 128      max_seq_len = DE_SEQ_LEN      reversible = True      return_embeddings = True ).cuda()  dec = SinkhornTransformerLM(     num_tokens = 20000      dim = 512      depth = 6      causal = True      bucket_size = 128      max_seq_len = EN_SEQ_LEN      receives_context = True      context_bucket_size = 128   #: context key / values can be bucketed differently     reversible = True ).cuda()  x = torch.randint(0  20000  (1  DE_SEQ_LEN)).cuda() y = torch.randint(0  20000  (1  EN_SEQ_LEN)).cuda()  x_mask = torch.ones_like(x).bool().cuda() y_mask = torch.ones_like(y).bool().cuda()  context = enc(x  input_mask=x_mask) dec(y  context=context  input_mask=y_mask  context_mask=x_mask) #: (1  4096  20000) ```   """;Natural Language Processing;https://github.com/lucidrains/sinkhorn-transformer
"""Install packages.  ``` sudo apt-get install tk-dev python-tk pip install cffi pip install cython pip install pandas pip install tensorboardX ```  Build NMS.  ``` cd Face_Attention_Network/lib sh build.sh ```  Create folders.  ``` cd Face_Attention_Network/ mkdir ckpt mAP_txt summary weight ```   resnet18: https://download.pytorch.org/models/resnet18-5c106cde.pth  resnet34: https://download.pytorch.org/models/resnet34-333f7ec4.pth  resnet50: https://download.pytorch.org/models/resnet50-19c8e357.pth  resnet101: https://download.pytorch.org/models/resnet101-5d3b4d8f.pth  resnet152: https://download.pytorch.org/models/resnet152-b121ed2d.pth   """;Computer Vision;https://github.com/rainofmine/Face_Attention_Network
"""Install packages.  ``` sudo apt-get install tk-dev python-tk pip install cffi pip install cython pip install pandas pip install tensorboardX ```  Build NMS.  ``` cd Face_Attention_Network/lib sh build.sh ```  Create folders.  ``` cd Face_Attention_Network/ mkdir ckpt mAP_txt summary weight ```   resnet18: https://download.pytorch.org/models/resnet18-5c106cde.pth  resnet34: https://download.pytorch.org/models/resnet34-333f7ec4.pth  resnet50: https://download.pytorch.org/models/resnet50-19c8e357.pth  resnet101: https://download.pytorch.org/models/resnet101-5d3b4d8f.pth  resnet152: https://download.pytorch.org/models/resnet152-b121ed2d.pth   """;General;https://github.com/rainofmine/Face_Attention_Network
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`  2. Download VOC2007 and VOC2012 dataset. By default  we assume the data is stored in `$HOME/data/`   ```Shell   #: Download the data.   cd $HOME/data   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar   #: Extract the data.   tar -xvf VOCtrainval_11-May-2012.tar   tar -xvf VOCtrainval_06-Nov-2007.tar   tar -xvf VOCtest_06-Nov-2007.tar   ```  3. Create the LMDB file.   ```Shell   cd $CAFFE_ROOT   #: Create the trainval.txt  test.txt  and test_name_size.txt in data/VOC0712/   ./data/VOC0712/create_list.sh   #: You can modify the parameters in create_data.sh if needed.   #: It will create lmdb files for trainval and test with encoded original image:   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb   #: and make soft links at examples/VOC0712/   ./data/VOC0712/create_data.sh   ```   1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`   ```Shell   git clone https://github.com/weiliu89/caffe.git   cd caffe   git checkout ssd   ```  2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.   ```Shell   #: Modify Makefile.config according to your Caffe installation.   cp Makefile.config.example Makefile.config   make -j8   #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.   make py   make test -j8   #: (Optional)   make runtest -j8   ```   COCO<sup>[1]</sup>: SSD300*  SSD512*  07+12+COCO: SSD300*  SSD512*  07++12+COCO: SSD300*  SSD512*  COCO models:   """;Computer Vision;https://github.com/zqplgl/caffe
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M and VGG2 datasets  which were already packed in the MxNet binary format. The network backbones include ResNet  InceptionResNet_v2  DenseNet  DPN and MobiletNet. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss. * loss-type=0:  Softmax * loss-type=1:  SphereFace * loss-type=2:  CosineFace * loss-type=4:  ArcFace * loss-type=5:  Combined Margin * loss-type=12: TripletLoss  ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu80   git clone --recursive https://github.com/deepinsight/insightface.git   """;General;https://github.com/sunil-rival/insightface
"""First you need to create a virtual environment.   Using Conda you can type:  conda create --name deeplab --python==3.7.1  conda activate deeplab  Download the dataset from:    The model can be trained with different backbones (resnet  xception  drn  mobilenet). The weights on the Drive has been trained with the ResNet backbone  so if you want to use another backbone you need to train from scratch (although the backbone weights are always pre-trained on ImageNet).   You can also check the ""inference.ipynb"" notebook for visual assessing the predictions.   """;Computer Vision;https://github.com/giovanniguidi/deeplabV3-PyTorch
"""``` wget https://github.com/deepmind/rc-data/raw/master/expected_[cnn/dailymail]_test.txt comm -3 <(cat expected_[cnn/dailymail]_test.txt) <(ls [cnn/dailymail]/questions/test/) ```  The filenames of the questions in the first column are missing generated questions. No output means everything is downloaded and generated correctly.  [arxiv]: http://arxiv.org/abs/1506.03340  ``` virtualenv venv source venv/bin/activate wget https://github.com/deepmind/rc-data/raw/master/requirements.txt pip install -r requirements.txt ```  You may need to install `libxml2` development packages to install `lxml`:  ``` sudo apt-get install libxml2-dev libxslt-dev ```   """;Natural Language Processing;https://github.com/deepmind/rc-data
"""–î–ª—è –Ω–∞–ø–∏—Å–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ pytorch. –û—Å–Ω–æ–≤—É –¥–ª—è —Ä–µ–∞–ª—å–∑–∞—Ü–∏–∏ NST —è –≤–∑—è–ª —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞: https://pytorch.org/tutorials/advanced/neural_style_tutorial.html. –†–µ–∞–ª–∏–∑–∞—Ü–∏—é CycleGan –≤–∑—è–ª –∏–∑ —Å–≤–æ–µ–≥–æ –ø—Ä–æ—à–ª–æ–≥–æ –¥–æ–º–∞—à–Ω–µ–≥–æ –∑–∞–¥–∞–Ω–∏—è –≤ dlschool: https://github.com/misuy/misuy_cycle_gan  –¥–ª—è –µ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —è –≤–¥–æ—Ö–Ω–æ–≤–ª—è–ª—Å—è —Å—Ç–∞—Ç—å–µ–π: https://arxiv.org/abs/1703.10593  –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix. –î–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –æ–±—É—á–∏–ª –≥–∞–Ω –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ —Å–æ—Å—Ç–æ—è—â–µ–º –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: https://www.kaggle.com/hsankesara/flickr-image-dataset  –∏ 1500 –∫–∞—Ä—Ç–∏–Ω –≤ —Å—Ç–∏–ª–µ –∫—É–±–∏–∑–º –∏–∑ —è–Ω–¥–µ–∫—Å –∫–∞—Ä—Ç–∏–Ω–æ–∫  –∫–æ—Ç–æ—Ä—ã–µ —è —Å–ø–∞—Ä—Å–∏–ª.   3)–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏–∑ requirements.txt –∫–æ–º–∞–Ω–¥–æ–π [python3.7 -m pip install -r requirements.txt]  –∏ torch+torchvision –∫–æ–º–∞–Ω–¥–æ–π [python3.7 -m pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html] (torch —Ç–æ–ª—å–∫–æ –¥–ª—è —Åpu);   """;Computer Vision;https://github.com/misuy/misuy_style_transfer_bot
"""–î–ª—è –Ω–∞–ø–∏—Å–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ pytorch. –û—Å–Ω–æ–≤—É –¥–ª—è —Ä–µ–∞–ª—å–∑–∞—Ü–∏–∏ NST —è –≤–∑—è–ª —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞: https://pytorch.org/tutorials/advanced/neural_style_tutorial.html. –†–µ–∞–ª–∏–∑–∞—Ü–∏—é CycleGan –≤–∑—è–ª –∏–∑ —Å–≤–æ–µ–≥–æ –ø—Ä–æ—à–ª–æ–≥–æ –¥–æ–º–∞—à–Ω–µ–≥–æ –∑–∞–¥–∞–Ω–∏—è –≤ dlschool: https://github.com/misuy/misuy_cycle_gan  –¥–ª—è –µ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —è –≤–¥–æ—Ö–Ω–æ–≤–ª—è–ª—Å—è —Å—Ç–∞—Ç—å–µ–π: https://arxiv.org/abs/1703.10593  –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix. –î–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –æ–±—É—á–∏–ª –≥–∞–Ω –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ —Å–æ—Å—Ç–æ—è—â–µ–º –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: https://www.kaggle.com/hsankesara/flickr-image-dataset  –∏ 1500 –∫–∞—Ä—Ç–∏–Ω –≤ —Å—Ç–∏–ª–µ –∫—É–±–∏–∑–º –∏–∑ —è–Ω–¥–µ–∫—Å –∫–∞—Ä—Ç–∏–Ω–æ–∫  –∫–æ—Ç–æ—Ä—ã–µ —è —Å–ø–∞—Ä—Å–∏–ª.   3)–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏–∑ requirements.txt –∫–æ–º–∞–Ω–¥–æ–π [python3.7 -m pip install -r requirements.txt]  –∏ torch+torchvision –∫–æ–º–∞–Ω–¥–æ–π [python3.7 -m pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html] (torch —Ç–æ–ª—å–∫–æ –¥–ª—è —Åpu);   """;General;https://github.com/misuy/misuy_style_transfer_bot
"""""";Computer Vision;https://github.com/asmith26/wide_resnets_keras
"""All you need to use centermask2 is [detectron2](https://github.com/facebookresearch/detectron2). It's easy!     you just install [detectron2](https://github.com/facebookresearch/detectron2) following [INSTALL.md](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md).    Prepare for coco dataset following [this instruction](https://github.com/facebookresearch/detectron2/tree/master/datasets).   cd centermask2   one should execute:  cd centermask2   cd centermask2   """;General;https://github.com/youngwanLEE/centermask2
"""All you need to use centermask2 is [detectron2](https://github.com/facebookresearch/detectron2). It's easy!     you just install [detectron2](https://github.com/facebookresearch/detectron2) following [INSTALL.md](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md).    Prepare for coco dataset following [this instruction](https://github.com/facebookresearch/detectron2/tree/master/datasets).   cd centermask2   one should execute:  cd centermask2   cd centermask2   """;Computer Vision;https://github.com/youngwanLEE/centermask2
"""Please clone the master branch and follow the instructions to run YellowFin on [ResNext](https://arxiv.org/abs/1611.05431) for CIFAR10 and [tied LSTM](https://arxiv.org/pdf/1611.01462.pdf) on Penn Treebank for language modeling. The models are adapted from [ResNext repo](https://github.com/kuangliu/pytorch-cifar) and [PyTorch example tied LSTM repo](https://github.com/pytorch/examples/tree/master/word_language_model) respectively. Thanks to the researchers for developing the models. **For more experiments on more convolutional and recurrent neural networks  please refer to our [Tensorflow implementation](https://github.com/JianGoForIt/YellowFin) of YellowFin**.  Note YellowFin is tested with PyTorch v0.2.0 for compatibility. It is tested under Python 2.7.   <!---For MXNet users  Github user [StargazerZhu](https://github.com/StargazerZhu) has already implemented a Theano version here: [YellowFin MXNet Repo](https://github.com/StargazerZhu/YellowFin_MXNet).--->  <!---For Theano users  Github user [botev](https://github.com/botev) has already implemented a Theano version here: [YellowFin Theano Repo](https://gist.github.com/botev/f8b32c00eafee222e47393f7f0747666).--->   """;Computer Vision;https://github.com/JianGoForIt/YellowFin_Pytorch
"""How to use each model is written in README in the each model. Basically you can training with  ``` $ pipenv run python -m $(MODEL_NAME)/train $(options) ```   """;General;https://github.com/Hayashi-Yudai/ML_models
"""Dependencies     <h2><a name=""dependencies"">Dependencies:</a></h2>   opencv-python        4.1.2.30   Matplotlib     <h2><a name=""acknowledgments"">Acknowledgments</a></h2>   <h2><a name=""links"">Links</a></h2>   """;Computer Vision;https://github.com/ngrayluna/NotCake
"""2021/11/11 (Thu) 13:00-14:00   """;Computer Vision;https://github.com/sekilab/image_processing_bootcamp2021
"""""";General;https://github.com/AhmadQasim/FixMatch
"""- Training:   Each of the three models has its own training code. To train each model  just run: > python train_srcnn.py <br/> > python train_srresnet.py <br/> > python train_srgan.py <br/> - Testing:   The three models have the same evaluation process. To test each model run: > python evaluate.py -m MODEL #The model youwant to test here we support ['srcnn' 'srresnet' 'srgan']     """;General;https://github.com/maiyuxiaoge/srgan
"""- Training:   Each of the three models has its own training code. To train each model  just run: > python train_srcnn.py <br/> > python train_srresnet.py <br/> > python train_srgan.py <br/> - Testing:   The three models have the same evaluation process. To test each model run: > python evaluate.py -m MODEL #The model youwant to test here we support ['srcnn' 'srresnet' 'srgan']     """;Computer Vision;https://github.com/maiyuxiaoge/srgan
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/icewing1996/bert_dep
"""1. Run python main.py to begin training on QA1. 2. To test other records change the dataset_id flag in main.py e.g dataset_id=""qa2"" to train & test with qa2 set 3. To train with 1k samples  set the flag only_1k=True and to train with 10k samples set flag only_1k=False   """;General;https://github.com/hsakas/Recurrent-Entity-Network-EntNet
"""This FCOS implementation is based on [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark). Therefore the installation is the same as original maskrcnn-benchmark.  Please check [INSTALL.md](INSTALL.md) for installation instructions. You may also want to see the original [README.md](MASKRCNN_README.md) of maskrcnn-benchmark.   For your convenience  we provide the following trained models (more models are coming soon).   We update batch normalization for MobileNet based models. If you want to use SyncBN  please install pytorch 1.1 or later.   Note that:   Once the installation is done  you can follow the below steps to run a quick demo.           """;Computer Vision;https://github.com/ZhZiKai/VisDrone_FCOS
"""```python from lsuv import lsuv_init  model = lsuv_init(ResNet34()  train_loader  needed_std=1.0  std_tol=0.1                    max_attempts=10  do_orthonorm=True  device=device) ```   """;General;https://github.com/shunk031/LSUV.pytorch
"""Once you have completed installation of all required packages   run the following command within a shell. ``` $ cd hjdqn && pip install -e . ``` It is recommended that you activate a separate virtual environment during installation.  We also provide custom gym environment to test algorithms on LQR tasks. To install these environments  move to `gym_lqr` folder and install the package as follows: ``` $ cd gym_lqr && pip install -e . ``` By doing so  you register these new environments to gym registry  which enables you to create LQR environment instances by simply calling `make` function. The following code will make a 20-dimensional LQR environment: ``` env = gym.make('LinearQuadraticRegulator20D-v0') ```     you may run the following command:   For instance  to reproduce the result over 20-dimensional LQR with HJ DQN  run the following command:   """;Reinforcement Learning;https://github.com/HJDQN/HJQ
"""""";Computer Vision;https://github.com/akshaysubr/TEGAN
"""""";General;https://github.com/akshaysubr/TEGAN
"""[Install Python 3 with anaconda](https://www.continuum.io/downloads)      $ git clone https://github.com/etetteh/sota-data-augmentation-and-optimizers     $ cd sota-data-augmentation-and-optimizers     $ pip install -r requirements.txt   You may run the following line of code in your bash terminal              [--adalook] [--deepmemory] [--ranger] [--resume] [--path PATH]   --path PATH           path to checkpoint. pass augmentation name   """;Computer Vision;https://github.com/etetteh/sota-data-augmentation-and-optimizers
"""3. Clone this repository: ``git clone --recurse-submodules https://github.com/maxfrei750/DeepParticleNet.git``   **Optional:** Start Tensorboard   1. Copy the jupyter token from your command line. 2. Enter the jupyter server by accessing ``localhost:8888/lab`` in your browser and pasting the jupyter token that you just copied.  """;Computer Vision;https://github.com/maxfrei750/DeepParticleNet
"""| enwik8_pers.sh | 114M | 1.00 bpb | 0.98 bpb |   """;General;https://github.com/facebookresearch/adaptive-span
"""```bash $ pip install big-sleep ```   You will be able to have the GAN dream up images using natural language with a one-line command in the terminal.   You can now train more than one phrase using the delimiter ""|""   ```bash $ dream ""a pyramid made of ice"" ```  Images will be saved to wherever the command is invoked   """;Computer Vision;https://github.com/lucidrains/big-sleep
"""```bash $ pip install big-sleep ```   You will be able to have the GAN dream up images using natural language with a one-line command in the terminal.   You can now train more than one phrase using the delimiter ""|""   ```bash $ dream ""a pyramid made of ice"" ```  Images will be saved to wherever the command is invoked   """;General;https://github.com/lucidrains/big-sleep
"""source: A streaming signal source   aggregator: Over one or several observed windows  update incrementally some aggregates of the streaming stats.   These steps will be detailed in the following sections.   One notable property to retain is the ‚Äúsnip confusion‚Äù metric     opens the possibility to merge codebooks or translate (at least approximately) one snipping system to another.   """;Audio;https://github.com/otosense/slang
"""source: A streaming signal source   aggregator: Over one or several observed windows  update incrementally some aggregates of the streaming stats.   These steps will be detailed in the following sections.   One notable property to retain is the ‚Äúsnip confusion‚Äù metric     opens the possibility to merge codebooks or translate (at least approximately) one snipping system to another.   """;Sequential;https://github.com/otosense/slang
"""``` python3 main.py -h  usage: main.py [-h] [--gpus GPUS] [--cpus CPUS] [--save_dir SAVE_DIR]                [--img_num IMG_NUM] [--optim_G {adam sgd}]                [--optim_D {adam sgd}] [--loss {wgangp lsgan}]                [--start_resl START_RESL] [--end_resl END_RESL]                [--beta [BETA [BETA ...]]] [--momentum MOMENTUM]                [--decay DECAY] [--gp_lambda GP_LAMBDA]  PGGAN  optional arguments:   -h  --help            show this help message and exit   --gpus GPUS           Select GPU Numbering | 0 1 2 3 |   --cpus CPUS           The number of CPU workers   --save_dir SAVE_DIR   Directory which models will be saved in   --img_num IMG_NUM     The number of images to be used for each phase   --optim_G {adam sgd}   --optim_D {adam sgd}   --loss {wgangp lsgan}   --start_resl START_RESL   --end_resl END_RESL   --beta [BETA [BETA ...]]                         Beta for Adam optimizer   --momentum MOMENTUM   Momentum for SGD optimizer   --decay DECAY         Weight decay for optimizers   --gp_lambda GP_LAMBDA                         Lambda as a weight of Gradient Panelty in WGAN-GP loss ```  <hr>   """;Computer Vision;https://github.com/zsef123/PGGAN-Pytorch
"""- clone the github repository and checkout the specific version.       git clone https://github.com/EdinburghNLP/nematus.git      cd nematus      git checkout tags/v0.3   About the Theano-experiments: You can download the Theano-version Nematus   We train the model using the following command   Clone the above repository   Once downloaded  follow the instructions on the main page for evaluating models. Notice that   Clone the above repository   We use the following command to train the model:   """;General;https://github.com/bzhangGo/rmsnorm
"""This repository is built using PyTorch. You can install the necessary libraries by pip installing the requirements text file `pip install -r ./requirements.txt` The code was set up using **python=3.6.7**   All the pre-trained models are available here. The user has to download the weight files and store them under the ./models/ directory.  GoogLeNet-R can be trained using the script provided in train.sh. The user has to install the robustness repo and provide the input directory for the ImageNet training images under data_path argument.   Note: Before running the train.sh scripts  replace the files under robustness/imagenet_models/ (e.g. ~/anaconda3/envs/your_env/lib/python3.6/site-packages/robustness/imagenet_models/) in the robustness library folder with the files in the folder madry_files.  These are the following modifations that we made to the robustness directory (now ./naman_robustness).    - The shell script for generating Figure 1 of our paper is in [teaser.sh](teaser.sh). Given an [image](./Images/teaser/ILSVRC2012_val_00002056.JPEG)  the script runs SmoothGrad  Sliding-Patch  LIME  and Meaningful Perturbation algorithm for their different hyperparameters and produces a montage image of their respective [attribution maps](./results/formal_teaser.jpg)   """;Computer Vision;https://github.com/anguyen8/sam
"""This repository is built using PyTorch. You can install the necessary libraries by pip installing the requirements text file `pip install -r ./requirements.txt` The code was set up using **python=3.6.7**   All the pre-trained models are available here. The user has to download the weight files and store them under the ./models/ directory.  GoogLeNet-R can be trained using the script provided in train.sh. The user has to install the robustness repo and provide the input directory for the ImageNet training images under data_path argument.   Note: Before running the train.sh scripts  replace the files under robustness/imagenet_models/ (e.g. ~/anaconda3/envs/your_env/lib/python3.6/site-packages/robustness/imagenet_models/) in the robustness library folder with the files in the folder madry_files.  These are the following modifations that we made to the robustness directory (now ./naman_robustness).    - The shell script for generating Figure 1 of our paper is in [teaser.sh](teaser.sh). Given an [image](./Images/teaser/ILSVRC2012_val_00002056.JPEG)  the script runs SmoothGrad  Sliding-Patch  LIME  and Meaningful Perturbation algorithm for their different hyperparameters and produces a montage image of their respective [attribution maps](./results/formal_teaser.jpg)   """;General;https://github.com/anguyen8/sam
"""installing anything  and goes through the following experiments / SIREN properties:   Some of the experiments were run using the BSD500 datast  which you can download here.   If you want to reproduce all the results (including the baselines) shown in the paper  the videos  point clouds  and  audio files can be found [here](https://drive.google.com/drive/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K?usp=sharing).  You can then set up a conda environment with all dependencies like so: ``` conda env create -f environment.yml conda activate siren ```   """;General;https://github.com/vsitzmann/siren
"""So we talked about what GNNs are  and what they can do for you (among other things). <br/> Let's get this thing running! Follow the next steps:  1. `git clone https://github.com/gordicaleksa/pytorch-GAT` 2. Open Anaconda console and navigate into project directory `cd path_to_repo` 3. Run `conda env create` from project directory (this will create a brand new conda environment). 4. Run `activate pytorch-gat` (for running scripts from your console or setup the interpreter in your IDE)  That's it! It should work out-of-the-box executing environment.yml file which deals with dependencies. <br/>  -----  PyTorch pip package will come bundled with some version of CUDA/cuDNN with it  but it is highly recommended that you install a system-wide CUDA beforehand  mostly because of the GPU drivers.  I also recommend using Miniconda installer as a way to get conda on your system. Follow through points 1 and 2 of [this setup](https://github.com/Petlja/PSIML/blob/master/docs/MachineSetup.md) and use the most up-to-date versions of Miniconda and CUDA/cuDNN for your system.   Hardware requirements   We'd love GAT's attention distributions to be skewed. You can see in orange how the histogram looks like for ideal uniform distributions    Just do pip uninstall pywin32 and then either pip install pywin32 or conda install pywin32 should fix it!   Everything needed to train GAT on Cora is already setup. To run it (from console) just call: <br/>   * Save metrics into runs/  just run tensorboard --logdir=runs from your Anaconda to visualize it   you don't have a strong GPU with at least 8 GBs you'll need to add the --force_cpu flag to train GAT on CPU.   On the left you can see the node with the highest degree in the whole Cora dataset.   You just need to link the Python environment you created in the [setup](#setup) section.   """;Graphs;https://github.com/gordicaleksa/pytorch-GAT
"""For Cityscapes or ADE20K  the datasets must be downloaded beforehand. Please download them on the respective webpages.    **Preparing ADE20K Dataset**. The dataset can be downloaded [here](http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip)  which is from [MIT Scene Parsing BenchMark](http://sceneparsing.csail.mit.edu/). After unzipping the datgaset  put the jpg image files `ADEChallengeData2016/images/` and png label files `ADEChallengeData2016/annotatoins/` in the same directory.   There are different modes to load images by specifying `--preprocess_mode` along with `--load_size`. `--crop_size`. There are options such as `resize_and_crop`  which resizes the images into square images of side length `load_size` and randomly crops to `crop_size`. `scale_shortside_and_crop` scales the image to have a short side of length `load_size` and crops to `crop_size` x `crop_size` square. To see all modes  please use `python train.py --help` and take a look at `data/base_dataset.py`. By default at the training phase  the images are randomly flipped horizontally. To prevent this use `--no_flip`. -->   Clone this repo. ```bash git clone https://github.com/NVlabs/SPADE.git cd SPADE/ ```  This code requires PyTorch 1.0 and python 3+. Please install dependencies by ```bash pip install -r requirements.txt ```  This code also requires the Synchronized-BatchNorm-PyTorch rep. ``` cd models/networks/ git clone https://github.com/vacancy/Synchronized-BatchNorm-PyTorch cp -rf Synchronized-BatchNorm-PyTorch/sync_batchnorm . cd ../../ ```  To reproduce the results reported in the paper  you would need an NVIDIA DGX1 machine with 8 V100 GPUs.   ![city](./Demo/example_1.png)  ![ade20k](./Demo/example_2.png)  ![city_seq](./Demo/seq_city.png)  ![ade_seq](./Demo/seq_ade20k.png)   <!--  Semantic Image Synthesis with Spatially-Adaptive Normalization.<br> [Taesung Park](http://taesung.me/)   [Ming-Yu Liu](http://mingyuliu.net/)  [Ting-Chun Wang](https://tcwang0509.github.io/)   and [Jun-Yan Zhu](http://people.csail.mit.edu/junyanz/).<br> In CVPR 2019 (Oral).   Follow the instructions in [SPADE](https://github.com/nvlabs/spade/) to prepare the Cityscapes and ADE20K datasets  and install the dependencies (PyTorch 1.0  python 3+  Synchronized-BatchNorm-PyTorch  etc.)  Download the [pretrained models and retrieval results](https://knightsucfedu39751-my.sharepoint.com/:f:/g/personal/liyandong_knights_ucf_edu/EhShfbCwdFJAoenVfw3FcSEBP1qe-RWa05CaCMvJjQ1wgQ?e=7eDPkl) and `unzip` the downloaded file (BachGan.zip).  ``` mv BachGan/ade_retrieval/validation.zip datasets/ADEChallengeData2016/retrival_halfset/ mv BachGan/city_retrieval/retrival_img_pairs_halfset_val_all.pkl scripts/retrival_ious_city_halfset/retrival_img_pairs_halfset_val_all.pkl cd datasets/ADEChallengeData2016/retrival_halfset/ unzip validation.zip ``` If you want to get the retrieval results by yourself  please refer to `scripts/retrival_ious_city_halfset/retrival_seg_halfset.py`  Evaluate the pretrained model on Cityscapes.  ``` mv BachGan/city_model/60_net_G.pth ./checkpoints/city_box_retrival_hallucinate/ python test.py --name city_box_retrival_hallucinate/ --dataset_mode cityscapes --dataroot datasets/cityscapes --retrival_memory --batchSize=10 --gpu_id=0 1 --no_instance --which_epoch=60 ```  Download [ADE20K meta data](https://github.com/CSAILVision/sceneparsing). And put the downloaded folder under `../` ``` git clone https://github.com/CSAILVision/sceneparsing mv sceneparsing ../ ``` Evaluate the pretrained model on ADE20K.  ``` mv BachGan/ade_model/150_net_G.pth ./checkpoints/ade_box_retrival_hallucinate/ python test.py --name ./ade_box_retrival_hallucinate/ --dataset_mode ade20k --dataroot ./datasets/ADEChallengeData2016/ --retrival_memory --batchSize=14 --gpu_id=0 1 --which_epoch=150 ```     """;General;https://github.com/Cold-Winter/BachGAN
"""```shell $ python3 -m venv venv $ source venv/bin/activate $ pip install torch tqdm $ brew install libomp $ export LC_ALL=en_US.UTF-8 $ export LANG=en_US.UTF-8 $ pip install -r requirements.txt ```   1. download GPT2 pre-trained model in Pytorch which huggingface/pytorch-pretrained-BERT already made! (Thanks for sharing! it's help my problem transferring tensorflow(ckpt) file to Pytorch Model!) ```shell $ git clone https://github.com/graykode/gpt-2-Pytorch && cd gpt-2-Pytorch #: download huggingface's pytorch model  $ curl --output gpt2-pytorch_model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin #: setup requirements  if using mac os  then run additional setup as descibed below $ pip install -r requirements.txt ```   2. Now  You can run like this.  - Text from Book 1984  George Orwell  ```shell $ python main.py --text ""It was a bright cold day in April  and the clocks were striking thirteen. Winston Smith  his chin nuzzled into his breast in an effort to escape the vile wind  slipped quickly through the glass doors of Victory Mansions  though not quickly enough to prevent a swirl of gritty dust from entering along with him."" ```  3. Also You can Quick Starting in [Google Colab](https://colab.research.google.com/github/graykode/gpt-2-Pytorch/blob/master/GPT2_Pytorch.ipynb)     """;General;https://github.com/graykode/gpt-2-Pytorch
"""```shell $ python3 -m venv venv $ source venv/bin/activate $ pip install torch tqdm $ brew install libomp $ export LC_ALL=en_US.UTF-8 $ export LANG=en_US.UTF-8 $ pip install -r requirements.txt ```   1. download GPT2 pre-trained model in Pytorch which huggingface/pytorch-pretrained-BERT already made! (Thanks for sharing! it's help my problem transferring tensorflow(ckpt) file to Pytorch Model!) ```shell $ git clone https://github.com/graykode/gpt-2-Pytorch && cd gpt-2-Pytorch #: download huggingface's pytorch model  $ curl --output gpt2-pytorch_model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin #: setup requirements  if using mac os  then run additional setup as descibed below $ pip install -r requirements.txt ```   2. Now  You can run like this.  - Text from Book 1984  George Orwell  ```shell $ python main.py --text ""It was a bright cold day in April  and the clocks were striking thirteen. Winston Smith  his chin nuzzled into his breast in an effort to escape the vile wind  slipped quickly through the glass doors of Victory Mansions  though not quickly enough to prevent a swirl of gritty dust from entering along with him."" ```  3. Also You can Quick Starting in [Google Colab](https://colab.research.google.com/github/graykode/gpt-2-Pytorch/blob/master/GPT2_Pytorch.ipynb)     """;Natural Language Processing;https://github.com/graykode/gpt-2-Pytorch
"""``` python benchmark.py ``` - Î°úÏª¨ÏóêÏÑú Ïó¨Îü¨ Î™®Îç∏ÏùÑ Ïã§ÌñâÏãú ÌèâÍ∑† Ï∂îÎ°†ÏãúÍ∞ÑÏùÑ Í≥ÑÏÇ∞ Ìï† Ïàò ÏûàÎã§. - Í∏∞Î≥∏Ï†ÅÏúºÎ°ú CPUÎ•º ÏÇ¨Ïö©ÌïòÎ©∞ GPUÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎäî ÌôòÍ≤ΩÏù∏ Í≤ΩÏö∞ GPUÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ïó∞ÏÇ∞ÌïúÎã§.  ``` python benchmark_with_streaming.py ``` - Ïä§Ìä∏Î¶¨Î∞çÏùÑ ÌÜµÌï¥ ÏûÖÎ†•Î∞õÏùÄ Î™®Îç∏ÏóêÏÑú Í∞ùÏ≤¥ ÌÉêÏßÄÎ•º ÏàòÌñâÌïúÎã§. -> Í∞úÏÑ†Îêú Î™®Îç∏Î°ú Ï∂îÍ∞Ä ÏòàÏ†ï - Ïä§Ìä∏Î¶¨Î∞çÏãú ÌîÑÎ†àÏûÑÎãπ Ï∂îÎ°†ÏãúÍ∞ÑÏùÑ Ï∂úÎ†•ÌïúÎã§. (ÌèâÍ∑† Ï∂îÎ°† ÏãúÍ∞ÑÏùÑ Íµ¨ÌïòÍ≥† Ïã∂Îã§Î©¥ benchmark.pyÎ•º ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÌïúÎã§.) - ÌòÑÏû¨ Îç∞Î™® ÏòÅÏÉÅÏùÑ ÏÇ¨Ïö©ÌïòÍ≥† ÏûàÏúºÎ©∞ ÎùºÏ¶àÎ≤†Î¶¨ÌååÏù¥ÏóêÏÑú ÏàòÏã†ÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî VideoCaptureÎ•º Îã§ÏùåÍ≥º Í∞ôÏù¥ Î≥ÄÍ≤ΩÌï¥ÏïºÌïúÎã§.  ÌòÑÏû¨ ```python cap = cv.VideoCapture('https://www.freedesktop.org/software/gstreamer-sdk/data/media/sintel_trailer-480p.webm') ```  Î≥ÄÍ≤Ω ```python cap = cv.VideoCapture('udpsrc port=9777 ! application/x-rtp ! rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! appsink'  cv.CAP_GSTREAMER) ``` - ÎùºÏ¶àÎ≤†Î¶¨ÌååÏù¥ÏôÄ Ïó∞ÎèôÏãú ÎÑ§Ìä∏ÏõåÌÅ¨ ÏóêÎÆ¨Î†àÏù¥ÌÑ∞Î•º ÌÜµÌï¥ÏÑú 4G ÎòêÎäî 5GÏùò ÌôòÍ≤ΩÏùÑ Ï°∞ÏÑ±Ìï¥ÏÑú Ïã§ÌñâÌïòÎèÑÎ°ùÌïúÎã§. - ÏÑúÎ≤ÑÏóêÏÑú Îã§Ïãú ÎùºÏ¶àÎ≤†Î¶¨ÌååÏù¥Î°ú ÏòÅÏÉÅÏùÑ Î≥¥ÎÇ¥Îäî Í≤ΩÏö∞Îäî ÏïÑÏßÅ Íµ¨ÌòÑÎêòÏñ¥ ÏûàÏßÄ ÏïäÎã§.    """;General;https://github.com/Video-Streaming-Pipeline/Video-Streaming-Pipeline
"""``` python benchmark.py ``` - Î°úÏª¨ÏóêÏÑú Ïó¨Îü¨ Î™®Îç∏ÏùÑ Ïã§ÌñâÏãú ÌèâÍ∑† Ï∂îÎ°†ÏãúÍ∞ÑÏùÑ Í≥ÑÏÇ∞ Ìï† Ïàò ÏûàÎã§. - Í∏∞Î≥∏Ï†ÅÏúºÎ°ú CPUÎ•º ÏÇ¨Ïö©ÌïòÎ©∞ GPUÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎäî ÌôòÍ≤ΩÏù∏ Í≤ΩÏö∞ GPUÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ïó∞ÏÇ∞ÌïúÎã§.  ``` python benchmark_with_streaming.py ``` - Ïä§Ìä∏Î¶¨Î∞çÏùÑ ÌÜµÌï¥ ÏûÖÎ†•Î∞õÏùÄ Î™®Îç∏ÏóêÏÑú Í∞ùÏ≤¥ ÌÉêÏßÄÎ•º ÏàòÌñâÌïúÎã§. -> Í∞úÏÑ†Îêú Î™®Îç∏Î°ú Ï∂îÍ∞Ä ÏòàÏ†ï - Ïä§Ìä∏Î¶¨Î∞çÏãú ÌîÑÎ†àÏûÑÎãπ Ï∂îÎ°†ÏãúÍ∞ÑÏùÑ Ï∂úÎ†•ÌïúÎã§. (ÌèâÍ∑† Ï∂îÎ°† ÏãúÍ∞ÑÏùÑ Íµ¨ÌïòÍ≥† Ïã∂Îã§Î©¥ benchmark.pyÎ•º ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÌïúÎã§.) - ÌòÑÏû¨ Îç∞Î™® ÏòÅÏÉÅÏùÑ ÏÇ¨Ïö©ÌïòÍ≥† ÏûàÏúºÎ©∞ ÎùºÏ¶àÎ≤†Î¶¨ÌååÏù¥ÏóêÏÑú ÏàòÏã†ÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî VideoCaptureÎ•º Îã§ÏùåÍ≥º Í∞ôÏù¥ Î≥ÄÍ≤ΩÌï¥ÏïºÌïúÎã§.  ÌòÑÏû¨ ```python cap = cv.VideoCapture('https://www.freedesktop.org/software/gstreamer-sdk/data/media/sintel_trailer-480p.webm') ```  Î≥ÄÍ≤Ω ```python cap = cv.VideoCapture('udpsrc port=9777 ! application/x-rtp ! rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! appsink'  cv.CAP_GSTREAMER) ``` - ÎùºÏ¶àÎ≤†Î¶¨ÌååÏù¥ÏôÄ Ïó∞ÎèôÏãú ÎÑ§Ìä∏ÏõåÌÅ¨ ÏóêÎÆ¨Î†àÏù¥ÌÑ∞Î•º ÌÜµÌï¥ÏÑú 4G ÎòêÎäî 5GÏùò ÌôòÍ≤ΩÏùÑ Ï°∞ÏÑ±Ìï¥ÏÑú Ïã§ÌñâÌïòÎèÑÎ°ùÌïúÎã§. - ÏÑúÎ≤ÑÏóêÏÑú Îã§Ïãú ÎùºÏ¶àÎ≤†Î¶¨ÌååÏù¥Î°ú ÏòÅÏÉÅÏùÑ Î≥¥ÎÇ¥Îäî Í≤ΩÏö∞Îäî ÏïÑÏßÅ Íµ¨ÌòÑÎêòÏñ¥ ÏûàÏßÄ ÏïäÎã§.    """;Computer Vision;https://github.com/Video-Streaming-Pipeline/Video-Streaming-Pipeline
"""- Source code:https://github.com/AlexeyAB/darknet   Download weight   For coco dataset you can use tool/coco_annotation.py.   ONNX and TensorRT models are converted from Pytorch (TianXiaomo): Pytorch->ONNX->TensorRT.   | Pytorch (TianXiaomo)|       0.466 |       0.704 |       0.505 |       0.267 |       0.524 |       0.629 |   | Pytorch (TianXiaomo)|       0.404 |       0.615 |       0.436 |       0.196 |       0.438 |       0.552 |   Pytorch version Recommended:   Install onnxruntime  pip install onnxruntime   Pytorch version Recommended:   Install onnxruntime  pip install onnxruntime   TensorRT version Recommended: 7.0  7.1   1: Thanks:github:https://github.com/onnx/onnx-tensorflow  2: Run git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow  Run pip install -e .  Note:Errors will occur when using ""pip install onnx-tf""  at least for me it is recommended to use source code installation  1. Compile the DeepStream Nvinfer Plugin   cd DeepStream        make   ```sh python demo_trt.py <tensorRT_engine_file> <input_image> <input_H> <input_W> ```  - This demo here only works when batchSize is dynamic (1 should be within dynamic range) or batchSize=1  but you can update this demo a little for other dynamic or static batch sizes.      - Note1: input_H and input_W should agree with the input size in the original ONNX file.      - Note2: extra NMS operations are needed for the tensorRT output. This demo uses python NMS code from `tool/utils.py`.    """;Computer Vision;https://github.com/sung0471/pytorch-YOLOv4
""" Follow [OLTR](https://github.com/zhmiao/OpenLongTailRecognition-OLTR) for data preparation.     - Step 1: Train Expert models  or use the pre-trained model in ./logs/ImageNet_LT/ ``` CUDA_VISIBLE_DEVICES=0 python main.py --config=./config/many_shot.py CUDA_VISIBLE_DEVICES=0 python main.py --config=./config/median_shot.py CUDA_VISIBLE_DEVICES=0 python main.py --config=./config/low_shot.py ```  - Step 2: Train a single model using the LFME ``` CUDA_VISIBLE_DEVICES=0 python main_LFME.py --config=./config/ImageNet_LT/LFME.py ```  - Evaluate LFME: ``` CUDA_VISIBLE_DEVICES=0 python main_LFME.py --config=./config/ImageNet_LT/LFME.py --test ```   """;General;https://github.com/xiangly55/LFME
"""""";Computer Vision;https://github.com/fupiao1998/res2net-keras
"""1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   Requirements: hardware  Basic installation   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   To train and test a Faster R-CNN detector using the **alternating optimization** algorithm from our NIPS 2015 paper  use `experiments/scripts/faster_rcnn_alt_opt.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  (""alt opt"" refers to the alternating optimization training algorithm described in the NIPS paper.)  To train and test a Faster R-CNN detector using the **approximate joint training** method  use `experiments/scripts/faster_rcnn_end2end.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  This method trains the RPN module jointly with the Fast R-CNN network  rather than alternating between training the two. It results in faster (~ 1.5x speedup) training times and similar detection accuracy. See these [slides](https://www.dropbox.com/s/xtr4yd4i5e0vw8g/iccv15_tutorial_training_rbg.pdf?dl=0) for more details.  Artifacts generated by the scripts in `tools` are written in this directory.  Trained Fast R-CNN networks are saved under:  ``` output/<experiment directory>/<dataset name>/ ```  Test outputs are saved under:  ``` output/<experiment directory>/<dataset name>/<network snapshot name>/ ```  """;Computer Vision;https://github.com/rbgirshick/py-faster-rcnn
"""- A disciplined approach to Neural Network Hyper-parameters:  - IIIT and Oxford Pet research paper:    """;General;https://github.com/AbhimanyuAryan/ImageClassification
"""""";Natural Language Processing;https://github.com/lkfo415579/MT-Readling-List
"""""";Computer Vision;https://github.com/atspatel/pydata_inception_v2
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/wodyjowski/colab-training
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV   Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/wodyjowski/colab-training
"""* Fix output dtype of `AddGaussianNoise`   * Fix a bug in `ClippingDistortion` where the min_percentile_threshold was not respected as expected. * Improve handling of empty input   * Fix filter instability bug in `FrequencyMask`. Thanks to kvilouras.   * Improve compatibility of output files written by the demo script. Thanks to xwJohn. * Fix division by zero bug in `Normalize`. Thanks to ZFTurbo.   * Correctly find audio files with upper case filename extensions. * Fix a bug where AddBackgroundNoise crashed when trying to add digital silence to an input. Thanks to juheeuu.   * Fix picklability of instances of `AddImpulseResponse`  `AddBackgroundNoise`  and `AddShortNoises`   * Fix a bug in `SpecChannelShuffle` where it did not support more than 3 audio channels. Thanks to omerferhatt. * Limit scipy version range to >=1.0 <1.6 to avoid issues with loading 24-bit wav files. Support for scipy>=1.6 will be added later.   * Avoid division by zero in `AddImpulseResponse` when input is digital silence (all zeros) * Fix inverse SNR characteristics in `AddGaussianSNR`. It will continue working as before   unless you switch to the new parameters `min_snr_in_db` and `max_snr_in_db`. If you   use the old parameters  you'll get a warning.   * Remove global `pydub` import which was accidentally introduced in v0.18.0. `pydub` is  considered an optional dependency and is imported only on demand now.   ![Python version support](https://img.shields.io/pypi/pyversions/audiomentations) [![PyPI version](https://img.shields.io/pypi/v/audiomentations.svg?style=flat)](https://pypi.org/project/audiomentations/) [![Number of downloads from PyPI per month](https://img.shields.io/pypi/dm/audiomentations.svg?style=flat)](https://pypi.org/project/audiomentations/)  `pip install audiomentations`   Need a Pytorch alternative with GPU support? Check out torch-audiomentations!   augmentation has proved to make speech recognition models more robust.   The code runs on CPU  not GPU. For a GPU-compatible version  check out pytorch-audiomentations       e.g. from audiomentations import calculate_rms  now you have to do   Install the dependencies specified in requirements.txt   | Name | Github stars | License | Last commit | GPU support? |   `python -m demo.demo`   """;Computer Vision;https://github.com/iver56/audiomentations
"""              all_conf = all_conf.numpy()                all_class = all_class.numpy()                all_bbox = all_bbox.numpy()   Python3.7  opencv-python  PaddlePaddle 2.1.2   ÁôªÂΩïAI StudioÂèØÂú®Á∫øËøêË°åÔºöhttps://aistudio.baidu.com/aistudio/projectdetail/2259467       parser.add_argument('-v'  '--version'  default='yolo'                        help='use gpu.')   parser.add_argument('-v'  '--version'  default='yolo'                        help='use cuda.')   parser.add_argument('-v'  '--version'  default='yolo'                        help='Use gpu')   """;Computer Vision;https://github.com/sunlizhuang/YOLOv1-PaddlePaddle
"""This is the official PyTorch implementation of the **Attribute2Font: Creating Fonts You Want From Attributes**.  ![Teaser](img/teaser.png)  Paper: [arXiv](https://arxiv.org/abs/2005.07865) | [Research Gate](https://www.researchgate.net/publication/341423467_Attribute2Font_Creating_Fonts_You_Want_From_Attributes/comments)    Supplementary Material: [link](paper/Siggraph2020_Attr2Font_Supplemental_Material.pdf)    Video: [link](img/att2font_demo.mov)    Code: [GitHub](https://github.com/hologerry/Attr2Font)        1. Install PyTorch  torchvison and dependencies from [https://pytorch.org](https://pytorch.org) 2. Clone this repo:    ```shell    git clone https://github.com/hologerry/Attr2Font    cd Attr2Font    ``` 3. Download the official pre-trained vgg19 model: [vgg19-dcbb9e9d.pth](https://download.pytorch.org/models/vgg19-dcbb9e9d.pth)  and put it under this project root folder    """;Computer Vision;https://github.com/hologerry/Attr2Font
"""                        lmdb database to use for (Required)                           lmdb database to use for testing (Required)                           per gpu   """;Computer Vision;https://github.com/usnistgov/semantic-segmentation-fc-densenet
"""```bash #: installation pip install -r requirements.txt #: to train python copy_task.py --train #: to evaluate python copy_task.py --eval  ```   """;General;https://github.com/clemkoa/ntm
"""```bash #: installation pip install -r requirements.txt #: to train python copy_task.py --train #: to evaluate python copy_task.py --eval  ```   """;Sequential;https://github.com/clemkoa/ntm
"""""";General;https://github.com/LinHuiTeng/TrashClassifier
"""""";Computer Vision;https://github.com/LinHuiTeng/TrashClassifier
"""paddle version for deeplab  ËøêË°åpip install -r requirements.txt  Âü∫‰∫épaddleSegÔºåËã•ÂÆâË£ÖpydencrfÂ§±Ë¥•ÔºåËØ∑Â∞ùËØïpip install git+https://github.com/lucasb-eyer/pydensecrf.git  Ëß£ÂéãÊ®°ÂûãÂêéÂ∞Übest_modelÊñá‰ª∂Â§πÊîæÁΩÆÂú®deeplabv2_cityscapes_b6‰∏ãÔºåËøêË°årun_eval.sh ‰∏≠Âç≥ÂèØÂæóÂà∞ÊúÄÁªàÁªìÊûú„ÄÇ   ÈìæÊé•: https://pan.baidu.com/s/1c9_LmvzK5wcsvipuw7FsrA   """;Computer Vision;https://github.com/Shun14/deeplab-paddle
"""See the examples directory.   Some examples use keras  a neural networks library.   """;Computer Vision;https://github.com/paulorauber/nn
""" version 1:   * version 2:    version 3:   """;Computer Vision;https://github.com/sandipsahajoy/Predicting-Lymphoma-using-CNN-in-Keras
"""- Install [PyTorch](http://pytorch.org/)(version v1.0 as of on March 2019) by selecting your environment on the website and running the appropriate command. - Please install cv2 and visdom form conda-forge.  - I recommend using anaconda 3.7.  - You will also need Matlab. If you have distributed computing license then it would be faster otherwise it should also be fine.  Just replace `parfor` with simple `for` in Matlab scripts. I would be happy to accept a PR for python version of this part. - Clone this repository.    * Note: We currently only support Python 3.7 with Pytorch version v1.0 on Linux system. - We currently support [UCF24](http://www.thumos.info/download.html) with [revised annotaions](https://github.com/gurkirt/corrected-UCF101-Annots) released with our [real-time online action detection paper](https://arxiv.org/pdf/1611.08563.pdf). Unlike [ROAD](https://github.com/gurkirt/realtime-action-detection) implementation  we support [JHMDB21](http://jhmdb.is.tue.mpg.de/) as well. - Similar to [ROAD](https://github.com/gurkirt/realtime-action-detection) setup  to simulate the same training and evaluation setup we provide extracted `rgb` images from videos along with optical flow images (both `brox flow` and `real-time flow`) computed for the UCF24 and JHMDB21 datasets. You can download it from my [google drive link](https://drive.google.com/drive/folders/1o0JNYZl2Wv9bi66wF_SQ4N5cxdCyHTJR?usp=sharing)) - Install opencv package for anaconda using ``conda install opencv`` - We also support [Visdom](https://github.com/facebookresearch/visdom) for visualization of loss and frame-meanAP on validation subset during training.   * To use Visdom in the browser:    ```Shell   #: First install Python server and client    conda install -c conda-forge visdom   #: Start the server (probably in a screen or tmux)   python -m visdom.server --port=8097   ```   * Then (during training) navigate to http://localhost:8097/ (see the Training section below for more details).   <a href='#installation'>Installation</a>   Similar to ROAD  we requires VGG-16 weights pretrained on UCF24 using ROAD implmentation.   If you want you can train for these weights using ROAD   OR download the above pretrained models to above directory.    You can use --fusion_type=CAT for concatnation fusion. Sum Fusion requires little less GPU memory.    For instructions on Visdom usage/installation  see the <a href='#installation'>Installation</a> section. By default  it is off.  If you don't like to use visdom then you always keep track of train using logfile which is saved under save_root directory   To compute frame-mAP you can use frameAP.m script. You will need to specify data_root  data_root.    - NMS is performed once in python then again in Matlab; one has to do that on GPU in python   Also  Feynman27 pushed a python version of the incremental_linking   """;Computer Vision;https://github.com/gurkirt/AMTNet
"""""";Computer Vision;https://github.com/CuberrChen/STDCNet-Paddle
"""""";Sequential;https://github.com/TarunVallabhuni/awesome-feature-engineering-master
"""""";Computer Vision;https://github.com/gouthamvgk/coreml_conversion_hub
"""""";General;https://github.com/gouthamvgk/coreml_conversion_hub
"""From pypi : &nbsp;  ```shell $ pip install LibRecommender==0.6.10 ```  To build from source  you 'll first need [Cython](<https://cython.org/>) and [Numpy](<https://numpy.org/>):  ```shell $ #: pip install numpy cython $ git clone https://github.com/massquantity/LibRecommender.git $ cd LibRecommender $ python setup.py install ```     ```python import numpy as np import pandas as pd from libreco.data import random_split  DatasetPure from libreco.algorithms import SVDpp  #: pure data  algorithm SVD++ from libreco.evaluation import evaluate  data = pd.read_csv(""examples/sample_data/sample_movielens_rating.dat""  sep=""::""                     names=[""user""  ""item""  ""label""  ""time""])  #: split whole data into three folds for training  evaluating and testing train_data  eval_data  test_data = random_split(data  multi_ratios=[0.8  0.1  0.1])  train_data  data_info = DatasetPure.build_trainset(train_data) eval_data = DatasetPure.build_evalset(eval_data) test_data = DatasetPure.build_testset(test_data) print(data_info)   #: n_users: 5894  n_items: 3253  data sparsity: 0.4172 %  svdpp = SVDpp(task=""rating""  data_info=data_info  embed_size=16  n_epochs=3  lr=0.001                reg=None  batch_size=256) #: monitor metrics on eval_data during training svdpp.fit(train_data  verbose=2  eval_data=eval_data  metrics=[""rmse""  ""mae""  ""r2""])  #: do final evaluation on test data print(""evaluate_result: ""  evaluate(model=svdpp  data=test_data                                      metrics=[""rmse""  ""mae""])) #: predict preference of user 2211 to item 110 print(""prediction: ""  svdpp.predict(user=2211  item=110)) #: recommend 7 items for user 2211 print(""recommendation: ""  svdpp.recommend_user(user=2211  n_rec=7))  #: cold-start prediction print(""cold prediction: ""  svdpp.predict(user=""ccc""  item=""not item""                                           cold_start=""average"")) #: cold-start recommendation print(""cold recommendation: ""  svdpp.recommend_user(user=""are we good?""                                                      n_rec=7                                                      cold_start=""popular"")) ```   ```python import numpy as np import pandas as pd from libreco.data import split_by_ratio_chrono  DatasetFeat from libreco.algorithms import YouTubeRanking  #: feat data  algorithm YouTubeRanking  data = pd.read_csv(""examples/sample_data/sample_movielens_merged.csv""  sep="" ""  header=0) data[""label""] = 1  #: convert to implicit data and do negative sampling afterwards  #: split into train and test data based on time train_data  test_data = split_by_ratio_chrono(data  test_size=0.2)  #: specify complete columns information sparse_col = [""sex""  ""occupation""  ""genre1""  ""genre2""  ""genre3""] dense_col = [""age""] user_col = [""sex""  ""age""  ""occupation""] item_col = [""genre1""  ""genre2""  ""genre3""]  train_data  data_info = DatasetFeat.build_trainset(     train_data  user_col  item_col  sparse_col  dense_col ) test_data = DatasetFeat.build_testset(test_data) train_data.build_negative_samples(data_info)  #: sample negative items for each record test_data.build_negative_samples(data_info) print(data_info)  #: n_users: 5962  n_items: 3226  data sparsity: 0.4185 %  ytb_ranking = YouTubeRanking(task=""ranking""  data_info=data_info  embed_size=16                               n_epochs=3  lr=1e-4  batch_size=512  use_bn=True                               hidden_units=""128 64 32"") ytb_ranking.fit(train_data  verbose=2  shuffle=True  eval_data=test_data                  metrics=[""loss""  ""roc_auc""  ""precision""  ""recall""  ""map""  ""ndcg""])  #: predict preference of user 2211 to item 110 print(""prediction: ""  ytb_ranking.predict(user=2211  item=110)) #: recommend 7 items for user 2211 print(""recommendation(id  probability): ""  ytb_ranking.recommend_user(user=2211  n_rec=7))  #: cold-start prediction print(""cold prediction: ""  ytb_ranking.predict(user=""ccc""  item=""not item""                                                 cold_start=""average"")) #: cold-start recommendation print(""cold recommendation: ""  ytb_ranking.recommend_user(user=""are we good?""                                                            n_rec=7                                                            cold_start=""popular"")) ```   """;General;https://github.com/massquantity/LibRecommender
"""""";Natural Language Processing;https://github.com/Soikonomou/albert_final_infer12
"""Before training  you should install all the required packages listed in the requirements.txt file.  Finally  go into the source code directory  and run:   Note that  you can use the flag --test_path to specify the dataset to evaluate.    Signed the following copyright announcement with your name and organization. Then complete the form online(https://forms.gle/APosP3W9vganmx9G6) and **mail** to shengao#pku.edu.cn ('#'->'@')  we will send you the corpus by e-mail when approved.   """;Natural Language Processing;https://github.com/gsh199449/stickerchat
"""Run the following command:   """;Computer Vision;https://github.com/ucuapps/WSMIS
"""```bash $ pip install axial_attention ```   Image  ```python import torch from axial_attention import AxialAttention  img = torch.randn(1  3  256  256)  attn = AxialAttention(     dim = 3                #: embedding dimension     dim_index = 1          #: where is the embedding dimension     dim_heads = 32         #: dimension of each head. defaults to dim // heads if not supplied     heads = 1              #: number of heads for multi-head attention     num_dimensions = 2     #: number of axial dimensions (images is 2  video is 3  or more)     sum_axial_out = True   #: whether to sum the contributions of attention on each axis  or to run the input through them sequentially. defaults to true )  attn(img) #: (1  3  256  256) ```  Channel-last image latents  ```python import torch from axial_attention import AxialAttention  img = torch.randn(1  20  20  512)  attn = AxialAttention(     dim = 512            #: embedding dimension     dim_index = -1       #: where is the embedding dimension     heads = 8            #: number of heads for multi-head attention     num_dimensions = 2   #: number of axial dimensions (images is 2  video is 3  or more) )  attn(img) #: (1  20  20  512) ```  Video  ```python import torch from axial_attention import AxialAttention  video = torch.randn(1  5  128  256  256)  attn = AxialAttention(     dim = 128            #: embedding dimension     dim_index = 2        #: where is the embedding dimension     heads = 8            #: number of heads for multi-head attention     num_dimensions = 3   #: number of axial dimensions (images is 2  video is 3  or more) )  attn(video) #: (1  5  128  256  256) ```  Image Transformer  with reversible network  ```python import torch from torch import nn from axial_attention import AxialImageTransformer  conv1x1 = nn.Conv2d(3  128  1)  transformer = AxialImageTransformer(     dim = 128      depth = 12      reversible = True )  img = torch.randn(1  3  512  512)  transformer(conv1x1(img)) #: (1  3  512  512) ```  With axial positional embedding  ```python import torch from axial_attention import AxialAttention  AxialPositionalEmbedding  img = torch.randn(1  512  20  20)  attn = AxialAttention(     dim = 512      heads = 8      dim_index = 1 )  pos_emb = AxialPositionalEmbedding(     dim = 512      shape = (20  20) )  img = pos_emb(img)  #: (1  512  20  20)  - now positionally embedded img = attn(img)     #: (1  512  20  20) ```   """;General;https://github.com/lucidrains/axial-attention
""" The easiest way to build and install all of PolyBeast's dependencies and run it is to use Docker:  ```shell $ docker build -t spiralpp . $ docker run --name spiralpp -it -p 8888:8888 spiralpp /bin/bash ```  or   ```shell $ docker run -it -p 8888:8888 urw7rs/spiralpp:latest /bin/bash ```  To run PolyBeast directly on Linux  follow this guide.   Create a new Conda environment  and install PolyBeast's requirements:   $ conda create -n spiralpp python=3.7  $ conda activate spiralpp  $ pip install -r requirements.txt  Install spiral-envs  Install required packages:   $ apt-get install cmake pkg-config protobuf-compiler libjson-c-dev intltool  $ pip install six setuptools numpy scipy gym  WARNING: Make sure that you have cmake 3.14 or later since we rely   Install cmake by running:   $ conda install cmake  Finally  run the following command to install the spiral-gym package itself:   $ git submodule update --init --recursive  $ pip install -e spiral-envs/  You will also need to obtain the brush files for the libmypaint environment   here. For example  you can   $ wget -c https://github.com/mypaint/mypaint-brushes/archive/v1.3.0.tar.gz -O - | tar -xz -C third_party  Finally  the Fluid Paint environment depends on the shaders from the original   $ git clone https://github.com/dli/paint third_party/paint   PolyBeast requires installing PyTorch  from source.  PolyBeast also requires gRPC  which can be installed by running:   $ conda install -c anaconda protobuf  $ ./scripts/install_grpc.sh   $ pip install nest/   $ python setup.py build develop   make sure you opened a port for the container.   """;General;https://github.com/urw7rs/spiralpp
""" The easiest way to build and install all of PolyBeast's dependencies and run it is to use Docker:  ```shell $ docker build -t spiralpp . $ docker run --name spiralpp -it -p 8888:8888 spiralpp /bin/bash ```  or   ```shell $ docker run -it -p 8888:8888 urw7rs/spiralpp:latest /bin/bash ```  To run PolyBeast directly on Linux  follow this guide.   Create a new Conda environment  and install PolyBeast's requirements:   $ conda create -n spiralpp python=3.7  $ conda activate spiralpp  $ pip install -r requirements.txt  Install spiral-envs  Install required packages:   $ apt-get install cmake pkg-config protobuf-compiler libjson-c-dev intltool  $ pip install six setuptools numpy scipy gym  WARNING: Make sure that you have cmake 3.14 or later since we rely   Install cmake by running:   $ conda install cmake  Finally  run the following command to install the spiral-gym package itself:   $ git submodule update --init --recursive  $ pip install -e spiral-envs/  You will also need to obtain the brush files for the libmypaint environment   here. For example  you can   $ wget -c https://github.com/mypaint/mypaint-brushes/archive/v1.3.0.tar.gz -O - | tar -xz -C third_party  Finally  the Fluid Paint environment depends on the shaders from the original   $ git clone https://github.com/dli/paint third_party/paint   PolyBeast requires installing PyTorch  from source.  PolyBeast also requires gRPC  which can be installed by running:   $ conda install -c anaconda protobuf  $ ./scripts/install_grpc.sh   $ pip install nest/   $ python setup.py build develop   make sure you opened a port for the container.   """;Reinforcement Learning;https://github.com/urw7rs/spiralpp
"""‚îÇ  ‚îÇ  ‚îî‚îÄ requirements.txt   ‚îÇ  ‚îÇ  ‚îú‚îÄ requirements.txt   """;Natural Language Processing;https://github.com/blawok/named-entity-recognition
"""To install from pip:   pip install xtcocotools   | COCO [6]         | 200K   | 17    |   ‚úîÔ∏è  |    ‚úîÔ∏è     |          |    *     |    ‚úîÔ∏è     |          |          | 250K  |   | COCO-WholeBody   | 200K   | 133   |   ‚úîÔ∏è  |   ‚úîÔ∏è      |    ‚úîÔ∏è     |    ‚úîÔ∏è     |    ‚úîÔ∏è     |    ‚úîÔ∏è     |    ‚úîÔ∏è     | 250K  |   1. COCO-WholeBody dataset is **ONLY** for research and non-commercial use.  2. The annotations of COCO-WholeBody dataset belong to [SenseTime Research](https://www.sensetime.com)  and are licensed under a [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by-nc/4.0/legalcode).  3. We do not own the copyright of the images. Use of the images must abide by the [Flickr Terms of Use](https://www.flickr.com/creativecommons/). The users of the images accept full responsibility for the use of the dataset  including but not limited to the use of any copies of copyrighted images that they may create from the dataset.   """;Computer Vision;https://github.com/jin-s13/COCO-WholeBody
"""""";General;https://github.com/keras-team/keras-applications
"""""";Computer Vision;https://github.com/keras-team/keras-applications
"""1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   Requirements: hardware  Basic installation   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   To train and test a Faster R-CNN detector using the **alternating optimization** algorithm from our NIPS 2015 paper  use `experiments/scripts/faster_rcnn_alt_opt.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  (""alt opt"" refers to the alternating optimization training algorithm described in the NIPS paper.)  To train and test a Faster R-CNN detector using the **approximate joint training** method  use `experiments/scripts/faster_rcnn_end2end.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  This method trains the RPN module jointly with the Fast R-CNN network  rather than alternating between training the two. It results in faster (~ 1.5x speedup) training times and similar detection accuracy. See these [slides](https://www.dropbox.com/s/xtr4yd4i5e0vw8g/iccv15_tutorial_training_rbg.pdf?dl=0) for more details.  Artifacts generated by the scripts in `tools` are written in this directory.  Trained Fast R-CNN networks are saved under:  ``` output/<experiment directory>/<dataset name>/ ```  Test outputs are saved under:  ``` output/<experiment directory>/<dataset name>/<network snapshot name>/ ```  """;Computer Vision;https://github.com/nikhithakarennagari/1311
"""Next  download and install the MPII Human Pose dataset:   $ ./run.sh python   $ ./run.sh pytest      bash      bash   """;General;https://github.com/anibali/dsnt-pose2d
"""Next  download and install the MPII Human Pose dataset:   $ ./run.sh python   $ ./run.sh pytest      bash      bash   """;Computer Vision;https://github.com/anibali/dsnt-pose2d
"""- LJSpeechÔºöhttps://keithito.com/LJ-Speech-Dataset/   """;General;https://github.com/mitsu-h/deepvoice3
"""- LJSpeechÔºöhttps://keithito.com/LJ-Speech-Dataset/   """;Sequential;https://github.com/mitsu-h/deepvoice3
"""""";General;https://github.com/Aditya-kiran/ResNet-VAE
"""This is a PyTorch implementation of the __L__ ow Rank F __a__ ctorization for Compact __M__ ulti-Head __A__ ttention (LAMA) mechanism and the corresponding pooler introduced in the paper: ""[Low Rank Factorization for Compact Multi-Head Self-Attention](https://arxiv.org/abs/1912.00835)"".  ![](img/figure_1.jpg)  > Figure 1 from [Low Rank Factorization for Compact Multi-Head Self-Attention](https://arxiv.org/abs/1912.00835).  Note: I am _not_ one of the authors on the paper.   from modules.lama import LAMA   from modules.lama_encoder import LAMAEncoder   The only dependency is PyTorch. Installation instructions can be found [here](https://pytorch.org/get-started/locally/).   """;General;https://github.com/JohnGiorgi/compact-multi-head-self-attention-pytorch
"""""";General;https://github.com/heiner/scalable_agent
"""""";Reinforcement Learning;https://github.com/heiner/scalable_agent
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`  2. Download VOC2007 and VOC2012 dataset. By default  we assume the data is stored in `$HOME/data/`   ```Shell   #: Download the data.   cd $HOME/data   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar   #: Extract the data.   tar -xvf VOCtrainval_11-May-2012.tar   tar -xvf VOCtrainval_06-Nov-2007.tar   tar -xvf VOCtest_06-Nov-2007.tar   ```  3. Create the LMDB file.   ```Shell   cd $CAFFE_ROOT   #: Create the trainval.txt  test.txt  and test_name_size.txt in data/VOC0712/   ./data/VOC0712/create_list.sh   #: You can modify the parameters in create_data.sh if needed.   #: It will create lmdb files for trainval and test with encoded original image:   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb   #: and make soft links at examples/VOC0712/   ./data/VOC0712/create_data.sh   ```   1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`   ```Shell   git clone https://github.com/weiliu89/caffe.git   cd caffe   git checkout ssd   ```  2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.   ```Shell   #: Modify Makefile.config according to your Caffe installation.   cp Makefile.config.example Makefile.config   make -j8   #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.   make py   make test -j8   #: (Optional)   make runtest -j8   ```   COCO<sup>[1]</sup>: SSD300*  SSD512*  07+12+COCO: SSD300*  SSD512*  07++12+COCO: SSD300*  SSD512*  COCO models:   """;Computer Vision;https://github.com/sunqiangxtcsun/SSD
"""Convolutional neural network is a key architure to recognize and classify the image to classes. However  variation of convolutional neural networks are very many and very vary their space/time usage for learning.   In this project  I compared the state-of-art convolutional neural networks and compared them based on **Performance** and **Resources**(space/time). Through this project  it helps to decide the model to solve problem based on problem size and limitation of resources.    --------   """;General;https://github.com/wonjaek36/sodeep_final
"""Deep Q-Learning Network (DQN) [2] is one of the most popular deep reinforcement learning algorithms. It is an off-policy learning algorithm that is highly sample efficient. Over the years  many improvements were proposed to improve the performance of DQN. Of the many extensions available for the DQN algorithm  some popular enhancements were combined by the DeepMind team and presented as the Rainbow DQN algorithm. These imporvements were found to be mostly orthogonal  with each component contributing to various degrees.  The six add-ons to the base DQN algorithm in the Rainbow version are      1. Double Q-Learning      2. Prioritized Experience Replay      3. Dueling Networks      4. Multi-step (n-step) Learning      5. Distributional Value Learning      6. Noisy Networks   This repository contains an implementation of the rainbow DQN algorithm put forward by the DeepMind team in the paper '[Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)'. [1]   <p align=""center"" >   <img width=""160"" height=""210"" src=""media/pong.gif"">   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   <img width=""315"" height=""210"" src=""media/cartpole.gif""> </p>    git clone https://github.com/roboticist-by-day/rainbow_dqn.git  cd rainbow_dqn  To create the python environment using Anaconda package manager   conda create --name &lt;env_name&gt; --file requirements.txt  conda activate &lt;env_name&gt;   Note: The 'Pong-v0' environment requires significant memory to store the replay buffer.   For those interested in studying the contributions of various rainbow components  the code supports functionlity to perform ablation studies. However  some combinations might not have the helper functions pre-defined. To disable improvements  modify the arguments passed to the rainbow class as necessary and pass the flags that disable the respective components.   Training data is written to tensorbaord. Currently  training can be restored by providing the model state dict. The code can also be easily extended to restore the optimizer and other epoch dependent data.   """;Reinforcement Learning;https://github.com/mohith-sakthivel/rainbow_dqn
"""This repository contains code to train and evaluate 3D Convolutional Neural Networks for semantic segmentation on medical images. The architectures developed in this framework are a combination of auto-encoder [UNet](https://arxiv.org/abs/1505.04597) with shortcut connections as in [ResNet](https://arxiv.org/abs/1512.03385)  densely connections for deep supervision as in [DensetNet](https://arxiv.org/abs/1608.06993) and Merge-And-Run mapping for attention focusing as in [MRGE](https://arxiv.org/abs/1611.07718).   Clone the repository and install the requirements ```shell $ git clone https://gitlab.com/iss_mia/cnn_segmentation/ desired_directory $ python3 -m pip install -r requirements.txt ```   Set all parameters in the [configuration file](./config/config_default.yaml). Check call arguments: ```shell $ python3 main.py -h  ```   """;General;https://github.com/lab-midas/med_segmentation
"""This repository contains code to train and evaluate 3D Convolutional Neural Networks for semantic segmentation on medical images. The architectures developed in this framework are a combination of auto-encoder [UNet](https://arxiv.org/abs/1505.04597) with shortcut connections as in [ResNet](https://arxiv.org/abs/1512.03385)  densely connections for deep supervision as in [DensetNet](https://arxiv.org/abs/1608.06993) and Merge-And-Run mapping for attention focusing as in [MRGE](https://arxiv.org/abs/1611.07718).   Clone the repository and install the requirements ```shell $ git clone https://gitlab.com/iss_mia/cnn_segmentation/ desired_directory $ python3 -m pip install -r requirements.txt ```   Set all parameters in the [configuration file](./config/config_default.yaml). Check call arguments: ```shell $ python3 main.py -h  ```   """;Computer Vision;https://github.com/lab-midas/med_segmentation
"""``` python3 -m venv venv source venv/bin/activate pip3 install -r requirements.txt ```  Tested with Python 3.8.5.   The experiment has been performed using hardware with the following features:   * GPU: NVIDIA GeForce RTX 3090 (shared with other processes)   To start a speedy training process  you can run the following command:   """;Graphs;https://github.com/giuseppefutia/link-prediction-code
"""- The Encoder weights are saved after each train using the `train.py` file. - We train a classifier with one hidden layer on top of the features spaces of the freezed Encoder in `classifier.py` file. - The classifier reach 85% accuracy on Mnist.    """;General;https://github.com/Medabid1/CPC
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/spacegoing/stylegan2
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;Computer Vision;https://github.com/spacegoing/stylegan2
"""‚îú‚îÄ‚îÄ requirements.txt                                // Ê®°Âûã‰æùËµñ   """;Computer Vision;https://github.com/Tu-kun/cultivated_landdivision
"""""";General;https://github.com/DanieleVeri/deep_comedy
"""""";Natural Language Processing;https://github.com/DanieleVeri/deep_comedy
"""""";General;https://github.com/wandb/awesome-dl-projects
"""* Install the python libraries ```pip install -r requirements.txt``` (This file contains the GPU libs for tensorflow and tensorflow_graphics  remove '-gpu' to use the cpu versions)   * Get the [H3.6M Dataset](http://vision.imar.ro/human3.6m/description.php) * The CLI is located in ```main.py```  it consists of two subprograms ```train``` and ```eval``` for training and evaluation of models  respectively. * Calling ```python main.py --help``` prints an overview of the CLI arguments  To train a model  call  ``` python main.py train ``` This will train a model with the default configuration (s. ```configs.py```) To evaluate a model  call  ``` python main.py eval --checkpoint <path_to_checkpoint> ``` This will run the default evaluation on a model with the default configuration (s. ```configs.py```)  restored from the checkpoint thats passed in ```path_to_checkpoint``` A checkpoint to run the model with default configuration is located in the [```ckpts```](https://github.com/LucaHermes/lightweight-motion-forecasting/tree/main/ckpts/epoch_3000_joint_level_enc_forecasting/20210516-032909) folder Alternatively  you can alter the defaults by passing additional cli arguments or directly modify the ```configs.py``` file.   """;Audio;https://github.com/LucaHermes/lightweight-motion-forecasting
"""* Install the python libraries ```pip install -r requirements.txt``` (This file contains the GPU libs for tensorflow and tensorflow_graphics  remove '-gpu' to use the cpu versions)   * Get the [H3.6M Dataset](http://vision.imar.ro/human3.6m/description.php) * The CLI is located in ```main.py```  it consists of two subprograms ```train``` and ```eval``` for training and evaluation of models  respectively. * Calling ```python main.py --help``` prints an overview of the CLI arguments  To train a model  call  ``` python main.py train ``` This will train a model with the default configuration (s. ```configs.py```) To evaluate a model  call  ``` python main.py eval --checkpoint <path_to_checkpoint> ``` This will run the default evaluation on a model with the default configuration (s. ```configs.py```)  restored from the checkpoint thats passed in ```path_to_checkpoint``` A checkpoint to run the model with default configuration is located in the [```ckpts```](https://github.com/LucaHermes/lightweight-motion-forecasting/tree/main/ckpts/epoch_3000_joint_level_enc_forecasting/20210516-032909) folder Alternatively  you can alter the defaults by passing additional cli arguments or directly modify the ```configs.py``` file.   """;Sequential;https://github.com/LucaHermes/lightweight-motion-forecasting
"""""";General;https://github.com/ayulockin/SwAV-TF
"""- [Installation](https://github.com/Cadene/pretrained-models.pytorch#installation) - [Quick examples](https://github.com/Cadene/pretrained-models.pytorch#quick-examples) - [Few use cases](https://github.com/Cadene/pretrained-models.pytorch#few-use-cases)     - [Compute imagenet logits](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-logits)     - [Compute imagenet validation metrics](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-validation-metrics) - [Evaluation on ImageNet](https://github.com/Cadene/pretrained-models.pytorch#evaluation-on-imagenet)     - [Accuracy on valset](https://github.com/Cadene/pretrained-models.pytorch#accuracy-on-validation-set)     - [Reproducing results](https://github.com/Cadene/pretrained-models.pytorch#reproducing-results) - [Documentation](https://github.com/Cadene/pretrained-models.pytorch#documentation)     - [Available models](https://github.com/Cadene/pretrained-models.pytorch#available-models)         - [AlexNet](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [BNInception](https://github.com/Cadene/pretrained-models.pytorch#bninception)         - [CaffeResNet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet)         - [DenseNet121](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet161](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet169](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DualPathNet68](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet92](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet98](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet107](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet113](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [FBResNet152](https://github.com/Cadene/pretrained-models.pytorch#facebook-resnet)         - [InceptionResNetV2](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [InceptionV3](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [InceptionV4](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [NASNet-A-Large](https://github.com/Cadene/pretrained-models.pytorch#nasnet)         - [NASNet-A-Mobile](https://github.com/Cadene/pretrained-models.pytorch#nasnet)         - [PNASNet-5-Large](https://github.com/Cadene/pretrained-models.pytorch#pnasnet)         - [PolyNet](https://github.com/Cadene/pretrained-models.pytorch#polynet)         - [ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)         - [ResNeXt101_64x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)         - [ResNet101](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet152](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet18](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet34](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet50](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [SENet154](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet50](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet101](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet152](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNeXt50_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SqueezeNet1_0](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [SqueezeNet1_1](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG11](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG13](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG16](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG19](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG11_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG13_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG16_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG19_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [Xception](https://github.com/Cadene/pretrained-models.pytorch#xception)     - [Model API](https://github.com/Cadene/pretrained-models.pytorch#model-api)         - [model.input_size](https://github.com/Cadene/pretrained-models.pytorch#modelinput_size)         - [model.input_space](https://github.com/Cadene/pretrained-models.pytorch#modelinput_space)         - [model.input_range](https://github.com/Cadene/pretrained-models.pytorch#modelinput_range)         - [model.mean](https://github.com/Cadene/pretrained-models.pytorch#modelmean)         - [model.std](https://github.com/Cadene/pretrained-models.pytorch#modelstd)         - [model.features](https://github.com/Cadene/pretrained-models.pytorch#modelfeatures)         - [model.logits](https://github.com/Cadene/pretrained-models.pytorch#modellogits)         - [model.forward](https://github.com/Cadene/pretrained-models.pytorch#modelforward) - [Reproducing porting](https://github.com/Cadene/pretrained-models.pytorch#reproducing)     - [ResNet*](https://github.com/Cadene/pretrained-models.pytorch#hand-porting-of-resnet152)     - [ResNeXt*](https://github.com/Cadene/pretrained-models.pytorch#automatic-porting-of-resnext)     - [Inception*](https://github.com/Cadene/pretrained-models.pytorch#hand-porting-of-inceptionv4-and-inceptionresnetv2)   Results were obtained using (center cropped) images of the same size than during the training process.  Model | Version | Acc@1 | Acc@5 --- | --- | --- | --- PNASNet-5-Large | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 82.858 | 96.182 [PNASNet-5-Large](https://github.com/Cadene/pretrained-models.pytorch#pnasnet) | Our porting | 82.736 | 95.992 NASNet-A-Large | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 82.693 | 96.163 [NASNet-A-Large](https://github.com/Cadene/pretrained-models.pytorch#nasnet) | Our porting | 82.566 | 96.086 SENet154 | [Caffe](https://github.com/hujie-frank/SENet) | 81.32 | 95.53 [SENet154](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 81.304 | 95.498 PolyNet | [Caffe](https://github.com/CUHK-MMLAB/polynet) | 81.29 | 95.75 [PolyNet](https://github.com/Cadene/pretrained-models.pytorch#polynet) | Our porting | 81.002 | 95.624 InceptionResNetV2 | [Tensorflow](https://github.com/tensorflow/models/tree/master/slim) | 80.4 | 95.3 InceptionV4 | [Tensorflow](https://github.com/tensorflow/models/tree/master/slim) | 80.2 | 95.3 [SE-ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 80.236 | 95.028 SE-ResNeXt101_32x4d | [Caffe](https://github.com/hujie-frank/SENet) | 80.19 | 95.04 [InceptionResNetV2](https://github.com/Cadene/pretrained-models.pytorch#inception) | Our porting | 80.170 | 95.234 [InceptionV4](https://github.com/Cadene/pretrained-models.pytorch#inception) | Our porting | 80.062 | 94.926 [DualPathNet107_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.746 | 94.684 ResNeXt101_64x4d | [Torch7](https://github.com/facebookresearch/ResNeXt) | 79.6 | 94.7 [DualPathNet131](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.432 | 94.574 [DualPathNet92_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.400 | 94.620 [DualPathNet98](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.224 | 94.488 [SE-ResNeXt50_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 79.076 | 94.434 SE-ResNeXt50_32x4d | [Caffe](https://github.com/hujie-frank/SENet) | 79.03 | 94.46 [Xception](https://github.com/Cadene/pretrained-models.pytorch#xception) | [Keras](https://github.com/keras-team/keras/blob/master/keras/applications/xception.py) | 79.000 | 94.500 [ResNeXt101_64x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext) | Our porting | 78.956 | 94.252 [Xception](https://github.com/Cadene/pretrained-models.pytorch#xception) | Our porting | 78.888 | 94.292 ResNeXt101_32x4d | [Torch7](https://github.com/facebookresearch/ResNeXt) | 78.8 | 94.4 SE-ResNet152 | [Caffe](https://github.com/hujie-frank/SENet) | 78.66 | 94.46 [SE-ResNet152](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 78.658 | 94.374 ResNet152 | [Pytorch](https://github.com/pytorch/vision#models) | 78.428 | 94.110 [SE-ResNet101](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 78.396 | 94.258 SE-ResNet101 | [Caffe](https://github.com/hujie-frank/SENet) | 78.25 | 94.28 [ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext) | Our porting | 78.188 | 93.886 FBResNet152 | [Torch7](https://github.com/facebook/fb.resnet.torch) | 77.84 | 93.84 SE-ResNet50 | [Caffe](https://github.com/hujie-frank/SENet) | 77.63 | 93.64 [SE-ResNet50](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 77.636 | 93.752 [DenseNet161](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.560 | 93.798 [ResNet101](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.438 | 93.672 [FBResNet152](https://github.com/Cadene/pretrained-models.pytorch#facebook-resnet) | Our porting | 77.386 | 93.594 [InceptionV3](https://github.com/Cadene/pretrained-models.pytorch#inception) | [Pytorch](https://github.com/pytorch/vision#models) | 77.294 | 93.454 [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.152 | 93.548 [DualPathNet68b_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 77.034 | 93.590 [CaffeResnet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet) | [Caffe](https://github.com/KaimingHe/deep-residual-networks) | 76.400 | 92.900 [CaffeResnet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet) | Our porting | 76.200 | 92.766 [DenseNet169](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 76.026 | 92.992 [ResNet50](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 76.002 | 92.980 [DualPathNet68](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 75.868 | 92.774 [DenseNet121](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 74.646 | 92.136 [VGG19_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 74.266 | 92.066 NASNet-A-Mobile | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 74.0 | 91.6 [NASNet-A-Mobile](https://github.com/veronikayurchuk/pretrained-models.pytorch/blob/master/pretrainedmodels/models/nasnet_mobile.py) | Our porting | 74.080 | 91.740 [ResNet34](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 73.554 | 91.456 [BNInception](https://github.com/Cadene/pretrained-models.pytorch#bninception) | Our porting | 73.524 | 91.562 [VGG16_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 73.518 | 91.608 [VGG19](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 72.080 | 90.822 [VGG16](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 71.636 | 90.354 [VGG13_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 71.508 | 90.494 [VGG11_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 70.452 | 89.818 [ResNet18](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 70.142 | 89.274 [VGG13](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 69.662 | 89.264 [VGG11](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 68.970 | 88.746 [SqueezeNet1_1](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 58.250 | 80.800 [SqueezeNet1_0](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 58.108 | 80.428 [Alexnet](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 56.432 | 79.194  Notes: - the Pytorch version of ResNet152 is not a porting of the Torch7 but has been retrained by facebook. - For the PolyNet evaluation each image was resized to 378x378 without preserving the aspect ratio and then the central 331√ó331 patch from the resulting image was used.  Beware  the accuracy reported here is not always representative of the transferable capacity of the network on other tasks and datasets. You must try them all! :P       3. `git clone https://github.com/Cadene/pretrained-models.pytorch.git` 4. `cd pretrained-models.pytorch` 5. `python setup.py install`    3. `pip install pretrainedmodels`   1. [python3 with anaconda](https://www.continuum.io/downloads) 2. [pytorch with/out CUDA](http://pytorch.org)   <a href=""https://travis-ci.org/Cadene/pretrained-models.pytorch""><img src=""https://api.travis-ci.org/Cadene/pretrained-models.pytorch.svg?branch=master""/></a>   - 13/01/2018: pip install pretrainedmodels  pretrainedmodels.model_names  pretrainedmodels.pretrained_settings   Source: TensorFlow Slim repo   Source: TensorFlow Slim repo and Pytorch/Vision repo for inceptionv3   Source: MXNET repo of Chen Yunpeng  The porting has been made possible by Ross Wightman in his PyTorch repo.   Source: Keras repo   Source: TensorFlow Slim repo   Source: Pytorch/Vision repo   - To import `pretrainedmodels`:  ```python import pretrainedmodels ```  - To print the available pretrained models:  ```python print(pretrainedmodels.model_names) > ['fbresnet152'  'bninception'  'resnext101_32x4d'  'resnext101_64x4d'  'inceptionv4'  'inceptionresnetv2'  'alexnet'  'densenet121'  'densenet169'  'densenet201'  'densenet161'  'resnet18'  'resnet34'  'resnet50'  'resnet101'  'resnet152'  'inceptionv3'  'squeezenet1_0'  'squeezenet1_1'  'vgg11'  'vgg11_bn'  'vgg13'  'vgg13_bn'  'vgg16'  'vgg16_bn'  'vgg19_bn'  'vgg19'  'nasnetalarge'  'nasnetamobile'  'cafferesnet101'  'senet154'   'se_resnet50'  'se_resnet101'  'se_resnet152'  'se_resnext50_32x4d'  'se_resnext101_32x4d'  'cafferesnet101'  'polynet'  'pnasnet5large'] ```  - To print the available pretrained settings for a chosen model:  ```python print(pretrainedmodels.pretrained_settings['nasnetalarge']) > {'imagenet': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth'  'input_space': 'RGB'  'input_size': [3  331  331]  'input_range': [0  1]  'mean': [0.5  0.5  0.5]  'std': [0.5  0.5  0.5]  'num_classes': 1000}  'imagenet+background': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth'  'input_space': 'RGB'  'input_size': [3  331  331]  'input_range': [0  1]  'mean': [0.5  0.5  0.5]  'std': [0.5  0.5  0.5]  'num_classes': 1001}} ```  - To load a pretrained models from imagenet:  ```python model_name = 'nasnetalarge' #: could be fbresnet152 or inceptionresnetv2 model = pretrainedmodels.__dict__[model_name](num_classes=1000  pretrained='imagenet') model.eval() ```  **Note**: By default  models will be downloaded to your `$HOME/.torch` folder. You can modify this behavior using the `$TORCH_HOME` variable as follow: `export TORCH_HOME=""/local/pretrainedmodels""`  - To load an image and do a complete forward pass:  ```python import torch import pretrainedmodels.utils as utils  load_img = utils.LoadImage()  #: transformations depending on the model #:¬†rescale  center crop  normalize  and others (ex: ToBGR  ToRange255) tf_img = utils.TransformImage(model)   path_img = 'data/cat.jpg'  input_img = load_img(path_img) input_tensor = tf_img(input_img)         #: 3x400x225 -> 3x299x299 size may differ input_tensor = input_tensor.unsqueeze(0) #: 3x299x299 -> 1x3x299x299 input = torch.autograd.Variable(input_tensor      requires_grad=False)  output_logits = model(input) #: 1x1000 ```  - To extract features (beware this API is not available for all networks):  ```python output_features = model.features(input) #: 1x14x14x2048 size may differ output_logits = model.logits(output_features) #: 1x1000 ```   """;Computer Vision;https://github.com/Cadene/pretrained-models.pytorch
"""- [Installation](https://github.com/Cadene/pretrained-models.pytorch#installation) - [Quick examples](https://github.com/Cadene/pretrained-models.pytorch#quick-examples) - [Few use cases](https://github.com/Cadene/pretrained-models.pytorch#few-use-cases)     - [Compute imagenet logits](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-logits)     - [Compute imagenet validation metrics](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-validation-metrics) - [Evaluation on ImageNet](https://github.com/Cadene/pretrained-models.pytorch#evaluation-on-imagenet)     - [Accuracy on valset](https://github.com/Cadene/pretrained-models.pytorch#accuracy-on-validation-set)     - [Reproducing results](https://github.com/Cadene/pretrained-models.pytorch#reproducing-results) - [Documentation](https://github.com/Cadene/pretrained-models.pytorch#documentation)     - [Available models](https://github.com/Cadene/pretrained-models.pytorch#available-models)         - [AlexNet](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [BNInception](https://github.com/Cadene/pretrained-models.pytorch#bninception)         - [CaffeResNet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet)         - [DenseNet121](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet161](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet169](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [DualPathNet68](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet92](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet98](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet107](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [DualPathNet113](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)         - [FBResNet152](https://github.com/Cadene/pretrained-models.pytorch#facebook-resnet)         - [InceptionResNetV2](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [InceptionV3](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [InceptionV4](https://github.com/Cadene/pretrained-models.pytorch#inception)         - [NASNet-A-Large](https://github.com/Cadene/pretrained-models.pytorch#nasnet)         - [NASNet-A-Mobile](https://github.com/Cadene/pretrained-models.pytorch#nasnet)         - [PNASNet-5-Large](https://github.com/Cadene/pretrained-models.pytorch#pnasnet)         - [PolyNet](https://github.com/Cadene/pretrained-models.pytorch#polynet)         - [ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)         - [ResNeXt101_64x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)         - [ResNet101](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet152](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet18](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet34](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [ResNet50](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [SENet154](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet50](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet101](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNet152](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNeXt50_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SE-ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)         - [SqueezeNet1_0](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [SqueezeNet1_1](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG11](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG13](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG16](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG19](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG11_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG13_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG16_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [VGG19_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)         - [Xception](https://github.com/Cadene/pretrained-models.pytorch#xception)     - [Model API](https://github.com/Cadene/pretrained-models.pytorch#model-api)         - [model.input_size](https://github.com/Cadene/pretrained-models.pytorch#modelinput_size)         - [model.input_space](https://github.com/Cadene/pretrained-models.pytorch#modelinput_space)         - [model.input_range](https://github.com/Cadene/pretrained-models.pytorch#modelinput_range)         - [model.mean](https://github.com/Cadene/pretrained-models.pytorch#modelmean)         - [model.std](https://github.com/Cadene/pretrained-models.pytorch#modelstd)         - [model.features](https://github.com/Cadene/pretrained-models.pytorch#modelfeatures)         - [model.logits](https://github.com/Cadene/pretrained-models.pytorch#modellogits)         - [model.forward](https://github.com/Cadene/pretrained-models.pytorch#modelforward) - [Reproducing porting](https://github.com/Cadene/pretrained-models.pytorch#reproducing)     - [ResNet*](https://github.com/Cadene/pretrained-models.pytorch#hand-porting-of-resnet152)     - [ResNeXt*](https://github.com/Cadene/pretrained-models.pytorch#automatic-porting-of-resnext)     - [Inception*](https://github.com/Cadene/pretrained-models.pytorch#hand-porting-of-inceptionv4-and-inceptionresnetv2)   Results were obtained using (center cropped) images of the same size than during the training process.  Model | Version | Acc@1 | Acc@5 --- | --- | --- | --- PNASNet-5-Large | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 82.858 | 96.182 [PNASNet-5-Large](https://github.com/Cadene/pretrained-models.pytorch#pnasnet) | Our porting | 82.736 | 95.992 NASNet-A-Large | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 82.693 | 96.163 [NASNet-A-Large](https://github.com/Cadene/pretrained-models.pytorch#nasnet) | Our porting | 82.566 | 96.086 SENet154 | [Caffe](https://github.com/hujie-frank/SENet) | 81.32 | 95.53 [SENet154](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 81.304 | 95.498 PolyNet | [Caffe](https://github.com/CUHK-MMLAB/polynet) | 81.29 | 95.75 [PolyNet](https://github.com/Cadene/pretrained-models.pytorch#polynet) | Our porting | 81.002 | 95.624 InceptionResNetV2 | [Tensorflow](https://github.com/tensorflow/models/tree/master/slim) | 80.4 | 95.3 InceptionV4 | [Tensorflow](https://github.com/tensorflow/models/tree/master/slim) | 80.2 | 95.3 [SE-ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 80.236 | 95.028 SE-ResNeXt101_32x4d | [Caffe](https://github.com/hujie-frank/SENet) | 80.19 | 95.04 [InceptionResNetV2](https://github.com/Cadene/pretrained-models.pytorch#inception) | Our porting | 80.170 | 95.234 [InceptionV4](https://github.com/Cadene/pretrained-models.pytorch#inception) | Our porting | 80.062 | 94.926 [DualPathNet107_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.746 | 94.684 ResNeXt101_64x4d | [Torch7](https://github.com/facebookresearch/ResNeXt) | 79.6 | 94.7 [DualPathNet131](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.432 | 94.574 [DualPathNet92_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.400 | 94.620 [DualPathNet98](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.224 | 94.488 [SE-ResNeXt50_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 79.076 | 94.434 SE-ResNeXt50_32x4d | [Caffe](https://github.com/hujie-frank/SENet) | 79.03 | 94.46 [Xception](https://github.com/Cadene/pretrained-models.pytorch#xception) | [Keras](https://github.com/keras-team/keras/blob/master/keras/applications/xception.py) | 79.000 | 94.500 [ResNeXt101_64x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext) | Our porting | 78.956 | 94.252 [Xception](https://github.com/Cadene/pretrained-models.pytorch#xception) | Our porting | 78.888 | 94.292 ResNeXt101_32x4d | [Torch7](https://github.com/facebookresearch/ResNeXt) | 78.8 | 94.4 SE-ResNet152 | [Caffe](https://github.com/hujie-frank/SENet) | 78.66 | 94.46 [SE-ResNet152](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 78.658 | 94.374 ResNet152 | [Pytorch](https://github.com/pytorch/vision#models) | 78.428 | 94.110 [SE-ResNet101](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 78.396 | 94.258 SE-ResNet101 | [Caffe](https://github.com/hujie-frank/SENet) | 78.25 | 94.28 [ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext) | Our porting | 78.188 | 93.886 FBResNet152 | [Torch7](https://github.com/facebook/fb.resnet.torch) | 77.84 | 93.84 SE-ResNet50 | [Caffe](https://github.com/hujie-frank/SENet) | 77.63 | 93.64 [SE-ResNet50](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 77.636 | 93.752 [DenseNet161](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.560 | 93.798 [ResNet101](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.438 | 93.672 [FBResNet152](https://github.com/Cadene/pretrained-models.pytorch#facebook-resnet) | Our porting | 77.386 | 93.594 [InceptionV3](https://github.com/Cadene/pretrained-models.pytorch#inception) | [Pytorch](https://github.com/pytorch/vision#models) | 77.294 | 93.454 [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.152 | 93.548 [DualPathNet68b_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 77.034 | 93.590 [CaffeResnet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet) | [Caffe](https://github.com/KaimingHe/deep-residual-networks) | 76.400 | 92.900 [CaffeResnet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet) | Our porting | 76.200 | 92.766 [DenseNet169](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 76.026 | 92.992 [ResNet50](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 76.002 | 92.980 [DualPathNet68](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 75.868 | 92.774 [DenseNet121](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 74.646 | 92.136 [VGG19_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 74.266 | 92.066 NASNet-A-Mobile | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 74.0 | 91.6 [NASNet-A-Mobile](https://github.com/veronikayurchuk/pretrained-models.pytorch/blob/master/pretrainedmodels/models/nasnet_mobile.py) | Our porting | 74.080 | 91.740 [ResNet34](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 73.554 | 91.456 [BNInception](https://github.com/Cadene/pretrained-models.pytorch#bninception) | Our porting | 73.524 | 91.562 [VGG16_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 73.518 | 91.608 [VGG19](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 72.080 | 90.822 [VGG16](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 71.636 | 90.354 [VGG13_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 71.508 | 90.494 [VGG11_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 70.452 | 89.818 [ResNet18](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 70.142 | 89.274 [VGG13](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 69.662 | 89.264 [VGG11](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 68.970 | 88.746 [SqueezeNet1_1](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 58.250 | 80.800 [SqueezeNet1_0](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 58.108 | 80.428 [Alexnet](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 56.432 | 79.194  Notes: - the Pytorch version of ResNet152 is not a porting of the Torch7 but has been retrained by facebook. - For the PolyNet evaluation each image was resized to 378x378 without preserving the aspect ratio and then the central 331√ó331 patch from the resulting image was used.  Beware  the accuracy reported here is not always representative of the transferable capacity of the network on other tasks and datasets. You must try them all! :P       3. `git clone https://github.com/Cadene/pretrained-models.pytorch.git` 4. `cd pretrained-models.pytorch` 5. `python setup.py install`    3. `pip install pretrainedmodels`   1. [python3 with anaconda](https://www.continuum.io/downloads) 2. [pytorch with/out CUDA](http://pytorch.org)   <a href=""https://travis-ci.org/Cadene/pretrained-models.pytorch""><img src=""https://api.travis-ci.org/Cadene/pretrained-models.pytorch.svg?branch=master""/></a>   - 13/01/2018: pip install pretrainedmodels  pretrainedmodels.model_names  pretrainedmodels.pretrained_settings   Source: TensorFlow Slim repo   Source: TensorFlow Slim repo and Pytorch/Vision repo for inceptionv3   Source: MXNET repo of Chen Yunpeng  The porting has been made possible by Ross Wightman in his PyTorch repo.   Source: Keras repo   Source: TensorFlow Slim repo   Source: Pytorch/Vision repo   - To import `pretrainedmodels`:  ```python import pretrainedmodels ```  - To print the available pretrained models:  ```python print(pretrainedmodels.model_names) > ['fbresnet152'  'bninception'  'resnext101_32x4d'  'resnext101_64x4d'  'inceptionv4'  'inceptionresnetv2'  'alexnet'  'densenet121'  'densenet169'  'densenet201'  'densenet161'  'resnet18'  'resnet34'  'resnet50'  'resnet101'  'resnet152'  'inceptionv3'  'squeezenet1_0'  'squeezenet1_1'  'vgg11'  'vgg11_bn'  'vgg13'  'vgg13_bn'  'vgg16'  'vgg16_bn'  'vgg19_bn'  'vgg19'  'nasnetalarge'  'nasnetamobile'  'cafferesnet101'  'senet154'   'se_resnet50'  'se_resnet101'  'se_resnet152'  'se_resnext50_32x4d'  'se_resnext101_32x4d'  'cafferesnet101'  'polynet'  'pnasnet5large'] ```  - To print the available pretrained settings for a chosen model:  ```python print(pretrainedmodels.pretrained_settings['nasnetalarge']) > {'imagenet': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth'  'input_space': 'RGB'  'input_size': [3  331  331]  'input_range': [0  1]  'mean': [0.5  0.5  0.5]  'std': [0.5  0.5  0.5]  'num_classes': 1000}  'imagenet+background': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth'  'input_space': 'RGB'  'input_size': [3  331  331]  'input_range': [0  1]  'mean': [0.5  0.5  0.5]  'std': [0.5  0.5  0.5]  'num_classes': 1001}} ```  - To load a pretrained models from imagenet:  ```python model_name = 'nasnetalarge' #: could be fbresnet152 or inceptionresnetv2 model = pretrainedmodels.__dict__[model_name](num_classes=1000  pretrained='imagenet') model.eval() ```  **Note**: By default  models will be downloaded to your `$HOME/.torch` folder. You can modify this behavior using the `$TORCH_HOME` variable as follow: `export TORCH_HOME=""/local/pretrainedmodels""`  - To load an image and do a complete forward pass:  ```python import torch import pretrainedmodels.utils as utils  load_img = utils.LoadImage()  #: transformations depending on the model #:¬†rescale  center crop  normalize  and others (ex: ToBGR  ToRange255) tf_img = utils.TransformImage(model)   path_img = 'data/cat.jpg'  input_img = load_img(path_img) input_tensor = tf_img(input_img)         #: 3x400x225 -> 3x299x299 size may differ input_tensor = input_tensor.unsqueeze(0) #: 3x299x299 -> 1x3x299x299 input = torch.autograd.Variable(input_tensor      requires_grad=False)  output_logits = model(input) #: 1x1000 ```  - To extract features (beware this API is not available for all networks):  ```python output_features = model.features(input) #: 1x14x14x2048 size may differ output_logits = model.logits(output_features) #: 1x1000 ```   """;General;https://github.com/Cadene/pretrained-models.pytorch
"""""";General;https://github.com/leona-ha/Skin-Screening_Web-App
"""""";Computer Vision;https://github.com/leona-ha/Skin-Screening_Web-App
"""We implement both the Nueral ODE and ResNet architectures for classification of electrocardiogram signals. The data is taken from the frequently used MIT-BIH ECG database  which contains over 100 000 labeled samples of ECG signals from a single heartbeat. The data and a description of the sampling procedure used can be found at https://www.physionet.org/content/mitdb/1.0.0/. We briefly visualize the data  which enables us an intuitive look at the various features and differences between each class that our neural networks will learn and distinguish.   We construct both network architectures with the help of PyTorch and the torchdiffeq library found in the original Neural ODE paper: https://arxiv.org/abs/1806.07366. The ResNet and ODENet  the implementation of the Neural ODE  are designed to be as similar as possible  so we can compare the two fairly. Both models contain identical downampling layers  1D convolutions  normalizations (Group)  activations (ReLU)  and output layers. We train both models and evaluate them on the testing set while also noting differences in speed  memory  and accuracy. Overall  both models perform comparably on the testing data. However  there is a tradeoff between speed and memory. The ResNet is faster to train while the ODENet contains fewer tunable parameters (less memory).   """;General;https://github.com/abaietto/neural_ode_classification
"""We implement both the Nueral ODE and ResNet architectures for classification of electrocardiogram signals. The data is taken from the frequently used MIT-BIH ECG database  which contains over 100 000 labeled samples of ECG signals from a single heartbeat. The data and a description of the sampling procedure used can be found at https://www.physionet.org/content/mitdb/1.0.0/. We briefly visualize the data  which enables us an intuitive look at the various features and differences between each class that our neural networks will learn and distinguish.   We construct both network architectures with the help of PyTorch and the torchdiffeq library found in the original Neural ODE paper: https://arxiv.org/abs/1806.07366. The ResNet and ODENet  the implementation of the Neural ODE  are designed to be as similar as possible  so we can compare the two fairly. Both models contain identical downampling layers  1D convolutions  normalizations (Group)  activations (ReLU)  and output layers. We train both models and evaluate them on the testing set while also noting differences in speed  memory  and accuracy. Overall  both models perform comparably on the testing data. However  there is a tradeoff between speed and memory. The ResNet is faster to train while the ODENet contains fewer tunable parameters (less memory).   """;Computer Vision;https://github.com/abaietto/neural_ode_classification
"""""";Graphs;https://github.com/wyd1502/DGNN
"""pytorch contains the various GNN models implemented in PyTorch:   """;Graphs;https://github.com/lukecavabarrett/pna
"""""";General;https://github.com/joe-siyuan-qiao/WeightStandardization
"""You can use the models for sampling by entering   First download your data and put it into the `./data` folder.  To train a new model  first create a config script similar to the ones provided in the `./configs` folder.  You can then train you model using ``` python train.py PATH_TO_CONFIG ```  To compute the inception score for your model and generate samples  use ``` python test.py PATH_TO_CONFIG ```  Finally  you can create nice latent space interpolations using ``` python interpolate.py PATH_TO_CONFIG ``` or ``` python interpolate_class.py PATH_TO_CONFIG ```   """;General;https://github.com/LMescheder/GAN_stability
"""You also need to install the MS COCO APIs. ``` cd <CornerNet dir>/data git clone git@github.com:cocodataset/cocoapi.git coco cd <CornerNet dir>/data/coco/PythonAPI make ```   python setup.py install --user   Please first install [Anaconda](https://anaconda.org) and create an Anaconda environment using the provided package list. ``` conda create --name CornerNet --file conda_packagelist.txt ```  After you create the environment  activate it. ``` source activate CornerNet ```  Our current implementation only supports GPU so you need a GPU and need to have CUDA installed on your machine.   """;Computer Vision;https://github.com/princeton-vl/CornerNet
"""In PyTorch: ```python optimizer = #: {any optimizer} e.g. torch.optim.Adam if args.lookahead:     optimizer = Lookahead(optimizer  la_steps=args.la_steps  la_alpha=args.la_alpha) ```  In TensorFlow: ```python optimizer = #: {any optimizer} e.g. tf.train.AdamOptimizer if args.lookahead:     optimizer = Lookahead(optimizer  la_steps=args.la_steps  la_alpha=args.la_alpha) ```  We found that evaluation performance is typically better using the slow weights. This can be done in PyTorch with something like this in your eval loop: ```python if args.lookahead:     optimizer._backup_and_load_cache()     val_loss = eval_func(model)     optimizer._clear_and_load_backup() ```   """;General;https://github.com/michaelrzhang/lookahead
"""| enwik8_pers.sh | 114M | 1.00 bpb | 0.98 bpb |   """;Natural Language Processing;https://github.com/facebookresearch/adaptive-span
"""Population Based Augmentation (PBA) is a algorithm that quickly and efficiently learns data augmentation functions for neural network training. PBA matches state-of-the-art results on CIFAR with one thousand times less compute  enabling researchers and practitioners to effectively learn new augmentation policies using a single workstation GPU.  This repository contains code for the work ""Population Based Augmentation: Efficient Learning of Augmentation Schedules"" (http://arxiv.org/abs/1905.05393) in TensorFlow and Python. It includes training of models with the reported augmentation schedules and discovery of new augmentation policy schedules.  See below for a visualization of our augmentation strategy.  <p align=""center""> <img src=""figs/augs_v2_crop.png"" width=""40%""> </p>   ```shell pip install -r requirements.txt ```   <b><i>Now with Python 3 support.</b></i>   bash scripts/table_1_cifar10.sh wrn_28_10   bash scripts/table_4_svhn.sh rsvhn_ss_96   Code supports Python 2 and 3.   """;Computer Vision;https://github.com/arcelien/pba
"""Mesh TensorFlow (`mtf`) is a language for distributed deep learning  capable of specifying a broad class of distributed tensor computations.  The purpose of Mesh TensorFlow is to formalize and implement distribution strategies for your computation graph over your hardware/processors. For example: ""Split the batch over rows of processors and split the units in the hidden layer across columns of processors."" Mesh TensorFlow is implemented as a layer over TensorFlow.  Watch our [YouTube video](https://www.youtube.com/watch?v=HgGyWS40g-g).    To install the latest stable version  run  ```sh pip install mesh-tensorflow ```  To install the latest development version  run  ```sh pip install -e ""git+https://github.com/tensorflow/mesh.git#:egg=mesh-tensorflow"" ```  Installing `mesh-tensorflow` does not automatically install or update TensorFlow. We recommend installing it via `pip install tensorflow` or `pip install tensorflow-gpu`. See TensorFlow‚Äôs [installation instructions for details](https://www.tensorflow.org/install/). If you're using a development version of Mesh TensorFlow  you may need to use TensorFlow's nightly package (`tf-nightly`).   different from the CPU/GPU implementation.   git clone https://github.com/tensorflow/mesh.git   To illustrate  let us consider a simple model for the MNIST image-classification task.  Our network has one hidden layer with 1024 units  and an output layer with 10 units (corresponding to the 10 digit classes).  The code consists of two parts  the first describing the mathematical operations  and the second describing the devices and tensor/computation layout. For the full example  see [`examples/mnist.py`]( https://github.com/tensorflow/mesh/blob/master/examples/mnist.py). TODO(noam): verify that this code works.  ```Python #: tf_images is a tf.Tensor with shape [100  28  28] and dtype tf.float32 #: tf_labels is a tf.Tensor with shape [100] and dtype tf.int32 graph = mtf.Graph() mesh = mtf.Mesh(graph  ""my_mesh"") batch_dim = mtf.Dimension(""batch""  100) rows_dim = mtf.Dimension(""rows""  28) cols_dim = mtf.Dimension(""cols""  28) hidden_dim = mtf.Dimension(""hidden""  1024) classes_dim = mtf.Dimension(""classes""  10) images = mtf.import_tf_tensor(     mesh  tf_images  shape=[batch_dim  rows_dim  cols_dim]) labels = mtf.import_tf_tensor(mesh  tf_labels  [batch_dim]) w1 = mtf.get_variable(mesh  ""w1""  [rows_dim  cols_dim  hidden_dim]) w2 = mtf.get_variable(mesh  ""w2""  [hidden_dim  classes_dim]) #: einsum is a generalization of matrix multiplication (see numpy.einsum) hidden = mtf.relu(mtf.einsum(images  w1  output_shape=[batch_dim  hidden_dim])) logits = mtf.einsum(hidden  w2  output_shape=[batch_dim  classes_dim]) loss = mtf.reduce_mean(mtf.layers.softmax_cross_entropy_with_logits(     logits  mtf.one_hot(labels  classes_dim)  classes_dim)) w1_grad  w2_grad = mtf.gradients([loss]  [w1  w2]) update_w1_op = mtf.assign(w1  w1 - w1_grad * 0.001) update_w2_op = mtf.assign(w2  w2 - w2_grad * 0.001) ```  In the code above  we have built a Mesh TensorFlow graph  which is simply a Python structure.  We have completely defined the mathematical operations. In the code below  we specify the mesh of processors and the layout of the computation.  ```Python devices = [""gpu:0""  ""gpu:1""  ""gpu:2""  ""gpu:3""] mesh_shape = [(""all_processors""  4)] layout_rules = [(""batch""  ""all_processors"")] mesh_impl = mtf.placement_mesh_impl.PlacementMeshImpl(     mesh_shape  layout_rules  devices) lowering = mtf.Lowering(graph  {mesh:mesh_impl}) tf_update_ops = [lowering.lowered_operation(update_w1_op)                   lowering.lowered_operation(update_w2_op)] ```  The particular layout above implements data-parallelism  splitting the batch of examples evenly across all four processors.  Any Tensor with a ""batch"" dimension (e.g. `images`  `h`  `logits`  and their gradients) is split in that dimension across all processors  while any tensor without a ""batch"" dimension (e.g. the model parameters) is replicated identically on every processor.  Alternatively  for model-parallelism  we can set `layout_rules=[(""hidden""  ""all_processors"")]`.  In this case  any tensor with a ""hidden"" dimension (e.g. `hidden`  `w1`  `w2`)  is split  while any other tensor (e.g. `image`  `logits`) is fully replicated.  We can even combine data-parallelism and model-parallelism on a 2-dimensional mesh of processors.  We split the batch along one dimension of the mesh  and the units in the hidden layer along the other dimension of the mesh  as below.  In this case  the hidden layer is actually tiled between the four processors  being split in both the ""batch"" and ""hidden_units"" dimensions.  ```Python mesh_shape = [(""processor_rows""  2)  (""processor_cols""  2)] layout_rules = [(""batch""  ""processor_rows"")  (""hidden""  ""processor_cols"")] ```   Take our example `Tensor` `image_batch` with shape:  `[(""batch""  100)  (""rows""  28"")  (""cols""  28)  (""channels""  3)]`  Assume that this `Tensor` is assigned to a mesh of 8 processors with shape: `[(""processor_rows""  2)  (""processor_cols""  4)]`  * If we use an empty set of layout rules `[]`  we get no splitting.  Each   processor contains the whole `Tensor`.  * If we use the layout rules `""batch:processor_cols""`  then the `""batch""`   dimension of the `Tensor` is split across the `""processor_cols""` dimension of   the batch.  This means that each processor contains a Tensor slice with shape   `[25  28  28  3]`.  For example  processors (0  3) and (1  3) contain   identical slices - `image_batch[75:100  :  :  :]`.  * If we use the layout rules `""rows:processor_rows;cols:processor_cols""`     then the image is split in two dimensions  with each processor containing one   spatial tile with shape `[100  14  7  3]`.   For example  processor (0  1)   contains the slice `image_batch[:  0:14  7:14  :]`.  Some layout rules would lead to illegal layouts:  * `""batch:processor_rows;rows:processor_rows""` is illegal because two tensor   dimensions could not be split across the same mesh dimension.  * `""channels:processor_rows""` is illegal because the size of the tensor   dimension is not evenly divisible by the size of the mesh dimension.   TODO(trandustin ylc): update given mtf pypi package  ```sh ctpu up -name=ylc-mtf-donut -tf-version=nightly -tpu-size=v2-8 -zone=us-central1-b ```   """;General;https://github.com/tensorflow/mesh
"""""";General;https://github.com/vincentbillaut/all-colors-matter
"""""";Computer Vision;https://github.com/xaviercanche/EfficientDet_V2
"""Manual: https://github.com/AlexeyAB/darknet/wiki   Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example   PyTorch > ONNX:    TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like CUDA  cudnn  ZED and build against those. It will also create a shared object library file to use darknet for code development.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows.  Install Visual Studio 2017 or 2019. In case you need to download it  please go here: Visual Studio Community  Install CUDA (at least v10.0) enabling VS Integration during installation.   PS Code\&gt;              git clone https://github.com/microsoft/vcpkg  PS Code\&gt;              cd vcpkg   PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base cuda cudnn]:x64-windows for a quicker install of dependencies  PS Code\vcpkg&gt;         cd ..  PS Code\&gt;              git clone https://github.com/AlexeyAB/darknet  PS Code\&gt;              cd darknet   Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   in Python: https://github.com/tzutalin/labelImg  in Python: https://github.com/Cartucho/OpenLabeling   in JavaScript: https://github.com/opencv/cvat   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux   * using `build.sh` or   * build `darknet` using `cmake` or   * set `LIBSO=1` in the `Makefile` and do `make` * on Windows   * using `build.ps1` or   * build `darknet` using `cmake` or   * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs:  * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h   * Python examples using the C API:     * https://github.com/AlexeyAB/darknet/blob/master/darknet.py     * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py  * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp   * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp  ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`      * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)  `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42)  ```cpp struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false);         std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/janmejoykar1807/CoolHead
"""    Clone the Repo.   """;Computer Vision;https://github.com/u7javed/Handwritten-Number-Generator
"""This repo supports importing modules through torch.hub. FocalLoss can be easily imported into your code via  for example:   - `FocalLoss` is an `nn.Module` and behaves very much like `nn.CrossEntropyLoss()` i.e.     - supports the `reduction` and `ignore_index` params  and     - is able to work with 2D inputs of shape `(N  C)` as well as K-dimensional inputs of shape `(N  C  d1  d2  ...  dK)`.  - Example usage     ```python3     focal_loss = FocalLoss(alpha  gamma) 	... 	inp  targets = batch     out = model(inp) 	loss = focal_loss(out  targets)     ```   """;Computer Vision;https://github.com/AdeelH/pytorch-multi-class-focal-loss
"""This repo supports importing modules through torch.hub. FocalLoss can be easily imported into your code via  for example:   - `FocalLoss` is an `nn.Module` and behaves very much like `nn.CrossEntropyLoss()` i.e.     - supports the `reduction` and `ignore_index` params  and     - is able to work with 2D inputs of shape `(N  C)` as well as K-dimensional inputs of shape `(N  C  d1  d2  ...  dK)`.  - Example usage     ```python3     focal_loss = FocalLoss(alpha  gamma) 	... 	inp  targets = batch     out = model(inp) 	loss = focal_loss(out  targets)     ```   """;General;https://github.com/AdeelH/pytorch-multi-class-focal-loss
"""""";General;https://github.com/keyuchen886/GoodReads
"""This version supports cudnn v2 acceleration. @TimoSaemann has a branch supporting a more recent version of Caffe (Dec 2016) with cudnn v5.1:   In solver.prototxt set a path for snapshot_prefix. Then in a terminal run   If you would just like to try out a pretrained example model  then you can find the model used in the [SegNet webdemo](http://mi.eng.cam.ac.uk/projects/segnet/) and a script to run a live webcam demo here: https://github.com/alexgkendall/SegNet-Tutorial  For a more detailed introduction to this software please see the tutorial here: http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html   """;Computer Vision;https://github.com/sducournau/caffe-segnet
"""""";Computer Vision;https://github.com/js-aguiar/wheat-object-detection
"""""";Computer Vision;https://github.com/ocinemod87/AML_project
"""* **Day_01 : Ë≥áÊñô‰ªãÁ¥πËàáË©ï‰º∞ÊåáÊ®ô**     * Êé¢Á¥¢ÊµÅÁ®ã : ÊâæÂà∞ÂïèÈ°å -> ÂàùÊé¢ -> ÊîπÈÄ≤ -> ÂàÜ‰∫´ -> Á∑¥Áøí -> ÂØ¶Êà∞     * ÊÄùËÄÉÈóúÈçµÈªû :         * ÁÇ∫‰ªÄÈ∫ºÈÄôÂÄãÂïèÈ°åÈáçË¶ÅÔºü         * Ë≥áÊñôÂæû‰ΩïËÄå‰æÜÔºü         * Ë≥áÊñôÂûãÊÖãÊòØ‰ªÄÈ∫ºÔºü         * ÂõûÁ≠îÂïèÈ°åÁöÑÈóúÈçµÊåáÊ®ôÊòØ‰ªÄÈ∫ºÔºü * **Day_02 : Ê©üÂô®Â≠∏ÁøíÊ¶ÇË´ñ**     * Ê©üÂô®Â≠∏ÁøíÁØÑÁñá : **Ê∑±Â∫¶Â≠∏Áøí (Deep Learning)** ‚äÇ **Ê©üÂô®Â≠∏Áøí (Machine Learning)** ‚äÇ **‰∫∫Â∑•Êô∫ÊÖß (Artificial Intelligence)**     * Ê©üÂô®Â≠∏ÁøíÊòØ‰ªÄÈ∫º :         * ËÆìÊ©üÂô®ÂæûË≥áÊñôÊâæÂ∞ãË¶èÂæãËàáË∂®Âã¢Ôºå‰∏çÈúÄË¶ÅÁµ¶ÂÆöÁâπÊÆäË¶èÂâá         * Áµ¶ÂÆöÁõÆÊ®ôÂáΩÊï∏ËàáË®ìÁ∑¥Ë≥áÊñôÔºåÂ≠∏ÁøíÂá∫ËÉΩËÆìÁõÆÊ®ôÂáΩÊï∏ÊúÄ‰Ω≥ÁöÑÊ®°ÂûãÂèÉÊï∏     * Ê©üÂô®Â≠∏ÁøíÁ∏ΩÈ°û :         * **Áõ£Áù£ÊòØÂ≠∏Áøí (Supervised Learning)** : ÂúñÂÉèÂàÜÈ°û (Classification)„ÄÅË©êÈ®ôÂÅµÊ∏¨ (Fraud detection)ÔºåÈúÄÊàêÂ∞çË≥áÊñô (x y)         * **ÈùûÁõ£Áù£ÊòØÂ≠∏Áøí (Unsupervised Learning)** : ÈôçÁ∂≠ (Dimension Reduction)„ÄÅÂàÜÁæ§ (Clustering)„ÄÅÂ£ìÁ∏ÆÔºåÂè™ÈúÄË≥áÊñô (x)         * **Âº∑ÂåñÂ≠∏Áøí (Reinforcement Learning)** : ‰∏ãÂúçÊ£ã„ÄÅÊâìÈõªÁé©ÔºåÈÄèÈÅé‰ª£ÁêÜÊ©üÂô®‰∫∫ (Agent) ËàáÁí∞Â¢É (Environment) ‰∫íÂãïÔºåÂ≠∏ÁøíÂ¶Ç‰ΩïÁç≤ÂèñÊúÄÈ´òÁçéÂãµ (Reward)Ôºå‰æãÂ¶Ç Alpha GO * **Day_03 : Ê©üÂô®Â≠∏ÁøíÊµÅÁ®ãËàáÊ≠•È©ü**     * **Ë≥áÊñôËíêÈõÜ„ÄÅÂâçËôïÁêÜ**         * ÊîøÂ∫úÂÖ¨ÈñãË≥áÊñô„ÄÅKaggle Ë≥áÊñô             * ÁµêÊßãÂåñË≥áÊñô : Excel Ê™î„ÄÅCSV Ê™î             * ÈùûÁµêÊßãÂåñË≥áÊñô : ÂúñÁâá„ÄÅÂΩ±Èü≥„ÄÅÊñáÂ≠ó         * ‰ΩøÁî® Python Â•ó‰ª∂             * ÈñãÂïüÂúñÁâá : `PIL`„ÄÅ`skimage`„ÄÅ`open-cv`             * ÈñãÂïüÊñá‰ª∂ : `pandas`         * Ë≥áÊñôÂâçËôïÁêÜ :             * Áº∫Â§±ÂÄºÂ°´Ë£ú             * Èõ¢Áæ§ÂÄºËôïÁêÜ             * Ê®ôÊ∫ñÂåñ     * **ÂÆöÁæ©ÁõÆÊ®ôËàáË©ï‰º∞Ê∫ñÂâá**         * ÂõûÊ≠∏ÂïèÈ°åÔºüÂàÜÈ°ûÂïèÈ°åÔºü         * È†êÊ∏¨ÁõÆÊ®ôÊòØ‰ªÄÈ∫ºÔºü(target or y)         * Áî®‰ªÄÈ∫ºË≥áÊñôÈÄ≤Ë°åÈ†êÊ∏¨Ôºü(predictor or x)         * Â∞áË≥áÊñôÂàÜÁÇ∫ :             * Ë®ìÁ∑¥ÈõÜÔºåtraining set             * È©óË≠âÈõÜÔºåvalidation set             * Ê∏¨Ë©¶ÈõÜÔºåtest set         * Ë©ï‰º∞ÊåáÊ®ô             * ÂõûÊ≠∏ÂïèÈ°å (È†êÊ∏¨ÂÄºÁÇ∫ÂØ¶Êï∏)                 * RMSE : Root Mean Squeare Error                 * MAE : Mean Absolute Error                 * R-Square             * ÂàÜÈ°ûÂïèÈ°å (È†êÊ∏¨ÂÄºÁÇ∫È°ûÂà•)                 * Accuracy                 * [F1-score](https://en.wikipedia.org/wiki/F1_score)                 * [AUC](https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF)ÔºåArea Under Curve     * **Âª∫Á´ãÊ®°ÂûãËàáË™øÊï¥ÂèÉÊï∏**         * RegressionÔºåÂõûÊ≠∏Ê®°Âûã         * Tree-base modelÔºåÊ®πÊ®°Âûã         * Neural networkÔºåÁ•ûÁ∂ìÁ∂≤Ë∑Ø         * HyperparameterÔºåÊ†πÊìöÂ∞çÊ®°Âûã‰∫ÜËß£ÂíåË®ìÁ∑¥ÊÉÖÂΩ¢ÈÄ≤Ë°åË™øÊï¥     * **Â∞éÂÖ•**         * Âª∫Á´ãË≥áÊñôËíêÈõÜ„ÄÅÂâçËôïÁêÜ(Preprocessing)Á≠âÊµÅÁ®ã         * ÈÄÅÈÄ≤Ê®°ÂûãÈÄ≤Ë°åÈ†êÊ∏¨         * Ëº∏Âá∫È†êÊ∏¨ÁµêÊûú         * Ë¶ñÂ∞àÊ°àÈúÄÊ±ÇË™øÊï¥ÂâçÂæåÁ´Ø * **Day_04 : ËÆÄÂèñË≥áÊñôËàáÂàÜÊûêÊµÅÁ®ã (EDAÔºåExploratory Data Analysis)**        * ÈÄèÈÅéË¶ñË¶∫ÂåñÂíåÁµ±Ë®àÂ∑•ÂÖ∑ÈÄ≤Ë°åÂàÜÊûê         * ‰∫ÜËß£Ë≥áÊñô : Áç≤ÂèñË≥áÊñôÂåÖÂê´ÁöÑË≥áË®ä„ÄÅÁµêÊßã„ÄÅÁâπÈªû         * ÁôºÁèæ outlier ÊàñÁï∞Â∏∏Êï∏ÂÄº : Ê™¢Êü•Ë≥áÊñôÊòØÂê¶ÊúâË™§         * ÂàÜÊûêÂêÑËÆäÊï∏ÈñìÁöÑÈóúËÅØÊÄß : ÊâæÂá∫ÈáçË¶ÅÁöÑËÆäÊï∏     * Êî∂ÈõÜË≥áÊñô -> Êï∏ÊìöÊ∏ÖÁêÜ -> ÁâπÂæµËêÉÂèñ -> Ë≥áÊñôË¶ñË¶∫Âåñ -> Âª∫Á´ãÊ®°Âûã -> È©óË≠âÊ®°Âûã -> Ê±∫Á≠ñÊáâÁî®              preprocessing_function=None                           rescale=None                           fill_mode='nearest'                           channel_shift_range=0.                           zoom_range=0.                           shear_range=0.                           samplewise_center=False                           featurewise_center=False                       (x_train  y_train)  (x_test  y_test) = cifar10.load_data()         print('x_train shape:'  x_train.shape)         print(x_train.shape[0]  'train samples')         print(x_test.shape[0]  'test samples')           Python npy       %matplotlib notebook #: ‰∫íÂãïÊñπÂºè       from scipy.stats import mode       * [Python Graph Gallery](https://python-graph-gallery.com/)       * [matplotlib ÂÆò‚ΩÖÊñπÁØÑ‰æãÔ¶µ](https://matplotlib.org/examples/pylab_examples/subplots_demo.html)       * [Âü∫Êú¨ Heatmap](https://matplotlib.org/gallery/images_contours_and_fields/image_annotated_heatmap.html)       from scipy import stats       * [ÁâπÂæµ‰∫§Âèâ](https://segmentfault.com/a/1190000014799038)       * [ÁâπÂæµÈÅ∏Êìá](https://zhuanlan.zhihu.com/p/32749489)       * [Êõ¥Â§öË©ï‰º∞ÊåáÊ®ô](https://zhuanlan.zhihu.com/p/30721429)   Kernels ÂèØ‰ª•ÁúãÔ®äË®±Â§öÈ´òÊâãÂÄëÂàÜ‰∫´ÁöÑÁ®ãÂºèÁ¢ºËàáÁµêÊûúÔºåÂ§öÂçäÊúÉ‰ª• jupyter notebook ÂëàÁèæ           * ÂúñÂΩ¢ËôïÔß§Âô® (GPU) ÁöÑË™ïÁîüÔºåÊåÅÁ∫åÔ¶∫Êô∂ÁâáÊë©ÁàæÂÆöÔßòÔºåËÆìË®àÁÆóÊàêÁÇ∫ÂèØÔ®à   TensorFlow Â∞áÊ∑±Â∫¶Â≠∏Áøí‰∏≠ÁöÑ GPU/CPU Êåá‰ª§Â∞ÅË£ù‰æÜÔ§≠ÔºåÊ∏õÂ∞ëË™ûÊ≥ïÂ∑ÆÔ•¢ÔºåKeras ÂâáÊòØÂ∞áÂâçËÄÖÔ§ÅËøë‰∏ÄÊ≠•Â∞ÅË£ùÊàêÂñÆ‰∏ÄÂ•ó‰ª∂ÔºåÁî®Â∞ëÔ•æÁöÑÁ®ãÂºèÔ••ËÉΩÂØ¶ÁèæÁ∂ìÂÖ∏Ê®°Âûã   * ÊòØÂê¶Êúâ GPU :       * Âõ†ÁÇ∫Êúâ GPU ÂâáÈúÄË¶ÅÂÖàË£ù GPU ÁöÑÊåá‰ª§ÈõÜÔºåÊâÄ‰ª•Êúâ GPU ÂâáÈúÄË¶Å 4 ÂÄãÊ≠•È©üÔºåÊ≤íÊúâÂ∞±Âè™ÈúÄË¶Å 2 Ê≠•È©ü       * Âõ†ÁÇ∫‰∏çÂêå‰ΩúÊ•≠Á≥ªÁµ±ÈñìÔºåGPU ÁöÑÂÆâË£ùÊ≠•È©üÊúÉÂõ†‰ªãÈù¢ÊàñÊåá‰ª§ÊúâÊâÄ‰∏çÂêåÔºåÊâÄ‰ª•ÊàëÂÄëÊúÉÂàÜ Windows / Linux (‰ª•UbuntuÁÇ∫Ô¶µ) / Mac ÂàÜÂà•‰ªãÁ¥πÊµÅÁ®ã       * Áî±Êñº GPU ÁöÑ CUDA / cuDNN ÁâàÊú¨Á∂ìÂ∏∏ÂçáÁ¥öÔºåÂõ†Ê≠§ TensorFlow / Keras ÁöÑÁâàÊú¨‰πüÈúÄË¶ÅÈ†ªÁπÅÔ§ÅÊèõÁâàÊú¨ÔºåÂõ†Ê≠§Âª∫Ë≠∞‰ª•ÂÆâË£ùÁï∂ÊôÇÁöÑÂÆòÁ∂≤Ë≥áË®äÁÇ∫Ê∫ñ  ÂÆâË£ù Keras Â§ßËá¥‰∏äÂàÜÁÇ∫ÂõõÂÄãÊ≠•È©ü : ‰æùÂ∫èÂÆâË£ù CUDA / cuDNN / TensorFlow / KerasÔºåÂè™Ë¶ÅÊ≥®ÊÑèÂõõÂÄãÁ®ãÂºèÈñìÁöÑÁâàÊú¨ÂïèÈ°å‰ª•ÂèäËôõÊì¨Áí∞Â¢ÉÂïèÈ°åÔºåÂü∫Êú¨‰∏äÊáâË©≤ËÉΩÈ†ÜÔßùÂÆâË£ùÂÆåÊàê  ÂÆâË£ùÊµÅÁ®ã - Ê≤íÊúâ GPU Áâà       pip install tensorflow  Ubuntu ÂâçÈù¢Âä†‰∏ä sudo       pip install keras  Python Êâæ‰∏çÂà∞ pip Êåá‰ª§ÔºåÂèØ‰ª•Êé°Áî® pip3 ‰ª£ÊõøÂü∑Ô®àÂÆâË£ù  ÂÆâË£ùÊµÅÁ®ã - Êúâ GPU Áâà   Step 3 - ÂÆâË£ù TensorFlow GPU Áâà      pip install tensorflow-gpu       pip install keras   (Âè™Êúâ Windows ÈúÄË¶Å  ÂÖ∂‰ªñ‰ΩúÊ•≠Á≥ªÁµ±Ë´ãË∑≥ÈÅé) Â¶ÇÊûúÊòØ Win10ÔºåÂèØÂæûÈñãÂßã / ÊéßÂà∂Âè∞ / Á≥ªÁµ±ÈñãÂïüË¶ñÁ™óÂæåÔºåÈªûÈÅ∏""ÈÄ≤Èöé""ÂàÜÈ†ÅÊúÄ‰∏ãÈù¢ÁöÑÊåâÈàï""Áí∞Â¢ÉËÆäÊï∏""ÔºåÊúÉË∑≥Âá∫‰∏ãÂàóÔ¶úË¶ñÁ™óÔºåË´ãÂú®‰∏ãÂçäË¶ñÁ™ó‰∏≠Â∞ãÊâæ""Path""ËÆäÊï∏ÔºåÊää‰∏ãÂàóÔ¶úÔ•∏ÂÄãÔ§∑ÂæëÂä†ÂÖ•      C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin      C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp      È†ÖÁõÆÈñìË¶ÅÁî®ÂàÜËôü ; ÈöîÈñã / CUDA ÁâàËôüË´ã‰æù Step1 ÂØ¶ÈöõÂÆâË£ùÁâàÊú¨ÁÇ∫Ê∫ñ   ÂÆ£Âëä‰∏ÄÂÄã NAME ÂéªÂÆöÁæ©Input   ÈÅ©Áî®Êñº Jupyter Notebook  ÂÆ£ÂëäÁõ¥Êé•Âú®cell ÂÖßÂç∞Âá∫Âü∑Ë°åÁµêÊûú               #ÂØ¶Ô¶µÂåñ‰∏ÄÂÄãÂÑ™ÂåñÂô®Â∞çË±°ÔºåÁÑ∂ÂæåÂ∞áÂÆÉÂÇ≥ÂÖ•model.compile()   ÂèØ‰ª•‰øÆÊîπÔ•´Êï∏                #ÂØ¶Ô¶µÂåñ‰∏ÄÂÄãÂÑ™ÂåñÂô®Â∞çË±°ÔºåÁÑ∂ÂæåÂ∞áÂÆÉÂÇ≥ÂÖ•model.compile()   ÂèØ‰ª•‰øÆÊîπÔ•´Êï∏       * [Overfitting vs. Underfitting](https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765)   Ë®ìÁ∑¥Ê®°ÂûãÁöÑÊôÇÈñìË∑üÊàêÊú¨ÈÉΩÂæàÂ§ß (Â¶Ç GPU quota & ‰Ω†/Â¶≥ÁöÑ‰∫∫Áîü)   ‰ΩøÁî®ÁöÑË£ùÁΩÆÔºöÊòØ‰ΩøÁî® CPU or GPU / ÊÉ≥Ë¶Å‰ΩøÁî®ÁöÑ GPU ÊòØÂê¶Â∑≤Á∂ìË¢´Âà•‰∫∫‰ΩîÁî®?   Ëé´ÁÖ© Python - ÂÑ≤Â≠òËàáËºâÂõûÊ®°Âûã       * [OpenCv - ÊïôÂ≠∏ÊñáÊ™î](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html)               A -- Ëº∏Âá∫ÁöÑÊ±†ÂåñÂ±§  Á∂≠Â∫¶ÁÇ∫ (m  n_H  n_W  n_C) ÁöÑ numpy Èô£Âàó   Â¶ÇÂêåË®ìÁ∑¥Á•ûÁ∂ìÁ∂≤Ô§∑ÊôÇÔºåBatch (ÊâπÊ¨°) ÁöÑÊ¶ÇÔ¶£‰∏ÄÊ®£„ÄÇÊàëÂÄëÂèØ‰ª•Â∞áË≥áÊñô‰∏ÄÊâπ‰∏ÄÊâπÁöÑËÆÄÈÄ≤Ë®òÊÜ∂È´îÔºåÁï∂Âæû GPU/CPU Ë®ìÁ∑¥ÂÆåÂæåÔºåÂ∞áÈÄôÊâπË≥áÊñôÂæûË®òÊÜ∂È´îÈáãÂá∫ÔºåÂú®ËÆÄÂèñ‰∏ã‰∏ÄÊâπË≥áÊñô  Â¶Ç‰ΩïÁî® Python Êí∞ÂØ´ÊâπÊ¨°ËÆÄÂèñË≥áÊñôÁöÑÁ®ãÂºèÁ¢º   version = 1               activation (string): activation name   COCO dataset   COCO dataset   COCO dataset   COCO dataset   """;General;https://github.com/Halesu/4th-ML100Days
"""* **Day_01 : Ë≥áÊñô‰ªãÁ¥πËàáË©ï‰º∞ÊåáÊ®ô**     * Êé¢Á¥¢ÊµÅÁ®ã : ÊâæÂà∞ÂïèÈ°å -> ÂàùÊé¢ -> ÊîπÈÄ≤ -> ÂàÜ‰∫´ -> Á∑¥Áøí -> ÂØ¶Êà∞     * ÊÄùËÄÉÈóúÈçµÈªû :         * ÁÇ∫‰ªÄÈ∫ºÈÄôÂÄãÂïèÈ°åÈáçË¶ÅÔºü         * Ë≥áÊñôÂæû‰ΩïËÄå‰æÜÔºü         * Ë≥áÊñôÂûãÊÖãÊòØ‰ªÄÈ∫ºÔºü         * ÂõûÁ≠îÂïèÈ°åÁöÑÈóúÈçµÊåáÊ®ôÊòØ‰ªÄÈ∫ºÔºü * **Day_02 : Ê©üÂô®Â≠∏ÁøíÊ¶ÇË´ñ**     * Ê©üÂô®Â≠∏ÁøíÁØÑÁñá : **Ê∑±Â∫¶Â≠∏Áøí (Deep Learning)** ‚äÇ **Ê©üÂô®Â≠∏Áøí (Machine Learning)** ‚äÇ **‰∫∫Â∑•Êô∫ÊÖß (Artificial Intelligence)**     * Ê©üÂô®Â≠∏ÁøíÊòØ‰ªÄÈ∫º :         * ËÆìÊ©üÂô®ÂæûË≥áÊñôÊâæÂ∞ãË¶èÂæãËàáË∂®Âã¢Ôºå‰∏çÈúÄË¶ÅÁµ¶ÂÆöÁâπÊÆäË¶èÂâá         * Áµ¶ÂÆöÁõÆÊ®ôÂáΩÊï∏ËàáË®ìÁ∑¥Ë≥áÊñôÔºåÂ≠∏ÁøíÂá∫ËÉΩËÆìÁõÆÊ®ôÂáΩÊï∏ÊúÄ‰Ω≥ÁöÑÊ®°ÂûãÂèÉÊï∏     * Ê©üÂô®Â≠∏ÁøíÁ∏ΩÈ°û :         * **Áõ£Áù£ÊòØÂ≠∏Áøí (Supervised Learning)** : ÂúñÂÉèÂàÜÈ°û (Classification)„ÄÅË©êÈ®ôÂÅµÊ∏¨ (Fraud detection)ÔºåÈúÄÊàêÂ∞çË≥áÊñô (x y)         * **ÈùûÁõ£Áù£ÊòØÂ≠∏Áøí (Unsupervised Learning)** : ÈôçÁ∂≠ (Dimension Reduction)„ÄÅÂàÜÁæ§ (Clustering)„ÄÅÂ£ìÁ∏ÆÔºåÂè™ÈúÄË≥áÊñô (x)         * **Âº∑ÂåñÂ≠∏Áøí (Reinforcement Learning)** : ‰∏ãÂúçÊ£ã„ÄÅÊâìÈõªÁé©ÔºåÈÄèÈÅé‰ª£ÁêÜÊ©üÂô®‰∫∫ (Agent) ËàáÁí∞Â¢É (Environment) ‰∫íÂãïÔºåÂ≠∏ÁøíÂ¶Ç‰ΩïÁç≤ÂèñÊúÄÈ´òÁçéÂãµ (Reward)Ôºå‰æãÂ¶Ç Alpha GO * **Day_03 : Ê©üÂô®Â≠∏ÁøíÊµÅÁ®ãËàáÊ≠•È©ü**     * **Ë≥áÊñôËíêÈõÜ„ÄÅÂâçËôïÁêÜ**         * ÊîøÂ∫úÂÖ¨ÈñãË≥áÊñô„ÄÅKaggle Ë≥áÊñô             * ÁµêÊßãÂåñË≥áÊñô : Excel Ê™î„ÄÅCSV Ê™î             * ÈùûÁµêÊßãÂåñË≥áÊñô : ÂúñÁâá„ÄÅÂΩ±Èü≥„ÄÅÊñáÂ≠ó         * ‰ΩøÁî® Python Â•ó‰ª∂             * ÈñãÂïüÂúñÁâá : `PIL`„ÄÅ`skimage`„ÄÅ`open-cv`             * ÈñãÂïüÊñá‰ª∂ : `pandas`         * Ë≥áÊñôÂâçËôïÁêÜ :             * Áº∫Â§±ÂÄºÂ°´Ë£ú             * Èõ¢Áæ§ÂÄºËôïÁêÜ             * Ê®ôÊ∫ñÂåñ     * **ÂÆöÁæ©ÁõÆÊ®ôËàáË©ï‰º∞Ê∫ñÂâá**         * ÂõûÊ≠∏ÂïèÈ°åÔºüÂàÜÈ°ûÂïèÈ°åÔºü         * È†êÊ∏¨ÁõÆÊ®ôÊòØ‰ªÄÈ∫ºÔºü(target or y)         * Áî®‰ªÄÈ∫ºË≥áÊñôÈÄ≤Ë°åÈ†êÊ∏¨Ôºü(predictor or x)         * Â∞áË≥áÊñôÂàÜÁÇ∫ :             * Ë®ìÁ∑¥ÈõÜÔºåtraining set             * È©óË≠âÈõÜÔºåvalidation set             * Ê∏¨Ë©¶ÈõÜÔºåtest set         * Ë©ï‰º∞ÊåáÊ®ô             * ÂõûÊ≠∏ÂïèÈ°å (È†êÊ∏¨ÂÄºÁÇ∫ÂØ¶Êï∏)                 * RMSE : Root Mean Squeare Error                 * MAE : Mean Absolute Error                 * R-Square             * ÂàÜÈ°ûÂïèÈ°å (È†êÊ∏¨ÂÄºÁÇ∫È°ûÂà•)                 * Accuracy                 * [F1-score](https://en.wikipedia.org/wiki/F1_score)                 * [AUC](https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF)ÔºåArea Under Curve     * **Âª∫Á´ãÊ®°ÂûãËàáË™øÊï¥ÂèÉÊï∏**         * RegressionÔºåÂõûÊ≠∏Ê®°Âûã         * Tree-base modelÔºåÊ®πÊ®°Âûã         * Neural networkÔºåÁ•ûÁ∂ìÁ∂≤Ë∑Ø         * HyperparameterÔºåÊ†πÊìöÂ∞çÊ®°Âûã‰∫ÜËß£ÂíåË®ìÁ∑¥ÊÉÖÂΩ¢ÈÄ≤Ë°åË™øÊï¥     * **Â∞éÂÖ•**         * Âª∫Á´ãË≥áÊñôËíêÈõÜ„ÄÅÂâçËôïÁêÜ(Preprocessing)Á≠âÊµÅÁ®ã         * ÈÄÅÈÄ≤Ê®°ÂûãÈÄ≤Ë°åÈ†êÊ∏¨         * Ëº∏Âá∫È†êÊ∏¨ÁµêÊûú         * Ë¶ñÂ∞àÊ°àÈúÄÊ±ÇË™øÊï¥ÂâçÂæåÁ´Ø * **Day_04 : ËÆÄÂèñË≥áÊñôËàáÂàÜÊûêÊµÅÁ®ã (EDAÔºåExploratory Data Analysis)**        * ÈÄèÈÅéË¶ñË¶∫ÂåñÂíåÁµ±Ë®àÂ∑•ÂÖ∑ÈÄ≤Ë°åÂàÜÊûê         * ‰∫ÜËß£Ë≥áÊñô : Áç≤ÂèñË≥áÊñôÂåÖÂê´ÁöÑË≥áË®ä„ÄÅÁµêÊßã„ÄÅÁâπÈªû         * ÁôºÁèæ outlier ÊàñÁï∞Â∏∏Êï∏ÂÄº : Ê™¢Êü•Ë≥áÊñôÊòØÂê¶ÊúâË™§         * ÂàÜÊûêÂêÑËÆäÊï∏ÈñìÁöÑÈóúËÅØÊÄß : ÊâæÂá∫ÈáçË¶ÅÁöÑËÆäÊï∏     * Êî∂ÈõÜË≥áÊñô -> Êï∏ÊìöÊ∏ÖÁêÜ -> ÁâπÂæµËêÉÂèñ -> Ë≥áÊñôË¶ñË¶∫Âåñ -> Âª∫Á´ãÊ®°Âûã -> È©óË≠âÊ®°Âûã -> Ê±∫Á≠ñÊáâÁî®              preprocessing_function=None                           rescale=None                           fill_mode='nearest'                           channel_shift_range=0.                           zoom_range=0.                           shear_range=0.                           samplewise_center=False                           featurewise_center=False                       (x_train  y_train)  (x_test  y_test) = cifar10.load_data()         print('x_train shape:'  x_train.shape)         print(x_train.shape[0]  'train samples')         print(x_test.shape[0]  'test samples')           Python npy       %matplotlib notebook #: ‰∫íÂãïÊñπÂºè       from scipy.stats import mode       * [Python Graph Gallery](https://python-graph-gallery.com/)       * [matplotlib ÂÆò‚ΩÖÊñπÁØÑ‰æãÔ¶µ](https://matplotlib.org/examples/pylab_examples/subplots_demo.html)       * [Âü∫Êú¨ Heatmap](https://matplotlib.org/gallery/images_contours_and_fields/image_annotated_heatmap.html)       from scipy import stats       * [ÁâπÂæµ‰∫§Âèâ](https://segmentfault.com/a/1190000014799038)       * [ÁâπÂæµÈÅ∏Êìá](https://zhuanlan.zhihu.com/p/32749489)       * [Êõ¥Â§öË©ï‰º∞ÊåáÊ®ô](https://zhuanlan.zhihu.com/p/30721429)   Kernels ÂèØ‰ª•ÁúãÔ®äË®±Â§öÈ´òÊâãÂÄëÂàÜ‰∫´ÁöÑÁ®ãÂºèÁ¢ºËàáÁµêÊûúÔºåÂ§öÂçäÊúÉ‰ª• jupyter notebook ÂëàÁèæ           * ÂúñÂΩ¢ËôïÔß§Âô® (GPU) ÁöÑË™ïÁîüÔºåÊåÅÁ∫åÔ¶∫Êô∂ÁâáÊë©ÁàæÂÆöÔßòÔºåËÆìË®àÁÆóÊàêÁÇ∫ÂèØÔ®à   TensorFlow Â∞áÊ∑±Â∫¶Â≠∏Áøí‰∏≠ÁöÑ GPU/CPU Êåá‰ª§Â∞ÅË£ù‰æÜÔ§≠ÔºåÊ∏õÂ∞ëË™ûÊ≥ïÂ∑ÆÔ•¢ÔºåKeras ÂâáÊòØÂ∞áÂâçËÄÖÔ§ÅËøë‰∏ÄÊ≠•Â∞ÅË£ùÊàêÂñÆ‰∏ÄÂ•ó‰ª∂ÔºåÁî®Â∞ëÔ•æÁöÑÁ®ãÂºèÔ••ËÉΩÂØ¶ÁèæÁ∂ìÂÖ∏Ê®°Âûã   * ÊòØÂê¶Êúâ GPU :       * Âõ†ÁÇ∫Êúâ GPU ÂâáÈúÄË¶ÅÂÖàË£ù GPU ÁöÑÊåá‰ª§ÈõÜÔºåÊâÄ‰ª•Êúâ GPU ÂâáÈúÄË¶Å 4 ÂÄãÊ≠•È©üÔºåÊ≤íÊúâÂ∞±Âè™ÈúÄË¶Å 2 Ê≠•È©ü       * Âõ†ÁÇ∫‰∏çÂêå‰ΩúÊ•≠Á≥ªÁµ±ÈñìÔºåGPU ÁöÑÂÆâË£ùÊ≠•È©üÊúÉÂõ†‰ªãÈù¢ÊàñÊåá‰ª§ÊúâÊâÄ‰∏çÂêåÔºåÊâÄ‰ª•ÊàëÂÄëÊúÉÂàÜ Windows / Linux (‰ª•UbuntuÁÇ∫Ô¶µ) / Mac ÂàÜÂà•‰ªãÁ¥πÊµÅÁ®ã       * Áî±Êñº GPU ÁöÑ CUDA / cuDNN ÁâàÊú¨Á∂ìÂ∏∏ÂçáÁ¥öÔºåÂõ†Ê≠§ TensorFlow / Keras ÁöÑÁâàÊú¨‰πüÈúÄË¶ÅÈ†ªÁπÅÔ§ÅÊèõÁâàÊú¨ÔºåÂõ†Ê≠§Âª∫Ë≠∞‰ª•ÂÆâË£ùÁï∂ÊôÇÁöÑÂÆòÁ∂≤Ë≥áË®äÁÇ∫Ê∫ñ  ÂÆâË£ù Keras Â§ßËá¥‰∏äÂàÜÁÇ∫ÂõõÂÄãÊ≠•È©ü : ‰æùÂ∫èÂÆâË£ù CUDA / cuDNN / TensorFlow / KerasÔºåÂè™Ë¶ÅÊ≥®ÊÑèÂõõÂÄãÁ®ãÂºèÈñìÁöÑÁâàÊú¨ÂïèÈ°å‰ª•ÂèäËôõÊì¨Áí∞Â¢ÉÂïèÈ°åÔºåÂü∫Êú¨‰∏äÊáâË©≤ËÉΩÈ†ÜÔßùÂÆâË£ùÂÆåÊàê  ÂÆâË£ùÊµÅÁ®ã - Ê≤íÊúâ GPU Áâà       pip install tensorflow  Ubuntu ÂâçÈù¢Âä†‰∏ä sudo       pip install keras  Python Êâæ‰∏çÂà∞ pip Êåá‰ª§ÔºåÂèØ‰ª•Êé°Áî® pip3 ‰ª£ÊõøÂü∑Ô®àÂÆâË£ù  ÂÆâË£ùÊµÅÁ®ã - Êúâ GPU Áâà   Step 3 - ÂÆâË£ù TensorFlow GPU Áâà      pip install tensorflow-gpu       pip install keras   (Âè™Êúâ Windows ÈúÄË¶Å  ÂÖ∂‰ªñ‰ΩúÊ•≠Á≥ªÁµ±Ë´ãË∑≥ÈÅé) Â¶ÇÊûúÊòØ Win10ÔºåÂèØÂæûÈñãÂßã / ÊéßÂà∂Âè∞ / Á≥ªÁµ±ÈñãÂïüË¶ñÁ™óÂæåÔºåÈªûÈÅ∏""ÈÄ≤Èöé""ÂàÜÈ†ÅÊúÄ‰∏ãÈù¢ÁöÑÊåâÈàï""Áí∞Â¢ÉËÆäÊï∏""ÔºåÊúÉË∑≥Âá∫‰∏ãÂàóÔ¶úË¶ñÁ™óÔºåË´ãÂú®‰∏ãÂçäË¶ñÁ™ó‰∏≠Â∞ãÊâæ""Path""ËÆäÊï∏ÔºåÊää‰∏ãÂàóÔ¶úÔ•∏ÂÄãÔ§∑ÂæëÂä†ÂÖ•      C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin      C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp      È†ÖÁõÆÈñìË¶ÅÁî®ÂàÜËôü ; ÈöîÈñã / CUDA ÁâàËôüË´ã‰æù Step1 ÂØ¶ÈöõÂÆâË£ùÁâàÊú¨ÁÇ∫Ê∫ñ   ÂÆ£Âëä‰∏ÄÂÄã NAME ÂéªÂÆöÁæ©Input   ÈÅ©Áî®Êñº Jupyter Notebook  ÂÆ£ÂëäÁõ¥Êé•Âú®cell ÂÖßÂç∞Âá∫Âü∑Ë°åÁµêÊûú               #ÂØ¶Ô¶µÂåñ‰∏ÄÂÄãÂÑ™ÂåñÂô®Â∞çË±°ÔºåÁÑ∂ÂæåÂ∞áÂÆÉÂÇ≥ÂÖ•model.compile()   ÂèØ‰ª•‰øÆÊîπÔ•´Êï∏                #ÂØ¶Ô¶µÂåñ‰∏ÄÂÄãÂÑ™ÂåñÂô®Â∞çË±°ÔºåÁÑ∂ÂæåÂ∞áÂÆÉÂÇ≥ÂÖ•model.compile()   ÂèØ‰ª•‰øÆÊîπÔ•´Êï∏       * [Overfitting vs. Underfitting](https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765)   Ë®ìÁ∑¥Ê®°ÂûãÁöÑÊôÇÈñìË∑üÊàêÊú¨ÈÉΩÂæàÂ§ß (Â¶Ç GPU quota & ‰Ω†/Â¶≥ÁöÑ‰∫∫Áîü)   ‰ΩøÁî®ÁöÑË£ùÁΩÆÔºöÊòØ‰ΩøÁî® CPU or GPU / ÊÉ≥Ë¶Å‰ΩøÁî®ÁöÑ GPU ÊòØÂê¶Â∑≤Á∂ìË¢´Âà•‰∫∫‰ΩîÁî®?   Ëé´ÁÖ© Python - ÂÑ≤Â≠òËàáËºâÂõûÊ®°Âûã       * [OpenCv - ÊïôÂ≠∏ÊñáÊ™î](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html)               A -- Ëº∏Âá∫ÁöÑÊ±†ÂåñÂ±§  Á∂≠Â∫¶ÁÇ∫ (m  n_H  n_W  n_C) ÁöÑ numpy Èô£Âàó   Â¶ÇÂêåË®ìÁ∑¥Á•ûÁ∂ìÁ∂≤Ô§∑ÊôÇÔºåBatch (ÊâπÊ¨°) ÁöÑÊ¶ÇÔ¶£‰∏ÄÊ®£„ÄÇÊàëÂÄëÂèØ‰ª•Â∞áË≥áÊñô‰∏ÄÊâπ‰∏ÄÊâπÁöÑËÆÄÈÄ≤Ë®òÊÜ∂È´îÔºåÁï∂Âæû GPU/CPU Ë®ìÁ∑¥ÂÆåÂæåÔºåÂ∞áÈÄôÊâπË≥áÊñôÂæûË®òÊÜ∂È´îÈáãÂá∫ÔºåÂú®ËÆÄÂèñ‰∏ã‰∏ÄÊâπË≥áÊñô  Â¶Ç‰ΩïÁî® Python Êí∞ÂØ´ÊâπÊ¨°ËÆÄÂèñË≥áÊñôÁöÑÁ®ãÂºèÁ¢º   version = 1               activation (string): activation name   COCO dataset   COCO dataset   COCO dataset   COCO dataset   """;Computer Vision;https://github.com/Halesu/4th-ML100Days
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example   PyTorch > ONNX:    TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/TusharMhaske28/Thermal-Object
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/thecodemasterk/BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/thecodemasterk/BERT
"""""";Graphs;https://github.com/whut2962575697/gat_sementic_segmentation
"""""";Computer Vision;https://github.com/NoahBarrett98/ProgressiveGAN
"""""";Computer Vision;https://github.com/yzhu319/dlnd_face_generation_git
"""   - Clone the repository in your GitHub account [Clone with HTTPS](https://github.com/amanjaiswal73892/changemypet.git) - To run the code  please download the pretrained pytorch weights first. [Pretrained Weights](https://github.com/ivclab/BigGAN-Generator-Pretrained-Pytorch/releases/tag/v0.0.0) ```shell     biggan512-release.pt    #: download this for generating 512*512 images ``` - Upload the biggan512-release.pt file to your google drive. - Open MSE_average_losses_clean.ipynb file in Google Colab or your Jupyter Notebook and run it. Comments are added to the file as needed.      """;Computer Vision;https://github.com/amanjaiswal73892/change_my_pet
"""   - Clone the repository in your GitHub account [Clone with HTTPS](https://github.com/amanjaiswal73892/changemypet.git) - To run the code  please download the pretrained pytorch weights first. [Pretrained Weights](https://github.com/ivclab/BigGAN-Generator-Pretrained-Pytorch/releases/tag/v0.0.0) ```shell     biggan512-release.pt    #: download this for generating 512*512 images ``` - Upload the biggan512-release.pt file to your google drive. - Open MSE_average_losses_clean.ipynb file in Google Colab or your Jupyter Notebook and run it. Comments are added to the file as needed.      """;General;https://github.com/amanjaiswal73892/change_my_pet
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/nurulxakmar/darknet
"""1. Archive your training data and upload it to an S3 bucket 2. Provision your EC2 instance (I used an Ubuntu AMI) 3. Log into your EC2 instance via SSH 4. Install the aws CLI client and configure it:  ```bash sudo snap install aws-cli --classic aws configure ```  You will then have to enter your AWS access keys  which you can retrieve from the management console under AWS Management Console > Profile > My Security Credentials > Access Keys  Then  run these commands  or maybe put them in a shell script and execute that:  ```bash mkdir data curl -O https://bootstrap.pypa.io/get-pip.py sudo apt-get install python3-distutils python3 get-pip.py pip3 install stylegan2_pytorch export PATH=$PATH:/home/ubuntu/.local/bin aws s3 sync s3://<Your bucket name> ~/data cd data tar -xf ../train.tar.gz ```  Now you should be able to train by simplying calling `stylegan2_pytorch [args]`.  Notes:  * If you have a lot of training data  you may need to provision extra block storage via EBS. * Also  you may need to spread your data across multiple archives. * You should run this on a `screen` window so it won't terminate once you log out of the SSH session.   You will need a machine with a GPU and CUDA installed. Then pip install the package like this  ```bash $ pip install stylegan2_pytorch ```  If you are using a windows machine  the following commands reportedly works.  ```bash $ conda install pytorch torchvision -c python $ pip install stylegan2_pytorch ```   ```bash $ stylegan2_pytorch --data /path/to/images ```  That's it. Sample images will be saved to `results/default` and models will be saved periodically to `models/default`.   You can specify the name of your project with  ```bash $ stylegan2_pytorch --data /path/to/images --name my-project-name ```  You can also specify the location where intermediate results and model checkpoints should be stored with  ```bash $ stylegan2_pytorch --data /path/to/images --name my-project-name --results_dir /path/to/results/dir --models_dir /path/to/models/dir ```  By default  if the training gets cut off  it will automatically resume from the last checkpointed file. If you want to restart with new settings  just add a `new` flag  ```bash $ stylegan2_pytorch --new --data /path/to/images --name my-project-name --image-size 512 --batch-size 1 --gradient-accumulate-every 16 --network-capacity 10 ```  Once you have finished training  you can generate images from your latest checkpoint like so.  ```bash $ stylegan2_pytorch  --generate ```  To generate a video of a interpolation through two random points in latent space.  ```bash $ stylegan2_pytorch --generate-interpolation ```  To save each individual frame of the interpolation  ```bash $ stylegan2_pytorch --generate-interpolation --save-frames ```  If a previous checkpoint contained a better generator  (which often happens as generators start degrading towards the end of training)  you can load from a previous checkpoint with another flag  ```bash $ stylegan2_pytorch --generate --load-from {checkpoint number} ```   """;General;https://github.com/ShreyasArthur/StyleGAN-2-with-Urban-Plans
"""1. Archive your training data and upload it to an S3 bucket 2. Provision your EC2 instance (I used an Ubuntu AMI) 3. Log into your EC2 instance via SSH 4. Install the aws CLI client and configure it:  ```bash sudo snap install aws-cli --classic aws configure ```  You will then have to enter your AWS access keys  which you can retrieve from the management console under AWS Management Console > Profile > My Security Credentials > Access Keys  Then  run these commands  or maybe put them in a shell script and execute that:  ```bash mkdir data curl -O https://bootstrap.pypa.io/get-pip.py sudo apt-get install python3-distutils python3 get-pip.py pip3 install stylegan2_pytorch export PATH=$PATH:/home/ubuntu/.local/bin aws s3 sync s3://<Your bucket name> ~/data cd data tar -xf ../train.tar.gz ```  Now you should be able to train by simplying calling `stylegan2_pytorch [args]`.  Notes:  * If you have a lot of training data  you may need to provision extra block storage via EBS. * Also  you may need to spread your data across multiple archives. * You should run this on a `screen` window so it won't terminate once you log out of the SSH session.   You will need a machine with a GPU and CUDA installed. Then pip install the package like this  ```bash $ pip install stylegan2_pytorch ```  If you are using a windows machine  the following commands reportedly works.  ```bash $ conda install pytorch torchvision -c python $ pip install stylegan2_pytorch ```   ```bash $ stylegan2_pytorch --data /path/to/images ```  That's it. Sample images will be saved to `results/default` and models will be saved periodically to `models/default`.   You can specify the name of your project with  ```bash $ stylegan2_pytorch --data /path/to/images --name my-project-name ```  You can also specify the location where intermediate results and model checkpoints should be stored with  ```bash $ stylegan2_pytorch --data /path/to/images --name my-project-name --results_dir /path/to/results/dir --models_dir /path/to/models/dir ```  By default  if the training gets cut off  it will automatically resume from the last checkpointed file. If you want to restart with new settings  just add a `new` flag  ```bash $ stylegan2_pytorch --new --data /path/to/images --name my-project-name --image-size 512 --batch-size 1 --gradient-accumulate-every 16 --network-capacity 10 ```  Once you have finished training  you can generate images from your latest checkpoint like so.  ```bash $ stylegan2_pytorch  --generate ```  To generate a video of a interpolation through two random points in latent space.  ```bash $ stylegan2_pytorch --generate-interpolation ```  To save each individual frame of the interpolation  ```bash $ stylegan2_pytorch --generate-interpolation --save-frames ```  If a previous checkpoint contained a better generator  (which often happens as generators start degrading towards the end of training)  you can load from a previous checkpoint with another flag  ```bash $ stylegan2_pytorch --generate --load-from {checkpoint number} ```   """;Computer Vision;https://github.com/ShreyasArthur/StyleGAN-2-with-Urban-Plans
"""""";Computer Vision;https://github.com/jameswang287/Car-Detection
"""Run the following cell to load the packages and dependencies that are going to be useful for your journey!   from matplotlib.pyplot import imshow   You are now ready to implement non-max suppression. The key steps are:    Note: The ""None"" dimension of the output tensors has obviously to be less than max_boxes. Note also that this   WARNING:tensorflow:From D:\ProgramFiles\Anaconda3\envs\tf1cpu\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.  Instructions for updating:   """;Computer Vision;https://github.com/Donvink/Car_Detection_for_Autonomous_Driving
"""The increasing number of cars in cities can cause high volume of traffic  and implies that traffic violations become more critical nowadays in Bangladesh and also around the world. This causes severe destruction of property and more accidents that may endanger the lives of the people. To solve the alarming problem and prevent such unfathomable consequences  traffic violation detection systems are needed. For which the system enforces proper traffic regulations at all times  and apprehend those who does not comply. A traffic violation detection system must be realized in real-time as the authorities track the roads all the time. Hence  traffic enforcers will not only be at ease in implementing safe roads accurately  but also efficiently; as the traffic detection system detects violations faster than humans. This system can detect most common three types of traffic violation in real-time which are signal violation  parking violation and wrong direction violation. A user friendly graphical interface is associated with the system to make it simple for the user to operate the system  monitor traffic and take action against the violations of traffic rules.   1. Log into your ThingSpeak account. 2. Create a channel. 3. Get the API write key for the channel. 4. Paste the value in `sample.py` 5. Add the `analysis.m` file to the channel. 6. Once the script is running on the Pi (directions below) the data can be seen on the ThingSpeak dashboard.    You can download the dataset via the link below.<br><br> <a href=""https://github.com/OlafenwaMoses/Traffic-Net/releases/tag/1.0"" >https://github.com/OlafenwaMoses/Traffic-Net/releases/tag/1.0</a>  <br><br>     <b><a href=""https://github.com/OlafenwaMoses/Traffic-Net/releases/download/1.0/trafficnet_resnet_model_ex-055_acc-0.913750.h5"" >https://github.com/OlafenwaMoses/Traffic-Net/releases/download/1.0/trafficnet_resnet_model_ex-055_acc-0.913750.h5</a></b><br>   Running the experiment or prediction requires that you have Tensorflow  and Keras  OpenCV and ImageAI installed. You can install this dependencies via the commands below.  <br><span><b>- Tensorflow 1.4.0 (and later versions)  </b>      <a href=""https://www.tensorflow.org/install/install_windows"" style=""text-decoration: none;"" > Install</a></span> or install via pip <pre> pip3 install --upgrade tensorflow </pre>   <span><b>- OpenCV  </b>        <a href=""https://pypi.python.org/pypi/opencv-python"" style=""text-decoration: none;"" >Install</a></span> or install via pip <pre> pip3 install opencv-python </pre>   <span><b>- Keras 2.x  </b>     <a href=""https://keras.io/#installation"" style=""text-decoration: none;"" >Install</a></span> or install via pip <pre> pip3 install keras </pre>       <span>      <pre>pip3 install imageai </pre></span> <br><br> <br>   it is likely we installed paradrop-imserve for you. Otherwise  you will  need to install paradrop-imserve or change the IMAGE_SOURCE_URL   To install paradrop-imserve  connect to the node using SSH and run  the following command.  snap install paradrop-imserve   1. `git clone https://github.com/rahatzamancse/EyeTask.git` 2. Install required python dependencies from `requirements.txt` into your python virtual environment. (`pip install -r requirements.txt`) 3. `python main.py`   """;General;https://github.com/rahulchaurasiya1/cautious-waddle
"""The increasing number of cars in cities can cause high volume of traffic  and implies that traffic violations become more critical nowadays in Bangladesh and also around the world. This causes severe destruction of property and more accidents that may endanger the lives of the people. To solve the alarming problem and prevent such unfathomable consequences  traffic violation detection systems are needed. For which the system enforces proper traffic regulations at all times  and apprehend those who does not comply. A traffic violation detection system must be realized in real-time as the authorities track the roads all the time. Hence  traffic enforcers will not only be at ease in implementing safe roads accurately  but also efficiently; as the traffic detection system detects violations faster than humans. This system can detect most common three types of traffic violation in real-time which are signal violation  parking violation and wrong direction violation. A user friendly graphical interface is associated with the system to make it simple for the user to operate the system  monitor traffic and take action against the violations of traffic rules.   1. Log into your ThingSpeak account. 2. Create a channel. 3. Get the API write key for the channel. 4. Paste the value in `sample.py` 5. Add the `analysis.m` file to the channel. 6. Once the script is running on the Pi (directions below) the data can be seen on the ThingSpeak dashboard.    You can download the dataset via the link below.<br><br> <a href=""https://github.com/OlafenwaMoses/Traffic-Net/releases/tag/1.0"" >https://github.com/OlafenwaMoses/Traffic-Net/releases/tag/1.0</a>  <br><br>     <b><a href=""https://github.com/OlafenwaMoses/Traffic-Net/releases/download/1.0/trafficnet_resnet_model_ex-055_acc-0.913750.h5"" >https://github.com/OlafenwaMoses/Traffic-Net/releases/download/1.0/trafficnet_resnet_model_ex-055_acc-0.913750.h5</a></b><br>   Running the experiment or prediction requires that you have Tensorflow  and Keras  OpenCV and ImageAI installed. You can install this dependencies via the commands below.  <br><span><b>- Tensorflow 1.4.0 (and later versions)  </b>      <a href=""https://www.tensorflow.org/install/install_windows"" style=""text-decoration: none;"" > Install</a></span> or install via pip <pre> pip3 install --upgrade tensorflow </pre>   <span><b>- OpenCV  </b>        <a href=""https://pypi.python.org/pypi/opencv-python"" style=""text-decoration: none;"" >Install</a></span> or install via pip <pre> pip3 install opencv-python </pre>   <span><b>- Keras 2.x  </b>     <a href=""https://keras.io/#installation"" style=""text-decoration: none;"" >Install</a></span> or install via pip <pre> pip3 install keras </pre>       <span>      <pre>pip3 install imageai </pre></span> <br><br> <br>   it is likely we installed paradrop-imserve for you. Otherwise  you will  need to install paradrop-imserve or change the IMAGE_SOURCE_URL   To install paradrop-imserve  connect to the node using SSH and run  the following command.  snap install paradrop-imserve   1. `git clone https://github.com/rahatzamancse/EyeTask.git` 2. Install required python dependencies from `requirements.txt` into your python virtual environment. (`pip install -r requirements.txt`) 3. `python main.py`   """;Computer Vision;https://github.com/rahulchaurasiya1/cautious-waddle
"""""";Computer Vision;https://github.com/overshiki/unet-pytorch
"""Following Alex's and Jing's instructions  I armed myself familiar with the following topics:   """;Reinforcement Learning;https://github.com/Wentworth1996/Summer_Intern_Progress
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/Herge/GAN2
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;Computer Vision;https://github.com/Herge/GAN2
"""""";General;https://github.com/myunghakLee/AdversarialTraining_manifoldMixup
"""Borrowed as it is from [pytorch repo](https://github.com/pytorch/examples/tree/master/vae). It is implementation of the paper [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) by Kingma and Welling.   **Usage** ``` python vae.py ... ... ... ====> Epoch: 10 Average loss: 106.3110 ====> Test set loss: 105.5890 ```  **Reconstructed samples**  Some sample reconstructions from the basic VAE (trained for 10 epochs on MNIST)    ![alt text](imgs/vae_recon_sample.png ""Reconstructions from Standard VAE"")  **Generated Samples**  We can also generate some sample digits from the basic VAE by providing random numbers generated from a normal distribution as input.  ![alt text](imgs/vae_sample.png ""Samples from Standard VAE"")   We will use this example as template for rest of the code.   We'll construct the exact same example using using `distributions` package now. We'll need to modify very little code. Notice the changes in `forward` and `loss_function`.   ``` python gaussian_vae.py ... ... ... ====> Epoch: 10 Average loss: 106.3209 ====> Test set loss: 105.6140 ```  **Reconstructed samples**  Some sample reconstructions from the basic VAE (trained for 10 epochs on MNIST)    ![alt text](imgs/gaussian_recon_sample.png ""Reconstructions from Standard VAE"")  **Generated Samples**  ![alt text](imgs/gaussian_vae_sample.png ""Samples from Standard VAE"")    We can make our latent representation bernoulli by using [relaxed bernoulli](https://pytorch.org/docs/stable/_modules/torch/distributions/relaxed_bernoulli.html#RelaxedBernoulli) distribution. The file `binconcrete.py` contains implementation with bottleneck layer of size 20.  ``` python binconcrete.py ... ... ... ====> Epoch: 10 Average loss: 126.6666 ====> Test set loss: 125.3123 ```  **Reconstructed samples**  Some sample reconstructions (trained for 10 epochs on MNIST)    ![alt text](imgs/binconcrete_recon_sample.png ""Reconstructions from Bernoulli VAE"")  **Generated Samples**  ![alt text](imgs/binconcrete_sample.png ""Samples from Bernoulli VAE"")    Similar to the bernoulli example  a more general usecase is when the latent dimension is categorical.    ``` python concrete.py ... ... ... ====> Epoch: 10 Average loss: 110.2161 ====> Test set loss: 109.1930 ```  **Reconstructed samples**  Some sample reconstructions (trained for 10 epochs on MNIST)    ![alt text](imgs/concrete_recon_sample.png ""Reconstructions from Categorical VAE"")  **Generated Samples**  ![alt text](imgs/concrete_sample.png ""Samples from Categorical VAE"")  For more details on relaxed bernoulli or relaxed categorical distributions  please refer to the following papers  [1] ""Categorical Reparameterization with Gumbel-Softmax"" -  Eric Jang  Shixiang Gu  Ben Poole - https://arxiv.org/abs/1611.01144  [2] ""The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables"" -  Chris J. Maddison  Andriy Mnih  Yee Whye Teh - https://arxiv.org/abs/1611.00712 """;Computer Vision;https://github.com/kampta/pytorch-distributions
"""Borrowed as it is from [pytorch repo](https://github.com/pytorch/examples/tree/master/vae). It is implementation of the paper [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) by Kingma and Welling.   **Usage** ``` python vae.py ... ... ... ====> Epoch: 10 Average loss: 106.3110 ====> Test set loss: 105.5890 ```  **Reconstructed samples**  Some sample reconstructions from the basic VAE (trained for 10 epochs on MNIST)    ![alt text](imgs/vae_recon_sample.png ""Reconstructions from Standard VAE"")  **Generated Samples**  We can also generate some sample digits from the basic VAE by providing random numbers generated from a normal distribution as input.  ![alt text](imgs/vae_sample.png ""Samples from Standard VAE"")   We will use this example as template for rest of the code.   We'll construct the exact same example using using `distributions` package now. We'll need to modify very little code. Notice the changes in `forward` and `loss_function`.   ``` python gaussian_vae.py ... ... ... ====> Epoch: 10 Average loss: 106.3209 ====> Test set loss: 105.6140 ```  **Reconstructed samples**  Some sample reconstructions from the basic VAE (trained for 10 epochs on MNIST)    ![alt text](imgs/gaussian_recon_sample.png ""Reconstructions from Standard VAE"")  **Generated Samples**  ![alt text](imgs/gaussian_vae_sample.png ""Samples from Standard VAE"")    We can make our latent representation bernoulli by using [relaxed bernoulli](https://pytorch.org/docs/stable/_modules/torch/distributions/relaxed_bernoulli.html#RelaxedBernoulli) distribution. The file `binconcrete.py` contains implementation with bottleneck layer of size 20.  ``` python binconcrete.py ... ... ... ====> Epoch: 10 Average loss: 126.6666 ====> Test set loss: 125.3123 ```  **Reconstructed samples**  Some sample reconstructions (trained for 10 epochs on MNIST)    ![alt text](imgs/binconcrete_recon_sample.png ""Reconstructions from Bernoulli VAE"")  **Generated Samples**  ![alt text](imgs/binconcrete_sample.png ""Samples from Bernoulli VAE"")    Similar to the bernoulli example  a more general usecase is when the latent dimension is categorical.    ``` python concrete.py ... ... ... ====> Epoch: 10 Average loss: 110.2161 ====> Test set loss: 109.1930 ```  **Reconstructed samples**  Some sample reconstructions (trained for 10 epochs on MNIST)    ![alt text](imgs/concrete_recon_sample.png ""Reconstructions from Categorical VAE"")  **Generated Samples**  ![alt text](imgs/concrete_sample.png ""Samples from Categorical VAE"")  For more details on relaxed bernoulli or relaxed categorical distributions  please refer to the following papers  [1] ""Categorical Reparameterization with Gumbel-Softmax"" -  Eric Jang  Shixiang Gu  Ben Poole - https://arxiv.org/abs/1611.01144  [2] ""The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables"" -  Chris J. Maddison  Andriy Mnih  Yee Whye Teh - https://arxiv.org/abs/1611.00712 """;General;https://github.com/kampta/pytorch-distributions
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/Rahmanzia3/yolo
"""""";Computer Vision;https://github.com/Daividao/semantic-segmentation-deeplearning
"""* First  download a dataset  e.g. apple2orange  ```bash $ bash download_dataset.sh apple2orange ```  * Write the dataset to tfrecords  ```bash $ python3 build_data.py ```  Check `$ python3 build_data.py --help` for more details.   Python 3.6.0   My pretrained models are available at https://github.com/vanhuyz/CycleGAN-TensorFlow/releases   """;Computer Vision;https://github.com/RezisEwig/cyclegan
"""* First  download a dataset  e.g. apple2orange  ```bash $ bash download_dataset.sh apple2orange ```  * Write the dataset to tfrecords  ```bash $ python3 build_data.py ```  Check `$ python3 build_data.py --help` for more details.   Python 3.6.0   My pretrained models are available at https://github.com/vanhuyz/CycleGAN-TensorFlow/releases   """;General;https://github.com/RezisEwig/cyclegan
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/easyadin/Object-Detection-YOLOv4
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   [![Yolo v4](http://img.youtube.com/vi/1_SiUOYUoOI/0.jpg)](https://youtu.be/1_SiUOYUoOI ""Yolo v4"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/Spain2394/darknet
"""""";General;https://github.com/mustafa-yavuz/chatbot-qa
"""One way to build good natural images is by training Generative Adversarial Networks (GANS).  and we can later even reuse parts of the generator and discriminator networks as feature extractors for supervised tasks .DCGANs is actually one of the class of GANs using CNNs architecture for both of its components i.e.  a generator and a discriminator.   ‚Ä¢	DCGAN  On colab notebook use the following commmand:  !git clone link-to-repo   `$ python3 main.py`   """;Computer Vision;https://github.com/shvmshri/DCGAN_Tensorflow
"""""";Computer Vision;https://github.com/AGroupofProbiotocs/YOLO_V2_in_Tensorflow
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/cbritom/GAN
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;Computer Vision;https://github.com/cbritom/GAN
"""You can easily test the output masks on your images via the CLI.   **Note : Use Python 3**  """;Computer Vision;https://github.com/laxmichary/Pytorch-UNet
"""* What   * GANs are based on adversarial training.   * Adversarial training is a basic technique to train generative models (so here primarily models that create new images).   * In an adversarial training one model (G  Generator) generates things (e.g. images). Another model (D  discriminator) sees real things (e.g. real images) as well as fake things (e.g. images from G) and has to learn how to differentiate the two.   * Neural Networks are models that can be trained in an adversarial way (and are the only models discussed here).  * How   * G is a simple neural net (e.g. just one fully connected hidden layer). It takes a vector as input (e.g. 100 dimensions) and produces an image as output.   * D is a simple neural net (e.g. just one fully connected hidden layer). It takes an image as input and produces a quality rating as output (0-1  so sigmoid).   * You need a training set of things to be generated  e.g. images of human faces.   * Let the batch size be B.   * G is trained the following way:     * Create B vectors of 100 random values each  e.g. sampled uniformly from [-1  +1]. (Number of values per components depends on the chosen input size of G.)     * Feed forward the vectors through G to create new images.     * Feed forward the images through D to create ratings.     * Use a cross entropy loss on these ratings. All of these (fake) images should be viewed as label=0 by D. If D gives them label=1  the error will be low (G did a good job).     * Perform a backward pass of the errors through D (without training D). That generates gradients/errors per image and pixel.     * Perform a backward pass of these errors through G to train G.   * D is trained the following way:     * Create B/2 images using G (again  B/2 random vectors  feed forward through G).     * Chose B/2 images from the training set. Real images get label=1.     * Merge the fake and real images to one batch. Fake images get label=0.     * Feed forward the batch through D.     * Measure the error using cross entropy.     * Perform a backward pass with the error through D.   * Train G for one batch  then D for one (or more) batches. Sometimes D can be too slow to catch up with D  then you need more iterations of D per batch of G.  * Results   * Good looking images MNIST-numbers and human faces. (Grayscale  rather homogeneous datasets.)   * Not so good looking images of CIFAR-10. (Color  rather heterogeneous datasets.)   ![Generated Faces](images/Generative_Adversarial_Networks__faces.jpg?raw=true ""Generated Faces"")  *Faces generated by MLP GANs. (Rightmost column shows examples from the training set.)*  -------------------------   Both are trained simultaneously  i.e. one batch for G  then one batch for D  then one batch for G...   """;Computer Vision;https://github.com/ashishmurali/Generative-Adversarial-Network
"""""";General;https://github.com/sainathdevulapalli/Super-Resolution-GAN
"""""";Computer Vision;https://github.com/sainathdevulapalli/Super-Resolution-GAN
"""Python 3   """;Audio;https://github.com/glakshay/Generating-audio-DL
"""Python 3   """;Sequential;https://github.com/glakshay/Generating-audio-DL
"""""";General;https://github.com/krishnakarthi/COVID-19_Prediction
"""""";Computer Vision;https://github.com/krishnakarthi/COVID-19_Prediction
"""DGL should work on  * all Linux distributions no earlier than Ubuntu 16.04 * macOS X * Windows 10  DGL requires Python 3.5 or later.  Right now  DGL works on [PyTorch](https://pytorch.org) 1.2.0+  [MXNet](https://mxnet.apache.org) 1.5.1+  and [TensorFlow](https://tensorflow.org) 2.1.0+.    from dgl.nn.pytorch import GraphConv   - PyTorch  - MXNet   conda install -c dglteam dgl           #: cpu version  conda install -c dglteam dgl-cuda9.0   #: CUDA 9.0  conda install -c dglteam dgl-cuda9.2   #: CUDA 9.2  conda install -c dglteam dgl-cuda10.0  #: CUDA 10.0  conda install -c dglteam dgl-cuda10.1  #: CUDA 10.1  |           | Latest Nightly Build Version  | Stable Version          |   | CPU       | pip install --pre dgl       | pip install dgl       |  | CUDA 9.0  | pip install --pre dgl-cu90  | pip install dgl-cu90  |  | CUDA 9.2  | pip install --pre dgl-cu92  | pip install dgl-cu92  |  | CUDA 10.0 | pip install --pre dgl-cu100 | pip install dgl-cu100 |  | CUDA 10.1 | pip install --pre dgl-cu101 | pip install dgl-cu101 |   """;Graphs;https://github.com/liketheflower/jimmy_dgl
"""*This is my entry into the [Bristol-Myers Squibb Molecular Translation](https://www.kaggle.com/c/bms-molecular-translation)  Kaggle competition. The notebook is publicly available at https://www.kaggle.com/c/bms-molecular-translation/discussion/url.*  Kagglers have coalesced around ""Attention is What You Need"" models  so I ask  *Is attention really all you need?*  This notebook include features to test that out: Enable/Disable CNN text feature extraction before the decoder self-attention; Increase model parameters without harming inference speed using decoder heads in series; and Experiment with my trainable & parallelizable alternative to beam search.  ----  """;Computer Vision;https://github.com/mvenouziou/Project-Attention-Is-What-You-Get
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/Walter-B/bert-20-classes
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   You can download all 24 from here  or individually from the table below:   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/Walter-B/bert-20-classes
"""""";General;https://github.com/adldotori/ResNet
"""""";Computer Vision;https://github.com/adldotori/ResNet
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   3. Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (**Note:** To disable Loss-Window use flag `-dont_show`. If you are using CPU  try `darknet_no_gpu.exe` instead of `darknet.exe`.)   Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/pritish-mahajan/Apricitas
""" [PhysicalDevice(name='/physical_device:CPU:0'  device_type='CPU')  PhysicalDevice(name='/physical_device:GPU:0'  device_type='GPU')]  The tensorboard extension is already loaded. To reload it  use:       int(path.name.split('.')[0]): cv2.imread(str(path))   preprocessed_coco_cache_dir = Path(f'{MSCOCO_PATH}/inception-{PRETRAINED_TRANSFORMER_VERSION}-preprocessed')                                   name='predictions')   """;Natural Language Processing;https://github.com/DylanCope/Image-Captioning-with-Bimodal-Transformers
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/mamromer/yolov3
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/deeplearnproject/darknet
"""Example: ./train --cuda   Example: ./test --cuda   See more under the _output_ directory  __High resolution / Low resolution / Recovered High Resolution__  ![Original doggy](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/41.png) <img src=""https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/41.png"" alt=""Low res doggy"" width=""96"" height=""96""> ![Generated doggy](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/41.png)  ![Original woman](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/38.png) <img src=""https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/38.png"" alt=""Low res woman"" width=""96"" height=""96""> ![Generated woman](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/38.png)  ![Original hair](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/127.png) <img src=""https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/127.png"" alt=""Low res hair"" width=""96"" height=""96""> ![Generated hair](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/127.png)  ![Original sand](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/72.png) <img src=""https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/72.png"" alt=""Low res sand"" width=""96"" height=""96""> ![Generated sand](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/72.png)  """;General;https://github.com/fengye-lu/PyTorch-SRGAN
"""Example: ./train --cuda   Example: ./test --cuda   See more under the _output_ directory  __High resolution / Low resolution / Recovered High Resolution__  ![Original doggy](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/41.png) <img src=""https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/41.png"" alt=""Low res doggy"" width=""96"" height=""96""> ![Generated doggy](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/41.png)  ![Original woman](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/38.png) <img src=""https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/38.png"" alt=""Low res woman"" width=""96"" height=""96""> ![Generated woman](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/38.png)  ![Original hair](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/127.png) <img src=""https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/127.png"" alt=""Low res hair"" width=""96"" height=""96""> ![Generated hair](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/127.png)  ![Original sand](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_real/72.png) <img src=""https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/low_res/72.png"" alt=""Low res sand"" width=""96"" height=""96""> ![Generated sand](https://raw.githubusercontent.com/ai-tor/PyTorchSRGAN/master/output/high_res_fake/72.png)  """;Computer Vision;https://github.com/fengye-lu/PyTorch-SRGAN
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/Gryffindor112358/Arcface
"""""";Computer Vision;https://github.com/seungjunlee96/DeepArtist
"""**Windows portable version**: Simply download and use the latest version from the [Releases](https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases) section.  Otherwise  check [doc/installation.md](doc/installation.md) for instructions on how to build OpenPose from source.     OS: Ubuntu (14  16)  Windows (8  10)  Mac OSX  Nvidia TX2.   CUDA (Nvidia GPU)  OpenCL (AMD GPU)  and CPU-only (no GPU) versions.   Most users do not need the OpenPose C++/Python API  but can simply use the OpenPose Demo:  - **OpenPose Demo**: To easily process images/video/webcam and display/save the results. See [doc/demo_overview.md](doc/demo_overview.md). E.g.  run OpenPose in a video with: ``` #: Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi ```  - **Calibration toolbox**: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See [doc/modules/calibration_module.md](doc/modules/calibration_module.md).  - **OpenPose C++ API**: If you want to read a specific input  and/or add your custom post-processing function  and/or implement your own display/saving  check the C++ API tutorial on [examples/tutorial_api_cpp/](examples/tutorial_api_cpp/) and [doc/library_introduction.md](doc/library_introduction.md). You can create your custom code on [examples/user_code/](examples/user_code/) and quickly compile it with CMake when compiling the whole OpenPose project. Quickly **add your custom code**: See [examples/user_code/README.md](examples/user_code/README.md) for further details.  - **OpenPose Python API**: Analogously to the C++ API  find the tutorial for the Python API on [examples/tutorial_api_python/](examples/tutorial_api_python/).  - **Adding an extra module**: Check [doc/library_add_new_module.md](./doc/library_add_new_module.md).  - **Standalone face or hand detector**:     - **Face** keypoint detection **without body** keypoint detection: If you want to speed it up (but also reduce amount of detected faces)  check the OpenCV-face-detector approach in [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     - **Use your own face/hand detector**: You can use the hand and/or face keypoint detectors with your own face or hand detectors  rather than using the body detector. E.g.  useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     """;General;https://github.com/xar47x/pose
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   pip install tensorflow-gpu==1.14  git clone https://github.com/rolux/stylegan2encoder.git  cd stylegan2encoder   """;General;https://github.com/jdbugs/stylegan2encoder
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   pip install tensorflow-gpu==1.14  git clone https://github.com/rolux/stylegan2encoder.git  cd stylegan2encoder   """;Computer Vision;https://github.com/jdbugs/stylegan2encoder
"""It was created by the following students from INSA Toulouse:   MsPacman on OpenAI Gym: https://gym.openai.com/envs/MsPacman-v0/   """;Reinforcement Learning;https://github.com/PaulCharnay/Projet_AIF
"""It was created by the following students from INSA Toulouse:   MsPacman on OpenAI Gym: https://gym.openai.com/envs/MsPacman-v0/   """;General;https://github.com/PaulCharnay/Projet_AIF
"""Install PyTorch 0.4   The codebase is now PyTorch 0.4 compatible for most use cases (a big shoutout to https://github.com/shawntan for a fairly comprehensive PR https://github.com/salesforce/awd-lstm-lm/pull/43). Mild readjustments to hyperparameters may be necessary to obtain quoted performance. If you desire exact reproducibility (or wish to run on PyTorch 0.3 or lower)  we suggest using an older commit of this repository. We are still working on pointer  finetune and generate functionalities.   For data setup  run ./getdata.sh.   PyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed.   """;Sequential;https://github.com/SachinIchake/KALM
"""Install PyTorch 0.4   The codebase is now PyTorch 0.4 compatible for most use cases (a big shoutout to https://github.com/shawntan for a fairly comprehensive PR https://github.com/salesforce/awd-lstm-lm/pull/43). Mild readjustments to hyperparameters may be necessary to obtain quoted performance. If you desire exact reproducibility (or wish to run on PyTorch 0.3 or lower)  we suggest using an older commit of this repository. We are still working on pointer  finetune and generate functionalities.   For data setup  run ./getdata.sh.   PyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed.   """;General;https://github.com/SachinIchake/KALM
"""Install PyTorch 0.4   The codebase is now PyTorch 0.4 compatible for most use cases (a big shoutout to https://github.com/shawntan for a fairly comprehensive PR https://github.com/salesforce/awd-lstm-lm/pull/43). Mild readjustments to hyperparameters may be necessary to obtain quoted performance. If you desire exact reproducibility (or wish to run on PyTorch 0.3 or lower)  we suggest using an older commit of this repository. We are still working on pointer  finetune and generate functionalities.   For data setup  run ./getdata.sh.   PyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed.   """;Natural Language Processing;https://github.com/SachinIchake/KALM
"""``` git clone https://github.com/songrotek/DDPG.git cd DDPG python gym_ddpg.py  ``` If you want to change the Gym environment  change ENV_NAME in gym_ddpg.py.  If you want to change the Network type  change import in ddpg.py such as   ``` from actor_network_bn import ActorNetwork to from actor_network import ActorNetwork ```   """;Reinforcement Learning;https://github.com/YoUNG824/DDPG
"""""";General;https://github.com/zhusiling/Pytorch-Encoding-boundary
"""conda 4.7.12  python 3.7.4   """;Computer Vision;https://github.com/elisiojsj/Kuzushiji-49
"""Clone this repo to your machine.  Create folder `datasets` and put the dataset you want into the folder `datasets`.  Notice the structure under `datasets` is as follows:  ``` vangogh2photo ‚îÇ ‚îî‚îÄ‚îÄ‚îÄTrainA ‚îÇ   ‚îÇ    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄApple_train ‚îÇ       ‚îÇ   pic1.png ‚îÇ       ‚îÇ   pic2.png ‚îÇ       ‚îÇ   ... ‚îÇ     ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄTrainB ‚îÇ   ‚îÇ    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄOrange_train ‚îÇ       ‚îÇ   pic1.png ‚îÇ       ‚îÇ   pic2.png ‚îÇ       ‚îÇ   ... ‚îÇ     ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄTestA ‚îÇ   ‚îÇ    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄApple_test ‚îÇ       ‚îÇ   pic1.png ‚îÇ       ‚îÇ   pic2.png ‚îÇ       ‚îÇ   ... ‚îÇ     ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄTestB     ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄOrange_test         ‚îÇ   pic1.png         ‚îÇ   pic2.png         ‚îÇ   ... ```  An example modified dataset can be downloaded [here](https://drive.google.com/open?id=1-t9Q2kMwcPxdUe-v6Gy_Kg3LaP68F27K)  Then run the following code in terminal to train:  `python main.py --epochs 200 --decay_epoch 100 --batch_size 2 --training True --testing True --data_name apple2orange`  If you only want to test:  `python main.py --test_batch_size 1 --testing True --data_name apple2orange`  If you want to do Monet  you should add identity loss  which needs extra arguments: `--use_id_loss True`  For more information  check `main.py` or run the following code:  `python main.py -h`   """;Computer Vision;https://github.com/YiteWang/CS547-final-project
"""Clone this repo to your machine.  Create folder `datasets` and put the dataset you want into the folder `datasets`.  Notice the structure under `datasets` is as follows:  ``` vangogh2photo ‚îÇ ‚îî‚îÄ‚îÄ‚îÄTrainA ‚îÇ   ‚îÇ    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄApple_train ‚îÇ       ‚îÇ   pic1.png ‚îÇ       ‚îÇ   pic2.png ‚îÇ       ‚îÇ   ... ‚îÇ     ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄTrainB ‚îÇ   ‚îÇ    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄOrange_train ‚îÇ       ‚îÇ   pic1.png ‚îÇ       ‚îÇ   pic2.png ‚îÇ       ‚îÇ   ... ‚îÇ     ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄTestA ‚îÇ   ‚îÇ    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄApple_test ‚îÇ       ‚îÇ   pic1.png ‚îÇ       ‚îÇ   pic2.png ‚îÇ       ‚îÇ   ... ‚îÇ     ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄTestB     ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄOrange_test         ‚îÇ   pic1.png         ‚îÇ   pic2.png         ‚îÇ   ... ```  An example modified dataset can be downloaded [here](https://drive.google.com/open?id=1-t9Q2kMwcPxdUe-v6Gy_Kg3LaP68F27K)  Then run the following code in terminal to train:  `python main.py --epochs 200 --decay_epoch 100 --batch_size 2 --training True --testing True --data_name apple2orange`  If you only want to test:  `python main.py --test_batch_size 1 --testing True --data_name apple2orange`  If you want to do Monet  you should add identity loss  which needs extra arguments: `--use_id_loss True`  For more information  check `main.py` or run the following code:  `python main.py -h`   """;General;https://github.com/YiteWang/CS547-final-project
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/malin9402/retiface
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/agutuyen-dev/darknet
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/agutuyen-dev/darknet
"""""";General;https://github.com/anktplwl91/Image-Superresolution
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/jinzhenfan/BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/jinzhenfan/BERT
"""""";Computer Vision;https://github.com/AzogDefiler/Sandbox
"""- Software: Ubuntu 16.04.3 LTS  CUDA>=8.0  Python>=3.5  PyTorch>=0.4.0  - Dependencies: numpy  scipy  opencv  yacs  tqdm   chmod +x download_ADE20K.sh   For example  you can start with our provided configurations:    1. Here is a simple demo to do inference on a single image: ```bash chmod +x demo_test.sh ./demo_test.sh ``` This script downloads a trained model (ResNet50dilated + PPM_deepsup) and a test image  runs the test script  and saves predicted segmentation (.png) to the working directory.  2. To test on an image or a folder of images (```$PATH_IMG```)  you can simply do the following: ``` python3 -u test.py --imgs $PATH_IMG --gpu $GPU --cfg $CFG ```   """;Computer Vision;https://github.com/fenglian425/Agriculture_AI
"""""";Natural Language Processing;https://github.com/chaneeh/Word2Vec_implementation
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   Install <a href=""https://www.tensorflow.org/get_started/os_setup"" target=""_blank"">TensorFlow</a>. You may also need to install h5py. The code has been tested with Python 2.7  TensorFlow 1.0.1  CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04.  If you are using PyTorch  you can find a third-party pytorch implementation <a href=""https://github.com/fxia22/pointnet.pytorch"" target=""_blank"">here</a>.  To install h5py for Python: ```bash sudo apt-get install libhdf5-dev sudo pip install h5py ```   Created by <a href=""http://charlesrqi.com"" target=""_blank"">Charles R. Qi</a>  <a href=""http://ai.stanford.edu/~haosu/"" target=""_blank"">Hao Su</a>  <a href=""http://cs.stanford.edu/~kaichun/"" target=""_blank"">Kaichun Mo</a>  <a href=""http://geometry.stanford.edu/member/guibas/"" target=""_blank"">Leonidas J. Guibas</a> from Stanford University.  ![prediction example](https://github.com/charlesq34/pointnet/blob/master/doc/teaser.png)   cd part_seg  sh download_data.sh   To train a model to classify point clouds sampled from 3D shapes:      python train.py  Log files and network parameters will be saved to `log` folder in default. Point clouds of <a href=""http://modelnet.cs.princeton.edu/"" target=""_blank"">ModelNet40</a> models in HDF5 files will be automatically downloaded (416MB) to the data folder. Each point cloud contains 2048 points uniformly sampled from a shape surface. Each cloud is zero-mean and normalized into an unit sphere. There are also text files in `data/modelnet40_ply_hdf5_2048` specifying the ids of shapes in h5 files.  To see HELP for the training script:      python train.py -h  We can use TensorBoard to view the network architecture and monitor the training progress.      tensorboard --logdir log  After the above training  we can evaluate the model and output some visualizations of the error cases.      python evaluate.py --visu  Point clouds that are wrongly classified will be saved to `dump` folder in default. We visualize the point cloud by rendering it into three-view images.  If you'd like to prepare your own data  you can refer to some helper functions in `utils/data_prep_util.py` for saving and loading HDF5 files.   * <a href=""http://stanford.edu/~rqi/pointnet2/"" target=""_blank"">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a> by Qi et al. (NIPS 2017) A hierarchical feature learning framework on point clouds. The PointNet++ architecture applies PointNet recursively on a nested partitioning of the input point set. It also proposes novel layers for point clouds with non-uniform densities. * <a href=""http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w13/Engelmann_Exploring_Spatial_Context_ICCV_2017_paper.pdf"" target=""_blank"">Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</a> by Engelmann et al. (ICCV 2017 workshop). This work extends PointNet for large-scale scene segmentation. * <a href=""https://arxiv.org/abs/1710.04954"" target=""_blank"">PCPNET: Learning Local Shape Properties from Raw Point Clouds</a> by Guerrero et al. (arXiv). The work adapts PointNet for local geometric properties (e.g. normal and curvature) estimation in noisy point clouds. * <a href=""https://arxiv.org/abs/1711.06396"" target=""_blank"">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a> by Zhou et al. from Apple (arXiv) This work studies 3D object detection using LiDAR point clouds. It splits space into voxels  use PointNet to learn local voxel features and then use 3D CNN for region proposal  object classification and 3D bounding box estimation. * <a href=""https://arxiv.org/abs/1711.08488"" target=""_blank"">Frustum PointNets for 3D Object Detection from RGB-D Data</a> by Qi et al. (arXiv) A novel framework for 3D object detection with RGB-D data. The method proposed has achieved first place on KITTI 3D object detection benchmark on all categories (last checked on 11/30/2017).  """;Computer Vision;https://github.com/aviros/roatationPointnet
"""""";General;https://github.com/sayred1/CNN
"""To train and evaluate a biLM  you need to provide:  * a vocabulary file * a set of training files * a set of heldout files  The vocabulary file is a a text file with one token per line.  It must also include the special tokens `<S>`  `</S>` and `<UNK>` (case sensitive) in the file.  <i>IMPORTANT</i>: the vocabulary file should be sorted in descending order by token count in your training data.  The first three lines should be the special tokens (`<S>`  `</S>` and `<UNK>`)  then the most common token in the training data  ending with the least common token.  <i>NOTE</i>: the vocabulary file used in training may differ from the one use for prediction.  The training data should be randomly split into many training files  each containing one slice of the data.  Each file contains pre-tokenized and white space separated text  one sentence per line. Don't include the `<S>` or `</S>` tokens in your training data.  All tokenization/normalization is done before training a model  so both the vocabulary file and training files should include normalized tokens. As the default settings use a fully character based token representation  in general we do not recommend any normalization other then tokenization.  Finally  reserve a small amount of the training data as heldout data for evaluating the trained biLM.   To run the image  you must use nvidia-docker  because this repository requires GPUs. ``` sudo nvidia-docker run -t allennlp/bilm-tf:training-gpu ```   Install python version 3.5 or later  tensorflow version 1.2 and h5py:  ``` pip install tensorflow-gpu==1.2 h5py python setup.py install ```  Ensure the tests pass in your environment by running: ``` python -m unittest discover tests/ ```   You may also find it easier to use the version provided in Tensorflow Hub if you just like to make predictions.   See the instructions above for using the output from Step #4 in downstream models.   As a result of the training method (see above)  the LSTMs are stateful  and carry their state forward from batch to batch. Consequently  this introduces a small amount of non-determinism  expecially for the first two batches.   Simple methods like average and max pooling of the word level ELMo representations across sentences works well  often outperforming supervised methods on benchmark datasets. See ""Evaluation of sentence embeddings in downstream and linguistic probing tasks""  Perone et al  2018 [arxiv link](https://arxiv.org/abs/1806.06259).    """;Natural Language Processing;https://github.com/young-zonglin/bilm-tf-extended
"""""";General;https://github.com/gan3sh500/dropblock
"""""";Computer Vision;https://github.com/Brunogomes97/Imdb
"""``` mkdir -p <CornerNet-Lite dir>/data cd <CornerNet-Lite dir>/data git clone git@github.com:cocodataset/cocoapi.git coco cd <CornerNet-Lite dir>/data/coco/PythonAPI make install ```   Please first install [Anaconda](https://anaconda.org) and create an Anaconda environment using the provided package list `conda_packagelist.txt`. ``` conda create --name CornerNet_Lite --file conda_packagelist.txt --channel pytorch ```  After you create the environment  please activate it. ``` source activate CornerNet_Lite ```   Compile the C++ implementation of the corner pooling layers. (GCC4.9.2 or above is required.)   python setup.py install --user   If you want to train or evaluate the detectors on COCO  please move on to the following steps.   After downloading the models  you should be able to use the detectors on your own images. We provide a demo script `demo.py` to test if the repo is installed correctly. ``` python demo.py ``` This script applies CornerNet-Saccade to `demo.jpg` and writes the results to `demo_out.jpg`.  In the demo script  the default detector is CornerNet-Saccade. You can modify the demo script to test different detectors. For example  if you want to test CornerNet-Squeeze: ```python #:!/usr/bin/env python  import cv2 from core.detectors import CornerNet_Squeeze from core.vis_utils import draw_bboxes  detector = CornerNet_Squeeze() image    = cv2.imread(""demo.jpg"")  bboxes = detector(image) image  = draw_bboxes(image  bboxes) cv2.imwrite(""demo_out.jpg""  image) ```   """;Computer Vision;https://github.com/Arno3165229/CornerNet_Traffic_Light
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla T4'  total_memory=15079MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/jih189/yolo3
"""Clone the master branch of the respository using git clone -b master --single-branch https://github.com/richzhang/colorization.git   The provided scripts run representation learning tests. Note that the scripts run on release models. Modify scripts accordingly if you want to test your own trained model.   We also include demo usage as an iPython notebook  under [`./demo/colorization_demo_v2.ipynb`](https://github.com/richzhang/colorization/blob/master/demo/colorization_demo_v2.ipynb). This IPython Notebook demonstrates how to use our colorization network to colorize a grayscale image. To run this  after cloning the directory  `cd` into the `demo` directory  run `ipython notebook` and open `colorization_demo_v2.ipynb` in your web browser.   The following contains instructions for training a colorization network from scratch. After cloning the repository  from the root directory:  (1) Run `./train/fetch_init_model.sh`. This will load model `./models/init_v2.caffemodel`. This model was obtained using the k-means initialization implemented in [Kraehenbuehl et al  ICLR 2016](https://github.com/philkr/magic_init).  (2) Run `./train/fetch_caffe.sh`. This will load a modified Caffe into directory `./caffe-colorization`. For guidelines and help with installation of Caffe  consult the [installation guide](http://caffe.berkeleyvision.org/) and [Caffe users group](https://groups.google.com/forum/#!forum/caffe-users).  (3) Add the `./resources/` directory (as an absolute path) to your system environment variable $PYTHONPATH. This directory contains custom Python layers.  (4) Modify paths in data layers `./models/colorization_train_val_v2.prototxt` to locate where ImageNet LMDB files are on your machine. These should be BGR images  non-mean centered  in [0 255].  (5) Run `./train/train_model.sh [GPU_ID]`  where `[GPU_ID]` is the gpu you choose to specify. Notes about training:  (a) Training completes around 450k iterations. Training is done on mirrored and randomly cropped 176x176 resolution images  with mini-batch size 40.  (b) Snapshots every 1000 iterations will be saved in `./train/models/colornet_iter_[ITERNUMBER].caffemodel` and `./train/models/colornet_iter_[ITERNUMBER].solverstate`.  (c) If training is interupted  resume training by running `./train/train_resume.sh ./train/models/colornet_iter_[ITERNUMBER].solverstate [GPU_ID]`  where `[ITERNUMBER]` is the last snapshotted model.  (d) Check validation loss by running `./val_model.sh ./train/models/colornet_iter_[ITERNUMBER].caffemodel [GPU_ID] 1000`  where [ITERNUMBER] is the model you would like to validate. This runs the first 10k imagenet validation images at full 256x256 resolution through the model. Validation loss on `colorization_release_v2.caffemodel` is 7715.  (e) Check model outputs by running the IPython notebook demo. Replace the release model with your snapshotted model.  (f) To download reference pre-trained model  run `./models/fetch_release_models.sh`. This will load reference model `./models/colorization_release_v2.caffemodel`. This model used to generate results in the [ECCV 2016 camera ready](arxiv.org/pdf/1603.08511.pdf).  For completeness  this will also load model `./models/colorization_release_v2_norebal.caffemodel`  which is was trained without class rebalancing. This model will provide duller but ""safer"" colorizations. This will also load model `./models/colorization_release_v1.caffemodel`  which was used to generate the results in the [arXiv v1](arxiv.org/pdf/1603.08511v1.pdf) paper.   """;General;https://github.com/radiabouhali/test
"""note that  like self-attention  the weights are changing per timestep   """;Sequential;https://github.com/dyunis/light_dynamic_conv
"""Darknet is an open source neural network framework written in C and CUDA. It is fast  easy to install  and supports CPU and GPU computation.  Yolo v4 paper: https://arxiv.org/abs/2004.10934  Yolo v4 source code: https://github.com/AlexeyAB/darknet  For more information see the [Darknet project website](http://pjreddie.com/darknet).  For questions or issues please use the [Google Group](https://groups.google.com/forum/#!forum/darknet).   Example : darknet.exe detector recall data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights   6)Increasing the number of anchors   increases the avg IOU and possibility to detect the small classes.But remember to change the masks.  """;Computer Vision;https://github.com/vamshi4067/Anomaly-detection-using-computer-vision
"""""";Computer Vision;https://github.com/sidify/areal_image_segmentation
"""""";General;https://github.com/jameswang287/Car-Detection
""" * Download aligned&cropped celebA dataset([link](https://www.dropbox.com/sh/8oqt9vytwxb3s4r/AADSNUu0bseoCKuxuI5ZeTl1a/Img?dl=0)) and unzip at ./data/img_align_celeba  * Train:    ```   $ python main.py --train   ```    Or you can set some arguments like:    ```   $ python main.py --dataset=celebA --max_epoch=50 --learning_rate=1e-4 --train   ```  * Test:    ```   $ python main.py   ```   """;General;https://github.com/changwoolee/WGAN-GP-tensorflow
""" * Download aligned&cropped celebA dataset([link](https://www.dropbox.com/sh/8oqt9vytwxb3s4r/AADSNUu0bseoCKuxuI5ZeTl1a/Img?dl=0)) and unzip at ./data/img_align_celeba  * Train:    ```   $ python main.py --train   ```    Or you can set some arguments like:    ```   $ python main.py --dataset=celebA --max_epoch=50 --learning_rate=1e-4 --train   ```  * Test:    ```   $ python main.py   ```   """;Computer Vision;https://github.com/changwoolee/WGAN-GP-tensorflow
"""Code runs on a single GPU and has been tested with  Python 3.7.2   """;Computer Vision;https://github.com/fmu2/realNVP
"""Please install latest version of TensorFlow (r1.6) and use Python 3.   - Download and extract  [PASCAL VOC training/validation data](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar)  (2GB tar file)  specifying the location with the `--data_dir`.   - Download and extract  [augmented segmentation data](https://www.dropbox.com/s/oeu149j8qtbs1x0/SegmentationClassAug.zip?dl=0)  (Thanks to DrSleep)  specifying the location with `--data_dir` and `--label_data_dir` (namely  `$data_dir/$label_data_dir`).   - For inference the trained model with `76.42%` mIoU on the Pascal VOC 2012 validation dataset  is available  [here](https://www.dropbox.com/s/gzwb0d6ydpfoxoa/deeplabv3_ver1.tar.gz?dl=0). Download and extract to  `--model_dir`. - For training  you need to download and extract  [pre-trained Resnet v2 101 model](http://download.tensorflow.org/models/resnet_v2_101_2017_04_14.tar.gz) from [slim](https://github.com/tensorflow/models/tree/master/research/slim) specifying the location with `--pre_trained_model`.   You can see other options with the following command:   To apply semantic segmentation to your images  one can use the following commands:   - [ ] Multi-GPU support   """;Computer Vision;https://github.com/rishizek/tensorflow-deeplab-v3
"""The code is based on Python 3.5  TensorFlow 1.15  and Spektral 0.1.2.  All required libraries are listed in `requirements.txt` and can be installed with  ```bash pip install -r requirements.txt ```    """;Graphs;https://github.com/FilippoMB/Spectral-Clustering-with-Graph-Neural-Networks-for-Graph-Pooling
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`  2. Download VOC2007 and VOC2012 dataset. By default  we assume the data is stored in `$HOME/data/`   ```Shell   #: Download the data.   cd $HOME/data   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar   #: Extract the data.   tar -xvf VOCtrainval_11-May-2012.tar   tar -xvf VOCtrainval_06-Nov-2007.tar   tar -xvf VOCtest_06-Nov-2007.tar   ```  3. Create the LMDB file.   ```Shell   cd $CAFFE_ROOT   #: Create the trainval.txt  test.txt  and test_name_size.txt in data/VOC0712/   ./data/VOC0712/create_list.sh   #: You can modify the parameters in create_data.sh if needed.   #: It will create lmdb files for trainval and test with encoded original image:   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb   #: and make soft links at examples/VOC0712/   ./data/VOC0712/create_data.sh   ```   1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`   ```Shell   git clone https://github.com/weiliu89/caffe.git   cd caffe   git checkout ssd   ```  2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.   ```Shell   #: Modify Makefile.config according to your Caffe installation.   cp Makefile.config.example Makefile.config   make -j8   #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.   make py   make test -j8   #: (Optional)   make runtest -j8   ```   COCO<sup>[1]</sup>: SSD300*  SSD512*  07+12+COCO: SSD300*  SSD512*  07++12+COCO: SSD300*  SSD512*  COCO models:   """;Computer Vision;https://github.com/shenyunhang/caffe-fwsl
"""PyTorch (tested on v1.7.1)  Python 3.7+ & dependencies (requirements.txt)    pip install -r requirements.txt   * Python 3.7+ & dependencies (requirements.txt)   For now you need to install the Python dependencies specified in requirements.txt (look above)   """;General;https://github.com/lukas-blecher/LaTeX-OCR
"""PyTorch (tested on v1.7.1)  Python 3.7+ & dependencies (requirements.txt)    pip install -r requirements.txt   * Python 3.7+ & dependencies (requirements.txt)   For now you need to install the Python dependencies specified in requirements.txt (look above)   """;Natural Language Processing;https://github.com/lukas-blecher/LaTeX-OCR
"""PyTorch (tested on v1.7.1)  Python 3.7+ & dependencies (requirements.txt)    pip install -r requirements.txt   * Python 3.7+ & dependencies (requirements.txt)   For now you need to install the Python dependencies specified in requirements.txt (look above)   """;Computer Vision;https://github.com/lukas-blecher/LaTeX-OCR
"""""";Sequential;https://github.com/henrysteinitz/neural-memory
"""Usage: similar to torch.nn.ReLU()...and torch.autograd.Function  ``` from swish import Swish from mish import Mish self.conv1 = nn.Sequential(                             nn.Linear(256  width)                              Swish()                              nn.BatchNorm1d(width)                              nn.Linear(width  1)                           )  self.conv2 = nn.Sequential(                             nn.Linear(256  width)                              Mish()                              nn.BatchNorm1d(width)                              nn.Linear(width  1)                           ) ```   """;General;https://github.com/tyunist/memory_efficient_mish_swish
"""Run on GPU. The current Fawkes package and binary does not support GPU. To use GPU  you need to clone this repo  install    the required packages in setup.py  and replace tensorflow with tensorflow-gpu. Then you can run Fawkes   We are actively working on this. Python scripts that can test the protection effectiveness will be ready shortly.   Install from PyPI:  pip install fawkes  If you don't have root privilege  please try to install on user namespace: pip install --user fawkes.   following guide to test Fawkes.   `fawkes -d ./imgs --mode low`  or `python3 protection.py -d ./imgs --mode low`    """;Computer Vision;https://github.com/Shawn-Shan/fawkes
"""""";Reinforcement Learning;https://github.com/madhur-tandon/RL-Project
"""We first prepare the monolingual and bilingual sentences for Chinese and English respectively. The data directory looks like:  ``` - data/   ‚îú‚îÄ mono/   |  ‚îú‚îÄ train.en   |  ‚îú‚îÄ train.zh   |  ‚îú‚îÄ valid.en   |  ‚îú‚îÄ valid.zh   |  ‚îú‚îÄ dict.en.txt   |  ‚îî‚îÄ dict.zh.txt   ‚îî‚îÄ para/      ‚îú‚îÄ train.en      ‚îú‚îÄ train.zh      ‚îú‚îÄ valid.en      ‚îú‚îÄ valid.zh      ‚îú‚îÄ dict.en.txt      ‚îî‚îÄ dict.zh.txt ``` The files under `mono` are monolingual data  while under `para` are bilingual data. `dict.en(zh).txt` in different directory should be identical. The dictionary for different language can be different. Running the following command can generate the binarized data:  ``` #: Ensure the output directory exists data_dir=data/ mono_data_dir=$data_dir/mono/ para_data_dir=$data_dir/para/ save_dir=$data_dir/processed/  #: set this relative path of MASS in your server user_dir=mass  mkdir -p $data_dir $save_dir $mono_data_dir $para_data_dir   #: Generate Monolingual Data for lg in en zh do    fairseq-preprocess \   --task cross_lingual_lm \   --srcdict $mono_data_dir/dict.$lg.txt \   --only-source \   --trainpref $mono_data_dir/train --validpref $mono_data_dir/valid \   --destdir $save_dir \   --workers 20 \   --source-lang $lg    #: Since we only have a source language  the output file has a None for the   #: target language. Remove this    for stage in train valid   do     mv $save_dir/$stage.$lg-None.$lg.bin $save_dir/$stage.$lg.bin     mv $save_dir/$stage.$lg-None.$lg.idx $save_dir/$stage.$lg.idx   done done  #: Generate Bilingual Data fairseq-preprocess \   --user-dir $mass_dir \   --task xmasked_seq2seq \   --source-lang en --target-lang zh \   --trainpref $para_data_dir/train --validpref $para_data_dir/valid \   --destdir $save_dir \   --srcdict $para_data_dir/dict.en.txt \   --tgtdict $para_data_dir/dict.zh.txt ```    We use the same BPE codes and vocabulary with XLM. Here we take English-French as an example.  ``` cd MASS  wget https://dl.fbaipublicfiles.com/XLM/codes_enfr wget https://dl.fbaipublicfiles.com/XLM/vocab_enfr  ./get-data-nmt.sh --src en --tgt fr --reload_codes codes_enfr --reload_vocab vocab_enfr ```   Download dataset by the below command:   mkdir -p $save_dir       --source-langs en zh \   mkdir -p $save_dir   mkdir -p mono   wget -c https://modelrelease.blob.core.windows.net/mass/mass-base-uncased.tar.gz  tar -zxvf mass-base-uncased.tar.gz   """;Natural Language Processing;https://github.com/microsoft/MASS
"""You can clone this repository by:  ```bash git clone https://github.com/atavakol/action-branching-agents.git ```     """;Reinforcement Learning;https://github.com/atavakol/action-branching-agents
"""""";General;https://github.com/jojonki/MemoryNetworks
"""The aegd.py file provides a PyTorch implementation of AEGD   ```python3 optimizer = aegd.AEGD(model.parameters()  lr=0.1) ``` AEGD with decoupled weight decay (AEGDW) can be constructed by setting `aegdw=True`. ```python3 optimizer = aegd.AEGD(model.parameters()  lr=0.1  aegdw=True) ``` Learn more about `decouple weight decay` at [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)    We test AEGD(W) on the standard CIFAR-10 and CIFAR-100 image classification datasets  comparing with several baseline methods including: SGD with momentum (SGDM)  Adam and AdamW. The implementation is highly based on [this repository](https://github.com/Luolc/AdaBound). We also provide a [notebook](./visualization.ipynb) to present our results for this example.  Supported models for CIFAR-10 are ResNet  DenseNet and CifarNet   for CIFAR-100 are SqueezeNet and GoogleNet. A weight decay of `1e-4` is applied to all the optimizers. The initial set of step size for each optimizer are:  * SGDM: {0.05  0.1  0.2  0.3} * Adam: {1e-4  3e-4  5e-4  1e-3  2e-3} * AdamW: {5e-4  1e-3  3e-3  5e-3} * AEGD: {0.1  0.2  0.3  0.4} * AEGDW: {0.6  0.7  0.8  0.9}  We note that the above setting for initial step size is calibrated for training complex deep networks. In general  suitable step sizes for AEGD(W) are slightly larger than those for SGDM. The best initial step size for each method in a certain task are given in respective plots in our paper to ease your reproduction.  Followings are examples to train ResNet-56 on CIFAR-10 using AEGD with a learning rate of 0.3  ```bash python cifar.py --dataset cifar10 --model resnet56 --optim AEGD --lr 0.3 ``` and train SqueezeNet on CIFAR-100 using AEGDW with a learning rate of 0.9 ```bash python cifar.py --dataset cifar100 --model squeezenet --optim AEGDW --lr 0.9 ``` The checkpoints will be saved in the `checkpoint` folder and the data points of the learning curve will be saved in the `curve` folder.    """;General;https://github.com/txping/AEGD
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/Gaozhen0816/BERT_QA_for_Chinese
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/Gaozhen0816/BERT_QA_for_Chinese
"""""";Computer Vision;https://github.com/shalabh147/DCGan
"""-   Acquire the data  e.g. as a snapshot called `256x256.zip` in [my data repository][data-repository]  -   Run [`StyleGAN2_ADA_training.ipynb`][colab-notebook-training] to train a StyleGAN2-ADA model from scratch. [![Open In Colab][colab-badge]][colab-notebook-training] -   Run [`StyleGAN2_ADA_image_sampling.ipynb`][colab-notebook-sampling] to generate images with a trained StyleGAN2-ADA model  [![Open In Colab][colab-badge]][colab-notebook-sampling] -   To automatically resume training from the latest checkpoint  you will have to use [my fork][stylegan2-ada-fork] of StyleGAN2-ADA.   """;Computer Vision;https://github.com/woctezuma/steam-stylegan2-ada
"""-   Acquire the data  e.g. as a snapshot called `256x256.zip` in [my data repository][data-repository]  -   Run [`StyleGAN2_ADA_training.ipynb`][colab-notebook-training] to train a StyleGAN2-ADA model from scratch. [![Open In Colab][colab-badge]][colab-notebook-training] -   Run [`StyleGAN2_ADA_image_sampling.ipynb`][colab-notebook-sampling] to generate images with a trained StyleGAN2-ADA model  [![Open In Colab][colab-badge]][colab-notebook-sampling] -   To automatically resume training from the latest checkpoint  you will have to use [my fork][stylegan2-ada-fork] of StyleGAN2-ADA.   """;General;https://github.com/woctezuma/steam-stylegan2-ada
"""For optical flow extraction and video list generation  please refer to [TSN](https://github.com/yjxiong/temporal-segment-networks#code--data-preparation) for details.   For example  command to train models with RGB modality on UCF101 can be   Use the following command to test its performance:   """;General;https://github.com/PPPrior/i3d-pytorch
"""For optical flow extraction and video list generation  please refer to [TSN](https://github.com/yjxiong/temporal-segment-networks#code--data-preparation) for details.   For example  command to train models with RGB modality on UCF101 can be   Use the following command to test its performance:   """;Computer Vision;https://github.com/PPPrior/i3d-pytorch
"""Modify from: https://github.com/facebookresearch/moco  Use Multi-GPU with Pytorch:    """;General;https://github.com/AndrewTal/MoCo-Pytorch
"""The codebases are built on top of [Detectron2](https://github.com/facebookresearch/detectron2) and [DETR](https://github.com/facebookresearch/detr).   Install and build libs  git clone https://github.com/PeizeSun/SparseR-CNN.git  cd SparseR-CNN  python setup.py build develop  Link coco dataset path to SparseR-CNN/datasets/coco  mkdir -p datasets/coco   """;Computer Vision;https://github.com/PeizeSun/SparseR-CNN
"""Download Windows/Linux/MacOS Executable for Intel/AMD/Nvidia GPU  This package includes all the binaries and models required. It is portable  so no CUDA or PyTorch runtime environment is needed :)   mkdir input_frames  mkdir output_frames   ffmpeg -framerate 48 -i output_frames/%08d.png -i audio.m4a -c:a copy -crf 20 -c:v libx264 -pix_fmt yuv420p output.mp4   Download and setup the Vulkan SDK from https://vulkan.lunarg.com/  For Linux distributions  you can either get the essential build requirements from package manager   dnf install vulkan-headers vulkan-loader-devel   apt-get install libvulkan-dev   git clone https://github.com/nihui/rife-ncnn-vulkan.git  cd rife-ncnn-vulkan  git submodule update --init --recursive  Build with CMake  You can pass -DUSE_STATIC_MOLTENVK=ON option to avoid linking the vulkan loader library on MacOS   mkdir build  cd build   cmake --build . -j 4   Input two frame images  output one interpolated frame image.   ```shell ./rife-ncnn-vulkan -0 0.jpg -1 1.jpg -o 01.jpg ./rife-ncnn-vulkan -i input_frames/ -o output_frames/ ```  Example below runs on CPU  Discrete GPU  and Integrated GPU all at the same time. Uses 2 threads for image decoding  4 threads for one CPU worker  4 threads for another CPU worker  2 threads for discrete GPU  1 thread for integrated GPU  and 4 threads for image encoding. ```shell ./rife-ncnn-vulkan -i input_frames/ -o output_frames/ -g -1 -1 0 1 -j 2:4 4 2 1:4 ```   ```console Usage: rife-ncnn-vulkan -0 infile -1 infile1 -o outfile [options]...        rife-ncnn-vulkan -i indir -o outdir [options]...    -h                   show this help   -v                   verbose output   -0 input0-path       input image0 path (jpg/png/webp)   -1 input1-path       input image1 path (jpg/png/webp)   -i input-path        input image directory (jpg/png/webp)   -o output-path       output image path (jpg/png/webp) or directory   -m model-path        rife model path (default=rife-HD)   -g gpu-id            gpu device to use (-1=cpu  default=auto) can be 0 1 2 for multi-gpu   -j load:proc:save    thread count for load/proc/save (default=1:2:2) can be 1:2 2 2:2 for multi-gpu   -x                   enable tta mode   -u                   enable UHD mode   -f pattern-format    output image filename pattern format (%08d.jpg/png/webp  default=ext/%08d.png) ```  - `input0-path`  `input1-path` and `output-path` accept file path - `input-path` and `output-path` accept file directory - `load:proc:save` = thread count for the three stages (image decoding + rife interpolation + image encoding)  using larger values may increase GPU usage and consume more GPU memory. You can tune this configuration with ""4:4:4"" for many small-size images  and ""2:2:2"" for large-size images. The default setting usually works fine for most situations. If you find that your GPU is hungry  try increasing thread count to achieve faster processing. - `pattern-format` = the filename pattern and format of the image to be output  png is better supported  however webp generally yields smaller file sizes  both are losslessly encoded  If you encounter a crash or error  try upgrading your GPU driver:  - Intel: https://downloadcenter.intel.com/product/80939/Graphics-Drivers - AMD: https://www.amd.com/en/support - NVIDIA: https://www.nvidia.com/Download/index.aspx   """;Computer Vision;https://github.com/nihui/rife-ncnn-vulkan
"""""";Computer Vision;https://github.com/pwochner/cmr_segmentation
"""""";General;https://github.com/Meituan-AutoML/CPVT
"""""";Computer Vision;https://github.com/Meituan-AutoML/CPVT
"""COVID-19Í∞Ä Ï†Ñ ÏÑ∏Í≥ÑÏ†ÅÏù∏ ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÎ©∞  ÏûêÏú†Î°úÏö¥ Ïô∏Î∂Ä ÌôúÎèôÏù¥ Í±∞Ïùò Î∂àÍ∞ÄÎä•Ìï¥ÏßÄÎäî ÌòÑÏÉÅÏù¥ ÏßÄÍµ¨ Ï†ÑÏ≤¥Ïóê Î∞úÏÉùÌïòÏòÄÏäµÎãàÎã§. Ïù¥Î°ú Ïù∏Ìï¥ Ïßë ÏïàÏóêÏÑú ÌôúÎèôÌï¥Ïïº ÌïòÎäî ÏãúÍ∞ÑÏù¥ ÎäòÏñ¥ÎÇòÎ©¥ÏÑú Ïô∏Î∂Ä ÌôúÎèôÍ≥º Í¥ÄÎ†®Îêú ÏÜåÎπÑÏï°Ïù¥ ÏûêÏó∞Ïä§Î†à ‚ÄòÏßëÏïà ÌôúÎèô‚ÄôÍ≥º Í¥ÄÎ†®Îêú Ìï≠Î™©ÏúºÎ°ú ÏòÆÍ≤®Í∞ÄÎäî ÌòÑÏÉÅÏù¥ Î∞úÏÉùÌïòÏòÄÏäµÎãàÎã§.   Íµ≠ÎÇ¥Ïô∏ Í∞ÄÍµ¨ Î∏åÎûúÎìúÏóêÏÑúÎäî AR  VR Îì± Îã§ÏñëÌïú Í∏∞Ïà†ÏùÑ Ï†ëÎ™©Ìïú Ìôà Ïä§ÌÉÄÏùºÎßÅ ÏÑúÎπÑÏä§Î•º ÏÑ†Î≥¥Ïù¥Í≥† ÏûàÏßÄÎßå  Ïó¨Ï†ÑÌûà Í≥µÍ∞Ñ Ï†ÑÎ∞òÏùÑ ÏïÑÏö∞Î•¥Îäî Ìôà Ïä§ÌÉÄÏùºÎßÅ ÏÑúÎπÑÏä§Î•º Ï†úÍ≥µÎ∞õÍ∏∞ ÏúÑÌï¥ÏÑúÎäî Îß§Ïû•ÏùÑ Î∞©Î¨∏Ìï¥Ïïº ÌïúÎã§Îäî ÏπòÎ™ÖÏ†ÅÏù∏ Îã®Ï†êÏù¥ Ï°¥Ïû¨ÌïòÎäî ÏÉÅÌô©ÏûÖÎãàÎã§.  Ïù¥Ïóê Ï†ÄÌù¨ ÌåÄÏùÄ Î™®Î∞îÏùº Ìôà Ïä§ÌÉÄÏùºÎßÅ ÏÜîÎ£®ÏÖò ‚ÄòFitting Room‚ÄôÏùÑ Ï†úÍ≥µÌïòÍ≥†Ïûê Ìï©ÎãàÎã§.   ÏÑúÎπÑÏä§Ïóê Í¥ÄÌïú ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ ""APP Repositories""Ïùò ReadmeÏóê Ï†ïÎ¶¨ÌïòÏòÄÏúºÎ©∞  Îã§ÏùåÏùò URLÏùÑ ÌÜµÌï¥ ÌôïÏù∏Ìï¥Ï£ºÏãúÎ©¥ Í∞êÏÇ¨ÌïòÍ≤†ÏäµÎãàÎã§.   h5py (3.1.0)   """;Computer Vision;https://github.com/KPMG-wiseuniv/AI
"""Jupyter Notebook   DeepLearning con PyTorch en 60 minutos   """;General;https://github.com/dccuchile/CC6204
"""Jupyter Notebook   DeepLearning con PyTorch en 60 minutos   """;Natural Language Processing;https://github.com/dccuchile/CC6204
"""""";Natural Language Processing;https://github.com/zhang2010hao/cw2vec-pytorch
"""**Python 3.6 or 3.7** is needed to run the toolbox.  * Install [PyTorch](https://pytorch.org/get-started/locally/) (>=1.1.0). * Install [ffmpeg](https://ffmpeg.org/download.html#get-packages). * Run `pip install -r requirements.txt` to install the remaining necessary packages.   14/02/21: This repo now runs on PyTorch instead of Tensorflow  thanks to the help of @bluefish. If you wish to run the tensorflow version instead  checkout commit 5425557.   20/08/19: I'm working on resemblyzer  an independent package for the voice encoder. You can use your trained encoder models from this repo with it.   Before you download any dataset  you can begin by testing your configuration with:   You can then try the toolbox:   """;General;https://github.com/CorentinJ/Real-Time-Voice-Cloning
"""To get the result  the steps have been listed below:   """;Computer Vision;https://github.com/samsh19/ML_project
"""U-Net is a fully convolutional neural network with an encoder-decoder structure designed for sementic image segmantation on biomedical images. [[1]](#1) It is a very effective meta-network architecture that has been adapted to incorporate other convolutional neural network architecture designs.   """;Computer Vision;https://github.com/hayashimasa/UNet-PyTorch
"""To run the algorithm on a GPU  I suggest to [install](https://github.com/google/jax#pip-installation) the gpu version of `jax` [[4]](https://github.com/google/jax). You can then install this repo using [Anaconda python](https://www.anaconda.com/products/individual) and [pip](https://pip.pypa.io/en/stable/installing/). ```sh conda env create -n dqn conda activate dqn pip install git+https://github.com/epignatelli/human-level-control-through-deep-reinforcement-learning ```   """;Reinforcement Learning;https://github.com/epignatelli/human-level-control-through-deep-reinforcement-learning
"""Pytorch modelleri repoya b√ºy√ºkl√ºƒü√ºnden dolayƒ± eklenmemi≈ütir.   - https://github.com/stefan-it/turkish-bert   - https://huggingface.co/transformers/   """;Natural Language Processing;https://github.com/okanvk/Turkish-Reading-Comprehension-Question-Answering-Dataset
"""```bash $ pip install x-transformers ```   Alternatively  if you would like to use entmax15  you can also do so with one setting as shown below.   Update: It may be that ALiBi enforces a strong local attention across the heads  and may hinder it from attending at distances greater than 1k. To avoid any issues with global message passing  I've decided to introduce another hyperparameter alibi_num_heads  so one can specify less heads for the ALiBi bias   Full encoder / decoder  ```python import torch from x_transformers import XTransformer  model = XTransformer(     dim = 512      enc_num_tokens = 256      enc_depth = 6      enc_heads = 8      enc_max_seq_len = 1024      dec_num_tokens = 256      dec_depth = 6      dec_heads = 8      dec_max_seq_len = 1024      tie_token_emb = True      #: tie embeddings of encoder and decoder )  src = torch.randint(0  256  (1  1024)) src_mask = torch.ones_like(src).bool() tgt = torch.randint(0  256  (1  1024)) tgt_mask = torch.ones_like(tgt).bool()  loss = model(src  tgt  src_mask = src_mask  tgt_mask = tgt_mask) #: (1  1024  512) loss.backward() ```  Decoder-only (GPT-like)  ```python import torch from x_transformers import TransformerWrapper  Decoder  model = TransformerWrapper(     num_tokens = 20000      max_seq_len = 1024      attn_layers = Decoder(         dim = 512          depth = 12          heads = 8     ) ).cuda()  x = torch.randint(0  256  (1  1024)).cuda()  model(x) #: (1  1024  20000) ```  GPT3 would be approximately the following (but you wouldn't be able to run it anyways)  ```python  gpt3 = TransformerWrapper(     num_tokens = 50000      max_seq_len = 2048      attn_layers = Decoder(         dim = 12288          depth = 96          heads = 96          attn_dim_head = 128     ) ).cuda() ```  Encoder-only (BERT-like)  ```python import torch from x_transformers import TransformerWrapper  Encoder  model = TransformerWrapper(     num_tokens = 20000      max_seq_len = 1024      attn_layers = Encoder(         dim = 512          depth = 12          heads = 8     ) ).cuda()  x = torch.randint(0  256  (1  1024)).cuda() mask = torch.ones_like(x).bool()  model(x  mask = mask) #: (1  1024  20000) ```  State of the art image classification  ```python import torch from x_transformers import ViTransformerWrapper  Encoder  model = ViTransformerWrapper(     image_size = 256      patch_size = 32      num_classes = 1000      attn_layers = Encoder(         dim = 512          depth = 6          heads = 8      ) )  img = torch.randn(1  3  256  256) model(img) #: (1  1000) ```  Image -> caption  ```python import torch from x_transformers import ViTransformerWrapper  TransformerWrapper  Encoder  Decoder  encoder = ViTransformerWrapper(     image_size = 256      patch_size = 32      attn_layers = Encoder(         dim = 512          depth = 6          heads = 8     ) )  decoder = TransformerWrapper(     num_tokens = 20000      max_seq_len = 1024      attn_layers = Decoder(         dim = 512          depth = 6          heads = 8          cross_attend = True     ) )  img = torch.randn(1  3  256  256) caption = torch.randint(0  20000  (1  1024))  encoded = encoder(img  return_embeddings = True) decoder(caption  context = encoded) #: (1  1024  20000) ```   """;Natural Language Processing;https://github.com/lucidrains/x-transformers
"""```bash $ pip install x-transformers ```   Alternatively  if you would like to use entmax15  you can also do so with one setting as shown below.   Update: It may be that ALiBi enforces a strong local attention across the heads  and may hinder it from attending at distances greater than 1k. To avoid any issues with global message passing  I've decided to introduce another hyperparameter alibi_num_heads  so one can specify less heads for the ALiBi bias   Full encoder / decoder  ```python import torch from x_transformers import XTransformer  model = XTransformer(     dim = 512      enc_num_tokens = 256      enc_depth = 6      enc_heads = 8      enc_max_seq_len = 1024      dec_num_tokens = 256      dec_depth = 6      dec_heads = 8      dec_max_seq_len = 1024      tie_token_emb = True      #: tie embeddings of encoder and decoder )  src = torch.randint(0  256  (1  1024)) src_mask = torch.ones_like(src).bool() tgt = torch.randint(0  256  (1  1024)) tgt_mask = torch.ones_like(tgt).bool()  loss = model(src  tgt  src_mask = src_mask  tgt_mask = tgt_mask) #: (1  1024  512) loss.backward() ```  Decoder-only (GPT-like)  ```python import torch from x_transformers import TransformerWrapper  Decoder  model = TransformerWrapper(     num_tokens = 20000      max_seq_len = 1024      attn_layers = Decoder(         dim = 512          depth = 12          heads = 8     ) ).cuda()  x = torch.randint(0  256  (1  1024)).cuda()  model(x) #: (1  1024  20000) ```  GPT3 would be approximately the following (but you wouldn't be able to run it anyways)  ```python  gpt3 = TransformerWrapper(     num_tokens = 50000      max_seq_len = 2048      attn_layers = Decoder(         dim = 12288          depth = 96          heads = 96          attn_dim_head = 128     ) ).cuda() ```  Encoder-only (BERT-like)  ```python import torch from x_transformers import TransformerWrapper  Encoder  model = TransformerWrapper(     num_tokens = 20000      max_seq_len = 1024      attn_layers = Encoder(         dim = 512          depth = 12          heads = 8     ) ).cuda()  x = torch.randint(0  256  (1  1024)).cuda() mask = torch.ones_like(x).bool()  model(x  mask = mask) #: (1  1024  20000) ```  State of the art image classification  ```python import torch from x_transformers import ViTransformerWrapper  Encoder  model = ViTransformerWrapper(     image_size = 256      patch_size = 32      num_classes = 1000      attn_layers = Encoder(         dim = 512          depth = 6          heads = 8      ) )  img = torch.randn(1  3  256  256) model(img) #: (1  1000) ```  Image -> caption  ```python import torch from x_transformers import ViTransformerWrapper  TransformerWrapper  Encoder  Decoder  encoder = ViTransformerWrapper(     image_size = 256      patch_size = 32      attn_layers = Encoder(         dim = 512          depth = 6          heads = 8     ) )  decoder = TransformerWrapper(     num_tokens = 20000      max_seq_len = 1024      attn_layers = Decoder(         dim = 512          depth = 6          heads = 8          cross_attend = True     ) )  img = torch.randn(1  3  256  256) caption = torch.randint(0  20000  (1  1024))  encoded = encoder(img  return_embeddings = True) decoder(caption  context = encoded) #: (1  1024  20000) ```   """;General;https://github.com/lucidrains/x-transformers
"""""";Sequential;https://github.com/dqqcasia/st
"""Clone DOTA_Devkit as a sub-module:  ```shell REPO_ROOT$ git submodule update --init --recursive REPO_ROOT/fcos_core/DOTA_devkit$ sudo apt-get install swig REPO_ROOT/fcos_core/DOTA_devkit$ swig -c++ -python polyiou.i REPO_ROOT/fcos_core/DOTA_devkit$ python setup.py build_ext --inplace ``` Edit the `config.json` and run:  ```shell REPO_ROOT$ python prepare.py ```   This FCOS implementation is based on [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark). Therefore the installation is the same as original maskrcnn-benchmark.  Please check [INSTALL.md](INSTALL.md) for installation instructions. You may also want to see the original [README.md](MASKRCNN_README.md) of maskrcnn-benchmark.   For users who only want to use FCOS as an object detector in their projects  they can install it by pip. To do so  run: ``` pip install torch  #: install pytorch if you do not have it pip install git+https://github.com/tianzhi0549/FCOS.git #: run this command line for a demo  fcos https://github.com/tianzhi0549/FCOS/raw/master/demo/images/COCO_val2014_000000000885.jpg ``` Please check out [here](fcos/bin/fcos) for the interface usage.   For your convenience  we provide the following trained models (more models are coming soon).   Make your directory layout like this:   """;Computer Vision;https://github.com/lijain/FCOS-change
"""! pip install xlrd  ! pip install --upgrade paddlehub  ! pip install paddle-ernie   !hub install Versailles   2020-12-10 22:31:38 676-INFO: font search path ['/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf'  '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/afm'  '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/pdfcorefonts']   Successfully installed Versailles   """;General;https://github.com/Sharpiless/Versailles-text-generation-with-paddlepaddle
"""Mesh TensorFlow (`mtf`) is a language for distributed deep learning  capable of specifying a broad class of distributed tensor computations.  The purpose of Mesh TensorFlow is to formalize and implement distribution strategies for your computation graph over your hardware/processors. For example: ""Split the batch over rows of processors and split the units in the hidden layer across columns of processors."" Mesh TensorFlow is implemented as a layer over TensorFlow.  Watch our [YouTube video](https://www.youtube.com/watch?v=HgGyWS40g-g).    To install the latest stable version  run  ```sh pip install mesh-tensorflow ```  To install the latest development version  run  ```sh pip install -e ""git+https://github.com/tensorflow/mesh.git#:egg=mesh-tensorflow"" ```  Installing `mesh-tensorflow` does not automatically install or update TensorFlow. We recommend installing it via `pip install tensorflow` or `pip install tensorflow-gpu`. See TensorFlow‚Äôs [installation instructions for details](https://www.tensorflow.org/install/). If you're using a development version of Mesh TensorFlow  you may need to use TensorFlow's nightly package (`tf-nightly`).   different from the CPU/GPU implementation.   git clone https://github.com/tensorflow/mesh.git   To illustrate  let us consider a simple model for the MNIST image-classification task.  Our network has one hidden layer with 1024 units  and an output layer with 10 units (corresponding to the 10 digit classes).  The code consists of two parts  the first describing the mathematical operations  and the second describing the devices and tensor/computation layout. For the full example  see [`examples/mnist.py`]( https://github.com/tensorflow/mesh/blob/master/examples/mnist.py). TODO(noam): verify that this code works.  ```Python #: tf_images is a tf.Tensor with shape [100  28  28] and dtype tf.float32 #: tf_labels is a tf.Tensor with shape [100] and dtype tf.int32 graph = mtf.Graph() mesh = mtf.Mesh(graph  ""my_mesh"") batch_dim = mtf.Dimension(""batch""  100) rows_dim = mtf.Dimension(""rows""  28) cols_dim = mtf.Dimension(""cols""  28) hidden_dim = mtf.Dimension(""hidden""  1024) classes_dim = mtf.Dimension(""classes""  10) images = mtf.import_tf_tensor(     mesh  tf_images  shape=[batch_dim  rows_dim  cols_dim]) labels = mtf.import_tf_tensor(mesh  tf_labels  [batch_dim]) w1 = mtf.get_variable(mesh  ""w1""  [rows_dim  cols_dim  hidden_dim]) w2 = mtf.get_variable(mesh  ""w2""  [hidden_dim  classes_dim]) #: einsum is a generalization of matrix multiplication (see numpy.einsum) hidden = mtf.relu(mtf.einsum(images  w1  output_shape=[batch_dim  hidden_dim])) logits = mtf.einsum(hidden  w2  output_shape=[batch_dim  classes_dim]) loss = mtf.reduce_mean(mtf.layers.softmax_cross_entropy_with_logits(     logits  mtf.one_hot(labels  classes_dim)  classes_dim)) w1_grad  w2_grad = mtf.gradients([loss]  [w1  w2]) update_w1_op = mtf.assign(w1  w1 - w1_grad * 0.001) update_w2_op = mtf.assign(w2  w2 - w2_grad * 0.001) ```  In the code above  we have built a Mesh TensorFlow graph  which is simply a Python structure.  We have completely defined the mathematical operations. In the code below  we specify the mesh of processors and the layout of the computation.  ```Python devices = [""gpu:0""  ""gpu:1""  ""gpu:2""  ""gpu:3""] mesh_shape = [(""all_processors""  4)] layout_rules = [(""batch""  ""all_processors"")] mesh_impl = mtf.placement_mesh_impl.PlacementMeshImpl(     mesh_shape  layout_rules  devices) lowering = mtf.Lowering(graph  {mesh:mesh_impl}) tf_update_ops = [lowering.lowered_operation(update_w1_op)                   lowering.lowered_operation(update_w2_op)] ```  The particular layout above implements data-parallelism  splitting the batch of examples evenly across all four processors.  Any Tensor with a ""batch"" dimension (e.g. `images`  `h`  `logits`  and their gradients) is split in that dimension across all processors  while any tensor without a ""batch"" dimension (e.g. the model parameters) is replicated identically on every processor.  Alternatively  for model-parallelism  we can set `layout_rules=[(""hidden""  ""all_processors"")]`.  In this case  any tensor with a ""hidden"" dimension (e.g. `hidden`  `w1`  `w2`)  is split  while any other tensor (e.g. `image`  `logits`) is fully replicated.  We can even combine data-parallelism and model-parallelism on a 2-dimensional mesh of processors.  We split the batch along one dimension of the mesh  and the units in the hidden layer along the other dimension of the mesh  as below.  In this case  the hidden layer is actually tiled between the four processors  being split in both the ""batch"" and ""hidden_units"" dimensions.  ```Python mesh_shape = [(""processor_rows""  2)  (""processor_cols""  2)] layout_rules = [(""batch""  ""processor_rows"")  (""hidden""  ""processor_cols"")] ```   Take our example `Tensor` `image_batch` with shape:  `[(""batch""  100)  (""rows""  28"")  (""cols""  28)  (""channels""  3)]`  Assume that this `Tensor` is assigned to a mesh of 8 processors with shape: `[(""processor_rows""  2)  (""processor_cols""  4)]`  * If we use an empty set of layout rules `[]`  we get no splitting.  Each   processor contains the whole `Tensor`.  * If we use the layout rules `""batch:processor_cols""`  then the `""batch""`   dimension of the `Tensor` is split across the `""processor_cols""` dimension of   the batch.  This means that each processor contains a Tensor slice with shape   `[25  28  28  3]`.  For example  processors (0  3) and (1  3) contain   identical slices - `image_batch[75:100  :  :  :]`.  * If we use the layout rules `""rows:processor_rows;cols:processor_cols""`     then the image is split in two dimensions  with each processor containing one   spatial tile with shape `[100  14  7  3]`.   For example  processor (0  1)   contains the slice `image_batch[:  0:14  7:14  :]`.  Some layout rules would lead to illegal layouts:  * `""batch:processor_rows;rows:processor_rows""` is illegal because two tensor   dimensions could not be split across the same mesh dimension.  * `""channels:processor_rows""` is illegal because the size of the tensor   dimension is not evenly divisible by the size of the mesh dimension.   TODO(trandustin ylc): update given mtf pypi package  ```sh ctpu up -name=ylc-mtf-donut -tf-version=nightly -tpu-size=v2-8 -zone=us-central1-b ```   """;Natural Language Processing;https://github.com/tensorflow/mesh
"""https://github.com/dvschultz/ml-art-colabs/blob/master/deepdream.ipynb <br>   https://github.com/dvschultz/ai/blob/master/SinGAN.ipynb <br>   https://github.com/dvschultz/ai/blob/master/neural_style_tf.ipynb <br>   https://github.com/dvschultz/ai/blob/master/StyleGAN2.ipynb <br>   Karras T.  Aittala  M.  Hellsten  J.  Laine  S.  Lehtinen  J.  Aila  T. (2020): Met- Faces. Version 1. <br>  https://github.com/NVlabs/metfaces-dataset <br>   https://www.kaggle.com/greg115/abstract-art/version/1 <br>   1. Analyze your own images using the [classification script](https://github.com/bennyqp/artificial-inspiration/blob/main/ai_image_classification.ipynb) and create the corresponding CSV file or download the original ""Creative Portrait Dataset"" for Unity and the corresponding CSV file [here](https://drive.google.com/file/d/1l8oa6ncwP0rItGJ3a2RVeEg1e5dkOgmf/view?usp=sharing). 1. Clone this repository and replace the file ""artificial-inspiration/Unity VR Dataset Explorer/Assets/Resources/img2vec.csv"" with your generated img2vec.csv or with the downloaded file.  1. Replace the folder ""artificial-inspiration/Unity VR Dataset Explorer/Assets/Resources/images/"" with your generated image folder or the one you downloaded. Important: The folder MUST be named ""images"" and the CSV file ""img2vec.cvs"" 1. Open the folder ""artificial-inspiration/Unity VR Dataset Explorer"" with Unity 2019.4.15f1 and open the scene ""vrDataExplorer"". When Unity asks you if you want to enable the backends because of the new input system  click no!  1. If you want to access your selected images online later  upload the content in the folder ""artificial-inspiration/selected images web app"" to a server. Then add the link to the file ""artificial-inspiration/selected images web app/images/uploadImages.php"" in Unity under ""Images Upload URL"" in the script ""Upload Images"". 1. If you don't use VR  activate ""Start in Explore Mode"" in the ""Constructor"" script. You can view the images and apply filters in the editor. Most of the functions are unfortunately not available. 1. If you use VR  you can use all the features. You can find them all in Unity and use most of them during the VR experience to explore your dataset and find the most exciting images.  1. Pretty much all the settings parameters are in the scripts on the ""GlobalScripts"" GameObject. Here you can play around and change the settings to try out different things.  1. Get inspired and develop new ideas ;-)   [You can also download the final Oculus build with the given sample data as an .apk file for your Oculus Quest here.](https://drive.google.com/file/d/1eiHNsIFS2pggfxFwTzurIDvFk4qxgDGs/view?usp=sharing) You can then run it using Sidequest  for example. However  it is recommended to run the application via Unity using Oculus Link  as it requires quite a bit of performance and can lag when run as a standalone.  <br><br>  The overriding goal is that the creativity of the user in relation to the subject matter is stimulated by this process and thus novel creative results can be developed.    www.artificial-inspiration.com <br><br><br><br> ![artificial inspiration images](https://github.com/bennyqp/artificial-inspiration/blob/main/Demo%20Images/artificial_inspiration_img06.jpg) ![artificial inspiration images](https://github.com/bennyqp/artificial-inspiration/blob/main/Demo%20Images/artificial_inspiration_img07.jpg) ![artificial inspiration images](https://github.com/bennyqp/artificial-inspiration/blob/main/Demo%20Images/artificial_inspiration_img08.jpg) ![artificial inspiration images](https://github.com/bennyqp/artificial-inspiration/blob/main/Demo%20Images/artificial_inspiration_img09.jpg)  <br><br>  """;Computer Vision;https://github.com/bennyqp/artificial-inspiration
"""You can install the released version from CRAN with:  ``` r install.packages(""tabnet"") ```  The development version can be installed from [GitHub](https://github.com/) with:  ``` r #: install.packages(""devtools"") remotes::install_github(""mlverse/tabnet"") ```   PyTorch‚Äôs implementation using the   ``` r library(tabnet) library(recipes) #:> Loading required package: dplyr #:>  #:> Attaching package: 'dplyr' #:> The following objects are masked from 'package:stats': #:>  #:>     filter  lag #:> The following objects are masked from 'package:base': #:>  #:>     intersect  setdiff  setequal  union #:>  #:> Attaching package: 'recipes' #:> The following object is masked from 'package:stats': #:>  #:>     step library(yardstick) #:> For binary classification  the first factor level is assumed to be the event. #:> Use the argument `event_level = ""second""` to alter this as needed. set.seed(1)  data(""attrition""  package = ""modeldata"") test_idx <- sample.int(nrow(attrition)  size = 0.2 * nrow(attrition))  train <- attrition[-test_idx ] test <- attrition[test_idx ]  rec <- recipe(Attrition ~ .  data = train) %>%    step_normalize(all_numeric()  -all_outcomes())  fit <- tabnet_fit(rec  train  epochs = 30)  metrics <- metric_set(accuracy  precision  recall) cbind(test  predict(fit  test)) %>%    metrics(Attrition  estimate = .pred_class) #:> #: A tibble: 3 x 3 #:>   .metric   .estimator .estimate #:>   <chr>     <chr>          <dbl> #:> 1 accuracy  binary         0.867 #:> 2 precision binary         0.885 #:> 3 recall    binary         0.967    cbind(test  predict(fit  test  type = ""prob"")) %>%    roc_auc(Attrition  .pred_No) #:> #: A tibble: 1 x 3 #:>   .metric .estimator .estimate #:>   <chr>   <chr>          <dbl> #:> 1 roc_auc binary         0.726 ```  """;General;https://github.com/mlverse/tabnet
"""This project requires Torch to be installed. The easiest way to install Torch is by following the installation instructions at `torch/distro`__.  To use the library  install it with LuaRocks by running the following command from the root directory.  .. code:: bash     luarocks make babitasks-scm-1.rockspec  __ https://github.com/torch/distro   To generate a task  run the command  .. code:: bash      babi-tasks <task-id>  where ``<task-id>`` is either a class name (like ``PathFinding``) or the task number (e.g. 19). To quickly generate 1000 examples of each task  you can use  .. code:: bash      for i in `seq 1 20`; do babi-tasks $i 1000 > task_$i.txt; done   """;General;https://github.com/qapitan/babi-marcus
"""Êï∞ÊçÆÈõÜÁôæÂ∫¶‰∫ëÁõò ÈìæÊé•: https://pan.baidu.com/s/1XywcO2gsm3AhKn9P8Ye7UA ÊèêÂèñÁ†Å: 2q9i  Note : Use Python 3   """;Computer Vision;https://github.com/anxingle/UNet-pytorch
"""```python img = torch.ones([1  3  224  224])  model = CvT(224  3  1000)  parameters = filter(lambda p: p.requires_grad  model.parameters()) parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000 print('Trainable Parameters: %.3fM' % parameters)  out = model(img)  print(""Shape of out :""  out.shape)  #: [B  num_classes] ```  """;Computer Vision;https://github.com/rishikksh20/convolution-vision-transformers
"""""";Computer Vision;https://github.com/sneakatyou/ViT-Tensorflow-2.0
"""The goal of **semantic segmentation** is to identify objects  like cars and dogs  in an image by labelling the corresponding groups of pixels according to their classes. For an introduction  see <a href=""https://nanonets.com/blog/semantic-image-segmentation-2020/"">this article</a>. As an example  below is an image and its labelled pixels.  | <img src=""assets/rider.jpg"" alt=""biker"" width=400> | <img src=""assets/rider_label.png"" alt=""true label"" width=400> | |:---:|:---:| | Image | True label |  A **fully convolutional network (FCN)** is an artificial neural network that performs semantic segmentation.  The bottom layers of a FCN are those of a convolutional neural network (CNN)  usually taken from a pre-trained network like VGGNet or GoogLeNet. The purpose of these layers is to perform classification on subregions of the image. The top layers of a FCN are **transposed convolution/deconvolution** layers  which upsample the results of the classification to the resolution of the original image. This gives us a label for each pixel. When upsampling  we can also utilize the intermediate layers of the CNN to improve the accuracy of the segmentation. For an introduction  see <a href=""https://nanonets.com/blog/how-to-do-semantic-segmentation-using-deep-learning/"">this article</a>.  The <a href=""http://host.robots.ox.ac.uk/pascal/VOC/"">Pascal VOC project</a> is a dataset containing images whose pixels have been labeled according to 20 classes (excluding the background)  which include aeroplanes  cars  and people. We will be performing semantic segmentation according to this dataset.   """;Computer Vision;https://github.com/kevinddchen/Keras-FCN
"""Download yolov3 weights here https://pjreddie.com/darknet/yolo/ (YOLOV3-320  45 FPS) and add it to Yolov3 directory. (This file is too heavy to push on github). Name this file yolov3.weights .  The best way to run predict_video.py is on google colab  which has a GPU.  I was unable to configure opencv to use the GPU on google colab. So the player detection part is not faster. The ball detection is. Any help is welcome to achieve this. This link could be useful https://towardsdatascience.com/how-to-use-opencv-with-gpu-on-colab-25594379945f .   1. Clone the repo. ```sh git clone https://github.com/MaximeBataille/tennis_tracking ```  2. Put the all the repo on a Google Drive.  3. Run predict_video.py to obtain a tracking video. (Google colab) ```sh !python3 ""predict_video.py""  --save_weights_path=""weights_tracknet/model.1"" --input_video_path=""/VideoInput/video_cut.mp4"" --output_video_path=""/VideoOutput/video_output.avi"" --n_classes=256 --path_yolo_classes=""/yolov3/yolov3.txt"" --path_yolo_weights=""/yolov3/yolov3.weights"" --path_yolo_config=""/yolov3/yolov3.cfg"" ```  4. Run generate_bird_eye_view.py to obtain a bird eye view. (Google colab) ```sh !python3 ""generate_bird_eye_view.py"" ```  5. The generated videos are in the VideoOutput directory.   """;Computer Vision;https://github.com/MaximeBataille/tennis_tracking
"""""";General;https://github.com/harshit158/paper-dots
"""""";Natural Language Processing;https://github.com/harshit158/paper-dots
"""|    Name         |*   ``` python3 main.py -h usage: main.py [-h] --save_dir SAVE_DIR [--root ROOT] [--gpus GPUS]                [--num_workers NUM_WORKERS] [--model MODEL] [--epoch EPOCH]                [--batch_size BATCH_SIZE] [--val_batch_size VAL_BATCH_SIZE]                [--test] [--print_freq PRINT_FREQ] [--num_classes NUM_CLASSES]                [--ema] [--color_jitter COLOR_JITTER] [--pca]                [--crop_pct CROP_PCT] [--cool_down COOL_DOWN]                [--ema_decay EMA_DECAY] [--dropout_rate DROPOUT_RATE]                [--dropconnect_rate DROPCONNECT_RATE]                [--optim {rmsprop rmsproptf sgd}] [--lr LR] [--warmup WARMUP]                [--beta [BETA [BETA ...]]] [--momentum MOMENTUM] [--eps EPS]                [--decay DECAY] [--scheduler {exp cosine none}] [--amp]                [--dali] [--se_r SE_R] [--REGNET_WA REGNET_WA]                [--REGNET_W0 REGNET_W0] [--REGNET_WM REGNET_WM]                [--REGNET_DEPTH REGNET_DEPTH] [--REGNET_STRIDE REGNET_STRIDE]                [--REGNET_GROUP_W REGNET_GROUP_W]                [--REGNET_BOT_MUL REGNET_BOT_MUL]                [--REGNET_STEM_W REGNET_STEM_W]  Pytorch EfficientNet  optional arguments:   -h  --help            show this help message and exit   --save_dir SAVE_DIR   Directory name to save the model   --root ROOT           The Directory of data path.   --gpus GPUS           Select GPU Numbers | 0 1 2 3 |   --num_workers NUM_WORKERS                         Select CPU Number workers   --model MODEL         The type of Efficient net.   --epoch EPOCH         The number of epochs   --batch_size BATCH_SIZE                         The size of batch   --val_batch_size VAL_BATCH_SIZE                         The size of batch in val set   --test                Only Test   --print_freq PRINT_FREQ                         The iterations of print results   --num_classes NUM_CLASSES                         Number of classes   --ema                 Using exponential moving average for testing   --color_jitter COLOR_JITTER                         Color jitter factor (default: 0.0)   --pca                 add AlexNet - style PCA - based noise   --crop_pct CROP_PCT   Input image center crop percent (for validation only)   --cool_down COOL_DOWN                         epochs to cooldown LR at min_lr  after cyclic schedule                         ends   --ema_decay EMA_DECAY                         Exponential Moving Average Term   --dropout_rate DROPOUT_RATE   --dropconnect_rate DROPCONNECT_RATE   --optim {rmsprop rmsproptf sgd}   --lr LR               Base learning rate when train batch size is 256.   --warmup WARMUP   --beta [BETA [BETA ...]]   --momentum MOMENTUM   --eps EPS   --decay DECAY   --scheduler {exp cosine none}                         Learning rate scheduler type   --amp                 Use Native Torch AMP mixed precision   --dali                Use Naidiv DaLi library for loading   --se_r SE_R           Squeeze-and-Excitation rate   --REGNET_WA REGNET_WA                         Slop   --REGNET_W0 REGNET_W0                         Initial width   --REGNET_WM REGNET_WM                         Quantization   --REGNET_DEPTH REGNET_DEPTH                         Depth   --REGNET_STRIDE REGNET_STRIDE                         Stride of each stage   --REGNET_GROUP_W REGNET_GROUP_W                         Group width   --REGNET_BOT_MUL REGNET_BOT_MUL                         Bottleneck multiplier (bm = 1 / b from the paper)   --REGNET_STEM_W REGNET_STEM_W                         Stem width    ```  <hr>   """;General;https://github.com/xslidi/EfficientNets_ddl_apex
"""|    Name         |*   ``` python3 main.py -h usage: main.py [-h] --save_dir SAVE_DIR [--root ROOT] [--gpus GPUS]                [--num_workers NUM_WORKERS] [--model MODEL] [--epoch EPOCH]                [--batch_size BATCH_SIZE] [--val_batch_size VAL_BATCH_SIZE]                [--test] [--print_freq PRINT_FREQ] [--num_classes NUM_CLASSES]                [--ema] [--color_jitter COLOR_JITTER] [--pca]                [--crop_pct CROP_PCT] [--cool_down COOL_DOWN]                [--ema_decay EMA_DECAY] [--dropout_rate DROPOUT_RATE]                [--dropconnect_rate DROPCONNECT_RATE]                [--optim {rmsprop rmsproptf sgd}] [--lr LR] [--warmup WARMUP]                [--beta [BETA [BETA ...]]] [--momentum MOMENTUM] [--eps EPS]                [--decay DECAY] [--scheduler {exp cosine none}] [--amp]                [--dali] [--se_r SE_R] [--REGNET_WA REGNET_WA]                [--REGNET_W0 REGNET_W0] [--REGNET_WM REGNET_WM]                [--REGNET_DEPTH REGNET_DEPTH] [--REGNET_STRIDE REGNET_STRIDE]                [--REGNET_GROUP_W REGNET_GROUP_W]                [--REGNET_BOT_MUL REGNET_BOT_MUL]                [--REGNET_STEM_W REGNET_STEM_W]  Pytorch EfficientNet  optional arguments:   -h  --help            show this help message and exit   --save_dir SAVE_DIR   Directory name to save the model   --root ROOT           The Directory of data path.   --gpus GPUS           Select GPU Numbers | 0 1 2 3 |   --num_workers NUM_WORKERS                         Select CPU Number workers   --model MODEL         The type of Efficient net.   --epoch EPOCH         The number of epochs   --batch_size BATCH_SIZE                         The size of batch   --val_batch_size VAL_BATCH_SIZE                         The size of batch in val set   --test                Only Test   --print_freq PRINT_FREQ                         The iterations of print results   --num_classes NUM_CLASSES                         Number of classes   --ema                 Using exponential moving average for testing   --color_jitter COLOR_JITTER                         Color jitter factor (default: 0.0)   --pca                 add AlexNet - style PCA - based noise   --crop_pct CROP_PCT   Input image center crop percent (for validation only)   --cool_down COOL_DOWN                         epochs to cooldown LR at min_lr  after cyclic schedule                         ends   --ema_decay EMA_DECAY                         Exponential Moving Average Term   --dropout_rate DROPOUT_RATE   --dropconnect_rate DROPCONNECT_RATE   --optim {rmsprop rmsproptf sgd}   --lr LR               Base learning rate when train batch size is 256.   --warmup WARMUP   --beta [BETA [BETA ...]]   --momentum MOMENTUM   --eps EPS   --decay DECAY   --scheduler {exp cosine none}                         Learning rate scheduler type   --amp                 Use Native Torch AMP mixed precision   --dali                Use Naidiv DaLi library for loading   --se_r SE_R           Squeeze-and-Excitation rate   --REGNET_WA REGNET_WA                         Slop   --REGNET_W0 REGNET_W0                         Initial width   --REGNET_WM REGNET_WM                         Quantization   --REGNET_DEPTH REGNET_DEPTH                         Depth   --REGNET_STRIDE REGNET_STRIDE                         Stride of each stage   --REGNET_GROUP_W REGNET_GROUP_W                         Group width   --REGNET_BOT_MUL REGNET_BOT_MUL                         Bottleneck multiplier (bm = 1 / b from the paper)   --REGNET_STEM_W REGNET_STEM_W                         Stem width    ```  <hr>   """;Computer Vision;https://github.com/xslidi/EfficientNets_ddl_apex
"""""";Computer Vision;https://github.com/ShigemichiMatsuzaki/MSPL
"""""";General;https://github.com/ShigemichiMatsuzaki/MSPL
"""""";Sequential;https://github.com/ShigemichiMatsuzaki/MSPL
"""```bash $ pip install electra-pytorch ```   forked from electra-pytorch   $ cd data  $ pip3 install gdown   $ cd ..   The following example uses `reformer-pytorch`  which is available to be pip installed.  ```python import torch from torch import nn from reformer_pytorch import ReformerLM  from electra_pytorch import Electra  #: (1) instantiate the generator and discriminator  making sure that the generator is roughly a quarter to a half of the size of the discriminator  generator = ReformerLM(     num_tokens = 20000      emb_dim = 128      dim = 256               #: smaller hidden dimension     heads = 4               #: less heads     ff_mult = 2             #: smaller feed forward intermediate dimension     dim_head = 64      depth = 12      max_seq_len = 1024 )  discriminator = ReformerLM(     num_tokens = 20000      emb_dim = 128      dim = 1024      dim_head = 64      heads = 16      depth = 12      ff_mult = 4      max_seq_len = 1024 )  #: (2) weight tie the token and positional embeddings of generator and discriminator  generator.token_emb = discriminator.token_emb generator.pos_emb = discriminator.pos_emb #: weight tie any other embeddings if available  token type embeddings  etc.  #: (3) instantiate electra  trainer = Electra(     generator      discriminator      discr_dim = 1024            #: the embedding dimension of the discriminator     discr_layer = 'reformer'    #: the layer name in the discriminator  whose output would be used for predicting token is still the same or replaced     mask_token_id = 2           #: the token id reserved for masking     pad_token_id = 0            #: the token id for padding     mask_prob = 0.15            #: masking probability for masked language modeling     mask_ignore_token_ids = []  #: ids of tokens to ignore for mask modeling ex. (cls  sep) )  #: (4) train  data = torch.randint(0  20000  (1  1024))  results = trainer(data) results.loss.backward()  #: after much training  the discriminator should have improved  torch.save(discriminator  f'./pretrained-model.pt') ```  If you would rather not have the framework auto-magically intercept the hidden output of the discriminator  you can pass in the discriminator (with the extra linear [dim x 1]) by yourself with the following.  ```python import torch from torch import nn from reformer_pytorch import ReformerLM  from electra_pytorch import Electra  #: (1) instantiate the generator and discriminator  making sure that the generator is roughly a quarter to a half of the size of the discriminator  generator = ReformerLM(     num_tokens = 20000      emb_dim = 128      dim = 256               #: smaller hidden dimension     heads = 4               #: less heads     ff_mult = 2             #: smaller feed forward intermediate dimension     dim_head = 64      depth = 12      max_seq_len = 1024 )  discriminator = ReformerLM(     num_tokens = 20000      emb_dim = 128      dim = 1024      dim_head = 64      heads = 16      depth = 12      ff_mult = 4      max_seq_len = 1024      return_embeddings = True )  #: (2) weight tie the token and positional embeddings of generator and discriminator  generator.token_emb = discriminator.token_emb generator.pos_emb = discriminator.pos_emb #: weight tie any other embeddings if available  token type embeddings  etc.  #: (3) instantiate electra  discriminator_with_adapter = nn.Sequential(discriminator  nn.Linear(1024  1))  trainer = Electra(     generator      discriminator_with_adapter      mask_token_id = 2           #: the token id reserved for masking     pad_token_id = 0            #: the token id for padding     mask_prob = 0.15            #: masking probability for masked language modeling     mask_ignore_token_ids = []  #: ids of tokens to ignore for mask modeling ex. (cls  sep) )  #: (4) train  data = torch.randint(0  20000  (1  1024))  results = trainer(data) results.loss.backward()  #: after much training  the discriminator should have improved  torch.save(discriminator  f'./pretrained-model.pt') ```   """;Natural Language Processing;https://github.com/smallbenchnlp/ELECTRA-DeBERTa
"""For OoD detection  you can train on CIFAR-10/100. You can also train on Dirty-MNIST by downloading Ambiguous-MNIST (amnist_labels.pt and amnist_samples.pt) from here and using the following training instructions.   """;General;https://github.com/omegafragger/DDU
"""For install and data preparation  please refer to the guidelines in [MMSegmentation v0.13.0](https://github.com/open-mmlab/mmsegmentation/tree/v0.13.0).  Other requirements: ```pip install timm==0.3.2```  An example (works for me): ```CUDA 10.1``` and  ```pytorch 1.7.1```   ``` pip install torchvision==0.8.2 pip install timm==0.3.2 pip install mmcv-full==1.2.7 pip install opencv-python==4.5.1.48 cd SegFormer && pip install -e . --user ```   ./tools/dist_test.sh local_configs/segformer/B1/segformer.b1.512x512.ade.160k.py /path/to/checkpoint_file <GPU_NUM>   tools/dist_test.sh local_configs/segformer/B1/segformer.b1.512x512.ade.160k.py /path/to/checkpoint_file <GPU_NUM> --aug-test   Example: train SegFormer-B1 on ADE20K:   SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers.<br> [Enze Xie](https://xieenze.github.io/)  [Wenhai Wang](https://whai362.github.io/)  [Zhiding Yu](https://chrisding.github.io/)  [Anima Anandkumar](http://tensorlab.cms.caltech.edu/users/anima/)  [Jose M. Alvarez](https://rsu.data61.csiro.au/people/jalvarez/)  and [Ping Luo](http://luoping.me/).<br> Technical Report 2021.  This repository contains the official Pytorch implementation of training & evaluation code and the pretrained models for [SegFormer](https://arxiv.org/abs/2105.15203).  SegFormer is a simple  efficient and powerful semantic segmentation method  as shown in Figure 1.  We use [MMSegmentation v0.13.0](https://github.com/open-mmlab/mmsegmentation/tree/v0.13.0) as the codebase.  üî•üî• SegFormer is on [MMSegmentation](https://github.com/open-mmlab/mmsegmentation/tree/master/configs/segformer). üî•üî•     """;General;https://github.com/kikacaty/RAP_Benchmark
"""For install and data preparation  please refer to the guidelines in [MMSegmentation v0.13.0](https://github.com/open-mmlab/mmsegmentation/tree/v0.13.0).  Other requirements: ```pip install timm==0.3.2```  An example (works for me): ```CUDA 10.1``` and  ```pytorch 1.7.1```   ``` pip install torchvision==0.8.2 pip install timm==0.3.2 pip install mmcv-full==1.2.7 pip install opencv-python==4.5.1.48 cd SegFormer && pip install -e . --user ```   ./tools/dist_test.sh local_configs/segformer/B1/segformer.b1.512x512.ade.160k.py /path/to/checkpoint_file <GPU_NUM>   tools/dist_test.sh local_configs/segformer/B1/segformer.b1.512x512.ade.160k.py /path/to/checkpoint_file <GPU_NUM> --aug-test   Example: train SegFormer-B1 on ADE20K:   SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers.<br> [Enze Xie](https://xieenze.github.io/)  [Wenhai Wang](https://whai362.github.io/)  [Zhiding Yu](https://chrisding.github.io/)  [Anima Anandkumar](http://tensorlab.cms.caltech.edu/users/anima/)  [Jose M. Alvarez](https://rsu.data61.csiro.au/people/jalvarez/)  and [Ping Luo](http://luoping.me/).<br> Technical Report 2021.  This repository contains the official Pytorch implementation of training & evaluation code and the pretrained models for [SegFormer](https://arxiv.org/abs/2105.15203).  SegFormer is a simple  efficient and powerful semantic segmentation method  as shown in Figure 1.  We use [MMSegmentation v0.13.0](https://github.com/open-mmlab/mmsegmentation/tree/v0.13.0) as the codebase.  üî•üî• SegFormer is on [MMSegmentation](https://github.com/open-mmlab/mmsegmentation/tree/master/configs/segformer). üî•üî•     """;Computer Vision;https://github.com/kikacaty/RAP_Benchmark
"""Download weights   Download train dataset   """;General;https://github.com/Lornatang/ESPCN-PyTorch
"""""";General;https://github.com/vinayprabhu/X-is-all-you-need
"""""";Natural Language Processing;https://github.com/vinayprabhu/X-is-all-you-need
"""```bash $ pip install vit-pytorch ```   For a Pytorch implementation with pretrained models  please see Ross Wightman's repository <a href=""https://github.com/rwightman/pytorch-image-models"">here</a>.   You can also use the handy .to_vit method on the DistillableViT instance to get back a ViT instance.   You can train this with a near SOTA self-supervised learning technique  <a href=""https://github.com/lucidrains/byol-pytorch"">BYOL</a>  with the following code.   $ pip install byol-pytorch   A pytorch-lightning script is ready for you to use at the repository link above.   $ pip install nystrom-attention   $ pip install x-transformers   ```python import torch from vit_pytorch import ViT  v = ViT(     image_size = 256      patch_size = 32      num_classes = 1000      dim = 1024      depth = 6      heads = 16      mlp_dim = 2048      dropout = 0.1      emb_dropout = 0.1 )  img = torch.randn(1  3  256  256) mask = torch.ones(1  8  8).bool() #: optional mask  designating which patch to attend to  preds = v(img  mask = mask) #: (1  1000) ```   """;Computer Vision;https://github.com/tianhai123/vit-pytorch
"""source code - Pytorch (use to reproduce results): https://github.com/WongKinYiu/ScaledYOLOv4  source code - Darknet: https://github.com/AlexeyAB/darknet   source code: https://github.com/AlexeyAB/darknet   useful links: https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&sk=6039748846bbcf1d960c3061542591d7   """;Computer Vision;https://github.com/leggedrobotics/darknet
"""""";Computer Vision;https://github.com/fdac18/ForensicImages
"""- pytorch  - numpy   Install the package:  pip install gon-pytorch   """;Computer Vision;https://github.com/kklemon/gon-pytorch
"""PyTorch 1.6.0 Please don't install the higher versions  Note: These instructions are for running with an NVDIA GPU on Linux.   Make sure your NVIDA drivers are correctly installed.  Running nvidia-smi should confirm your driver version and memory usage                 jupyter/tensorflow-notebook:python-3.8.8   ![Imgur](https://i.imgur.com/y7NM1f8.jpg)   ![Imgur](https://i.imgur.com/WEnbL8w.jpg)   ![Imgur](https://i.imgur.com/X5lZEse.jpg)   The project is still a work in progress  but I want to put it out so that I get some good suggestions.  The easiest way to get started is to simply try out on Colab: [<img src=""https://colab.research.google.com/assets/colab-badge.svg"" align=""center"">](https://colab.research.google.com/github/vijishmadhavan/SkinDeep/blob/master/SkinDeep_good.ipynb)  [<img src=""https://colab.research.google.com/assets/colab-badge.svg"" align=""center"">](https://colab.research.google.com/github/vijishmadhavan/SkinDeep/blob/master/SkinDeep.ipynb)  The output is limited to 500px and it needs high quality images to do well.  I would request you to have a look at the limitations given below.   The model still struggles and needs a lot of improvement  if you are interested please contribute lets improve it.   """;Computer Vision;https://github.com/vijishmadhavan/SkinDeep
"""PyTorch 1.6.0 Please don't install the higher versions  Note: These instructions are for running with an NVDIA GPU on Linux.   Make sure your NVIDA drivers are correctly installed.  Running nvidia-smi should confirm your driver version and memory usage                 jupyter/tensorflow-notebook:python-3.8.8   ![Imgur](https://i.imgur.com/y7NM1f8.jpg)   ![Imgur](https://i.imgur.com/WEnbL8w.jpg)   ![Imgur](https://i.imgur.com/X5lZEse.jpg)   The project is still a work in progress  but I want to put it out so that I get some good suggestions.  The easiest way to get started is to simply try out on Colab: [<img src=""https://colab.research.google.com/assets/colab-badge.svg"" align=""center"">](https://colab.research.google.com/github/vijishmadhavan/SkinDeep/blob/master/SkinDeep_good.ipynb)  [<img src=""https://colab.research.google.com/assets/colab-badge.svg"" align=""center"">](https://colab.research.google.com/github/vijishmadhavan/SkinDeep/blob/master/SkinDeep.ipynb)  The output is limited to 500px and it needs high quality images to do well.  I would request you to have a look at the limitations given below.   The model still struggles and needs a lot of improvement  if you are interested please contribute lets improve it.   """;General;https://github.com/vijishmadhavan/SkinDeep
"""|------build: cmakeÁõÆÂΩï      bash Anaconda3-2020.11-Linux-x86_64.sh      conda --version     conda upgrade --all   ÂàõÂª∫ËôöÊãüÁéØÂ¢ÉÔºöconda create -name ascend python=3.7.5      source activate ascend  deactivateÈÄÄÂá∫               name=n.name    pip3 install onnx-simplifier   Âà©Áî®sudo find / -name libopencv_...Á±ª‰ººÁöÑÂëΩ‰ª§Êü•Êâæ‰ΩçÁΩÆÔºåÂ∞ÜÂØπÂ∫îË∑ØÂæÑÊåâÁÖß‰∏ãÈù¢ÊâÄÁ§∫ÁöÑÊ≠•È™§Ê∑ªÂä†Âà∞ÁéØÂ¢ÉÂèòÈáè‰∏≠       source ~/.bashrc   ÂÖàÂÆâË£ÖonnxsimÔºöpip3 install onnx-simplifier   """;Computer Vision;https://github.com/WinstonLy/Electricity-Inspection-Based-Ascend310
"""* Environment dependence:    - PaddlePaddle >= 2.1.0    - Python >= 3.6    - CUDA >= 10.1 * [Full installation tutorial](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/install.md)   Complete tutorials for deployment: https://github.com/wzmsltw/PaintTransformer   v0.1.0 (2020.11.02)   - [Quick start](./docs/en_US/get_started.md) - [Data Preparation](./docs/en_US/data_prepare.md) - [Instruction of APIs](./docs/en_US/apis/apps.md) - [Instruction of Config Files](./docs/en_US/config_doc.md)   * [Pixel2Pixel](./docs/en_US/tutorials/pix2pix_cyclegan.md) * [CycleGAN](./docs/en_US/tutorials/pix2pix_cyclegan.md) * [LapStyle](./docs/en_US/tutorials/lap_style.md) * [PSGAN](./docs/en_US/tutorials/psgan.md) * [First Order Motion Model](./docs/en_US/tutorials/motion_driving.md) * [FaceParsing](./docs/en_US/tutorials/face_parse.md) * [AnimeGANv2](./docs/en_US/tutorials/animegan.md) * [U-GAT-IT](./docs/en_US/tutorials/ugatit.md) * [Photo2Cartoon](./docs/en_US/tutorials/photo2cartoon.md) * [Wav2Lip](./docs/en_US/tutorials/wav2lip.md) * [Single Image Super Resolution(SISR)](./docs/en_US/tutorials/single_image_super_resolution.md) * [Video Super Resolution(VSR)](./docs/en_US/tutorials/video_super_resolution.md) * [StyleGAN2](./docs/en_US/tutorials/styleganv2.md) * [Pixel2Style2Pixel](./docs/en_US/tutorials/pixel2style2pixel.md)    You can run those projects in the [AI Studio](https://aistudio.baidu.com/aistudio/projectoverview/public/1?kw=paddlegan) to learn how to use the models above:  |Online Tutorial      |    link  | |--------------|-----------| |Motion Driving-multi-personal ""Mai-ha-hi"" | [Click and Try](https://aistudio.baidu.com/aistudio/projectdetail/1603391) | |Restore the video of Beijing hundreds years ago|[Click and Try](https://aistudio.baidu.com/aistudio/projectdetail/1161285)| |Motion Driving-When ""Su Daqiang"" sings ""unravel"" |[Click and Try](https://aistudio.baidu.com/aistudio/projectdetail/1048840)|   """;Computer Vision;https://github.com/PaddlePaddle/PaddleGAN
"""```console foo@bar:‚ùØ pip install mlp_mixer ```  ```Python from mlp_mixer import MLPMixer  model = MLPMixer(         img_size=IMG_SZ          img_channels=IMG_CHANNELS          num_classes=NUM_CLASSES          mixer_depth=DEPTH          num_patches=NUM_PATCHES          num_channels=NUM_CHANNELS          expansion=EXPANSION          dropout=DROPOUT      ) ```    """;Computer Vision;https://github.com/himanshu-dutta/MLPMixer-pytorch
"""![Combined](https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/cam_gb_dog.jpg?raw=true)   pip install grad-cam   : You can also use it within a with statement  to make sure it is freed    To reduce noise in the CAMs  and make it fit better on the objects  two smoothing methods are supported:  - `aug_smooth=True`    Test time augmentation: increases the run time by x6.    Applies a combination of horizontal flips  and mutiplying the image   by [1.0  1.1  0.9].    This has the effect of better centering the CAM around the objects.   - `eigen_smooth=True`    First principle component of `activations*weights`    This has the effect of removing a lot of noise.   |AblationCAM | aug smooth | eigen smooth | aug+eigen smooth| |------------|------------|--------------|--------------------| ![](./examples/nosmooth.jpg) | ![](./examples/augsmooth.jpg) | ![](./examples/eigensmooth.jpg) | ![](./examples/eigenaug.jpg) |   ----------   Usage: `python cam.py --image-path <path_to_image> --method <method>`  To use with CUDA: `python cam.py --image-path <path_to_image> --use-cuda`  ----------  You can choose between:  `GradCAM`   `ScoreCAM`  `GradCAMPlusPlus`  `AblationCAM`  `XGradCAM`   `LayerCAM` and `EigenCAM`.  Some methods like ScoreCAM and AblationCAM require a large number of forward passes  and have a batched implementation.  You can control the batch size with `cam.batch_size = `  ----------   """;Computer Vision;https://github.com/jacobgil/pytorch-grad-cam
"""Commit your Changes (git commit -m 'Add some AmazingFeature')   You can either get a local copy by downloading this repo or either use [Google Colaboratory](https://colab.research.google.com/) by copy-pasting the link of the notebook (.ipynb file) of your choice.    In the two following notebooks we are going to focus on a Kaggle competition  namely: the [CommonLit Readability Prize](https://www.kaggle.com/c/commonlitreadabilityprize/)   You can directly run it on [Kaggle](https://www.kaggle.com/simoneazeglio/word2vec-umap-w2vaugmentation/)   In the two following notebook we are going to focus on a Kaggle competition  namely: the [CommonLit Readability Prize](https://www.kaggle.com/c/commonlitreadabilityprize/)   In the following notebooks (in [this](https://github.com/MachineLearningJournalClub/CommonLitReadabilityChallenge/) Github repo) we outlined our solution for the [CommonLit Readibility Prize](https://www.kaggle.com/c/commonlitreadabilityprize/)   """;General;https://github.com/MachineLearningJournalClub/LearningNLP
"""""";Computer Vision;https://github.com/yuuun/clip_pytorch
"""pip install -U transformers   | roformer_chinese_small             | chinese_roformer_L-6_H-384_A-6.zip (download codeÔºögy97)               |   | roformer_chinese_char_small        | chinese_roformer-char_L-6_H-384_A-6.zip (download codeÔºöa44c)          |   | roformer_chinese_sim_char_small    | chinese_roformer-sim-char_L-6_H-384_A-6.zip (download codeÔºöh68q)      |   | roformer_chinese_sim_char_ft_small | chinese_roformer-sim-char-ft_L-6_H-384_A-6.zip (download codeÔºögty5)   |   : pytorch   pt_outputs_sentence = ""pytorch: ""   : pytorch: ‰ªäÂ§©[Â§©Ê∞î||Â§©||ÂøÉÊÉÖ||Èò≥ÂÖâ||Á©∫Ê∞î]ÂæàÂ•ΩÔºåÊàë[ÊÉ≥||Ë¶Å||ÊâìÁÆó||ÂáÜÂ§á||ÂñúÊ¨¢]ÂéªÂÖ¨Âõ≠Áé©„ÄÇ   bert4keras vs pytorch   bert4keras vs pytorch   """;General;https://github.com/JunnYu/RoFormer_pytorch
"""[![Demo of model](https://github.com/albertsokol/yolov3-tf2/blob/main/readme_images/youtube_link.png)](https://www.youtube.com/watch?v=tXYPUMHGe7A ""My YOLOv3 implementation : object detection on dashcam footage"")   """;Computer Vision;https://github.com/albertsokol/yolov3-tf2
"""cd (change directory) into vehicle-speed-check cd vehicle-speed-check  Create virtual environment python -m venv   Activate virtual environment ./venv/bin/activate  Install requirements pip install -r requirements.txt   """;Computer Vision;https://github.com/samirgholipour/speed_estimation
"""‚îú‚îÄ‚îÄ kws    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ metrics     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ fnr_fpr.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py   ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ attention.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ crnn.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ densenet.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dpn.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ resnet.py    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ resnext.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ treasure_net.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ vgg.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ wideresnet.py   ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ transforms   ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ utils.py   ‚îú‚îÄ‚îÄ config.py    * *./kws/metrics* : Evaluation matrics  defining the False Rejection Rate (FRR) and False Alarm Rate (FAR) for keyword spotting * *./kws/models* : Diffferent network architecture  * *.config.py* : Configuration about parameters and hyperparameters  - Python >= 3.7 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html)) - [PyTorch >= 1.3](https://pytorch.org/) - NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads)   1. Install dependent packages      ```bash     cd E2E-Keyword-Spotting     pip install -r requirements.txt     ``` 2. Or use conda      ```bash     cd E2E-Keyword-Spotting     conda env create -f environment.yaml     ```   Dataset is from [Google Speech Command](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) published in  [arxiv](https://arxiv.org/abs/1804.03209). * Data Pre-processing (Has already been done) 1. According to the file  dataset has already been splited into three folders  train  test  and valid.  1. The splited [Google Speech Command dataset](https://drive.google.com/file/d/1InqR8n7l5Qj6voJREpcjHYWHVTKG-BbB/view?usp=sharing) is saved in Google Drive folder.        """;General;https://github.com/bozliu/E2E-Keyword-Spotting
"""‚îú‚îÄ‚îÄ kws    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ metrics     ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ fnr_fpr.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py   ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ attention.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ crnn.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ densenet.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dpn.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ resnet.py    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ resnext.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ treasure_net.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ vgg.py   ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ wideresnet.py   ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ transforms   ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ utils.py   ‚îú‚îÄ‚îÄ config.py    * *./kws/metrics* : Evaluation matrics  defining the False Rejection Rate (FRR) and False Alarm Rate (FAR) for keyword spotting * *./kws/models* : Diffferent network architecture  * *.config.py* : Configuration about parameters and hyperparameters  - Python >= 3.7 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html)) - [PyTorch >= 1.3](https://pytorch.org/) - NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads)   1. Install dependent packages      ```bash     cd E2E-Keyword-Spotting     pip install -r requirements.txt     ``` 2. Or use conda      ```bash     cd E2E-Keyword-Spotting     conda env create -f environment.yaml     ```   Dataset is from [Google Speech Command](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) published in  [arxiv](https://arxiv.org/abs/1804.03209). * Data Pre-processing (Has already been done) 1. According to the file  dataset has already been splited into three folders  train  test  and valid.  1. The splited [Google Speech Command dataset](https://drive.google.com/file/d/1InqR8n7l5Qj6voJREpcjHYWHVTKG-BbB/view?usp=sharing) is saved in Google Drive folder.        """;Computer Vision;https://github.com/bozliu/E2E-Keyword-Spotting
"""Keras 2.2.0  Tensorflow 1.8.0  Ubuntu 16.04  Python 3.5   """;Computer Vision;https://github.com/vkduy19/paper_implementations-master
"""The easiest way to install deepface is to download it from [`PyPI`](https://pypi.org/project/deepface/). It's going to install the library itself and its prerequisites as well. The library is mainly based on TensorFlow and Keras.  ```python pip install deepface ```  Then you will be able to import the library and use its functionalities.  ```python from deepface import DeepFace ```  **Facial Recognition** - [`Demo`](https://youtu.be/WnUVYQP4h44)  A modern [**face recognition pipeline**](https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/) consists of 5 common stages: [detect](https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/)  [align](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/)  [normalize](https://sefiks.com/2020/11/20/facial-landmarks-for-face-recognition-with-dlib/)  [represent](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) and [verify](https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/). Deepface handles all these common stages in the background. You can just call its verification  find or analysis function with a single line of code.  **Face Verification** - [`Demo`](https://youtu.be/KRCvkNCOphE)  This function verifies face pairs as same person or different persons. It expects exact image paths as inputs. Passing numpy or based64 encoded images is also welcome.  ```python result = DeepFace.verify(img1_path = ""img1.jpg""  img2_path = ""img2.jpg"") ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-1.jpg"" width=""95%"" height=""95%""></p>  **Face recognition** - [`Demo`](https://youtu.be/Hrjp-EStM_s)  [Face recognition](https://sefiks.com/2020/05/25/large-scale-face-recognition-for-deep-learning/) requires applying face verification many times. Herein  deepface has an out-of-the-box find function to handle this action. It's going to look for the identity of input image in the database path and it will return pandas data frame as output.  ```python df = DeepFace.find(img_path = ""img1.jpg""  db_path = ""C:/workspace/my_db"") ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-6-v2.jpg"" width=""95%"" height=""95%""></p>  **Face recognition models** - [`Demo`](https://youtu.be/i_MOwvhbLdI)  Deepface is a **hybrid** face recognition package. It currently wraps many **state-of-the-art** face recognition models: [`VGG-Face`](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/)   [`Google FaceNet`](https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/)  [`OpenFace`](https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/)  [`Facebook DeepFace`](https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/)  [`DeepID`](https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/)  [`ArcFace`](https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/) and [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/). The default configuration uses VGG-Face model.  ```python models = [""VGG-Face""  ""Facenet""  ""Facenet512""  ""OpenFace""  ""DeepFace""  ""DeepID""  ""ArcFace""  ""Dlib""] result = DeepFace.verify(img1_path = ""img1.jpg""  img2_path = ""img2.jpg""  model_name = models[1]) df = DeepFace.find(img_path = ""img1.jpg""  db_path = ""C:/workspace/my_db""  model_name = models[1]) ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-wrapped-models.png"" width=""95%"" height=""95%""></p>  FaceNet  VGG-Face  ArcFace and Dlib [overperforms](https://youtu.be/i_MOwvhbLdI) than OpenFace  DeepFace and DeepID based on experiments. Supportively  FaceNet /w 512d got 99.65%; FaceNet /w 128d got 99.2%; ArcFace got 99.41%; Dlib got 99.38%; VGG-Face got 98.78%; DeepID got 97.05; OpenFace got 93.80% accuracy scores on [LFW data set](https://sefiks.com/2020/08/27/labeled-faces-in-the-wild-for-face-recognition/) whereas human beings could have just 97.53%.  **Similarity**  Face recognition models are regular [convolutional neural networks](https://sefiks.com/2018/03/23/convolutional-autoencoder-clustering-images-with-neural-networks/) and they are responsible to represent faces as vectors. We expect that a face pair of same person should be [more similar](https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/) than a face pair of different persons.  Similarity could be calculated by different metrics such as [Cosine Similarity](https://sefiks.com/2018/08/13/cosine-similarity-in-machine-learning/)  Euclidean Distance and L2 form. The default configuration uses cosine similarity.  ```python metrics = [""cosine""  ""euclidean""  ""euclidean_l2""] result = DeepFace.verify(img1_path = ""img1.jpg""  img2_path = ""img2.jpg""  distance_metric = metrics[1]) df = DeepFace.find(img_path = ""img1.jpg""  db_path = ""C:/workspace/my_db""  distance_metric = metrics[1]) ```  Euclidean L2 form [seems](https://youtu.be/i_MOwvhbLdI) to be more stable than cosine and regular Euclidean distance based on experiments.  **Facial Attribute Analysis** - [`Demo`](https://youtu.be/GT2UeN85BdA)  Deepface also comes with a strong facial attribute analysis module including [`age`](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/)  [`gender`](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/)  [`facial expression`](https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/) (including angry  fear  neutral  sad  disgust  happy and surprise) and [`race`](https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/) (including asian  white  middle eastern  indian  latino and black) predictions.  ```python obj = DeepFace.analyze(img_path = ""img4.jpg""  actions = ['age'  'gender'  'race'  'emotion']) ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-2.jpg"" width=""95%"" height=""95%""></p>  Age model got ¬± 4.65 MAE; gender model got 97.44% accuracy  96.29% precision and 95.05% recall as mentioned in its [tutorial](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/).  **Streaming and Real Time Analysis** - [`Demo`](https://youtu.be/-c9sSJcx6wI)  You can run deepface for real time videos as well. Stream function will access your webcam and apply both face recognition and facial attribute analysis. The function starts to analyze a frame if it can focus a face sequantially 5 frames. Then  it shows results 5 seconds.  ```python DeepFace.stream(db_path = ""C:/User/Sefik/Desktop/database"") ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-3.jpg"" width=""90%"" height=""90%""></p>  Even though face recognition is based on one-shot learning  you can use multiple face pictures of a person as well. You should rearrange your directory structure as illustrated below.  ```bash user ‚îú‚îÄ‚îÄ database ‚îÇ   ‚îú‚îÄ‚îÄ Alice ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Alice1.jpg ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Alice2.jpg ‚îÇ   ‚îú‚îÄ‚îÄ Bob ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Bob.jpg ```  **Face Detectors** - [`Demo`](https://youtu.be/GZ2p2hj2H5k)  Face detection and alignment are early stages of a modern face recognition pipeline. Experiments show that just alignment increases the face recognition accuracy almost 1%. [`OpenCV`](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/)  [`SSD`](https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/)  [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/)   [`MTCNN`](https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/) and [`RetinaFace`](https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/) detectors are wrapped in deepface. OpenCV is the default detector.  ```python backends = ['opencv'  'ssd'  'dlib'  'mtcnn'  'retinaface']  #:face detection and alignment detected_face = DeepFace.detectFace(img_path = ""img.jpg""  detector_backend = backends[4])  #:face verification obj = DeepFace.verify(img1_path = ""img1.jpg""  img2_path = ""img2.jpg""  detector_backend = backends[4])  #:face recognition df = DeepFace.find(img_path = ""img.jpg""  db_path = ""my_db""  detector_backend = backends[4])  #:facial analysis demography = DeepFace.analyze(img_path = ""img4.jpg""  detector_backend = backends[4]) ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-detectors.png"" width=""90%"" height=""90%""></p>  [RetinaFace](https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/) and [MTCNN](https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/) seem to overperform in detection and alignment stages but they are slower than others. If the speed of your pipeline is more important  then you should use opencv or ssd. On the other hand  if you consider the accuracy  then you should use retinaface or mtcnn.  <!-- **Ensemble learning for face recognition** - [`Demo`](https://youtu.be/EIBJJJ0ECXU)  A face recognition task can be handled by several models and similarity metrics. Herein  deepface offers a [special boosting and combination solution](https://sefiks.com/2020/06/03/mastering-face-recognition-with-ensemble-learning/) to improve the accuracy of a face recognition task. This provides a huge improvement on accuracy metrics. On the other hand  this runs much slower than single models.  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-4.jpg"" width=""70%"" height=""70%""></p>  ```python resp_obj = DeepFace.verify(""img1.jpg""  ""img2.jpg""  model_name = ""Ensemble"") df = DeepFace.find(img_path = ""img1.jpg""  db_path = ""my_db""  model_name = ""Ensemble"") ``` -->  **API** - [`Demo`](https://youtu.be/HeKCQ6U9XmI)  Deepface serves an API as well. You can clone [`/api/api.py`](https://github.com/serengil/deepface/tree/master/api/api.py) and pass it to python command as an argument. This will get a rest service up. In this way  you can call deepface from an external system such as mobile app or web.  ``` python api.py ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-api.jpg"" width=""90%"" height=""90%""></p>  Face recognition  facial attribute analysis and vector representation functions are covered in the API. You are expected to call these functions as http post methods. Service endpoints will be `http://127.0.0.1:5000/verify` for face recognition  `http://127.0.0.1:5000/analyze` for facial attribute analysis  and `http://127.0.0.1:5000/represent` for vector representation. You should pass input images as base64 encoded string in this case. [Here](https://github.com/serengil/deepface/tree/master/api)  you can find a postman project.  **Tech Stack** - [`Vlog`](https://youtu.be/R8fHsL7u3eE)  [`Tutorial`](https://sefiks.com/2021/03/31/tech-stack-recommendations-for-face-recognition/)  Face recognition models represent facial images as vector embeddings. The idea behind facial recognition is that vectors should be more similar for same person than different persons. The question is that where and how to store facial embeddings in a large scale system. Herein  deepface offers a represention function to find vector embeddings from facial images.  ```python embedding = DeepFace.represent(img_path = ""img.jpg""  model_name = 'Facenet') ```  Tech stack is vast to store vector embeddings. To determine the right tool  you should consider your task such as face verification or face recognition  priority such as speed or confidence  and also data size.  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/tech-stack.png"" width=""90%"" height=""90%""></p>   """;General;https://github.com/serengil/deepface
"""The easiest way to install deepface is to download it from [`PyPI`](https://pypi.org/project/deepface/). It's going to install the library itself and its prerequisites as well. The library is mainly based on TensorFlow and Keras.  ```python pip install deepface ```  Then you will be able to import the library and use its functionalities.  ```python from deepface import DeepFace ```  **Facial Recognition** - [`Demo`](https://youtu.be/WnUVYQP4h44)  A modern [**face recognition pipeline**](https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/) consists of 5 common stages: [detect](https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/)  [align](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/)  [normalize](https://sefiks.com/2020/11/20/facial-landmarks-for-face-recognition-with-dlib/)  [represent](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) and [verify](https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/). Deepface handles all these common stages in the background. You can just call its verification  find or analysis function with a single line of code.  **Face Verification** - [`Demo`](https://youtu.be/KRCvkNCOphE)  This function verifies face pairs as same person or different persons. It expects exact image paths as inputs. Passing numpy or based64 encoded images is also welcome.  ```python result = DeepFace.verify(img1_path = ""img1.jpg""  img2_path = ""img2.jpg"") ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-1.jpg"" width=""95%"" height=""95%""></p>  **Face recognition** - [`Demo`](https://youtu.be/Hrjp-EStM_s)  [Face recognition](https://sefiks.com/2020/05/25/large-scale-face-recognition-for-deep-learning/) requires applying face verification many times. Herein  deepface has an out-of-the-box find function to handle this action. It's going to look for the identity of input image in the database path and it will return pandas data frame as output.  ```python df = DeepFace.find(img_path = ""img1.jpg""  db_path = ""C:/workspace/my_db"") ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-6-v2.jpg"" width=""95%"" height=""95%""></p>  **Face recognition models** - [`Demo`](https://youtu.be/i_MOwvhbLdI)  Deepface is a **hybrid** face recognition package. It currently wraps many **state-of-the-art** face recognition models: [`VGG-Face`](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/)   [`Google FaceNet`](https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/)  [`OpenFace`](https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/)  [`Facebook DeepFace`](https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/)  [`DeepID`](https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/)  [`ArcFace`](https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/) and [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/). The default configuration uses VGG-Face model.  ```python models = [""VGG-Face""  ""Facenet""  ""Facenet512""  ""OpenFace""  ""DeepFace""  ""DeepID""  ""ArcFace""  ""Dlib""] result = DeepFace.verify(img1_path = ""img1.jpg""  img2_path = ""img2.jpg""  model_name = models[1]) df = DeepFace.find(img_path = ""img1.jpg""  db_path = ""C:/workspace/my_db""  model_name = models[1]) ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-wrapped-models.png"" width=""95%"" height=""95%""></p>  FaceNet  VGG-Face  ArcFace and Dlib [overperforms](https://youtu.be/i_MOwvhbLdI) than OpenFace  DeepFace and DeepID based on experiments. Supportively  FaceNet /w 512d got 99.65%; FaceNet /w 128d got 99.2%; ArcFace got 99.41%; Dlib got 99.38%; VGG-Face got 98.78%; DeepID got 97.05; OpenFace got 93.80% accuracy scores on [LFW data set](https://sefiks.com/2020/08/27/labeled-faces-in-the-wild-for-face-recognition/) whereas human beings could have just 97.53%.  **Similarity**  Face recognition models are regular [convolutional neural networks](https://sefiks.com/2018/03/23/convolutional-autoencoder-clustering-images-with-neural-networks/) and they are responsible to represent faces as vectors. We expect that a face pair of same person should be [more similar](https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/) than a face pair of different persons.  Similarity could be calculated by different metrics such as [Cosine Similarity](https://sefiks.com/2018/08/13/cosine-similarity-in-machine-learning/)  Euclidean Distance and L2 form. The default configuration uses cosine similarity.  ```python metrics = [""cosine""  ""euclidean""  ""euclidean_l2""] result = DeepFace.verify(img1_path = ""img1.jpg""  img2_path = ""img2.jpg""  distance_metric = metrics[1]) df = DeepFace.find(img_path = ""img1.jpg""  db_path = ""C:/workspace/my_db""  distance_metric = metrics[1]) ```  Euclidean L2 form [seems](https://youtu.be/i_MOwvhbLdI) to be more stable than cosine and regular Euclidean distance based on experiments.  **Facial Attribute Analysis** - [`Demo`](https://youtu.be/GT2UeN85BdA)  Deepface also comes with a strong facial attribute analysis module including [`age`](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/)  [`gender`](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/)  [`facial expression`](https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/) (including angry  fear  neutral  sad  disgust  happy and surprise) and [`race`](https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/) (including asian  white  middle eastern  indian  latino and black) predictions.  ```python obj = DeepFace.analyze(img_path = ""img4.jpg""  actions = ['age'  'gender'  'race'  'emotion']) ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-2.jpg"" width=""95%"" height=""95%""></p>  Age model got ¬± 4.65 MAE; gender model got 97.44% accuracy  96.29% precision and 95.05% recall as mentioned in its [tutorial](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/).  **Streaming and Real Time Analysis** - [`Demo`](https://youtu.be/-c9sSJcx6wI)  You can run deepface for real time videos as well. Stream function will access your webcam and apply both face recognition and facial attribute analysis. The function starts to analyze a frame if it can focus a face sequantially 5 frames. Then  it shows results 5 seconds.  ```python DeepFace.stream(db_path = ""C:/User/Sefik/Desktop/database"") ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-3.jpg"" width=""90%"" height=""90%""></p>  Even though face recognition is based on one-shot learning  you can use multiple face pictures of a person as well. You should rearrange your directory structure as illustrated below.  ```bash user ‚îú‚îÄ‚îÄ database ‚îÇ   ‚îú‚îÄ‚îÄ Alice ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Alice1.jpg ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Alice2.jpg ‚îÇ   ‚îú‚îÄ‚îÄ Bob ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Bob.jpg ```  **Face Detectors** - [`Demo`](https://youtu.be/GZ2p2hj2H5k)  Face detection and alignment are early stages of a modern face recognition pipeline. Experiments show that just alignment increases the face recognition accuracy almost 1%. [`OpenCV`](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/)  [`SSD`](https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/)  [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/)   [`MTCNN`](https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/) and [`RetinaFace`](https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/) detectors are wrapped in deepface. OpenCV is the default detector.  ```python backends = ['opencv'  'ssd'  'dlib'  'mtcnn'  'retinaface']  #:face detection and alignment detected_face = DeepFace.detectFace(img_path = ""img.jpg""  detector_backend = backends[4])  #:face verification obj = DeepFace.verify(img1_path = ""img1.jpg""  img2_path = ""img2.jpg""  detector_backend = backends[4])  #:face recognition df = DeepFace.find(img_path = ""img.jpg""  db_path = ""my_db""  detector_backend = backends[4])  #:facial analysis demography = DeepFace.analyze(img_path = ""img4.jpg""  detector_backend = backends[4]) ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-detectors.png"" width=""90%"" height=""90%""></p>  [RetinaFace](https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/) and [MTCNN](https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/) seem to overperform in detection and alignment stages but they are slower than others. If the speed of your pipeline is more important  then you should use opencv or ssd. On the other hand  if you consider the accuracy  then you should use retinaface or mtcnn.  <!-- **Ensemble learning for face recognition** - [`Demo`](https://youtu.be/EIBJJJ0ECXU)  A face recognition task can be handled by several models and similarity metrics. Herein  deepface offers a [special boosting and combination solution](https://sefiks.com/2020/06/03/mastering-face-recognition-with-ensemble-learning/) to improve the accuracy of a face recognition task. This provides a huge improvement on accuracy metrics. On the other hand  this runs much slower than single models.  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-4.jpg"" width=""70%"" height=""70%""></p>  ```python resp_obj = DeepFace.verify(""img1.jpg""  ""img2.jpg""  model_name = ""Ensemble"") df = DeepFace.find(img_path = ""img1.jpg""  db_path = ""my_db""  model_name = ""Ensemble"") ``` -->  **API** - [`Demo`](https://youtu.be/HeKCQ6U9XmI)  Deepface serves an API as well. You can clone [`/api/api.py`](https://github.com/serengil/deepface/tree/master/api/api.py) and pass it to python command as an argument. This will get a rest service up. In this way  you can call deepface from an external system such as mobile app or web.  ``` python api.py ```  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-api.jpg"" width=""90%"" height=""90%""></p>  Face recognition  facial attribute analysis and vector representation functions are covered in the API. You are expected to call these functions as http post methods. Service endpoints will be `http://127.0.0.1:5000/verify` for face recognition  `http://127.0.0.1:5000/analyze` for facial attribute analysis  and `http://127.0.0.1:5000/represent` for vector representation. You should pass input images as base64 encoded string in this case. [Here](https://github.com/serengil/deepface/tree/master/api)  you can find a postman project.  **Tech Stack** - [`Vlog`](https://youtu.be/R8fHsL7u3eE)  [`Tutorial`](https://sefiks.com/2021/03/31/tech-stack-recommendations-for-face-recognition/)  Face recognition models represent facial images as vector embeddings. The idea behind facial recognition is that vectors should be more similar for same person than different persons. The question is that where and how to store facial embeddings in a large scale system. Herein  deepface offers a represention function to find vector embeddings from facial images.  ```python embedding = DeepFace.represent(img_path = ""img.jpg""  model_name = 'Facenet') ```  Tech stack is vast to store vector embeddings. To determine the right tool  you should consider your task such as face verification or face recognition  priority such as speed or confidence  and also data size.  <p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/tech-stack.png"" width=""90%"" height=""90%""></p>   """;Computer Vision;https://github.com/serengil/deepface
"""```python import torch import numpy as np from mlp-mixer import MLPMixer  img = torch.ones([1  3  224  224])  model = MLPMixer(in_channels=3  image_size=224  patch_size=16  num_classes=1000                   dim=512  depth=8  token_dim=256  channel_dim=2048)  parameters = filter(lambda p: p.requires_grad  model.parameters()) parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000 print('Trainable Parameters: %.3fM' % parameters)  out_img = model(img)  print(""Shape of out :""  out_img.shape)  #: [B  in_channels  image_size  image_size] ```   """;Computer Vision;https://github.com/rishikksh20/MLP-Mixer-pytorch
"""If you want to use OpenPose without installing or writing any code  simply [download and use the latest Windows portable version of OpenPose](doc/installation/0_index.md#windows-portable-demo)!  Otherwise  you could [build OpenPose from source](doc/installation/0_index.md#compiling-and-running-openpose-from-source). See the [installation doc](doc/installation/0_index.md) for all the alternatives.     OS: Ubuntu (20  18  16  14)  Windows (10  8)  Mac OSX  Nvidia TX2.  Hardware compatibility: CUDA (Nvidia GPU)  OpenCL (AMD GPU)  and non-GPU (CPU-only) versions.   Simply use the OpenPose Demo from your favorite command-line tool (e.g.  Windows PowerShell or Ubuntu Terminal). E.g.  this example runs OpenPose on your webcam and displays the body keypoints: ``` #: Ubuntu ./build/examples/openpose/openpose.bin ``` ``` :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi ```  You can also add any of the available flags in any order. E.g.  the following example runs on a video (`--video {PATH}`)  enables face (`--face`) and hands (`--hand`)  and saves the output keypoints on JSON files on disk (`--write_json {PATH}`). ``` #: Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi --face --hand --write_json output_json_folder/ ``` ``` :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi --face --hand --write_json output_json_folder/ ```  Optionally  you can also extend OpenPose's functionality from its Python and C++ APIs. After [installing](doc/installation/0_index.md) OpenPose  check its [official doc](doc/00_index.md) for a quick overview of all the alternatives and tutorials.     """;General;https://github.com/mgolnezhad/openpose
"""Here we use raw image data format for simplicity  please follow [GluonCV tutorial](https://gluon-cv.mxnet.io/build/examples_datasets/recordio.html) if you would like to use RecordIO format.  ```bash cd scripts/dataset/ #: assuming you have downloaded the dataset in the current folder python prepare_imagenet.py --download-dir ./ ```   0. Install this package repo  note that you only need to choose one of the options  ```bash #: using github url pip install git+https://github.com/zhanghang1989/ResNeSt  #: using pypi pip install resnest --pre ```   The ResNeSt backbone has been adopted by MMDetection.   cd scripts/torch/   cd scripts/gluon/   """;Computer Vision;https://github.com/mohitktanwr/ResNeSt_Inverse
"""reference : https://github.com/jeonsworld/ViT-pytorch/blob/main/visualize_attention_map.ipynb   """;Computer Vision;https://github.com/jo1jun/Vision_Transformer
"""Automatic image colorization has been a popular image-to-image translation problem of significant interest for several practical application areas including restoration of aged or degraded images. This project attempts to utilize CycleGANs to colorize grayscale images back to their colorful RGB form.   """;Computer Vision;https://github.com/ArkaJU/Image-Colorization-CycleGAN
"""Automatic image colorization has been a popular image-to-image translation problem of significant interest for several practical application areas including restoration of aged or degraded images. This project attempts to utilize CycleGANs to colorize grayscale images back to their colorful RGB form.   """;General;https://github.com/ArkaJU/Image-Colorization-CycleGAN
"""* install tesnorflow ( skip this step if it's already installed test environment:tensorflow 2.4.0) *     pip install -r requirements.txt    [2021-02-11] Add support for: one-click deployment using tensorflow Serving(very fast)<br><br>   git clone https://github.com/wangermeng2021/Scaled-YOLOv4-tensorflow2.git    cd Scaled-YOLOv4-tensorflow2   Download Pre-trained p5 coco pretrain models and place it under directory 'pretrained/ScaledYOLOV4_p5_coco_pretrain' :<br>   Download Pre-trained p6 coco pretrain models and place it under directory 'pretrained/ScaledYOLOV4_p6_coco_pretrain' :<br>   Download Pre-trained tiny coco pretrain models and place it under directory 'pretrained/ScaledYOLOV4_tiny_coco_pretrain' :<br>   cd  deployment/tfserving       1. install client package          `   pip install tfservingclient-1.0.0-cp37-cp37m-manylinux1_x86_64.whl   `   ScaledYOLOv4_p5_detection_result:  ![pothole_p5_detection_3.png](https://github.com/wangermeng2021/ScaledYOLOv4-tensorflow2/blob/main/images/pothole_p5_detection_3.png) ![chess_p5_detection.png](https://github.com/wangermeng2021/ScaledYOLOv4-tensorflow2/blob/main/images/chess_p5_detection.png)  ScaledYOLOv4_tiny_detection_result:  ![safehat_tiny_detection_1.png](https://github.com/wangermeng2021/ScaledYOLOv4-tensorflow2/blob/main/images/safehat_tiny_detection_1.png) ![safehat_tiny_detection_2.png](https://github.com/wangermeng2021/ScaledYOLOv4-tensorflow2/blob/main/images/safehat_tiny_detection_2.png)   """;General;https://github.com/wangermeng2021/Scaled-YOLOv4-tensorflow2
"""* install tesnorflow ( skip this step if it's already installed test environment:tensorflow 2.4.0) *     pip install -r requirements.txt    [2021-02-11] Add support for: one-click deployment using tensorflow Serving(very fast)<br><br>   git clone https://github.com/wangermeng2021/Scaled-YOLOv4-tensorflow2.git    cd Scaled-YOLOv4-tensorflow2   Download Pre-trained p5 coco pretrain models and place it under directory 'pretrained/ScaledYOLOV4_p5_coco_pretrain' :<br>   Download Pre-trained p6 coco pretrain models and place it under directory 'pretrained/ScaledYOLOV4_p6_coco_pretrain' :<br>   Download Pre-trained tiny coco pretrain models and place it under directory 'pretrained/ScaledYOLOV4_tiny_coco_pretrain' :<br>   cd  deployment/tfserving       1. install client package          `   pip install tfservingclient-1.0.0-cp37-cp37m-manylinux1_x86_64.whl   `   ScaledYOLOv4_p5_detection_result:  ![pothole_p5_detection_3.png](https://github.com/wangermeng2021/ScaledYOLOv4-tensorflow2/blob/main/images/pothole_p5_detection_3.png) ![chess_p5_detection.png](https://github.com/wangermeng2021/ScaledYOLOv4-tensorflow2/blob/main/images/chess_p5_detection.png)  ScaledYOLOv4_tiny_detection_result:  ![safehat_tiny_detection_1.png](https://github.com/wangermeng2021/ScaledYOLOv4-tensorflow2/blob/main/images/safehat_tiny_detection_1.png) ![safehat_tiny_detection_2.png](https://github.com/wangermeng2021/ScaledYOLOv4-tensorflow2/blob/main/images/safehat_tiny_detection_2.png)   """;Computer Vision;https://github.com/wangermeng2021/Scaled-YOLOv4-tensorflow2
"""pip install foolbox  Foolbox requires Python 3.6 or newer. To use it with PyTorch &lt;https://pytorch.org&gt;  TensorFlow &lt;https://www.tensorflow.org&gt;  or JAX &lt;https://github.com/google/jax&gt;_  the respective framework needs to be installed separately. These frameworks are not declared as dependencies because not everyone wants to use and thus install all of them and because some of these packages have different builds for different architectures and CUDA versions. Besides that  all essential dependencies are automatically installed.   If you would like to help  you can also have a look at the issues that are   PyTorch 1.4.0   .. code-block:: python     import foolbox as fb     model = ...    fmodel = fb.PyTorchModel(model  bounds=(0  1))     attack = fb.attacks.LinfPGD()    epsilons = [0.0  0.001  0.01  0.03  0.1  0.3  0.5  1.0]    _  advs  success = attack(fmodel  images  labels  epsilons=epsilons)   More examples can be found in the `examples <./examples/>`_ folder  e.g. a full `ResNet-18 example <./examples/single_attack_pytorch_resnet18.py>`_.   """;General;https://github.com/bethgelab/foolbox
"""Kaggle Notebook link: https://www.kaggle.com/ashishsingh226/unconditional-gan-fashionmnist   Kaggle notebook link : https://www.kaggle.com/ashishsingh226/mnist-conditional-gan   """;Computer Vision;https://github.com/AshishSingh2261/GAN
"""""";Sequential;https://github.com/NTT123/pointer-networks
"""TrajNet++ is a large scale interaction-centric trajectory forecasting benchmark comprising explicit agent-agent scenarios. Our code is built on top of the numerous baselines that are [available with Trajnet++](https://github.com/vita-epfl/trajnetplusplusbaselines).  If you want to replicate our results  follow the [guidelines from the Trajnet++ benchmark hosts](https://thedebugger811.github.io/posts/2020/03/intro_trajnetpp/) to ensure you are good to go on the Trajnet++ dataset  thereafter fork our repository with respect to its architecture (/rnns/) and follow the guidelines for training our models.     """;Sequential;https://github.com/JosephGesnouin/Asymmetrical-Bi-RNNs-to-encode-pedestrian-trajectories
"""Initializing the model: ```python from swintransformer import SwinTransformer  model = SwinTransformer('swin_tiny_224'  num_classes=1000  include_top=True  pretrained=False) ``` You can use a pretrained model like this: ```python import tensorflow as tf from swintransformer import SwinTransformer  model = tf.keras.Sequential([   tf.keras.layers.Lambda(lambda data: tf.keras.applications.imagenet_utils.preprocess_input(tf.cast(data  tf.float32)  mode=""torch"")  input_shape=[*IMAGE_SIZE  3])    SwinTransformer('swin_tiny_224'  include_top=False  pretrained=True)    tf.keras.layers.Dense(NUM_CLASSES  activation='softmax') ]) ``` If you use a pretrained model with TPU on kaggle  specify `use_tpu` option: ```python import tensorflow as tf from swintransformer import SwinTransformer  model = tf.keras.Sequential([   tf.keras.layers.Lambda(lambda data: tf.keras.applications.imagenet_utils.preprocess_input(tf.cast(data  tf.float32)  mode=""torch"")  input_shape=[*IMAGE_SIZE  3])    SwinTransformer('swin_tiny_224'  include_top=False  pretrained=True  use_tpu=True)    tf.keras.layers.Dense(NUM_CLASSES  activation='softmax') ]) ``` Example: [TPU training on Kaggle](https://www.kaggle.com/rishigami/tpu-swin-transformer-tensorflow)  """;Computer Vision;https://github.com/rishigami/Swin-Transformer-TF
"""![Swin Transformer Architecture Diagram](./images/swin-transformer.png)  **Swin Transformer** (the name `Swin` stands for **S**hifted **win**dow) is initially described in [arxiv](https://arxiv.org/abs/2103.14030)  which capably serves as a general-purpose backbone for computer vision. It is basically a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection.  Swin Transformer achieves strong performance on COCO object detection (`58.7 box AP` and `51.1 mask AP` on test-dev) and ADE20K semantic segmentation (`53.5 mIoU` on val)  surpassing previous models by a large margin.    from models.build import build_model   """;Computer Vision;https://github.com/VcampSoldiers/Swin-Transformer-Tensorflow
"""  Can be installed via pip install prox_tv. In our experience  this works on Linux and Mac  but not on Windows.   GCC 7.2+. The code has been tested with GCC 7.2.0   The directory pic_recon contains the following sub-directories:   Download StyleGAN2 weights  for example <br /> bash scripts/get_network_weights.sh FastMRIT1T2.   1. Download the appropriate StyleGAN2 weights. For brain images  use <br /> bash scripts/get_network_weights.sh CompMRIT1T2.<br /> For face images  use <br /> bash scripts/get_network_weights.sh FFHQ. <br /> The weights are stored in stylegan2/nets/   4. Run <br />bash scripts/recon_${alg}.sh <br> for an algorithm alg  where alg can be plstv  csgm  piccs or picgm.   """;Computer Vision;https://github.com/comp-imaging-sci/pic-recon
"""""";Computer Vision;https://github.com/SahinTiryaki/Brain-tumor-segmentation-Vgg19UNet
"""python train.py --algo sac --env Pendulum-v0 --save-replay-buffer   First  you need to install rliable.  Note: Python 3.7+ is required in that case.   Note: to download the repo with the trained agents  you must use git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo in order to clone the submodule too.   python train.py --algo ppo --env MountainCar-v0 -optimize --study-name test --storage sqlite:///example.db   You can specify in the hyperparameter config one or more wrapper to use around the environment:   Note that you can easily specify parameters too.   Note: if you want to pass a string  you need to escape it like that: my_string:""'value'""   See https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/gym/pybullet_envs.  Similar to MuJoCo Envs but with a ~free~ (MuJoCo 2.1.0+ is now free!) easy to install simulator: pybullet. We are using BulletEnv-v0 version.  Note: those environments are derived from Roboschool and are harder than the Mujoco version (see Pybullet issue)   See https://gym.openai.com/envs/#robotics and https://github.com/DLR-RM/rl-baselines3-zoo/pull/71  MuJoCo version: 1.50.1.0  Gym version: 0.18.0   See https://github.com/qgallouedec/panda-gym/.   See https://github.com/maximecb/gym-minigrid   Note that you need to specify --gym-packages gym_minigrid with enjoy.py and train.py as it is not a standard Gym environment  as well as installing the custom Gym package module or putting it in python path.  pip install gym-minigrid  python train.py --algo ppo --env MiniGrid-DoorKey-5x5-v0 --gym-packages gym_minigrid   You can train agents online using colab notebook.   apt-get install swig cmake ffmpeg  pip install -r requirements.txt   GPU image:   ./scripts/run_docker_cpu.sh python train.py --algo ppo --env CartPole-v1  To run tests  first install pytest  then:  make pytest   """;Reinforcement Learning;https://github.com/DLR-RM/rl-baselines3-zoo
"""***Note***: there is now a PyTorch version of this toolkit ([fairseq-py](https://github.com/pytorch/fairseq)) and new development efforts will focus on it. The Lua version is preserved here  but is provided without any support.  This is fairseq  a sequence-to-sequence learning toolkit for [Torch](http://torch.ch/) from Facebook AI Research tailored to Neural Machine Translation (NMT). It implements the convolutional NMT models proposed in [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) and [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344) as well as a standard LSTM-based model. It features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU. We provide pre-trained models for English to French  English to German and English to Romanian translation.  ![Model](fairseq.gif)   * A computer running macOS or Linux * For training new models  you'll also need a NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl) * A [Torch installation](http://torch.ch/docs/getting-started.html). For maximum speed  we recommend using LuaJIT and [Intel MKL](https://software.intel.com/en-us/intel-mkl). * A recent version [nn](https://github.com/torch/nn). The minimum required version is from May 5th  2017. A simple `luarocks install nn` is sufficient to update your locally installed version.  Install fairseq by cloning the GitHub repository and running ``` luarocks make rocks/fairseq-scm-1.rockspec ``` LuaRocks will fetch and build any additional dependencies that may be missing. In order to install the CPU-only version (which is only useful for translating new data with an existing model)  do ``` luarocks make rocks/fairseq-cpu-scm-1.rockspec ```  The LuaRocks installation provides a command-line tool that includes the following functionality: * `fairseq preprocess`: Data pre-processing: build vocabularies and binarize training data * `fairseq train`: Train a new model on one or multiple GPUs * `fairseq generate`: Translate pre-processed data with a trained model * `fairseq generate-lines`: Translate raw text with a trained model * `fairseq score`: BLEU scoring of generated translations against reference translations * `fairseq tofloat`: Convert a trained model to a CPU model * `fairseq optimize-fconv`: Optimize a fully convolutional model for generation. This can also be achieved by passing the `-fconvfast` flag to the generation scripts.   $ cd data/  $ bash prepare-iwslt14.sh  $ cd ..   $ mkdir -p trainings/blstm   $ mkdir -p trainings/fconv   $ mkdir -p trainings/convenc   Use the CUDA_VISIBLE_DEVICES environment variable to select specific GPUs or -ngpus to change the number of GPU devices that will be used.   | Timings: setup 0.1s (0.1%)  encoder 1.9s (1.4%)  decoder 108.9s (79.9%)  search_results 0.0s (0.0%)  search_prune 12.5s (9.2%)   """;Audio;https://github.com/facebookresearch/fairseq
"""![EPS](figure/figure_EPS.png) Existing studies in weakly-supervised semantic segmentation (WSSS) using image-level weak supervision have several limitations:  sparse object coverage  inaccurate object boundaries   and co-occurring pixels from non-target objects.  To overcome these challenges  we propose a novel framework   namely Explicit Pseudo-pixel Supervision (EPS)   which learns from pixel-level feedback by combining two weak supervisions;  the image-level label provides the object identity via the localization map  and the saliency map from the off-the-shelf saliency detection model  offers rich boundaries. We devise a joint training strategy to fully  utilize the complementary relationship between both information.  Our method can obtain accurate object boundaries and discard co-occurring pixels   thereby significantly improving the quality of pseudo-masks.    - Python 3.6 - Pytorch >= 1.0.0 - Torchvision >= 0.2.2 - MXNet - Pillow - opencv-python (opencv for Python)        ```bash   bash script/vo12_cls.sh   bash script/voc12_eps.sh   bash script/coco_cls.sh   bash script/coco_eps.sh   """;Computer Vision;https://github.com/halbielee/EPS
"""YOLOX is an anchor-free version of YOLO  with a simpler design but better performance! It aims to bridge the gap between research and industrial communities. For more details  please refer to our [report on Arxiv](https://arxiv.org/abs/2107.08430).  This repo is an implementation of [MegEngine](https://github.com/MegEngine/MegEngine) version YOLOX  there is also a [PyTorch implementation](https://github.com/Megvii-BaseDetection/YOLOX).  <img src=""assets/git_fig.png"" width=""1000"" >   „Äê2021/08/05„Äë We release MegEngine version YOLOX.   [ ] More models of megEngine version.   CUDA -- 1080TI @ cuda-10.1-cudnn-v7.6.3-TensorRT-6.0.1.5.sh @ Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz   <details> <summary>Installation</summary>  Step1. Install YOLOX. ```shell git clone git@github.com:MegEngine/YOLOX.git cd YOLOX pip3 install -U pip && pip3 install -r requirements.txt pip3 install -v -e .  #: or  python3 setup.py develop ``` Step2. Install [pycocotools](https://github.com/cocodataset/cocoapi).  ```shell pip3 install cython; pip3 install 'git+https://github.com/cocodataset/cocoapi.git#:subdirectory=PythonAPI' ```  </details>  <details> <summary>Demo</summary>  Step1. Download a pretrained model from the benchmark table.  Step2. Use either -n or -f to specify your detector's config. For example:  ```shell python tools/demo.py image -n yolox-tiny -c /path/to/your/yolox_tiny.pkl --path assets/dog.jpg --conf 0.25 --nms 0.45 --tsize 416 --save_result --device [cpu/gpu] ``` or ```shell python tools/demo.py image -f exps/default/yolox_tiny.py -c /path/to/your/yolox_tiny.pkl --path assets/dog.jpg --conf 0.25 --nms 0.45 --tsize 416 --save_result --device [cpu/gpu] ``` Demo for video: ```shell python tools/demo.py video -n yolox-s -c /path/to/your/yolox_s.pkl --path /path/to/your/video --conf 0.25 --nms 0.45 --tsize 416 --save_result --device [cpu/gpu] ```   </details>  <details> <summary>Reproduce our results on COCO</summary>  Step1. Prepare COCO dataset ```shell cd <YOLOX_HOME> ln -s /path/to/your/COCO ./datasets/COCO ```  Step2. Reproduce our results on COCO by specifying -n:  ```shell python tools/train.py -n yolox-tiny -d 8 -b 128 ``` * -d: number of gpu devices * -b: total batch size  the recommended number for -b is num-gpu * 8  When using -f  the above commands are equivalent to:  ```shell python tools/train.py -f exps/default/yolox-tiny.py -d 8 -b 128 ```  </details>   <details> <summary>Evaluation</summary>  We support batch testing for fast evaluation:  ```shell python tools/eval.py -n  yolox-tiny -c yolox_tiny.pkl -b 64 -d 8 --conf 0.001 [--fuse] ``` * --fuse: fuse conv and bn * -d: number of GPUs used for evaluation. DEFAULT: All GPUs available will be used. * -b: total batch size across on all GPUs  To reproduce speed test  we use the following command: ```shell python tools/eval.py -n  yolox-tiny -c yolox_tiny.pkl -b 1 -d 1 --conf 0.001 --fuse ```  </details>   <details> <summary>Tutorials</summary>  *  [Training on custom data](docs/train_custom_data.md).  </details>     """;Computer Vision;https://github.com/MegEngine/YOLOX
"""***Note***: there is now a PyTorch version of this toolkit ([fairseq-py](https://github.com/pytorch/fairseq)) and new development efforts will focus on it. The Lua version is preserved here  but is provided without any support.  This is fairseq  a sequence-to-sequence learning toolkit for [Torch](http://torch.ch/) from Facebook AI Research tailored to Neural Machine Translation (NMT). It implements the convolutional NMT models proposed in [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) and [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344) as well as a standard LSTM-based model. It features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU. We provide pre-trained models for English to French  English to German and English to Romanian translation.  ![Model](fairseq.gif)   * A computer running macOS or Linux * For training new models  you'll also need a NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl) * A [Torch installation](http://torch.ch/docs/getting-started.html). For maximum speed  we recommend using LuaJIT and [Intel MKL](https://software.intel.com/en-us/intel-mkl). * A recent version [nn](https://github.com/torch/nn). The minimum required version is from May 5th  2017. A simple `luarocks install nn` is sufficient to update your locally installed version.  Install fairseq by cloning the GitHub repository and running ``` luarocks make rocks/fairseq-scm-1.rockspec ``` LuaRocks will fetch and build any additional dependencies that may be missing. In order to install the CPU-only version (which is only useful for translating new data with an existing model)  do ``` luarocks make rocks/fairseq-cpu-scm-1.rockspec ```  The LuaRocks installation provides a command-line tool that includes the following functionality: * `fairseq preprocess`: Data pre-processing: build vocabularies and binarize training data * `fairseq train`: Train a new model on one or multiple GPUs * `fairseq generate`: Translate pre-processed data with a trained model * `fairseq generate-lines`: Translate raw text with a trained model * `fairseq score`: BLEU scoring of generated translations against reference translations * `fairseq tofloat`: Convert a trained model to a CPU model * `fairseq optimize-fconv`: Optimize a fully convolutional model for generation. This can also be achieved by passing the `-fconvfast` flag to the generation scripts.   $ cd data/  $ bash prepare-iwslt14.sh  $ cd ..   $ mkdir -p trainings/blstm   $ mkdir -p trainings/fconv   $ mkdir -p trainings/convenc   Use the CUDA_VISIBLE_DEVICES environment variable to select specific GPUs or -ngpus to change the number of GPU devices that will be used.   | Timings: setup 0.1s (0.1%)  encoder 1.9s (1.4%)  decoder 108.9s (79.9%)  search_results 0.0s (0.0%)  search_prune 12.5s (9.2%)   """;General;https://github.com/facebookresearch/fairseq
"""***Note***: there is now a PyTorch version of this toolkit ([fairseq-py](https://github.com/pytorch/fairseq)) and new development efforts will focus on it. The Lua version is preserved here  but is provided without any support.  This is fairseq  a sequence-to-sequence learning toolkit for [Torch](http://torch.ch/) from Facebook AI Research tailored to Neural Machine Translation (NMT). It implements the convolutional NMT models proposed in [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) and [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344) as well as a standard LSTM-based model. It features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU. We provide pre-trained models for English to French  English to German and English to Romanian translation.  ![Model](fairseq.gif)   * A computer running macOS or Linux * For training new models  you'll also need a NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl) * A [Torch installation](http://torch.ch/docs/getting-started.html). For maximum speed  we recommend using LuaJIT and [Intel MKL](https://software.intel.com/en-us/intel-mkl). * A recent version [nn](https://github.com/torch/nn). The minimum required version is from May 5th  2017. A simple `luarocks install nn` is sufficient to update your locally installed version.  Install fairseq by cloning the GitHub repository and running ``` luarocks make rocks/fairseq-scm-1.rockspec ``` LuaRocks will fetch and build any additional dependencies that may be missing. In order to install the CPU-only version (which is only useful for translating new data with an existing model)  do ``` luarocks make rocks/fairseq-cpu-scm-1.rockspec ```  The LuaRocks installation provides a command-line tool that includes the following functionality: * `fairseq preprocess`: Data pre-processing: build vocabularies and binarize training data * `fairseq train`: Train a new model on one or multiple GPUs * `fairseq generate`: Translate pre-processed data with a trained model * `fairseq generate-lines`: Translate raw text with a trained model * `fairseq score`: BLEU scoring of generated translations against reference translations * `fairseq tofloat`: Convert a trained model to a CPU model * `fairseq optimize-fconv`: Optimize a fully convolutional model for generation. This can also be achieved by passing the `-fconvfast` flag to the generation scripts.   $ cd data/  $ bash prepare-iwslt14.sh  $ cd ..   $ mkdir -p trainings/blstm   $ mkdir -p trainings/fconv   $ mkdir -p trainings/convenc   Use the CUDA_VISIBLE_DEVICES environment variable to select specific GPUs or -ngpus to change the number of GPU devices that will be used.   | Timings: setup 0.1s (0.1%)  encoder 1.9s (1.4%)  decoder 108.9s (79.9%)  search_results 0.0s (0.0%)  search_prune 12.5s (9.2%)   """;Sequential;https://github.com/facebookresearch/fairseq
"""pytorch&gt;=1.7.0  python&gt;=3.6  Ubuntu/Windows  see more in 'requirements.txt'  cd /path/to/your/work  git clone https://github.com/zhangming8/yolox-pytorch.git  cd yolox-pytorch   All weights can be downloaded   download COCO:   See more example in 'train.sh'      Your can also use the following shell scripts:   4. (Optional) you can visualize the converted annotations by:   5. run train.sh  evaluate.sh  predict.sh (are the same as COCO)   """;Computer Vision;https://github.com/zhangming8/yolox-pytorch
"""""";Computer Vision;https://github.com/yeyinthtoon/tf2-resmlp
"""""";General;https://github.com/yeyinthtoon/tf2-resmlp
"""Tested on Python 3.7 with PyTorch 1.7.1 or 1.8.   pip install -r requirements.txt  pip install git+https://github.com/openai/CLIP.git   """;Computer Vision;https://github.com/eps696/aphantasia
"""Please refer to [get_started.md](https://github.com/open-mmlab/mmdetection/blob/master/docs/get_started.md) for installation and dataset preparation.   MoBY pre-trained models can be downloaded from MoBY with Swin Transformer.   We use apex for mixed precision training by default. To install apex  run:  git clone https://github.com/NVIDIA/apex  cd apex  pip install -v --disable-pip-version-check --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./   : do not use mmdet version fp16   """;Computer Vision;https://github.com/hsnam-lunit/mmdetection_swin
"""Try `pip install -r requirements.txt` after you clone the repository.  If you want to use [BPE](https://github.com/rsennrich/subword-nmt)  to enable convertion to C libraries  to try the simple MT server and to support Chinese word segmentation supported by [pynlpir](https://github.com/tsroten/pynlpir) in this implementation  you should also install those dependencies in `requirements.opt.txt` with `pip install -r requirements.opt.txt`.   bash scripts/mktest.sh  configure variables in scripts/mktest.sh for your usage (while keep the other settings consistent with those in scripts/mkbpe.sh and scripts/mktrain.sh).   """;General;https://github.com/hfxunlp/transformer
"""Try `pip install -r requirements.txt` after you clone the repository.  If you want to use [BPE](https://github.com/rsennrich/subword-nmt)  to enable convertion to C libraries  to try the simple MT server and to support Chinese word segmentation supported by [pynlpir](https://github.com/tsroten/pynlpir) in this implementation  you should also install those dependencies in `requirements.opt.txt` with `pip install -r requirements.opt.txt`.   bash scripts/mktest.sh  configure variables in scripts/mktest.sh for your usage (while keep the other settings consistent with those in scripts/mkbpe.sh and scripts/mktrain.sh).   """;Natural Language Processing;https://github.com/hfxunlp/transformer
"""1. Launch a new CloudFormation stack with the provided template under `setup/template.yaml`. To learn about how to deploy CloudFormation stacks  please refer to the [documentation](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-create-stack.html). 2. Define a name for the stack and enter a *Project Name* parameter  that is unique in your account. It must be compliant with Amazon S3 bucket names  so please choose a lowercase string here. The project name that you define during stack creation defines the name of many of the resources that are being created with the stack. Make sure to take note of this parameter. 3. Have a look at the CloudFormation stack outputs and take note of the provided information.   This repository has the following directory structure:   ¬†¬†      ‚îú‚îÄ‚îÄ requirements.txt                <-- python dependencies needed for training   Install the dependencies by running sudo apt update -y &amp;&amp; sudo apt install -y build-essential procps and pip install -r requirements.txt to install the necessary python dependencies.   Before running the actual application  you need to define an environment variable which determines whether you want to run the app with the Flask development server or the with a production-ready uWSGI server (using waitress). For now  lets use the production server by setting export SM_APP_ENV=prod. For debugging  you might want to later change this to dev.         ""name"": ""img-classification""         ""version"": ""1""    """;Computer Vision;https://github.com/aws-samples/amazon-sagemaker-edge-defect-detection-computer-vision
"""DCA: https://www.nature.com/articles/s41467-018-07931-2   https://github.com/theislab/dca   """;Computer Vision;https://github.com/ElliottKasoar/gene-dag-vae
"""DCA: https://www.nature.com/articles/s41467-018-07931-2   https://github.com/theislab/dca   """;General;https://github.com/ElliottKasoar/gene-dag-vae
"""""";Computer Vision;https://github.com/GeoDQ/Seismic-image-segmentation
"""    |‚Äî‚ÄîCOCO   """;Computer Vision;https://github.com/liulianjushi/yolo_v3_tf2.0
""" I have been using this product for a few years now and it is the best thing on the market to keep your teeth white. It does not taste bad at all like some of these other products do. The only problem with this product is that you need to use it every day or else they will start coming back in after about 2 weeks. But if you do that  then it's worth it. You can also buy them from Amazon but shipping takes forever. So just make sure you order enough so you don't run out.     I have been using this product for a few years now and it is the best thing on the market to keep your teeth white. It does not taste bad at all like some of these other products do. The only problem with this product is that you need to use it every day or else they will start coming back in after about 2 weeks. But if you do that  then it's worth it. You can also buy them from Amazon but shipping takes forever. So just make sure you order enough so you don't run out.    Questions Q: What is the capital of India?  A: mumbai.   Q: Who was a British politician who served as Prime Minister from 1922 to 1924?   A: edward viibert   Q: The name of which city in New South Wales has been used for many years by the Australian National Football team?   A: sydney   Q: Which American actor starred with his wife and daughter on the television series 'Family Affair'?   A: james coburn   Q: In what year did the first edition of this book appear?   A: 1962   Q: How long does it take to make one pound of sausage?   Questions Q: What is the capital of India?   A: mumbai.   Q: Who was a British politician who served as Prime Minister from 1922 to 1924?   A: edward viibert   Q: The name of which city in New South Wales has been used for many years by the Australian National Football team?   A: sydney   Q: Which American actor starred with his wife and daughter on the television series 'Family Affair'?   A: james coburn   Q: In what year did the first edition of this book appear?   A: 1962   Q: How long does it take to make one pound of sausage?   """;Natural Language Processing;https://github.com/JunnYu/paddle_ctrl
"""""";Computer Vision;https://github.com/tkuanlun350/Kaggle_Ship_Detection_2018
"""See the directory fastgcn. Start from test_fastgcn.m.   See the directory sgd_paper. Start from test_1layer.m and test_2layer.m.   """;Graphs;https://github.com/jiechenjiechen/FastGCN-matlab
"""""";Computer Vision;https://github.com/smhall97/deep_dreaming_music
"""- Install requirements   ```bash   cd pix2pixGAN   pip install -r requirements.txt   ``` - [Prepare dataset](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md) - File structure   ```   pix2pixGAN   ‚îú‚îÄ‚îÄ ...   ‚îî‚îÄ‚îÄdatasets      ‚îú‚îÄ‚îÄ ...      ‚îî‚îÄ‚îÄLLVIP         ‚îú‚îÄ‚îÄ train         |   ‚îú‚îÄ‚îÄ 010001.jpg         |   ‚îú‚îÄ‚îÄ 010002.jpg         |   ‚îú‚îÄ‚îÄ 010003.jpg         |   ‚îî‚îÄ‚îÄ ...         ‚îî‚îÄ‚îÄ test             ‚îú‚îÄ‚îÄ 190001.jpg             ‚îú‚îÄ‚îÄ 190002.jpg             ‚îú‚îÄ‚îÄ 190003.jpg             ‚îî‚îÄ‚îÄ ...   ```   - Install requirements   ```bash   git clone https://github.com/bupt-ai-cz/LLVIP.git   cd LLVIP/imagefusion_densefuse      #: Create your virtual environment using anaconda   conda create -n Densefuse python=3.7   conda activate Densefuse      conda install scikit-image scipy==1.2.1 tensorflow-gpu==1.14.0   ``` - File structure   ```   imagefusion_densefuse   ‚îú‚îÄ‚îÄ ...   ‚îú‚îÄ‚îÄdatasets   |  ‚îú‚îÄ‚îÄ010001_ir.jpg   |  ‚îú‚îÄ‚îÄ010001_vi.jpg   |  ‚îî‚îÄ‚îÄ ...   ‚îú‚îÄ‚îÄtest   |  ‚îú‚îÄ‚îÄ190001_ir.jpg   |  ‚îú‚îÄ‚îÄ190001_vi.jpg   |  ‚îî‚îÄ‚îÄ ...   ‚îî‚îÄ‚îÄLLVIP      ‚îú‚îÄ‚îÄ infrared      |   ‚îú‚îÄ‚îÄtrain      |   |  ‚îú‚îÄ‚îÄ 010001.jpg      |   |  ‚îú‚îÄ‚îÄ 010002.jpg      |   |  ‚îî‚îÄ‚îÄ ...      |   ‚îî‚îÄ‚îÄtest      |      ‚îú‚îÄ‚îÄ 190001.jpg      |      ‚îú‚îÄ‚îÄ 190002.jpg      |      ‚îî‚îÄ‚îÄ ...      ‚îî‚îÄ‚îÄ visible          ‚îú‚îÄ‚îÄtrain          |   ‚îú‚îÄ‚îÄ 010001.jpg          |   ‚îú‚îÄ‚îÄ 010002.jpg          |   ‚îî‚îÄ‚îÄ ...          ‚îî‚îÄ‚îÄ test              ‚îú‚îÄ‚îÄ 190001.jpg              ‚îú‚îÄ‚îÄ 190002.jpg              ‚îî‚îÄ‚îÄ ...   ```     - Install requirements   ```bash   git clone https://github.com/bupt-ai-cz/LLVIP.git   cd LLVIP/FusionGAN   #: Create your virtual environment using anaconda   conda create -n FusionGAN python=3.7   conda activate FusionGAN      conda install matplotlib scipy==1.2.1 tensorflow-gpu==1.14.0    pip install opencv-python   sudo apt install libgl1-mesa-glx   ``` - File structure   ```   FusionGAN   ‚îú‚îÄ‚îÄ ...   ‚îú‚îÄ‚îÄ Test_LLVIP_ir   |   ‚îú‚îÄ‚îÄ 190001.jpg   |   ‚îú‚îÄ‚îÄ 190002.jpg   |   ‚îî‚îÄ‚îÄ ...   ‚îú‚îÄ‚îÄ Test_LLVIP_vi   |   ‚îú‚îÄ‚îÄ 190001.jpg   |   ‚îú‚îÄ‚îÄ 190002.jpg   |   ‚îî‚îÄ‚îÄ ...   ‚îú‚îÄ‚îÄ Train_LLVIP_ir   |   ‚îú‚îÄ‚îÄ 010001.jpg   |   ‚îú‚îÄ‚îÄ 010002.jpg   |   ‚îî‚îÄ‚îÄ ...   ‚îî‚îÄ‚îÄ Train_LLVIP_vi       ‚îú‚îÄ‚îÄ 010001.jpg       ‚îú‚îÄ‚îÄ 010002.jpg       ‚îî‚îÄ‚îÄ ...   ```  Install requirements    bash    git clone https://github.com/bupt-ai-cz/LLVIP.git    cd LLVIP/yolov5    pip install -r requirements.txt   """;Computer Vision;https://github.com/bupt-ai-cz/LLVIP
"""- Install requirements   ```bash   cd pix2pixGAN   pip install -r requirements.txt   ``` - [Prepare dataset](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md) - File structure   ```   pix2pixGAN   ‚îú‚îÄ‚îÄ ...   ‚îî‚îÄ‚îÄdatasets      ‚îú‚îÄ‚îÄ ...      ‚îî‚îÄ‚îÄLLVIP         ‚îú‚îÄ‚îÄ train         |   ‚îú‚îÄ‚îÄ 010001.jpg         |   ‚îú‚îÄ‚îÄ 010002.jpg         |   ‚îú‚îÄ‚îÄ 010003.jpg         |   ‚îî‚îÄ‚îÄ ...         ‚îî‚îÄ‚îÄ test             ‚îú‚îÄ‚îÄ 190001.jpg             ‚îú‚îÄ‚îÄ 190002.jpg             ‚îú‚îÄ‚îÄ 190003.jpg             ‚îî‚îÄ‚îÄ ...   ```   - Install requirements   ```bash   git clone https://github.com/bupt-ai-cz/LLVIP.git   cd LLVIP/imagefusion_densefuse      #: Create your virtual environment using anaconda   conda create -n Densefuse python=3.7   conda activate Densefuse      conda install scikit-image scipy==1.2.1 tensorflow-gpu==1.14.0   ``` - File structure   ```   imagefusion_densefuse   ‚îú‚îÄ‚îÄ ...   ‚îú‚îÄ‚îÄdatasets   |  ‚îú‚îÄ‚îÄ010001_ir.jpg   |  ‚îú‚îÄ‚îÄ010001_vi.jpg   |  ‚îî‚îÄ‚îÄ ...   ‚îú‚îÄ‚îÄtest   |  ‚îú‚îÄ‚îÄ190001_ir.jpg   |  ‚îú‚îÄ‚îÄ190001_vi.jpg   |  ‚îî‚îÄ‚îÄ ...   ‚îî‚îÄ‚îÄLLVIP      ‚îú‚îÄ‚îÄ infrared      |   ‚îú‚îÄ‚îÄtrain      |   |  ‚îú‚îÄ‚îÄ 010001.jpg      |   |  ‚îú‚îÄ‚îÄ 010002.jpg      |   |  ‚îî‚îÄ‚îÄ ...      |   ‚îî‚îÄ‚îÄtest      |      ‚îú‚îÄ‚îÄ 190001.jpg      |      ‚îú‚îÄ‚îÄ 190002.jpg      |      ‚îî‚îÄ‚îÄ ...      ‚îî‚îÄ‚îÄ visible          ‚îú‚îÄ‚îÄtrain          |   ‚îú‚îÄ‚îÄ 010001.jpg          |   ‚îú‚îÄ‚îÄ 010002.jpg          |   ‚îî‚îÄ‚îÄ ...          ‚îî‚îÄ‚îÄ test              ‚îú‚îÄ‚îÄ 190001.jpg              ‚îú‚îÄ‚îÄ 190002.jpg              ‚îî‚îÄ‚îÄ ...   ```     - Install requirements   ```bash   git clone https://github.com/bupt-ai-cz/LLVIP.git   cd LLVIP/FusionGAN   #: Create your virtual environment using anaconda   conda create -n FusionGAN python=3.7   conda activate FusionGAN      conda install matplotlib scipy==1.2.1 tensorflow-gpu==1.14.0    pip install opencv-python   sudo apt install libgl1-mesa-glx   ``` - File structure   ```   FusionGAN   ‚îú‚îÄ‚îÄ ...   ‚îú‚îÄ‚îÄ Test_LLVIP_ir   |   ‚îú‚îÄ‚îÄ 190001.jpg   |   ‚îú‚îÄ‚îÄ 190002.jpg   |   ‚îî‚îÄ‚îÄ ...   ‚îú‚îÄ‚îÄ Test_LLVIP_vi   |   ‚îú‚îÄ‚îÄ 190001.jpg   |   ‚îú‚îÄ‚îÄ 190002.jpg   |   ‚îî‚îÄ‚îÄ ...   ‚îú‚îÄ‚îÄ Train_LLVIP_ir   |   ‚îú‚îÄ‚îÄ 010001.jpg   |   ‚îú‚îÄ‚îÄ 010002.jpg   |   ‚îî‚îÄ‚îÄ ...   ‚îî‚îÄ‚îÄ Train_LLVIP_vi       ‚îú‚îÄ‚îÄ 010001.jpg       ‚îú‚îÄ‚îÄ 010002.jpg       ‚îî‚îÄ‚îÄ ...   ```  Install requirements    bash    git clone https://github.com/bupt-ai-cz/LLVIP.git    cd LLVIP/yolov5    pip install -r requirements.txt   """;General;https://github.com/bupt-ai-cz/LLVIP
"""train LJSpeech 20 epochs with tf-diffwave   To download LJ-Speech dataset  run under script.  Dataset will be downloaded in '~/tensorflow_datasets' in tfrecord format. If you want to change the download directory  specify `data_dir` parameter of `LJSpeech` initializer.  ```python from dataset.ljspeech import LJSpeech  #: lj = LJSpeech(data_dir=path  download=True) lj = LJSpeech(download=True)  ```  To train model  run [train.py](./train.py).   Checkpoint will be written on `TrainConfig.ckpt`  tensorboard summary on `TrainConfig.log`.  ```bash python train.py tensorboard --logdir .\log ```  If you want to train model from raw audio  specify audio directory and turn on the flag `--from-raw`.  ```bash python .\train.py --data-dir D:\LJSpeech-1.1\wavs --from-raw ```  To start to train from previous checkpoint  `--load-epoch` is available.  ```bash python .\train.py --load-epoch 20 --config D:\tf\ckpt\glowtts.json ```  To inference the audio  run [inference.py](./inference.py). \ Since this code is for POC  only alphabets and several special characters are available  reference [TextNormalizer.GRAPHEMES](./datasets/normalizer.py).  ```bash python .\inference.py     --config D:\tf\ckpt\glowtts.json     --ckpt D:\tf\ckpt\glowtts\glowtts_20.ckpt-1     --text ""Hello  my name is revsic"" ```  Pretrained checkpoints are relased on [releases](https://github.com/revsic/tf-glow-tts/releases).  To use pretrained model  download files and unzip it. Followings are sample script.  ```py from config import Config from glowtts import GlowTTS  with open('glowtts.json') as f:     config = Config.load(json.load(f))  tts = GlowTTS(config.model) tts.restore('./glowtts_20.ckpt-1').expect_partial() ```   """;Audio;https://github.com/revsic/tf-glow-tts
"""In order to build fastText  use the following:  $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ make   You can also quantize a supervised model to reduce its memory usage with the following command:   This library has two main use cases: word representation learning and text classification. These were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).   """;Natural Language Processing;https://github.com/xuzhezhaozhao/fastText_reading
"""Based on the recommendation from HuggingFace  both finetuning and eval are 30% faster with ```--fp16```. For that you need to install ```apex```. ```bash $ git clone https://github.com/NVIDIA/apex $ cd apex $ pip install -v --disable-pip-version-check --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```   Tested with Python 3.7 via virtual environment. Clone the repo  go to the repo folder  setup the virtual environment  and install the required packages: ```bash $ python3.7 -m venv venv $ source venv/bin/activate $ pip install -r requirements.txt ```   $ wget https://cdn-datasets.huggingface.co/summarization/xsum.tar.gz  $ tar -xzvf xsum.tar.gz   """;Natural Language Processing;https://github.com/chz816/esacl
"""The following are example detections.   """;Computer Vision;https://github.com/DrMMZ/RetinaNet
"""The following are example detections.   """;General;https://github.com/DrMMZ/RetinaNet
"""bash download_teddata.sh    bash tools/bpe_pipeline_bilingual.sh src_lang tgt_lang   bash tools/bpe_pipeline_fully_shared_multilingual.sh src_lang tgt_lang1 tgt_lang2   bash tools/bpe_pipeline_MT.sh src_lang tgt_lang1 tgt_lang2 share_sublayer share_attn   bash tools/bpe_pipeline_MT.sh src_lang tgt_lang1 tgt_lang2 k q self source   """;General;https://github.com/DevSinghSachan/multilingual_nmt
"""bash download_teddata.sh    bash tools/bpe_pipeline_bilingual.sh src_lang tgt_lang   bash tools/bpe_pipeline_fully_shared_multilingual.sh src_lang tgt_lang1 tgt_lang2   bash tools/bpe_pipeline_MT.sh src_lang tgt_lang1 tgt_lang2 share_sublayer share_attn   bash tools/bpe_pipeline_MT.sh src_lang tgt_lang1 tgt_lang2 k q self source   """;Natural Language Processing;https://github.com/DevSinghSachan/multilingual_nmt
"""""";General;https://github.com/hendrycks/GELUs
"""1) Pick one or more models and download corresponding weights to the `weights` folder:  - For highest quality: [dpt_large](https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt) - For moderately less quality  but better speed on CPU and slower GPUs: [dpt_hybrid](https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-midas-501f0c75.pt) - For real-time applications on resource-constrained devices: [midas_v21_small](https://github.com/AlexeyAB/MiDaS/releases/download/midas_dpt/midas_v21_small-70d6b9c8.pt) - Legacy convolutional model: [midas_v21](https://github.com/AlexeyAB/MiDaS/releases/download/midas_dpt/midas_v21-f6b98070.pt)  2) Set up dependencies:       ```shell     conda install pytorch torchvision opencv     pip install timm     ```     The code was tested with Python 3.7  PyTorch 1.8.0  OpenCV 4.5.1  and timm 0.4.5.        | Big models: | | | | | | | GPU RTX 3090 |   1) Place one or more input images in the folder `input`.  2) Run the model:      ```shell     python run.py --model_type dpt_large     python run.py --model_type dpt_hybrid      python run.py --model_type midas_v21_small     python run.py --model_type midas_v21     ```  3) The resulting inverse depth maps are written to the `output` folder.    """;Computer Vision;https://github.com/ahmedmostafa0x61/Depth_Estimation
"""""";Natural Language Processing;https://github.com/vdogmcgee/SimCSE-Chinese-Pytorch
"""that it has previously seen.   ``` python   ``` python   Machine learning has become an integral part of many of the tools that are used every day.  There has been a huge amount of progress on improving the global accuracy of machine learning  models but calculating how likely a single prediction is to be correct has seen considerably less  progress.  Most algorithms will still produce a prediction  even if this is in a part of the feature space the  algorithm has no information about.  This could be because the feature vector is unlike anything seen during training  or because the  feature vector falls in a part of the feature space where there is a large amount of uncertainty  such as if the border between two classes overlaps. In cases like this the prediction may well be meaningless.  In most models  it is impossible to distinguish this sort of meaningless prediction from a sensible  prediction.  MACEst addresses this situation by providing an additional confidence estimate.  In some areas such as Finance  Infrastructure  or Healthcare  making a single bad prediction can  have major consequences. It is important in these situations that a model is able to understand how likely any prediction it  makes is to be correct before acting upon it.  It is often even more important in these situations that any model *knows what it doesn't know* so  that it will not blindly make bad predictions.   If a model has been trained to classify images of cats and dogs  and we want to predict an image of  a poodle  we find the k most poodle-like cats and the k most poodle-like dogs.  We then calculate how accurate the model was on these sets of images  and how similar the poodle is  to each of these k cats and k dogs. We combine these two to produce a confidence estimate for each  class.  As the poodle-like cats will likely be strange cats  they will be harder to classify and the  accuracy will be lower for these than the poodle-like dogs this combined with the fact that image  will be considerably more similar to poodle-like dogs the confidence of the dog prediction will be  high.  If we now try to classify an image of a horse  we find that the new image is very **dissimilar** to  both cats and dogs  so the similarity term dominates and the model will return an approximately  uniform distribution  this can be interpreted as MACEst saying ""I don't know what this is because  I've never seen an image of a horse!"".   To install MACEst run the following cmd: ```shell script pip install macest ```  Or add `macest` to your project's `requirements.txt` file as a dependency.    Below shows examples of using MACEst for classification and regression. For more examples  and advanced usage  please see the example [notebooks](./notebooks).   """;General;https://github.com/oracle/macest
"""""";Natural Language Processing;https://github.com/AGiannoutsos/COVID19-document-retrieval-with-BERT
"""""";General;https://github.com/Sakib1263/DenseNet-1D-2D-Tensorflow-Keras
"""""";Computer Vision;https://github.com/Sakib1263/DenseNet-1D-2D-Tensorflow-Keras
"""It is recommended to symlink the dataset root to $orientedreppoints/data. If your folder structure is different  you may need to change the corresponding paths in config files. ``` orientedreppoints |‚Äî‚Äîmmdet |‚Äî‚Äîtools |‚Äî‚Äîconfigs |  |--dota |  |  |--orientedreppoints_r50.py |  |  |--orientedreppoints_r101.py |  |  |--orientedreppoints_swin-t.py |  |--hrsc2016 |  |--ucas-aod |  |--dior-r |‚Äî‚Äîdata |  |‚Äî‚Äîdota |  |  |‚Äî‚Äîtrainval_split |  |  |  |‚Äî‚Äîimages |  |  |  |‚Äî‚ÄîlabelTxt |  |  |  |‚Äî‚Äîtrainval.json |  |  |‚Äî‚Äîtest_split |  |  |  |‚Äî‚Äîimages |  |  |  |‚Äî‚Äîtest.json |  |‚Äî‚ÄîHRSC2016 |  |  |‚Äî‚ÄîTrain |  |  |  |‚Äî‚Äîimages |  |  |  |‚Äî‚ÄîlabelTxt |  |  |  |‚Äî‚Äîtrainval.txt |  |  |  |‚Äî‚Äîtrainval.json |  |  |‚Äî‚ÄîTest |  |  |  |‚Äî‚Äîimages |  |  |  |‚Äî‚Äîtest.txt |  |  |  |‚Äî‚Äîtest.json |  |‚Äî‚ÄîUCAS-AOD |  |‚Äî‚ÄîDIOR-R ``` Note: * `trainval.txt` and `test.txt` in HRSC2016  UCASAOD and DIOR-R are `.txt` files recording image names without extension.    ``` sudo apt-get install swig ``` ``` cd DOTA_devkit swig -c++ -python polyiou.i python setup.py build_ext --inplace ```  a. Create a conda virtual environment and activate it.   ``` conda create -n orientedreppoints python=3.7 -y  source activate orientedreppoints ``` b. Install PyTorch and torchvision following the [official instructions](https://pytorch.org/get-started/previous-versions/)  e.g.  ``` conda install pytorch=1.3 torchvision cudatoolkit=10.0 -c pytorch ``` c. Install orientedreppoints.  ```python  cd OrientedRepPoints pip install -r requirements.txt python setup.py develop  #:or ""pip install -v -e ."" ```   Train  with a single GPU    Test OrientedRepPoints with single GPU.   Our code is based on ![mmdetection](https://github.com/open-mmlab/mmdetection).    """;Computer Vision;https://github.com/LiWentomng/OrientedRepPoints
"""SSKD is implemented based on **FastReID v1.0.0**  it provides a semi-supervised feature learning framework to learn domain-general representations. The framework is shown in   <img src=""images/framework.png"" width=""850"" >   """;General;https://github.com/xiaomingzhid/sskd
"""""";Computer Vision;https://github.com/MasoumehVahedi/Yolo-Object-Detection
"""Aloception's packages are built on top of multiple libraries. Most of them are listed in the **requirements.txt** ``` pip install -r requirements.txt ```  Once the others packages are installed  you still need to install pytorch based on your hardware and environment configuration. Please  ref to the `pytorch website <https://pytorch.org/>`_  for this install.   pytorch  and  pytorch lightning.   <ul>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/getting_started.html"">Getting started</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/aloscene.html"">Aloscene: Computer vision with ease</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/alodataset.html"">Alodataset: Loading your vision datasets</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/alonet.html"">Alonet: Loading & training your models</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/augmented_tensor.html"">About augmented tensors</a></li> </ul>    <ul>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/data_setup.html"">How to setup your data?</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/training_detr.html"">Training Detr</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/finetuning_detr.html"">Finetuning DETR</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/training_panoptic.html"">Training Panoptic Head</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/training_deformable_detr.html"">Training Deformable DETR</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/finetuning_deformable_detr.html"">Finetuning Deformanble DETR</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/tensort_inference.html"">Exporting DETR / Deformable-DETR to TensorRT</a></li> </ul>   """;Computer Vision;https://github.com/Visual-Behavior/aloception
"""Aloception's packages are built on top of multiple libraries. Most of them are listed in the **requirements.txt** ``` pip install -r requirements.txt ```  Once the others packages are installed  you still need to install pytorch based on your hardware and environment configuration. Please  ref to the `pytorch website <https://pytorch.org/>`_  for this install.   pytorch  and  pytorch lightning.   <ul>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/getting_started.html"">Getting started</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/aloscene.html"">Aloscene: Computer vision with ease</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/alodataset.html"">Alodataset: Loading your vision datasets</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/alonet.html"">Alonet: Loading & training your models</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/getting_started/augmented_tensor.html"">About augmented tensors</a></li> </ul>    <ul>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/data_setup.html"">How to setup your data?</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/training_detr.html"">Training Detr</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/finetuning_detr.html"">Finetuning DETR</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/training_panoptic.html"">Training Panoptic Head</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/training_deformable_detr.html"">Training Deformable DETR</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/finetuning_deformable_detr.html"">Finetuning Deformanble DETR</a></li>   <li><a href=""https://visual-behavior.github.io/aloception/tutorials/tensort_inference.html"">Exporting DETR / Deformable-DETR to TensorRT</a></li> </ul>   """;General;https://github.com/Visual-Behavior/aloception
"""""";Computer Vision;https://github.com/salem-devloper/COVID-Lung-Segment
"""""";General;https://github.com/pollyyu/Final_Project_MachineLearning_in_TensorFlow_Berkeley
"""    - https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix   """;Computer Vision;https://github.com/NathanDeMaria/AugmentedCycleGAN
"""    - https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix   """;General;https://github.com/NathanDeMaria/AugmentedCycleGAN
"""""";Computer Vision;https://github.com/chinakook/U-Net
"""Install (Ubuntu {14 16}  GPU)  cuDNN required.   sudo pip install pydot-ng   sudo apt-get install libhdf5-dev  sudo pip install h5py  sudo pip install pydot  sudo pip install nose_parameterized  sudo pip install keras   sudo apt-get install python-opencv  sudo apt-get install python-sklearn   mkdir np_data   For batch_size=64 6Gb GPU memory is required.   """;Computer Vision;https://github.com/EdwardTyantov/ultrasound-nerve-segmentation
"""Run it on colab!   The hyperparameters are for a V100 GPU with 16 GB GPU memory. The 1b_lyrics  5b  and 5b_lyrics top-level priors take up 3.8 GB  10.3 GB  and 11.5 GB.  If you continue to have memory issues after this (or run into issues on your own home setup)  switch to the 1B model.   [PLAYLIST](https://www.youtube.com/playlist?list=PL8kGuiVdKeKhejC3rLR-t5JVweFHzwE9-)  Music is an art of time. It is formed by the colaboration of instruments -composed with many instruments collectively- harmonization of notes. So  music generation with deep neural networks strictly connected with this features of music. There are many models have been proposed so far for generating music. Some of them based on the structure of Recurrent Neural Networks or Generative Adversarial Networks or Variational Autoencoders.  In this work  we tackle the generating music with deep neural networks  especially with Vector Quantized Variational Autoencoders (Oord et al.  2017).   [This folder](https://github.com/inzva/music-generation/tree/main/jukebox/samples) contains our examples for music generation.    """;Computer Vision;https://github.com/inzva/music-generation
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/SYangDong/bert-with-frozen-code
"""Clone the repository:  ```sh git clone https://github.com/ufal/wembedding_service ```  Create a Python virtual environment:  ```sh python -m venv venv ```  Install requirements:  ```sh venv/bin/pip3 install -r requirements.txt ```  Run the service:  ```sh venv/bin/python3 ./start_wembembeddings_server.py 8000 ```   """;Natural Language Processing;https://github.com/ufal/wembedding_service
"""""";General;https://github.com/nashawnch/ComputerVision-FerrariDetective
"""""";Computer Vision;https://github.com/nashawnch/ComputerVision-FerrariDetective
"""I place all the data into `data/` directory  please adjust the following code to match yours data location. Run  ```bash python create_class_map.py --train_df data/train_curated.csv --output_file data/classmap.json ```  This simply creates a JSON file with deterministic classname->label mapping used in all future experiments.   I recommend using some environment manager such as conda or virtualenv in order to avoid potential conflicts between different versions of packages. To install all required packages  simply run `pip install -r requirements.txt`. This might take up to 15 minutes depending on your internet connection speed.   mkdir predictions/   """;General;https://github.com/ex4sperans/freesound-classification
"""vgg : https://github.com/machrisaa/tensorflow-vgg    WGAN : https://github.com/jiamings/wgan   $ os.path.join(dcm_path  patent_no  [LDCT_path|NDCT_path]  '*.' + extension)  The dcm_path directory should look like:   test_npy_save_dir : save directory - test numpy file   """;Computer Vision;https://github.com/daintlab/ct-denoising
"""vgg : https://github.com/machrisaa/tensorflow-vgg    WGAN : https://github.com/jiamings/wgan   $ os.path.join(dcm_path  patent_no  [LDCT_path|NDCT_path]  '*.' + extension)  The dcm_path directory should look like:   test_npy_save_dir : save directory - test numpy file   """;General;https://github.com/daintlab/ct-denoising
"""In order to install the library you just have to follow these steps:  1) Clone this repository: ``` git clone https://github.com/MarcBS/keras.git ``` 2) Include the repository path into your PYTHONPATH: ``` export PYTHONPATH=$PYTHONPATH:/path/to/keras ```   GitHub page: https://github.com/MarcBS   """;General;https://github.com/MarcBS/keras
"""In order to install the library you just have to follow these steps:  1) Clone this repository: ``` git clone https://github.com/MarcBS/keras.git ``` 2) Include the repository path into your PYTHONPATH: ``` export PYTHONPATH=$PYTHONPATH:/path/to/keras ```   GitHub page: https://github.com/MarcBS   """;Natural Language Processing;https://github.com/MarcBS/keras
"""An easy  alternative way to install Dopamine is as a Python library:  ``` #: Alternatively brew install  see Mac OS X instructions above. sudo apt-get update && sudo apt-get install cmake pip install dopamine-rl pip install atari-py ```  Depending on your particular system configuration  you may also need to install zlib (see ""Install via source"" above).   Installing from source allows you to modify the agents and experiments as you please  and is likely to be the pathway of choice for long-term use. These instructions assume that you've already set up your favourite package manager (e.g. `apt` on Ubuntu  `homebrew` on Mac OS X)  and that a C++ compiler is available from the command-line (almost certainly the case if your favourite package manager works).  The instructions below assume that you will be running Dopamine in a *virtual environment*. A virtual environment lets you control which dependencies are installed for which program; however  this step is optional and you may choose to ignore it.  Dopamine is a Tensorflow-based framework  and we recommend you also consult the [Tensorflow documentation](https://www.tensorflow.org/install) for additional details.  Finally  these instructions are for Python 2.7. While Dopamine is Python 3 compatible  there may be some additional steps needed during installation.  First install [Anaconda](https://docs.anaconda.com/anaconda/install/)  which we will use as the environment manager  then proceed below.  ``` conda create --name dopamine-env python=3.6 conda activate dopamine-env ```  This will create a directory called `dopamine-env` in which your virtual environment lives. The last command activates the environment.  Install the dependencies below  based on your operating system  and then finally download the Dopamine source  e.g.  ``` git clone https://github.com/google/dopamine.git ```   If you don't have access to a GPU  then replace tensorflow-gpu with   sudo apt-get update &amp;&amp; sudo apt-get install cmake zlib1g-dev  pip install absl-py atari-py gin-config gym opencv-python tensorflow-gpu  brew install cmake zlib  pip install absl-py atari-py gin-config gym opencv-python tensorflow   python -um dopamine.discrete_domains.train \   Acrobot. For example  to train C51 on Cartpole with default settings  run the  following command:  python -um dopamine.discrete_domains.train \   You can train Rainbow on Acrobot with the following command:  python -um dopamine.discrete_domains.train \   """;Reinforcement Learning;https://github.com/ACampero/dopamine
"""""";Computer Vision;https://github.com/mp31192/3D-SpatialTransformationLayer-pytorch
"""""";Graphs;https://github.com/shuix007/HMGNN
"""First  download the above pretrained weights to the `imagenet_models` folder.  Run `test_inference.py` for an example of how to use the pretrained model to make inference.  ``` python test_inference.py ```   """;General;https://github.com/flyyufelix/DenseNet-Keras
"""First  download the above pretrained weights to the `imagenet_models` folder.  Run `test_inference.py` for an example of how to use the pretrained model to make inference.  ``` python test_inference.py ```   """;Computer Vision;https://github.com/flyyufelix/DenseNet-Keras
"""Install python (3.6.3) with TensorFlow (1.8.0) and NumPy (1.16). Numbers in parenthesis were the vesion used for development  but may not be required. Pull the repo  and run if from that folder.   It runs with any OpenAI gym environment.   python3 runner.py --train --env E --steps S --name X   python3 runner.py --test --env E --steps S --name X   """;Reinforcement Learning;https://github.com/mightypirate1/PPO_homebrew
"""""";Computer Vision;https://github.com/becauseofAI/DetNet-Keras
"""- Python (v3.7)  - PyTorch (v0.4)   """;Reinforcement Learning;https://github.com/haje01/distper
"""""";Computer Vision;https://github.com/pvardanis/LevNet
"""Here i just use the default paramaters  but as Google's paper says a 0.2% error is reasonable(reported 92.4%). Maybe some tricks need to be added to the above model.   ``` BERT-NER |____ bert                          #: need git from [here](https://github.com/google-research/bert) |____ cased_L-12_H-768_A-12	    #: need download from [here](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip) |____ data		            #: train data |____ middle_data	            #: middle data (label id map) |____ output			    #: output (final model  predict results) |____ BERT_NER.py		    #: mian code |____ conlleval.pl		    #: eval code |____ run_ner.sh    		    #: run model and eval result  ```    * do_lower_case=False  * num_train_epochs=4.0 * crf=False    ``` accuracy:  98.15%; precision:  90.61%; recall:  88.85%; FB1:  89.72               LOC: precision:  91.93%; recall:  91.79%; FB1:  91.86  1387              MISC: precision:  83.83%; recall:  78.43%; FB1:  81.04  668               ORG: precision:  87.83%; recall:  85.18%; FB1:  86.48  1191               PER: precision:  95.19%; recall:  94.83%; FB1:  95.01  1311 ```  ``` bash run_ner.sh ```   """;Natural Language Processing;https://github.com/habibullah-araphat/BERT-NER-TPU
"""Example of training *resnet20_v1* with *anti-aliasing* and *random crop* augmentation:<br/> ``` python3 train_cifar10.py --mode hybrid --num-gpus 1 -j 8 --batch-size 128 --num-epochs 186 --lr 0.003 --lr-decay 0.1 --lr-decay-epoch 81 122 --wd 0.0001 --optimizer adam --model cifar_resnet20_v1 --antialiasing --random-crop ```   """;Computer Vision;https://github.com/mnikitin/Shift-Invariant-CNNs
"""```bash git clone https://github.com/ShaojieJiang/tldr.git ~/tldr cd ~/tldr; python setup.py develop ```   The following command trains a transformer model with TLDR loss by default  on the DailyDialog dataset: ```bash python examples/train_model.py -m transformer/generator -t dailydialog ```  With arguments `--weight-level sequence` and `--weight-func fl`  you can then train a model using sequence-level weighting and Focal Loss  respectively.  For using our proposed Seq2seq with pre attention  please specify `-m seq2seq` and add argument `--attention-time pre`.  > :information_source: Our implementation of pre-attention is different than > that implemented in the original ParlAI framework.   """;Computer Vision;https://github.com/ShaojieJiang/tldr
"""```bash git clone https://github.com/ShaojieJiang/tldr.git ~/tldr cd ~/tldr; python setup.py develop ```   The following command trains a transformer model with TLDR loss by default  on the DailyDialog dataset: ```bash python examples/train_model.py -m transformer/generator -t dailydialog ```  With arguments `--weight-level sequence` and `--weight-func fl`  you can then train a model using sequence-level weighting and Focal Loss  respectively.  For using our proposed Seq2seq with pre attention  please specify `-m seq2seq` and add argument `--attention-time pre`.  > :information_source: Our implementation of pre-attention is different than > that implemented in the original ParlAI framework.   """;General;https://github.com/ShaojieJiang/tldr
"""<!-- - Guide user how to download your data and set the data pipeline  --> 1. Download the data: - Download dataset [here](http://download.tensorflow.org/example_images/flower_photos.tgz) 2. Extract file and put folder ```train``` and ```validation``` to ```./data``` by using [splitfolders](https://pypi.org/project/split-folders/) - train folder was used for the training process - validation folder was used for validating training result after each epoch  This library use ImageDataGenerator API from Tensorflow 2.0 to load images. Make sure you have some understanding of how it works via [its document](https://keras.io/api/preprocessing/image/) Structure of these folders in ```./data```  ``` train/ ...daisy/ ......daisy0.jpg ......daisy1.jpg ...dandelion/ ......dandelion0.jpg ......dandelion1.jpg ...roses/ ......roses0.jpg ......roses1.jpg ...sunflowers/ ......sunflowers0.jpg ......sunflowers1.jpg ...tulips/ ......tulips0.jpg ......tulips1.jpg ```  ``` validation/ ...daisy/ ......daisy2000.jpg ......daisy2001.jpg ...dandelion/ ......dandelion2000.jpg ......dandelion2001.jpg ...roses/ ......roses2000.jpg ......roses2001.jpg ...sunflowers/ ......sunflowers2000.jpg ......sunflowers2001.jpg ...tulips/ ......tulips2000.jpg ......tulips2001.jpg ```   - Step 1: Make sure you have installed Miniconda. If not yet  see the setup document <a href=""https://docs.conda.io/en/latest/miniconda.html"">here</a>   - Step 2: `cd` into `MLP-Mixer` and use command line ``` conda env create -f environment.yml ```  - Step 3: Run conda environment using the command  ``` conda activate mlp-mixer ```    """;Computer Vision;https://github.com/Nguyendat-bit/MLP-Mixer
"""""";Computer Vision;https://github.com/Yodai1996/CV
"""""";General;https://github.com/Yodai1996/CV
"""""";General;https://github.com/PaperCodeReview/SupCL-TF
"""```sh poetry install poetry shell ```  Now you should be in a console with your virtualenv environment and all your custom python dependencies. Now you can run python.   ```sh pyenv local 3.7.2 ```  > Please python devs  stop using Python 2.x  this is not possible anymore to use such bloated oldies.   Following instructions to install [Poetry](https://python-poetry.org/docs/).  >Why poetry instead of basic requirements.txt?  Because its dependency management is more automatic. Poetry has big defaults & bugs but its dependency management is much more production ready than other Python solutions I've tested and it isolates by default your python environment in a virtual env (The other best solution I've found is a hand-made virtualenv in which I install all my dependencies with a requirements.txt).   If you have GPU(s) to share with me for some time to perform more experiments on those topics  don't hesitate to contact me here on github or on twitter @mandubian.   Poetry Python dependencies management with isolated virtualenv (Poetry config.   ```sh pyenv local 3.7.2 ```  > Please python devs  stop using Python 2.x  this is not possible anymore to use such bloated oldies.   """;Natural Language Processing;https://github.com/mandubian/codenets
""".. image:: https://readthedocs.org/projects/tf-unet/badge/?version=latest   """;Computer Vision;https://github.com/jakeret/tf_unet
"""""";General;https://github.com/ahmedbahaaeldin/Papers-from-Scratch
"""This FCOS implementation is based on [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark). Therefore the installation is the same as original maskrcnn-benchmark.  Please check [INSTALL.md](INSTALL.md) for installation instructions. You may also want to see the original [README.md](MASKRCNN_README.md) of maskrcnn-benchmark.   For users who only want to use FCOS as an object detector in their projects  they can install it by pip. To do so  run: ``` pip install torch  #: install pytorch if you do not have it pip install git+https://github.com/tianzhi0549/FCOS.git #: run this command line for a demo  fcos https://github.com/tianzhi0549/FCOS/raw/master/demo/images/COCO_val2014_000000000885.jpg ``` Please check out [here](fcos/bin/fcos) for the interface usage.   For your convenience  we provide the following trained models (more models are coming soon).   We update batch normalization for MobileNet based models. If you want to use SyncBN  please install pytorch 1.1 or later.   Note that:   Once the installation is done  you can follow the below steps to run a quick demo.           """;Computer Vision;https://github.com/touchylk/fcoseccv
"""""";General;https://github.com/LeeGitaek/Kaggle_Solving
"""""";Computer Vision;https://github.com/edchandler00/unet
"""It's as simple as using the default Keras BatchNormalization:  ```python from frn import FRN as FilterResponseNormalization  model = Sequential() #: [...] model.add(FilterResponseNormalization()) ```  """;General;https://github.com/philipperemy/keras-frn
"""    1) git clone https://github.com/iamilyasedunov/key_word_spotting.git     2) cd key_word_spotting     3) bash docker_start.sh     4) docker attach ISedunov-kws_report     5) cd /home/key_word_spotting/key_word_spotting     6) python train.py     7) python test.py  -------     When you implement streaming mode note that you need to:       self.name = name   from thop import profile  #: !pip install thop   """;Computer Vision;https://github.com/iamilyasedunov/key_word_spotting
"""``` git clone <this repo> cd QHACK2021 python setup.py install ```   For basic functionality of QOSE [QOSE: Finding the optimal circuit architecture](demos/QOSE_demo.ipynb)  For exploring the effects of different embedding methods [QOSE: Different Embeddings](demos/QOSE_different_embeddings.ipynb)  For a demonstration of parallelized execution  [QOSE: MPI and SV1 parallelization on AWS](demos/QOSE_demo_parallel.ipynb)    """;Natural Language Processing;https://github.com/kmz4/QHACK2021
"""""";General;https://github.com/dqqcasia/st
"""""";Natural Language Processing;https://github.com/dqqcasia/st
"""""";Sequential;https://github.com/bionick87/ConvGRUCell-pytorch
"""python 3.7.1  pip install -r requirements.txt„ÅßÁí∞Â¢É„ÇíÊèÉ„Åà„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ   """;Audio;https://github.com/zassou65535/WaveGAN
"""In this repository  we release two best models **detoxGPT** and **condBERT** (see [Methodology](https://github.com/skoltech-nlp/rudetoxifier#methodology) for more details). You can try detoxification inference example in this [notebook](https://github.com/skoltech-nlp/rudetoxifier/blob/main/notebooks/rudetoxifier_inference.ipynb) or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1lSXh8PHGeKTLtuhxYCwHL74qG-V-pkLK?usp=sharing).   Also  you can test our models via [web-demo](https://detoxifier.nlp.zhores.net/) or you can pour out your anger on our [Telegram bot](https://t.me/rudetoxifierbot).  ***  """;Natural Language Processing;https://github.com/skoltech-nlp/rudetoxifier
"""         ËøôÈáåÁõ¥Êé•Á≤òË¥¥Áü•‰πéÁöÑ‰∏ÄÁØáÂ∏ñÂ≠ê: https://zhuanlan.zhihu.com/p/362127299.   """;General;https://github.com/Uranium-Deng/Steganalysis-StegoRemoval
"""""";Computer Vision;https://github.com/Jastot/Rodinka_Neural_Network
"""Go to [Kaggle MNIST Dataset](https://www.kaggle.com/avnishnish/mnist-original) and download Extract data file to get `mnist.mat`data file.   * Recommend using an virtual environment to run ```bash pip install -r requirements.txt ```   ```shell usage: python train.py [-h] -d DATA [-hd HIDDEN] [-ld LATENT] [-lr LEARNING] [-e EPOCHS] [-b BATCH_SIZE] [-m MODEL]  optional arguments:   -h  --help            show this help message and exit   -d DATA  --data DATA  path/to/train/data   -hd HIDDEN  --hidden HIDDEN                         number of hidden unit   -ld LATENT  --latent LATENT                         number of latent unit   -lr LEARNING  --learning LEARNING                         learning rate   -e EPOCHS  --epochs EPOCHS                         epochs   -b BATCH_SIZE  --batch_size BATCH_SIZE                         Batch size   -m MODEL  --model MODEL                         path/to/model/saving/location ```   """;Computer Vision;https://github.com/dungxibo123/vae
"""Go to [Kaggle MNIST Dataset](https://www.kaggle.com/avnishnish/mnist-original) and download Extract data file to get `mnist.mat`data file.   * Recommend using an virtual environment to run ```bash pip install -r requirements.txt ```   ```shell usage: python train.py [-h] -d DATA [-hd HIDDEN] [-ld LATENT] [-lr LEARNING] [-e EPOCHS] [-b BATCH_SIZE] [-m MODEL]  optional arguments:   -h  --help            show this help message and exit   -d DATA  --data DATA  path/to/train/data   -hd HIDDEN  --hidden HIDDEN                         number of hidden unit   -ld LATENT  --latent LATENT                         number of latent unit   -lr LEARNING  --learning LEARNING                         learning rate   -e EPOCHS  --epochs EPOCHS                         epochs   -b BATCH_SIZE  --batch_size BATCH_SIZE                         Batch size   -m MODEL  --model MODEL                         path/to/model/saving/location ```   """;General;https://github.com/dungxibo123/vae
"""If you want to download weights the link in ""weights.txt"" file you can check that otherwise if you want to train on your own coustom dataset you can use man losses like ciou losses giou loss if you want reference you can check it in ""losses.py"" file   """;Computer Vision;https://github.com/ravindra579/object-detection_yolov4
"""Download and extract COCO 2017 train and val images with annotations from [http://cocodataset.org](http://cocodataset.org/#download). We expect the directory structure to be the following: ``` path/to/coco/   annotations/  #: annotation json files   train2017/    #: train images   val2017/      #: val images ```   To evaluate DETR R50 on COCO val5k with a single GPU run:   pip install submitit   First  clone the repository locally: ``` git clone https://github.com/twangnh/pnp-detr ``` Then  install PyTorch 1.5+ and torchvision 0.6+: ``` conda install -c pytorch pytorch torchvision ``` Install pycocotools (for evaluation on COCO) and scipy (for training): ``` conda install cython scipy pip install -U 'git+https://github.com/cocodataset/cocoapi.git#:subdirectory=PythonAPI' ``` That's it  should be good to train and evaluate detection models.  (optional) to work with panoptic install panopticapi: ``` pip install git+https://github.com/cocodataset/panopticapi.git ```   """;Computer Vision;https://github.com/twangnh/pnp-detr
"""              all_conf = all_conf.numpy()                all_class = all_class.numpy()                all_bbox = all_bbox.numpy()   Python3.7  opencv-python  PaddlePaddle 2.1.2   ÁôªÂΩïAI StudioÂèØÂú®Á∫øËøêË°åÔºöhttps://aistudio.baidu.com/aistudio/projectdetail/2259467       parser.add_argument('-v'  '--version'  default='yolo'                        help='use gpu.')   parser.add_argument('-v'  '--version'  default='yolo'                        help='use cuda.')   parser.add_argument('-v'  '--version'  default='yolo'                        help='Use gpu')   """;General;https://github.com/sunlizhuang/YOLOv1-PaddlePaddle
"""step1: clone   - https://github.com/xternalz/WideResNet-pytorch   HardwareÔºöGPU CPU   : clone this repo  git clone https://github.com/PaddlePaddle/Contrib.git  cd wide_resnet   python3 -m pip install -r requirements.txt   ‚îÇ  requirements.txt                #: requirement   | Frame Version | >=Paddle 2.0.2|   | Hardware | GPU„ÄÅCPU |  | download Link |aistudio |   """;Computer Vision;https://github.com/xmy0916/wide_resnet_paddle
"""SCRFD is an efficient high accuracy face detection approach which initially described in [Arxiv](https://arxiv.org/abs/2105.04714).  <img src=""https://github.com/nttstar/insightface-resources/blob/master/images/scrfd_evelope.jpg"" width=""400"" alt=""prcurve""/>   Please refer to [mmdetection](https://github.com/open-mmlab/mmdetection/blob/master/docs/get_started.md) for installation.     1. Install [mmcv](https://github.com/open-mmlab/mmcv). (mmcv-full==1.2.6 and 1.3.3 was tested)   2. Install build requirements and then install mmdet.        ```        pip install -r requirements/build.txt        pip install -v -e .  #: or ""python setup.py develop""        ```   """;Computer Vision;https://github.com/SajjadAemmi/SCR-Face-Detection
"""SCRFD is an efficient high accuracy face detection approach which initially described in [Arxiv](https://arxiv.org/abs/2105.04714).  <img src=""https://github.com/nttstar/insightface-resources/blob/master/images/scrfd_evelope.jpg"" width=""400"" alt=""prcurve""/>   Please refer to [mmdetection](https://github.com/open-mmlab/mmdetection/blob/master/docs/get_started.md) for installation.     1. Install [mmcv](https://github.com/open-mmlab/mmcv). (mmcv-full==1.2.6 and 1.3.3 was tested)   2. Install build requirements and then install mmdet.        ```        pip install -r requirements/build.txt        pip install -v -e .  #: or ""python setup.py develop""        ```   """;General;https://github.com/SajjadAemmi/SCR-Face-Detection
"""```bash $ pip install x-transformers ```   Alternatively  if you would like to use entmax15  you can also do so with one setting as shown below.   Update: It may be that ALiBi enforces a strong local attention across the heads  and may hinder it from attending at distances greater than 1k. To avoid any issues with global message passing  I've decided to introduce another hyperparameter alibi_num_heads  so one can specify less heads for the ALiBi bias   Full encoder / decoder  ```python import torch from x_transformers import XTransformer  model = XTransformer(     dim = 512      enc_num_tokens = 256      enc_depth = 6      enc_heads = 8      enc_max_seq_len = 1024      dec_num_tokens = 256      dec_depth = 6      dec_heads = 8      dec_max_seq_len = 1024      tie_token_emb = True      #: tie embeddings of encoder and decoder )  src = torch.randint(0  256  (1  1024)) src_mask = torch.ones_like(src).bool() tgt = torch.randint(0  256  (1  1024)) tgt_mask = torch.ones_like(tgt).bool()  loss = model(src  tgt  src_mask = src_mask  tgt_mask = tgt_mask) #: (1  1024  512) loss.backward() ```  Decoder-only (GPT-like)  ```python import torch from x_transformers import TransformerWrapper  Decoder  model = TransformerWrapper(     num_tokens = 20000      max_seq_len = 1024      attn_layers = Decoder(         dim = 512          depth = 12          heads = 8     ) ).cuda()  x = torch.randint(0  256  (1  1024)).cuda()  model(x) #: (1  1024  20000) ```  GPT3 would be approximately the following (but you wouldn't be able to run it anyways)  ```python  gpt3 = TransformerWrapper(     num_tokens = 50000      max_seq_len = 2048      attn_layers = Decoder(         dim = 12288          depth = 96          heads = 96          attn_dim_head = 128     ) ).cuda() ```  Encoder-only (BERT-like)  ```python import torch from x_transformers import TransformerWrapper  Encoder  model = TransformerWrapper(     num_tokens = 20000      max_seq_len = 1024      attn_layers = Encoder(         dim = 512          depth = 12          heads = 8     ) ).cuda()  x = torch.randint(0  256  (1  1024)).cuda() mask = torch.ones_like(x).bool()  model(x  mask = mask) #: (1  1024  20000) ```  State of the art image classification  ```python import torch from x_transformers import ViTransformerWrapper  Encoder  model = ViTransformerWrapper(     image_size = 256      patch_size = 32      num_classes = 1000      attn_layers = Encoder(         dim = 512          depth = 6          heads = 8      ) )  img = torch.randn(1  3  256  256) model(img) #: (1  1000) ```  Image -> caption  ```python import torch from x_transformers import ViTransformerWrapper  TransformerWrapper  Encoder  Decoder  encoder = ViTransformerWrapper(     image_size = 256      patch_size = 32      attn_layers = Encoder(         dim = 512          depth = 6          heads = 8     ) )  decoder = TransformerWrapper(     num_tokens = 20000      max_seq_len = 1024      attn_layers = Decoder(         dim = 512          depth = 6          heads = 8          cross_attend = True     ) )  img = torch.randn(1  3  256  256) caption = torch.randint(0  20000  (1  1024))  encoded = encoder(img  return_embeddings = True) decoder(caption  context = encoded) #: (1  1024  20000) ```   """;Reinforcement Learning;https://github.com/lucidrains/x-transformers
"""Part-II: https://medium.com/swlh/abstractive-text-summarization-using-transformers-3e774cc42453   """;General;https://github.com/rojagtap/abstractive_summarizer
"""Part-II: https://medium.com/swlh/abstractive-text-summarization-using-transformers-3e774cc42453   """;Natural Language Processing;https://github.com/rojagtap/abstractive_summarizer
"""**EmoCause** is a dataset of annotated emotion cause words in emotional situations from the [EmpatheticDialogues](https://aclanthology.org/P19-1534.pdf) valid and test set. The goal is to recognize emotion cause words in sentences by training only on sentence-level emotion labels without word-level labels (*i.e.  weakly-supervised emotion cause recognition*). **EmoCause** is based on the fact that humans do not recognize the cause of emotions with supervised learning on word-level cause labels. Thus  we do not provide a training set.  ![figure](images/many_emocause.png)  You can download the **EmoCause** eval set [[here]](https://drive.google.com/file/d/1LR4B47Fna_l63G1X4DZtuttG-GrinnaY/view?usp=sharing).<br> Note  the dataset will be downloaded automatically when you run the experiment command below.    Our code is built on the [ParlAI](https://parl.ai/) framework. We recommend you create a conda environment as follows  ```bash conda env create -f environment.yml ```  and activate it with  ```bash conda activate focused-empathy python -m spacy download en ```   """;Natural Language Processing;https://github.com/skywalker023/focused-empathy
"""./mirror_scripts/mirror_sentence_bert.sh 0 1   """;Natural Language Processing;https://github.com/cambridgeltl/mirror-bert
"""1. Clone or download     - Use the command bellow in terminal to git clone:         ```git clone https://github.com/samson6460/tf2_pose_estimation.git```      - Or just download whole files using the **[Code > Download ZIP]** button in the upper right corner.      2. Install dependent packages:      ```pip install -r requirements.txt```  3. Import tf2_pose_estimation:    ```import tf2_pose_estimation``` """;Computer Vision;https://github.com/samson6460/tf2_pose_estimation
"""We provide an 'environment.yml' file to set up a conda environment with all required packages. Run the following command to clone the repository and create the environment.  ```bash #: Clone repository and swtich into the directory git clone https://github.com/ml-jku/cloob cd cloob  #: Create the environment and activate it conda env create --file environment.yml conda activate cloob  #: Additionally  webdataset needs to be installed from git repo for pre-training on YFCC  pip install git+https://github.com/tmbdev/webdataset.git  #: Add the directory to the PYTHONPATH environment variable export PYTHONPATH=""$PYTHONPATH:$PWD/src"" ```   In the following there is an example command for pretraining on CC with an effective batch size of 512 when used on 4 GPUs.  ```bash python -u src/training/main.py \ --train-data=""<dataset-dir>/conceptual_captions/Train-GCC-training_output.csv"" \ --val-data=""<dataset-dir>/conceptual_captions/Validation_GCC-1.1.0-Validation_output.csv"" \ --path-data=""<dataset-dir>/conceptual_captions"" \ --imagenet-val=""<dataset-dir>/imagenet/val"" \ --warmup 20000 \ --batch-size=128 \ --lr=1e-3 \ --wd=0.1 \ --lr-scheduler=""cosine-restarts"" \ --restart-cycles=10 \ --epochs=70 \ --method=""cloob"" \ --init-inv-tau=30 \ --init-scale-hopfield=8 \ --workers=8 \ --model=""RN50"" \ --dist-url=""tcp://127.0.0.1:6100"" \ --batch-size-eval=512 ```   """;Computer Vision;https://github.com/ml-jku/cloob
"""```bash $ pip install swin-transformer-pytorch ```  or (if you clone the repository)  ```bash $ pip install -r requirements.txt ```   ```python import torch from swin_transformer_pytorch import SwinTransformer  net = SwinTransformer(     hidden_dim=96      layers=(2  2  6  2)      heads=(3  6  12  24)      channels=3      num_classes=3      head_dim=32      window_size=7      downscaling_factors=(4  2  2  2)      relative_pos_embedding=True ) dummy_x = torch.randn(1  3  224  224) logits = net(dummy_x)  #: (1 3) print(net) print(logits) ```  """;Computer Vision;https://github.com/zhangbo2008/swin-transformer_noted_very_detail
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   This is a Github template repo you can use to create your own copy of the forked StyleGAN2 sample from NVLabs.   There are a few prerequisites you will need to have in place:   4. Create a github connected project by opening the Projects tab  pressing the ""LINK A GITHUB REPO"" button  and following the instructions on the next page.  5. You next need to authorize Gradient to access your github account if you have not done so already.   """;General;https://github.com/hanehein921/StyleGAN2
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   This is a Github template repo you can use to create your own copy of the forked StyleGAN2 sample from NVLabs.   There are a few prerequisites you will need to have in place:   4. Create a github connected project by opening the Projects tab  pressing the ""LINK A GITHUB REPO"" button  and following the instructions on the next page.  5. You next need to authorize Gradient to access your github account if you have not done so already.   """;Computer Vision;https://github.com/hanehein921/StyleGAN2
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   This is a Github template repo you can use to create your own copy of the forked StyleGAN2 sample from NVLabs.   There are a few prerequisites you will need to have in place:   4. Create a github connected project by opening the Projects tab  pressing the ""LINK A GITHUB REPO"" button  and following the instructions on the next page.  5. You next need to authorize Gradient to access your github account if you have not done so already.   """;General;https://github.com/thomaspool97/StyleGAN2
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   This is a Github template repo you can use to create your own copy of the forked StyleGAN2 sample from NVLabs.   There are a few prerequisites you will need to have in place:   4. Create a github connected project by opening the Projects tab  pressing the ""LINK A GITHUB REPO"" button  and following the instructions on the next page.  5. You next need to authorize Gradient to access your github account if you have not done so already.   """;Computer Vision;https://github.com/thomaspool97/StyleGAN2
"""""";General;https://github.com/iamalexkorotin/Wasserstein2Benchmark
"""1. Install and use the included Ananconda environment ``` $ conda env create -f environment/[linux-cpu|linux-gpu|mac]-env.yml $ source activate rlkit (rlkit) $ python examples/ddpg.py ``` Choose the appropriate `.yml` file for your system. These Anaconda environments use MuJoCo 1.5 and gym 0.10.5. You'll need to [get your own MuJoCo key](https://www.roboti.us/license.html) if you want to use MuJoCo.  2. Add this repo directory to your `PYTHONPATH` environment variable or simply run: ``` pip install -e . ```  3. (Optional) Copy `conf.py` to `conf_private.py` and edit to override defaults: ``` cp rlkit/launchers/conf.py rlkit/launchers/conf_private.py ```  4. (Optional) If you plan on running the Skew-Fit experiments or the HER example with the Sawyer environment  then you need to install [multiworld](https://github.com/vitchyr/multiworld).  DISCLAIMER: the mac environment has only been tested without a GPU.  For an even more portable solution  try using the docker image provided in `environment/docker`. The Anaconda env should be enough  but this docker image addresses some of the rendering issues that may arise when using MuJoCo 1.5 and GPUs. The docker image supports GPU  but it should work without a GPU. To use a GPU with the image  you need to have [nvidia-docker installed](https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)).       - Requires multiworld to be installed       version   To get started  checkout the example scripts  linked above.  Use new multiworld code that requires explicit environment registration.   The initial release for 0.2 has the following major changes:   Upgraded to PyTorch v0.4   You can use a GPU by calling   If you are using doodad (see below)  simply use the use_gpu flag:    - LOCAL_LOG_DIR is the directory set by rlkit.launchers.config.LOCAL_LOG_DIR. Default name is 'output'.   If you have rllab installed  you can also visualize the results   to visualize all experiments with a prefix of exp_prefix. To only visualize a single run  you can do   """;Reinforcement Learning;https://github.com/rail-berkeley/rlkit
"""- https://github.com/ildoonet/pytorch-gradual-warmup-lr  - https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg   """;Computer Vision;https://github.com/cowfrica-1702/HandDetection
"""As a DEQ model can use any *black-box* root solver. We provide PyTorch fixed-point solver implementations `anderson(...)` and `broyden(...)` in `lib/solvers.py` that output a dictionary containing the basic information of the optimization process. By default  we use the *relative residual difference* (i.e.  |f(z)-z|/|z|) as the criterion for stopping the iterative process.  The forward pass can then be reduced to 2 lines: ```python with torch.no_grad():     #: x is the input injection; z0 is the initial estimate of the fixed point.     z_star = self.solver(lambda z: f(z  x  *args)  z0  threshold=f_thres)['result'] ``` where we note that the forward pass does not need to store **any** intermediate state  so we put it in a `torch.no_grad()` block.   (Version 2.0 released now! :grinning:)   As a DEQ model can use any *black-box* root solver. We provide PyTorch fixed-point solver implementations `anderson(...)` and `broyden(...)` in `lib/solvers.py` that output a dictionary containing the basic information of the optimization process. By default  we use the *relative residual difference* (i.e.  |f(z)-z|/|z|) as the criterion for stopping the iterative process.  The forward pass can then be reduced to 2 lines: ```python with torch.no_grad():     #: x is the input injection; z0 is the initial estimate of the fixed point.     z_star = self.solver(lambda z: f(z  x  *args)  z0  threshold=f_thres)['result'] ``` where we note that the forward pass does not need to store **any** intermediate state  so we put it in a `torch.no_grad()` block.   Finally  we need to ensure there is a way to compute the backward pass of a DEQ  which relies on implicit function theorem. To do this  we can use the `register_hook` function in PyTorch that registers a backward hook function to be executed in the backward pass. As we noted in the paper  the backward pass is simply solving for the fixed point of a *linear system* involving the Jacobian at the equilibrium: ```python new_z_star = self.f(z_star.requires_grad_()  x  *args)  def backward_hook(grad):     if self.hook is not None:         self.hook.remove()         torch.cuda.synchronize()   #: To avoid infinite recursion     #: Compute the fixed point of yJ + grad  where J=J_f is the Jacobian of f at z_star     new_grad = self.solver(lambda y: autograd.grad(new_z_star  z_star  y  retain_graph=True)[0] + grad  \                            torch.zeros_like(grad)  threshold=b_thres)['result']     return new_grad  self.hook = new_z_star.register_hook(backward_hook) ```   """;General;https://github.com/locuslab/deq
"""see [Install.md](./install.md)   see [preprocess/readme.md](./preprocess/readme.md)   cat run.sh  cat one_time_inference.sh  Pytorch Geometric Documentation: https://pytorch-geometric.readthedocs.io/en/latest/   Pytorch LSTM: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html   """;Graphs;https://github.com/jianwang-ntu/CE7490_ass1_gnn
"""This repo provides the official implementation of Hamburger for further research. We sincerely hope that this paper can bring you inspiration about the Attention Mechanism  especially how **the low-rankness and the optimization-driven method** can help model the so-called *Global Information* in deep learning. We also highlight **Hamburger as a semi-implicit model and one-step gradient as an alternative for training both implicit and semi-implicit models**.  We model the global context issue as a low-rank completion problem and show that its optimization algorithms can help design global information blocks. This paper then proposes a series of Hamburgers  in which we employ the optimization algorithms for solving MDs to factorize the input representations into sub-matrices and reconstruct a low-rank embedding. Hamburgers with different MDs can perform favorably against the popular global context module self-attention when carefully coping with gradients back-propagated through MDs.  ![contents](assets/Hamburger.jpg)  We are working on some exciting topics. Please wait for our new papers. :)  Enjoy Hamburger  please!   [ ] PyTorch Hamburger using less encapsulation.   """;General;https://github.com/Gsunshine/Enjoy-Hamburger
"""Both Tensorflow and Pytorch checkpoint are released here.   """;General;https://github.com/phamtrancsek12/offensive-identification
"""""";General;https://github.com/Anderies/stochastic-constant-pruninglayers
"""""";Computer Vision;https://github.com/Anderies/stochastic-constant-pruninglayers
"""bash run.sh train ConE wn18rr 0 1 1024 50 500 10 0.5 0.001 40000 4 -de \   bash run.sh category ConE wn18rr 0 1 1024 50 500 0.1 0.5 0.001 20000 4 -de \   Knowledge Graph Data:  - `entities.dict`: a dictionary mapping entities to unique ids  - `relations.dict`: a dictionary mapping relations to unique ids  - `train.txt`: the KGE model is trained to fit this data set  - `valid.txt`: create a blank file if no validation data is available  - `test.txt`: the KGE model is evaluated on this data set  - `relation_category.txt`: a dictionary mapping relations to their type (1-1 indicates non-hierarchical  1-M indicates hyponym  M-1 indicates hypernym)  required for ConE model  - `class_test_X.txt`: Test data for ancestor-descendant prediction task  *X*=easy: 0% inferred descendant pairs  *X*=medium: 50% inferred descendant pairs  *X*=hard: 100% inferred descendant pairs  - `lca_test_X.txt`: LCA prediction under *X*-hop is evaluated on this data set   """;General;https://github.com/snap-stanford/ConE
"""bash run.sh train ConE wn18rr 0 1 1024 50 500 10 0.5 0.001 40000 4 -de \   bash run.sh category ConE wn18rr 0 1 1024 50 500 0.1 0.5 0.001 20000 4 -de \   Knowledge Graph Data:  - `entities.dict`: a dictionary mapping entities to unique ids  - `relations.dict`: a dictionary mapping relations to unique ids  - `train.txt`: the KGE model is trained to fit this data set  - `valid.txt`: create a blank file if no validation data is available  - `test.txt`: the KGE model is evaluated on this data set  - `relation_category.txt`: a dictionary mapping relations to their type (1-1 indicates non-hierarchical  1-M indicates hyponym  M-1 indicates hypernym)  required for ConE model  - `class_test_X.txt`: Test data for ancestor-descendant prediction task  *X*=easy: 0% inferred descendant pairs  *X*=medium: 50% inferred descendant pairs  *X*=hard: 100% inferred descendant pairs  - `lca_test_X.txt`: LCA prediction under *X*-hop is evaluated on this data set   """;Graphs;https://github.com/snap-stanford/ConE
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   [![Codacy Badge](https://api.codacy.com/project/badge/Grade/4df57b0d996344cc94ca0d3e78c93211)](https://app.codacy.com/app/h4ste/oscar?utm_source=github.com&utm_medium=referral&utm_content=h4ste/oscar&utm_campaign=Badge_Grade_Settings)  This is a fork of the original (Google's) BERT implementation.   * Add Multi-GPU support with Horovod  This [blog](https://lambdalabs.com/blog/bert-multi-gpu-implementation-using-tensorflow-and-horovod-with-code/) explains all the changes we made to the original implementation.  __Install__ Please first [install Horovod](https://github.com/uber/horovod#install)  __Run__ See the commands in each section to run BERT with Multi-GPUs:  * [Sentence (and sentence-pair) classification tasks](#sentencepair)  * [SQuAD 1.1](#squad1.1)  * [SQuAD 2.0](#squad2.0)  * [Pre-training](#pretraining)      PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).   files (at least one per GPU). Assuming you have split your input dataset        you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/h4ste/oscar
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   [![Codacy Badge](https://api.codacy.com/project/badge/Grade/4df57b0d996344cc94ca0d3e78c93211)](https://app.codacy.com/app/h4ste/oscar?utm_source=github.com&utm_medium=referral&utm_content=h4ste/oscar&utm_campaign=Badge_Grade_Settings)  This is a fork of the original (Google's) BERT implementation.   * Add Multi-GPU support with Horovod  This [blog](https://lambdalabs.com/blog/bert-multi-gpu-implementation-using-tensorflow-and-horovod-with-code/) explains all the changes we made to the original implementation.  __Install__ Please first [install Horovod](https://github.com/uber/horovod#install)  __Run__ See the commands in each section to run BERT with Multi-GPUs:  * [Sentence (and sentence-pair) classification tasks](#sentencepair)  * [SQuAD 1.1](#squad1.1)  * [SQuAD 2.0](#squad2.0)  * [Pre-training](#pretraining)      PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).   files (at least one per GPU). Assuming you have split your input dataset        you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/h4ste/oscar
"""Supports Keras with Theano and Tensorflow backend. Due to recent report that Theano will no longer be updated  Tensorflow is the default backend for this project now.  Requires Pillow  imageio  sklearn  scipy  keras 2.3.1  tensorflow 1.15.0  **Note**: The project is going to be reworked. Therefore please refer to [Framework-Updates.md](https://github.com/titu1994/Image-Super-Resolution/blob/master/Framework-Update.md) to see the changes which will affect performance.  The model weights are already provided in the weights folder  therefore simply running :<br> `python main.py ""imgpath""`  where imgpath is a full path to the image.  The default model is DDSRCNN (dsr)  which outperforms the other three models. To switch models <br> `python main.py ""imgpath"" --model=""type""`  where type = `sr`  `esr`  `dsr`  `ddsr`  If the scaling factor needs to be altered then :<br> `python main.py ""imgpath"" --scale=s`  where s can be any number. Default `s = 2`  If the intermediate step (bilinear scaled image) is needed  then:<br> `python main.py ""imgpath"" --scale=s --save_intermediate=""True""`   There are 14 extra images provided in results  2 of which (Monarch Butterfly and Zebra) have been scaled using both bilinear  SRCNN  ESRCNN and DSRCNN.   """;General;https://github.com/titu1994/Image-Super-Resolution
"""Supports Keras with Theano and Tensorflow backend. Due to recent report that Theano will no longer be updated  Tensorflow is the default backend for this project now.  Requires Pillow  imageio  sklearn  scipy  keras 2.3.1  tensorflow 1.15.0  **Note**: The project is going to be reworked. Therefore please refer to [Framework-Updates.md](https://github.com/titu1994/Image-Super-Resolution/blob/master/Framework-Update.md) to see the changes which will affect performance.  The model weights are already provided in the weights folder  therefore simply running :<br> `python main.py ""imgpath""`  where imgpath is a full path to the image.  The default model is DDSRCNN (dsr)  which outperforms the other three models. To switch models <br> `python main.py ""imgpath"" --model=""type""`  where type = `sr`  `esr`  `dsr`  `ddsr`  If the scaling factor needs to be altered then :<br> `python main.py ""imgpath"" --scale=s`  where s can be any number. Default `s = 2`  If the intermediate step (bilinear scaled image) is needed  then:<br> `python main.py ""imgpath"" --scale=s --save_intermediate=""True""`   There are 14 extra images provided in results  2 of which (Monarch Butterfly and Zebra) have been scaled using both bilinear  SRCNN  ESRCNN and DSRCNN.   """;Computer Vision;https://github.com/titu1994/Image-Super-Resolution
"""The following dependencies are needed:  - numpy >= 1.11.1   - h5py >=2.6.0   Here you can specify:   If available  a GPU will be used.   If available  a GPU will be used.   """;Computer Vision;https://github.com/orobix/retina-unet
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/xunings/styleganime2
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;Computer Vision;https://github.com/xunings/styleganime2
"""<strong>Created YELP clone application using React</strong>   - Deep Learning Framework: Pytorch (https://pytorch.org/)   - React App deployment: Heroku (https://www.heroku.com/)   - Example1   - Reviews: I absolutely loved the nachos here. I consider myself a nacho connoisseur. These are some of the best I've ever had. Big enough for two people. Interesting variety of nachos available. I love this place so much if possible id open a franchise where I live  in Virginia Beach. A MUST VISIT for us from now on when we go on our yearly Vegas trip!!!!!!   - Answer: 5   - True Sentiment: Positive   - Prediction: 5   - Prediction Sentiment: Positive - Example2   - Reviews: Not too bad! Been to this location several times  still have not loved it yet. The antipasta platter was ok. The mozzarella was good; the brushette had too much pesto; and calamari was not crispy enough. The chicken marsala was ok too. I tasted more of the grill of the chicken than the marsala sauce. The mash potatoes were good. I should have stuck to my usual pasta carrabba. The service is always excellent!   - Answer: 3   - True Sentiment: Neutral   - Prediction: 3   - Prediction Sentiment: Neutral - Example3   - Reviews: I honestly do not understand peoples infatuation with this place. The fries are terrible and the burgers are barely edible. I have tried several In-N-Out Burgers to make a fair assessment  and they're all nasty.   - Answer: 1   - True Sentiment: Negative   - Prediction: 1   - Prediction Sentiment: Negative   - Example4   - Reviews: We got there for an early dinner. Place didn't look that busy when we arrived. They took about five minutes to great and another five to sit us. I was not impressed by the way place look. Floors were dirty with food. But then I saw waiter cleaning tabla and dumping crumbs on floor. After we sat down waiter left and didn't come back to take our drink order for a long time. We almost got up and left because of how long they took to take our orders. We had the al Pastor mahi fish and mole tacos. They were super good!! I also had a michelada and it was delicious!! Food wise I give them Five stars. But service and cleanliness I give them two stars. Next time I'll give the one in Glendale a chance. Hoping food is as good as here but with better service and a more clean environment.   - Answer: 4   - True Sentiment: Positive   - Prediction: 2   - Prediction Sentiment: Negative - Example5   - Reviews: We tried the corned beef sandwich. I'm not the biggest fan of corned beef  but when I get a hankering for it  I need the real thing. The sandwich is pretty  with swirled pumpernickel bread and cheddar  but the corned beef appears to be the kind that comes in slices or a pack rather than the brisket we're used to. Plus  they fried the meat! We'll search some more for REAL corned beef.   - Answer: 2   - True Sentiment: Negative   - Prediction: 3   - Prediction Sentiment: Neutral   """;Natural Language Processing;https://github.com/jsantoso2/yelp-clone-ml-project
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   For R-FCN/Faster R-CNN\: 1. Please download COCO and VOC 2007+2012 datasets  and make sure it looks like this:  	``` 	./data/coco/ 	./data/VOCdevkit/VOC2007/ 	./data/VOCdevkit/VOC2012/ 	```  2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  For DeepLab\: 1. Please download Cityscapes and VOC 2012 datasets and make sure it looks like this:  	``` 	./data/cityscapes/ 	./data/VOCdevkit/VOC2012/ 	``` 2. Please download argumented VOC 2012 annotations/image lists  and put the argumented annotations and the argumented train/val lists into:  	``` 	./data/VOCdevkit/VOC2012/SegmentationClass/ 	./data/VOCdevkit/VOC2012/ImageSets/Main/ 	```      Respectively.     2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  1. Clone the Deformable ConvNets repository  and we'll call the directory that you cloned Deformable-ConvNets as ${DCN_ROOT}. ``` git clone https://github.com/msracver/Deformable-ConvNets.git ```  2. For Windows users  run ``cmd .\init.bat``. For Linux user  run `sh ./init.sh`. The scripts will build cython module automatically and create some folders.  3. Install MXNet: 	 	**Note: The MXNet's Custom Op cannot execute parallelly using multi-gpus after this [PR](https://github.com/apache/incubator-mxnet/pull/6928). We strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.5).**  	***Quick start***  	3.1 Install MXNet and all dependencies by  	``` 	pip install -r requirements.txt 	``` 	If there is no other error message  MXNet should be installed successfully.  	 	***Build from source (alternative way)***  	3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by 	``` 	git clone --recursive https://github.com/dmlc/mxnet.git 	git checkout 998378a 	git submodule update 	#: if it's the first time to checkout  just use: git submodule update --init --recursive 	``` 	3.3 Compile MXNet 	``` 	cd ${MXNET_ROOT} 	make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1 	``` 	3.4 Install the MXNet Python binding by 	 	***Note: If you will actively switch between different versions of MXNet  please follow 3.5 instead of 3.4*** 	``` 	cd python 	sudo python setup.py install 	``` 	3.5 For advanced users  you may put your Python packge into `./external/mxnet/$(YOUR_MXNET_PACKAGE)`  and modify `MXNET_VERSION` in `./experiments/rfcn/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`. Thus you can switch among different versions of MXNet quickly.  4. For Deeplab  we use the argumented VOC 2012 dataset. The argumented annotations are provided by [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html) dataset. For convenience  we provide the converted PNG annotations and the lists of train/val images  please download them from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMRhVImMI1jRrsxDg).   We provide trained deformable convnet models  including the deformable R-FCN & Faster R-CNN models trained on COCO trainval  and the deformable DeepLab model trained on CityScapes train.  1. To use the demo with our pre-trained deformable models  please download manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMSjehIcCgAhvEAHw) or [BaiduYun](https://pan.baidu.com/s/1dFlPFED)  and put it under folder `model/`.  	Make sure it looks like this: 	``` 	./model/rfcn_dcn_coco-0000.params 	./model/rfcn_coco-0000.params 	./model/fpn_dcn_coco-0000.params 	./model/fpn_coco-0000.params 	./model/rcnn_dcn_coco-0000.params 	./model/rcnn_coco-0000.params 	./model/deeplab_dcn_cityscapes-0000.params 	./model/deeplab_cityscapes-0000.params 	./model/deform_conv-0000.params 	./model/deform_psroi-0000.params 	``` 2. To run the R-FCN demo  run 	``` 	python ./rfcn/demo.py 	``` 	By default it will run Deformable R-FCN and gives several prediction results  to run R-FCN  use 	``` 	python ./rfcn/demo.py --rfcn_only 	``` 3. To run the DeepLab demo  run 	``` 	python ./deeplab/demo.py 	``` 	By default it will run Deformable Deeplab and gives several prediction results  to run DeepLab  use 	``` 	python ./deeplab/demo.py --deeplab_only 	``` 4. To visualize the offset of deformable convolution and deformable psroipooling  run 	``` 	python ./rfcn/deform_conv_demo.py 	python ./rfcn/deform_psroi_demo.py 	```    1. All of our experiment settings (GPU #  dataset  etc.) are kept in yaml config files at folder `./experiments/rfcn/cfgs`  `./experiments/faster_rcnn/cfgs` and `./experiments/deeplab/cfgs/`. 2. Eight config files have been provided so far  namely  R-FCN for COCO/VOC  Deformable R-FCN for COCO/VOC  Faster R-CNN(2fc) for COCO/VOC  Deformable Faster R-CNN(2fc) for COCO/VOC  Deeplab for Cityscapes/VOC and Deformable Deeplab for Cityscapes/VOC  respectively. We use 8 and 4 GPUs to train models on COCO and on VOC for R-FCN  respectively. For deeplab  we use 4 GPUs for all experiments.  3. To perform experiments  run the python scripts with the corresponding config file as input. For example  to train and test deformable convnets on COCO with ResNet-v1-101  use the following command     ```     python experiments\rfcn\rfcn_end2end_train_test.py --cfg experiments\rfcn\cfgs\resnet_v1_101_coco_trainval_rfcn_dcn_end2end_ohem.yaml     ```     A cache folder would be created automatically to save the model and the log under `output/rfcn_dcn_coco/`. 4. Please find more details in config files and in our code.   """;Computer Vision;https://github.com/qilei123/fpn_crop
"""``` python train.py --bs=128 --test --adaptive --load_model=adaptive_6910 ``` When `test` flag is passed  only inference is performed on the test set. Ground truths for test set for VQA are not publicly available. This command will dump the JSON file in the `/snap` directory. Submit the JSON file through the [EvalAI competition page](https://evalai.cloudcv.org/web/challenges/challenge-page/514/overview).   - Download the raw VQA 2.0 dataset from the [official website](https://visualqa.org/download.html).  Make sure that your data directory looks similar to the following structure (you can change the paths if you want a different structure in `train.py`).  - These instructions are from [LXMERT repo]((https://github.com/airsplay/lxmert#vqa)). Download the re-distributed JSON files. ``` mkdir -p data/vqa wget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/vqa/train.json -P data/vqa/ wget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/vqa/nominival.json -P  data/vqa/ wget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/vqa/minival.json -P data/vqa/ ``` For downloading FasterRCNN features  use these instructions: ``` mkdir -p data/mscoco_imgfeat wget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/mscoco_imgfeat/train2014_obj36.zip -P data/mscoco_imgfeat unzip data/mscoco_imgfeat/train2014_obj36.zip -d data/mscoco_imgfeat && rm data/mscoco_imgfeat/train2014_obj36.zip wget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/mscoco_imgfeat/val2014_obj36.zip -P data/mscoco_imgfeat unzip data/mscoco_imgfeat/val2014_obj36.zip -d data && rm data/mscoco_imgfeat/val2014_obj36.zip ``` If the links don't work  you can use [Google drive link](https://drive.google.com/drive/folders/1Gq1uLUk6NdD0CcJOptXjxE6ssY5XAuat?usp=sharing) to get access. For more details  please refer [LXMERT repo](https://github.com/airsplay/lxmert).  Setup the directory structure like this: In `/home/user/` ``` +-- data |   +-- lxmert |   +-- mscoco_imgfeat |   +-- vqa +-- adaptive_transformer +-- snap ....... ``` Create a directory snap  that's where checkpoints will be store by default. All of this structure can be changed but suitable modifications will be needed in `train.py`.  FasterRCNN features are loaded all at once in the RAM  so you'd require an instance with >48 GB of RAM. For training  I used a single P100 Nvidia GPU.    $ git clone https://github.com/prajjwal1/adaptive_transformer  $ cd adaptive_transformer   """;General;https://github.com/prajjwal1/adaptive_transformer
"""``` python train.py --bs=128 --test --adaptive --load_model=adaptive_6910 ``` When `test` flag is passed  only inference is performed on the test set. Ground truths for test set for VQA are not publicly available. This command will dump the JSON file in the `/snap` directory. Submit the JSON file through the [EvalAI competition page](https://evalai.cloudcv.org/web/challenges/challenge-page/514/overview).   - Download the raw VQA 2.0 dataset from the [official website](https://visualqa.org/download.html).  Make sure that your data directory looks similar to the following structure (you can change the paths if you want a different structure in `train.py`).  - These instructions are from [LXMERT repo]((https://github.com/airsplay/lxmert#vqa)). Download the re-distributed JSON files. ``` mkdir -p data/vqa wget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/vqa/train.json -P data/vqa/ wget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/vqa/nominival.json -P  data/vqa/ wget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/vqa/minival.json -P data/vqa/ ``` For downloading FasterRCNN features  use these instructions: ``` mkdir -p data/mscoco_imgfeat wget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/mscoco_imgfeat/train2014_obj36.zip -P data/mscoco_imgfeat unzip data/mscoco_imgfeat/train2014_obj36.zip -d data/mscoco_imgfeat && rm data/mscoco_imgfeat/train2014_obj36.zip wget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/mscoco_imgfeat/val2014_obj36.zip -P data/mscoco_imgfeat unzip data/mscoco_imgfeat/val2014_obj36.zip -d data && rm data/mscoco_imgfeat/val2014_obj36.zip ``` If the links don't work  you can use [Google drive link](https://drive.google.com/drive/folders/1Gq1uLUk6NdD0CcJOptXjxE6ssY5XAuat?usp=sharing) to get access. For more details  please refer [LXMERT repo](https://github.com/airsplay/lxmert).  Setup the directory structure like this: In `/home/user/` ``` +-- data |   +-- lxmert |   +-- mscoco_imgfeat |   +-- vqa +-- adaptive_transformer +-- snap ....... ``` Create a directory snap  that's where checkpoints will be store by default. All of this structure can be changed but suitable modifications will be needed in `train.py`.  FasterRCNN features are loaded all at once in the RAM  so you'd require an instance with >48 GB of RAM. For training  I used a single P100 Nvidia GPU.    $ git clone https://github.com/prajjwal1/adaptive_transformer  $ cd adaptive_transformer   """;Natural Language Processing;https://github.com/prajjwal1/adaptive_transformer
"""Yolo is written in Darknet  Open Source Neural Networks in C  for its speed. Click here for the link to [Darknet](https://pjreddie.com/darknet/yolo/).  But we are died hard python fan. Luckily some one translated darknet to a Tensorflow version  hence the name darkflow. You can clone it from this [Darkflow link](https://github.com/thtrieu/darkflow). Follow thru the installation guide written there. Basically it uses Cython language to bridge between C to Python. Then run my code 'yolo_noel.py"" in terminal.  Download you own weight too  as filesize too large  I am unable to push it up to my github.   Darkflow Repo: https://github.com/thtrieu/darkflow <br>  YOLO weights: https://pjreddie.com/darknet/yolo/ <br>   I did not have time to use my custom image on this  preparation of the labelmaps and xml format is slightly different from the tensorflow method. I also noticed that the accuracy of Yolo model is not as good as Faster R-CNN  so I did not bother to waste time on it. However  it is still possible to use custom images. Instruction is on the same link above.   For advertising tagging purpose  we just need to replace labels with resepctive slogan/promotions. Example: ``` label = result['label'] if label == 'laptop':      label='Laptop now $999' elif label == 'Wine':      label='Buy 1 Wine  Get 1 Free' ```       ![alt text](https://i.imgur.com/F0GhB4D.jpg)  """;Computer Vision;https://github.com/noelcodes/YOLO
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;Computer Vision;https://github.com/kyunghwan/darknet_v3
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;General;https://github.com/kyunghwan/darknet_v3
"""""";Computer Vision;https://github.com/TTMRonald/MobileNet_Zoo
"""""";General;https://github.com/TTMRonald/MobileNet_Zoo
"""**Requirements**: Visual Studio 2013   |  Linux (CPU)   |  Windows (CPU) |   Installation instructions   Copy .\windows\CommonSettings.props.example to .\windows\CommonSettings.props  By defaults Windows build requires CUDA and cuDNN libraries.  Both can be disabled by adjusting build variables in .\windows\CommonSettings.props.   3rd party dependencies required by Caffe are automatically resolved via NuGet.  Download CUDA Toolkit 7.5 from nVidia website.  If you don't have CUDA installed  you can experiment with CPU_ONLY build.   Download cuDNN v4 or cuDNN v5 from nVidia website.  Unpack downloaded zip to %CUDA_PATH% (environment variable set by CUDA installer).   Install for all users and add Python to PATH (through installer).  Run the following commands from elevated command prompt:  conda install --yes numpy scipy matplotlib scikit-image pip  pip install protobuf   * set PythonPath environment variable to point to &lt;caffe_root&gt;\Build\x64\Release\pycaffe  or   To build Caffe Matlab wrapper set MatlabSupport to true and MatlabDir to the root of your Matlab installation in .\windows\CommonSettings.props.   """;Computer Vision;https://github.com/wincelinux/ai-dn-FasterRCNN-caffe
"""""";Computer Vision;https://github.com/VIRUS-ATTACK/DCGAN-implementation.
"""Before you proceed  pip install -r ./requirements.txt   """;Sequential;https://github.com/alexandra-chron/ntua-slp-wassa-iest2018
"""Before you proceed  pip install -r ./requirements.txt   """;General;https://github.com/alexandra-chron/ntua-slp-wassa-iest2018
"""  pip install kornia    pip install kornia[x]  #: to get the training API !     python setup.py install    pip install -e .    pip install git+https://github.com/kornia/kornia   GitHub Issues: bug reports  feature requests  install issues  RFCs  thoughts  etc. OPEN   Run our Jupyter notebooks [tutorials](https://kornia-tutorials.readthedocs.io/en/latest/) to learn to use the library.  <div align=""center"">   <a href=""https://colab.research.google.com/github/kornia/tutorials/blob/master/source/hello_world_tutorial.ipynb"" target=""_blank"">     <img src=""https://raw.githubusercontent.com/kornia/data/main/hello_world_arturito.png"" width=""75%"" height=""75%"">   </a> </div>  :triangular_flag_on_post: **Updates** - :white_check_mark: Integrated to [Huggingface Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See [Gradio Web Demo](https://huggingface.co/spaces/akhaliq/Kornia-LoFTR).   """;Computer Vision;https://github.com/kornia/kornia
"""Python 2.7   [4] Lasagne:        https://github.com/Lasagne/Lasagne   [6] CUDA:       https://developer.nvidia.com/cudnn   """;Reinforcement Learning;https://github.com/pavitrakumar78/Playing-custom-games-using-Deep-Learning
"""To use training/evaluating scripts as well as all models  you need to clone the repository and install dependencies: ``` git clone git@github.com:osmr/imgclsmob.git pip install -r requirements.txt ```   - pytorchcv for PyTorch    - PyTorch models    """;Computer Vision;https://github.com/osmr/imgclsmob
"""Please follow the instructions of py-faster-rcnn [here](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to setup VOC and COCO datasets (Part of COCO is done). The steps involve downloading data and optionally creating soft links in the ``data`` folder. Since faster RCNN does not rely on pre-computed proposals  it is safe to ignore the steps that setup proposals.  If you find it useful  the ``data/cache`` folder created on my side is also shared [here](http://ladoga.graphics.cs.cmu.edu/xinleic/tf-faster-rcnn/cache.tgz).   1. Clone the repository   ```Shell   git clone https://github.com/endernewton/tf-faster-rcnn.git   ```  2. Update your -arch in setup script to match your GPU   ```Shell   cd tf-faster-rcnn/lib   #: Change the GPU architecture (-arch) if necessary   vim setup.py   ```    | GPU model  | Architecture |   | ------------- | ------------- |   | TitanX (Maxwell/Pascal) | sm_52 |   | GTX 960M | sm_50 |   | GTX 1080 (Ti) | sm_61 |   | Grid K520 (AWS g2.2xlarge) | sm_30 |   | Tesla K80 (AWS p2.xlarge) | sm_37 |    **Note**: You are welcome to contribute the settings on your end if you have made the code work properly on other GPUs. Also even if you are only using CPU tensorflow  GPU based code (for NMS) will be used by default  so please set **USE_GPU_NMS False** to get the correct output.   3. Build the Cython modules   ```Shell   make clean   make   cd ..   ```  4. Install the [Python COCO API](https://github.com/pdollar/coco). The code requires the API to access COCO dataset.   ```Shell   cd data   git clone https://github.com/pdollar/coco.git   cd coco/PythonAPI   make   cd ../../..   ```     - Due to the randomness in GPU training with Tensorflow especially for VOC  the best numbers are reported (with 2-3 attempts) here. According to my experience  for COCO you can almost always get a very close number (within ~0.2%) despite the randomness.      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz     tar -xzvf vgg_16_2016_08_28.tar.gz      cd ../..     For Resnet101  you can set up like:      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz     tar -xzvf resnet_v1_101_2016_08_28.tar.gz      cd ../..     ./experiments/scripts/train_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/train_faster_rcnn.sh 1 coco res101     ./experiments/scripts/test_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/test_faster_rcnn.sh 1 coco res101   1. Download pre-trained model   ```Shell   #: Resnet101 for voc pre-trained on 07+12 set   ./data/scripts/fetch_faster_rcnn_models.sh   ```   **Note**: if you cannot download the models through the link  or you want to try more models  you can check out the following solutions and optionally update the downloading script:   - Another server [here](http://xinlei.sp.cs.cmu.edu/xinleic/tf-faster-rcnn/).   - Google drive [here](https://drive.google.com/open?id=0B1_fAEgxdnvJSmF3YUlZcHFqWTQ).  2. Create a folder and a soft link to use the pre-trained model   ```Shell   NET=res101   TRAIN_IMDB=voc_2007_trainval+voc_2012_trainval   mkdir -p output/${NET}/${TRAIN_IMDB}   cd output/${NET}/${TRAIN_IMDB}   ln -s ../../../data/voc_2007_trainval+voc_2012_trainval ./default   cd ../../..   ```  3. Demo for testing on custom images   ```Shell   #: at repository root   GPU_ID=0   CUDA_VISIBLE_DEVICES=${GPU_ID} ./tools/demo.py   ```   **Note**: Resnet101 testing probably requires several gigabytes of memory  so if you encounter memory capacity issues  please install it with CPU support only. Refer to [Issue 25](https://github.com/endernewton/tf-faster-rcnn/issues/25).  4. Test with pre-trained Resnet101 models   ```Shell   GPU_ID=0   ./experiments/scripts/test_faster_rcnn.sh $GPU_ID pascal_voc_0712 res101   ```   **Note**: If you cannot get the reported numbers (79.8 on my side)  then probably the NMS function is compiled improperly  refer to [Issue 5](https://github.com/endernewton/tf-faster-rcnn/issues/5).   """;Computer Vision;https://github.com/csmiler/tf-faster-rcnn-cpu
"""<a href='g3doc/installation.md'>Installation.</a><br>   To get help with issues you may encounter while using the DeepLab Tensorflow implementation  create a new question on [StackOverflow](https://stackoverflow.com/) with the tags ""tensorflow"" and ""deeplab"".  Please report bugs (i.e.  broken code  not usage questions) to the tensorflow/models GitHub [issue tracker](https://github.com/tensorflow/models/issues)  prefixing the issue name with ""deeplab"".   """;Computer Vision;https://github.com/Robinatp/Deeplab_Tensorflow
"""<a href='g3doc/installation.md'>Installation.</a><br>   To get help with issues you may encounter while using the DeepLab Tensorflow implementation  create a new question on [StackOverflow](https://stackoverflow.com/) with the tags ""tensorflow"" and ""deeplab"".  Please report bugs (i.e.  broken code  not usage questions) to the tensorflow/models GitHub [issue tracker](https://github.com/tensorflow/models/issues)  prefixing the issue name with ""deeplab"".   """;General;https://github.com/Robinatp/Deeplab_Tensorflow
"""    ""source-nickname"": """"    You can use `rldb.find_all({})` to retrieve all existing entries in `rldb`.  ```python import rldb   all_entries = rldb.find_all({}) ```  You can also filter entries by specifying key-value pairs that the entry must match:  ```python import rldb   dqn_entries = rldb.find_all({'algo-nickname': 'DQN'}) breakout_noop_entries = rldb.find_all({     'env-title': 'atari-breakout'      'env-variant': 'No-op start'  }) ```  You can also use `rldbl.find_one(filter_dict)` to find one entry that matches the key-value pair specified in `filter_dict`:  ```python import rldb import pprint   entry = rldb.find_one({     'env-title': 'atari-pong'      'algo-title': 'Human'  }) pprint.pprint(entry) ```   <details><summary>Output</summary> <p>  ```python {     'algo-nickname': 'Human'      'algo-title': 'Human'      'env-title': 'atari-pong'      'env-variant': 'No-op start'      'score': 14.6      'source-arxiv-id': '1511.06581'      'source-arxiv-version': 3      'source-authors': [   'Ziyu Wang'                            'Tom Schaul'                            'Matteo Hessel'                            'Hado van Hasselt'                            'Marc Lanctot'                            'Nando de Freitas']      'source-bibtex': '@article{DBLP:journals/corr/WangFL15 \n'                      '    author    = {Ziyu Wang and\n'                      '                 Nando de Freitas and\n'                      '                 Marc Lanctot} \n'                      '    title     = {Dueling Network Architectures for Deep '                      'Reinforcement Learning} \n'                      '    journal   = {CoRR} \n'                      '    volume    = {abs/1511.06581} \n'                      '    year      = {2015} \n'                      '    url       = {http://arxiv.org/abs/1511.06581} \n'                      '    archivePrefix = {arXiv} \n'                      '    eprint    = {1511.06581} \n'                      '    timestamp = {Mon  13 Aug 2018 16:48:17 +0200} \n'                      '    biburl    = '                      '{https://dblp.org/rec/bib/journals/corr/WangFL15} \n'                      '    bibsource = {dblp computer science bibliography  '                      'https://dblp.org}\n'                      '}'      'source-nickname': 'DuDQN'      'source-title': 'Dueling Network Architectures for Deep Reinforcement '                     'Learning' } ```  </p> </details>   """;Reinforcement Learning;https://github.com/seungjaeryanlee/rldb
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-91-download-archive  OpenCV 3.3.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.3.0/opencv-3.3.0-vc14.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;Computer Vision;https://github.com/tianhai123/yolov3
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-91-download-archive  OpenCV 3.3.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.3.0/opencv-3.3.0-vc14.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;General;https://github.com/tianhai123/yolov3
"""![workflow](doc/model.png)  We have prepared a fully automated workflow (Figure 1) for you to classify programs against known algorithm names.  ![workflow](doc/workflow.png) Figure 1. The workflow to use our tools  To do so  you need to first install docker command  ``` sudo apt-get install docker-ce ```  Enter the following scripts on the command line to build and run the system respectively: ``` ./r clean ```   You can find the papers here: https://bdqnghi.github.io/publications/   """;Graphs;https://github.com/bdqnghi/bi-tbcnn
"""First  install either CPU-based `tensorflow`: ``` pip install tensorflow==1.10.1 ``` or if you have CUDA-enabled GPU and want to use it for this project (see [TensorFlow documentation](https://www.tensorflow.org/install/) for more details about TensorFlow GPU support):  ``` pip install tensorflow-gpu==1.10.1 ```  Then install the rest of the requirements:  ``` pip install -r requirements.txt ```  Now you can:   one formula:  On the left  you can see the standard actor-critic A2C loss   ``` python -m baselines.acktr.run_eval --model models/cool_model.npy ```  This will load a pretrained model supplied with this repository.  You should expect to see a screen pop up  where the neural net agent is going to play the game. It should clear the first world (as taught by the expert) and pass some part of the second world.  The pre-trained model is stored in `<project-root>/models/cool_model.npy`. If you run training script  it will periodically write policy parameters. You can use the current script  passing the writen policy as `--model` parameter  to see how well they are playing.  """;Reinforcement Learning;https://github.com/ghostFaceKillah/expert
"""""";Computer Vision;https://github.com/leongatys/DeepTextures
"""Clone the repository to your working space: ```bash git clone https://github.com/kcmeehan/SmartDetect.git ```  **Install requirements and download data** ```bash bash install_smartdetect.sh ```   To run the streamlit app  go into the RepPoints directory  and run the streamlit command:  ```bash cd Reppoints streamlit run mmdetection/smart_detect.py --server.port 5000 ```  Then pull up the ip address at port 5000 in a web browser. The address should be something like:  **http://<your_ip_address>:5000**  """;Computer Vision;https://github.com/kcmeehan/SmartDetect
"""""";General;https://github.com/vishal-burman/Neural-Machine-Translation
"""- Dont sample from a Uniform distribution  ![cube](images/cube.png ""Cube"")  - Sample from a gaussian distribution  ![sphere](images/sphere.png ""Sphere"")  - When doing interpolations  do the interpolation via a great circle  rather than a straight line from point A to point B - Tom White's [Sampling Generative Networks](https://arxiv.org/abs/1609.04468) ref code https://github.com/dribnet/plat has more details    - Label Smoothing  i.e. if you have two target labels: Real=1 and Fake=0  then for each incoming sample  if it is real  then replace the label with a random number between 0.7 and 1.2  and if it is a fake sample  replace it with 0.0 and 0.3 (for example).   - Salimans et. al. 2016 - make the labels the noisy for the discriminator: occasionally flip the labels when training the discriminator   - Experience Replay   - Keep a replay buffer of past generations and occassionally show them   - Keep checkpoints from the past of G and D and occassionaly swap them out for a few iterations - All stability tricks that work for deep deterministic policy gradients - See Pfau & Vinyals (2016)   - optim.Adam rules!   - See Radford et. al. 2015 - Use SGD for discriminator and ADAM for generator   - if you have labels available  training the discriminator to also classify the samples: auxillary GANs   - Provide noise in the form of dropout (50%). - Apply on several layers of our generator at both training and test time - https://arxiv.org/pdf/1611.07004v1.pdf    """;Computer Vision;https://github.com/kingcheng2000/GAN
"""- Dont sample from a Uniform distribution  ![cube](images/cube.png ""Cube"")  - Sample from a gaussian distribution  ![sphere](images/sphere.png ""Sphere"")  - When doing interpolations  do the interpolation via a great circle  rather than a straight line from point A to point B - Tom White's [Sampling Generative Networks](https://arxiv.org/abs/1609.04468) ref code https://github.com/dribnet/plat has more details    - Label Smoothing  i.e. if you have two target labels: Real=1 and Fake=0  then for each incoming sample  if it is real  then replace the label with a random number between 0.7 and 1.2  and if it is a fake sample  replace it with 0.0 and 0.3 (for example).   - Salimans et. al. 2016 - make the labels the noisy for the discriminator: occasionally flip the labels when training the discriminator   - Experience Replay   - Keep a replay buffer of past generations and occassionally show them   - Keep checkpoints from the past of G and D and occassionaly swap them out for a few iterations - All stability tricks that work for deep deterministic policy gradients - See Pfau & Vinyals (2016)   - optim.Adam rules!   - See Radford et. al. 2015 - Use SGD for discriminator and ADAM for generator   - if you have labels available  training the discriminator to also classify the samples: auxillary GANs   - Provide noise in the form of dropout (50%). - Apply on several layers of our generator at both training and test time - https://arxiv.org/pdf/1611.07004v1.pdf    """;General;https://github.com/kingcheng2000/GAN
"""""";Computer Vision;https://github.com/justinessert/hierarchical-deep-cnn
"""""";General;https://github.com/jean-kunz/ml_research_papers
"""""";Natural Language Processing;https://github.com/jean-kunz/ml_research_papers
"""""";General;https://github.com/VinayBN8997/CIFAR-10-object-detection
"""""";Computer Vision;https://github.com/VinayBN8997/CIFAR-10-object-detection
"""A brief content description is provided here  for detailed descriptions check the notebook comments     [TRAIN] notebook   Just change the directories according to your environment.    Google Colab deployed versions are available for   **[TRAIN]** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ITSR4jkFPuusqCM6Ob2yUkI43wmDLv0T?usp=sharing)   **[CV]** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Qvwr1MyDN96sUNl5DTUcTuW8xsAmMzXy?usp=sharing)    In case of any deprecation issues/warnings in future  use the modules available in YoloV5-Mixup folder.     """;Computer Vision;https://github.com/5m0k3/gwd-yolov5-pytorch
"""![alt text][image7]  ---    ![alt text][image8]  ---   """;Computer Vision;https://github.com/JunshengFu/vehicle-detection
"""Original Source Code - https://github.com/salesforce/awd-lstm-lm   """;Sequential;https://github.com/vganesh46/awd-lstm-pytorch-implementation
"""Original Source Code - https://github.com/salesforce/awd-lstm-lm   """;General;https://github.com/vganesh46/awd-lstm-pytorch-implementation
"""""";General;https://github.com/chengyangfu/pytorch-groupnormalization
"""- PyTorch 0.4.1+ - Python3 (Recommend Anaconda) - `pip install -r requirements.txt` - If you need to convert wjs0 to wav format and generate mixture files  `cd tools; make`   $ cd egs/wsj0/; . ./path.sh   If you want to visualize your loss  you can use visdom to do that:   2. Open a new terminal and run $ bash run.sh --visdom 1 --visdom_id ""&lt;any-string&gt;"" or $ train.py ... --visdom 1 --vidsdom_id ""&lt;any-string&gt;""   $ bash run.sh --continue_from &lt;model-path&gt;   If you already have mixture wsj0 data: 1. `$ cd egs/wsj0`  modify wsj0 data path `data` to your path in the beginning of `run.sh`. 2. `$ bash run.sh`  that's all!  If you just have origin wsj0 data (sphere format): 1. `$ cd egs/wsj0`  modify three wsj0 data path to your path in the beginning of `run.sh`. 2. Convert sphere format wsj0 to wav format and generate mixture. `Stage 0` part provides an example. 3. `$ bash run.sh`  that's all!  You can change hyper-parameter by `$ bash run.sh --parameter_name parameter_value`  egs  `$ bash run.sh --stage 3`. See parameter name in `egs/aishell/run.sh` before `. utils/parse_options.sh`.  Use comma separated gpu-id sequence  such as: ```bash $ bash run.sh --id ""0 1"" ```  """;Audio;https://github.com/kaituoxu/Conv-TasNet
"""SWA-Gaussian (SWAG) is a convenient method for uncertainty representation and calibration in Bayesian deep learning. The key idea of SWAG is that the SGD iterates  with a modified learning rate schedule  act like samples from a Gaussian distribution; SWAG fits this Gaussian distribution by capturing the [SWA](https://arxiv.org/abs/1803.05407) mean and a covariance matrix  representing the first two moments of SGD iterates. We use this Gaussian distribution as a posterior over neural network weights  and then perform a Bayesian model average  for uncertainty representation and calibration.  <p align=""center"">   <img src=""https://user-images.githubusercontent.com/14368801/52224039-09ab0b80-2875-11e9-9c12-c72b88abf4a9.png"" width=350>   <img src=""https://user-images.githubusercontent.com/14368801/52224049-0dd72900-2875-11e9-9de8-540ceaae60b3.png"" width=350> </p>   In this repo  we implement SWAG for image classification with several different architectures on both CIFAR datasets and ImageNet. We also implement SWAG for semantic segmentation on CamVid using our implementation of a FCDenseNet67. We additionally include several other experiments on exploring the covariance of the gradients of the SGD iterates  the eigenvalues of the Hessian  and width/PCA decompositions of the SWAG approximate posterior.  CIFAR10 -> STL10             |  CIFAR100 :-------------------------:|:-------------------------: ![](plots/stl_wrn.jpg)  |  ![](plots/c100_resnet110.jpg)  Please cite our work if you find it useful: ```bibtex @inproceedings{maddox_2019_simple    title={A simple baseline for bayesian uncertainty in deep learning}    author={Maddox  Wesley J and Izmailov  Pavel and Garipov  Timur and Vetrov  Dmitry P and Wilson  Andrew Gordon}    booktitle={Advances in Neural Information Processing Systems}    pages={13153--13164}    year={2019} } ```   ```bash python setup.py develop ```  See requirements.txt file for requirements that came from our setup. We use Pytorch 1.0.0 in our experiments.  Unless otherwise described  all experiments were run on a single GPU. Note that if you are using CUDA 10 you may need to manually install Pytorch with the correct CUDA toolkit.   **See experiments/* for particular READMEs**  [Image Classification](experiments/train/README.md)  [Segmentation](experiments/segmentation/README.md)  [Uncertainty](experiments/uncertainty/README.md)  Some other commands are listed here:  *Hessian eigenvalues*  ```cd experiments/hessian_eigs; python run_hess_eigs.py --dataset CIFAR100 --data_path [data_path] --model PreResNet110 --use_test --file [ckpt] --save_path [output.npz] ```  *Gradient covariances*  ```cd experiments/grad_cov; python run_grad_cov.py --dataset CIFAR100 --data_path [data_path] --model VGG16 --use_test --epochs=300 --lr_init=0.05 --wd=5e-4 --swa --swa_start 161 --swa_lr=0.01 --grad_cov_start 251 --dir [dir] ```  Note that this will output the gradient covariances onto the console  so you ought to write these into a log file and retrieve them afterwards.   """;General;https://github.com/wjmaddox/swa_gaussian
"""This project aims to implement biomedical image segmentation with the use of U-Net model. The below image briefly explains the output we want:  <p align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/segmentation_image.jpg"">   The dataset we used is Transmission Electron Microscopy (ssTEM) data set of the Drosophila first instar larva ventral nerve cord (VNC)  which is dowloaded from [ISBI Challenge: Segmentation of of neural structures in EM stacks](http://brainiac2.mit.edu/isbi_challenge/home)  The dataset contains 30 images (.png) of size 512x512 for each train  train-labels and test.                <td width=""27%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/flip_vert""> <br />Vertical  </td>               <td width=""27%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/flip_hori"">  <br />Horizontal</td>              <td width=""27%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/flip_both""> <br />Both</td>               <td width=""27%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/gn_50""> <br />Standard Deviation: 50</td>              <td width=""27%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/gn_100""> <br />Standard Deviation: 100</td>               <td width=""27%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/un_50""> <br />Intensity: 50</td>              <td width=""27%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/un_100""> <br />Intensity: 100</td>               <td width=""27%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/br_50.png""> <br />Intensity: 20</td>              <td width=""27%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/br_100.png""> <br />Intensity: 30</td>               <td width=""33%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/SGD_graph.png""> </td>               <td width=""33%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/RMS_graph.png""> </td>              <td width=""33%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/Adam_graph.png""> </td>   <p align=""center"">   <img width=""250"" height=""250"" src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/validation_img.png""> <br /> Input Image</td> </p>  <table border=0 width=""99%"" > 	<tbody>      <tr>		<td width=""99%"" align=""center"" colspan=""5""><strong>Results comparsion</td> 	    </tr> 		<tr> 			<td width=""24%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/validation_mask.png""> </td> 			<td width=""24%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/validation_RMS.png""> </td> 			<td width=""24%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/validation_SGD.png""></td>  			<td width=""24%"" align=""center""> <img src=""https://github.com/ugent-korea/pytorch-unet-segmentation/blob/master/readme_images/validation_Adam.png""> </td> 		</tr> 		<tr> 			<td align=""center"">original image mask</td> 			<td align=""center"">RMS prop optimizer <br />(Accuracy 92.48 %)</td> 			<td align=""center"">SGD optimizer <br />(Accuracy 91.52 %)</td> 			<td align=""center"">Adam optimizer <br />(Accuracy 92.55 %)</td>       		</tr> 	</tbody> </table>          """;Computer Vision;https://github.com/ugent-korea/pytorch-unet-segmentation
"""$ git clone https://github.com/miyosuda/Arcade-Learning-Environment.git  $ cd Arcade-Learning-Environment   $ make -j 4  $ pip install .  I recommend to install it on VirtualEnv environment.   | GPU | 1722 steps per sec |864 steps per sec |   """;Reinforcement Learning;https://github.com/miyosuda/async_deep_reinforce
"""$ git clone https://github.com/miyosuda/Arcade-Learning-Environment.git  $ cd Arcade-Learning-Environment   $ make -j 4  $ pip install .  I recommend to install it on VirtualEnv environment.   | GPU | 1722 steps per sec |864 steps per sec |   """;General;https://github.com/miyosuda/async_deep_reinforce
"""    my_inputs =  The easiest installation is just to use pip:  1. Follow the instructions at     [tensorflow.org](https://www.tensorflow.org/versions/master/get_started/os_setup.html#pip_install) 2. `pip install prettytensor`   **Note:** Head is tested against the TensorFlow nightly builds and pip is tested against TensorFlow release.             .softmax(labels  name=softmax_name))   Pretty Tensors can be used (almost) everywhere that a tensor can.  Just call `pt.wrap` to make a tensor pretty.  You can also add any existing TensorFlow function to the chain using `apply`. `apply` applies the current Tensor as the first argument and takes all the other arguments as normal.  *Note:* because apply is so generic  Pretty Tensor doesn't try to wrap the world.   """;Computer Vision;https://github.com/google/prettytensor
"""| ShuffleNet-SSD   | Improved             | Ubuntu with cuda    |   Dependencies: Opencv   Github: https://github.com/weiliu89/caffe/tree/ssd   Code: https://github.com/DeepScale/SqueezeNet   Code: https://github.com/farmingyard/ShuffleNet ; https://github.com/MG2033/ShuffleNet   """;General;https://github.com/MrRen-sdhm/Embedded_Multi_Object_Detection_CNN
"""| ShuffleNet-SSD   | Improved             | Ubuntu with cuda    |   Dependencies: Opencv   Github: https://github.com/weiliu89/caffe/tree/ssd   Code: https://github.com/DeepScale/SqueezeNet   Code: https://github.com/farmingyard/ShuffleNet ; https://github.com/MG2033/ShuffleNet   """;Computer Vision;https://github.com/MrRen-sdhm/Embedded_Multi_Object_Detection_CNN
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release  and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show < data/train.txt > result.txt`     You can comment this line so that each image does not require pressing the button ESC: https://github.com/AlexeyAB/darknet/blob/6ccb41808caf753feea58ca9df79d6367dedc434/src/detector.c#L509   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;Computer Vision;https://github.com/eric-erki/yolov3
"""""";Natural Language Processing;https://github.com/Doffery/BERT-Sentiment-Analysis-Amazon-Review
"""- Install tensorflow/pytorch  git clone git@github.com:yaxingwang/MineGAN.git to get MineGA   """;Computer Vision;https://github.com/yaxingwang/MineGAN
"""<a href='g3doc/installation.md'>Installation.</a><br>   To get help with issues you may encounter while using the DeepLab Tensorflow implementation  create a new question on [StackOverflow](https://stackoverflow.com/) with the tag ""tensorflow"".  Please report bugs (i.e.  broken code  not usage questions) to the tensorflow/models GitHub [issue tracker](https://github.com/tensorflow/models/issues)  prefixing the issue name with ""deeplab"".   """;Computer Vision;https://github.com/Vignesh-95/cnn-semantic-segmentation-satellite-images
"""<a href='g3doc/installation.md'>Installation.</a><br>   To get help with issues you may encounter while using the DeepLab Tensorflow implementation  create a new question on [StackOverflow](https://stackoverflow.com/) with the tag ""tensorflow"".  Please report bugs (i.e.  broken code  not usage questions) to the tensorflow/models GitHub [issue tracker](https://github.com/tensorflow/models/issues)  prefixing the issue name with ""deeplab"".   """;General;https://github.com/Vignesh-95/cnn-semantic-segmentation-satellite-images
"""<a href='g3doc/installation.md'>Installation.</a><br>   To get help with issues you may encounter while using the DeepLab Tensorflow implementation  create a new question on [StackOverflow](https://stackoverflow.com/) with the tag ""tensorflow"".  Please report bugs (i.e.  broken code  not usage questions) to the tensorflow/models GitHub [issue tracker](https://github.com/tensorflow/models/issues)  prefixing the issue name with ""deeplab"".   """;Computer Vision;https://github.com/kekeller/semantic_soy_deeplabv3plus
"""<a href='g3doc/installation.md'>Installation.</a><br>   To get help with issues you may encounter while using the DeepLab Tensorflow implementation  create a new question on [StackOverflow](https://stackoverflow.com/) with the tag ""tensorflow"".  Please report bugs (i.e.  broken code  not usage questions) to the tensorflow/models GitHub [issue tracker](https://github.com/tensorflow/models/issues)  prefixing the issue name with ""deeplab"".   """;General;https://github.com/kekeller/semantic_soy_deeplabv3plus
""" **Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>    For R-FCN/Faster R-CNN\: 1. Please download COCO and VOC 2007+2012 datasets  and make sure it looks like this:  	``` 	./data/coco/ 	./data/VOCdevkit/VOC2007/ 	./data/VOCdevkit/VOC2012/ 	```  2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  For DeepLab\: 1. Please download Cityscapes and VOC 2012 datasets and make sure it looks like this:  	``` 	./data/cityscapes/ 	./data/VOCdevkit/VOC2012/ 	``` 2. Please download argumented VOC 2012 annotations/image lists  and put the argumented annotations and the argumented train/val lists into:  	``` 	./data/VOCdevkit/VOC2012/SegmentationClass/ 	./data/VOCdevkit/VOC2012/ImageSets/Main/ 	```      Respectively.     2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```   1. Clone the Deformable ConvNets repository  and we'll call the directory that you cloned Deformable-ConvNets as ${DCN_ROOT}. ``` git clone https://github.com/msracver/Deformable-ConvNets.git ```  2. For Windows users  run ``cmd .\init.bat``. For Linux user  run `sh ./init.sh`. The scripts will build cython module automatically and create some folders.  3. Install MXNet: 	 	**Note: The MXNet's Custom Op cannot execute parallelly using multi-gpus after this [PR](https://github.com/apache/incubator-mxnet/pull/6928). We strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.5).**  	***Quick start***  	3.1 Install MXNet and all dependencies by  	``` 	pip install -r requirements.txt 	``` 	If there is no other error message  MXNet should be installed successfully.  	 	***Build from source (alternative way)***  	3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by 	``` 	git clone --recursive https://github.com/dmlc/mxnet.git 	git checkout 998378a 	git submodule update 	#: if it's the first time to checkout  just use: git submodule update --init --recursive 	``` 	3.3 Compile MXNet 	``` 	cd ${MXNET_ROOT} 	make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1 	``` 	3.4 Install the MXNet Python binding by 	 	***Note: If you will actively switch between different versions of MXNet  please follow 3.5 instead of 3.4*** 	``` 	cd python 	sudo python setup.py install 	``` 	3.5 For advanced users  you may put your Python packge into `./external/mxnet/$(YOUR_MXNET_PACKAGE)`  and modify `MXNET_VERSION` in `./experiments/rfcn/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`. Thus you can switch among different versions of MXNet quickly.  4. For Deeplab  we use the argumented VOC 2012 dataset. The argumented annotations are provided by [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html) dataset. For convenience  we provide the converted PNG annotations and the lists of train/val images  please download them from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMRhVImMI1jRrsxDg).    We provide trained deformable convnet models  including the deformable R-FCN & Faster R-CNN models trained on COCO trainval  and the deformable DeepLab model trained on CityScapes train.  1. To use the demo with our pre-trained deformable models  please download manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMSjehIcCgAhvEAHw) or [BaiduYun](https://pan.baidu.com/s/1dFlPFED)  and put it under folder `model/`.  	Make sure it looks like this: 	``` 	./model/rfcn_dcn_coco-0000.params 	./model/rfcn_coco-0000.params 	./model/fpn_dcn_coco-0000.params 	./model/fpn_coco-0000.params 	./model/rcnn_dcn_coco-0000.params 	./model/rcnn_coco-0000.params 	./model/deeplab_dcn_cityscapes-0000.params 	./model/deeplab_cityscapes-0000.params 	./model/deform_conv-0000.params 	./model/deform_psroi-0000.params 	``` 2. To run the R-FCN demo  run 	``` 	python ./rfcn/demo.py 	``` 	By default it will run Deformable R-FCN and gives several prediction results  to run R-FCN  use 	``` 	python ./rfcn/demo.py --rfcn_only 	``` 3. To run the DeepLab demo  run 	``` 	python ./deeplab/demo.py 	``` 	By default it will run Deformable Deeplab and gives several prediction results  to run DeepLab  use 	``` 	  	``` 4. To visualize the offset of deformable convolution and deformable psroipooling  run 	``` 	python ./rfcn/deform_conv_demo.py 	python ./rfcn/deform_psroi_demo.py 	```     1. All of our experiment settings (GPU #  dataset  etc.) are kept in yaml config files at folder `./experiments/rfcn/cfgs`  `./experiments/faster_rcnn/cfgs` and `./experiments/deeplab/cfgs/`. 2. Eight config files have been provided so far  namely  R-FCN for COCO/VOC  Deformable R-FCN for COCO/VOC  Faster R-CNN(2fc) for COCO/VOC  Deformable Faster R-CNN(2fc) for COCO/VOC  Deeplab for Cityscapes/VOC and Deformable Deeplab for Cityscapes/VOC  respectively. We use 8 and 4 GPUs to train models on COCO and on VOC for R-FCN  respectively. For deeplab  we use 4 GPUs for all experiments.  3. To perform experiments  run the python scripts with the corresponding config file as input. For example  to train and test deformable convnets on COCO with ResNet-v1-101  use the following command     ```     python experiments\rfcn\rfcn_end2end_train_test.py --cfg experiments\rfcn\cfgs\resnet_v1_101_coco_trainval_rfcn_dcn_end2end_ohem.yaml     ```     A cache folder would be created automatically to save the model and the log under `output/rfcn_dcn_coco/`. 4. Please find more details in config files and in our code.   """;Computer Vision;https://github.com/fourmi1995/IronExperiment-DCN
"""""";General;https://github.com/dimawork1981/super_resolution_bot
"""""";Computer Vision;https://github.com/dimawork1981/super_resolution_bot
"""~~~bash pip install learn2learn ~~~       task_model = maml.clone()   l2l.update_module(clone  updates=updates)   The following snippets provide a sneak peek at the functionalities of learn2learn.   for task_config in env.sample_tasks(20):     env.set_task(task)       action = my_policy(env)     env.step(action) ~~~ </details>   """;General;https://github.com/learnables/learn2learn
"""We have two networks  G (Generator) and D (Discriminator).The Generator is a network for generating images.  It receives a random noise z and generates images from this noise  which is called G(z).Discriminator is  a discriminant network that discriminates whether an image is real. The input is x  x is a picture   and the output is D of x is the probability that x is a real picture  and if it's 1  it's 100% real   and if it's 0  it's not real.   ```bash $ git clone https://github.com/Lornatang/ESRGAN-PyTorch.git $ cd ESRGAN-PyTorch/ $ pip3 install -r requirements.txt ```   Clone and install requirements                           CUDA:0).                           CUDA:0).                           CUDA:0).   If you want to load weights that you've trained before  run the following command.   """;General;https://github.com/HyeongJu916/Boaz-SR-ESRGAN-PyTorch
"""We have two networks  G (Generator) and D (Discriminator).The Generator is a network for generating images.  It receives a random noise z and generates images from this noise  which is called G(z).Discriminator is  a discriminant network that discriminates whether an image is real. The input is x  x is a picture   and the output is D of x is the probability that x is a real picture  and if it's 1  it's 100% real   and if it's 0  it's not real.   ```bash $ git clone https://github.com/Lornatang/ESRGAN-PyTorch.git $ cd ESRGAN-PyTorch/ $ pip3 install -r requirements.txt ```   Clone and install requirements                           CUDA:0).                           CUDA:0).                           CUDA:0).   If you want to load weights that you've trained before  run the following command.   """;Computer Vision;https://github.com/HyeongJu916/Boaz-SR-ESRGAN-PyTorch
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;Computer Vision;https://github.com/anookeen/yolo
"""I highly recommend to check a sychronous version and other algorithms: pytorch-a2c-ppo-acktr.   ```bash #: Works only wih Python 3. python3 main.py --env-name ""PongDeterministic-v4"" --num-processes 16 ```  This code runs evaluation in a separate thread in addition to 16 processes.   """;Reinforcement Learning;https://github.com/ikostrikov/pytorch-a3c
"""I highly recommend to check a sychronous version and other algorithms: pytorch-a2c-ppo-acktr.   ```bash #: Works only wih Python 3. python3 main.py --env-name ""PongDeterministic-v4"" --num-processes 16 ```  This code runs evaluation in a separate thread in addition to 16 processes.   """;General;https://github.com/ikostrikov/pytorch-a3c
"""Manual: https://github.com/AlexeyAB/darknet/wiki   Requirements (and how to install dependencies)   Requirements for Windows  Linux and macOS   How to compile on Linux/macOS (using CMake)   How to compile on Linux (using make)  How to compile on Windows (using CMake)  How to compile on Windows (using vcpkg)  How to train with multi-GPU   tkDNN: https://github.com/ceccocats/tkDNN  OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile   - `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   - `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/  Pytorch - Scaled-YOLOv4: https://github.com/WongKinYiu/ScaledYOLOv4  TensorFlow: pip install yolov4 YOLOv4 on TensorFlow 2.0 / TFlite / Android: https://github.com/hunglc007/tensorflow-yolov4-tflite   OpenCV the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example  Intel OpenVINO 2021.2: supports YOLOv4 (NPU Myriad X / USB Neural Compute Stick / Arria FPGA): https://devmesh.intel.com/projects/openvino-yolov4-49c756 read this manual (old manual ) (for Scaled-YOLOv4 models use https://github.com/Chen-MingChang/pytorch_YOLO_OpenVINO_demo )  PyTorch > ONNX:   DirectML https://github.com/microsoft/DirectML/tree/master/Samples/yolov4  OpenCL (Intel  AMD  Mali GPUs for macOS & GNU/Linux) https://github.com/sowson/darknet   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like CUDA  cudnn  ZED and build against those. It will also create a shared object library file to use darknet for code development.  To update CMake on Ubuntu  it's better to follow guide here: https://apt.kitware.com/ or https://cmake.org/download/  git clone https://github.com/AlexeyAB/darknet  cd darknet  mkdir build_release  cd build_release   cmake --build . --target install --parallel 8  Install: Cmake  CUDA  cuDNN How to install dependencies  Install powershell for your OS (Linux or MacOS) (guide here).   git clone https://github.com/AlexeyAB/darknet  cd darknet  ./build.ps1 -UseVCPKG -EnableOPENCV -EnableCUDA -EnableCUDNN   remove option -UseVCPKG if you plan to manually provide OpenCV library to darknet or if you do not want to enable OpenCV integration  add option -EnableOPENCV_CUDA if you want to build OpenCV with CUDA support - very slow to build! (requires -UseVCPKG)  If you open the build.ps1 script at the beginning you will find all available switches.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   You also need to specify for which graphics card the code is generated. This is done by setting ARCH=. If you use a never version than CUDA 11 you further need to edit line 20 from Makefile and remove -gencode arch=compute_30 code=sm_30 \ as Kepler GPU support was dropped in CUDA 11. You can also drop the general ARCH= and just uncomment ARCH= for your graphics card.   MSVC: https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community  CMake GUI: Windows win64-x64 Installerhttps://cmake.org/download/   find the executable file darknet.exe in the output path to the binaries you specified  This is the recommended approach to build Darknet on Windows.  Install Visual Studio 2017 or 2019. In case you need to download it  please go here: Visual Studio Community. Remember to install English language pack  this is mandatory for vcpkg!  Install CUDA enabling VS Integration during installation.   git clone https://github.com/AlexeyAB/darknet  cd darknet  .\build.ps1 -UseVCPKG -EnableOPENCV -EnableCUDA -EnableCUDNN  (add option -EnableOPENCV_CUDA if you want to build OpenCV with CUDA support - very slow to build! - or remove options like -EnableCUDA or -EnableCUDNN if you are not interested in them). If you open the build.ps1 script at the beginning you will find all available switches.  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   in Python: https://github.com/tzutalin/labelImg  in Python: https://github.com/Cartucho/OpenLabeling   in JavaScript: https://github.com/opencv/cvat   If you use `build.ps1` script or the makefile (Linux only) you will find `darknet` in the root directory.  If you use the deprecated Visual Studio solutions  you will find `darknet` in the directory `\build\darknet\x64`.  If you customize build with CMake GUI  darknet executable will be installed in your preferred folder.  - Yolo v4 COCO - **image**: `./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` - **Output coordinates** of objects: `./darknet detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` - Yolo v4 COCO - **video**: `./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` - Yolo v4 COCO - **WebCam 0**: `./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` - Yolo v4 COCO for **net-videocam** - Smart WebCam: `./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` - Yolo v4 - **save result videofile res.avi**: `./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` - Yolo v3 **Tiny** COCO - video: `./darknet detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` - **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` - Yolo v3 Tiny **on GPU #1**: `./darknet detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` - Alternative method Yolo v3 COCO - image: `./darknet detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` - Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):     `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` - 186 MB Yolo9000 - image: `./darknet detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` - Remember to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app - To process a list of images `data/train.txt` and save results of detection to `result.json` file use:     `./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` - To process a list of images `data/train.txt` and save results of detection to `result.txt` use:     `./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` - Pseudo-labelling - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` - To calculate anchors: `./darknet detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` - To check accuracy mAP@IoU=50: `./darknet detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` - To check accuracy mAP@IoU=75: `./darknet detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   - on Linux   - using `build.sh` or   - build `darknet` using `cmake` or   - set `LIBSO=1` in the `Makefile` and do `make` - on Windows   - using `build.ps1` or   - build `darknet` using `cmake` or   - compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs:  - C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h   - Python examples using the C API:     - https://github.com/AlexeyAB/darknet/blob/master/darknet.py     - https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py  - C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp   - C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp  ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     - You should have installed **CUDA 10.2**     - To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      - you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`      - after launching your console application and entering the image file name - you will see info for each object:     `<obj_id> <left_x> <top_y> <width> <height> <probability>`     - to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     - you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)  `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42)  ```cpp struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false);         std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```   """;Computer Vision;https://github.com/AlexeyAB/darknet
"""cuDNN v.6 has been released. I have tested it using Titan X Pascal. It doesn't bring any noticeable improvements for SegNet. For that reason I will not update the repository to cuDNN6.   """;Computer Vision;https://github.com/TimoSaemann/caffe-segnet-cudnn5
"""The block diagram of our system is demonstrated below:  ![alt text](https://github.com/itsasimiqbal/SeBRe/blob/master/SeBRe_block_diagram.png)  Run the notebook [SeBRe_FINAL.ipynb](https://github.com/itsasimiqbal/SeBRe/blob/master/SeBRe_FINAL.ipynb) to reproduce the results in SeBRe's paper. Make sure to install the necessary libraries in your machine before running the code. A step-by-step explanation of feature processing in SeBRe is provided in [SeBRe_feature_processing.ipynb](https://github.com/itsasimiqbal/SeBRe/blob/master/SeBRe_feature_processing.ipynb) notebook.  ![alt text](https://github.com/itsasimiqbal/SeBRe/blob/master/FP_SeBRe.png) ![alt text](https://github.com/itsasimiqbal/SeBRe/blob/master/FP_SeBRe_1.png) ![alt text](https://github.com/itsasimiqbal/SeBRe/blob/master/FP_SeBRe_2.png)   """;Computer Vision;https://github.com/itsasimiqbal/SeBRe
"""Run 'python main.py (gym_environment_name)' in terminal   """;Reinforcement Learning;https://github.com/llSourcell/OpenAI_Five_vs_Dota2_Explained
"""Please run pip install . in order to ensure you got all dependencies needed  To start up the project: python -m train.py   All hyper-paramters are in:  config.py   The config includes PLAY_ONLY argument which decides whether to start Agent with pre-trained weights or spend a few hours and train it from scratch :)   More details on the project can be found in:   [Report](/Report.md)      You don‚Äôt have to build the environment yourself the prebuilt one included in the project will work fine - please note it‚Äôs only compatible with Unity-ML 0.4.0b NOT the current newest version. I don‚Äôt have access to the source of the environment as it was prebuilt by Udacity.   """;Reinforcement Learning;https://github.com/jsztompka/MultiAgent-PPO
"""Recent Ubuntu releases come with python3 installed. I use pip3 for installing dependencies so install that with `sudo apt install python3-pip`. Install git if you don't already have it with `sudo apt install git`.  Then clone this repo with `git clone https://github.com/harvitronix/reinforcement-learning-car.git`. It has some pretty big weights files saved in past commits  so to just get the latest the fastest  do `git clone https://github.com/harvitronix/reinforcement-learning-car.git --depth 1`.   This is the physics engine used by the simulation. It just went through a pretty significant rewrite (v5) so you need to grab the older v4 version. v4 is written for Python 2 so there are a couple extra steps.  Go back to your home or downloads and get Pymunk 4:  `wget https://github.com/viblo/pymunk/archive/pymunk-4.0.0.tar.gz`  Unpack it:  `tar zxvf pymunk-4.0.0.tar.gz`  Update from Python 2 to 3:  `cd pymunk-pymukn-4.0.0/pymunk`  `2to3 -w *.py`  Install it:  `cd ..` `python3 setup.py install`  Now go back to where you cloned `reinforcement-learning-car` and make sure everything worked with a quick `python3 learning.py`. If you see a screen come up with a little dot flying around the screen  you're ready to go!   Install Pygame's dependencies with:  `sudo apt install mercurial libfreetype6-dev libsdl-dev libsdl-image1.2-dev libsdl-ttf2.0-dev libsmpeg-dev libportmidi-dev libavformat-dev libsdl-mixer1.2-dev libswscale-dev libjpeg-dev`  Then install Pygame itself:  `pip3 install hg+http://bitbucket.org/pygame/pygame`   These instructions are for a fresh Ubuntu 16.04 box. Most of the same should apply to OS X. If you have issues installing  feel free to open an issue with your error and I'll do my best to help.   NOTE: If you're coming here from parts 1 or 2 of the Medium posts  you want to visit the releases section and check out version 1.0.0  as the code has evolved passed that.   Full writeups that pertain to version 1.0.0 can be found here:   """;Reinforcement Learning;https://github.com/harvitronix/reinforcement-learning-car
"""One can also start the service on one (GPU) machine and call it from another (CPU) machine as follows   Q: Where do you get the fixed representation? Did you do pooling or something?   A: Yes. Make sure you have the following three items in model_dir:   Q: Can I run it in python 2?   ```bash python app.py -num_worker=4 -model_dir /tmp/english_L-12_H-768_A-12/ ``` This will start a service with four workers  meaning that it can handel up to four **concurrent** requests. (These workers are behind a simple load balancer.)   > NOTE: please make sure your project includes [`client.py`](service/client.py)  as we need to import `BertClient` class from this file. This is the **only file** that you will need as a client. You don't even need Tensorflow on client.  Now you can use pretrained BERT to encode sentences in your Python code simply as follows: ```python from service.client import BertClient ec = BertClient() ec.encode(['First do it'  'then do it right'  'then do it better']) ``` This will return a python object with type `List[List[float]]`  each element of the outer `List` is the fixed representation of a sentence.   """;General;https://github.com/jageshmaharjan/BERT_Service
"""One can also start the service on one (GPU) machine and call it from another (CPU) machine as follows   Q: Where do you get the fixed representation? Did you do pooling or something?   A: Yes. Make sure you have the following three items in model_dir:   Q: Can I run it in python 2?   ```bash python app.py -num_worker=4 -model_dir /tmp/english_L-12_H-768_A-12/ ``` This will start a service with four workers  meaning that it can handel up to four **concurrent** requests. (These workers are behind a simple load balancer.)   > NOTE: please make sure your project includes [`client.py`](service/client.py)  as we need to import `BertClient` class from this file. This is the **only file** that you will need as a client. You don't even need Tensorflow on client.  Now you can use pretrained BERT to encode sentences in your Python code simply as follows: ```python from service.client import BertClient ec = BertClient() ec.encode(['First do it'  'then do it right'  'then do it better']) ``` This will return a python object with type `List[List[float]]`  each element of the outer `List` is the fixed representation of a sentence.   """;Natural Language Processing;https://github.com/jageshmaharjan/BERT_Service
"""Python 3   : Clone the repo.  git clone https://github.com/dalgu90/wrn-tensorflow.git  cd wrn-tensorflow   """;Computer Vision;https://github.com/idobronstein/my_WRN
"""RLax can be installed with pip directly from github  with the following command:  `pip install git+git://github.com/deepmind/rlax.git`.  or from PyPI:  `pip install rlax`  All RLax code may then be just in time compiled for different hardware (e.g. CPU  GPU  TPU) using `jax.jit`.  In order to run the `examples/` you will also need to clone the repo and install the additional requirements: [optax](https://github.com/deepmind/optax)  [haiku](https://github.com/deepmind/haiku)  and [bsuite](https://github.com/deepmind/bsuite).   See `examples/` for examples of using some of the functions in RLax to implement a few simple reinforcement learning agents  and demonstrate learning on BSuite's version of the Catch environment (a common unit-test for agent development in the reinforcement learning literature):  Other examples of JAX reinforcement learning agents using `rlax` can be found in [bsuite](https://github.com/deepmind/bsuite/tree/master/bsuite/baselines).   """;General;https://github.com/deepmind/rlax
"""RLax can be installed with pip directly from github  with the following command:  `pip install git+git://github.com/deepmind/rlax.git`.  or from PyPI:  `pip install rlax`  All RLax code may then be just in time compiled for different hardware (e.g. CPU  GPU  TPU) using `jax.jit`.  In order to run the `examples/` you will also need to clone the repo and install the additional requirements: [optax](https://github.com/deepmind/optax)  [haiku](https://github.com/deepmind/haiku)  and [bsuite](https://github.com/deepmind/bsuite).   See `examples/` for examples of using some of the functions in RLax to implement a few simple reinforcement learning agents  and demonstrate learning on BSuite's version of the Catch environment (a common unit-test for agent development in the reinforcement learning literature):  Other examples of JAX reinforcement learning agents using `rlax` can be found in [bsuite](https://github.com/deepmind/bsuite/tree/master/bsuite/baselines).   """;Reinforcement Learning;https://github.com/deepmind/rlax
"""Python 3   """;Natural Language Processing;https://github.com/SeonbeomKim/Python-Byte_Pair_Encoding
"""**Requirements**: Visual Studio 2013   |  Linux (CPU)   |  Windows (CPU) |   Installation instructions   Copy .\windows\CommonSettings.props.example to .\windows\CommonSettings.props  By defaults Windows build requires CUDA and cuDNN libraries.  Both can be disabled by adjusting build variables in .\windows\CommonSettings.props.   3rd party dependencies required by Caffe are automatically resolved via NuGet.  Download CUDA Toolkit 7.5 from nVidia website.  If you don't have CUDA installed  you can experiment with CPU_ONLY build.   Download cuDNN v4 or cuDNN v5 from nVidia website.  Unpack downloaded zip to %CUDA_PATH% (environment variable set by CUDA installer).   Install for all users and add Python to PATH (through installer).  Run the following commands from elevated command prompt:  conda install --yes numpy scipy matplotlib scikit-image pip  pip install protobuf   * set PythonPath environment variable to point to &lt;caffe_root&gt;\Build\x64\Release\pycaffe  or   To build Caffe Matlab wrapper set MatlabSupport to true and MatlabDir to the root of your Matlab installation in .\windows\CommonSettings.props.   """;Computer Vision;https://github.com/chenghuaiyu/caffe
"""Clone the repository  then install with  ``` bash $ pip install -e ALI ```   From the repo's root directory   $ cd papers   Make sure you're in the repo's root directory.   $ THEANORC=theanorc scripts/interpolate [which_dataset] [main_loop.tar]   $ THEANORC=theanorc scripts/interpolate celeba ali_celeba.tar  $ THEANORC=theanorc scripts/reconstruct [which_dataset] [main_loop.tar]   ``` bash   $ THEANORC=theanorc scripts/generate_mixture_plots [ali_main_loop.tar] [gan_main_loop.tar]   """;Computer Vision;https://github.com/IshmaelBelghazi/ALI
"""Note:    Code runs on a single GPU and has been tested with  Python 3.7.2   """;General;https://github.com/fmu2/NICE
"""Note:    Code runs on a single GPU and has been tested with  Python 3.7.2   """;Computer Vision;https://github.com/fmu2/NICE
"""``` > python main.py --dataset spade_celebA --img_ch 3 --segmap_ch 3 --phase guide --guide_img ./guide_img.png ```     * **YOUR DATASET**   * Image   * Segmentation map     * Don't worry. I do one-hot encoding of segmentation map automatically (whether color or gray) * **CelebAMask-HQ**   * Download from [here](https://github.com/switchablenorms/CelebAMask-HQ)       Download checkpoint   ``` ‚îú‚îÄ‚îÄ dataset ¬†¬† ‚îî‚îÄ‚îÄ YOUR_DATASET_NAME ¬†¬†     ‚îú‚îÄ‚îÄ image  ¬† ¬† ¬† ¬† ¬† ‚îú‚îÄ‚îÄ 000001.jpg             ‚îú‚îÄ‚îÄ 000002.png            ‚îî‚îÄ‚îÄ ...        ‚îú‚îÄ‚îÄ segmap            ‚îú‚îÄ‚îÄ 000001.jpg            ‚îú‚îÄ‚îÄ 000002.png            ‚îî‚îÄ‚îÄ ...        ‚îú‚îÄ‚îÄ segmap_test            ‚îú‚îÄ‚îÄ a.jpg             ‚îú‚îÄ‚îÄ b.png            ‚îî‚îÄ‚îÄ ...        ‚îú‚îÄ‚îÄ segmap_label.txt (Automatically created)          ‚îú‚îÄ‚îÄ guide.jpg (example for guided image translation task) ```   """;General;https://github.com/taki0112/SPADE-Tensorflow
"""Samples:   **reference speaker A:** [S0913(./data/S0913/BAC009S0913W0351.wav)](https://drive.google.com/file/d/14zU1mI8QtoBwb8cHkNdZiPmXI6Mj6pVW/view?usp=sharing)  **reference speaker B:** [GaoXiaoSong(./data/gaoxiaosong/gaoxiaosong_1.wav)](https://drive.google.com/file/d/1s0ip6JwnWmYoWFcEQBwVIIdHJSqPThR3/view?usp=sharing)    **speaker A's speech changes to speaker B's voice:** [Converted from S0913 to GaoXiaoSong (./converted_sound/S0913/BAC009S0913W0351.wav)](https://drive.google.com/file/d/1S4vSNGM-T0RTo_aclxRgIPkUJ7NEqmjU/view?usp=sharing)  ------   """;Computer Vision;https://github.com/jackaduma/CycleGAN-VC2
"""Samples:   **reference speaker A:** [S0913(./data/S0913/BAC009S0913W0351.wav)](https://drive.google.com/file/d/14zU1mI8QtoBwb8cHkNdZiPmXI6Mj6pVW/view?usp=sharing)  **reference speaker B:** [GaoXiaoSong(./data/gaoxiaosong/gaoxiaosong_1.wav)](https://drive.google.com/file/d/1s0ip6JwnWmYoWFcEQBwVIIdHJSqPThR3/view?usp=sharing)    **speaker A's speech changes to speaker B's voice:** [Converted from S0913 to GaoXiaoSong (./converted_sound/S0913/BAC009S0913W0351.wav)](https://drive.google.com/file/d/1S4vSNGM-T0RTo_aclxRgIPkUJ7NEqmjU/view?usp=sharing)  ------   """;General;https://github.com/jackaduma/CycleGAN-VC2
"""""";Computer Vision;https://github.com/taki0112/Self-Attention-GAN-Tensorflow
"""""";General;https://github.com/taki0112/Self-Attention-GAN-Tensorflow
"""  Using pip:   .. code:: bash    pip install git+https://github.com/petko-nikolov/pysemseg           """;Computer Vision;https://github.com/petko-nikolov/pysemseg
"""This repository provides a Minimal PyTorch implementation of Proximal Policy Optimization (PPO) with clipped objective for OpenAI gym environments. It is primarily intended for beginners in [Reinforcement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning) for understanding the PPO algorithm. It can still be used for complex environments but may require some hyperparameter-tuning or changes in the code.  To keep the training procedure simple :    - It has a **constant standard deviation** for the output action distribution (**multivariate normal with diagonal covariance matrix**) for the continuous environments  i.e. it is a hyperparameter and NOT a trainable parameter. However  it is **linearly decayed**. (action_std significantly affects performance)   - It uses simple **monte-carlo estimate** for calculating advantages and NOT Generalized Advantage Estimate (check out the OpenAI spinning up implementation for that).   - It is a **single threaded implementation**  i.e. only one worker collects experience. [One of the older forks](https://github.com/rhklite/Parallel-PPO-PyTorch) of this repository has been modified to have Parallel workers  A concise explaination of PPO algorithm can be found [here](https://stackoverflow.com/questions/46422845/what-is-the-way-to-understand-proximal-policy-optimization-algorithm-in-rl)    - To train a new network : run `train.py` - To test a preTrained network : run `test.py` - To plot graphs using log files : run `plot_graph.py` - To save images for gif and make gif using a preTrained network : run `make_gif.py` - All parameters and hyperparamters to control training / testing / graphs / gifs are in their respective `.py` file - `PPO_colab.ipynb` combines all the files in a jupyter-notebook - All the **hyperparameters used for training (preTrained) policies are listed** in the [`README.md` in PPO_preTrained directory](https://github.com/nikhilbarhate99/PPO-PyTorch/tree/master/PPO_preTrained)   """;Reinforcement Learning;https://github.com/nikhilbarhate99/PPO-PyTorch
"""$ python -m run_reinforce_fc   $ python -m run_reinforce_lstm   $ python -m run_a2c   """;Reinforcement Learning;https://github.com/saschaschramm/Pong
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.  git clone https://github.com/affinelayer/pix2pix-tensorflow.git  cd pix2pix-tensorflow   ```sh #: clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git cd pix2pix-tensorflow #: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir facades_train \   --max_epochs 200 \   --input_dir facades/train \   --which_direction BtoA #: test the model python pix2pix.py \   --mode test \   --output_dir facades_test \   --input_dir facades/val \   --checkpoint facades_train ```  The test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.  If you have Docker installed  you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:  ```sh #: train the model python tools/dockrun.py python pix2pix.py \       --mode train \       --output_dir facades_train \       --max_epochs 200 \       --input_dir facades/train \       --which_direction BtoA #: test the model python tools/dockrun.py python pix2pix.py \       --mode test \       --output_dir facades_test \       --input_dir facades/val \       --checkpoint facades_train ```   <img src=""docs/combine.png"" width=""900px""/>  ```sh #: Resize source images python tools/process.py \   --input_dir photos/original \   --operation resize \   --output_dir photos/resized #: Create images with blank centers python tools/process.py \   --input_dir photos/resized \   --operation blank \   --output_dir photos/blank #: Combine resized images with blanked images python tools/process.py \   --input_dir photos/resized \   --b_dir photos/blank \   --operation combine \   --output_dir photos/combined #: Split into train/val set python tools/split.py \   --dir photos/combined ```  The folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.   """;Computer Vision;https://github.com/sidneykingsley/pix2pix-tensorflow
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.  git clone https://github.com/affinelayer/pix2pix-tensorflow.git  cd pix2pix-tensorflow   ```sh #: clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git cd pix2pix-tensorflow #: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir facades_train \   --max_epochs 200 \   --input_dir facades/train \   --which_direction BtoA #: test the model python pix2pix.py \   --mode test \   --output_dir facades_test \   --input_dir facades/val \   --checkpoint facades_train ```  The test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.  If you have Docker installed  you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:  ```sh #: train the model python tools/dockrun.py python pix2pix.py \       --mode train \       --output_dir facades_train \       --max_epochs 200 \       --input_dir facades/train \       --which_direction BtoA #: test the model python tools/dockrun.py python pix2pix.py \       --mode test \       --output_dir facades_test \       --input_dir facades/val \       --checkpoint facades_train ```   <img src=""docs/combine.png"" width=""900px""/>  ```sh #: Resize source images python tools/process.py \   --input_dir photos/original \   --operation resize \   --output_dir photos/resized #: Create images with blank centers python tools/process.py \   --input_dir photos/resized \   --operation blank \   --output_dir photos/blank #: Combine resized images with blanked images python tools/process.py \   --input_dir photos/resized \   --b_dir photos/blank \   --operation combine \   --output_dir photos/combined #: Split into train/val set python tools/split.py \   --dir photos/combined ```  The folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.   """;General;https://github.com/sidneykingsley/pix2pix-tensorflow
"""You can start working with our models using the demonstration example: `Demo.ipynb`_  ..  _`Demo.ipynb`: Demo.ipynb .. _`Alexander Rakhlin`: https://www.linkedin.com/in/alrakhlin/ .. _`Alexey Shvets`: https://www.linkedin.com/in/shvetsiya/ .. _`Vladimir Iglovikov`: https://www.linkedin.com/in/iglovikov/ .. _`Alexandr A. Kalinin`: https://alxndrkalinin.github.io/ .. _`MICCAI 2017 Endoscopic Vision SubChallenge Angiodysplasia Detection and Localization`: https://endovissub2017-giana.grand-challenge.org/angiodysplasia-etisdb/ .. _`TernausNet`: https://arxiv.org/abs/1801.05746 .. _`U-Net`: https://arxiv.org/abs/1505.04597 .. _`AlbuNet34`: https://arxiv.org/abs/1803.01207 .. _`LinkNet`: https://arxiv.org/abs/1707.03718 .. _`google drive`: https://drive.google.com/drive/folders/1V_bLBTzsl_Z8Ln9Iq8gjcFDxodfiHxul  .. |br| raw:: html     <br />  .. |plusmn| raw:: html     &plusmn  .. |times| raw:: html     &times  .. |micro| raw:: html     &microm  .. |y| image:: https://hsto.org/webt/jm/sn/i0/jmsni0y8mao8vnaij8a4eyuoqmu.gif .. |y_hat| image:: https://hsto.org/webt/xf/j2/a4/xfj2a4obgqhdzneysar5_us5pgk.gif .. |i| image:: https://hsto.org/webt/87/cc/ca/87cccaz4gjp2lgyeip17utljvvi.gif  """;Computer Vision;https://github.com/ternaus/angiodysplasia-segmentation
"""To install the library you need to clone the repository      git clone https://github.com/kpot/keras-transformer.git  then switch to the cloned directory and run pip      cd keras-transformer     pip install .  Please note that the project requires Python >= 3.6.       name='transformer'        name='coordinate_embedding')   you can build your version of Transformer  by re-arranging them   This repository contains simple [examples](./example) showing how Keras-transformer works. It's not a rigorous evaluation of the model's capabilities  but rather a demonstration on how to use the code.  The code trains [simple language-modeling networks](./example/models.py) on the [WikiText-2](https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset) dataset and evaluates their perplexity. The model is either a [vanilla Transformer][1]  or an [Adaptive Universal Transformer][2] (by default) with five layers  each can be trained using either:  * [Generative pre-training][4] (GPT)  which involves using masked self-attention   to prevent the model from ""looking into the future"". * [BERT][3]  which doesn't restrict self-attention  allowing the model   to fill the gaps using both left and right context.   To launch the code  you will first need to install the requirements listed in [example/requirements.txt](./example/requirements.txt). Assuming you work from a Python virtual environment  you can do this by running      pip install -r example/requirements.txt  You will also need to make sure you have a backend for Keras. For instance  you can install Tensorflow (the sample was tested using Tensorflow and PlaidML as backends):      pip install tensorflow  Now you can launch the GPT example as      python -m example.run_gpt --save lm_model.h5  to see all command line options and their default values  try      python -m example.run_gpt --help  If all goes well  after launching the example you should see the perplexity falling with each epoch.      Building vocabulary: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36718/36718 [00:04<00:00  7642.33it/s]     Learning BPE...Done     Building BPE vocabulary: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36718/36718 [00:06<00:00  5743.74it/s]     Train on 9414 samples  validate on 957 samples     Epoch 1/50     9414/9414 [==============================] - 76s 8ms/step - loss: 7.0847 - perplexity: 1044.2455         - val_loss: 6.3167 - val_perplexity: 406.5031     ...  After 200 epochs (~5 hours) of training on GeForce 1080 Ti  I've got validation perplexity about 51.61 and test perplexity 50.82. The score can be further improved  but that is not the point of this demo.  BERT model example can be launched similarly      python -m example.run_bert --save lm_model.h5 --model vanilla  but you will need to be patient. BERT easily achieves better performance than GPT  but requires much more training time to converge.  [1]: https://arxiv.org/abs/1706.03762 ""Attention Is All You Need"" [2]: https://arxiv.org/abs/1807.03819 ""Universal Transformers"" [3]: https://arxiv.org/abs/1810.04805 ""BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"" [4]: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf      ""Improving Language Understanding by Generative Pre-Training""  """;Natural Language Processing;https://github.com/kpot/keras-transformer
"""PyTorch book   """;Natural Language Processing;https://github.com/TEAMLAB-Lecture/deep_nlp_101
"""""";Computer Vision;https://github.com/landeros10/infoganJL
"""``` source activate tensorflow_p36 git clone https://www.github.com/keras-team/keras-contrib.git cd keras-contrib python setup.py install ```   * Select Deep Learning AMI (Ubuntu) Version 14.0 * Instance Type `GPU Compute` such as p2.xlarge * 125GB sda1  Connect to instance  copy contents of [aws-setup.sh](aws-setup.sh) to file in /home/ubuntu and run: ``` vi aws-setup.sh chmod +x aws-setup.sh ./aws-setup.sh ```   """;General;https://github.com/hollygrimm/cyclegan-keras-art-attrs
"""path = r""/Users/edvardhulten/real_nvp_2d/""  #: change to your own path (unless your name is Edvard Hult√©n too)   """;Computer Vision;https://github.com/e-hulten/real-nvp-2d
"""""";Computer Vision;https://github.com/yumaloop/mobilenetV2-cifar
"""""";General;https://github.com/yumaloop/mobilenetV2-cifar
"""To quickly get up and running the training script (DDPG + locally available csv file)  do the following:      1. Run this set of commands to create and active python virtual environment and afterwards install required dependencies     ```{shell}     > python -m venv ./env     > ./env/Scripts/activate     > pip install -r ./requirements.txt     ```      2. Install PyTorch accroding to: https://pytorch.org/get-started/locally/      3. Run the following command:     ```{shell}     > python ./main.py     ```      4. View the progress of the model in Tensorboard:     ```{shell}     > tensorboard --logdir=./.cache/tensorboard     ```   """;Reinforcement Learning;https://github.com/majercakdavid/gym-virtual-quant-trading
"""The python notebook lists all the code required for running the model. The code is commented for ease of understanding and also highlights some key points which need to be taken care of while creating the model.<br> The model is supposed to predict the bounding boxes for a digit embedded in the image as well as a confidence score for the digit inside the bounding box.   """;Computer Vision;https://github.com/saunack/MobileNetv2-SSD
"""""";Computer Vision;https://github.com/sherdencooper/dcgan-mnist
"""```bash pip install --upgrade --editable . ```  or  ```bash conda env create -f environment.yml ```   ```bash conda env create -f environment.yml ```    Below is an example of how to construct an FNN classifier. The classifier has  - variable input and output dimensions - variable number of hidden layers and dimensions - specifiable activation function - potential batchnorm and dropout layers  ```python class FNNClassifier(nn.Module):     def __init__(self  in_features  out_classes  hidden_dims=[256  128  64]  activation=nn.ReLU  batchnorm=False  dropout=False):         super(FNNClassifier  self).__init__()         dims = [in_features  *hidden_dims  out_classes]         for i in range(len(dims) - 1):             is_output_layer = i == len(dims) - 2             self.add_module(""Linear"" + str(i)  nn.Linear(dims[i]  dims[i+1]))             if batchnorm and not is_output_layer:                 self.add_module(""BatchNorm"" + str(i)  nn.BatchNorm1D(dims[i+1]))             if dropout and not is_output_layer:                 self.add_module(""Dropout"" + str(i)  nn.Dropout(p=dropout))             if not is_output_layer:                 self.add_module(""Activation"" + str(i)  activation())             else:                 self.add_module(""Activation"" + str(i)  nn.Softmax())      def forward(self  x):         x = x.reshape(x.shape[0]  -1)         for module in self._modules.values():             x = module.forward(x)         return x      def backward(self  delta):         for module in reversed(self._modules.values()):             delta = module.backward(delta) ```    Below is an example of how to construct an CNN classifier. The classifier has  - variable input and output dimensions - variable number of hidden layers and dimensions - specifiable activation function - potential batchnorm and dropout layers  For this classifier however  changing the convolutional layers require a corresponding change to the fully connected classifier layers. Alternatively  a completely convolutional model could be created.  ```python class CNNClassifier(nn.Module):     def __init__(self  in_features  out_classes  feature_maps=[16  32]  hidden_dims=[512]  activation=nn.ReLU  batchnorm=False  dropout=False):         super(CNNClassifier  self).__init__()         #: Convolutional layers         self.add_module(""Convolutional0""  nn.Conv2D(1  feature_maps[0]  kernel_size=(5  5)))         self.add_module(""Maxpool0""  nn.MaxPool2D(kernel_size=(2  2)  stride=2  padding=0))         self.add_module(""Activation0""  activation())         self.add_module(""Convolutional1""  nn.Conv2D(feature_maps[0]  feature_maps[1]  kernel_size=(5  5)))         self.add_module(""Maxpool1""  nn.MaxPool2D(kernel_size=(2  2)  stride=2  padding=0))         self.add_module(""Activation1""  activation())         self.add_module(""Flatten""  nn.Flatten())         #: Feedforward classifier         dims = [*hidden_dims  out_classes]         for i in range(len(dims) - 1):             is_output_layer = i == len(dims) - 2             if batchnorm:                 self.add_module(""BatchNorm"" + str(i)  nn.BatchNorm1D(dims[i]))             self.add_module(""Linear"" + str(i)  nn.Linear(dims[i]  dims[i+1]))             if dropout and not is_output_layer:                 self.add_module(""Dropout"" + str(i)  nn.Dropout(p=dropout))             if not is_output_layer:                 self.add_module(""Activation"" + str(i+2)  activation())             else:                 self.add_module(""Activation"" + str(i+2)  nn.Softmax())      def forward(self  x):         for module in self._modules.values():             x = module.forward(x)         return x      def backward(self  delta):         for module in reversed(self._modules.values()):             delta = module.backward(delta) ```   In /examples  two MNIST examples has been created for testing purposes.   The above FNN classifier without batchnorm and dropout as well as the CNN classifier were trained. The FNN was overfitted on the training set while the convolutional architecture is much less prone to overfitting. Final performance was  |               |  FNN |          | CNN  |          | | ------------- | ---- | -------- | ---- | -------- | | Data set      | Loss | Accuracy | Loss | Accuracy | | Training      | 0.01 | 99.97%   | 0.02 | 99.31%   | | Validation    | 0.07 | 97.95%   | 0.04 | 99.63%   |  The learning curves are seen below.   """;General;https://github.com/JakobHavtorn/nn
"""```python    This repo. is absolutely based on official impl. from https://github.com/quark0/darts with trivial modificatio to make it run on pytorch 0.4+ version.   Instructions for acquiring PTB and WT2 can be found here. While CIFAR-10 can be automatically downloaded by torchvision  ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions here.   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/dragen1860/DARTS-PyTorch
"""1. Download the environment from one of the links below.  You need only select the environment that matches your operating system:     - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip)     - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip)     - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip)     - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip)          (_For Windows users_) Check out [this link](https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64) if you need help with determining if your computer is running a 32-bit version or 64-bit version of the Windows operating system.      (_For AWS_) If you'd like to train the agent on AWS (and have not [enabled a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md))  then please use [this link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux_NoVis.zip) to obtain the ""headless"" version of the environment.  You will **not** be able to watch the agent without enabling a virtual screen  but you will be able to train the agent.  (_To watch the agent  you should follow the instructions to [enable a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md)  and then download the environment for the **Linux** operating system above._)  2. See file `requirements.txt` for python dependencies.    """;Reinforcement Learning;https://github.com/bonniesjli/MADDPG_Tennis
"""You should have the latest versions for (as of 7/2017): * keras * nltk * numpy * pandas * tensorflow (1.3.0 or greater  with CUDA 8.0 and cuDNN 6.0 or greater) * unidecode * sacremoses ([see issue regarding this](https://github.com/jmyrberg/finnlem/issues/1))  After this  clone this repository to your local machine.  Update 10.9.2020: You could also try to first clone and then run `pip install -r requirements.txt` at the root of this repository. This will install the latest versions of the required packages automatically  but notice that the very latest versions of some of the packages might nowadays be incompatible with the source code provided here. Feel free to make a pull request with fixed versions of the packages  in case you manage to run the source code successfully :)   python -m dict_train   python -m model_train   python -m model_decode   * one source document per line  or   To use tensorboard  run command python -m tensorflow.tensorboard --logdir=model_dir     Three-steps are required in order to get from zero to making predictions with a trained model:  1. **Dictionary training**: Dictionary is created from training documents  which are processed the same way as the Seq2Seq model inputs later on. 	Dictionary handles vocabulary/integer mappings required by Seq2Seq. 2. **Model training**: Seq2Seq model is trained in batches with training documents that contain source and target. 3. **Model decoding**: Unseen source documents are fed into Seq2Seq model  which makes predictions on the target.   """;Sequential;https://github.com/jmyrberg/finnlem
"""You should have the latest versions for (as of 7/2017): * keras * nltk * numpy * pandas * tensorflow (1.3.0 or greater  with CUDA 8.0 and cuDNN 6.0 or greater) * unidecode * sacremoses ([see issue regarding this](https://github.com/jmyrberg/finnlem/issues/1))  After this  clone this repository to your local machine.  Update 10.9.2020: You could also try to first clone and then run `pip install -r requirements.txt` at the root of this repository. This will install the latest versions of the required packages automatically  but notice that the very latest versions of some of the packages might nowadays be incompatible with the source code provided here. Feel free to make a pull request with fixed versions of the packages  in case you manage to run the source code successfully :)   python -m dict_train   python -m model_train   python -m model_decode   * one source document per line  or   To use tensorboard  run command python -m tensorflow.tensorboard --logdir=model_dir     Three-steps are required in order to get from zero to making predictions with a trained model:  1. **Dictionary training**: Dictionary is created from training documents  which are processed the same way as the Seq2Seq model inputs later on. 	Dictionary handles vocabulary/integer mappings required by Seq2Seq. 2. **Model training**: Seq2Seq model is trained in batches with training documents that contain source and target. 3. **Model decoding**: Unseen source documents are fed into Seq2Seq model  which makes predictions on the target.   """;General;https://github.com/jmyrberg/finnlem
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.  git clone https://github.com/affinelayer/pix2pix-tensorflow.git  cd pix2pix-tensorflow   ```sh #: clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git cd pix2pix-tensorflow #: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir facades_train \   --max_epochs 200 \   --input_dir facades/train \   --which_direction BtoA #: test the model python pix2pix.py \   --mode test \   --output_dir facades_test \   --input_dir facades/val \   --checkpoint facades_train ```  The test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.  If you have Docker installed  you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:  ```sh #: train the model python tools/dockrun.py python pix2pix.py \       --mode train \       --output_dir facades_train \       --max_epochs 200 \       --input_dir facades/train \       --which_direction BtoA #: test the model python tools/dockrun.py python pix2pix.py \       --mode test \       --output_dir facades_test \       --input_dir facades/val \       --checkpoint facades_train ```   <img src=""docs/combine.png"" width=""900px""/>  ```sh #: Resize source images python tools/process.py \   --input_dir photos/original \   --operation resize \   --output_dir photos/resized #: Create images with blank centers python tools/process.py \   --input_dir photos/resized \   --operation blank \   --output_dir photos/blank #: Combine resized images with blanked images python tools/process.py \   --input_dir photos/resized \   --b_dir photos/blank \   --operation combine \   --output_dir photos/combined #: Split into train/val set python tools/split.py \   --dir photos/combined ```  The folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.   """;Computer Vision;https://github.com/utunga/pix2pix-tensorflow
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.  git clone https://github.com/affinelayer/pix2pix-tensorflow.git  cd pix2pix-tensorflow   ```sh #: clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git cd pix2pix-tensorflow #: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir facades_train \   --max_epochs 200 \   --input_dir facades/train \   --which_direction BtoA #: test the model python pix2pix.py \   --mode test \   --output_dir facades_test \   --input_dir facades/val \   --checkpoint facades_train ```  The test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.  If you have Docker installed  you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:  ```sh #: train the model python tools/dockrun.py python pix2pix.py \       --mode train \       --output_dir facades_train \       --max_epochs 200 \       --input_dir facades/train \       --which_direction BtoA #: test the model python tools/dockrun.py python pix2pix.py \       --mode test \       --output_dir facades_test \       --input_dir facades/val \       --checkpoint facades_train ```   <img src=""docs/combine.png"" width=""900px""/>  ```sh #: Resize source images python tools/process.py \   --input_dir photos/original \   --operation resize \   --output_dir photos/resized #: Create images with blank centers python tools/process.py \   --input_dir photos/resized \   --operation blank \   --output_dir photos/blank #: Combine resized images with blanked images python tools/process.py \   --input_dir photos/resized \   --b_dir photos/blank \   --operation combine \   --output_dir photos/combined #: Split into train/val set python tools/split.py \   --dir photos/combined ```  The folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.   """;General;https://github.com/utunga/pix2pix-tensorflow
"""Topological quantum error correcting codes  and in particular the surface code  currently provide the most promising path to scalable fault tolerant quantum computation. While a variety of decoders exist for such codes  recently decoders obtained via machine learning techniques have attracted attention due to both their potential flexibility  with respect to codes and noise models  and their potentially fast run times. Here  we demonstrate how reinforcement learning techniques  and in particular deepQ learning  can be utilized to solve this problem and obtain such decoders.   Now that we have specified all the required parameters we can instantiate our environment:   2) Run the command ""bash make_executable.sh"". This will allow the controller - a python script which will be run periodically to control the training process - to submit jobs via slurm.  3) Using vim or some other in-terminal editor  modify the following in Controller.py:   - simulation_script.sh   - run the command ""watch -n interval_in_seconds python Controller.py""   - We are now looking at the watch output - kill this with ctrl+c   """;Reinforcement Learning;https://github.com/R-Sweke/DeepQ-Decoding
"""""";General;https://github.com/Nirvan101/Image-Restoration-deep-learning
"""A Zoneout implemetion based on pytorch   """;General;https://github.com/WelkinYang/Zoneout-Pytorch
"""download [wall.alphacoders.com](https://wall.alphacoders.com) thumb image      cd bw2color     python tools/wallpaper.py  crop downloaded image to 256x256 and convert to grey  ```bash cd bw2color #: basic use python tools/preprocess.py #: set data dir python tools/preprocess.py --data [data dir] --save  [output dir] #: set process method python tools/preprocess.py --method gray python tools/preprocess.py --method sketch --mod_path [mod file path] ```  generate tfrecord      python generateds.py      train      python backward.py  test      python test.py   """;Computer Vision;https://github.com/Montia/bw2color
"""download [wall.alphacoders.com](https://wall.alphacoders.com) thumb image      cd bw2color     python tools/wallpaper.py  crop downloaded image to 256x256 and convert to grey  ```bash cd bw2color #: basic use python tools/preprocess.py #: set data dir python tools/preprocess.py --data [data dir] --save  [output dir] #: set process method python tools/preprocess.py --method gray python tools/preprocess.py --method sketch --mod_path [mod file path] ```  generate tfrecord      python generateds.py      train      python backward.py  test      python test.py   """;General;https://github.com/Montia/bw2color
"""- Download Cityscapes.   $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -  $ sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable""  $ sudo apt-get update  $ sudo apt-get install -y docker-ce  Install CUDA drivers (see ""Install CUDA drivers for NC VMs"" in https://docs.microsoft.com/en-us/azure/virtual-machines/linux/n-series-driver-setup):  $ CUDA_REPO_PKG=cuda-repo-ubuntu1604_8.0.61-1_amd64.deb   $ sudo apt-get update  $ sudo apt-get install cuda-drivers           --name ""$NAME""""$GPUIDS"" \           tensorflow/tensorflow:latest-gpu bash   /root/ will now be mapped to /home/fregu856 (i.e.  $ cd -- takes you to the regular home folder).    $ sudo sh start_docker_image.sh    Open a new terminal window.   To open more than one terminal window at the same time:   """;Computer Vision;https://github.com/adrshm91/segmentationEnet
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;General;https://github.com/tjwei/stylegan2_workshop
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   """;Computer Vision;https://github.com/tjwei/stylegan2_workshop
"""**[Update:]** I've further simplified the code to pytorch 1.5  torchvision 0.6  and replace the customized ops roipool and nms with the one from torchvision.  if you want the old version code  please checkout branch [v1.0](https://github.com/chenyuntc/simple-faster-rcnn-pytorch/tree/v1.0)    This project is a **Simplified** Faster R-CNN implementation based on [chainercv](https://github.com/chainer/chainercv) and other [projects](#acknowledgement) . I hope it can serve as an start code for those who want to know the detail of Faster R-CNN.  It aims to:  - Simplify the code (*Simple is better than complex*) - Make the code more straightforward (*Flat is better than nested*) - Match the performance reported in [origin paper](https://arxiv.org/abs/1506.01497) (*Speed Counts and mAP Matters*)  And it has the following features: - It can be run as pure Python code  no more build affair.  - It's a minimal implemention in around 2000 lines valid code with a lot of comment and instruction.(thanks to chainercv's excellent documentation) - It achieves higher mAP than the origin implementation (0.712 VS 0.699) - It achieve speed compariable with other implementation (6fps and 14fps for train and test in TITAN XP) - It's memory-efficient (about 3GB for vgg16)   ![img](imgs/faster-speed.jpg)     Here is an example of create environ **from scratch** with `anaconda`  ```sh #: create conda env conda create --name simp python=3.7 conda activate simp #: install pytorch conda install pytorch torchvision cudatoolkit=10.2 -c pytorch  #: install other dependancy pip install visdom scikit-image tqdm fire ipdb pprint matplotlib torchnet  #: start visdom nohup python -m visdom.server &  ```  If you don't use anaconda  then:  - install PyTorch with GPU (code are GPU-only)  refer to [official website](http://pytorch.org)  - install other dependencies:  `pip install visdom scikit-image tqdm fire ipdb pprint matplotlib torchnet`  - start visdom for visualization  ```Bash nohup python -m visdom.server & ```     [1]: make sure you install cupy correctly and only one program run on the GPU. The training speed is sensitive to your gpu status. see troubleshooting for more info. Morever it's slow in the start of the program -- it need time to warm up.   Windows support   Download pretrained model from [Google Drive](https://drive.google.com/open?id=1cQ27LIn-Rig4-Uayzy_gH5-cW-NRGVzY) or [Baidu Netdisk( passwd: scxn)](https://pan.baidu.com/s/1o87RuXW)   See [demo.ipynb](https://github.com/chenyuntc/simple-faster-rcnn-pytorch/blob/master/demo.ipynb) for more detail.   ```bash python train.py train --env='fasterrcnn' --plot-every=100 ```  you may refer to `utils/config.py` for more argument.  Some Key arguments:  - `--caffe-pretrain=False`: use pretrain model from caffe or torchvision (Default: torchvison) - `--plot-every=n`: visualize prediction  loss etc every `n` batches. - `--env`: visdom env for visualization - `--voc_data_dir`: where the VOC data stored - `--use-drop`: use dropout in RoI head  default False - `--use-Adam`: use Adam instead of SGD  default SGD. (You need set a very low `lr` for Adam) - `--load-path`: pretrained model path  default `None`  if it's specified  it would be loaded.  you may open browser  visit `http://<ip>:8097` and see the visualization of training procedure as below:  ![visdom](imgs/visdom-fasterrcnn.png)   """;Computer Vision;https://github.com/chenyuntc/simple-faster-rcnn-pytorch
"""Follow the instructions below to get our project running on your local machine.  1. Clone the repository and make sure you have Python 3 to run the project. 2. Go to `src/PyTorch/` and run `python gan-mnist-pytorch.py` 3. All the outputs and related plots can be found in `src/PyTorch/output` folder generated. 4. The various parameters that can be tweaked before run can be found at `python gan-mnist-pytorch.py --help`   """;Computer Vision;https://github.com/vamsi3/simple-GAN
"""Customize paths first in `setup.sh` (data folder  model save folder  etc.). ```bash git clone git://github.com/renmengye/revnet-public.git cd revnet-public #: Change paths in setup.sh #: It also provides options to download CIFAR and ImageNet data. (ImageNet #: experiments require dataset in tfrecord format). ./setup.sh ```   """;Computer Vision;https://github.com/renmengye/revnet-public
"""Customize paths first in `setup.sh` (data folder  model save folder  etc.). ```bash git clone git://github.com/renmengye/revnet-public.git cd revnet-public #: Change paths in setup.sh #: It also provides options to download CIFAR and ImageNet data. (ImageNet #: experiments require dataset in tfrecord format). ./setup.sh ```   """;General;https://github.com/renmengye/revnet-public
"""To use training/evaluating scripts as well as all models  you need to clone the repository and install dependencies: ``` git clone git@github.com:osmr/imgclsmob.git pip install -r requirements.txt ```   - pytorchcv for PyTorch    - PyTorch models    """;General;https://github.com/osmr/imgclsmob
"""Please ensure you have [Pipenv](https://pipenv.readthedocs.io/en/latest/) installed. Clone the repository and use `pipenv --three install` to create yourself an environment to run the code in. Otherwise just install the packages mentioned in Pipfile.  Due to the transitive dependency to tensorflow that comes from unity ml-agents and the [bug](https://github.com/pypa/pipenv/issues/1716) causing incompatibility to jupyter you might want to either drop the jupyter from the list of dependencies or run `pipenv --three install --skip-lock` to overcome it.  To activate a virtual environment with pipenv issue `pipenv shell` while in the root directory of the repository.  After creating and entering the virtual environment you need to set a `DRLUD_P1_ENV` shell environment which must point to the binaries of the Unity environment. Example of for Mac OS version of binaries it might be  ``` DRLUD_P1_ENV=../deep-reinforcement-learning/p1_navigation/Banana.app; export DRLUD_P1_ENV ```  Details of downloading and setting of the environment are described in Udacity nanodegree materials.   """;Reinforcement Learning;https://github.com/jezzarax/drlnd_p1_navigation
"""""";Computer Vision;https://github.com/jason90330/EdgeFinal
"""""";Computer Vision;https://github.com/saransh317/YOLOv3-Easy-Implementation
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;General;https://github.com/anookeen/yolo
"""The code will look like the following:   Q-Learning on Frozen Lake Environment  DQN and DDQN on Cartpole Environment   DQN on Cartpole Environment:   """;Computer Vision;https://github.com/cbxs123/Advanced-Deep-Learning-with-Keras
"""Version 2.2  released on 06/06/2019.  Version 2.1  released on 04/06/2019.  Version 2.0  released on 26/03/2019.   """;General;https://github.com/curto2/mckernel
"""""";General;https://github.com/VascoLopes/AutoClassifier
"""""";General;https://github.com/lxdragoon/Modified-mnist
"""""";Sequential;https://github.com/lxdragoon/Modified-mnist
"""""";Natural Language Processing;https://github.com/lxdragoon/Modified-mnist
"""| parameter            | description | |---                   |--- | | --source_dataset PATH |  parallel training corpus (source side) | | --target_dataset PATH |  parallel training corpus (target side) | | --dictionaries PATH [PATH ...] | network vocabularies (one per source factor  plus target vocabulary) | | --model PATH         |  model file name (default: model.npz) | | --saveFreq INT       |  save frequency (default: 30000) | | --reload             |  load existing model (if '--model' points to existing model) | | --no_reload_training_progress | don't reload training progress (only used if --reload is enabled) | | --summary_dir        |  directory for saving summaries (default: same directory as the --saveto file) | | --summaryFreq        |  Save summaries after INT updates  if 0 do not save summaries (default: 0) |   | --valid_source_dataset PATH | parallel validation corpus (source side)| (default: None) |   | parameter            | description | |---                   |--- | | -k K                 | Beam size (default: 5)) | | -p P                 | Number of processes (default: 5)) | | -n                   | Normalize scores by sentence length | | -v                   | verbose mode. | | --models MODELS [MODELS ...]  -m MODELS [MODELS ...] | model to use. Provide multiple models (with same vocabulary) for ensemble decoding | | --input PATH  -i PATH | Input file (default: standard input) | | --output PATH  -o PATH | Output file (default: standard output) | | --n-best             | Write n-best list (of size k) |   | parameter              | description | |---                     |--- | | -b B                   |   Minibatch size (default: 80)) | | -n                     |   Normalize scores by sentence length | | -v                     |   verbose mode. | | --models MODELS [MODELS ...]  -m MODELS [MODELS ...] | model to use. Provide multiple models (with same vocabulary) for ensemble decoding | | --source PATH  -s PATH | Source text file | | --target PATH  -t PATH | Target text file | | --output PATH  -o PATH | Output file (default: standard output) |    The n-best list is assumed to have the same format as Moses:      sentence-ID (starting from 0) ||| translation ||| scores  new scores will be appended to the end. `rescore.py` has the same arguments as `score.py`  with the exception of this additional parameter:  | parameter             | description | |---                    |--- | | --input PATH  -i PATH | Input n-best list file (default: standard input) |    """;General;https://github.com/bhaddow/dev-nematus
"""Pixel CNNs are a type of autoregressive generative models which try to model the generation of images as a sequence of generation of pixels. They use multiple convolutional layers to model the generation of next pixel conditioned on the pixels of the image which have already been generated. The layers preserve the spatial resolution of the input image in order to output the image of same size. During training phase  we start from the input image as shown below and perform convolution over it with the kernel of our first layer.   ![Representation of Convolution on the input without masking](images/Unmasked_Conv.png)  In the example above  we try to generate the pixel in the centre using the pixels which have already been generated. As described in the paper we are generating the pixels in the sequence as shown below:  ![Generating image as a sequence of Pixels](images/Sequence.png)  Clearly  pixel `a` should therefore not take into account the `b  f and g` since as per the sequence  during testing time  it won't have access to them. In order to replicate this even during the training stage as well  [A Oord et. al.](https://arxiv.org/abs/1601.06759) propose modification to the convolutional kernel by applying a mask to it. The mask will make that portion  which is not accessible to the model during testing time while generating the central pixel  0 as can be seen below:  ![Representation of Convolution on the input without masking](images/Masked_Conv.png)  Thus sequence by sequence we keep on generating the pixels one by one until the entire image is generated. This can be visualised very neatly with the help of the graphic image below:  ![Visualisation](images/Visualisation.gif)   """;Computer Vision;https://github.com/singh-hrituraj/PixelCNN-Pytorch
"""A dataset for book recommendations: ten thousand books  one million ratings   *  [UFLDL Tutorial 1](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial) *  [UFLDL Tutorial 2](http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/) *  [Deep Learning for NLP (without Magic)](http://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial) *  [A Deep Learning Tutorial: From Perceptrons to Deep Networks](http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks) *  [Deep Learning from the Bottom up](http://www.metacademy.org/roadmaps/rgrosse/deep_learning) *  [Theano Tutorial](http://deeplearning.net/tutorial/deeplearning.pdf) *  [TensorFlow tutorials](https://github.com/nlintz/TensorFlow-Tutorials)  More details in [tutorials](tutorials.md)   """;Computer Vision;https://github.com/dineshzende/awesome-deep-learning-resources
"""The code will look like the following:   Q-Learning on Frozen Lake Environment  DQN and DDQN on Cartpole Environment   DQN on Cartpole Environment:   """;General;https://github.com/cbxs123/Advanced-Deep-Learning-with-Keras
"""""";Computer Vision;https://github.com/nmeripo/Reducing-the-Dimensionality-of-Data-with-Neural-Networks
"""- Clone this repo: ```bash git clone https://github.com/rtanno21609/AdaptiveNeuralTrees.git cd AdaptiveNeuralTrees ``` - (Optional) create a new Conda environment and activate it: ```bash conda create -n ANT python=2.7 source activate ANT ``` - Run the following to install required packages. ```  bash ./install.sh ```   An example command for training/testing an ANT is given below.  ```bash python tree.py --experiment test_ant_cifar10  #:name of experiment \                --subexperiment myant  #:name of subexperiment \                --dataset cifar10   #:dataset \                 #: Model details:    \                --router_ver 3        #:type of router module \                --router_ngf 128      #:no. of kernels in routers \                --router_k 3          #:spatial size of kernels in routers \                --transformer_ver 5   #:type of transformer module \                --transformer_ngf 128 #:no. of kernels in transformers \                --transformer_k 3     #:spatial size of kernels in transformers \                --solver_ver 6        #:type of solver module \                --batch_norm          #:apply batch-norm \                --maxdepth 10         #:maximum depth of the tree-structure \                 #: Training details: \                --batch-size 512    #:batch size \                --augmentation_on   #:apply data augmentation \                --scheduler step_lr #:learning rate scheduling \                --criteria avg_valid_loss #: splitting criteria                --epochs_patience 5 #:no. of patience per node for growth phase \                --epochs_node 100   #:max no. of epochs per node for growth phase \                --epochs_finetune 200 #:no. of epochs for fine-tuning phase \                #: Others: \                --seed 0            #:randomisation seed                --num_workers 0     #:no. of CPU subprocesses used for data loading \                --visualise_split  #: save the tree structure every epoch \ ``` The model configurations and optimisation trajectory (e.g value of train/validation loss at each time point) are saved in `records.jason` in the  directory `./experiments/dataset/experiment/subexperiment/checkpoints`. Similarly  tree structure and best trained model are saved as `tree_structures.json` and `model.pth`  respectively under the same directory. If the visualisation option  `--visualise_split` is used  the tree architecture of the ANT is saved in the PNG format in the directory `./experiments/dataset/experiment/subexperiment/cfigures`.  By default  the average classification accuracy is also computed on train/valid/test sets for every epoch and saved in `records.jason` file  so running `tree.py` would suffice for both training and testing an ANT of particular  configurations.   **Jupyter Notebooks**  We have also included two Jupter notebooks `./notebooks/example_mnist.ipynb` and `./notebooks/example_cifar10.ipynb`  which illustrate how this repository  can be used to train ANTs on MNIST and CIFAR-10 image recognition datasets.    **Primitive modules**  Defining an ANT amounts to specifying the forms of primitive modules: routers  transformers and solvers. The table below provides the list of currently implemented primitive modules. You can try any combination of three to construct an ANT.   | Type | Router | Transformer  | Solver | | ------------- |:-------------:  | :-----------:|:-----:| | 1     | 1 x Conv + GAP + Sigmoid | Identity function | Linear classifier  | | 2     | 1 x Conv + GAP + 1 x FC   | 1 x Conv | MLP with 2 hidden layers  | | 3     | 2 x Conv + GAP + 1 x FC   | 1 x Conv + 1 x MaxPool | MLP with 1 hidden layer | | 4     | MLP with 1 hidden layer   | Bottleneck residual block ([He et al.  2015](https://arxiv.org/abs/1512.03385)) | GAP + 2 FC layers + Softmax | | 5     | GAP + 2 x FC layers ([Veit et al.  2017](https://arxiv.org/abs/1711.11503)) | 2 x Conv + 1 x MaxPool | MLP with 1 hidden layer in AlexNet ([layers-80sec.cfg](https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt))    | | 6     | 1 x Conv +  GAP + 2 x FC | Whole VGG13 architecture (without the linear layer) | GAP + 1 FC layers + Softmax  |  For the detailed definitions of respective modules  please see `utils.py` and  `models.py`.    """;General;https://github.com/rtanno21609/AdaptiveNeuralTrees
"""- Clone this repo: ```bash git clone https://github.com/rtanno21609/AdaptiveNeuralTrees.git cd AdaptiveNeuralTrees ``` - (Optional) create a new Conda environment and activate it: ```bash conda create -n ANT python=2.7 source activate ANT ``` - Run the following to install required packages. ```  bash ./install.sh ```   An example command for training/testing an ANT is given below.  ```bash python tree.py --experiment test_ant_cifar10  #:name of experiment \                --subexperiment myant  #:name of subexperiment \                --dataset cifar10   #:dataset \                 #: Model details:    \                --router_ver 3        #:type of router module \                --router_ngf 128      #:no. of kernels in routers \                --router_k 3          #:spatial size of kernels in routers \                --transformer_ver 5   #:type of transformer module \                --transformer_ngf 128 #:no. of kernels in transformers \                --transformer_k 3     #:spatial size of kernels in transformers \                --solver_ver 6        #:type of solver module \                --batch_norm          #:apply batch-norm \                --maxdepth 10         #:maximum depth of the tree-structure \                 #: Training details: \                --batch-size 512    #:batch size \                --augmentation_on   #:apply data augmentation \                --scheduler step_lr #:learning rate scheduling \                --criteria avg_valid_loss #: splitting criteria                --epochs_patience 5 #:no. of patience per node for growth phase \                --epochs_node 100   #:max no. of epochs per node for growth phase \                --epochs_finetune 200 #:no. of epochs for fine-tuning phase \                #: Others: \                --seed 0            #:randomisation seed                --num_workers 0     #:no. of CPU subprocesses used for data loading \                --visualise_split  #: save the tree structure every epoch \ ``` The model configurations and optimisation trajectory (e.g value of train/validation loss at each time point) are saved in `records.jason` in the  directory `./experiments/dataset/experiment/subexperiment/checkpoints`. Similarly  tree structure and best trained model are saved as `tree_structures.json` and `model.pth`  respectively under the same directory. If the visualisation option  `--visualise_split` is used  the tree architecture of the ANT is saved in the PNG format in the directory `./experiments/dataset/experiment/subexperiment/cfigures`.  By default  the average classification accuracy is also computed on train/valid/test sets for every epoch and saved in `records.jason` file  so running `tree.py` would suffice for both training and testing an ANT of particular  configurations.   **Jupyter Notebooks**  We have also included two Jupter notebooks `./notebooks/example_mnist.ipynb` and `./notebooks/example_cifar10.ipynb`  which illustrate how this repository  can be used to train ANTs on MNIST and CIFAR-10 image recognition datasets.    **Primitive modules**  Defining an ANT amounts to specifying the forms of primitive modules: routers  transformers and solvers. The table below provides the list of currently implemented primitive modules. You can try any combination of three to construct an ANT.   | Type | Router | Transformer  | Solver | | ------------- |:-------------:  | :-----------:|:-----:| | 1     | 1 x Conv + GAP + Sigmoid | Identity function | Linear classifier  | | 2     | 1 x Conv + GAP + 1 x FC   | 1 x Conv | MLP with 2 hidden layers  | | 3     | 2 x Conv + GAP + 1 x FC   | 1 x Conv + 1 x MaxPool | MLP with 1 hidden layer | | 4     | MLP with 1 hidden layer   | Bottleneck residual block ([He et al.  2015](https://arxiv.org/abs/1512.03385)) | GAP + 2 FC layers + Softmax | | 5     | GAP + 2 x FC layers ([Veit et al.  2017](https://arxiv.org/abs/1711.11503)) | 2 x Conv + 1 x MaxPool | MLP with 1 hidden layer in AlexNet ([layers-80sec.cfg](https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt))    | | 6     | 1 x Conv +  GAP + 2 x FC | Whole VGG13 architecture (without the linear layer) | GAP + 1 FC layers + Softmax  |  For the detailed definitions of respective modules  please see `utils.py` and  `models.py`.    """;Computer Vision;https://github.com/rtanno21609/AdaptiveNeuralTrees
"""git clone git@github.com:cybertronai/pytorch-lamb.git  cd pytorch-lamb  pip install -e .   """;General;https://github.com/cybertronai/pytorch-lamb
"""Python 3 dependencies:  * Tensorflow 1.15 * matplotlib * numpy * imageio *  configargparse  The LLFF data loader requires ImageMagick.  We provide a conda environment setup file including all of the above dependencies. Create the conda environment `nerf` by running: ``` conda env create -f environment.yml ```  You will also need the [LLFF code](http://github.com/fyusion/llff) (and COLMAP) set up to compute poses if you want to run on your own real data.   conda env create -f environment.yml  conda activate nerf  bash download_example_data.sh   bash download_example_data.sh   bash download_example_weights.sh   pip install trimesh pyrender PyMCubes   """;Computer Vision;https://github.com/bmild/nerf
""" You can download the dataset via the link below.<br><br> <a href=""https://github.com/OlafenwaMoses/FireNET/releases/download/v1.0/fire-dataset.zip"" >https://github.com/OlafenwaMoses/FireNET/releases/download/v1.0/fire-dataset.zip</a>  <br><br>     <b><a href=""https://github.com/OlafenwaMoses/FireNET/releases/download/v1.0/detection_model-ex-33--loss-4.97.h5"" >https://github.com/OlafenwaMoses/FireNET/releases/download/v1.0/detection_model-ex-33--loss-4.97.h5</a></b><br>   Running the experiment or detection requires that you have Tensorflow  and Keras  OpenCV and ImageAI installed. You can install this dependencies via the commands below.  <br><span><b>- Tensorflow 1.4.0 (and later versions)  </b>      <a href=""https://www.tensorflow.org/install/install_windows"" style=""text-decoration: none;"" > Install</a></span> or install via pip <pre> pip3 install --upgrade tensorflow </pre>   <span><b>- OpenCV  </b>        <a href=""https://pypi.python.org/pypi/opencv-python"" style=""text-decoration: none;"" >Install</a></span> or install via pip <pre> pip3 install opencv-python </pre>   <span><b>- Keras 2.x  </b>     <a href=""https://keras.io/#installation"" style=""text-decoration: none;"" >Install</a></span> or install via pip <pre> pip3 install keras </pre>       <span>      <pre>pip3 install imageai --upgrade </pre></span> <br><br> <br>   """;Computer Vision;https://github.com/OlafenwaMoses/FireNET
"""Pendenlum from GYM : https://gym.openai.com/   """;Reinforcement Learning;https://github.com/Gouet/DDPG_PendulumV1
"""With the vast amount of landmark images on the Internet  the time has come to think about landmarks globally  namely to build a landmark prediction engine  on the scale of the entire earth.  Develop a technology that can predict landmark labels directly from image pixels  to help people better understand and organize the photo collections.The project challenges to build models that recognize the correct landmark in a dataset of challenging test images. The Kaggle challenge provides access to annotated data which consists of various links to google images along with their respective labeled classes.  Link to the Kaggle competition: [https://www.kaggle.com/c/landmark-recognition-challenge](https://www.kaggle.com/c/landmark-recognition-challenge)   - process : Specify if you want to train or test the dataset and keep empty if train and test both required                     Ex: --path=""/home/ubuntu/img.npy""   """;General;https://github.com/ankit-vaghela30/Google-landmark-prediction
"""With the vast amount of landmark images on the Internet  the time has come to think about landmarks globally  namely to build a landmark prediction engine  on the scale of the entire earth.  Develop a technology that can predict landmark labels directly from image pixels  to help people better understand and organize the photo collections.The project challenges to build models that recognize the correct landmark in a dataset of challenging test images. The Kaggle challenge provides access to annotated data which consists of various links to google images along with their respective labeled classes.  Link to the Kaggle competition: [https://www.kaggle.com/c/landmark-recognition-challenge](https://www.kaggle.com/c/landmark-recognition-challenge)   - process : Specify if you want to train or test the dataset and keep empty if train and test both required                     Ex: --path=""/home/ubuntu/img.npy""   """;Computer Vision;https://github.com/ankit-vaghela30/Google-landmark-prediction
"""- numpy  - opencv-pyhton (version 4.0)   download the pretrained model from insight-face <a href=""https://github.com/AIInAi/tf-insightface/tree/master/pretrained"">here</a> and store it in the /pretrained directory  run in order:  - train_init.py - train_new_face.py (avec comme argument le nom de la personne que vous voulez identifier) - face_recognition_.py    """;General;https://github.com/gregoiredervaux/face_recognition
"""Following the instructions in install  you could compile them by yourself. If you install tensorflow by pip  one potential error can be some source files of tensorflow set the wrong relative path of cuda.h  you just need to manually change them according to your cuda path.   cd standard_training/   cd standard_training/   cd progressive_training/   More video comparison  see the following youtube links:   """;Computer Vision;https://github.com/musikisomorphie/swd
"""The  `create_dataset` function will cut random slices from an images to create a new data set. This function requires you to put images in a new directory before hand ```python import matplotlib.pyplot as plt import numpy as np  from dcgan import create_dataset   #: first resize the original image to 75%  #: then cut 100 random 128x128 subframes from each image in the directory  x_train  y_train = create_dataset(128 128  nSlices=100  resize=0.75  directory='space/')  #: scale RGB data between 0 and 1 x_train /= 255   #: plot results to make sure data looks good! fig  axs = plt.subplots(4  4) for i in range(4):     for j in range(4):         axs[i j].imshow( x_train[ np.random.randint(x_train.shape[0]) ] )         axs[i j].axis('off') plt.show() ``` An example output should look like this:   ![](https://github.com/pearsonkyle/Neural-Nebula/blob/master/images/nebula_training_sample.png)  If `x_train` is empty make sure you have `.jpg` or `.png` files in the directory where your images are stored (e.g. `space/`)     Clone the repo  cd into the directory  launch iPython and paste the example below  ```python  import tensorflow as tf from dcgan import DCGAN  create_dataset  if __name__ == '__main__':      x_train  y_train = create_dataset(128 128  nSlices=150  resize=0.75  directory='space/')     assert(x_train.shape[0]>0)      x_train /= 255       dcgan = DCGAN(img_rows = x_train[0].shape[0]                      img_cols = x_train[0].shape[1]                      channels = x_train[0].shape[2]                       latent_dim=32                      name='nebula_32_128')                          dcgan.train(x_train  epochs=1000  batch_size=32  save_interval=100) ``` After it's done training check the `images/` folder for outputs during the training process   Prior to running the code below you will have to remove the upsampling layers in the GAN ([line 84](https://github.com/pearsonkyle/Neural-Nebula/blob/master/dcgan.py#L84) and [line 95](https://github.com/pearsonkyle/Neural-Nebula/blob/master/dcgan.py#L95) ) in order to preserve the 32 x 32 output resolution of the generator ```python from keras.datasets import cifar10 from dcgan import DCGAN  if __name__ == '__main__':      (x_train  y_train)  (x_test  y_test) = cifar10.load_data()      #: only birds  then scale images between 0-1     x_train = x_train[ (y_train==2).reshape(-1) ]      x_train = x_train/255          dcgan = DCGAN(img_rows = x_train[0].shape[0]                      img_cols = x_train[0].shape[1]                      channels = x_train[0].shape[2]                       latent_dim=128                      name='cifar_128')      dcgan.train(x_train  epochs=10001  batch_size=32  save_interval=100)          dcgan.save_imgs('final')  ``` Below is an animation of the training process every 500 training batches. The code above took ~10 minutes to run on a GTX 1070. These are random samples from the generator during training. After just 10 minutes of training you can start to see structure that resembles a bird. There's only so much structure you can get from a 32 x 32 pixel image to begin with... More realistic images can be chosen by evaluating them with the discriminator after generating.   ![](https://github.com/pearsonkyle/Neural-Nebula/blob/master/images/cifar_bird.gif)   """;Computer Vision;https://github.com/pearsonkyle/Neural-Nebula
"""This is a standard train-dev-test split on all the 8732 datapoints from the dataset.  <br />   """;General;https://github.com/nitinvwaran/UrbanSound8K-audio-classification-with-ResNet
"""This is a standard train-dev-test split on all the 8732 datapoints from the dataset.  <br />   """;Computer Vision;https://github.com/nitinvwaran/UrbanSound8K-audio-classification-with-ResNet
"""This method aims at helping computer vision practitioners faced with an overfit problem. The idea is to replace  in a 3-branch ResNet  the standard summation of residual branches by a stochastic affine combination. The largest tested model improves on the best single shot published result on CIFAR-10 by reaching 2.72% test error.  ![shake-shake](https://s3.eu-central-1.amazonaws.com/github-xg/architecture3.png)  Figure 1: **Left:** Forward training pass. **Center:** Backward training pass. **Right:** At test time.   This repository contains the code for the paper Decoupled Weight Decay Regularization (old title: Fixing Weight Decay Regularization in Adam) by Ilya Loshchilov and Frank Hutter  ICLR 2019 [arXiv](https://arxiv.org/abs/1711.05101).   The code represents a tiny modification of the source code provided for the Shake-Shake regularization by Xavier Gastaldi [arXiv](https://arxiv.org/abs/1705.07485). Since the usage of both is very similar  the introduction and description of the original Shake-Shake code is given below. Please consider to  *first* run the Shake-Shake code and then our code.   Find below a few examples to train a 26 2x96d ""Shake-Shake-Image"" ResNet on CIFAR-10 with 1 GPU. To run on 4 GPUs  set `CUDA_VISIBLE_DEVICES=0 1 2 3` and `-nGPU 4`. For test purposes you may reduce `-nEpochs` from 1500 to e.g. 150 and set `-widenFactor` to 4 to use a smaller network.  To run on ImageNet32x32  set `-dataset` to imagenet32 and reduce `-nEpochs` to 150. You may consider to use `-weightDecay=0.05` for CIFAR-10.   Importantly  please copy with replacement `adam.lua` and `sgd.lua` from `UPDATETORCHFILES` to `YOURTORCHFOLDER/install/share/lua/5.1/optim/`  To run AdamW for `nEpochs=1500` epochs without restarts with initial learning rate `LR=0.001`  normalized weight decay `weightDecay=0.025`     ``` CUDA_VISIBLE_DEVICES=0 th main.lua -algorithmType ADAMW -nEpochs 1500 -Te 1500 -Tmult 2 -widenFactor 6 -LR 0.001 -weightDecay 0.025 -dataset cifar10 -nGPU 1 -depth 26 -irun 1 -batchSize 128 -momentum 0.9 -shareGradInput false -optnet true -netType shakeshake -forwardShake true -backwardShake true -shakeImage true -lrShape cosine -LRdec true ```  To run AdamW for `nEpochs=1500` epochs with restarts  where the first restart will happen after `Te=100` epochs and the second restart after 200 more epochs because `100*Tmult=200`.   ``` CUDA_VISIBLE_DEVICES=0 th main.lua -algorithmType ADAMW -nEpochs 1500 -Te 100 -Tmult 2 -widenFactor 6 -LR 0.001 -weightDecay 0.025 -dataset cifar10 -nGPU 1 -depth 26 -irun 1 -batchSize 128 -momentum 0.9 -shareGradInput false -optnet true -netType shakeshake -forwardShake true -backwardShake true -shakeImage true -lrShape cosine -LRdec true ```  To run SGDW for `nEpochs=150` epochs without restarts with initial learning rate `LR=0.05`  normalized weight decay `weightDecay=0.025`     ``` CUDA_VISIBLE_DEVICES=0 th main.lua -algorithmType SGDW -nEpochs 1500 -Te 1500 -Tmult 2 -widenFactor 6 -LR 0.05 -weightDecay 0.025 -dataset cifar10 -nGPU 1 -depth 26 -irun 1 -batchSize 128 -momentum 0.9 -shareGradInput false -optnet true -netType shakeshake -forwardShake true -backwardShake true -shakeImage true -lrShape cosine -LRdec true ```  To run SGDW for `nEpochs=150` epochs with restarts  where the first restart will happen after `Te=100` epochs and the second restart after 200 more epochs because `100*Tmult=200`.   ``` CUDA_VISIBLE_DEVICES=0 th main.lua -algorithmType SGDW -nEpochs 1500 -Te 100 -Tmult 2 -widenFactor 6 -LR 0.001 -weightDecay 0.025 -dataset cifar10 -nGPU 1 -depth 26 -irun 1 -batchSize 128 -momentum 0.9 -shareGradInput false -optnet true -netType shakeshake -forwardShake true -backwardShake true -shakeImage true -lrShape cosine -LRdec true ```  Acknowledgments: We thank Patryk Chrabaszcz for creating functions dealing with ImageNet32x32 dataset.    0. Install [fb.resnet.torch] (https://github.com/facebook/fb.resnet.torch)  [optnet](https://github.com/fmassa/optimize-net) and [lua-stdlib](https://github.com/lua-stdlib/lua-stdlib). 1. Download Shake-Shake ``` git clone https://github.com/xgastaldi/shake-shake.git ``` 2. Copy the elements in the shake-shake folder and paste them in the fb.resnet.torch folder. This will overwrite 5 files (*main.lua*  *train.lua*  *opts.lua*  *checkpoints.lua* and *models/init.lua*) and add 3 new files (*models/shakeshake.lua*  *models/shakeshakeblock.lua* and *models/mulconstantslices.lua*). 3. You can train a 26 2x32d ""Shake-Shake-Image"" ResNet on CIFAR-10+ using  ``` th main.lua -dataset cifar10 -nGPU 1 -batchSize 128 -depth 26 -shareGradInput false -optnet true -nEpochs 1800 -netType shakeshake -lrShape cosine -widenFactor 2 -LR 0.2 -forwardShake true -backwardShake true -shakeImage true ```   You can train a 26 2x96d ""Shake-Shake-Image"" ResNet on 2 GPUs using  ``` CUDA_VISIBLE_DEVICES=0 1 th main.lua -dataset cifar10 -nGPU 2 -batchSize 128 -depth 26 -shareGradInput false -optnet true -nEpochs 1800 -netType shakeshake -lrShape cosine -widenFactor 6 -LR 0.2 -forwardShake true -backwardShake true -shakeImage true ```  A widenFactor of 2 corresponds to 32d  4 to 64d  etc..   """;General;https://github.com/loshchil/AdamW-and-SGDW
"""AMLA is a common framework to run different AutoML algorithms for neural networks without changing  the underlying systems needed to configure  train and evaluate the generated networks. This has two benefits: * It ensures that different AutoML algorithms can be easily compared using the same set of hyperparameters and infrastructure  allowing for  easy evaluation  comparison and ablation studies of AutoML algorithms. * It provides a easy way to deploy AutoML algorithms on multi-cloud infrastructure.  With a framework  we can manage the lifecycle of autoML easily. Without this  hyperparameters and architecture design are spread out  some embedded in the code  others in config files and other as command line parameters  making it hard to compare two algorithms or perform ablation studies.  Some design principles of AMLA: * The network generation process is decoupled from the training/evaluation process. * The network specification model is independent of the implementation of the training/evaluation/generation code and ML library (i.e. whether it uses TensorFlow/PyTorch etc.).  AMLA currently supports the [NAC using EnvelopeNets](http://arxiv.org/pdf/1803.06744) AutoML algorithm  and we are actively adding newer algorithms to the framework. More information on AutoML algorithms for Neural Networks can be found [here](https://github.com/hibayesian/awesome-automl-papers)   ```     git clone https://github.com/ciscoai/amla     cd amla/amla     pip install -r requirements.txt ```   Here are some areas that we need help with:   """;General;https://github.com/CiscoAI/amla
"""When *training with the backbone of [IBN-ResNet-50](https://arxiv.org/abs/1807.09441)*  you need to download the [ImageNet](http://www.image-net.org/) pre-trained model from this [link](https://drive.google.com/drive/folders/1thS2B8UOSBi_cJX6zRy6YYRwz_nVFI_S) and save it under the path of `logs/pretrained/`. ```shell mkdir logs && cd logs mkdir pretrained ``` The file tree should be ``` MMT/logs ‚îî‚îÄ‚îÄ pretrained  ¬†¬† ‚îî‚îÄ‚îÄ resnet50_ibn_a.pth.tar ```   ```shell cd examples && mkdir data ``` Download the raw datasets [DukeMTMC-reID](https://arxiv.org/abs/1609.01775)  [Market-1501](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf)  [MSMT17](https://arxiv.org/abs/1711.08565)  and then unzip them under the directory like ``` MMT/examples/data ‚îú‚îÄ‚îÄ dukemtmc ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ DukeMTMC-reID ‚îú‚îÄ‚îÄ market1501 ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ Market-1501-v15.09.15 ‚îî‚îÄ‚îÄ msmt17     ‚îî‚îÄ‚îÄ MSMT17_V1 ```   ```shell git clone https://github.com/yxgeee/MMT.git cd MMT python setup.py install ```   sh scripts/pretrain.sh dukemtmc market1501 resnet50 1  sh scripts/pretrain.sh dukemtmc market1501 resnet50 2   sh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet50 500   Note that you could add --rr-gpu in the training scripts for faster clustering but requiring more GPU memory.   sh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet50   sh scripts/test.sh market1501 resnet50 logs/dukemtmcTOmarket1501/resnet50-MMT-500/model_best.pth.tar   sh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 500  sh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 700  sh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 900   sh scripts/train_baseline_dbscan.sh dukemtmc market1501 resnet50    Transferring from [DukeMTMC-reID](https://arxiv.org/abs/1609.01775) to [Market-1501](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf) on the backbone of [ResNet-50](https://arxiv.org/abs/1512.03385)  *i.e. Duke-to-Market (ResNet-50)*.   **Duke-to-Market (IBN-ResNet-50)** ```shell #: pre-training on the source domain sh scripts/pretrain.sh dukemtmc market1501 resnet_ibn50a 1 sh scripts/pretrain.sh dukemtmc market1501 resnet_ibn50a 2 #: end-to-end training with MMT-500 sh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet_ibn50a 500 #: or MMT-700 sh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet_ibn50a 700 #: or MMT-DBSCAN sh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet_ibn50a  #: testing the best model sh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-500/model_best.pth.tar sh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-700/model_best.pth.tar sh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-DBSCAN/model_best.pth.tar ``` **Duke-to-MSMT (ResNet-50)** ```shell #: pre-training on the source domain sh scripts/pretrain.sh dukemtmc msmt17 resnet50 1 sh scripts/pretrain.sh dukemtmc msmt17 resnet50 2 #: end-to-end training with MMT-500 sh scripts/train_mmt_kmeans.sh dukemtmc msmt17 resnet50 500 #: or MMT-1000 sh scripts/train_mmt_kmeans.sh dukemtmc msmt17 resnet50 1000 #: or MMT-DBSCAN sh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet50  #: testing the best model sh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-500/model_best.pth.tar sh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-1000/model_best.pth.tar sh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-DBSCAN/model_best.pth.tar ```   """;General;https://github.com/yxgeee/MMT
"""When *training with the backbone of [IBN-ResNet-50](https://arxiv.org/abs/1807.09441)*  you need to download the [ImageNet](http://www.image-net.org/) pre-trained model from this [link](https://drive.google.com/drive/folders/1thS2B8UOSBi_cJX6zRy6YYRwz_nVFI_S) and save it under the path of `logs/pretrained/`. ```shell mkdir logs && cd logs mkdir pretrained ``` The file tree should be ``` MMT/logs ‚îî‚îÄ‚îÄ pretrained  ¬†¬† ‚îî‚îÄ‚îÄ resnet50_ibn_a.pth.tar ```   ```shell cd examples && mkdir data ``` Download the raw datasets [DukeMTMC-reID](https://arxiv.org/abs/1609.01775)  [Market-1501](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf)  [MSMT17](https://arxiv.org/abs/1711.08565)  and then unzip them under the directory like ``` MMT/examples/data ‚îú‚îÄ‚îÄ dukemtmc ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ DukeMTMC-reID ‚îú‚îÄ‚îÄ market1501 ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ Market-1501-v15.09.15 ‚îî‚îÄ‚îÄ msmt17     ‚îî‚îÄ‚îÄ MSMT17_V1 ```   ```shell git clone https://github.com/yxgeee/MMT.git cd MMT python setup.py install ```   sh scripts/pretrain.sh dukemtmc market1501 resnet50 1  sh scripts/pretrain.sh dukemtmc market1501 resnet50 2   sh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet50 500   Note that you could add --rr-gpu in the training scripts for faster clustering but requiring more GPU memory.   sh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet50   sh scripts/test.sh market1501 resnet50 logs/dukemtmcTOmarket1501/resnet50-MMT-500/model_best.pth.tar   sh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 500  sh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 700  sh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 900   sh scripts/train_baseline_dbscan.sh dukemtmc market1501 resnet50    Transferring from [DukeMTMC-reID](https://arxiv.org/abs/1609.01775) to [Market-1501](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf) on the backbone of [ResNet-50](https://arxiv.org/abs/1512.03385)  *i.e. Duke-to-Market (ResNet-50)*.   **Duke-to-Market (IBN-ResNet-50)** ```shell #: pre-training on the source domain sh scripts/pretrain.sh dukemtmc market1501 resnet_ibn50a 1 sh scripts/pretrain.sh dukemtmc market1501 resnet_ibn50a 2 #: end-to-end training with MMT-500 sh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet_ibn50a 500 #: or MMT-700 sh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet_ibn50a 700 #: or MMT-DBSCAN sh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet_ibn50a  #: testing the best model sh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-500/model_best.pth.tar sh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-700/model_best.pth.tar sh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-DBSCAN/model_best.pth.tar ``` **Duke-to-MSMT (ResNet-50)** ```shell #: pre-training on the source domain sh scripts/pretrain.sh dukemtmc msmt17 resnet50 1 sh scripts/pretrain.sh dukemtmc msmt17 resnet50 2 #: end-to-end training with MMT-500 sh scripts/train_mmt_kmeans.sh dukemtmc msmt17 resnet50 500 #: or MMT-1000 sh scripts/train_mmt_kmeans.sh dukemtmc msmt17 resnet50 1000 #: or MMT-DBSCAN sh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet50  #: testing the best model sh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-500/model_best.pth.tar sh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-1000/model_best.pth.tar sh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-DBSCAN/model_best.pth.tar ```   """;Computer Vision;https://github.com/yxgeee/MMT
"""""";Reinforcement Learning;https://github.com/170928/-Review-Proximal-Policy-Optimization-Algorithms
"""cocostuff-10k-v1.0.zip | COCO-Stuff dataset version 1.0  including images and annotations | 2.6 GB   Use the following steps to download and setup the DeepLab [4] semantic segmentation model trained on COCO-Stuff. It requires deeplab-public-ver2  which is built on Caffe:  Install Cuda. I recommend version 7.0. For version 8.0 you will need to apply the fix described here in step 3.  Download deeplab-public-ver2: git submodule update --init models/deeplab/deeplab-public-ver2  Compile and configure deeplab-public-ver2 following the author's instructions. Depending on your system setup you might have to install additional packages  but a minimum setup could look like this:  cd models/deeplab/deeplab-public-ver2   make all -j8  cd ../..  Configure the COCO-Stuff dataset:   Run cd models/deeplab &amp;&amp; ./run_cocostuff_vgg16.sh to train and test the network on COCO-Stuff.   cd models/deeplab   Run ./run_cocostuff_resnet101.sh to train and test the network on COCO-Stuff.   To use the COCO-Stuff dataset  please follow these steps:  1. Download or clone this repository using git: `git clone https://github.com/nightrome/cocostuff10k.git` 2. Open the dataset folder in your shell: `cd cocostuff10k` 3. If you have Matlab  run the following commands:   - Add the code folder to your Matlab path: `startup();`   - Run the demo script in Matlab `demo_cocoStuff();`   - The script displays an image  its thing  stuff and thing+stuff annotations  as well as the image captions. 4. Alternatively run the following Linux commands or manually download and unpack the dataset:   - `wget --directory-prefix=downloads http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/cocostuff-10k-v1.1.zip`   - `unzip downloads/cocostuff-10k-v1.1.zip -d dataset/`   """;Computer Vision;https://github.com/nightrome/cocostuff10k
"""1. Download [VoVNet39-ImageNet](https://dl.dropbox.com/s/zbys2uzvpfi7ko4/VoVNet39_ImageNet_Pretrained.caffemodel?dl=1). By default  we assume the model is stored in `$RefineDet_ROOT/models/ImageNet/VoVNet/`.  3. Follow the [data/VOC0712/README.md](https://github.com/sfzhang15/RefineDet/blob/master/data/VOC0712/README.md) to download VOC2007 and VOC2012 dataset and create the LMDB file for the VOC2007 training and testing.  4. Follow the [data/VOC0712Plus/README.md](https://github.com/sfzhang15/RefineDet/blob/master/data/VOC0712Plus/README.md) to download VOC2007 and VOC2012 dataset and create the LMDB file for the VOC2012 training and testing.  5. Follow the [data/coco/README.md](https://github.com/sfzhang15/RefineDet/blob/master/data/coco/README.md) to download MS COCO dataset and create the LMDB file for the COCO training and testing.   1. Get the code. We will call the cloned directory as `$RefineDet_ROOT`.   ```Shell   git clone https://github.com/youngwanLEE/VoVNet-RefineDet.git   ```  2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.   ```Shell   cd $RefineDet_ROOT   #: Modify Makefile.config according to your Caffe installation.   #: Make sure to include $RefineDet_ROOT/python to your PYTHONPATH.   cp Makefile.config.example Makefile.config   make all -j && make py   ```   Train your model on COCO.   Build the Cython modules.     cd $RefineDet_ROOT/test/lib    make -j     #: For GPU users   COCO models:   """;Computer Vision;https://github.com/youngwanLEE/VoVNet-RefineDet
"""Before you proceed  pip install -r ./requirements.txt   """;Natural Language Processing;https://github.com/alexandra-chron/ntua-slp-wassa-iest2018
"""Optax is a gradient processing and optimization library for JAX.  Optax is designed to facilitate research by providing building blocks that can be easily recombined in custom ways.  Our goals are to:  *   Provide simple  well-tested  efficient implementations of core components. *   Improve research productivity by enabling to easily combine low level     ingredients into custom optimisers (or other gradient processing components). *   Accelerate adoption of new ideas by making it easy for anyone to contribute.  We favour focusing on small composable building blocks that can be effectively combined into custom solutions. Others may build upon these basic components more complicated abstractions. Whenever reasonable  implementations prioritise readability and structuring code to match standard equations  over code reuse.  An initial prototype of this library was made available in JAX's experimental folder as `jax.experimental.optix`. Given the wide adoption across DeepMind of `optix`  and after a few iterations on the API  `optix` was eventually moved out of `experimental` as a standalone open-source library  renamed `optax`.  Documentation on Optax can be found at [optax.readthedocs.io](https://optax.readthedocs.io/).   Optax can be installed with pip directly from github  with the following command:  ```shell pip install git+git://github.com/deepmind/optax.git ```  or from PyPI:  ```shell pip install optax ```   typically intractable due to the quadratic memory requirements. Solving for the   functions for computing these diagonals with sub-quadratic memory requirements.   distribution. These can then be used to update distributional parameters  or   """;General;https://github.com/deepmind/optax
"""""";General;https://github.com/Doffery/BERT-Sentiment-Analysis-Amazon-Review
"""""";General;https://github.com/yfreedomliTHU/mos-pytorch1.1
"""""";Natural Language Processing;https://github.com/BroCoLySTyLe/SQLovaReview
"""""";General;https://github.com/seujung/WeightStandardization_gluon
"""MNC is an instance-aware semantic segmentation system based on deep convolutional networks  which won the first place in COCO segmentation challenge 2015  and test at a fraction of a second per image. We decompose the task of instance-aware semantic segmentation into related sub-tasks  which are solved by multi-task network cascades (MNC) with shared features. The entire MNC network is trained end-to-end with error gradients across cascaded stages.   ![example](data/readme_img/example.png)   MNC was initially described in a [CVPR 2016 oral paper](http://arxiv.org/abs/1512.04412).  This repository contains a python implementation of MNC  which is ~10% slower than the original matlab implementation.  This repository includes a bilinear RoI warping layer  which enables gradient back-propagation with respect to RoI coordinates.   0. Run `./data/scripts/fetch_imagenet_models.sh` to download the ImageNet pre-trained VGG-16 net.  0. Download the VOC 2007 dataset to ./data/VOCdevkit2007 0. Run `./data/scripts/fetch_sbd_data.sh` to download the VOC 2012 dataset together with the additional segmentation annotations in [SBD](https://9bc0b5eb4c18f1fc9a28517a91305702c68a10ae.googledrive.com/host/0ByUkob0WA1-NQi1sNlg4WkJQbTg/codes/SBD/download.html) to ./data/VOCdevkitSDS.   1. Clone the MNC repository:   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/daijifeng001/MNC.git   ```   2. Install Python packages: `numpy`  `scipy`  `cython`  `python-opencv`  `easydict`  `yaml`.  3. Build the Cython modules and the gpu_nms  gpu_mask_voting modules by:   ```Shell   cd $MNC_ROOT/lib   make   ```  4. Install `Caffe` and `pycaffe` dependencies (see: [Caffe installation instructions](http://caffe.berkeleyvision.org/installation.html) for official installation guide)    **Note:** Caffe *must* be built with support for Python layers!    ```make   #: In your Makefile.config  make sure to have this line uncommented   WITH_PYTHON_LAYER := 1   #: CUDNN is recommended in building to reduce memory footprint   USE_CUDNN := 1   ```  5. Build Caffe and pycaffe:     ```Shell     cd $MNC_ROOT/caffe-mnc     #: If you have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```   This code has been tested on Linux (Ubuntu 14.04)  using K40/Titan X GPUs.   cd $MNC_ROOT   : GPU_ID is the GPU you want to train on   cd $MNC_ROOT   : GPU_ID is the GPU you want to train on   cd $MNC_ROOT   : GPU_ID is the GPU you want to train on   First  download the trained MNC model. ```Shell ./data/scripts/fetch_mnc_model.sh ```   Run the demo: ```Shell cd $MNC_ROOT ./tools/demo.py ``` Result demo images will be stored to ```data/demo/```.  The demo performs instance-aware semantic segmentation with a trained MNC model (using VGG-16 net). The model is pre-trained on ImageNet  and finetuned on VOC 2012 train set with additional annotations from [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html). The mAP^r of the model is 65.0% on VOC 2012 validation set. The test speed per image is ~0.33sec on Titian X and ~0.42sec on K40.   """;Computer Vision;https://github.com/daijifeng001/MNC
"""Python 3.6   matplotlib 2.2.2   ![best reconstruction](imgs/best_rec.png)   """;Computer Vision;https://github.com/LordAlucard90/Variational-AutoEncoder-For-Novelty-Detection
"""Python 3.6   matplotlib 2.2.2   ![best reconstruction](imgs/best_rec.png)   """;General;https://github.com/LordAlucard90/Variational-AutoEncoder-For-Novelty-Detection
"""|   https://github.com/xiaolai-sqlai/mobilenetv3  | 75.5             |       3.96            |       272               |   |   https://github.com/d-li14/mobilenetv3.pytorch         |  73.2             |   5.15            |   246              |       | https://github.com/Randl/MobileNetV3-pytorch      |73.5             |  5.48           |  220               |    | https://github.com/rwightman/gen-efficientnet-pytorch | 75.6 | 5.5 | 219 |    We train mobilenetv3-ssd use mmdetection framework(based on pytorch)  we use PASCAL VOC0712 trainval dataset to train  it reaches 71.7mAP on VOC2007 test dataset.   mobilenetv3-ssd pytorch model ÁôæÂ∫¶ÁΩëÁõòÈìæÊé•: https://pan.baidu.com/s/1sTGrTHxpv4yZJUpTJD8BNw ÊèêÂèñÁ†Å: sid9    you can refer to https://github.com/Tencent/ncnn/blob/master/examples/mobilenetv3ssdlite.cpp   """;General;https://github.com/ujsyehao/mobilenetv3-ssd
"""|   https://github.com/xiaolai-sqlai/mobilenetv3  | 75.5             |       3.96            |       272               |   |   https://github.com/d-li14/mobilenetv3.pytorch         |  73.2             |   5.15            |   246              |       | https://github.com/Randl/MobileNetV3-pytorch      |73.5             |  5.48           |  220               |    | https://github.com/rwightman/gen-efficientnet-pytorch | 75.6 | 5.5 | 219 |    We train mobilenetv3-ssd use mmdetection framework(based on pytorch)  we use PASCAL VOC0712 trainval dataset to train  it reaches 71.7mAP on VOC2007 test dataset.   mobilenetv3-ssd pytorch model ÁôæÂ∫¶ÁΩëÁõòÈìæÊé•: https://pan.baidu.com/s/1sTGrTHxpv4yZJUpTJD8BNw ÊèêÂèñÁ†Å: sid9    you can refer to https://github.com/Tencent/ncnn/blob/master/examples/mobilenetv3ssdlite.cpp   """;Computer Vision;https://github.com/ujsyehao/mobilenetv3-ssd
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release  and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show < data/train.txt > result.txt`     You can comment this line so that each image does not require pressing the button ESC: https://github.com/AlexeyAB/darknet/blob/6ccb41808caf753feea58ca9df79d6367dedc434/src/detector.c#L509   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;General;https://github.com/eric-erki/yolov3
"""""";Computer Vision;https://github.com/wbenbihi/hourglasstensorlfow
"""The hyper-parameters are divided in 4 categories.    This pipeline's purpose is to train a neural network to segment NifTi files from examples.   Since the training requires example  the first step consists in producing manual segmentations of a fraction of the files. 10 to 50% of the files should be a good proportion  however this sample must be representative of the rest of the dataset. Datasets with great variability might require bigger fractions to be manually segmented.   The network is trained through a gradient back-propagation algorithm on the loss. The loss quantifies the difference between the predictions of the network and the manual segementations.   Once trained  the network can be used to automtically segment the entire dataset.  For training and inference  the volumes are sliced along the vertical axis and treated as collections of 2D images. Thus the image processing operations are 2D operations. Data augmentation is used on the training data. It consists in random modifications of the images and their corresponding GT to create more various examples.   <img src=""./media/process.png"" alt=""process schema"" width=""600""/>    Rename the *parameters_template.json* file to *parameters.json* and modify the values with the hyper-parameters you want.   See the section **Description of the hyper-parameters** below for a complete description of their functions.   A copy of the *parameters.json* file is added to the folder of the run where the model is saved.      Clone the repo:   ``` bash git clone https://github.com/neuropoly/multiclass-segmentation cd multiclass-segmentation ```  The required librairies can be easily installed with pip:  ``` bash pip install -r requirements.txt ```    > Note: To use tensorboard you must also install tensorflow with    > ``` pip install tensorflow```   You can use the --cuda option to use cuda (thus running on GPU)  and the --GPU_id argument (int) to define the id of the GPU to use (default is 0). For example :    """;Computer Vision;https://github.com/neuropoly/multiclass-segmentation
"""""";Computer Vision;https://github.com/CheesyB/cpointnet
"""OpenNMT-tf requires:  * Python 3.6 or above * TensorFlow 2.4  2.5  2.6  or 2.7  We recommend installing it with `pip`:  ```bash pip install --upgrade pip pip install OpenNMT-tf ```  *See the [documentation](https://opennmt.net/OpenNMT-tf/installation.html) for more information.*   """;General;https://github.com/OpenNMT/OpenNMT-tf
"""""";General;https://github.com/ceballots/Adam-with-cosine-scheduler-implementation
"""**Monolingual data (MLM)**: Follow the same procedure as in [I.1](https://github.com/facebookresearch/XLM#1-preparing-the-data)  and download multiple monolingual corpora  such as the Wikipedias.  Note that we provide a [tokenizer script](https://github.com/facebookresearch/XLM/blob/master/tools/tokenize.sh):  ``` lg=en cat my_file.$lg | ./tools/tokenize.sh $lg > my_tokenized_file.$lg & ```  **Parallel data (TLM)**: We provide download scripts for some language pairs in the *get-data-para.sh* script. ``` #: Download and tokenize parallel data in 'data/wiki/para/en-zh.{en zh}.{train valid test}' ./get-data-para.sh en-zh & ```  For other language pairs  look at the [OPUS collection](http://opus.nlpl.eu/)  and modify the get-data-para.sh script [here)(https://github.com/facebookresearch/XLM/blob/master/get-data-para.sh#L179-L180) to add your own language pair.  Now create you training set for the BPE vocabulary  for instance by taking 100M sentences from each monolingua corpora. ``` #: build the training set for BPE tokenization (50k codes) OUTPATH=data/processed/XLM_en_zh/50k mkdir -p $OUTPATH shuf -r -n 10000000 data/wiki/train.en >> $OUTPATH/bpe.train shuf -r -n 10000000 data/wiki/train.zh >> $OUTPATH/bpe.train ``` And learn the 50k BPE code as in the previous section on the bpe.train file. Apply BPE tokenization on the monolingual and parallel corpora  and binarize everything using *preprocess.py*:  ``` pair=en-zh  for lg in $(echo $pair | sed -e 's/\-/ /g'); do   for split in train valid test; do     $FASTBPE applybpe $OUTPATH/$pair.$lg.$split data/wiki/para/$pair.$lg.$split $OUTPATH/codes     python preprocess.py $OUTPATH/vocab $OUTPATH/$pair.$lg.$split   done done ```   First  get the monolingual data (English Wikipedia  the [TBC corpus](https://yknzhu.wixsite.com/mbweb) is not hosted anymore). ``` #: Download and tokenize Wikipedia data in 'data/wiki/en.{train valid test}' #: Note: the tokenization includes lower-casing and accent-removal ./get-data-wiki.sh en ```  [Install fastBPE](https://github.com/facebookresearch/XLM/tree/master/tools#fastbpe) and **learn BPE** vocabulary (with 30 000 codes here): ``` OUTPATH=data/processed/XLM_en/30k  #: path where processed files will be stored FASTBPE=tools/fastBPE/fast  #: path to the fastBPE tool  #: create output path mkdir -p $OUTPATH  #: learn bpe codes on the training set (or only use a subset of it) $FASTBPE learnbpe 30000 data/wiki/txt/en.train > $OUTPATH/codes ```  Now **apply BPE** tokenization to train/valid/test files: ``` $FASTBPE applybpe $OUTPATH/train.en data/wiki/txt/en.train $OUTPATH/codes & $FASTBPE applybpe $OUTPATH/valid.en data/wiki/txt/en.valid $OUTPATH/codes & $FASTBPE applybpe $OUTPATH/test.en data/wiki/txt/en.test $OUTPATH/codes & ```  and get the post-BPE vocabulary: ``` cat $OUTPATH/train.en | $FASTBPE getvocab - > $OUTPATH/vocab & ```  **Binarize the data** to limit the size of the data we load in memory: ``` #: This will create three files: $OUTPATH/{train valid test}.en.pth #: After that we're all set python preprocess.py $OUTPATH/vocab $OUTPATH/train.en & python preprocess.py $OUTPATH/vocab $OUTPATH/valid.en & python preprocess.py $OUTPATH/vocab $OUTPATH/test.en & ```   Install the python package in editable mode with ```bash pip install -e . ```   You can now use the pretrained model for cross-lingual classification. To download a model trained with the command above on the MLM-TLM objective  run:   Before running the scripts below  make sure you download the tokenizers from the [tools/](https://github.com/facebookresearch/XLM/tree/master/tools) directory.   """;Natural Language Processing;https://github.com/facebookresearch/XLM
"""A DCGAN built on the MNIST dataset using pytorch   """;Computer Vision;https://github.com/Ksuryateja/DCGAN-MNIST-pytorch
"""Follow the steps in the installation documentation in [doc/installation.md](doc/installation.md).     """;General;https://github.com/jreisam/Unity-OpenPose-Edutable
"""``` $ python train.py --depth 20 --use_cutmix --outdir results ```    """;Computer Vision;https://github.com/hysts/pytorch_cutmix
"""""";Computer Vision;https://github.com/hongyi-zhang/mixup
"""""";Reinforcement Learning;https://github.com/tarod13/SAC
"""python test_new.py -i <input_directory>   """;General;https://github.com/anujtyagi2802/SRGAN
"""python test_new.py -i <input_directory>   """;Computer Vision;https://github.com/anujtyagi2802/SRGAN
"""This work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593)  which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.  Point cloud is an important type of geometric data structure. Due to its irregular format  most researchers transform such data to regular 3D voxel grids or collections of images. This  however  renders data unnecessarily voluminous and causes issues. In this paper  we design a novel type of neural network that directly consumes point clouds  which well respects the permutation invariance of points in the input.  Our network  named PointNet  provides a unified architecture for applications ranging from object classification  part segmentation  to scene semantic parsing. Though simple  PointNet is highly efficient and effective.  In this repository  we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes  as well as for training a part segmentation network on ShapeNet Part dataset.   Install <a href=""https://www.tensorflow.org/get_started/os_setup"" target=""_blank"">TensorFlow</a>. You may also need to install h5py. The code has been tested with Python 2.7  TensorFlow 1.0.1  CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04.  If you are using PyTorch  you can find a third-party pytorch implementation <a href=""https://github.com/fxia22/pointnet.pytorch"" target=""_blank"">here</a>.  To install h5py for Python: ```bash sudo apt-get install libhdf5-dev sudo pip install h5py ```   Created by <a href=""http://charlesrqi.com"" target=""_blank"">Charles R. Qi</a>  <a href=""http://ai.stanford.edu/~haosu/"" target=""_blank"">Hao Su</a>  <a href=""http://cs.stanford.edu/~kaichun/"" target=""_blank"">Kaichun Mo</a>  <a href=""http://geometry.stanford.edu/member/guibas/"" target=""_blank"">Leonidas J. Guibas</a> from Stanford University.  ![prediction example](https://github.com/charlesq34/pointnet/blob/master/doc/teaser.png)   cd part_seg  sh download_data.sh   To train a model to classify point clouds sampled from 3D shapes:      python train.py  Log files and network parameters will be saved to `log` folder in default. Point clouds of <a href=""http://modelnet.cs.princeton.edu/"" target=""_blank"">ModelNet40</a> models in HDF5 files will be automatically downloaded (416MB) to the data folder. Each point cloud contains 2048 points uniformly sampled from a shape surface. Each cloud is zero-mean and normalized into an unit sphere. There are also text files in `data/modelnet40_ply_hdf5_2048` specifying the ids of shapes in h5 files.  To see HELP for the training script:      python train.py -h  We can use TensorBoard to view the network architecture and monitor the training progress.      tensorboard --logdir log  After the above training  we can evaluate the model and output some visualizations of the error cases.      python evaluate.py --visu  Point clouds that are wrongly classified will be saved to `dump` folder in default. We visualize the point cloud by rendering it into three-view images.  If you'd like to prepare your own data  you can refer to some helper functions in `utils/data_prep_util.py` for saving and loading HDF5 files.   * <a href=""http://stanford.edu/~rqi/pointnet2/"" target=""_blank"">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a> by Qi et al. (NIPS 2017) A hierarchical feature learning framework on point clouds. The PointNet++ architecture applies PointNet recursively on a nested partitioning of the input point set. It also proposes novel layers for point clouds with non-uniform densities. * <a href=""http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w13/Engelmann_Exploring_Spatial_Context_ICCV_2017_paper.pdf"" target=""_blank"">Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</a> by Engelmann et al. (ICCV 2017 workshop). This work extends PointNet for large-scale scene segmentation. * <a href=""https://arxiv.org/abs/1710.04954"" target=""_blank"">PCPNET: Learning Local Shape Properties from Raw Point Clouds</a> by Guerrero et al. (arXiv). The work adapts PointNet for local geometric properties (e.g. normal and curvature) estimation in noisy point clouds. * <a href=""https://arxiv.org/abs/1711.06396"" target=""_blank"">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a> by Zhou et al. from Apple (arXiv) This work studies 3D object detection using LiDAR point clouds. It splits space into voxels  use PointNet to learn local voxel features and then use 3D CNN for region proposal  object classification and 3D bounding box estimation. * <a href=""https://arxiv.org/abs/1711.08488"" target=""_blank"">Frustum PointNets for 3D Object Detection from RGB-D Data</a> by Qi et al. (arXiv) A novel framework for 3D object detection with RGB-D data. The method proposed has achieved first place on KITTI 3D object detection benchmark on all categories (last checked on 11/30/2017).  """;Computer Vision;https://github.com/zgx0534/pointnet_win
"""Run `python main.py --help` for full detail.  Example: ``` python main.py --batch_size 128 --imsize 64 --dataset mura --adv_loss inverse --version sabigan_wrist --image_path ~/datasets/ --use_tensorboard true --mura_class XR_WRIST --mura_type negative ```   """;General;https://github.com/pavasgdb/Anomaly-detector-using-GAN
"""""";General;https://github.com/varshaneya/Res-SE-Net
"""""";Computer Vision;https://github.com/varshaneya/Res-SE-Net
"""Install TensorFlow version >= 1.13 for both GCE VM and Cloud.        """;Computer Vision;https://github.com/mingxingtan/efficientnet
"""Install TensorFlow version >= 1.13 for both GCE VM and Cloud.        """;General;https://github.com/mingxingtan/efficientnet
"""This repository provides code for reproducing StyleGAN truncation sweep and realism score experiments. This code was tested with Python 3.6  Tensorflow 1.12 and NVIDIA V100 GPU.  To run the below code examples  you need to obtain the FFHQ dataset in TFRecords format. You can download it from [Flickr-Faces-HQ repository](http://stylegan.xyz/ffhq).   """;Computer Vision;https://github.com/kynkaat/improved-precision-and-recall-metric
"""1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   Requirements: hardware  Basic installation   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   To train and test a Faster R-CNN detector using the **alternating optimization** algorithm from our NIPS 2015 paper  use `experiments/scripts/faster_rcnn_alt_opt.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  (""alt opt"" refers to the alternating optimization training algorithm described in the NIPS paper.)  To train and test a Faster R-CNN detector using the **approximate joint training** method  use `experiments/scripts/faster_rcnn_end2end.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  This method trains the RPN module jointly with the Fast R-CNN network  rather than alternating between training the two. It results in faster (~ 1.5x speedup) training times and similar detection accuracy. See these [slides](https://www.dropbox.com/s/xtr4yd4i5e0vw8g/iccv15_tutorial_training_rbg.pdf?dl=0) for more details.  Artifacts generated by the scripts in `tools` are written in this directory.  Trained Fast R-CNN networks are saved under:  ``` output/<experiment directory>/<dataset name>/ ```  Test outputs are saved under:  ``` output/<experiment directory>/<dataset name>/<network snapshot name>/ ```  """;Computer Vision;https://github.com/rickyHong/py-faster-rcnn-repl
"""For now on  let's suppose the following paths:   - The directory where images have been downloaded:  /opt/openimages/[train validation test]  - The directory where darknet has been cloned: /opt/darknet/   $ cd /opt/  $ git clone https://github.com/AlexeyAB/darknet  $ cd /opt/darknet  $ make  Note: Edit the Makefile to enable GPU and Cuda support.   After compile darknet  go to the working directory ${DARKNET_FOLDER}/darknet/build/darknet/x64  and build the following directory:   $ ${DARKNET_FOLDER}/darknet/build/darknet/x64   """;Computer Vision;https://github.com/rocapal/fish_detection
"""""";Computer Vision;https://github.com/daixiangzi/ImprovedGan-pytorch
"""""";General;https://github.com/daixiangzi/ImprovedGan-pytorch
"""""";General;https://github.com/ForrestPi/GHM_Loss
"""Using batch normalization. ``` python main.py --epochs=300 --steps_per_epoch=600 --bn=True ``` Without batch normalization. ``` python main.py --epochs=300 --steps_per_epoch=600 --bn=False ``` Tensorboard to visualize ``` tensorboard --logdir=logs ```   """;Computer Vision;https://github.com/minoring/batch-norm-visualize
"""Using batch normalization. ``` python main.py --epochs=300 --steps_per_epoch=600 --bn=True ``` Without batch normalization. ``` python main.py --epochs=300 --steps_per_epoch=600 --bn=False ``` Tensorboard to visualize ``` tensorboard --logdir=logs ```   """;General;https://github.com/minoring/batch-norm-visualize
"""To get help with issues you may encounter while using the DeepLab Tensorflow implementation  create a new question on [StackOverflow](https://stackoverflow.com/) with the tags ""tensorflow"" and ""deeplab"".  Please report bugs (i.e.  broken code  not usage questions) to the tensorflow/models GitHub [issue tracker](https://github.com/tensorflow/models/issues)  prefixing the issue name with ""deeplab"".   """;Computer Vision;https://github.com/AutomatedAI/deeplab_segmentation_example
"""To get help with issues you may encounter while using the DeepLab Tensorflow implementation  create a new question on [StackOverflow](https://stackoverflow.com/) with the tags ""tensorflow"" and ""deeplab"".  Please report bugs (i.e.  broken code  not usage questions) to the tensorflow/models GitHub [issue tracker](https://github.com/tensorflow/models/issues)  prefixing the issue name with ""deeplab"".   """;General;https://github.com/AutomatedAI/deeplab_segmentation_example
"""The code depends on Torch http://torch.ch. Follow instructions [here](http://torch.ch/docs/getting-started.html) and run:  ``` luarocks install torchnet luarocks install optnet luarocks install iterm ```  For visualizing training curves we used ipython notebook with pandas and bokeh.   Download (263MB): https://yadi.sk/d/-8AWymOPyVZns   pre-activation ResNet (from https://github.com/KaimingHe/resnet-1k-layers)   """;Computer Vision;https://github.com/szagoruyko/wide-residual-networks
"""To install the library you need to clone the repository      git clone https://github.com/kpot/keras-transformer.git  then switch to the cloned directory and run pip      cd keras-transformer     pip install .  Please note that the project requires Python >= 3.6.       name='transformer'        name='coordinate_embedding')   you can build your version of Transformer  by re-arranging them   This repository contains simple [examples](./example) showing how Keras-transformer works. It's not a rigorous evaluation of the model's capabilities  but rather a demonstration on how to use the code.  The code trains [simple language-modeling networks](./example/models.py) on the [WikiText-2](https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset) dataset and evaluates their perplexity. The model is either a [vanilla Transformer][1]  or an [Adaptive Universal Transformer][2] (by default) with five layers  each can be trained using either:  * [Generative pre-training][4] (GPT)  which involves using masked self-attention   to prevent the model from ""looking into the future"". * [BERT][3]  which doesn't restrict self-attention  allowing the model   to fill the gaps using both left and right context.   To launch the code  you will first need to install the requirements listed in [example/requirements.txt](./example/requirements.txt). Assuming you work from a Python virtual environment  you can do this by running      pip install -r example/requirements.txt  You will also need to make sure you have a backend for Keras. For instance  you can install Tensorflow (the sample was tested using Tensorflow and PlaidML as backends):      pip install tensorflow  Now you can launch the GPT example as      python -m example.run_gpt --save lm_model.h5  to see all command line options and their default values  try      python -m example.run_gpt --help  If all goes well  after launching the example you should see the perplexity falling with each epoch.      Building vocabulary: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36718/36718 [00:04<00:00  7642.33it/s]     Learning BPE...Done     Building BPE vocabulary: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36718/36718 [00:06<00:00  5743.74it/s]     Train on 9414 samples  validate on 957 samples     Epoch 1/50     9414/9414 [==============================] - 76s 8ms/step - loss: 7.0847 - perplexity: 1044.2455         - val_loss: 6.3167 - val_perplexity: 406.5031     ...  After 200 epochs (~5 hours) of training on GeForce 1080 Ti  I've got validation perplexity about 51.61 and test perplexity 50.82. The score can be further improved  but that is not the point of this demo.  BERT model example can be launched similarly      python -m example.run_bert --save lm_model.h5 --model vanilla  but you will need to be patient. BERT easily achieves better performance than GPT  but requires much more training time to converge.  [1]: https://arxiv.org/abs/1706.03762 ""Attention Is All You Need"" [2]: https://arxiv.org/abs/1807.03819 ""Universal Transformers"" [3]: https://arxiv.org/abs/1810.04805 ""BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"" [4]: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf      ""Improving Language Understanding by Generative Pre-Training""  """;General;https://github.com/kpot/keras-transformer
"""The last snapshot (network-snapshot-005000) can be directly downloaded from the following links for:   -   Acquire the data  e.g. as a snapshot called `256x256.zip` in [another of my repositories](https://github.com/woctezuma/download-steam-banners-data)  -   Run [`StyleGAN2_training.ipynb`][StyleGAN2_training] to train a StyleGAN2 model from scratch  [![Open In Colab][colab-badge]][StyleGAN2_training] -   Run [`StyleGAN2_image_sampling.ipynb`][StyleGAN2_image_sampling] to generate images with a trained StyleGAN2 model  [![Open In Colab][colab-badge]][StyleGAN2_image_sampling] -   To automatically resume training from the latest checkpoint  you will have to use [my fork](https://github.com/woctezuma/stylegan2/tree/google-colab) of StyleGAN2.  For a more thorough analysis of the trained model: -   Run [`StyleGAN2_metrics.ipynb`][StyleGAN2_metrics] to compute metrics (FID  PPL  etc.)  [![Open In Colab][colab-badge]][StyleGAN2_metrics] -   Run [`StyleGAN2_latent_discovery.ipynb`][StyleGAN2_latent_discovery] to discover meaningful latent directions  [![Open In Colab][colab-badge]][StyleGAN2_latent_discovery] -   Run [`Ganspace_colab_for_steam.ipynb`][Ganspace_colab_for_steam] to specifically use GANSpace for discovery  [![Open In Colab][colab-badge]][Ganspace_colab_for_steam]   """;Computer Vision;https://github.com/woctezuma/steam-stylegan2
"""Python 3.7.9   This codebase was designed for a Masters Course at Leiden University  we utilized the code to create visualizations of the learned MDP model within MuZero.  We did this exclusively for MountainCar  the visualization tool can be viewed here: https://kaesve.nl/projects/muzero-model-inspector/#/; an example illustration of this is shown below. This figure illustrates the entire state-space from the MountainCar being embedded by MuZero's encoding network projected to the 3-PC space of the embedding's neural activation values.   ![example](publish/figures/MC_MDP_l8_illustration.png)  We quantified the efficacy of our MuZero and AlphaZero implementations also on the CartPole environment over numerous hyperparameters.  The canonical MuZero can be quite unstable depending on the hyperparameters  the figure shows this through median and mean training rewards over 8 training runs.  ![example2](publish/figures/CP_NumericalResults.png)  The figure below illustrates the efficacy of learned models on MountainCar  when we only provide the MuZero agent observations every n'th environment step along with the agent's learning progress with dense observations.  ![example3](publish/figures/MC_NumericalResultsCombinedUpdated.png)  No boardgames were tested for MuZero as computation time quickly became an issue for us  even on smaller boardsizes. We did find that AlphaZero could learn good policies on boardgames  we found that it depends on the observation encoding.  Heuristic encoding as used in AlphaZero seemed less effective to the canonicalBoard representation used in AlphaZero-General.  Our paper can be read for more details here: [arxiv:2102.12924](https://arxiv.org/abs/2102.12924).    """;Reinforcement Learning;https://github.com/kaesve/muzero
"""This project help you can use PyramidNet_SakeDrop model just like how you use the PyramidNet model.   You only need join the model folder into your project and use the PyramidNet_SakeDrop model with the simple following codes:   ``` python from model import PyramidNet_ShakeDrop net = PyramidNet_ShakeDrop(depth=101 alpha=270 num_classes=1000) ```  """;General;https://github.com/Dragonsson/SakeDrop-Pytorch
"""""";Natural Language Processing;https://github.com/zenanz/ChemPatentEmbeddings
"""""";Computer Vision;https://github.com/davda54/chainer-wideresnet
""":build path to train dir   path_trained_model = os.path.abspath(trainedModel_filename)   """;General;https://github.com/MeAmarP/Fruit-Classification
""":build path to train dir   path_trained_model = os.path.abspath(trainedModel_filename)   """;Computer Vision;https://github.com/MeAmarP/Fruit-Classification
"""To use training/evaluating scripts as well as all models  you need to clone the repository and install dependencies: ``` git clone git@github.com:osmr/imgclsmob.git pip install -r requirements.txt ```   - pytorchcv for PyTorch    - PyTorch models    """;Sequential;https://github.com/osmr/imgclsmob
"""1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/` 2. Download VOC2007 and VOC2012 dataset. By default  we assume the data is stored in `$HOME/data/`   ```Shell   #: Download the data.   cd $HOME/data   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar   #: Extract the data.   tar -xvf VOCtrainval_11-May-2012.tar   tar -xvf VOCtrainval_06-Nov-2007.tar   tar -xvf VOCtest_06-Nov-2007.tar   ```  3. Create the LMDB file.   ```Shell   cd $CAFFE_ROOT   #: Create the trainval.txt  test.txt  and test_name_size.txt in data/VOC0712/   ./data/VOC0712/create_list.sh   #: You can modify the parameters in create_data.sh if needed.   #: It will create lmdb files for trainval and test with encoded original image:   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb   #: and make soft links at examples/VOC0712/   ./data/VOC0712/create_data.sh   ```   git clone https://github.com/imistyrain/ssd  cd ssd   git clone https://github.com/imistyrain/ssd  cd ssd  mkdir build  cd build  make -j4  make install  Note: on Mac  you should pass the PYTHON_LIBRARY by runnning cmake  cmake -DPYTHON_LIBRARY=$(python-config --prefix)/lib/libpython2.7.dylib -DPYTHON_INCLUDE_DIR=$(python-config --prefix)/include/python2.7 ..   cmake -DCMAKE_INSTALL_PREFIX=/usr/local/ -DPYTHON_LIBRARY=$(python3-config --prefix)/lib/libpython3.6.dylib -DPYTHON_INCLUDE_DIR=$(python3-config --prefix)/include/python3.6m ..   COCO<sup>[1]</sup>: SSD300*  SSD512*  07+12+COCO: SSD300*  SSD512*  07++12+COCO: SSD300*  SSD512*  COCO models:   """;Computer Vision;https://github.com/imistyrain/ssd
"""1. Download data (see [Challenge Website](http://medicalood.dkfz.de/web/)) to `./data/original`. 2. Save 2D slices along all axes     ```bash     python mood/utils/preprocessing/save_2D.py -i ./data/original/brain_train/ -o ./data/preprocessed/brain_train/2d_axis_0 -a 0     python mood/utils/preprocessing/save_2D.py -i ./data/original/brain_train/ -o ./data/preprocessed/brain_train/2d_axis_1 -a 1     python mood/utils/preprocessing/save_2D.py -i ./data/original/brain_train/ -o ./data/preprocessed/brain_train/2d_axis_2 -a 2    ...    ``` 3. Optionally  create folds for cross-validation or **use ours folds** (`folds` dir)     ```bash     python mood/utils/preprocessing/create_folds.py -i ./data/original/brain_train/ -o ./folds/brain/train_folds_10.csv -n 10     python mood/utils/preprocessing/create_folds.py -i ./data/original/abdom_train/ -o ./folds/abdom/train_folds_10.csv -n 10    ``` 4. Optionally: create a synthetic dataset for validation     ```bash     python mood/utils/data/create_val_dataset_with_synthetic_anomalies.py \             -i ./data/original/brain_train/ \             -o ./data/preprocessed/brain_train/3d_test \             -m ./data/preprocessed/brain_train/3d_test_masks/ \             --folds_path ./folds/brain/train_folds_10.csv             --fold 0        python mood/utils/data/create_val_dataset_with_synthetic_anomalies.py \             -i ./data/original/abdom_train/ \             -o ./data/preprocessed/abdom_train/3d_test \             -m ./data/preprocessed/abdom_train/3d_test_masks/ \             --folds_path ./folds/abdom/train_folds_10.csv             --fold 0        ```   ```bash pip install -r requirements.txt pip install -e . --user ```                            Installation:                           pip install -r requirements.txt                          pip install -e . --user   See examples of configs for training and inference in `configs` dir.  To train Deep Perceptual Autoencoder (DPA)  run: ```bash python mood/main.py train ./configs/train_example.yaml ```  To inference and evaluate your model on synthetic dataset  run ```bash python mood/main.py inference_evaluate_3d ./configs/inference_3d_example.yaml ```   """;General;https://github.com/ninatu/mood_challenge
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/Satan012/BERT
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/Satan012/BERT
""" you need download pretrained chinese bert model (`chinese_L-12_H-768_A-12.zip`)  1. Download the Bert pretrained model from [Google](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip) and place it into the `/pybert/model/pretrain` directory. 2. `pip install pytorch-pretrained-bert` from [github](https://github.com/huggingface/pytorch-pretrained-BERT). 3. Run `python convert_tf_checkpoint_to_pytorch.py` to transfer the pretrained model(tensorflow version)  into pytorch form . 4. Prepare [ATEC NLP data](https://dc.cloud.alipay.com/index#/topic/data?id=8)  you can modify the `io.data_transformer.py` to adapt your data. 5. Modify configuration information in `pybert/config/basic_config.py`(the path of data ...). 6. Run `python data_join.py`  7. Run `python train_bert_atec_nlp.py`.   """;Natural Language Processing;https://github.com/lonePatient/bert-sentence-similarity-pytorch
"""This repository contains the original modified residual network which modified the classical resnet ""Deep Residual Learning for Image Recognition"" (http://arxiv.org/abs/1512.03385). Original residual block was modified to improve model performance.    """;General;https://github.com/xinkuansong/modified-resnet-acc-0.9638-10.7M-parameters
"""This repository contains the original modified residual network which modified the classical resnet ""Deep Residual Learning for Image Recognition"" (http://arxiv.org/abs/1512.03385). Original residual block was modified to improve model performance.    """;Computer Vision;https://github.com/xinkuansong/modified-resnet-acc-0.9638-10.7M-parameters
"""""";Computer Vision;https://github.com/ekagra-ranjan/AE-CNN
"""This project is developed with Python 3.6. If you are using [miniconda](https://docs.conda.io/en/latest/miniconda.html) or [anaconda](https://anaconda.org/)  you can create an environment:  ```bash conda create -n vlnce python3.6 conda activate vlnce ```  VLN-CE uses [Habitat-Sim](https://github.com/facebookresearch/habitat-sim/tree/v0.1.7) 0.1.7 which can be [built from source](https://github.com/facebookresearch/habitat-sim/tree/v0.1.7#installation) or installed from conda:  ```bash conda install -c aihabitat -c conda-forge habitat-sim=0.1.7 headless ```  Then install [Habitat-Lab](https://github.com/facebookresearch/habitat-lab/tree/v0.1.7):  ```bash git clone --branch v0.1.7 git@github.com:facebookresearch/habitat-lab.git cd habitat-lab #: installs both habitat and habitat_baselines python -m pip install -r requirements.txt python -m pip install -r habitat_baselines/rl/requirements.txt python -m pip install -r habitat_baselines/rl/ddppo/requirements.txt python setup.py develop --all ```  Now you can install VLN-CE:  ```bash git clone git@github.com:jacobkrantz/VLN-CE.git cd VLN-CE python -m pip install -r requirements.txt ```   Download: RxR_VLNCE_v0.zip   Both trainers inherit from BaseVLNCETrainer.   TORCH_GPU_ID: 0  #: GPU for pytorch-related code (the model)   """;Reinforcement Learning;https://github.com/jacobkrantz/VLN-CE
"""""";Computer Vision;https://github.com/alejandrodebus/SegNet
"""cd Handwritten-Character-Recognition<br/>   cd Handwritten-Character-Recognition<br/>   """;Computer Vision;https://github.com/Shantanu48114860/Handwritten-Character-Recognition
"""cd Handwritten-Character-Recognition<br/>   cd Handwritten-Character-Recognition<br/>   """;General;https://github.com/Shantanu48114860/Handwritten-Character-Recognition
"""1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   Requirements: hardware  Basic installation   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   To train and test a Faster R-CNN detector using the **alternating optimization** algorithm from our NIPS 2015 paper  use `experiments/scripts/faster_rcnn_alt_opt.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  (""alt opt"" refers to the alternating optimization training algorithm described in the NIPS paper.)  To train and test a Faster R-CNN detector using the **approximate joint training** method  use `experiments/scripts/faster_rcnn_end2end.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  This method trains the RPN module jointly with the Fast R-CNN network  rather than alternating between training the two. It results in faster (~ 1.5x speedup) training times and similar detection accuracy. See these [slides](https://www.dropbox.com/s/xtr4yd4i5e0vw8g/iccv15_tutorial_training_rbg.pdf?dl=0) for more details.  Artifacts generated by the scripts in `tools` are written in this directory.  Trained Fast R-CNN networks are saved under:  ``` output/<experiment directory>/<dataset name>/ ```  Test outputs are saved under:  ``` output/<experiment directory>/<dataset name>/<network snapshot name>/ ```  """;Computer Vision;https://github.com/lincaiming/py-faster-rcnn-windows
"""You can train Soft Actor-Critic agent like this example [here](https://github.com/ku2482/soft-actor-critic.pytorch/blob/master/code/main.py).  ``` python code/main.py \ [--env_id str(default HalfCheetah-v2)] \ [--cuda (optional)] \ [--seed int(default 0)] ```  If you want to use n-step rewards and prioritized experience replay  set `multi_step=5` and `per=True` in configs.   """;Reinforcement Learning;https://github.com/ku2482/soft-actor-critic.pytorch
""" **BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.    In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant    See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/thanhlong1997/bert_quora
"""* The original paper of this code is: https://arxiv.org/abs/1603.00748 * The code is mainly based on: https://github.com/carpedm20/NAF-tensorflow/ * Additionally I added the prioritized experience replay: https://arxiv.org/abs/1511.05952 * Using the OpenAI baseline implementation: https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py  Thanks openAI and Kim!    """;Reinforcement Learning;https://github.com/MathPhysSim/PER-NAF
""":beer:  All datasets used in this repository can be found from [face.evoLVe.PyTorch's Data-Zoo](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch#Data-Zoo).  Note:  - Both training and testing dataset are ""Align_112x112"" version.   :pizza:  Create a new python virtual environment by [Anaconda](https://www.anaconda.com/) or just use pip in your python environment and then clone this repository as following.   git clone https://github.com/peteryuX/arcface-tf2.git  cd arcface-tf2  conda env create -f environment.yml  conda activate arcface-tf2  pip install -r requirements.txt   - You can run python ./dataset_checker.py to check if the dataloader work.  Download LFW  Aged30 and CFP-FP datasets  then extract them to /your/path/to/test_dataset. These testing data are already binary files  so it's not necessary to do any preprocessing. The directory structure should be like bellow.   test_dataset: '/your/path/to/test_dataset'   """;General;https://github.com/peteryuX/arcface-tf2
""":beer:  All datasets used in this repository can be found from [face.evoLVe.PyTorch's Data-Zoo](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch#Data-Zoo).  Note:  - Both training and testing dataset are ""Align_112x112"" version.   :pizza:  Create a new python virtual environment by [Anaconda](https://www.anaconda.com/) or just use pip in your python environment and then clone this repository as following.   git clone https://github.com/peteryuX/arcface-tf2.git  cd arcface-tf2  conda env create -f environment.yml  conda activate arcface-tf2  pip install -r requirements.txt   - You can run python ./dataset_checker.py to check if the dataloader work.  Download LFW  Aged30 and CFP-FP datasets  then extract them to /your/path/to/test_dataset. These testing data are already binary files  so it's not necessary to do any preprocessing. The directory structure should be like bellow.   test_dataset: '/your/path/to/test_dataset'   """;Computer Vision;https://github.com/peteryuX/arcface-tf2
"""I decided to use Google Cloud Platform compute with ¬£230.58 free credit for 1 year. [This is a wonderful tutorial](https://course.fast.ai/start_gcp.html) to get started and refer back to.  My notes are [here](https://github.com/datalass1/fastai/issues/18)   My notes are [here](https://github.com/datalass1/fastai/issues/20)   - [Neural Network playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-gauss&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4 2&seed=0.44189&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=regression&initZero=false&hideText=false)  """;Computer Vision;https://github.com/datalass1/fastai
"""Pantheon requires Python 2 while mvfst-rl training requires Python 3.8+. The recommended setup is to explicitly use python2/python3 commands.   conda create -n mvfst-rl python=3.8 -y &amp;&amp; conda activate mvfst-rl  If you have a previous installation and need to re-install from scratch after updating  the code  run the following commands:   conda activate base &amp;&amp; conda env remove -n mvfst-rl  conda create -n mvfst-rl python=3.8 -y &amp;&amp; conda activate mvfst-rl  ./setup.sh --clean   ./setup.sh --inference   The above starts 40 Pantheon instances in parallel that communicate with the torchbeast actors via RPC.   Note the following settings in the above example:     for instance when the machines on the cluster have not been setup with all the libraries    required in test mode.     the joblib launcher is also installed     Note that the launcher name must be prefixed with an underscore to match the config files   (thus alleviating the need for manual cleanup).   """;Reinforcement Learning;https://github.com/facebookresearch/mvfst-rl
"""It is currently distributed as a source only PyTorch extension. So you need a properly set up toolchain and CUDA compilers to install. 1) _Toolchain_ - In conda the `gxx_linux-64` package provides an appropriate toolchain. However there can still be compatbility issues with this depending on system. You can also try with the system toolchain. 2) _CUDA Toolkit_ - The [nVidia CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) is required in addition to drivers to provide needed headers and tools. Get the appropriate version for your Linux distro from nVidia or check for distro specific instructions otherwise.  _It is important your CUDA Toolkit matches the version PyTorch is built for or errors can occur. Currently PyTorch builds for v10.0 and v9.2._   """;General;https://github.com/thomasbrandon/swish-torch
"""[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.   extremeText like fastText can be build as executable using Make (recommended) or/and CMake:  $ git clone https://github.com/mwydmuch/extremeText.git  $ cd extremeText  (optional) $ cmake .  $ make   The easiest way to get extremeText is to use pip.  $ pip install extremetext  Installing on MacOS may require setting MACOSX_DEPLOYMENT_TARGET=10.9 first:   $ pip install extremetext  The latest version of extremeText can be build from sources using pip or alternatively setuptools.  $ git clone https://github.com/mwydmuch/extremeText.git  $ cd extremeText  $ pip install .  (or) $ python setup.py install   Merge with the latest changes from fastText.   Getting the source code   $ wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip   $ cd fastText-0.1.0  $ make   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ mkdir build &amp;&amp; cd build &amp;&amp; cmake ..  $ make &amp;&amp; make install   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ pip install .   You can also quantize a supervised model to reduce its memory usage with the following command:   extremeText adds new options for fastText supervised command:  ``` $ ./extremetext supervised  New losses for multi-label classification:   -loss sigmoid          -loss plt           (Probabilistic Labels Tree)  With the following optional arguments:   General:   -l2                 L2 regularization (default = 0)   -tfidfWeights       calculate TF-IDF weights for words   -wordsWeights       read word weights from file (format: <word>:<weights>)   -weight             document weight prefix (default = __weight__; format: <weight prefix>:<document weight>)   -tag                tags prefix (default = __tag__)  tags are ignored words  that are outputed with prediction   -eosWeight          weight of EOS token (default = 1.0)   -freezeVectors      freeze pretrained word vectors for supervised learning      PLT (Probabilistic Labels Tree):   -treeType           type of PLT: complete  huffman  kmeans (default = kmeans)   -arity              arity of PLT (default = 2)   -maxLeaves          maximum number of leaves (labels) in one internal node of PLT (default = 100)   -kMeansEps          stopping criteria for k-means clustering (default = 0.001)      Ensemble:   -ensemble           size of the ensemble (default = 1)   -bagging            bagging ratio (default = 1.0) ```  extremeText also adds new commands and makes other to work in parallel: ``` $ ./extremetext predict[-prob] <model> <test-data> [<k>] [<th>] [<output>] [<thread>] $ ./extremetext get-prob <model> <input> [<th>] [<output>] [<thread>] ```   You can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.  There is also the master branch that contains all of our most recent work  but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.   This library has two main use cases: word representation learning and text classification. These were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).   """;Natural Language Processing;https://github.com/mwydmuch/extremeText
"""""";Reinforcement Learning;https://github.com/rk1998/robot-sac
""" 1. Head over to [Google Colab](https://colab.research.google.com/). It is recommended that you switch to a GPU notebook as things will usually run a little faster that way. There are instructions for this on the colaboratory site. 2. Download the .ipynb file in this repository 3. Upload that file to Google Colabatory and run from there!  **Note: Google Colaboratory has time limits for their systems  so you may not be able to fully train the Mogrifier LSTM on their system without some special effort.**   Recommended you use Python `3.7`. You will need to make sure that your virtualenv setup is of the correct version of python. We will be using *PyTorch*.  Please see below for executing a virtual environment.  ```shell cd MogrifierLSTM pip3 install virtualenv #: If you didn't install it virtualenv -p $(which python3) ./venv_lstm source ./venv_lstm/bin/activate  #: Install dependencies pip3 install -r requirements.txt  #: View the notebook  #: Deactivate the virtual environment when you are done deactivate ```   To view the notebook  simply run the following command to start an ipython kernel.   : add your virtual environment to jupyter notebook  python -m ipykernel install --user --name=venv_lstm  : port is only needed if you want to work on more than one notebook  jupyter notebook --port=<your_port>   Check the python environment you are using on the top right corner.  If the name of environment doesn't match  change it to your virtual environment in ""Kernel>Change kernel"".   :replace ""run"" with whatever directory you have saved   """;Sequential;https://github.com/RMichaelSwan/MogrifierLSTM
"""""";Reinforcement Learning;https://github.com/gouxiangchen/dueling-DQN-pytorch
"""The `ConvLSTM` module derives from `nn.Module` so it can be used as any other PyTorch module.  The ConvLSTM class supports an arbitrary number of layers. In this case  it can be specified the hidden dimension (that is  the number of channels) and the kernel size of each layer. In the case more layers are present but a single value is provided  this is replicated for all the layers. For example  in the following snippet each of the three layers has a different hidden dimension but the same kernel size.  Example usage: ``` model = ConvLSTM(input_dim=channels                   hidden_dim=[64  64  128]                   kernel_size=(3  3)                   num_layers=3                   batch_first=True                  bias=True                   return_all_layers=False) ```   """;Sequential;https://github.com/ndrplz/ConvLSTM_pytorch
"""Fix torchscript compatibility for PyTorch 1.4  add torchscript support for MixedConv2d using ModuleDict   Activation factory to select best version of activation by name or override one globally   All development and testing has been done in Conda Python 3 environments on Linux x86-64 systems  specifically Python 3.6.x  3.7.x  3.8.x.  Users have reported that a Python 3 Anaconda install in Windows works. I have not verified this myself.  PyTorch versions 1.4  1.5  1.6 have been tested with this code.  I've tried to keep the dependencies minimal  the setup is as per the PyTorch default install instructions for Conda:  conda create -n torch-env  conda activate torch-env  conda install -c pytorch pytorch torchvision cudatoolkit=10.2  Models can be accessed via the PyTorch Hub API   This package can be installed via pip.  Install (after conda env/install):  pip install geffnet   """;Computer Vision;https://github.com/rwightman/gen-efficientnet-pytorch
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    For the following examples to work  you need to first install DetectionHub.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `DetectionHub` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/Miracle1991/DetectionHub
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    For the following examples to work  you need to first install DetectionHub.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `DetectionHub` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;General;https://github.com/Miracle1991/DetectionHub
"""For example  the above gif depicts multiple customers filling the self-checkout line with what seems to be just one employee.  spotti will recognize that at least one more employee is needed to efficiently uphold the multiple tasks of a self-checkout employee. Once this has been recognized  then spotti will be configured to notify any personnel who is available to fill in the position.    """;Computer Vision;https://github.com/cmf3673/spotti
"""| provenance    | https://github.com/onnx/models/tree/master/squeezenet |   """;Computer Vision;https://github.com/modelhub-ai/squeezenet
"""python 3.8   """;Reinforcement Learning;https://github.com/eddynelson/dqn
"""""";Computer Vision;https://github.com/GiantPandaCV/yolov3-point
"""This project was designed for: * Python 3.6 * TensorFlow 1.12.0  Please install requirements & project: ``` $ cd /path/to/project/ $ git clone https://github.com/filippogiruzzi/deep_q_learning.git $ cd deep_q_learning/ $ pip3 install -r requirements.txt $ pip3 install -e . --user --upgrade ```   Installation    The project deep_q_learning/ has the following structure:   ``` $ cd /path/to/project/deep_q_learning/dqn/ ```   """;Reinforcement Learning;https://github.com/filippogiruzzi/deep_q_learning
"""Adds a short description for the log files.    To run the image simply run this command in the terminal. (Replace `image:version` with your imageId. You can also get this id by running the command `docker images`)  It creates a container and start the jupyter notebook server which you can access at http://localhost:8888 ``` docker run -it --init --rm \ 	-p 8888:8888 \ 	--runtime=nvidia \ 	--name=absa \ 	--volume=$(pwd):/app \ 	image:version ```  For the windows command line (not powershell)  use `%cd%` instead to mount the current directory as a volume so that the run command looks like this:  ``` docker run -it --init --rm \ 	-p 8888:8888 \ 	--runtime=nvidia \ 	--name=absa \ 	--volume=%cd%:/app \ 	image:version ```   To build the Docker image  navigate to the repository folder which contains the `Dockerfile`.  Next  run the Docker build command:  ``` docker build . ```  Make sure you include the `.` at the end of the command to specify that Docker should search for the `Dockerfile` in the current directory.  Note down the Docker image id which should have a format like `3624c152fb28`.   I recommend to check out the excellent Annotated Transformer guide from Harvard or the Illustrated Transformer by Jay Alammar. Both are excellent resources on the topic.   The image is based on https://github.com/anibali/docker-pytorch. In order to run it with CUDA support you need to install the latest NVIDIA drivers and libraries as well as CUDA.  You also need to install the NVIDIA Docker runtime which you can find here: https://github.com/NVIDIA/nvidia-docker   There is one caveat regarding jupyter notebooks though. The browser tab on your local machine has to remain open the whole time. Once you lose connection or close the tab  the script does not stop but you will not get any further output.   The command    """;General;https://github.com/felixSchober/ABSA-Transformer
"""Adds a short description for the log files.    To run the image simply run this command in the terminal. (Replace `image:version` with your imageId. You can also get this id by running the command `docker images`)  It creates a container and start the jupyter notebook server which you can access at http://localhost:8888 ``` docker run -it --init --rm \ 	-p 8888:8888 \ 	--runtime=nvidia \ 	--name=absa \ 	--volume=$(pwd):/app \ 	image:version ```  For the windows command line (not powershell)  use `%cd%` instead to mount the current directory as a volume so that the run command looks like this:  ``` docker run -it --init --rm \ 	-p 8888:8888 \ 	--runtime=nvidia \ 	--name=absa \ 	--volume=%cd%:/app \ 	image:version ```   To build the Docker image  navigate to the repository folder which contains the `Dockerfile`.  Next  run the Docker build command:  ``` docker build . ```  Make sure you include the `.` at the end of the command to specify that Docker should search for the `Dockerfile` in the current directory.  Note down the Docker image id which should have a format like `3624c152fb28`.   I recommend to check out the excellent Annotated Transformer guide from Harvard or the Illustrated Transformer by Jay Alammar. Both are excellent resources on the topic.   The image is based on https://github.com/anibali/docker-pytorch. In order to run it with CUDA support you need to install the latest NVIDIA drivers and libraries as well as CUDA.  You also need to install the NVIDIA Docker runtime which you can find here: https://github.com/NVIDIA/nvidia-docker   There is one caveat regarding jupyter notebooks though. The browser tab on your local machine has to remain open the whole time. Once you lose connection or close the tab  the script does not stop but you will not get any further output.   The command    """;Natural Language Processing;https://github.com/felixSchober/ABSA-Transformer
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software and an iOS App developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   git clone https://github.com/ultralytics/yolov3 && cd yolov3   %<i></i> | ultralytics/yolov3 OR-NMS 5:52@416 (`pycocotools`) | darknet     ``` bash  git clone https://github.com/ultralytics/yolov3   git clone https://github.com/cocodataset/cocoapi && cd cocoapi/PythonAPI && make && cd ../.. && cp -r cocoapi/PythonAPI/pycocotools yolov3  cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/long95288/car-logo-detect
"""- Numpy   - Matplotlib  - Scipy   """;Reinforcement Learning;https://github.com/JL321/Proximal-Policy-Optimization
"""- Prepare   ``` pip install visdom dominate ``` - Get datasets (**ÊôÇÈñì„Åå„Åã„Åã„Çã„ÅÆ„ÅßÊ≥®ÊÑè!! Ë©¶„Åô„Å†„Åë„Å™„Çâ„ÇÑ„Çâ„Å™„Åè„Å¶OK„Åß„Åô**)   - „Éá„É¢„Åß„ÅØ ./datasets/tmp_facades „ÅÆË∂ÖÂ∞èË¶èÊ®°„Éá„Éº„Çø‰Ωø„ÅÑ„Åæ„ÅôÔºé(10Êûötrain  10Êûötest) ``` #:bash ./datasets/download_pix2pix_dataset.sh facades #:bash ./datasets/download_pix2pix_dataset.sh cityscapes #:bash ./datasets/download_pix2pix_dataset.sh maps ``` - Demo Train/Test (GPU) ``` bash ./scripts/demo_pix2pix.sh [GPU_ID] bash ./scripts/demo_pan.sh [GPU_ID] ``` - Demo Train/Test (CPU) ``` bash ./scripts/demo_pix2pix_cpu.sh bash ./scripts/demo_pan_cpu.sh ``` - Real Dataset   - Train:TITAN X„ÅßÂçäÊó•Á®ãÂ∫¶   - Test:Êï∞ÂàÜÔºé ``` #:bash ./scripts/map_pix2pix.sh [GPU_ID] #:bash ./scripts/map_pan.sh [GPU_ID] #:bash ./scripts/city_pix2pix.sh [GPU_ID] #:bash ./scripts/city_pan.sh [GPU_ID] #:bash ./scripts/facades_pix2pix.sh [GPU_ID] #:bash ./scripts/facades_pan.sh [GPU_ID] ``` - ./scripts/ „ÅÆ‰∏≠Ë∫´„Çí„Åø„Å¶„ÇÇ„Çâ„Åà„Å∞ÂàÜ„Åã„Çä„Åæ„Åô„ÅåÔºå„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åî„Å®„Å´Â§â„Çè„Çã„ÅÆ„ÅØ(„Åª„Å®„Çì„Å©)‰ª•‰∏ã„Å†„Åë„Åß„ÅôÔºé   - ```--dataroot```: „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂ†¥ÊâÄ     - ```--which_direction```: Â∑¶‚ÜíÂè≥ or Âè≥‚ÜíÂ∑¶     - ```--pan_mergin_m```: PAN„ÅÆpositive margin m     - ```--niter```: Â≠¶ÁøíÁéá‰∏ÄÂÆö„ÅÆ„Ç®„Éù„ÉÉ„ÇØÊï∞Ôºéniter_decay„Å®Âêà„Çè„Åõ„Å¶Á∑è„Ç®„Éù„ÉÉ„ÇØÊï∞ („Åì„ÅÆÂÆüÈ®ì„Åß„ÅØÂÖ®„Å¶200)   - ```--niter_decay```: Â≠¶ÁøíÁéá„ÇíÊ∏õË°∞„Åï„Åõ„Çã„Ç®„Éù„ÉÉ„ÇØÊï∞Ôºéniter„Å®Âêà„Çè„Åõ„Å¶Á∑è„Ç®„Éù„ÉÉ„ÇØÊï∞ („Åì„ÅÆÂÆüÈ®ì„Åß„ÅØÂÖ®„Å¶200)   - ```--batchSize```: „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫ („Åì„ÅÆÂÆüÈ®ì„Åß„ÅØÂÖ®„Å¶4)     """;Computer Vision;https://github.com/DLHacks/pix2pix_PAN
"""- Prepare   ``` pip install visdom dominate ``` - Get datasets (**ÊôÇÈñì„Åå„Åã„Åã„Çã„ÅÆ„ÅßÊ≥®ÊÑè!! Ë©¶„Åô„Å†„Åë„Å™„Çâ„ÇÑ„Çâ„Å™„Åè„Å¶OK„Åß„Åô**)   - „Éá„É¢„Åß„ÅØ ./datasets/tmp_facades „ÅÆË∂ÖÂ∞èË¶èÊ®°„Éá„Éº„Çø‰Ωø„ÅÑ„Åæ„ÅôÔºé(10Êûötrain  10Êûötest) ``` #:bash ./datasets/download_pix2pix_dataset.sh facades #:bash ./datasets/download_pix2pix_dataset.sh cityscapes #:bash ./datasets/download_pix2pix_dataset.sh maps ``` - Demo Train/Test (GPU) ``` bash ./scripts/demo_pix2pix.sh [GPU_ID] bash ./scripts/demo_pan.sh [GPU_ID] ``` - Demo Train/Test (CPU) ``` bash ./scripts/demo_pix2pix_cpu.sh bash ./scripts/demo_pan_cpu.sh ``` - Real Dataset   - Train:TITAN X„ÅßÂçäÊó•Á®ãÂ∫¶   - Test:Êï∞ÂàÜÔºé ``` #:bash ./scripts/map_pix2pix.sh [GPU_ID] #:bash ./scripts/map_pan.sh [GPU_ID] #:bash ./scripts/city_pix2pix.sh [GPU_ID] #:bash ./scripts/city_pan.sh [GPU_ID] #:bash ./scripts/facades_pix2pix.sh [GPU_ID] #:bash ./scripts/facades_pan.sh [GPU_ID] ``` - ./scripts/ „ÅÆ‰∏≠Ë∫´„Çí„Åø„Å¶„ÇÇ„Çâ„Åà„Å∞ÂàÜ„Åã„Çä„Åæ„Åô„ÅåÔºå„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åî„Å®„Å´Â§â„Çè„Çã„ÅÆ„ÅØ(„Åª„Å®„Çì„Å©)‰ª•‰∏ã„Å†„Åë„Åß„ÅôÔºé   - ```--dataroot```: „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂ†¥ÊâÄ     - ```--which_direction```: Â∑¶‚ÜíÂè≥ or Âè≥‚ÜíÂ∑¶     - ```--pan_mergin_m```: PAN„ÅÆpositive margin m     - ```--niter```: Â≠¶ÁøíÁéá‰∏ÄÂÆö„ÅÆ„Ç®„Éù„ÉÉ„ÇØÊï∞Ôºéniter_decay„Å®Âêà„Çè„Åõ„Å¶Á∑è„Ç®„Éù„ÉÉ„ÇØÊï∞ („Åì„ÅÆÂÆüÈ®ì„Åß„ÅØÂÖ®„Å¶200)   - ```--niter_decay```: Â≠¶ÁøíÁéá„ÇíÊ∏õË°∞„Åï„Åõ„Çã„Ç®„Éù„ÉÉ„ÇØÊï∞Ôºéniter„Å®Âêà„Çè„Åõ„Å¶Á∑è„Ç®„Éù„ÉÉ„ÇØÊï∞ („Åì„ÅÆÂÆüÈ®ì„Åß„ÅØÂÖ®„Å¶200)   - ```--batchSize```: „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫ („Åì„ÅÆÂÆüÈ®ì„Åß„ÅØÂÖ®„Å¶4)     """;General;https://github.com/DLHacks/pix2pix_PAN
"""""";Sequential;https://github.com/ra1nty/sru-naive
"""""";Computer Vision;https://github.com/kwcckw/unet_magnetic_tiles_defects
"""You will also need aria2 installed  Install python dependencies via pip install -r requirements.txt   Note that you will need ffmpeg to be installed on your machine to run this script   """;General;https://github.com/EmptySamurai/pytorch-reconet
"""You will also need aria2 installed  Install python dependencies via pip install -r requirements.txt   Note that you will need ffmpeg to be installed on your machine to run this script   """;Computer Vision;https://github.com/EmptySamurai/pytorch-reconet
"""This project consists of a simulation that simulates a partially observable  multi-agent  dynamic  continuous in space  discrete in time and partly unknown (missing knowledge about laws of physics) environment.  There are two actors that can interact consciously with the environment: a cow and wolf.  Additionally  there is another entity called grass.  Each entity has a certain energy level. The cow gets energy by touching grass  the wolf by touching cows. Each entity loses energy by touching its counterpart or moving around. The goal of each actor is to obtain as much energy as possible. If the energy level of the cow or the grass drops below zero the environment is reset. An actor perceives its environment  by sending out rays with a limited reach.  The rays return the color of the actor they intersect with  black if they intersected with the game border or white if they did not intersect with anything. The next figure shows a visualisation of the rays  the cow (brown)  the wolf (blue)  the grass (red) and a visualisation of the rays.  ![figure1](screenshot.png)  The little black circles represent their head. To implement the actors' AI deep Q learning as described in the lecture was used  however it does not achieve wanted results as of yet.    [Control a cart](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)  [Youtube Playlist I got my inspiration from](https://www.youtube.com/watch?v=xukp4MMTTFI&list=PL58qjcU5nk8u4Ajat6ppWVBmS_BCN_T7-&index=1 ""Youtube Playlist Inspiration"")  [Multi-agent actor-critic for mixed cooperative-competitive environments](https://arxiv.org/abs/1706.02275)  [Emergent Tool Use From Multi-Agent Autocurricula](https://arxiv.org/abs/1909.07528)  [When Worlds Collide: Simulating Circle-Circle Collisions](https://gamedevelopment.tutsplus.com/tutorials/when-worlds-collide-simulating-circle-circle-collisions--gamedev-769)  [Quick Tip: Use Quadtrees to Detect Likely Collisions in 2D Space](https://gamedevelopment.tutsplus.com/tutorials/quick-tip-use-quadtrees-to-detect-likely-collisions-in-2d-space--gamedev-374)   in your terminal.   | building environment                                       | 10h                  |  14h      |  | setting up cuda  cudnn... on manjaro                       | 20m                  |  21h      |   In order to run the application install the dependencies and type:  python3 deepcow/run.py play  """;Reinforcement Learning;https://github.com/Stippler/cow-simulator
"""following the pseudocode and implementing with PyTorch.   * environment packages  envs/   Environment packages (envs/) contain:   To train an agent using A3C  come to the root directory   and simply run: ```sh python -m a3c.main ``` which train an agent to solve the game Breakout with random seed 0.   Results will be saved in [res/BreakoutNoFrameskip-v4_a3c_0/](res/BreakoutNoFrameskip-v4_a3c_0) by default.  To evaluate the trained A3C model  run: ```sh python -m a3c.eval ```  More options could be found in arguments (see [main.py](a3c/main.py) and [eval.py](a3c/eval.py)).   """;Reinforcement Learning;https://github.com/liuyuezhang/pyrl
"""following the pseudocode and implementing with PyTorch.   * environment packages  envs/   Environment packages (envs/) contain:   To train an agent using A3C  come to the root directory   and simply run: ```sh python -m a3c.main ``` which train an agent to solve the game Breakout with random seed 0.   Results will be saved in [res/BreakoutNoFrameskip-v4_a3c_0/](res/BreakoutNoFrameskip-v4_a3c_0) by default.  To evaluate the trained A3C model  run: ```sh python -m a3c.eval ```  More options could be found in arguments (see [main.py](a3c/main.py) and [eval.py](a3c/eval.py)).   """;General;https://github.com/liuyuezhang/pyrl
"""""";Reinforcement Learning;https://github.com/bacdavid/HER
"""""";Computer Vision;https://github.com/AlphaKong/CycleGAN
"""""";General;https://github.com/AlphaKong/CycleGAN
"""With the hyper parameters below  it takes 5min54s to train 20 epochs on PTB corpus  the final perplexity on test set    Train with AdaptiveSoftmax: ```shell python train_lm.py --data_path=ptb_data --gpuid=0 --use_adaptive_softmax=1 ``` Train with full softmax: ```shell python train_lm.py --data_path=ptb_data --gpuid=0 --use_adaptive_softmax=0 ```   """;General;https://github.com/yangsaiyong/tf-adaptive-softmax-lstm-lm
"""1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   Requirements: hardware  Basic installation   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   To train and test a Faster R-CNN detector using the **alternating optimization** algorithm from our NIPS 2015 paper  use `experiments/scripts/faster_rcnn_alt_opt.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  (""alt opt"" refers to the alternating optimization training algorithm described in the NIPS paper.)  To train and test a Faster R-CNN detector using the **approximate joint training** method  use `experiments/scripts/faster_rcnn_end2end.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  This method trains the RPN module jointly with the Fast R-CNN network  rather than alternating between training the two. It results in faster (~ 1.5x speedup) training times and similar detection accuracy. See these [slides](https://www.dropbox.com/s/xtr4yd4i5e0vw8g/iccv15_tutorial_training_rbg.pdf?dl=0) for more details.  Artifacts generated by the scripts in `tools` are written in this directory.  Trained Fast R-CNN networks are saved under:  ``` output/<experiment directory>/<dataset name>/ ```  Test outputs are saved under:  ``` output/<experiment directory>/<dataset name>/<network snapshot name>/ ```  """;Computer Vision;https://github.com/cwbabel/faster-cnn
"""""";Computer Vision;https://github.com/Lkruitwagen/remote-sensing-solar-pv
"""Following are the papers:   $ git clone https://github.com/MicroprocessorX069/Comparison-of-DC-GANS-and-SA-GANS.git   """;Computer Vision;https://github.com/MicroprocessorX069/Comparison-of-DC-GANS-and-SA-GANS
"""Following are the papers:   $ git clone https://github.com/MicroprocessorX069/Comparison-of-DC-GANS-and-SA-GANS.git   """;General;https://github.com/MicroprocessorX069/Comparison-of-DC-GANS-and-SA-GANS
"""""";Computer Vision;https://github.com/FLHonker/Losses-in-image-classification-task
"""""";General;https://github.com/FLHonker/Losses-in-image-classification-task
"""""";Computer Vision;https://github.com/WangZesen/Spectral-Normalization-GAN
"""""";General;https://github.com/WangZesen/Spectral-Normalization-GAN
"""Word2vec with numpy.  A neural dependency parser with pytorch.   """;General;https://github.com/fqyshelly/CS224n-Winter2019-assignment-solutions
"""""";Graphs;https://github.com/Yindong-Zhang/myGAT
"""""";Computer Vision;https://github.com/victespinoza/NeuralStyleTransfer
"""""";Reinforcement Learning;https://github.com/fengsterooni/dql
"""""";Computer Vision;https://github.com/LeeGitaek/Detection-Using-Convolutional-Networks
"""""";Computer Vision;https://github.com/KarthikDulam/U-NetCNN
""" First create lmdb datasets:  > python prepare_data.py --out LMDB_PATH --n_worker N_WORKER --size SIZE1 SIZE2 SIZE3 ... DATASET_PATH  This will convert images to jpeg and pre-resizes it. This implementation does not use progressive growing  but you can create multiple resolution datasets using size arguments with comma separated lists  for the cases that you want to try another resolutions later.  Then you can train model in distributed settings  > python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train.py --batch BATCH_SIZE LMDB_PATH  train.py supports Weights & Biases logging. If you want to use it  add --wandb arguments to the script.   """;General;https://github.com/mbbrodie/stylegan2
""" First create lmdb datasets:  > python prepare_data.py --out LMDB_PATH --n_worker N_WORKER --size SIZE1 SIZE2 SIZE3 ... DATASET_PATH  This will convert images to jpeg and pre-resizes it. This implementation does not use progressive growing  but you can create multiple resolution datasets using size arguments with comma separated lists  for the cases that you want to try another resolutions later.  Then you can train model in distributed settings  > python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train.py --batch BATCH_SIZE LMDB_PATH  train.py supports Weights & Biases logging. If you want to use it  add --wandb arguments to the script.   """;Computer Vision;https://github.com/mbbrodie/stylegan2
"""> **UPDATE 24/01/2020:** Thank you for your e-mails asking about _batchboost_. As promised  I will update the results soon and present comparisons with other solutions (paperswithcode.com). This is a draft and research needs to be continued to be complete work  if someone is interested in helping me  please contact.   * A computer running macOS or Linux * For training new models  you'll also need a NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl) * Python version 3.6 * A [PyTorch installation](http://pytorch.org/)           new_inputs  new_targets = new_inputs.cuda()  new_targets.cuda()   """;Computer Vision;https://github.com/maciejczyzewski/batchboost
"""""";General;https://github.com/nbansal90/bAbi_QA
"""Install other requirements for setup ```sh pip install -r requirements.txt ```   Before you start  make sure you have python3.6 installed and  if not follow:  sudo add-apt-repository ppa:jonathonf/python-3.6  sudo apt-get update  sudo apt-get install python3.6  After that create virtual environment with Python3.6  virtualenv --python=python3.6 myvenv  Then Activate virtual environment   source ./myvenv/bin/activate   To get inference from the model  in the ""request.py"" just input the image path for which output is required in the data dictionary then run. ```sh python3 request.py ```   """;General;https://github.com/pradyu1/Defect-Classification
"""Install other requirements for setup ```sh pip install -r requirements.txt ```   Before you start  make sure you have python3.6 installed and  if not follow:  sudo add-apt-repository ppa:jonathonf/python-3.6  sudo apt-get update  sudo apt-get install python3.6  After that create virtual environment with Python3.6  virtualenv --python=python3.6 myvenv  Then Activate virtual environment   source ./myvenv/bin/activate   To get inference from the model  in the ""request.py"" just input the image path for which output is required in the data dictionary then run. ```sh python3 request.py ```   """;Computer Vision;https://github.com/pradyu1/Defect-Classification
"""pip install paddlex -i https://mirror.baidu.com/pypi/simple  !pip install paddlex -i https://mirror.baidu.com/pypi/simple   matplotlib.use('Agg')    def load_one_info(name):       #: Use one slash for paths.   """;Computer Vision;https://github.com/Sharpiless/paddlex-vehicle-detection-with-YOLOv3
"""Pre-trained models can be downloaded from here.   """;General;https://github.com/georgesung/TD3
"""Pre-trained models can be downloaded from here.   """;Reinforcement Learning;https://github.com/georgesung/TD3
"""Install-Package ZeroProximity.MachineLearning   """;Computer Vision;https://github.com/brendankowitz/machine-learning-csharp
"""Extract all of these tars into one directory and rename them  which should have the following basic structure.   $ cd checkpoint  $ wget https://github.com/YunYang1994/tensorflow-yolov3/releases/download/v1.0/yolov3_coco.tar.gz  $ tar -xvf yolov3_coco.tar.gz  $ cd ..   $ cd mAP   1. Clone this file ```bashrc $ git clone https://github.com/YunYang1994/tensorflow-yolov3.git ``` 2.  You are supposed  to install some dependencies before getting out hands with these codes. ```bashrc $ cd tensorflow-yolov3 $ pip install -r ./docs/requirements.txt ``` 3. Exporting loaded COCO weights as TF checkpoint(`yolov3_coco.ckpt`)„Äê[BaiduCloud](https://pan.baidu.com/s/11mwiUy8KotjUVQXqkGGPFQ&shfl=sharepset)„Äë ```bashrc $ cd checkpoint $ wget https://github.com/YunYang1994/tensorflow-yolov3/releases/download/v1.0/yolov3_coco.tar.gz $ tar -xvf yolov3_coco.tar.gz $ cd .. $ python convert_weight.py $ python freeze_graph.py ``` 4. Then you will get some `.pb` files in the root path.   and run the demo script ```bashrc $ python image_demo.py $ python video_demo.py #: if use camera  set video_path = 0 ``` <p align=""center"">     <img width=""100%"" src=""https://user-images.githubusercontent.com/30433053/68088581-9255e700-fe9b-11e9-8672-2672ab398abe.jpg"" style=""max-width:100%;"">     </a> </p>   """;Computer Vision;https://github.com/YunYang1994/tensorflow-yolov3
""" The source code requires PyTorch 0.4.0 (there is known incompatible issue when using PyTorch 0.4.1  haven't tested on PyTorch 1.0). Python 3.5+ is needed (there is known incompatible issue when using Python 2.7).  The full list of arguments can be accessed using `--help`   """;Computer Vision;https://github.com/Yang-YiFan/DiracDeltaNet
"""1. Clone this repository:  `git clone https://github.com/MADONOKOUKI/aaai_ws.git`  2. Install [Pytorch](https://pytorch.org/).  3. ```pip install -r src/requirements.txt```   We have tested our method on [cifar10/100 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html) (The dataset can be download via [pytorch code](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html))   """;General;https://github.com/MADONOKOUKI/Block-wise-Scrambled-Image-Recognition
"""""";General;https://github.com/TommyWongww/MemoryNetworks
"""""";Computer Vision;https://github.com/RAGHAV2998/Everybody-Can-Dance-Now-Video-game-version-
"""""";General;https://github.com/RAGHAV2998/Everybody-Can-Dance-Now-Video-game-version-
"""""";Computer Vision;https://github.com/xiey1/Nucleus_detection
"""Download the ImageNet dataset and put them into the `{repo_root}/data/imagenet`.   """;Computer Vision;https://github.com/JiaminRen/RandWireNN
"""<img src=""demo/24.png"" width=""81%"">   Check [Installation](https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/INSTALL.md) for installation instructions.  ``` bash vis_demo.sh ```  We assume that your symlinked `datasets/totaltext` directory has the following structure:  ``` totaltext |_ test_images |  |_ 0000000.jpg |  |_ ... |  |_ 0000299.jpg |_ annotations |  |_ total_test.json  ```  Model [[Google Drive]](https://drive.google.com/open?id=1JiIhZXYE5VvT7f7BmaBbtDJThOkR36bo)   Totaltext test data  [[Google Drive]](https://drive.google.com/open?id=1Y0fkBy0uy6uFKdlv6IVTZPvERqAoK_j2)  Syntext-150k (Part1: 54 327 [[imgs]](https://universityofadelaide.box.com/s/1jcvu6z9jojmhzojuqrwxvwxmrlw7uib)[[annos]](https://universityofadelaide.box.com/s/zc73pyzvymqkjg3vkb2ayjol7y5a4fsk). Part2: 94 723 [[imgs]](https://universityofadelaide.box.com/s/ibihmhkzpc1zuh56mxyehad1dv1l73ua)[[annos]](https://universityofadelaide.box.com/s/rk55zheij8ubvwgzg7dfjbxgi27l8xld).)   ``` python tools/tests/single_demo_bezier.py ``` <img src=""demo/BezierAlign_result.png"" width=""50%"">   """;Computer Vision;https://github.com/Yuliang-Liu/bezier_curve_text_spotting
"""faster_rcnn_pytorch | Faster RCNN with PyTorch   """;General;https://github.com/busyboxs/Some-resources-useful-for-me
"""""";Computer Vision;https://github.com/nikhilroxtomar/Unet-for-Person-Segmentation
"""""";General;https://github.com/cmu-enyac/Renofeation
"""So you can simply download the reconstructed images from here or here and then update the dataset.   """;General;https://github.com/ShaofengZou/A-CNN-Based-Blind-Denoising-Method
"""So you can simply download the reconstructed images from here or here and then update the dataset.   """;Computer Vision;https://github.com/ShaofengZou/A-CNN-Based-Blind-Denoising-Method
"""""";Natural Language Processing;https://github.com/KzKe/Transformer-models
"""""";Computer Vision;https://github.com/avoroshilov/tf-squeezenet
"""""";Computer Vision;https://github.com/linxi159/FCN-caffe
"""""";General;https://github.com/ArdalanM/nlp-benchmarks
"""""";Natural Language Processing;https://github.com/ArdalanM/nlp-benchmarks
"""I will describe the layout of the dataset. The CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes  with 6000 images per class. There are 50000 training images and 10000 test images.  The dataset is divided into five training batches and one test batch  each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order  but some training batches may contain more images from one class than another. Between them  the training batches contain exactly 5000 images from each class. The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. ""Automobile"" includes sedans  SUVs  things of that sort. ""Truck"" includes only big trucks. Neither includes pickup trucks. The archive contains the files data_batch_1  data_batch_2  ...  data_batch_5  as well as test_batch. For each batch files: ÔÅ¨	data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 color image. The first 1024 entries contain the red channel values  the next 1024 the green  and the final 1024 the blue. The image is stored in row-major order  so that the first 32 entries of the array are the red channel values of the first row of the image. ÔÅ¨	labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data. The dataset contains another file  called batches.meta. It too contains a Python dictionary object. It has the following entries: ÔÅ¨	label_names -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example  label_names[0] == ""airplane""  label_names[1] == ""automobile""  etc.  """;Computer Vision;https://github.com/Xinyi6/CIFAR10-CNN-by-Keras
"""Pytorch:  Python-based scientific computing package   MongoDB 4.4 Manual   Current best practices in single-cell RNA-seq analysis: a tutorial: https://www.embopress.org/doi/10.15252/msb.20188746 https://github.com/theislab/single-cell-tutorial  Orchestrating Single-Cell Analysis with Bioconductor http://osca.bioconductor.org   Complete course on Single-cell RNA-seq data analysis  Univ Cambridge (2018) http://hemberg-lab.github.io/scRNA.seq.course/index.html  Bioinformatics Training channel on YouTube http://goo.gl/uaG8ce  Roscoff single-cell transcriptomics & epigenomics workshop 2019 (slides & scripts) from our french working group on single-cell data analysis: http://goo.gl/m1q1Rs    A step-by-step workflow for low-level analysis of single-cell RNA-seq data https://f1000research.com/articles/5-2122/v2  ‚ÄúAll-in-one‚Äù environments (I): Seurat R toolkit for single-cell genomics https://satijalab.org/seurat/get_started.html  ‚ÄúAll-in-one‚Äù environments (II):  SCANPY: Scanpy ‚Äì Single-Cell Analysis in Python https://scanpy.readthedocs.io/en/latest/index.html   Single-Cell Workshop 2014: RNA-seq  Harvard http://pklab.med.harvard.edu/scw2014/  """;Graphs;https://github.com/RausellLab/Recommended_learning_ressources
"""MLP has following architecture.   `python run_main.py --weight-init <weight initializer> --bias-init <bias initializer> --batch-norm <True or False>`  `<weight initializer>` must be selected in [normal  truncated_normal  xavier  he].</br> `<bias initializer>` must be selected in [normal  zero].  You may command like `python run_main.py --weight-init xavier --bias-init zero --batch-norm True`.   """;Computer Vision;https://github.com/hwalsuklee/tensorflow-mnist-MLP-batch_normalization-weight_initializers
"""MLP has following architecture.   `python run_main.py --weight-init <weight initializer> --bias-init <bias initializer> --batch-norm <True or False>`  `<weight initializer>` must be selected in [normal  truncated_normal  xavier  he].</br> `<bias initializer>` must be selected in [normal  zero].  You may command like `python run_main.py --weight-init xavier --bias-init zero --batch-norm True`.   """;General;https://github.com/hwalsuklee/tensorflow-mnist-MLP-batch_normalization-weight_initializers
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	``` 	 2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```   	 4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. Follow the next sections to download pre-computed object proposals and pre-trained ImageNet models   1. Clone the Fast R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/fast-rcnn.git   ```    2. We'll call the directory that you cloned Fast R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*        **Note 1:** If you didn't clone Fast R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `fast-rcnn` branch (or equivalent detached state). This will happen automatically *if you follow these instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```      4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```      5. Download pre-computed Fast R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_fast_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `fast_rcnn_models`. See `data/README.md` for details.   Requirements: hardware  Basic installation   1. Clone the Fast R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/fast-rcnn.git   ```    2. We'll call the directory that you cloned Fast R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*        **Note 1:** If you didn't clone Fast R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `fast-rcnn` branch (or equivalent detached state). This will happen automatically *if you follow these instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```      4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```      5. Download pre-computed Fast R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_fast_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `fast_rcnn_models`. See `data/README.md` for details.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  **Python**  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007. The object proposals are pre-computed in order to reduce installation requirements.  **Note:** If the demo crashes Caffe because your GPU doesn't have enough memory  try running the demo with a small network  e.g.  `./tools/demo.py --net caffenet` or with `--net vgg_cnn_m_1024`. Or run in CPU mode `./tools/demo.py --cpu`. Type `./tools/demo.py -h` for usage.  **MATLAB**  There's also a *basic* MATLAB demo  though it's missing some minor bells and whistles compared to the Python version. ```Shell cd $FRCN_ROOT/matlab matlab #: wait for matlab to start...  #: At the matlab prompt  run the script: >> fast_rcnn_demo ```  Fast R-CNN training is implemented in Python only  but test-time detection functionality also exists in MATLAB. See `matlab/fast_rcnn_demo.m` and `matlab/fast_rcnn_im_detect.m` for details.  **Computing object proposals**  The demo uses pre-computed selective search proposals computed with [this code](https://github.com/rbgirshick/rcnn/blob/master/selective_search/selective_search_boxes.m). If you'd like to compute proposals on your own images  there are many options. Here are some pointers; if you run into trouble using these resources please direct questions to the respective authors.  1. Selective Search: [original matlab code](http://disi.unitn.it/~uijlings/MyHomepage/index.php#page=projects1)  [python wrapper](https://github.com/sergeyk/selective_search_ijcv_with_python) 2. EdgeBoxes: [matlab code](https://github.com/pdollar/edges) 3. GOP and LPO: [python code](http://www.philkr.net/) 4. MCG: [matlab code](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/) 5. RIGOR: [matlab code](http://cpl.cc.gatech.edu/projects/RIGOR/)  Apologies if I've left your method off this list. Feel free to contact me and ask for it to be included.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	``` 	 2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```   	 4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. Follow the next sections to download pre-computed object proposals and pre-trained ImageNet models   **Train** a Fast R-CNN detector. For example  train a VGG16 network on VOC 2007 trainval:  ```Shell ./tools/train_net.py --gpu 0 --solver models/VGG16/solver.prototxt \ 	--weights data/imagenet_models/VGG16.v2.caffemodel ```  If you see this error  ``` EnvironmentError: MATLAB command 'matlab' not found. Please add 'matlab' to your PATH. ```  then you need to make sure the `matlab` binary is in your `$PATH`. MATLAB is currently required for PASCAL VOC evaluation.  **Test** a Fast R-CNN detector. For example  test the VGG 16 network on VOC 2007 test:  ```Shell ./tools/test_net.py --gpu 1 --def models/VGG16/test.prototxt \ 	--net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel ```  Test output is written underneath `$FRCN_ROOT/output`.  **Compress** a Fast R-CNN model using truncated SVD on the fully-connected layers:  ```Shell ./tools/compress_net.py --def models/VGG16/test.prototxt \ 	--def-svd models/VGG16/compressed/test.prototxt \     --net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel #: Test the model you just compressed ./tools/test_net.py --gpu 0 --def models/VGG16/compressed/test.prototxt \ 	--net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000_svd_fc6_1024_fc7_256.caffemodel ```   """;Computer Vision;https://github.com/qq330488563/TEST
"""- MXNet   """;Computer Vision;https://github.com/UnofficialJuliaMirror/DeepMark-deepmark
"""Instructions for acquiring PTB and WT2 can be found here. While CIFAR-10 can be automatically downloaded by torchvision  ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions here.   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/flymin/darts
""" Currently support tensorflow in   - **ResNeSt**  2d&3d  - **RegNet**  - **DETR** (modified classfication)  - **GENet** (2020 GPU-Efficient Network)   model only  no pertrain model for download (simply not enough free time and resource).   easy to read and modified. welcome for using it  ask question  test it  find some bugs maybe.  ResNeSt based on [offical github](https://github.com/zhanghang1989/ResNeSt) .   for RegNet  cause there are various version  you can easily set it by stage_depth stage_width stage_G.   usage is simple: ``` from models.model_factory import get_model  model_name = 'ResNest50' input_shape = [224 244 3] n_classes = 81 fc_activation = 'softmax' active = 'relu' #: relu or mish  model = get_model(model_name=model_name                    input_shape=input_shape                    n_classes=n_classes                    fc_activation=fc_activation                    active=active'                    verbose=False                   )  model.compile(optimizer='adam'  loss=tf.keras.losses.BinaryCrossentropy()) model.fit(...) ```   - if you add CB_Net in ResNeSt: add `using_cb=True` like: ``` """""" Beware that if using CB_Net  the input height and width should be divisibility of  2 at least 5 times  like input [224 448]:  [224 448]<->[112 224]<->[56 112]<->[28 56]<->[14 28]<->[7 14] correct way: [224 224] downsample->[112 112]  [112 112] upsample->[224 224]  then [224 224]+[224 224]  incorrect way: [223 223] downsample->[112 112]  [112 112] upsample->[224 224]  [223 223] != [224 224] cannt add """"""  model = get_model(... using_cb=True) ``` - DETR experiment model  free to modified the transformer setting. ``` #: ResNest50+CB+transfomer looks powerful! but heavily cost. model_name = 'ResNest50_DETR'   #:res34 not implement using_cb yet  it supporse to be a lighter verison. model_name = 'res34_DETR'   model = get_model(...                    hidden_dim=512                    nheads=8                    num_encoder_layers=6                    num_decoder_layers=6                    n_query_pos=100) ```    """;Computer Vision;https://github.com/QiaoranC/tf_ResNeSt_RegNet_model
"""""";Computer Vision;https://github.com/wayexists02/vqvae-pytorch-impl
"""- https://github.com/bic4907/BiCNet  - https://github.com/0b01/CommNet  - https://github.com/starry-sky6688/StarCraft   - pip install -r requirements.txt - cd MAProj/algo - python ma_main.py --algo maddpg --mode train   """;Reinforcement Learning;https://github.com/isp1tze/MAProj
"""Follow the instructions for installing SpinningUp  you might also need environment libraries such as Gym and MuJoCo.   To run Peng's Q(lambda) with delayed environment (with k=3)  n-step buffer with n=5  run the following   To run n-step with delayed environment (with k=3)  n-step buffer with n=5  run the following   To run Retrace with delayed environment (with k=3)  n-step buffer with n=5  just set lambda=1.0 and run the following   """;Reinforcement Learning;https://github.com/robintyh1/icml2021-pengqlambda
"""""";Graphs;https://github.com/zhengjingwei/cluster_GCN
"""""";Graphs;https://github.com/olety/TIMLinUCB
"""""";General;https://github.com/SeonghoBaek/FrameSequencePrediction
"""""";General;https://github.com/Cherrydomini/Effects-of-feature-dropping-on-COMPAS-with-the-influence-of-LIME-
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/lennonzurich/lalala
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/lennonzurich/lalala
""": Policy 1  make SOA_ONE_OBJECT = Ture  or if you        Make sure the file ""/augmentation_zoo/Myautoaugment_utils.py"" is in project folder.   ```python   from Myautoaugment_utils import AutoAugmenter   #: if you want to use the learned augmentation policy custom or v0-v4(v4 was recommended):   autoaugmenter = AutoAugmenter('v4')   #: or if you want to use some spatial transformation or color distortion data augmentationÔºå   #: add the data augmentation method that you want to use to the policy_test in Myautoaugment_utils.py    #: and set the prob and magnitude. For excample:   #: def policy_vtest():   #:    policy = [   #:        [('Color'  0.0  6)  ('Cutout'  0.6  8)]    #:    ]   #:    return policy   autoaugmenter = AutoAugmenter('test')   #: Input:    #:   Sample = {'img': img  'annot': annots}   #:   img = [H  W  C]  RGB  value between [0 1]   #:   annot = [xmin  ymin  xmax  ymax  label]   #: Return:   #:   Sample = {'img': img  'annot': annots}   Sample = autoaugmenter(Sample)   #: Use in Pytorch   dataset = Dataset(root  transform=transforms.Compose([autoaugmenter]))  ```   """;Computer Vision;https://github.com/zzl-pointcloud/Data_Augmentation_Zoo_for_Object_Detection
"""""";General;https://github.com/nekitmm/StyleGan2
"""""";Computer Vision;https://github.com/nekitmm/StyleGan2
"""I will describe the layout of the dataset. The CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes  with 6000 images per class. There are 50000 training images and 10000 test images.  The dataset is divided into five training batches and one test batch  each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order  but some training batches may contain more images from one class than another. Between them  the training batches contain exactly 5000 images from each class. The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. ""Automobile"" includes sedans  SUVs  things of that sort. ""Truck"" includes only big trucks. Neither includes pickup trucks. The archive contains the files data_batch_1  data_batch_2  ...  data_batch_5  as well as test_batch. For each batch files: ÔÅ¨	data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 color image. The first 1024 entries contain the red channel values  the next 1024 the green  and the final 1024 the blue. The image is stored in row-major order  so that the first 32 entries of the array are the red channel values of the first row of the image. ÔÅ¨	labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data. The dataset contains another file  called batches.meta. It too contains a Python dictionary object. It has the following entries: ÔÅ¨	label_names -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example  label_names[0] == ""airplane""  label_names[1] == ""automobile""  etc.  """;General;https://github.com/Xinyi6/CIFAR10-CNN-by-Keras
"""""";Reinforcement Learning;https://github.com/sainijagjit/A3C-Pytorch
"""""";General;https://github.com/sainijagjit/A3C-Pytorch
"""The code is tested under a Linux desktop with a single GTX-1080 Ti GPU.   can be downloaded via  bash download_dataset.sh   cd DVBPR   cd GAN   cd PM   A quick way to use our model is using pretrained models which can be acquired via:   ``` bash download_pretrained_models.sh  ```  With pretrained models  you can see the AUC results of DVBPR  and run GAN and PM code to generate images.   """;General;https://github.com/kang205/DVBPR
"""The code is tested under a Linux desktop with a single GTX-1080 Ti GPU.   can be downloaded via  bash download_dataset.sh   cd DVBPR   cd GAN   cd PM   A quick way to use our model is using pretrained models which can be acquired via:   ``` bash download_pretrained_models.sh  ```  With pretrained models  you can see the AUC results of DVBPR  and run GAN and PM code to generate images.   """;Computer Vision;https://github.com/kang205/DVBPR
"""Tested in a Python 3.7 - 3.9 conda environment in Linux with: * PyTorch 1.6 - 1.10 * PyTorch Image Models (timm) >= 0.4.12  `pip install timm` or local install from (https://github.com/rwightman/pytorch-image-models) * Apex AMP master (as of 2020-08). I recommend using native PyTorch AMP and DDP now.  *NOTE* - There is a conflict/bug with Numpy 1.18+ and pycocotools 2.0  force install numpy <= 1.17.5 or ensure you install pycocotools >= 2.0.2   Python notebook   Heads can have a different activation from FPN via config   PyTorch >= 1.6 now required   NOTE: Official scores for all modules now using soft-nms  but still using normal NMS here.   find . -name '*.tar' -exec tar xf {} \;   The 500 (Challenge2019) or 601 (V5/V6) class head for OI takes up a LOT more GPU memory vs COCO. You'll likely need to half batch sizes.   * A new dataset interface with dataset support (via parser classes) for COCO  VOC 2007/2012  and OpenImages V5/Challenge2019 * New focal loss def w/ label smoothing available as an option  support for jit of loss fn for (potential) speedup * Improved a few hot spots that squeek out a couple % of throughput gains  higher GPU utilization * Pascal / OpenImages evaluators based on Tensorflow Models Evaluator framework (usable for other datasets as well) * Support for native PyTorch DDP  SyncBN  and AMP in PyTorch >= 1.6. Still defaults to APEX if installed. * Non-square input image sizes are allowed for the model (the anchor layout). Specified by image_size tuple in model config. Currently still restricted to `size % 128 = 0` on each dim. * Allow anchor target generation to be done in either dataloader process' via collate or in model as in past. Can help balance compute. * Filter out unused target cls/box from dataset annotations in fixed size batch tensors before passing to target assigner. Seems to speed convergence. * Letterbox aware Random Erasing augmentation added. * A (very slow) SoftNMS impl added for inference/validation use. It can be manually enabled right now  can add arg if demand. * Tested with PyTorch 1.7 * Add ResDet50 model weights  41.6 mAP.  A few things on priority list I haven't tackled yet: * Mosaic augmentation * bbox IOU loss (tried a bit but so far not a great result  need time to debug/improve)  **NOTE** There are some breaking changes: * Predict and Train benches now output XYXY boxes  NOT XYWH as before. This was done to support other datasets as XYWH is COCO's evaluator requirement. * The TF Models Evaluator operates on YXYX boxes like the models. Conversion from XYXY is currently done by default. Why don't I just keep everything YXYX? Because PyTorch GPU NMS operates in XYXY. * You must update your version of `timm` to the latest (>=0.3)  as some APIs for helpers changed a bit.  Training sanity checks were done on VOC and OI   * 80.0 @ 50 mAP finetune on voc0712 with no attempt to tune params (roughly as per command below)   * 18.0 mAP @ 50 for OI Challenge2019 after couple days of training (only 6 epochs  eek!). It's much bigger  and takes a LOONG time  many classes are quite challenging.    The models here have been used with custom training routines and datasets with great results. There are lots of details to figure out so please don't file any 'I get crap results on my custom dataset issues'. If you can illustrate a reproducible problem on a public  non-proprietary  downloadable dataset  with public github fork of this repo including working dataset/parser implementations  I MAY have time to take a look.  Examples: * Chris Hughes has put together a great example of training w/ `timm` EfficientNetV2 backbones and the latest versions of the EfficientDet models here   * [Medium blog post](https://medium.com/data-science-at-microsoft/training-efficientdet-on-custom-data-with-pytorch-lightning-using-an-efficientnetv2-backbone-1cdf3bd7921f)   * [Python notebook](https://gist.github.com/Chris-hughes10/73628b1d8d6fc7d359b3dcbbbb8869d7) * Alex Shonenkov has a clear and concise Kaggle kernel which illustrates fine-tuning these models for detecting wheat heads: https://www.kaggle.com/shonenkov/training-efficientdet (NOTE: this is out of date wrt to latest versions here  many details have changed)  If you have a good example script or kernel training these models with a different dataset  feel free to notify me for inclusion here...   """;Computer Vision;https://github.com/rwightman/efficientdet-pytorch
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   ``` git clone https://github.com/dvschultz/stylegan2 cd stylegan2 bash ./easy-install.sh ``` This repo adds an easy install script to install dependencies and install pkl files that might hit rate limits from the original repo. It also includes some new image generation functions.   """;General;https://github.com/mhuse/StyleGAN2LatentspaceInterpolation
"""Datasets are stored as multi-resolution TFRecords  similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory  e.g.  `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections  the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments  e.g.  `--dataset=ffhq --data-dir=~/datasets`.  **FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords  run:  ```.bash pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git cd ffhq-dataset python download_ffhq.py --tfrecords popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ```  **LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ```  **Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords  run:  ```.bash python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images python dataset_tool.py display ~/datasets/my-custom-dataset ```   ``` git clone https://github.com/dvschultz/stylegan2 cd stylegan2 bash ./easy-install.sh ``` This repo adds an easy install script to install dependencies and install pkl files that might hit rate limits from the original repo. It also includes some new image generation functions.   """;Computer Vision;https://github.com/mhuse/StyleGAN2LatentspaceInterpolation
"""#: You can also use the dense version of GAT   ``` $ python main.py ```   """;Graphs;https://github.com/marblet/gat-pytorch
"""git clone https://github.com/jonathan-laurent/AlphaZero.jl.git  cd AlphaZero.jl   """;Reinforcement Learning;https://github.com/jonathan-laurent/AlphaZero.jl
"""AutoAugment: https://github.com/DeepVoltaire/AutoAugment   """;General;https://github.com/paddorch/SupContrast.paddle
"""Tensorflow's sequential model is a very intuitive way to start learning about Deep Neural Networks. However it is quite hard to dive into more complex networks without learning more about Keras.  Well it won't be hard anymore with Fast-layers! Define your Sequences and start building complex layers in a sequential fashion.  I created fast-layers for beginners who wants to build more advanced networks and for experimented users who needs to quickly build and test complex module architectures.   Installation: !pip install fast-layers   """;Computer Vision;https://github.com/AlexandreMahdhaoui/fast_layers
"""1. Clone the repository ``` git clone https://github.com/LSSTDESC/DeblenderVAE.git cd DeblenderVAE ``` 2. Install  - with [conda](https://www.anaconda.com/products/individual) or [miniconda](https://docs.conda.io/en/latest/miniconda.html)   - if you don't want to use GPU     ```     conda env create -f ressources/env_TF.yml     conda activate env_vae_tensorflow     ```   - if you want to use GPU     ```     conda env create -f ressources/env_TF_gpu.yml     conda activate env_vae_tensorflow_gpu     ```    The images are generated with GalSim (https://github.com/GalSim-developers/GalSim  doc: http://galsim-developers.github.io/GalSim/_build/html/index.html) from parametric models fitted to real galaxies from the HST COSMOS catalog (which can be found from here: https://github.com/GalSim-developers/GalSim/wiki/RealGalaxy%20Data).   """;Computer Vision;https://github.com/LSSTDESC/DeblenderVAE
"""1. Clone the repository ``` git clone https://github.com/LSSTDESC/DeblenderVAE.git cd DeblenderVAE ``` 2. Install  - with [conda](https://www.anaconda.com/products/individual) or [miniconda](https://docs.conda.io/en/latest/miniconda.html)   - if you don't want to use GPU     ```     conda env create -f ressources/env_TF.yml     conda activate env_vae_tensorflow     ```   - if you want to use GPU     ```     conda env create -f ressources/env_TF_gpu.yml     conda activate env_vae_tensorflow_gpu     ```    The images are generated with GalSim (https://github.com/GalSim-developers/GalSim  doc: http://galsim-developers.github.io/GalSim/_build/html/index.html) from parametric models fitted to real galaxies from the HST COSMOS catalog (which can be found from here: https://github.com/GalSim-developers/GalSim/wiki/RealGalaxy%20Data).   """;General;https://github.com/LSSTDESC/DeblenderVAE
"""""";Reinforcement Learning;https://github.com/shreyesss/PPO-implementation-keras-tensorflow
"""To track the runing time  you can use timeit. pip intall timeit if it has not been installed.   Bonus (optional)   First clone this repo  then install all dependencies ``` pip install -r requirements.txt ```   """;General;https://github.com/xxlya/COS598D_Assignment1
"""""";Computer Vision;https://github.com/BoyuanJackChen/NeRF-Implementation
"""```bash $ pip install siren-pytorch ```   A SIREN based multi-layered neural network  ```python import torch from torch import nn from siren_pytorch import SirenNet  net = SirenNet(     dim_in = 2                         #: input dimension  ex. 2d coor     dim_hidden = 256                   #: hidden dimension     dim_out = 3                        #: output dimension  ex. rgb value     num_layers = 5                     #: number of layers     final_activation = nn.Sigmoid()    #: activation of final layer (nn.Identity() for direct output)     w0_initial = 30.                   #: different signals may require different omega_0 in the first layer - this is a hyperparameter )  coor = torch.randn(1  2) net(coor) #: (1  3) <- rgb value ```  One SIREN layer  ```python import torch from siren_pytorch import Siren  neuron = Siren(     dim_in = 3      dim_out = 256 )  coor = torch.randn(1  3) neuron(coor) #: (1  256) ```  Sine activation (just a wrapper around `torch.sin`)  ```python import torch from siren_pytorch import Sine  act = Sine(1.) coor = torch.randn(1  2) act(coor) ```  Wrapper to train on a specific image of specified height and width from a given `SirenNet`  and then to subsequently generate.   ```python import torch from torch import nn from siren_pytorch import SirenNet  SirenWrapper  net = SirenNet(     dim_in = 2                         #: input dimension  ex. 2d coor     dim_hidden = 256                   #: hidden dimension     dim_out = 3                        #: output dimension  ex. rgb value     num_layers = 5                     #: number of layers     w0_initial = 30.                   #: different signals may require different omega_0 in the first layer - this is a hyperparameter )  wrapper = SirenWrapper(     net      image_width = 256      image_height = 256 )  img = torch.randn(1  3  256  256) loss = wrapper(img) loss.backward()  #: after much training ... #: simply invoke the wrapper without passing in anything  pred_img = wrapper() #: (1  3  256  256) ```   """;General;https://github.com/lucidrains/siren-pytorch
"""    X = X.cuda()     softmax_model.cuda()     softmax_model.eval()     linear_model.cuda()     linear_model.eval()       softmax_model = builder.get()   linear_model = builder.get()   Dependencies & Installation  The fast transformers library has the following dependencies:   pip install --user pytorch-fast-transformers  Note: macOS users should ensure they have llvm and libomp installed.  Using the homebrew &lt;https://brew.sh&gt;_ package manager  this can be  accomplished by running brew install llvm libomp.   """;General;https://github.com/idiap/fast-transformers
"""    X = X.cuda()     softmax_model.cuda()     softmax_model.eval()     linear_model.cuda()     linear_model.eval()       softmax_model = builder.get()   linear_model = builder.get()   Dependencies & Installation  The fast transformers library has the following dependencies:   pip install --user pytorch-fast-transformers  Note: macOS users should ensure they have llvm and libomp installed.  Using the homebrew &lt;https://brew.sh&gt;_ package manager  this can be  accomplished by running brew install llvm libomp.   """;Natural Language Processing;https://github.com/idiap/fast-transformers
"""""";General;https://github.com/JoonHyung-Park/SimCLR
"""Check `README.md` under `data` for more details.  Check  `vlbert_tasks.yml` for more details.     1. Create a fresh conda environment  and install all dependencies.  ```text conda create -n vilbert python=3.6 conda activate vilbert git clone https://github.com/jiasenlu/vilbert_beta cd vilbert_beta pip install -r requirements.txt ```  2. Install pytorch ``` conda install pytorch torchvision cudatoolkit=10.0 -c pytorch ```  3. Install apx  follows https://github.com/NVIDIA/apex  4. compile tools  ``` cd tools/refer make ```  2: To test on held out validation split  use the following command:    """;Natural Language Processing;https://github.com/Mehrab-Tanjim/enforce-reasoning
"""The python notebook lists all the code required for running the model. The code is commented for ease of understanding and also highlights some key points which need to be taken care of while creating the model.<br> The model is supposed to predict the bounding boxes for a digit embedded in the image as well as a confidence score for the digit inside the bounding box.   """;General;https://github.com/saunack/MobileNetv2-SSD
"""""";General;https://github.com/pipilurj/BONAS
"""üëØ Clone this repo:  $ git clone https://github.com/saimj7/People-Counting-in-Real-Time.git   """;Computer Vision;https://github.com/saimj7/People-Counting-in-Real-Time
"""üëØ Clone this repo:  $ git clone https://github.com/saimj7/People-Counting-in-Real-Time.git   """;General;https://github.com/saimj7/People-Counting-in-Real-Time
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`  2. Download VOC2007 and VOC2012 dataset. By default  we assume the data is stored in `$HOME/data/`   ```Shell   #: Download the data.   cd $HOME/data   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar   #: Extract the data.   tar -xvf VOCtrainval_11-May-2012.tar   tar -xvf VOCtrainval_06-Nov-2007.tar   tar -xvf VOCtest_06-Nov-2007.tar   ```  3. Create the LMDB file.   ```Shell   cd $CAFFE_ROOT   #: Create the trainval.txt  test.txt  and test_name_size.txt in data/VOC0712/   ./data/VOC0712/create_list.sh   #: You can modify the parameters in create_data.sh if needed.   #: It will create lmdb files for trainval and test with encoded original image:   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb   #: and make soft links at examples/VOC0712/   ./data/VOC0712/create_data.sh   ```   1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`   ```Shell   git clone https://github.com/weiliu89/caffe.git   cd caffe   git checkout ssd   ```  2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.   ```Shell   #: Modify Makefile.config according to your Caffe installation.   cp Makefile.config.example Makefile.config   make -j8   #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.   make py   make test -j8   #: (Optional)   make runtest -j8   ```   COCO<sup>[1]</sup>: SSD300*  SSD512*  07+12+COCO: SSD300*  SSD512*  07++12+COCO: SSD300*  SSD512*  COCO models:   """;Computer Vision;https://github.com/wj1tr0y/SSD
"""COCO-Stuff 10k/164k   """;Computer Vision;https://github.com/woonhahaha/place-classification
"""- better heuristics for testing the networks  - the addition of Alpha-Beta pruning algorithms which for some of the games also takes into account the depth in the search tree to provide a stronger heuristics and to test the network capacity of generalization.  - the games start from random position when pitting the network  in this case the network is evaluated better  as games tend to be different. Also  in this way we can see how well the network generalize.  - using Dirichlet noise  this repo manages to randomize(to a certain degree) even the games that are generated in self-play  so the games are more unique  and the network tends to learn better.  - the networks used are almost like those in AlphaZero and AlphaGo Zero  with minor tweaks in order to be able to run them on resource-constrained system(systems with limited RAM  GPU memory  computation power  etc.)  - the MCTS is reset after each game  so neither player has any advantage of having a developed tree and moving first. In certain games the second player has an advantage. This advantage combined with a developed tree makes the game easy for the second player. - Othello game is updated in order to take a draw into account.  - this implementation provides means of tracking the progress of the network through the training. This info is provided as the number of games won lost or which resulted in a draw in each epoch against Greedy  Random and Alpha-Beta pruning. However  you can turn this feature off.  - this implementation provide complex neural networks that can be used for every game and are capable of learning any game. The project mentioned above uses very small networks which are unsuitable to learn more complex games  thus not being general enough to be used for all games.   """;Reinforcement Learning;https://github.com/MerceaOtniel/HybridAlpha
"""Install PyTorch and ImageNet dataset following the [official PyTorch ImageNet training code](https://github.com/pytorch/examples/tree/master/imagenet).  This repo aims to be minimal modifications on that code. Check the modifications by: ``` diff main_moco.py <(curl https://raw.githubusercontent.com/pytorch/examples/master/imagenet/main.py) diff main_lincls.py <(curl https://raw.githubusercontent.com/pytorch/examples/master/imagenet/main.py) ```      --pretrained [your checkpoint path]/checkpoint_0199.pth.tar \   """;General;https://github.com/1170500804/MoCo
"""Downloads and prepares dataset in a folder like in a normal setting. ``` guild run prepare ```  _You may get an error saying ""bad zip file"". This means that the limit on downloading the celeba dataset has been reached._   ``` virtualenv venv --python python3.8 source venv/bin/activate pip install -r requirements.txt ```   guild run train   """;Computer Vision;https://github.com/Aiwizo/template-nvae
"""Downloads and prepares dataset in a folder like in a normal setting. ``` guild run prepare ```  _You may get an error saying ""bad zip file"". This means that the limit on downloading the celeba dataset has been reached._   ``` virtualenv venv --python python3.8 source venv/bin/activate pip install -r requirements.txt ```   guild run train   """;General;https://github.com/Aiwizo/template-nvae
"""```shell wget http://images.cocodataset.org/zips/train2017.zip wget http://images.cocodataset.org/zips/val2017.zip wget http://images.cocodataset.org/zips/test2017.zip wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip unzip ${train/val/test}2017.zip mv ${train/val/test}2017  ${mmdetection/data/coco/} ```    The directories should be arranged like this:       >   mmdetection      >     ‚îú‚îÄ‚îÄ mmdet      >     ‚îú‚îÄ‚îÄ tools      >     ‚îú‚îÄ‚îÄ configs      >     ‚îú‚îÄ‚îÄ data      >     ‚îÇ   ‚îú‚îÄ‚îÄ coco      >     ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ annotations      >     ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train2017      >     ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ val2017      >     ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test2017       This implementation is based on [mmdetection](https://github.com/open-mmlab/mmdetection)(v1.1.0+8732ed9).        Please refer to [INSTALL.md](docs/INSTALL.md) for more information.     a. Create a conda virtual environment and activate it.    ```shell    conda create -n mmlab python=3.7 -y    conda activate mmlab    ```     b. Install PyTorch and torchvision following the [official instructions](https://pytorch.org/)  e.g.      ```shell    conda install pytorch==1.4.0 torchvision==0.5.0 cudatoolkit=10.1 -c pytorch    ```    c. Install mmcv        ```shell    pip install mmcv==0.4.3    ```          d. Clone the mmdetection repository.     ```shell    git clone https://github.com/open-mmlab/mmdetection.git    cd mmdetection    git checkout 8732ed9    ```     e. Install build requirements and then install mmdetection.    (We install pycocotools via the github repo instead of pypi because the pypi version is old and not compatible with the latest numpy.)     ```shell    pip install -r requirements/build.txt    pip install ""git+https://github.com/cocodataset/cocoapi.git#:subdirectory=PythonAPI""    pip install -v -e .  #: or ""python setup.py develop""    ```   * lucifer443/SpineNet-Pytorch   git clone https://github.com/yan-roo/SpineNet-Pytorch.git   You also can set the warm-up iterations.   bash tools/dist_train.sh ${CONFIG_FILE} ${GPU_NUM} [optional arguments]   """;Computer Vision;https://github.com/yan-roo/SpineNet-Pytorch
"""Please follow the instructions of py-faster-rcnn [here](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to setup VOC and COCO datasets (Part of COCO is done). The steps involve downloading data and optionally creating soft links in the ``data`` folder. Since faster RCNN does not rely on pre-computed proposals  it is safe to ignore the steps that setup proposals.  If you find it useful  the ``data/cache`` folder created on my side is also shared [here](http://ladoga.graphics.cs.cmu.edu/xinleic/tf-faster-rcnn/cache.tgz).   1. Clone the repository   ```Shell   git clone https://github.com/endernewton/tf-faster-rcnn.git   ```  2. Update your -arch in setup script to match your GPU   ```Shell   cd tf-faster-rcnn/lib   #: Change the GPU architecture (-arch) if necessary   vim setup.py   ```    | GPU model  | Architecture |   | ------------- | ------------- |   | TitanX (Maxwell/Pascal) | sm_52 |   | GTX 960M | sm_50 |   | GTX 1080 (Ti) | sm_61 |   | Grid K520 (AWS g2.2xlarge) | sm_30 |   | Tesla K80 (AWS p2.xlarge) | sm_37 |    **Note**: You are welcome to contribute the settings on your end if you have made the code work properly on other GPUs. Also even if you are only using CPU tensorflow  GPU based code (for NMS) will be used by default  so please set **USE_GPU_NMS False** to get the correct output.   3. Build the Cython modules   ```Shell   make clean   make   cd ..   ```  4. Install the [Python COCO API](https://github.com/pdollar/coco). The code requires the API to access COCO dataset.   ```Shell   cd data   git clone https://github.com/pdollar/coco.git   cd coco/PythonAPI   make   cd ../../..   ```     - Due to the randomness in GPU training with Tensorflow especially for VOC  the best numbers are reported (with 2-3 attempts) here. According to my experience  for COCO you can almost always get a very close number (within ~0.2%) despite the randomness.      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz     tar -xzvf vgg_16_2016_08_28.tar.gz      cd ../..     For Resnet101  you can set up like:      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz     tar -xzvf resnet_v1_101_2016_08_28.tar.gz      cd ../..     ./experiments/scripts/train_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/train_faster_rcnn.sh 1 coco res101     ./experiments/scripts/test_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/test_faster_rcnn.sh 1 coco res101   1. Download pre-trained model   ```Shell   #: Resnet101 for voc pre-trained on 07+12 set   ./data/scripts/fetch_faster_rcnn_models.sh   ```   **Note**: if you cannot download the models through the link  or you want to try more models  you can check out the following solutions and optionally update the downloading script:   - Another server [here](http://xinlei.sp.cs.cmu.edu/xinleic/tf-faster-rcnn/).   - Google drive [here](https://drive.google.com/open?id=0B1_fAEgxdnvJSmF3YUlZcHFqWTQ).  2. Create a folder and a soft link to use the pre-trained model   ```Shell   NET=res101   TRAIN_IMDB=voc_2007_trainval+voc_2012_trainval   mkdir -p output/${NET}/${TRAIN_IMDB}   cd output/${NET}/${TRAIN_IMDB}   ln -s ../../../data/voc_2007_trainval+voc_2012_trainval ./default   cd ../../..   ```  3. Demo for testing on custom images   ```Shell   #: at repository root   GPU_ID=0   CUDA_VISIBLE_DEVICES=${GPU_ID} ./tools/demo.py   ```   **Note**: Resnet101 testing probably requires several gigabytes of memory  so if you encounter memory capacity issues  please install it with CPU support only. Refer to [Issue 25](https://github.com/endernewton/tf-faster-rcnn/issues/25).  4. Test with pre-trained Resnet101 models   ```Shell   GPU_ID=0   ./experiments/scripts/test_faster_rcnn.sh $GPU_ID pascal_voc_0712 res101   ```   **Note**: If you cannot get the reported numbers (79.8 on my side)  then probably the NMS function is compiled improperly  refer to [Issue 5](https://github.com/endernewton/tf-faster-rcnn/issues/5).   """;Computer Vision;https://github.com/AlphaJia/tf-faster-rcnn
"""Now only support Python3.x  pytorch 1.3.  pip3 install -r requirements.txt  cd lib/exts  sh make.sh   | DeepLabv3 | 3x3-Res101 | train | val | 44.13 | 81.42 | 16 | 15W | DeepLabV3 |   cd scripts/seg/cityscapes/  bash run_fs_pspnet_cityscapes_seg.sh train tag   cd scripts/seg/cityscapes/  bash run_fs_pspnet_cityscapes_seg.sh train tag   cd scripts/seg/cityscapes/  bash run_fs_pspnet_cityscapes_seg.sh val tag   - Mask R-CNN   <div align=""center"">  <img src=""demo/openpose/samples/000000319721_vis.png"" width=""500px""/>  <p> Example output of <b>VGG19-OpenPose</b></p>  <img src=""demo/openpose/samples/000000475191_vis.png"" width=""500px""/>  <p> Example output of <b>VGG19-OpenPose</b></p>  </div>   """;Computer Vision;https://github.com/donnyyou/torchcv
"""To run experiments  MushroomRL requires a script file that provides the necessary information for the experiment. Follow the scripts in the ""examples"" folder to have an idea of how an experiment can be run.  For instance  to run a quick experiment with one of the provided example scripts  run:  .. code:: shell      python3 examples/car_on_hill_fqi.py      You can also perform a local editable installation by using:  .. code:: shell      pip install --no-use-pep517 -e .  To install also optional dependencies:  .. code:: shell      pip install --no-use-pep517 -e .[all]     To install the whole set of features  you will need additional packages installed. You can install everything by running:  .. code:: shell      pip3 install mushroom_rl[all]  This will install every dependency of MushroomRL  except MuJoCo and Plots dependencies. For ubuntu>20.04  you may need to install pygame and gym dependencies:  .. code:: shell      sudo apt -y install libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev \                      libsdl1.2-dev libsmpeg-dev libportmidi-dev ffmpeg libswscale-dev \                      libavformat-dev libavcodec-dev swig  To use the ``mujoco-py`` MushroomRL interface you can run the command:  .. code:: shell      pip3 install mushroom_rl[mujoco]  Below is the code that you need to run to install the Plots dependencies:  .. code:: shell      sudo apt -y install python3-pyqt5     pip3 install mushroom_rl[plots]  You might need to install external dependencies first. For more information about mujoco-py installation follow the instructions on the `project page <https://github.com/openai/mujoco-py>`_  To use dm_control MushroomRL interface  install ``dm_control`` following the instruction that can be found `here <https://github.com/deepmind/dm_control>`_   You can do a minimal installation of ``MushroomRL`` with:  .. code:: shell      pip3 install mushroom_rl   to easily use well-known Python libraries for tensor computation (e.g. PyTorch    iGibson Installation   You can also use third party datasets &lt;https://github.com/StanfordVL/iGibson/tree/master/igibson/utils/data_utils/ext_scene&gt;__.   Habitat Installation   Follow the official guide &lt;https://github.com/facebookresearch/habitat-lab/#installation&gt;   Then you can download interactive datasets following   If you need to download other datasets  you can use   When you create a Habitat environment  you need to pass a wrapper name and two     relative to where you launched the python code. See the navigation example below   sudo apt-get install pigz  git clone https://github.com/facebookresearch/Replica-Dataset.git  cd Replica-Dataset  ./download.sh replica-path   """;Reinforcement Learning;https://github.com/MushroomRL/mushroom-rl
"""Welcome to PaddleSeg! PaddleSeg is an end-to-end image segmentation development kit developed based on [PaddlePaddle](https://www.paddlepaddle.org.cn)  which covers a large number of high-quality segmentation models in different directions such as *high-performance* and *lightweight*. With the help of modular design  we provide two application methods: *Configuration Drive* and *API Calling*. So one can conveniently complete the entire image segmentation application from training to deployment through configuration calls or API calls.  *  Run the following command. If you can train normally  you have installed it successfully.  ```shell python train.py --config configs/quick_start/bisenet_optic_disc_512x512_1k.yml ```    Support to construct a customized segmentation framework with *API Calling* method for flexible development.  ```shell pip install paddleseg ```    System Requirements: * PaddlePaddle >= 2.0.0 * Python >= 3.6+  Highly recommend you install the GPU version of PaddlePaddle  due to large overhead of segmentation models  otherwise it could be out of memory while running the models. For more detailed installation tutorials  please refer to the official website of [PaddlePaddle](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/2.0/install/)„ÄÇ    [x] COCO stuff   * [Installation](./docs/install.md) * [Get Started](./docs/whole_process.md) *  Prepare Datasets    * [Preparation of Annotation Data](./docs/data/marker/marker.md)    * [Annotating Tutorial](./docs/data/transform/transform.md)    * [Custom Dataset](./docs/data/custom/data_prepare.md)  *  Custom Development     * [Detailed Configuration File](./docs/design/use/use.md)     * [Create Your Own Model](./docs/design/create/add_new_model.md)     * [PR Tutorial](./docs/pr/pr/pr.md)     * [Model Guideline](./docs/pr/pr/style_cn.md) * [Model Training](/docs/train/train.md) * [Model Evaluation](./docs/evaluation/evaluate/evaluate.md) * [Prediction](./docs/predict/predict.md)  * Model Export     * [Export Inference Model](./docs/model_export.md)     * [Export ONNX Model](./docs/model_export_onnx.md)  *  Model Deploy     * [Paddle Inference (Python)](./docs/deployment/inference/python_inference.md)     * [Paddle Inference (C++)](./docs/deployment/inference/cpp_inference.md)     * [Paddle Lite](./docs/deployment/lite/lite.md)     * [Paddle Serving](./docs/deployment/serving/serving.md)     * [Paddle JS](./docs/deployment/web/web.md)     * [Benchmark](./docs/deployment/inference/infer_benchmark.md)  * Model Compression     * [Quantization](./docs/slim/quant/quant.md)     * [Distillation](./docs/slim/distill/distill.md)     * [Prune](./docs/slim/prune/prune.md)  *  API Tutorial     * [API Documention](./docs/apis/README.md)     * [API Application](./docs/api_example.md) *  Description of Important Modules     * [Data Augmentation](./docs/module/data/data.md)     * [Loss Description](./docs/module/loss/losses_en.md)     * [Tricks](./docs/module/tricks/tricks.md) * Description of Classical Models     * [DeeplabV3](./docs/models/deeplabv3.md)     * [UNet](./docs/models/unet.md)     * [OCRNet](./docs/models/ocrnet.md)     * [Fast-SCNN](./docs/models/fascnn.md) * [Static Graph Version](./docs/static/static.md) * [FAQ](./docs/faq/faq/faq.md)   """;Computer Vision;https://github.com/PaddlePaddle/PaddleSeg
"""Follow the [maskrcnn-benchmark](./OLD_README.md) to install code and set up the dataset. Use config files in ./configs/retina/ for Training and Testing.    """;General;https://github.com/chengyangfu/retinamask
"""Follow the [maskrcnn-benchmark](./OLD_README.md) to install code and set up the dataset. Use config files in ./configs/retina/ for Training and Testing.    """;Computer Vision;https://github.com/chengyangfu/retinamask
"""Welcome to PaddleSeg! PaddleSeg is an end-to-end image segmentation development kit developed based on [PaddlePaddle](https://www.paddlepaddle.org.cn)  which covers a large number of high-quality segmentation models in different directions such as *high-performance* and *lightweight*. With the help of modular design  we provide two application methods: *Configuration Drive* and *API Calling*. So one can conveniently complete the entire image segmentation application from training to deployment through configuration calls or API calls.  *  Run the following command. If you can train normally  you have installed it successfully.  ```shell python train.py --config configs/quick_start/bisenet_optic_disc_512x512_1k.yml ```    Support to construct a customized segmentation framework with *API Calling* method for flexible development.  ```shell pip install paddleseg ```    System Requirements: * PaddlePaddle >= 2.0.0 * Python >= 3.6+  Highly recommend you install the GPU version of PaddlePaddle  due to large overhead of segmentation models  otherwise it could be out of memory while running the models. For more detailed installation tutorials  please refer to the official website of [PaddlePaddle](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/2.0/install/)„ÄÇ    [x] COCO stuff   * [Installation](./docs/install.md) * [Get Started](./docs/whole_process.md) *  Prepare Datasets    * [Preparation of Annotation Data](./docs/data/marker/marker.md)    * [Annotating Tutorial](./docs/data/transform/transform.md)    * [Custom Dataset](./docs/data/custom/data_prepare.md)  *  Custom Development     * [Detailed Configuration File](./docs/design/use/use.md)     * [Create Your Own Model](./docs/design/create/add_new_model.md)     * [PR Tutorial](./docs/pr/pr/pr.md)     * [Model Guideline](./docs/pr/pr/style_cn.md) * [Model Training](/docs/train/train.md) * [Model Evaluation](./docs/evaluation/evaluate/evaluate.md) * [Prediction](./docs/predict/predict.md)  * Model Export     * [Export Inference Model](./docs/model_export.md)     * [Export ONNX Model](./docs/model_export_onnx.md)  *  Model Deploy     * [Paddle Inference (Python)](./docs/deployment/inference/python_inference.md)     * [Paddle Inference (C++)](./docs/deployment/inference/cpp_inference.md)     * [Paddle Lite](./docs/deployment/lite/lite.md)     * [Paddle Serving](./docs/deployment/serving/serving.md)     * [Paddle JS](./docs/deployment/web/web.md)     * [Benchmark](./docs/deployment/inference/infer_benchmark.md)  * Model Compression     * [Quantization](./docs/slim/quant/quant.md)     * [Distillation](./docs/slim/distill/distill.md)     * [Prune](./docs/slim/prune/prune.md)  *  API Tutorial     * [API Documention](./docs/apis/README.md)     * [API Application](./docs/api_example.md) *  Description of Important Modules     * [Data Augmentation](./docs/module/data/data.md)     * [Loss Description](./docs/module/loss/losses_en.md)     * [Tricks](./docs/module/tricks/tricks.md) * Description of Classical Models     * [DeeplabV3](./docs/models/deeplabv3.md)     * [UNet](./docs/models/unet.md)     * [OCRNet](./docs/models/ocrnet.md)     * [Fast-SCNN](./docs/models/fascnn.md) * [Static Graph Version](./docs/static/static.md) * [FAQ](./docs/faq/faq/faq.md)   """;General;https://github.com/PaddlePaddle/PaddleSeg
"""""";Sequential;https://github.com/rogertrullo/pytorch_convlstm
"""To train on the MNIST dataset  run   """;General;https://github.com/arturml/pytorch-wgan-gp
"""To train on the MNIST dataset  run   """;Computer Vision;https://github.com/arturml/pytorch-wgan-gp
"""1. First  install suitable TensorFlow version. See instruction [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md).  2. Install other project dependencies:      `./install_deps.sh`   """;Sequential;https://github.com/sakurusurya2000/RNN
"""Windows 64bit [Exe] / [Jar]    ![Original](https://github.com/Araxeus/PNG-Upscale/blob/main/test/original.png)    |      Original             |  Bicubic Interpolation    |        EDSR               | | ------------------------- |------------------------- |------------------------- | ![Original](https://github.com/Araxeus/PNG-Upscale/blob/main/test/x2/original.png)   |  ![Bicubic](https://github.com/Araxeus/PNG-Upscale/blob/main/test/x2/input(BicubicX2).png) |  ![EDSR](https://github.com/Araxeus/PNG-Upscale/blob/main/test/x2/input(EDSRx2).png) |  |         ESPCN             |       FSRCNN              |        LapSRN             | | ------------------------- | ------------------------- | ------------------------- | ![ESPCN](https://github.com/Araxeus/PNG-Upscale/blob/main/test/x2/input(ESPCNx2).png) | ![FSRCNN](https://github.com/Araxeus/PNG-Upscale/blob/main/test/x2/input(FSRCNNx2).png) |  ![LapSRN](https://github.com/Araxeus/PNG-Upscale/blob/main/test/x2/input(LapSRNx2).png) |  > Bicubic Interpolation is the standart resizing technique used by most editing tools like photoship etc..    |      Original             |  Bicubic Interpolation    |        EDSR               | | ------------------------- | ------------------------- | ------------------------- | ![Original](https://github.com/Araxeus/PNG-Upscale/blob/main/test/x4/original.png)   |  ![Bicubic](https://github.com/Araxeus/PNG-Upscale/blob/main/test/x4/input(BicubicX4).png)|  ![EDSR](https://github.com/Araxeus/PNG-Upscale/blob/main/test/x4/input(EDSRx4).png)|    |        ESPCN             |       FSRCNN              |        LapSRN             | | ------------------------- | ------------------------- | ------------------------- | ![ESPCN](https://github.com/Araxeus/PNG-Upscale/blob/main/test/x4/input(ESPCNx4).png)   |  ![FSRCNN](https://github.com/Araxeus/PNG-Upscale/blob/main/test/x4/input(FSRCNNx4).png)|  ![LapSRN](https://github.com/Araxeus/PNG-Upscale/blob/main/test/x4/input(LapSRNx4).png)|   """;General;https://github.com/Araxeus/PNG-Upscale
"""The pickled data is a dictionary with 4 key/value pairs:  - `'features'` is a 4D array containing raw pixel data of the traffic sign images  (num examples  width  height  channels). - `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id. - `'sizes'` is a list containing tuples  (width  height) representing the original width and height the image. - `'coords'` is a list containing tuples  (x1  y1  x2  y2) representing coordinates of a bounding box around the sign in the image.  **First  we will use `numpy` provide the number of images in each subset  in addition to the image size  and the number of unique classes.** Number of training examples:  34799 Number of testing examples:  12630 Number of validation examples:  4410 Image data shape = (32  32  3) Number of classes = 43  **Then  we used `matplotlib` plot sample images from each subset.**  **And finally  we will use `numpy` to plot a histogram of the count of images in each unique class.** ---   Now  we'll use the testing set to measure the accuracy of the model over unknown examples. We've been able to reach a **Test accuracy of 96.06%**. A remarkable performance.  Now we'll plot the confusion matrix to see where the model actually fails.  We observe some clusters in the confusion matrix above. It turns out that the various speed limits are sometimes misclassified among themselves. Similarly  traffic signs with traingular shape are misclassified among themselves. We can further improve on the model using hierarchical CNNs to first identify broader groups (like speed signs) and then have CNNs to classify finer features (such as the actual speed limit).  ---   Python 3.6.2  TensorFlow 0.12.1 (GPU support)   """;General;https://github.com/rahulsonone1234/Traffic-Sign-Recognition
"""""";Computer Vision;https://github.com/SimonWT/cv-project-2020
"""""";Reinforcement Learning;https://github.com/rybread1/deep-rl-trex
"""<img alt=""corrrespondence maps"" src=""/figures/PredictionExample.png"" width=""25%""/>   """;General;https://github.com/mshaikh2/AttentionHandwritingVerification
"""If you want to test the software without training or any other steps described above  then follow the following steps:   """;General;https://github.com/u7javed/Transformer-Multi-Language-Translator
"""If you want to test the software without training or any other steps described above  then follow the following steps:   """;Natural Language Processing;https://github.com/u7javed/Transformer-Multi-Language-Translator
"""This code is tested on Ubuntu 16.04 with Python 3.5  CUDA 9.2 and Pytorch 1.3.1.  1  Install the following dependencies by either `pip install -r requirements.txt` or manual installation. * numpy * pytorch * tqdm * yaml * Cython * [numba](https://github.com/numba/numba) * [torch-scatter](https://github.com/rusty1s/pytorch_scatter) * [dropblock](https://github.com/miguelvr/dropblock)  2  Download Velodyne point clouds and label data in SemanticKITTI dataset [here](http://www.semantic-kitti.org/dataset.html#overview).  3  Extract everything into the same folder. The folder structure inside the zip files of label data matches the folder structure of the LiDAR point cloud data.  4  Data file structure should look like this:  ``` ./ ‚îú‚îÄ‚îÄ train.py ‚îú‚îÄ‚îÄ ... ‚îî‚îÄ‚îÄ data/     ‚îú‚îÄ‚îÄsequences         ‚îú‚îÄ‚îÄ 00/                    ‚îÇ   ‚îú‚îÄ‚îÄ velodyne/	#: Unzip from KITTI Odometry Benchmark Velodyne point clouds.         |   |	‚îú‚îÄ‚îÄ 000000.bin         |   |	‚îú‚îÄ‚îÄ 000001.bin         |   |	‚îî‚îÄ‚îÄ ...         ‚îÇ   ‚îî‚îÄ‚îÄ labels/ 	#: Unzip from SemanticKITTI label data.         |       ‚îú‚îÄ‚îÄ 000000.label         |       ‚îú‚îÄ‚îÄ 000001.label         |       ‚îî‚îÄ‚îÄ ...         ‚îú‚îÄ‚îÄ ...         ‚îî‚îÄ‚îÄ 21/ 	    ‚îî‚îÄ‚îÄ ... ```  """;Computer Vision;https://github.com/Yvanali/KITTISeg
""": Get the extended-vocab probability distribution   python versionÔºö3.6  pytorch versionÔºö1.3.1   """;General;https://github.com/BugOMan/summary_generator
"""""";Computer Vision;https://github.com/prasadsawant5/PointNet
"""To clone and run this application  you'll need Git and Node.js (which comes with npm) installed on your computer. From your command line:  ``` #: Clone this repository $ git clone https://github.com/Arcady1/Doodle-Recognition-Web.git  #: Go into the repository $ cd Doodle-Recognition-Web  #: Install dependencies $ npm install  #: Run the app $ npm build $ npm start ```  npm dependencies: ``` ""browserify"": ""^16.5.2""  ""@tensorflow/tfjs"": ""^2.0.1""  ""express"": ""^4.17.1""  ""mathjs"": ""^7.1.0"" ```   """;Computer Vision;https://github.com/Arcady1/Doodle-Recognition-Web
"""To clone and run this application  you'll need Git and Node.js (which comes with npm) installed on your computer. From your command line:  ``` #: Clone this repository $ git clone https://github.com/Arcady1/Doodle-Recognition-Web.git  #: Go into the repository $ cd Doodle-Recognition-Web  #: Install dependencies $ npm install  #: Run the app $ npm build $ npm start ```  npm dependencies: ``` ""browserify"": ""^16.5.2""  ""@tensorflow/tfjs"": ""^2.0.1""  ""express"": ""^4.17.1""  ""mathjs"": ""^7.1.0"" ```   """;General;https://github.com/Arcady1/Doodle-Recognition-Web
"""You can set hyperparameters in the `configuration.json` file such as maximum training epoch  type of image  batch size  etc. Here are some descriptions of hyperparameters.  Name | Example | Description :-: | :-: | :-: image_type | ""small"" | The image type of data. <br /> For the original image (439GB)  set as `""""`. pre_trained | false | Whether the model is pre-trained. nnClassCount | 5 | The number of training observations. Set to 5. policy | ""diff"" | If set to `""diff""`  the optimal policy will be applied to each of the 5 competition observations. <br /> If set to `""ones""`  the U-Ones policy will be applied. <br /> If else  the U-Zeroes policy will be applied. batch_size | 16 | The number of batch size. epochs | 3 | The number of training epochs. imgtransResize | 320 | Resizing image in transformation. train_ratio | 1 | Training data ratio. lr | 0.0001 | Learning rate of optimizer. betas | [0.9  0.999] | Betas of optimizer. eps | 1e-08 | Eps of optimizer. weight_decay | 0 | Weight decay of optimizer.   This repository especially referenced here for the coding part. You can easily run the code with the following instructions.  This code is written for the GPU environment. It could be hard to run in CPU environment.   - Python 3.6+  - PyTorch 1.7.1   First  use git clone command to download this repository.  git clone https://github.com/Stomper10/CheXpert.git   """;General;https://github.com/Stomper10/CheXpert
"""You can set hyperparameters in the `configuration.json` file such as maximum training epoch  type of image  batch size  etc. Here are some descriptions of hyperparameters.  Name | Example | Description :-: | :-: | :-: image_type | ""small"" | The image type of data. <br /> For the original image (439GB)  set as `""""`. pre_trained | false | Whether the model is pre-trained. nnClassCount | 5 | The number of training observations. Set to 5. policy | ""diff"" | If set to `""diff""`  the optimal policy will be applied to each of the 5 competition observations. <br /> If set to `""ones""`  the U-Ones policy will be applied. <br /> If else  the U-Zeroes policy will be applied. batch_size | 16 | The number of batch size. epochs | 3 | The number of training epochs. imgtransResize | 320 | Resizing image in transformation. train_ratio | 1 | Training data ratio. lr | 0.0001 | Learning rate of optimizer. betas | [0.9  0.999] | Betas of optimizer. eps | 1e-08 | Eps of optimizer. weight_decay | 0 | Weight decay of optimizer.   This repository especially referenced here for the coding part. You can easily run the code with the following instructions.  This code is written for the GPU environment. It could be hard to run in CPU environment.   - Python 3.6+  - PyTorch 1.7.1   First  use git clone command to download this repository.  git clone https://github.com/Stomper10/CheXpert.git   """;Computer Vision;https://github.com/Stomper10/CheXpert
"""You will first need to launch a Virtual Machine (VM) on Google Cloud. Details about launching the VM can be found at the [Google Cloud Documentation](https://cloud.google.com/compute/docs/instances/create-start-instance).  In order to run training or eval on Cloud TPUs  you must set up the following variables based on your project  zone and GCS bucket appropriately. Please refer to the [Cloud TPU Quickstart](https://cloud.google.com/tpu/docs/quickstart) guide for more details.  ```sh export PROJECT=your_project_name export ZONE=your_project_zone export BUCKET=gs://yourbucket/ export TPU_NAME=t5-tpu export TPU_SIZE=v3-8 export DATA_DIR=""${BUCKET}/your_data_dir"" export MODEL_DIR=""${BUCKET}/your_model_dir"" ```  Please use the following command to create a TPU device in the Cloud VM.  ```sh ctpu up --name=$TPU_NAME --project=$PROJECT --zone=$ZONE --tpu-size=$TPU_SIZE \         --tpu-only   --tf-version=2.1 --noconf ```    To install the T5 package  simply run:  ```sh pip install t5[gcp] ```   You may either use a new or pre-existing `Task`  or you may load examples from a preprocessed TSV file.   GPU Usage   If you are interested fine-tuning our models on a GPU in PyTorch  you should try the HfPyTorchModel API.   Depending on your data source (see above)  you will need to prepare your data appropriately.   After defining MY_PROJECT and MY_BUCKET appropriately  you can build the datast in DataFlow from GCP using the following commands:  pip install tfds-nightly[c4]   python -m tensorflow_datasets.scripts.download_and_prepare \   Make sure your files are accessible to the TPU (i.e.  are in a GCS bucket)  and you should be good to go!   You can also use greedy_decode.gin or sample_decode.gin instead of beam_search.gin in the command above.   You can also use beam_search.gin or greedy_decode.gin instead of sample_decode.gin in the command above.   The easiest way to try out T5 is with a free TPU in our [Colab Tutorial](https://tiny.cc/t5-colab).  Below we provide examples for how to pre-train  fine-tune  evaluate  and decode from a model from the command-line with our codebase. You can use these instructions to reproduce our results  fine-tune one of our released checkpoints with your own data and/or hyperparameters  or pre-train a model from scratch.   If you would like to use GPU instead of TPUs  you can modify the above commands by removing TPU-specific flags (`--tpu`  `--tpu_zone`  `--gcp_project`) and setting the gin params for `mesh_shape` and `mesh_devices` based on your desired setup.  For example  if your machine has access to 6 GPUs and you'd like to do 3-way model parallelism and 2-way data parallelism  the fine-tuning command above would become:  ```sh t5_mesh_transformer  \   --model_dir=""${MODEL_DIR}"" \   --t5_tfds_data_dir=""${DATA_DIR}"" \   --gin_file=""dataset.gin"" \   --gin_param=""utils.run.mesh_shape = 'model:3 batch:2'"" \   --gin_param=""utils.run.mesh_devices = ['gpu:0' 'gpu:1' 'gpu:2' 'gpu:3' 'gpu:4' 'gpu:5']"" \   --gin_param=""MIXTURE_NAME = 'glue_mrpc_v002'"" \   --gin_file=""gs://t5-data/pretrained_models/small/operative_config.gin"" ```  With a single GPU  the command is:  ```sh t5_mesh_transformer  \   --model_dir=""${MODEL_DIR}"" \   --t5_tfds_data_dir=""${DATA_DIR}"" \   --gin_file=""dataset.gin"" \   --gin_param=""utils.run.mesh_shape = 'model:1 batch:1'"" \   --gin_param=""utils.run.mesh_devices = ['gpu:0']"" \   --gin_param=""MIXTURE_NAME = 'glue_mrpc_v002'"" \   --gin_file=""gs://t5-data/pretrained_models/small/operative_config.gin"" ```    """;Natural Language Processing;https://github.com/thecodemasterk/Text-to-Text-transfer-transformers
"""If you want to go somewhere regarding implementation  please skip this part.    YOLOv3 is a light-weight but powerful one-stage object detector  which means it regresses the positions of objects and predict the probability of objects directly from the feature maps of CNN. Typical example of one-state detector will be YOLO and SSD series.On the contrary   two stage detector like R-CNN  Fast R-CNN and Faster R-CNN may include Selective Search  Support Vector Machine (SVM) and Region Proposal Network (RPN) besides CNN. Two-stage detectors will be sightly more accurate but much slower.   YOLOv3 consists of 2 parts: feature extractor and detector. Feature extractor is a Darknet-53 without its fully connected layer  which is originally designed for classification task on ImageNet dataset.    ![darknet](/fig/Darknet.png)   *Darknet-53 architecture(Source: YOLOv3: An Incremental Improvement https://arxiv.org/abs/1804.02767)*  Detector uses multi-scale fused features to predict the position and the class of the corresponding object.   ![yolov3](/fig/yolo.png)*(Source: https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)*  As you can see from the picture above  there are 3 prediction scales in total. For example  if the spatial resolution of an input image is 32N X 32N  the output of the first prediction convolution layer(strides32) will be N X N X (B X (C+5)). B indicates amount of anchors at this scale and C stands for probabilities of different classes. 5 represents 5 different regressions  the  horizontal offset t_x  the vertical offset t_y  resizing factor of the given anchor height t_hand width t_wand objectness score o (whether an object exists in this square of the checkerboard). The second prediction layer will output feature maps of 2N X 2N X (B X (C+5)). And the third prediction output will be much finer  which is 4N X 4N X (B X (C+5).  Reading papers of YOLO  YOLOv2 and YOLOv3  I summarize the loss function of YOLOv3 as follows:   ![](/fig/loss1.PNG) <!-- $$ L_{Localization} = \lambda_1\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}[(t_{x} - t_{\hat{x}})^2 + (t_{y} - t_{\hat{y}})^2] \\L_{Shaping} =\lambda_2\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}[(t_{w} - t_{\hat{w}})^2 + (t_{h} - t_{\hat{h}})^2]\\ L_{objectness-obj} =\lambda_3\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}\log(o_{ij})$$ $$L_{objectness-noobj} =\lambda_4\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}\log(1-o_{ij}) \\L_{class} =\lambda_5\sum_{i=0}^{N^2}\sum_{j=0}^{B}1_{ij}^{obj}\sum_{c\in classes}[p_{\hat{ij}}(c)\log(p_{ij}(c))+ (1-p_{\hat{ij}}(c))\log(1-p_{ij}(c))]) \\ L_{Scale_{1}} = L_{Localization} + L_{Shaping} + L_{objectness-obj} + L_{objectness-noobj} + L_{class} \\ L_{total} = L_{Scale_{1}}+L_{Scale_{2}}+L_{Scale_{3}}$$ -->   This is my implementation of YOLOv3 using TensorFlow 2.0 backend. The main purpose of this project is to get me familiar with deep learning and specific concepts in domain object detection. Two usages are provided: * Object detection based on official pre-trained weights in COCO * Object detection of optic nerve on Indian Diabetic Retinopathy Image Dataset (IDRiD) using fine tuning.  ![nerve](/fig/optics_nerve.png) *Fundus and the corresponding optic nerve*  The following content will be provided in this repo: * Introduction of YOLOv3 * Object detection based on the official pre-trained weights * Object detection - fine tuning on IDRiD       Name | anchors   Following YOLOv2:   [![](http://img.youtube.com/vi/6mWNgng6CfY/0.jpg)](http://www.youtube.com/watch?v=6mWNgng6CfY """")    https://www.youtube.com/watch?v=6mWNgng6CfY&t=3s  ![](/fig/k-means.gif)  What in the red frame line is our ground truth. ![](/fig/detection1.gif)  """;Computer Vision;https://github.com/LEGO999/YOLOV3-TF2
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/TusharMhaske28/Object-Detection
"""""";Computer Vision;https://github.com/jingma-git/NeRF_Pytorch
"""""";Computer Vision;https://github.com/prakhargoyal106/MelanomaClassification
"""You will need to install cmake first (required for dlib  which is used for face alignment). Currently the code only works with CUDA installed (and therefore requires an appropriate GPU) and has been tested on Linux and Windows. For the full set of required Python packages  create a Conda environment from the provided YAML  e.g.  conda create -f pulse.yml  or (Anaconda on Windows):  conda env create -n pulse -f pulse.yml  conda activate pulse   The main file of interest for applying PULSE is `run.py`. A full list of arguments with descriptions can be found in that file; here we describe those relevant to getting started.   """;Computer Vision;https://github.com/eltechno/pulse
"""""";Computer Vision;https://github.com/yukichou/PET
"""LUKE can be installed using [Poetry](https://python-poetry.org/):  ```bash $ poetry install ```  The virtual environment automatically created by Poetry can be activated by `poetry shell`.   examples_allennlp directory.   The experiments were conducted using Python3.6 and PyTorch 1.2.0 installed on a   $ git clone https://github.com/NVIDIA/apex.git  $ cd apex  $ git checkout c3fad1ad120b23055f6630da0b029c8b626db78f   """;Natural Language Processing;https://github.com/studio-ousia/luke
"""The codes are coded in the python 3.x version   """;Computer Vision;https://github.com/Anirudh0707/Roads-and-Building-Segmentation
"""- To install  `cd` into the root directory and type `pip install -e .`  - To interactively view moving to landmark scenario (see others in ./scenarios/): `bin/interactive.py --scenario simple.py`  - Known dependencies: Python (3.5.4)  OpenAI gym (0.10.5)  numpy (1.14.5)  - To use the environments  look at the code for importing them in `make_env.py`.   """;Reinforcement Learning;https://github.com/tkarr21/multiagent-particle-envs
"""**Note : Use Python 3.6 or newer**  """;Computer Vision;https://github.com/sandeepsinghsengar/U-Net
"""""";General;https://github.com/Prachi-Agr/Image-Super-Resolution
"""""";Computer Vision;https://github.com/Prachi-Agr/Image-Super-Resolution
"""""";Computer Vision;https://github.com/iamrehman/chestXrayDiseaseDetection
"""- OneCycleCosine should be used for optimizers which have a 'momentum' parameter - OneCycleCosineAdam should be used for Adam based optimizers which have a 'betas' parameter tuple   """;General;https://github.com/csvance/onecycle-cosine
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/tanmayj000/darknet
"""""";General;https://github.com/waze96/SRGAN
"""""";Computer Vision;https://github.com/waze96/SRGAN
"""**Windows portable version**: Simply download and use the latest version from the [Releases](https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases) section.  Otherwise  check [doc/installation.md](doc/installation.md) for instructions on how to build OpenPose from source.     OS: Ubuntu (14  16)  Windows (8  10)  Mac OSX  Nvidia TX2.   CUDA (Nvidia GPU)  OpenCL (AMD GPU)  and CPU-only (no GPU) versions.   Most users do not need the OpenPose C++/Python API  but can simply use the OpenPose Demo:  - **OpenPose Demo**: To easily process images/video/webcam and display/save the results. See [doc/demo_overview.md](doc/demo_overview.md). E.g.  run OpenPose in a video with: ``` #: Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi ```  - **Calibration toolbox**: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See [doc/modules/calibration_module.md](doc/modules/calibration_module.md).  - **OpenPose C++ API**: If you want to read a specific input  and/or add your custom post-processing function  and/or implement your own display/saving  check the C++ API tutorial on [examples/tutorial_api_cpp/](examples/tutorial_api_cpp/) and [doc/library_introduction.md](doc/library_introduction.md). You can create your custom code on [examples/user_code/](examples/user_code/) and quickly compile it with CMake when compiling the whole OpenPose project. Quickly **add your custom code**: See [examples/user_code/README.md](examples/user_code/README.md) for further details.  - **OpenPose Python API**: Analogously to the C++ API  find the tutorial for the Python API on [examples/tutorial_api_python/](examples/tutorial_api_python/).  - **Adding an extra module**: Check [doc/library_add_new_module.md](./doc/library_add_new_module.md).  - **Standalone face or hand detector**:     - **Face** keypoint detection **without body** keypoint detection: If you want to speed it up (but also reduce amount of detected faces)  check the OpenCV-face-detector approach in [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     - **Use your own face/hand detector**: You can use the hand and/or face keypoint detectors with your own face or hand detectors  rather than using the body detector. E.g.  useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     """;General;https://github.com/lwxGitHub123/openpose
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   GPUs: K80 ($0.14/hr)  T4 ($0.11/hr)  V100 ($0.74/hr) CUDA with Nvidia Apex FP16/32     Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) < highly recommended!! * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Google Colab Notebook](https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb) with quick training  inference and testing examples * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart)  * [A TensorRT Implementation of YOLOv3 and YOLOv4](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp)    """;Computer Vision;https://github.com/adairliulei/yolov3
"""""";Computer Vision;https://github.com/VirajBagal/FMix-Paper-Implementation
"""This FCOS implementation is based on [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark). Therefore the installation is the same as original maskrcnn-benchmark.  Please check [INSTALL.md](INSTALL.md) for installation instructions. You may also want to see the original [README.md](MASKRCNN_README.md) of maskrcnn-benchmark.   For users who only want to use FCOS as an object detector in their projects  they can install it by pip. To do so  run: ``` pip install torch  #: install pytorch if you do not have it pip install git+https://github.com/tianzhi0549/FCOS.git #: run this command line for a demo  fcos https://github.com/tianzhi0549/FCOS/raw/master/demo/images/COCO_val2014_000000000885.jpg ``` Please check out [here](fcos/bin/fcos) for the interface usage.   For your convenience  we provide the following trained models (more models are coming soon).   We update batch normalization for MobileNet based models. If you want to use SyncBN  please install pytorch 1.1 or later.   Note that:   Once the installation is done  you can follow the below steps to run a quick demo.           """;Computer Vision;https://github.com/08173021/FCOS
"""   To get the required setups run    > pip3 install -r requirements.txt    """;General;https://github.com/acholston/SimCLR
"""Clone this repo by running  ``` git@github.com:jason-hu7/PyTorch-YOLOv4.git ```  cd to the directory and install from source  ``` cd PyTorch-YOLOv4 pip install . ```    """;Computer Vision;https://github.com/jason-hu7/PyTorch-YOLOv4
"""""";Computer Vision;https://github.com/Jintao-Huang/YOLOv3_PyTorch
"""""";General;https://github.com/COMP6248-Reproducability-Challenge/NODE_FOR_DL_ON_TABULAR_DATA
"""Run the code with  `python3 main.py`.   """;Reinforcement Learning;https://github.com/RLeike/connect-four
"""""";Reinforcement Learning;https://github.com/FauzaanQureshi/deep-Q-learning
"""As this vovnet-detectron2 is implemented as a [extension form](https://github.com/youngwanLEE/detectron2/tree/vovnet/projects/VoVNet) (detectron2/projects) upon detectron2  you just install [detectron2](https://github.com/facebookresearch/detectron2) following [INSTALL.md](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md).  Prepare for coco dataset following [this instruction](https://github.com/facebookresearch/detectron2/tree/master/datasets).   We train all models using V100 8GPUs.   one should execute:   """;Computer Vision;https://github.com/suvasis/birdnet2cs231n
"""As this vovnet-detectron2 is implemented as a [extension form](https://github.com/youngwanLEE/detectron2/tree/vovnet/projects/VoVNet) (detectron2/projects) upon detectron2  you just install [detectron2](https://github.com/facebookresearch/detectron2) following [INSTALL.md](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md).  Prepare for coco dataset following [this instruction](https://github.com/facebookresearch/detectron2/tree/master/datasets).   We train all models using V100 8GPUs.   one should execute:   """;General;https://github.com/suvasis/birdnet2cs231n
"""""";Computer Vision;https://github.com/nisharaichur/SegNet-Encoder-Decoder-Architecture-for-Image-Segmentation
"""""";Computer Vision;https://github.com/oellop/GAN_MNIST
"""CenterNet is a framework for object detection with deep convolutional neural networks. You can use the code to train and evaluate a network for object detection on the MS-COCO dataset.  * It achieves state-of-the-art performance (an AP of 47.0%) on one of the most challenging dataset: MS-COCO.  * Our code is written in Python  based on [CornerNet](https://github.com/princeton-vl/CornerNet).  *More detailed descriptions of our approach and code will be made available soon.*  **If you encounter any problems in using our code  please contact Kaiwen Duan: kaiwen.duan@vipl.ict.ac.cn.**   ``` cd <CenterNet dir>/data/coco/PythonAPI make ```   Please first install [Anaconda](https://anaconda.org) and create an Anaconda environment using the provided package list. ``` conda create --name CenterNet --file conda_packagelist.txt ```  After you create the environment  activate it. ``` source activate CenterNet ```   python setup.py install --user   """;Computer Vision;https://github.com/tekeburak/CenterNet
"""""";Computer Vision;https://github.com/dantecomedia/CycleGAN
"""""";General;https://github.com/dantecomedia/CycleGAN
"""The authors of the paper present gMLP  an an attention-free all-MLP architecture based on spatial gating units. gMLP achieves parity with transformer models such as ViT and BERT on language and vision downstream tasks. The authors also show that gMLP scales with increased data and number of parameters  suggesting that self-attention is not a necessary component for designing performant models.   git clone https://github.com/jaketae/g-mlp.git  Navigate to the cloned directory. You can use the barebone gMLP model via   The repository also contains gMLP models specifically for language modeling and image classification.   """;Computer Vision;https://github.com/jaketae/g-mlp
"""The authors of the paper present gMLP  an an attention-free all-MLP architecture based on spatial gating units. gMLP achieves parity with transformer models such as ViT and BERT on language and vision downstream tasks. The authors also show that gMLP scales with increased data and number of parameters  suggesting that self-attention is not a necessary component for designing performant models.   git clone https://github.com/jaketae/g-mlp.git  Navigate to the cloned directory. You can use the barebone gMLP model via   The repository also contains gMLP models specifically for language modeling and image classification.   """;General;https://github.com/jaketae/g-mlp
"""""";Computer Vision;https://github.com/Spider-scnu/Instance-Segmentation-For-Cancer
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/zzfpython/yolov3-change
"""You can find an example of using this caption generator at [Inference.ipynb](https://github.com/alex-f1tor/Image-Caption/blob/master/Inference.ipynb) notebook.  Few examples of generated captions for images:  ![Image](https://github.com/alex-f1tor/Image-Caption/blob/master/imgs/bird_sample.png)  ![Image](https://github.com/alex-f1tor/Image-Caption/blob/master/imgs/pizza_sample.png)   You can also:   - Train your own caption network with MS-COCO dataset based on pipeline at [Training.ipynb](https://github.com/alex-f1tor/Image-Caption/blob/master/Training.ipynb)   - Estimate model performance at [cocoEvalCap.ipynb](https://github.com/alex-f1tor/Image-Caption/blob/master/cocoEvalCap.ipynb) via different [metrics](https://github.com/tylin/coco-caption)  like CIDEr  Rouge-L and etc.      - This project based on original arxiv paper [Show and Tell: A Neural Image Caption Generator (2015)](https://arxiv.org/abs/1411.4555) paper;   - The encoder is pretrained [resnet50](https://arxiv.org/abs/1512.03385) deep CNN [available](https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html#resnet50) in pyTorch;    - The caption generator was trained on [MS-COCO 2014 dataset](http://cocodataset.org/#download).    The following steps for default using pretrained model: ```sh  $ git clone git clone https://github.com/alex-f1tor/Image-Caption.git $ cd Image-Caption $ mkdir models $ cd models $  wget https://drive.google.com/open?id=19mcr08t6gY0UcUiAKTkBPO8MP0_wsghV -O 'decoder-4.pkl' && wget https://drive.google.com/open?id=1xe4zTMQAnH8QxcwHF7-i2lnmoBecJPYT -O 'encoder-4.pkl' ```   """;General;https://github.com/alex-f1tor/Image-Caption
"""You can find an example of using this caption generator at [Inference.ipynb](https://github.com/alex-f1tor/Image-Caption/blob/master/Inference.ipynb) notebook.  Few examples of generated captions for images:  ![Image](https://github.com/alex-f1tor/Image-Caption/blob/master/imgs/bird_sample.png)  ![Image](https://github.com/alex-f1tor/Image-Caption/blob/master/imgs/pizza_sample.png)   You can also:   - Train your own caption network with MS-COCO dataset based on pipeline at [Training.ipynb](https://github.com/alex-f1tor/Image-Caption/blob/master/Training.ipynb)   - Estimate model performance at [cocoEvalCap.ipynb](https://github.com/alex-f1tor/Image-Caption/blob/master/cocoEvalCap.ipynb) via different [metrics](https://github.com/tylin/coco-caption)  like CIDEr  Rouge-L and etc.      - This project based on original arxiv paper [Show and Tell: A Neural Image Caption Generator (2015)](https://arxiv.org/abs/1411.4555) paper;   - The encoder is pretrained [resnet50](https://arxiv.org/abs/1512.03385) deep CNN [available](https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html#resnet50) in pyTorch;    - The caption generator was trained on [MS-COCO 2014 dataset](http://cocodataset.org/#download).    The following steps for default using pretrained model: ```sh  $ git clone git clone https://github.com/alex-f1tor/Image-Caption.git $ cd Image-Caption $ mkdir models $ cd models $  wget https://drive.google.com/open?id=19mcr08t6gY0UcUiAKTkBPO8MP0_wsghV -O 'decoder-4.pkl' && wget https://drive.google.com/open?id=1xe4zTMQAnH8QxcwHF7-i2lnmoBecJPYT -O 'encoder-4.pkl' ```   """;Computer Vision;https://github.com/alex-f1tor/Image-Caption
"""``` pip install numpy scipy torch networkx matplotlib searborn ```   """;Graphs;https://github.com/Anieca/GCN
"""For this project  we train an agent to navigate (and collect bananas!) in a large  square world.    ![Trained Agent][image1]  A reward of +1 is provided for collecting a yellow banana  and a reward of -1 is provided for collecting a blue banana.  Thus  the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.    The state space has 37 dimensions and contains the agent's velocity  along with ray-based perception of objects around agent's forward direction.  Given this information  the agent has to learn how to best select actions.  Four discrete actions are available  corresponding to: - **`0`** - move forward. - **`1`** - move backward. - **`2`** - turn left. - **`3`** - turn right.  The task is episodic  and in order to solve the environment  the agent must get an average score of +13 over 100 consecutive episodes.   1. Download the environment from one of the links below.  You need only select the environment that matches your operating system:     - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip)     - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip)     - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip)     - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)          (_For Windows users_) Check out [this link](https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64) if you need help with determining if your computer is running a 32-bit version or 64-bit version of the Windows operating system.      (_For AWS_) If you'd like to train the agent on AWS (and have not [enabled a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md))  then please use [this link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux_NoVis.zip) to obtain the environment.  2. Place the file in the `RL_Navigation/` folder  and unzip (or decompress) the file.    """;Reinforcement Learning;https://github.com/shehrum/RL_Navigation
"""""";Computer Vision;https://github.com/chrisway613/MixConv
"""**Windows portable version**: Simply download and use the latest version from the [Releases](https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases) section.  Otherwise  check [doc/installation.md](doc/installation.md) for instructions on how to build OpenPose from source.     OS: Ubuntu (14  16)  Windows (8  10)  Mac OSX  Nvidia TX2.   CUDA (Nvidia GPU)  OpenCL (AMD GPU)  and CPU-only (no GPU) versions.   Most users do not need the OpenPose C++/Python API  but can simply use the OpenPose Demo:  - **OpenPose Demo**: To easily process images/video/webcam and display/save the results. See [doc/demo_overview.md](doc/demo_overview.md). E.g.  run OpenPose in a video with: ``` #: Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi :: Windows - Portable Demo bin\OpenPoseDemo.exe --video examples\media\video.avi ```  - **Calibration toolbox**: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See [doc/modules/calibration_module.md](doc/modules/calibration_module.md).  - **OpenPose C++ API**: If you want to read a specific input  and/or add your custom post-processing function  and/or implement your own display/saving  check the C++ API tutorial on [examples/tutorial_api_cpp/](examples/tutorial_api_cpp/) and [doc/library_introduction.md](doc/library_introduction.md). You can create your custom code on [examples/user_code/](examples/user_code/) and quickly compile it with CMake when compiling the whole OpenPose project. Quickly **add your custom code**: See [examples/user_code/README.md](examples/user_code/README.md) for further details.  - **OpenPose Python API**: Analogously to the C++ API  find the tutorial for the Python API on [examples/tutorial_api_python/](examples/tutorial_api_python/).  - **Adding an extra module**: Check [doc/library_add_new_module.md](./doc/library_add_new_module.md).  - **Standalone face or hand detector**:     - **Face** keypoint detection **without body** keypoint detection: If you want to speed it up (but also reduce amount of detected faces)  check the OpenCV-face-detector approach in [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     - **Use your own face/hand detector**: You can use the hand and/or face keypoint detectors with your own face or hand detectors  rather than using the body detector. E.g.  useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See [doc/standalone_face_or_hand_keypoint_detector.md](doc/standalone_face_or_hand_keypoint_detector.md).     """;General;https://github.com/liang-faan/openpose
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/mengfu188/insightface.bak
"""  - Please refer [Code for Data Generation](https://github.com/twtygqyy/pytorch-SRResNet/tree/master/data) for creating training files.   - Data augmentations including flipping  rotation  downsizing are adopted.      --cuda                Use cuda?     --cuda             use cuda?   ``` usage: demo.py [-h] [--cuda] [--model MODEL] [--image IMAGE]                [--dataset DATASET] [--scale SCALE] [--gpus GPUS]  optional arguments:   -h  --help         show this help message and exit   --cuda             use cuda?   --model MODEL      model path   --image IMAGE      image name   --dataset DATASET  dataset name   --scale SCALE      scale factor  Default: 4   --gpus GPUS        gpu ids (default: 0) ``` We convert Set5 test set images to mat format using Matlab  for simple image reading An example of usage is shown as follows: ``` python demo.py --model model/model_srresnet.pth --dataset Set5 --image butterfly_GT --scale 4 --cuda ```   """;General;https://github.com/V0LsTeR/mySRResNet
"""  - Please refer [Code for Data Generation](https://github.com/twtygqyy/pytorch-SRResNet/tree/master/data) for creating training files.   - Data augmentations including flipping  rotation  downsizing are adopted.      --cuda                Use cuda?     --cuda             use cuda?   ``` usage: demo.py [-h] [--cuda] [--model MODEL] [--image IMAGE]                [--dataset DATASET] [--scale SCALE] [--gpus GPUS]  optional arguments:   -h  --help         show this help message and exit   --cuda             use cuda?   --model MODEL      model path   --image IMAGE      image name   --dataset DATASET  dataset name   --scale SCALE      scale factor  Default: 4   --gpus GPUS        gpu ids (default: 0) ``` We convert Set5 test set images to mat format using Matlab  for simple image reading An example of usage is shown as follows: ``` python demo.py --model model/model_srresnet.pth --dataset Set5 --image butterfly_GT --scale 4 --cuda ```   """;Computer Vision;https://github.com/V0LsTeR/mySRResNet
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/peter8301120/darknet_t
""":beer:  All datasets used in this repository can be found from [face.evoLVe.PyTorch's Data-Zoo](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch#Data-Zoo).  Note:  - Both training and testing dataset are ""Align_112x112"" version.   :pizza:  Create a new python virtual environment by [Anaconda](https://www.anaconda.com/) or just use pip in your python environment and then clone this repository as following.   git clone https://github.com/peteryuX/arcface-tf2.git  cd arcface-tf2  conda env create -f environment.yml  conda activate arcface-tf2  pip install -r requirements.txt   Download LFW  Aged30 and CFP-FP datasets  then extract them to /your/path/to/test_dataset. These testing data are already binary files  so it's not necessary to do any preprocessing. The directory structure should be like bellow.   test_dataset: '/your/path/to/test_dataset'   """;Computer Vision;https://github.com/malin9402/2020-0221
""":beer:  All datasets used in this repository can be found from [face.evoLVe.PyTorch's Data-Zoo](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch#Data-Zoo).  Note:  - Both training and testing dataset are ""Align_112x112"" version.   :pizza:  Create a new python virtual environment by [Anaconda](https://www.anaconda.com/) or just use pip in your python environment and then clone this repository as following.   git clone https://github.com/peteryuX/arcface-tf2.git  cd arcface-tf2  conda env create -f environment.yml  conda activate arcface-tf2  pip install -r requirements.txt   Download LFW  Aged30 and CFP-FP datasets  then extract them to /your/path/to/test_dataset. These testing data are already binary files  so it's not necessary to do any preprocessing. The directory structure should be like bellow.   test_dataset: '/your/path/to/test_dataset'   """;General;https://github.com/malin9402/2020-0221
"""""";Computer Vision;https://github.com/NMADALI97/Learning-With-Wasserstein-Loss
"""""";General;https://github.com/NMADALI97/Learning-With-Wasserstein-Loss
"""""";Natural Language Processing;https://github.com/saravanakumarjsk/BERT-collection
"""We used `TensorFlow 0.11` and `Python 2`. `Sklearn` is also used.  The two datasets can be loaded by running `python download_datasets.py` in the `data/` folder.  To preprocess the second dataset (opportunity challenge dataset)  the `signal` submodule of scipy is needed  as well as `pandas`.    """;Natural Language Processing;https://github.com/Liut2016/stackedResBiLSTM
"""https://github.com/kuangliu/pytorch-cifar <- base code   """;Computer Vision;https://github.com/cjf8899/Pytorch_ResNeXt
"""""";Computer Vision;https://github.com/bmstu-iu8-g1-2019-project/road-signs-recognition
"""1. [Download](https://www.roboti.us/index.html) and install MuJoCo 1.50 and 2.00 from the MuJoCo website. We assume that the MuJoCo files are extracted to the default location (`~/.mujoco/mjpro150` and `~/.mujoco/mujoco200_{platform}`). Unfortunately  `gym` and `dm_control` expect different paths for MuJoCo 2.00 installation  which is why you will need to have it installed both in `~/.mujoco/mujoco200_{platform}` and `~/.mujoco/mujoco200`. The easiest way is to create a symlink from `~/.mujoco/mujoco200_{plaftorm}` -> `~/.mujoco/mujoco200` with: `ln -s ~/.mujoco/mujoco200_{platform} ~/.mujoco/mujoco200`.  2. Copy your MuJoCo license key (mjkey.txt) to ~/.mujoco/mjkey.txt:  3. Clone `softlearning` ``` git clone https://github.com/rail-berkeley/softlearning.git ${SOFTLEARNING_PATH} ```  4. Create and activate conda environment  install softlearning to enable command line interface. ``` cd ${SOFTLEARNING_PATH} conda env create -f environment.yml conda activate softlearning pip install -e ${SOFTLEARNING_PATH} ```  The environment should be ready to run. See examples section for examples of how to train and simulate the agents.  Finally  to deactivate and remove the conda environment: ``` conda deactivate conda remove --name softlearning --all ```   """;Reinforcement Learning;https://github.com/maxiaoba/SoftLearning
"""""";Reinforcement Learning;https://github.com/nbopardi/smb
"""This version supports cudnn v2 acceleration. @TimoSaemann has a branch supporting a more recent version of Caffe (Dec 2016) with cudnn v5.1:   In solver.prototxt set a path for snapshot_prefix. Then in a terminal run   If you would just like to try out a pretrained example model  then you can find the model used in the [SegNet webdemo](http://mi.eng.cam.ac.uk/projects/segnet/) and a script to run a live webcam demo here: https://github.com/alexgkendall/SegNet-Tutorial  For a more detailed introduction to this software please see the tutorial here: http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html   """;Computer Vision;https://github.com/Paultool/segnet
"""The project is an implementation of Wasserstein GAN in Tensorflow 2.0.  Paper link: https://arxiv.org/abs/1701.07875   Run the following command for usage.  ``` python train.py --help ```   """;Computer Vision;https://github.com/WangZesen/WGAN-GP-Tensorflow-v2
"""""";Computer Vision;https://github.com/leoHeidel/vae-gan-tf2
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/cfl0122/face
"""Instructions for acquiring PTB and WT2 can be found here. While CIFAR-10 can be automatically downloaded by torchvision  ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions here.   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/mayankk6196/darts-python
"""* Pr√©parez les outils pour configurer l'environnement virtuel (si vous l'avez d√©j√† fait  ignorez-le): ``` sudo apt-get install -y python-pip python3-pip cmake mkdir ~/.virtualenvs cd ~/.virtualenvs sudo pip install virtualenv virtualenvwrapper sudo pip3 install virtualenv virtualenvwrapper echo ""#: virtualenv and virtualenvwrapper"" >> ~/.bashrc echo ""export WORKON_HOME=$HOME/.virtualenvs"" >> ~/.bashrc echo ""export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3"" >> ~/.bashrc echo ""source /usr/local/bin/virtualenvwrapper.sh"" >> ~/.bashrc source ~/.bashrc ```  * Cr√©ez un nouvel environnement virtuel  nomm√© * iris *: ``` virtualenv -p python3.6 iris workon iris ```  * Cloner git et installer les paquets requis:  ``` cd Iris-Recognition-PyTorch pip install -r requirements.txt pip install git+https://github.com/Jmak12/pytorch-image-models#:#:egg=timm ```                        --use-cuda   """;General;https://github.com/Jmak12/Iris1
"""Requirements: Since my google credits are expired I have decided to build this project on Google Colab. The project will use keras with tensorflow as backend. We don't need to install libraries on Colab and can start straight away.    """;Computer Vision;https://github.com/ymittal23/PlayWithCifar
"""Requirements: Since my google credits are expired I have decided to build this project on Google Colab. The project will use keras with tensorflow as backend. We don't need to install libraries on Colab and can start straight away.    """;General;https://github.com/ymittal23/PlayWithCifar
"""""";General;https://github.com/praveenk2k7/Image-CLassifier_MobileNet
"""""";Computer Vision;https://github.com/praveenk2k7/Image-CLassifier_MobileNet
"""Instructions for acquiring PTB and WT2 can be found here. While CIFAR-10 can be automatically downloaded by torchvision  ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions here.   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/siddikui/Fast-DARTS
"""Image size: 100 x 100 pixel  Image channels: Adjustable using spectral_step  Training and testing sets:  | Image class | Training size | Testing size | Total | |:-:|:-:|:-:|:-:| | Almond | 189 | 21 | 210 | | Amber | 108 | 12 | 120 | | Shell | 97 | 11 | 108 | | Stone | 113 | 13 | 126 | | Wood Chip | 36 | 4 | 40 |  Sadly the image data used for this project is company proprietary and cannot be shared publicly.   """;Computer Vision;https://github.com/williamwang96/Almond-Recognition
"""""";Computer Vision;https://github.com/Baichenjia/DCGAN-eager
"""""";Sequential;https://github.com/ShichaoSun/math_seq2tree
"""**GCNet** is initially described in [arxiv](https://arxiv.org/abs/1904.11492). Via absorbing advantages of Non-Local Networks (NLNet) and Squeeze-Excitation Networks (SENet)   GCNet provides a simple  fast and effective approach for global context modeling  which generally outperforms both NLNet and SENet on major benchmarks for various recognition tasks.   a. Install PyTorch 1.1 and torchvision following the [official instructions](https://pytorch.org/).  b. Install latest apex with CUDA and C++ extensions following this [instructions](https://github.com/NVIDIA/apex#quick-start).  The [Sync BN](https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm) implemented by apex is required.  c. Clone the GCNet repository.   ```bash  git clone https://github.com/xvjiarui/GCNet.git  ```  d. Compile cuda extensions.  ```bash cd GCNet pip install cython  #: or ""conda install cython"" if you prefer conda ./compile.sh  #: or ""PYTHON=python3 ./compile.sh"" if you use system python3 without virtual environments ```  e. Install GCNet version mmdetection (other dependencies will be installed automatically).  ```bash python(3) setup.py install  #: add --user if you want to install it locally #: or ""pip install ."" ```  Note: You need to run the last step each time you pull updates from github.  Or you can run `python(3) setup.py develop` or `pip install -e .` to install mmdetection if you want to make modifications to it frequently.  Please refer to mmdetection install [instruction](https://github.com/open-mmlab/mmdetection/blob/master/INSTALL.md) for more details.   Python 3.6.7  PyTorch 1.1.0  CUDA 9.0   """;Computer Vision;https://github.com/zhusiling/GCNet
"""**GCNet** is initially described in [arxiv](https://arxiv.org/abs/1904.11492). Via absorbing advantages of Non-Local Networks (NLNet) and Squeeze-Excitation Networks (SENet)   GCNet provides a simple  fast and effective approach for global context modeling  which generally outperforms both NLNet and SENet on major benchmarks for various recognition tasks.   a. Install PyTorch 1.1 and torchvision following the [official instructions](https://pytorch.org/).  b. Install latest apex with CUDA and C++ extensions following this [instructions](https://github.com/NVIDIA/apex#quick-start).  The [Sync BN](https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm) implemented by apex is required.  c. Clone the GCNet repository.   ```bash  git clone https://github.com/xvjiarui/GCNet.git  ```  d. Compile cuda extensions.  ```bash cd GCNet pip install cython  #: or ""conda install cython"" if you prefer conda ./compile.sh  #: or ""PYTHON=python3 ./compile.sh"" if you use system python3 without virtual environments ```  e. Install GCNet version mmdetection (other dependencies will be installed automatically).  ```bash python(3) setup.py install  #: add --user if you want to install it locally #: or ""pip install ."" ```  Note: You need to run the last step each time you pull updates from github.  Or you can run `python(3) setup.py develop` or `pip install -e .` to install mmdetection if you want to make modifications to it frequently.  Please refer to mmdetection install [instruction](https://github.com/open-mmlab/mmdetection/blob/master/INSTALL.md) for more details.   Python 3.6.7  PyTorch 1.1.0  CUDA 9.0   """;General;https://github.com/zhusiling/GCNet
"""``` import os import keras.backend as K  from data import DATA_SET_DIR from elmo.lm_generator import LMDataGenerator from elmo.model import ELMo  parameters = {     'multi_processing': False      'n_threads': 4      'cuDNN': True if len(K.tensorflow_backend._get_available_gpus()) else False      'train_dataset': 'wikitext-2/wiki.train.tokens'      'valid_dataset': 'wikitext-2/wiki.valid.tokens'      'test_dataset': 'wikitext-2/wiki.test.tokens'      'vocab': 'wikitext-2/wiki.vocab'      'vocab_size': 28914      'num_sampled': 1000      'charset_size': 262      'sentence_maxlen': 100      'token_maxlen': 50      'token_encoding': 'word'      'epochs': 10      'patience': 2      'batch_size': 1      'clip_value': 5      'cell_clip': 5      'proj_clip': 5      'lr': 0.2      'shuffle': True      'n_lstm_layers': 2      'n_highway_layers': 2      'cnn_filters': [[1  32]                      [2  32]                      [3  64]                      [4  128]                      [5  256]                      [6  512]                      [7  512]                     ]      'lstm_units_size': 400      'hidden_units_size': 200      'char_embedding_size': 16      'dropout_rate': 0.1      'word_dropout_rate': 0.05      'weight_tying': True  }  #: Set-up Generators train_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['train_dataset'])                                    os.path.join(DATA_SET_DIR  parameters['vocab'])                                    sentence_maxlen=parameters['sentence_maxlen']                                    token_maxlen=parameters['token_maxlen']                                    batch_size=parameters['batch_size']                                    shuffle=parameters['shuffle']                                    token_encoding=parameters['token_encoding'])  val_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['valid_dataset'])                                  os.path.join(DATA_SET_DIR  parameters['vocab'])                                  sentence_maxlen=parameters['sentence_maxlen']                                  token_maxlen=parameters['token_maxlen']                                  batch_size=parameters['batch_size']                                  shuffle=parameters['shuffle']                                  token_encoding=parameters['token_encoding'])  test_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['test_dataset'])                                  os.path.join(DATA_SET_DIR  parameters['vocab'])                                  sentence_maxlen=parameters['sentence_maxlen']                                  token_maxlen=parameters['token_maxlen']                                  batch_size=parameters['batch_size']                                  shuffle=parameters['shuffle']                                  token_encoding=parameters['token_encoding'])  #: Compile ELMo elmo_model = ELMo(parameters) elmo_model.compile_elmo(print_summary=True)  #: Train ELMo elmo_model.train(train_data=train_generator  valid_data=val_generator)  #: Persist ELMo Bidirectional Language Model in disk elmo_model.save(sampled_softmax=False)  #: Evaluate Bidirectional Language Model elmo_model.evaluate(test_generator)  #: Build ELMo meta-model to deploy for production and persist in disk elmo_model.wrap_multi_elmo_encoder(print_summary=True  save=True)  ```   """;Sequential;https://github.com/yangrui123/Hidden
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/witwickey/darknet
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/witwickey/darknet
"""Ubuntu 16.04 (Dual OS with Windows)  GPUÔºö GTX1060 3GB   + sudo apt-get install python-opencv python-matplotlib python-numpy  + sudo apt-get install mxnet     + sudo apt-get install mxnet-cu92    + https://mxnet.incubator.apache.org/versions/master/install/ubuntu_setup.html#cuda-dependencies   <a href=https://github.com/zhreshold/mxnet-ssd/releases/download/v0.6/resnet50_ssd_512_voc0712_trainval.zip>pretrained model</a>   install ubuntu as dual OSs with windows   """;Computer Vision;https://github.com/NCNU-OpenSource/What-are-you-
"""* Pr√©parez les outils pour configurer l'environnement virtuel (si vous l'avez d√©j√† fait  ignorez-le): ``` sudo apt-get install -y python-pip python3-pip cmake mkdir ~/.virtualenvs cd ~/.virtualenvs sudo pip install virtualenv virtualenvwrapper sudo pip3 install virtualenv virtualenvwrapper echo ""#: virtualenv and virtualenvwrapper"" >> ~/.bashrc echo ""export WORKON_HOME=$HOME/.virtualenvs"" >> ~/.bashrc echo ""export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3"" >> ~/.bashrc echo ""source /usr/local/bin/virtualenvwrapper.sh"" >> ~/.bashrc source ~/.bashrc ```  * Cr√©ez un nouvel environnement virtuel  nomm√© * iris *: ``` virtualenv -p python3.6 iris workon iris ```  * Cloner git et installer les paquets requis:  ``` cd Iris-Recognition-PyTorch pip install -r requirements.txt pip install git+https://github.com/Jmak12/pytorch-image-models#:#:egg=timm ```                        --use-cuda   """;Computer Vision;https://github.com/Jmak12/Iris1
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/edgarrt/custom_darknet
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   mkdir build-release  cd build-release   make install  Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Build solution   cuDNN > 7.0  OpenCV > 2.4  then to compile Darknet it is recommended to use   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/edgarrt/custom_darknet
"""The only dependencies are h5py  Theano and Keras. Run the following commands ``` pip install --user cython h5py pip install --user git+https://github.com/Theano/Theano.git pip install --user git+https://github.com/fchollet/keras.git ```  Then  you need to install the convnetskeras module : ``` git clone https://github.com/heuritech/convnets-keras.git cd convnets-keras sudo python setup.py install ```   The weights can be found here :  * <a href=""http://files.heuritech.com/weights/alexnet_weights.h5"">AlexNet weights</a> * <a href=""http://files.heuritech.com/weights/vgg16_weights.h5"">VGG16 weights</a> * <a href=""http://files.heuritech.com/weights/vgg19_weights.h5"">VGG19 weights</a>     **BEWARE** !! : Since the networks have been trained in different settings  the preprocessing is different for the differents networks :  * For the AlexNet  the images (for the mode without the heatmap) have to be of shape (227 227). It is recommended to resize the images with a size of (256 256)  and then do a crop of size (227 227). The colors are in RGB order. ```python from keras.optimizers import SGD from convnetskeras.convnets import preprocess_image_batch  convnet  im = preprocess_image_batch(['examples/dog.jpg'] img_size=(256 256)  crop_size=(227 227)  color_mode=""rgb"")  sgd = SGD(lr=0.1  decay=1e-6  momentum=0.9  nesterov=True) model = convnet('alexnet' weights_path=""weights/alexnet_weights.h5""  heatmap=False) model.compile(optimizer=sgd  loss='mse')  out = model.predict(im) ```  * For the VGG  the images (for the mode without the heatmap) have to be of shape (224 224). It is recommended to resize the images with a size of (256 256)  and then do a crop of size (224 224). The colors are in BGR order. ```python from keras.optimizers import SGD from convnetskeras.convnets import preprocess_image_batch  convnet  im = preprocess_image_batch(['examples/dog.jpg'] img_size=(256 256)  crop_size=(224 224)  color_mode=""bgr"")  sgd = SGD(lr=0.1  decay=1e-6  momentum=0.9  nesterov=True) #:#: For the VGG16  use this command model = convnet('vgg_16' weights_path=""weights/vgg16_weights.h5""  heatmap=False) #:#: For the VGG19  use this one instead #: model = convnet('vgg_19' weights_path=""weights/vgg19_weights.h5""  heatmap=False) model.compile(optimizer=sgd  loss='mse')  out = model.predict(im)  ```    The heatmap are produced by converting the model into a fully convolutionize model. The fully connected layers are transformed into convolution layers (by using the same weights)  so we are able to compute the output of the network on each sub-frame of size (227 227) (or (224 224)) of a bigger picture. This produces a heatmap for each label of the classifier.  Using the heatmap is almost the same thing than directly classify. We suppose that we want the heatmap of the all the synsets linked with dogs  which are all the children in Wordnet of the synset ""n02084071"" (see next section to know how to find how we can get all the labels linked with a given synset) :  ```python from keras.optimizers import SGD from convnetskeras.convnets import preprocess_image_batch  convnet from convnetskeras.imagenet_tool import synset_to_dfs_ids  im = preprocess_image_batch(['examples/dog.jpg']  color_mode=""bgr"")  sgd = SGD(lr=0.1  decay=1e-6  momentum=0.9  nesterov=True) model = convnet('alexnet' weights_path=""weights/alexnet_weights.h5""  heatmap=True) model.compile(optimizer=sgd  loss='mse')  out = model.predict(im)  s = ""n02084071"" ids = synset_to_dfs_ids(s) heatmap = out[0 ids].sum(axis=0)  #: Then  we can get the image import matplotlib.pyplot as plt plt.imsave(""heatmap_dog.png"" heatmap) ``` <img src=https://raw.githubusercontent.com/heuritech/convnets-keras/master/examples/dog.jpg width=""400px"">  <img src=https://raw.githubusercontent.com/heuritech/convnets-keras/master/examples/heatmap_dog.png width=""400px"">   If you want to detect all cars  you might need to have a classification of higher level than the one given by the wordnets of ImageNet. Indeed  a lot of different synsets are present for different kinds of cars. We can then choose a synset in the tree  and select all the ids of its children :  ```python >>>synset_to_dfs_ids(""n04576211"") [670  870  880  444  671  565  705  428  791  561  757  829  866  847  547  820  408  573  575  803  407  436  468  511  609  627  656  661  751  817  665  555  569  717  864  867  675  734  656  586  847  802  660  603  612  690] ```   """;Computer Vision;https://github.com/carolgithubv1/convnets-keras
"""The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux  MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. **Credit to Joseph Redmon for YOLO:** https://pjreddie.com/darknet/yolo/.   This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC  and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://www.ultralytics.com.   Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='GeForce RTX 2080 Ti'  total_memory=11019MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Transfer Learning](https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning) * [Train Single Image](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image) * [Train Single Class](https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class) * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data)   """;Computer Vision;https://github.com/yushuz/me599_yolo
"""""";Computer Vision;https://github.com/Yashgh7076/CPSC-8810-Project
"""""";General;https://github.com/Sobeit-Tim/NuguEyeTest
"""""";Computer Vision;https://github.com/Sobeit-Tim/NuguEyeTest
"""The following notebooks will help you to build these models and apply them to your datasets: * [Iris Segmentation Tutorial](https://github.com/Osdel/ssnets/blob/master/Iris_Segmentation_Tutorial.ipynb)  """;Computer Vision;https://github.com/Osdel/ssnets
"""It is tested with pytorch-1.0.   """;Computer Vision;https://github.com/mengxingshifen1218/pointnet.pytorch
"""Tested with PyTorch 1.3  CUDA 10.0  and Python 3.3 with [Conda](https://www.anaconda.com/).  ``` conda install pytorch torchvision cudatoolkit=10.0 -c pytorch #: See https://pytorch.org/get-started/locally/ git clone git@github.com:facebookresearch/SparseConvNet.git cd SparseConvNet/ bash develop.sh ``` To run the examples you may also need to install unrar: ``` apt-get install unrar ```   : Use the GPU if there is one  otherwise CPU   Examples in the examples folder include * [Assamese handwriting recognition](https://archive.ics.uci.edu/ml/datasets/Online+Handwritten+Assamese+Characters+Dataset#) * [Chinese handwriting for recognition](http://www.nlpr.ia.ac.cn/databases/handwriting/Online_database.html) * [3D Segmentation](https://shapenet.cs.stanford.edu/iccv17/) using ShapeNet Core-55 * [ScanNet](http://www.scan-net.org/) 3D Semantic label benchmark  For example: ``` cd examples/Assamese_handwriting python VGGplus.py ```   """;Computer Vision;https://github.com/LONG-9621/SparseConvNet
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;Computer Vision;https://github.com/vierachen/maskrcnn
"""Check [INSTALL.md](INSTALL.md) for installation instructions.    Mixed precision training: trains faster with less GPU memory on NVIDIA tensor cores.   For the following examples to work  you need to first install maskrcnn_benchmark.  You will also need to download the COCO dataset.  We recommend to symlink the path to the coco dataset to datasets/ as follows   : symlink the coco dataset  cd ~/github/maskrcnn-benchmark  mkdir -p datasets/coco   : or use COCO 2017 version   You can also configure your own paths to the datasets.   1. Run the following without modifications   But the drawback is that it will use much more GPU memory. The reason is that we set in the   have a single GPU  this means that the batch size for that GPU will be 8x larger  which might lead   We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference: ```bash cd demo #: by default  it runs on the GPU #: for best results  use min-image-size 800 python webcam.py --min-image-size 800 #: can also run it on the CPU python webcam.py --min-image-size 300 MODEL.DEVICE cpu #: or change the model that you want to use python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu #: in order to see the probability heatmaps  pass --show-mask-heatmaps python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu #: for the keypoint demo python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu ```  A notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).   """;General;https://github.com/vierachen/maskrcnn
"""GCP (or any other cloud services that provide a robust environment)   """;Natural Language Processing;https://github.com/lucashueda/long_sentence_transformer
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov3.weights/cfg with: C++ example or Python example   TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/JEONGHA-SHIN/YOGO_Server_v2
"""""";Reinforcement Learning;https://github.com/alessandrositta/Flatland_challenge
"""As per usual  strongly suggested to create a virtual environment of your liking before installing the dependencies: ``` #: using Anaconda: conda create --name compressive-transformer python=3.8 source activate compressive-transformer ```  The required packages can then be installed by running ``` make install ```   ```bash python ct.py train ```  Runtime configurations - for tokenization  model options  etc. - can be configured in `ct/config/default.py`. _omegaconf_ is used for configuration.   A simple documentation of the code  together with some additional examples can be found in `docs/build/index.html`.  """;General;https://github.com/ViktorStagge/CompressiveTransformer
"""As per usual  strongly suggested to create a virtual environment of your liking before installing the dependencies: ``` #: using Anaconda: conda create --name compressive-transformer python=3.8 source activate compressive-transformer ```  The required packages can then be installed by running ``` make install ```   ```bash python ct.py train ```  Runtime configurations - for tokenization  model options  etc. - can be configured in `ct/config/default.py`. _omegaconf_ is used for configuration.   A simple documentation of the code  together with some additional examples can be found in `docs/build/index.html`.  """;Natural Language Processing;https://github.com/ViktorStagge/CompressiveTransformer
"""Warning: This implementation isn't user friendly yet. If you have any questions  [create a github issue](https://github.com/MattKleinsmith/pbt/issues/new) and I'll try to help you.  Steps:  1. Wrestle with dependencies. 2. Edit config.py to set your options. 3. Store your data as bcolz carrays. See datasets.py for an example. 4. In a terminal  enter: `python main.py --exploiter` 5. If you want to use a second GPU  then in a second terminal  enter: `python main.py --gpu 1 --population_id -1`  where ""1"" refers to your GPU's ID in nvidia-smi  and ""-1"" means to work on the most recently created population.  When finished  the process will print the path to the weights of the best performing model.   """;General;https://github.com/MattKleinsmith/pbt
"""``` pip install -r requirements.txt ```    """;Sequential;https://github.com/ciaua/score_lyrics_free_svg
"""""";General;https://github.com/vitoralbiero/face_analysis_pytorch
"""1. python3.5 -m pip install -r requirements.txt 1. install tensorflow or tensorflow-gpu  depending on whether your machine supports GPU configurations   Current version : 0.0.0.1   CUDA_VISIBLE_DEVICES=0 1 python train.py ( <== Use only GPU 0  1 )   bash launch_tensorboard.sh   CUDA_VISIBLE_DEVICES=0 1 python test.py ( <== Use only GPU 0  1 )   """;Audio;https://github.com/randomrandom/deep-atrous-cnn-sentiment
"""1. python3.5 -m pip install -r requirements.txt 1. install tensorflow or tensorflow-gpu  depending on whether your machine supports GPU configurations   Current version : 0.0.0.1   CUDA_VISIBLE_DEVICES=0 1 python train.py ( <== Use only GPU 0  1 )   bash launch_tensorboard.sh   CUDA_VISIBLE_DEVICES=0 1 python test.py ( <== Use only GPU 0  1 )   """;Sequential;https://github.com/randomrandom/deep-atrous-cnn-sentiment
"""SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).  <p align=""center""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd.png"" alt=""SSD Framework"" width=""600px""> </p>  | System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution |:-------|:-----:|:-------:|:-------:|:-------:| | [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 | | [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 | | SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 | | SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |   <p align=""left""> <img src=""http://www.cs.unc.edu/~wliu/papers/ssd_results.png"" alt=""SSD results on multiple datasets"" width=""800px""> </p>  _Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._   1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`  2. Download VOC2007 and VOC2012 dataset. By default  we assume the data is stored in `$HOME/data/`   ```Shell   #: Download the data.   cd $HOME/data   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar   wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar   #: Extract the data.   tar -xvf VOCtrainval_11-May-2012.tar   tar -xvf VOCtrainval_06-Nov-2007.tar   tar -xvf VOCtest_06-Nov-2007.tar   ```  3. Create the LMDB file.   ```Shell   cd $CAFFE_ROOT   #: Create the trainval.txt  test.txt  and test_name_size.txt in data/VOC0712/   ./data/VOC0712/create_list.sh   #: You can modify the parameters in create_data.sh if needed.   #: It will create lmdb files for trainval and test with encoded original image:   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb   #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb   #: and make soft links at examples/VOC0712/   ./data/VOC0712/create_data.sh   ```   1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`   ```Shell   git clone https://github.com/weiliu89/caffe.git   cd caffe   git checkout ssd   ```  2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.   ```Shell   #: Modify Makefile.config according to your Caffe installation.   cp Makefile.config.example Makefile.config   make -j8   #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.   make py   make test -j8   #: (Optional)   make runtest -j8   ```   COCO<sup>[1]</sup>: SSD300*  SSD512*  07+12+COCO: SSD300*  SSD512*  07++12+COCO: SSD300*  SSD512*  COCO models:   """;Computer Vision;https://github.com/kfx7577/Caffe_VehicleCounter
"""Requirements (and how to install dependecies)   How to compile on Linux   Using make  How to compile on Windows   How to train with multi-GPU:   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile (or use the same settings with Cmake)   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   OpenCV-dnn the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example   PyTorch > ONNX:    TensorRT YOLOv4 on TensorRT+tkDNN: https://github.com/ceccocats/tkDNN   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   This is the recommended approach to build Darknet on Windows if you have already   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl   PS \&gt;                  cd vcpkg  PS Code\vcpkg&gt;         .\vcpkg install darknet[full]:x64-windows #:replace with darknet[opencv-base weights]:x64-windows for a quicker install; use --head if you want to build latest commit on master branch and not latest release  You will find darknet inside the vcpkg\installed\x64-windows\tools\darknet folder  together with all the necessary weight and cfg files  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   2. in Python: https://github.com/tzutalin/labelImg  3. in Python: https://github.com/Cartucho/OpenLabeling   5. in JavaScript: https://github.com/opencv/cvat    On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`    * on Linux     * using `build.sh` or     * build `darknet` using `cmake` or     * set `LIBSO=1` in the `Makefile` and do `make` * on Windows     * using `build.ps1` or     * build `darknet` using `cmake` or     * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/Keshavkant/YOLO_Toll
"""""";Sequential;https://github.com/HuihuiChyan/BJTUNLP_Practice2021
"""LUKE can be installed using [Poetry](https://python-poetry.org/):  ```bash $ poetry install ```  The virtual environment automatically created by Poetry can be activated by `poetry shell`.   examples_allennlp directory.   The experiments were conducted using Python3.6 and PyTorch 1.2.0 installed on a   $ git clone https://github.com/NVIDIA/apex.git  $ cd apex  $ git checkout c3fad1ad120b23055f6630da0b029c8b626db78f   """;General;https://github.com/studio-ousia/luke
"""The sketch dataset can be downloaded as follow:  ./download_data.sh sketch   wget https://dl.fbaipublicfiles.com/moco/moco_checkpoints/moco_v2_800ep/moco_v2_800ep_pretrain.pth.tar   python src/main.py --exp-name vmtc-repr --cuda --run-id sketch-real vmtc_repr --ss-path moco_v2_800ep_pretrain.pth.tar   To create the MNIST->SVHN grids  simply run the following command:   To create the Sketches->Reals grid  simply run the following command:   """;Computer Vision;https://github.com/lavoiems/Cats-UDT
"""The sketch dataset can be downloaded as follow:  ./download_data.sh sketch   wget https://dl.fbaipublicfiles.com/moco/moco_checkpoints/moco_v2_800ep/moco_v2_800ep_pretrain.pth.tar   python src/main.py --exp-name vmtc-repr --cuda --run-id sketch-real vmtc_repr --ss-path moco_v2_800ep_pretrain.pth.tar   To create the MNIST->SVHN grids  simply run the following command:   To create the Sketches->Reals grid  simply run the following command:   """;General;https://github.com/lavoiems/Cats-UDT
"""1) create and activate conda virtual environment with Python 3.7.4 (recommended) ``` $ conda create --name hetseq $ conda activate hetseq $ conda install python=3.7.4 ```  2) Git clone directory and install nessasory package ``` $ git clone https://github.com/yifding/hetseq.git $ cd /path/to/hetseq $ pip install -r requirements.txt  $ pip install --editable .  ```  3) **To Run BERT:** Download data files including training corpus  model configuration  and BPE dictionary. Test corpus from [here](https://drive.google.com/file/d/1ZPJVAiV7PsewChi7xKACrjuniJ2N9Sry/view?usp=sharing)  full data from [this link](https://drive.google.com/file/d/1Vq_UO-T9345uYs8a7zloukGfhDXSDd2A/view?usp=sharing). Download test_DATA.zip for test or DATA.zip for full run  unzip it and place the ```preprocessing/``` directory inside the package directory. Available corpus under ```preprocessing/```    * phase one of BERT training corpus : ```preprocessing/hdf5_lower_case_1_seq_len_128.../wikicorpus_en/``` * phase two of BERT training corpus : ```preprocessing/hdf5_lower_case_1_seq_len_512.../wikicorpus_en/``` * sample test for phase one : ```preprocessing/test128/``` * sample test for phase two : ```preprocessing/test512/``` * see [NVIDIA-pytorch-BERT](https://arxiv.org/abs/1810.04805)  [google_original_BERT](https://github.com/google-research/bert) and [BERT paper](https://arxiv.org/abs/1810.04805) for more information. * current provided is generated from [NVIDIA-pytorch-BERT](https://arxiv.org/abs/1810.04805) with wikipedia data (book data is not available)  4) Running HetSeq script is available at https://hetseq.readthedocs.io/en/master/examples.html     HetSeq requires installation of PyTorch with GPU support and NCCL.   [ ] hetseq not supporting download from pip   """;Natural Language Processing;https://github.com/yifding/hetseq
"""""";Reinforcement Learning;https://github.com/hulanwin/A3C-DRL
"""""";General;https://github.com/hulanwin/A3C-DRL
"""""";Computer Vision;https://github.com/rickyHong/FCN-segmentation-repl
"""1. Clone this repo and install requirements     ```command    git clone https://github.com/L0SG/NanoFlow.git    cd NanoFlow    pip install -r requirements.txt    ```  2. Install [Apex] for mixed-precision training    Below are the example commands using nanoflow-h16-r128-emb512.json      insert `checkpoint_path: ""experiments/nanoflow-h16-r128-emb512/waveflow_5000""` in the config file then run    ```command    python train.py -c configs/nanoflow-h16-r128-emb512.json    ```     for loading averaged weights over 10 recent checkpoints  insert `checkpoint_path: ""experiments/nanoflow-h16-r128-emb512""` in the config file then run    ```command    python train.py -a 10 -c configs/nanoflow-h16-r128-emb512.json    ```     you can reset the optimizer and training scheduler (and keep the weights) by providing `--warm_start`    ```command    python train.py --warm_start -c configs/nanoflow-h16-r128-emb512.json    ```     4. Synthesize waveform from the trained model.     insert `checkpoint_path` in the config file and use `--synthesize` to `train.py`. The model generates waveform by looping over `test_files.txt`.    ```command    python train.py --synthesize -c configs/nanoflow-h16-r128-emb512.json    ```    if `fp16_run: true`  the model uses FP16 (half-precision) arithmetic for faster performance (on GPUs equipped with Tensor Cores).    """;Computer Vision;https://github.com/L0SG/NanoFlow
"""1. Clone this repo and install requirements     ```command    git clone https://github.com/L0SG/NanoFlow.git    cd NanoFlow    pip install -r requirements.txt    ```  2. Install [Apex] for mixed-precision training    Below are the example commands using nanoflow-h16-r128-emb512.json      insert `checkpoint_path: ""experiments/nanoflow-h16-r128-emb512/waveflow_5000""` in the config file then run    ```command    python train.py -c configs/nanoflow-h16-r128-emb512.json    ```     for loading averaged weights over 10 recent checkpoints  insert `checkpoint_path: ""experiments/nanoflow-h16-r128-emb512""` in the config file then run    ```command    python train.py -a 10 -c configs/nanoflow-h16-r128-emb512.json    ```     you can reset the optimizer and training scheduler (and keep the weights) by providing `--warm_start`    ```command    python train.py --warm_start -c configs/nanoflow-h16-r128-emb512.json    ```     4. Synthesize waveform from the trained model.     insert `checkpoint_path` in the config file and use `--synthesize` to `train.py`. The model generates waveform by looping over `test_files.txt`.    ```command    python train.py --synthesize -c configs/nanoflow-h16-r128-emb512.json    ```    if `fp16_run: true`  the model uses FP16 (half-precision) arithmetic for faster performance (on GPUs equipped with Tensor Cores).    """;General;https://github.com/L0SG/NanoFlow
"""""";General;https://github.com/amurthy1/dagan_torch
"""""";Computer Vision;https://github.com/amurthy1/dagan_torch
"""Environment GYM : https://gym.openai.com/   """;Reinforcement Learning;https://github.com/Gouet/PPO-gym
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/zhang-huihui/git-repository
"""""";Graphs;https://github.com/zetayue/MXMNet
"""<img src=""imgs/img1.png"" width=""320"">   <img src=""imgs/img2.png"" width=""320"">  It is important to note that as a result of the adversarial loss function  approximating the original colors of the images is not the only goal of the generators. They also have to fill the images with realistic/believable colors in order to fool the discriminators (although  these tasks can be equivalent). In this regard  the models perform quite well  often creating colorful and lifelike samples.   """;Computer Vision;https://github.com/karoly-hars/GAN_image_colorizing
"""<img src=""imgs/img1.png"" width=""320"">   <img src=""imgs/img2.png"" width=""320"">  It is important to note that as a result of the adversarial loss function  approximating the original colors of the images is not the only goal of the generators. They also have to fill the images with realistic/believable colors in order to fool the discriminators (although  these tasks can be equivalent). In this regard  the models perform quite well  often creating colorful and lifelike samples.   """;General;https://github.com/karoly-hars/GAN_image_colorizing
"""We recommend using a virtual environment that is specific to using CharacterBERT.  If you do not already have `conda` installed  you can install Miniconda from [this link](https://docs.conda.io/en/latest/miniconda.html#linux-installers) (~450Mb). Then  check that conda is up to date:  ```bash conda update -n base -c defaults conda ```  And create a fresh conda environment (~220Mb):  ```bash conda create python=3.8 --name=character-bert ```  If not already activated  activate the new conda environment using:  ```bash conda activate character-bert ```  Then install the following packages (~3Gb):  ```bash conda install pytorch cudatoolkit=10.2 -c pytorch pip install transformers==3.3.1 scikit-learn==0.23.2 ```  > Note 1: If you will not be running experiments on a GPU  install pyTorch via this command instead `conda install pytorch cpuonly -c pytorch`  > Note 2: If you just want to be able to load pre-trained CharacterBERT weigths  you do not have to install `scikit-learn` which is only used for computing Precision  Recall  F1 metrics during evaluation.   You can use the download.py script to download any of the models below:   For example  to download the medical version of CharacterBERT you can run:   Or you can download all models by running:   ```python """"""Basic example: getting word embeddings from CharacterBERT"""""" from transformers import BertTokenizer from modeling.character_bert import CharacterBertModel from utils.character_cnn import CharacterIndexer  #: Example text x = ""Hello World!""  #: Tokenize the text tokenizer = BertTokenizer.from_pretrained(     './pretrained-models/bert-base-uncased/') x = tokenizer.basic_tokenizer.tokenize(x)  #: Add [CLS] and [SEP] x = ['[CLS]'  *x  '[SEP]']  #: Convert token sequence into character indices indexer = CharacterIndexer() batch = [x]  #: This is a batch with a single token sequence x batch_ids = indexer.as_padded_tensor(batch)  #: Load some pre-trained CharacterBERT model = CharacterBertModel.from_pretrained(     './pretrained-models/medical_character_bert/')  #: Feed batch to CharacterBERT & get the embeddings embeddings_for_batch  _ = model(batch_ids) embeddings_for_x = embeddings_for_batch[0] print('These are the embeddings produces by CharacterBERT (last transformer layer)') for token  embedding in zip(x  embeddings_for_x):     print(token  embedding) ```   ```python """""" Basic example: using CharacterBERT for binary classification """""" from transformers import BertForSequenceClassification  BertConfig from modeling.character_bert import CharacterBertModel  #:#:#:#: LOADING BERT FOR CLASSIFICATION #:#:#:#:  config = BertConfig.from_pretrained('bert-base-uncased'  num_labels=2)  #: binary classification model = BertForSequenceClassification(config=config)  model.bert.embeddings.word_embeddings  #: wordpiece embeddings >>> Embedding(30522  768  padding_idx=0)  #:#:#:#: REPLACING BERT WITH CHARACTER_BERT #:#:#:#:  character_bert_model = CharacterBertModel.from_pretrained(     './pretrained-models/medical_character_bert/') model.bert = character_bert_model  model.bert.embeddings.word_embeddings  #: wordpieces are replaced with a CharacterCNN >>> CharacterCNN(         (char_conv_0): Conv1d(16  32  kernel_size=(1 )  stride=(1 ))         (char_conv_1): Conv1d(16  32  kernel_size=(2 )  stride=(1 ))         (char_conv_2): Conv1d(16  64  kernel_size=(3 )  stride=(1 ))         (char_conv_3): Conv1d(16  128  kernel_size=(4 )  stride=(1 ))         (char_conv_4): Conv1d(16  256  kernel_size=(5 )  stride=(1 ))         (char_conv_5): Conv1d(16  512  kernel_size=(6 )  stride=(1 ))         (char_conv_6): Conv1d(16  1024  kernel_size=(7 )  stride=(1 ))         (_highways): Highway(         (_layers): ModuleList(             (0): Linear(in_features=2048  out_features=4096  bias=True)             (1): Linear(in_features=2048  out_features=4096  bias=True)         )         )         (_projection): Linear(in_features=2048  out_features=768  bias=True)     )  #:#:#:#: PREPARING RAW TEXT #:#:#:#:  from transformers import BertTokenizer from utils.character_cnn import CharacterIndexer  text = ""CharacterBERT attends to each token's characters"" bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') tokenized_text = bert_tokenizer.basic_tokenizer.tokenize(text) #: this is NOT wordpiece tokenization  tokenized_text >>> ['characterbert'  'attends'  'to'  'each'  'token'  ""'""  's'  'characters']  indexer = CharacterIndexer()  #: This converts each token into a list of character indices input_tensor = indexer.as_padded_tensor([tokenized_text])  #: we build a batch of only one sequence input_tensor.shape >>> torch.Size([1  8  50])  #: (batch_size  sequence_length  character_embedding_dim)  #:#:#:#: USING CHARACTER_BERT FOR INFERENCE #:#:#:#:  output = model(input_tensor)[0] >>> tensor([[-0.3378  -0.2772]]  grad_fn=<AddmmBackward>)  #: class logits ```  For more complete (but still illustrative) examples you can refer to the `run_experiments.sh` script which runs a few Classification/SequenceLabelling experiments using BERT/CharacterBERT.  ```bash bash run_experiments.sh ```  You can adapt the `run_experiments.sh` script to try out any available model. You should also be able to add real classification and sequence labelling tasks by adapting the `data.py` script.   """;Natural Language Processing;https://github.com/helboukkouri/character-bert
"""--cpu           #: do not use GPU   --cpu           #: do not use GPU   Run VQ-VAE-2 training using the config `task_name` found in `hps.py`. Defaults to `cifar10`: ``` python main-vqvae.py --task task_name ```  Evaluate VQ-VAE-2 from parameters `state_dict_path` on task `task_name`. Defaults to `cifar10`: ``` python main-vqvae.py --task task_name --load-path state_dict_path --evaluate ```  Other useful flags: ``` --no-save       #: disables saving of files during training --cpu           #: do not use GPU --batch-size    #: overrides batch size in cfg.py  useful for evaluating on larger batch size --no-tqdm       #: disable tqdm status bars --no-save       #: disables saving of files --no-amp        #: disables using native AMP (Automatic Mixed Precision) operations --save-jpg      #: save all images as jpg instead of png  useful for extreme resolutions ```   Run level `level` PixelSnail discrete prior training using the config `task_name` found in `hps.py` using latent dataset saved at path `latent_dataset.pt` and VQ-VAE `vqvae_path` to dequantize conditioning variables. Defaults to `cifar10`: ``` python main-pixelsnail.py latent_dataset.pt vqvae_path.pt level --task task_name ```  Other useful flags: ``` --cpu           #: do not use GPU --load-path     #: resume from saved state on disk --batch-size    #: overrides batch size in cfg.py  useful for evaluating on larger batch size --save-jpg      #: save all images as jpg instead of png  useful for extreme resolutions --no-tqdm       #: disable tqdm status bars --no-save       #: disables saving of files ```   """;Computer Vision;https://github.com/vvvm23/vqvae-2
"""Install PyTorch and ImageNet dataset following the [official PyTorch ImageNet training code](https://github.com/pytorch/examples/tree/master/imagenet).  This repo aims to be minimal modifications on that code. Check the modifications by: ``` diff main_densecl.py <(curl https://raw.githubusercontent.com/pytorch/examples/master/imagenet/main.py) diff main_lincls.py <(curl https://raw.githubusercontent.com/pytorch/examples/master/imagenet/main.py) ```    sh dist_train.sh [your imagenet-folder with train and val folders]     --pretrained [your checkpoint path]/checkpoint_0199.pth.tar \   <td align=""center""><a href=""https://github.com/CoinCheung/denseCL/releases/download/v0.0.1/checkpoint_0199.pth.tar"">download</a></td>   """;General;https://github.com/CoinCheung/denseCL
"""""";Natural Language Processing;https://github.com/facebookresearch/code-prediction-transformer
"""""";General;https://github.com/ewdowiak/Sicilian_Translator
"""""";Natural Language Processing;https://github.com/ewdowiak/Sicilian_Translator
"""``` import os import keras.backend as K  from data import DATA_SET_DIR from elmo.lm_generator import LMDataGenerator from elmo.model import ELMo  parameters = {     'multi_processing': False      'n_threads': 4      'cuDNN': True if len(K.tensorflow_backend._get_available_gpus()) else False      'train_dataset': 'wikitext-2/wiki.train.tokens'      'valid_dataset': 'wikitext-2/wiki.valid.tokens'      'test_dataset': 'wikitext-2/wiki.test.tokens'      'vocab': 'wikitext-2/wiki.vocab'      'vocab_size': 28914      'num_sampled': 1000      'charset_size': 262      'sentence_maxlen': 100      'token_maxlen': 50      'token_encoding': 'word'      'epochs': 10      'patience': 2      'batch_size': 1      'clip_value': 5      'cell_clip': 5      'proj_clip': 5      'lr': 0.2      'shuffle': True      'n_lstm_layers': 2      'n_highway_layers': 2      'cnn_filters': [[1  32]                      [2  32]                      [3  64]                      [4  128]                      [5  256]                      [6  512]                      [7  512]                     ]      'lstm_units_size': 400      'hidden_units_size': 200      'char_embedding_size': 16      'dropout_rate': 0.1      'word_dropout_rate': 0.05      'weight_tying': True  }  #: Set-up Generators train_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['train_dataset'])                                    os.path.join(DATA_SET_DIR  parameters['vocab'])                                    sentence_maxlen=parameters['sentence_maxlen']                                    token_maxlen=parameters['token_maxlen']                                    batch_size=parameters['batch_size']                                    shuffle=parameters['shuffle']                                    token_encoding=parameters['token_encoding'])  val_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['valid_dataset'])                                  os.path.join(DATA_SET_DIR  parameters['vocab'])                                  sentence_maxlen=parameters['sentence_maxlen']                                  token_maxlen=parameters['token_maxlen']                                  batch_size=parameters['batch_size']                                  shuffle=parameters['shuffle']                                  token_encoding=parameters['token_encoding'])  test_generator = LMDataGenerator(os.path.join(DATA_SET_DIR  parameters['test_dataset'])                                  os.path.join(DATA_SET_DIR  parameters['vocab'])                                  sentence_maxlen=parameters['sentence_maxlen']                                  token_maxlen=parameters['token_maxlen']                                  batch_size=parameters['batch_size']                                  shuffle=parameters['shuffle']                                  token_encoding=parameters['token_encoding'])  #: Compile ELMo elmo_model = ELMo(parameters) elmo_model.compile_elmo(print_summary=True)  #: Train ELMo elmo_model.train(train_data=train_generator  valid_data=val_generator)  #: Persist ELMo Bidirectional Language Model in disk elmo_model.save(sampled_softmax=False)  #: Evaluate Bidirectional Language Model elmo_model.evaluate(test_generator)  #: Build ELMo meta-model to deploy for production and persist in disk elmo_model.wrap_multi_elmo_encoder(print_summary=True  save=True)  ```   """;Natural Language Processing;https://github.com/yangrui123/Hidden
"""The training and evaluation scripts operate on datasets stored as multi-resolution TFRecords. Each dataset is represented by a directory containing the same image data in several resolutions to enable efficient streaming. There is a separate *.tfrecords file for each resolution  and if the dataset contains labels  they are stored in a separate file as well. By default  the scripts expect to find the datasets at `datasets/<NAME>/<NAME>-<RESOLUTION>.tfrecords`. The directory can be changed by editing [config.py](./config.py):  ``` result_dir = 'results' data_dir = 'datasets' cache_dir = 'cache' ```  To obtain the FFHQ dataset (`datasets/ffhq`)  please refer to the [Flickr-Faces-HQ repository](http://stylegan.xyz/ffhq).  To obtain the CelebA-HQ dataset (`datasets/celebahq`)  please refer to the [Progressive GAN repository](https://github.com/tkarras/progressive_growing_of_gans).  To obtain other datasets  including LSUN  please consult their corresponding project pages. The datasets can be converted to multi-resolution TFRecords using the provided [dataset_tool.py](./dataset_tool.py):  ``` > python dataset_tool.py create_lsun datasets/lsun-bedroom-full ~/lsun/bedroom_lmdb --resolution 256 > python dataset_tool.py create_lsun_wide datasets/lsun-car-512x384 ~/lsun/car_lmdb --width 512 --height 384 > python dataset_tool.py create_lsun datasets/lsun-cat-full ~/lsun/cat_lmdb --resolution 256 > python dataset_tool.py create_cifar10 datasets/cifar10 ~/cifar10 > python dataset_tool.py create_from_images datasets/custom-dataset ~/custom-images ```   More examples you can find in the Jupyter notebook   3) Then you can play with Jupyter notebook   """;Computer Vision;https://github.com/RudreshVeerkhare/StyleGan
"""All the Sentinel-2 images (Level-1C) we used were downloaded from the United States Geological Survey (USGS) EarthExplorer website and preprocessed to Bottom-Of-Atmosphere (BOA) reflectance Level-2A using the Sen2Cor plugin v2.8 and the Sentinel Application Platform (SNAP 7.0). The Multispectral Instrument (MSI) sensor provides 13 spectral bands  i.e.  four bands at 10-m (Blue  Green  Red  NIR)  six bands at 20-m (Vegetation Red Edge 1-3  Narrow NIR  SWIR 1-2)  and three atmospheric bands at 60-m spatial resolution. With the exception of the atmospheric bands  all 10 bands (i.e.  Sentinel band-2 3 4 5 6 7 8 8A 11 12)  were used in this study. Bands at 20-m resolution were resampled to 10-m via nearest sampling. A scene  classification map was generated for each image along with the Level-2A processing  which assigned pixels to clouds  cloud shadows  vegetation  soils/deserts  water  snow  etc. According to the scene classification map  low-quality observations belonging to clouds (including cirrus)  cloud-shadows  and snow were discarded when extracting the annual time series of each pixel.  Time series data is organized in *CSV* format. Each row of a csv file corresponds to an annual satellite observation  time series of a pixel. For each row  the storage order is [[observation data]  [DOY data]  [class label(optional)]]   where [observation data] is stored in the order of bands:  A pre-training time series is recorded like this:  <div align=""center"">   <img src=""fig/pretraining-data-organization.png"" width=""400""><br><br> </div>  A fine-tuning time series is recorded like this:  <div align=""center"">   <img src=""fig/finetuning-data-organization.png"" width=""400""><br><br> </div>  Note: [DOY data] record the acquisition Julian dates of the observations in a time series.  [class label] is not included in the pre-training data.    You can run the following Linux command for pre-training a SITS-BERT model on your own data.   You can run the following Linux command to run the experiment:   You can also run the following command to train a SITS-BERT model from scratch for comparison:   This implementation is based on the repository https://github.com/codertimo/BERT-pytorch  which is a Pytorch   """;Natural Language Processing;https://github.com/linlei1214/SITS-BERT
"""You can install this package from PyPI:  ```sh pip install gmlp-flax ```  Or directly from GitHub:  ```sh pip install --upgrade git+https://github.com/SauravMaheshkar/gMLP.git ```   conda env create --name &lt;env-name&gt; sauravmaheshkar/gmlp  conda activate &lt;env-name&gt;   ```python import jax from gmlp_flax import gMLP  random_key = jax.random.PRNGKey(0)  x = jax.random.randint(key=random_key  minval=0  maxval=20000  shape=(1  1000))  init_rngs = {""params"": random_key}  gMLP(num_tokens=20000  dim=512  depth=4).init(init_rngs x) ```   """;Computer Vision;https://github.com/SauravMaheshkar/gMLP
"""You can install this package from PyPI:  ```sh pip install gmlp-flax ```  Or directly from GitHub:  ```sh pip install --upgrade git+https://github.com/SauravMaheshkar/gMLP.git ```   conda env create --name &lt;env-name&gt; sauravmaheshkar/gmlp  conda activate &lt;env-name&gt;   ```python import jax from gmlp_flax import gMLP  random_key = jax.random.PRNGKey(0)  x = jax.random.randint(key=random_key  minval=0  maxval=20000  shape=(1  1000))  init_rngs = {""params"": random_key}  gMLP(num_tokens=20000  dim=512  depth=4).init(init_rngs x) ```   """;General;https://github.com/SauravMaheshkar/gMLP
"""Welcome to the GitHub repo for the introduction course in Introduction to Machine Learning at Uppsala University. This repo contains all necessary material and information for the course.   You can find a rough course plan with reading instructions here.  You can find the course schedule on TimeEdit here (search for course code 2ST122).   Watch the videos to get more indepth knowledge/understanding (however  optional)   """;Natural Language Processing;https://github.com/MansMeg/IntroML
"""""";General;https://github.com/chris-tng/semi-supervised-nlp
"""<br>The most likely classification is acceptable for the picture -> both get a 1<br>   |     Image    | Current accepted results (based on gt)                | TOP-5 results from NASNetLarge                                                             | |:------------:|-------------------------------------------------------|--------------------------------------------------------------------------------------------| | ![](https://drive.google.com/uc?export=view&id=14J8Lir-uKsqtujJF7GbJHduqPBLA_2dU)0AKZCRZA.png <br>| train  Train  industry  railway  Railway  sky  sunset | 'freight_car'  'electric_locomotive'  'passenger_car'  'trailer_truck'  'steam_locomotive' |    **NasNet Large** - [x] Brute force approach: finished (@Alex)    * Code https://github.com/asad-62/IVP-DNN/blob/main/full-res-nas-net_alex_291220.py    * Results https://github.com/asad-62/IVP-DNN/blob/main/results_29-12_nasnet-large.csv  - ~~[ ] PCA approach: doing until 03.01.2021 (@Asad  @Abhinav  @Muhammad)~~  **EfficientNetB7** - [X] Brute force approach: finished (@Alex)   * Code https://github.com/asad-62/IVP-DNN/blob/main/efficient-net_alex_291220.py    * Results https://github.com/asad-62/IVP-DNN/blob/main/results_29-12_effnetB7.csv - ~~[ ] PCA approach: doing until 03.01.2021 (@Asad  @Abhinav  @Muhammad)~~  **DenseNet 201** - [X] Brute force approach: finished (@Alex)   * Code https://github.com/asad-62/IVP-DNN/blob/main/dense-net_alex_291220.py    * Results https://github.com/asad-62/IVP-DNN/blob/main/results_29-12_densenet.csv  - ~~[ ] PCA approach: doing until 03.01.2021 (@Asad  @Abhinav  @Muhammad)~~    """;Computer Vision;https://github.com/asad-62/IVP-DNN
"""<br>The most likely classification is acceptable for the picture -> both get a 1<br>   |     Image    | Current accepted results (based on gt)                | TOP-5 results from NASNetLarge                                                             | |:------------:|-------------------------------------------------------|--------------------------------------------------------------------------------------------| | ![](https://drive.google.com/uc?export=view&id=14J8Lir-uKsqtujJF7GbJHduqPBLA_2dU)0AKZCRZA.png <br>| train  Train  industry  railway  Railway  sky  sunset | 'freight_car'  'electric_locomotive'  'passenger_car'  'trailer_truck'  'steam_locomotive' |    **NasNet Large** - [x] Brute force approach: finished (@Alex)    * Code https://github.com/asad-62/IVP-DNN/blob/main/full-res-nas-net_alex_291220.py    * Results https://github.com/asad-62/IVP-DNN/blob/main/results_29-12_nasnet-large.csv  - ~~[ ] PCA approach: doing until 03.01.2021 (@Asad  @Abhinav  @Muhammad)~~  **EfficientNetB7** - [X] Brute force approach: finished (@Alex)   * Code https://github.com/asad-62/IVP-DNN/blob/main/efficient-net_alex_291220.py    * Results https://github.com/asad-62/IVP-DNN/blob/main/results_29-12_effnetB7.csv - ~~[ ] PCA approach: doing until 03.01.2021 (@Asad  @Abhinav  @Muhammad)~~  **DenseNet 201** - [X] Brute force approach: finished (@Alex)   * Code https://github.com/asad-62/IVP-DNN/blob/main/dense-net_alex_291220.py    * Results https://github.com/asad-62/IVP-DNN/blob/main/results_29-12_densenet.csv  - ~~[ ] PCA approach: doing until 03.01.2021 (@Asad  @Abhinav  @Muhammad)~~    """;General;https://github.com/asad-62/IVP-DNN
"""This repository implements mulitple popular object detection algorithms  including Faster R-CNN  R-FCN  FPN  and our recently proposed Cascade R-CNN  on the MS-COCO and PASCAL VOC datasets. Multiple choices are available for backbone network  including AlexNet  VGG-Net and ResNet. It is written in C++ and powered by [Caffe](https://github.com/BVLC/caffe) deep learning toolbox.   [Cascade R-CNN](http://www.svcl.ucsd.edu/publications/conference/2018/cvpr/cascade-rcnn.pdf) is a multi-stage extension of the popular two-stage R-CNN object detection framework. The goal is to obtain high quality object detection  which can effectively reject close false positives. It consists of a sequence of detectors trained end-to-end with increasing IoU thresholds  to be sequentially more selective against close false positives. The output of a previous stage detector is forwarded to a later stage detector  and the detection results will be improved stage by stage. This idea can be applied to any detector based on the two-stage R-CNN framework  including Faster R-CNN  R-FCN  FPN  Mask R-CNN  etc  and reliable gains are available independently of baseline strength. A vanilla Cascade R-CNN on FPN detector of ResNet-101 backbone network  without any training or inference bells and whistles  achieved state-of-the-art results on the challenging MS-COCO dataset.   1. Clone the Cascade-RCNN repository  and we'll call the directory that you cloned Cascade-RCNN into `CASCADE_ROOT`     ```Shell     git clone https://github.com/zhaoweicai/cascade-rcnn.git     ```    2. Build Cascade-RCNN     ```Shell     cd $CASCADE_ROOT/     #: Follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make all -j 16      #: If you want to run Cascade-RCNN detection/evaluation demo  build MATLAB wrapper as well     make matcaffe     ```   Res50-RFCN-Cascade ¬† ¬† | 2 | 2 ¬†  |2e-3| 90k   |  9 hr | 51.8 | 78.5 | 57.1   If you already have a COCO/VOC copy but not as organized as below  you can simply create Symlinks to have the same directory structure.   Assumed that your local COCO dataset copy is at /your/path/to/coco  make sure it has the following directory structure:   Assumed that your local VOCdevkit copy is at /your/path/to/VOCdevkit  make sure it has the following directory structure:       cd $CASCADE_ROOT/data/      sh get_coco_data.sh  This will download the window files required for the experiments. You can also use the provided MATLAB scripts coco_window_file.m under $CASCADE_ROOT/data/coco/ to generate your own window files.       cd $CASCADE_ROOT/models/      sh fetch_vggnet.sh       cd $CASCADE_ROOT/examples/coco/vgg-12s-600-rpn-cascade/      sh train_detection.sh   cd $CASCADE_ROOT/examples/coco/  sh fetch_cascadercnn_models.sh    Once the models pretrained or trained by yourself are available  you can use the MATLAB script `run_cascadercnn_coco.m` to obtain the detection and evaluation results. Set the right dataset path and choose the model of your interest to test in the demo script. The default setting is for the pretrained model. The final detection results will be saved under `$CASCADE_ROOT/examples/coco/detections/` and the evaluation results will be saved under the model folder.  You also can run the shell script `test_coco_detection.sh` under each model folder for evalution  but it is not identical to the official evaluation. For publication  use the MATLAB script.   """;Computer Vision;https://github.com/zhaoweicai/cascade-rcnn
"""Ensure you have:  * Python >= 3.6 * [Pytorch 1 with CUDA](https://pytorch.org/)  Then install the rest with pip:  > pip install -r requirements.txt   2 - Generate Sentences with both models using:   """;Sequential;https://github.com/Rongjiehuang/Multiband-WaveRNN
"""Chrome extension   """;General;https://github.com/gfursin/browser-extension-for-reproducible-research
"""Chrome extension   """;Computer Vision;https://github.com/gfursin/browser-extension-for-reproducible-research
"""""";Natural Language Processing;https://github.com/HuihuiChyan/BJTUNLP_Practice2021
"""""";Natural Language Processing;https://github.com/Shonacw/Conversation_Analysis_Project
"""You have an audio recording  and you want to know where certain classes of sounds are.  SongExplorer is trained to recognize such words by manually giving it a few examples.  It will then automatically calculate the probability  over time  of when those words occur in all of your recordings.  Applications suitable for SongExplorer include quantifying the rate or pattern of words emitted by a particular species  distinguishing a recording of one species from another  and discerning whether individuals of the same species produce different song.  Underneath the hood is a deep convolutional neural network.  The input is the raw audio stream  and the output is a set of mutually-exclusive probability waveforms corresponding to each word of interest.  Training begins by first thresholding one of your recordings in the time- and frequency-domains to find sounds that exceed the ambient noise. These sounds are then clustered into similar categories for you to manually annotate with however many word labels naturally occur.  A classifier is then trained on this corpus of ground truth  and a new recording is analyzed by it.  The words it automatically finds are then clustered as before  but this time are displayed with predicted labels.  You manually correct the mistakes  both re-labeling words that it got wrong  as well as labeling words it missed.  These new annotations are added to the ground truth  and the process of retraining the classifier and analyzing and correcting new recordings is repeated until the desired accuracy is reached.        V.time_sigma_string.value = ""4 2""     V.time_smooth_ms_string.value = ""6.4""     V.frequency_n_ms_string.value = ""25.6""     V.frequency_nw_string.value = ""4""     V.frequency_p_string.value = ""0.1 1.0""     V.frequency_smooth_ms_string.value = ""25.6""       SongExplorer can be run on all three major platforms.  The installation procedure is different on each due to various support of the technologies used. We recommend using Singularity on Linux  and Docker on Microsoft Windows and Apple Macintosh.  Training your own classifier is fastest with an Nvidia graphics processing unit (GPU).  TensorFlow  the machine learning framework from Google that SongExplorer uses  supports Ubuntu  Windows and Mac.  The catch is that Nvidia (and hence TensorFlow) currently doesn't support GPUs on Macs.  So while using a pre-trained classifier would be fine on a Mac  because inference is just as fast on the CPU  training your own would take longer.  Docker  a popular container framework which provides an easy way to deploy software across platforms  supports Linux  Windows and Mac  but only supports GPUs on Linux.  Moreover  on Windows and Mac it runs within a heavy-weight virtual machine  and on all platforms it requires administrator privileges to both install and run.  Singularity is an alternative to Docker that does not require root access. For this reason it is required in certain high-performance computing (HPC) environments.  Currently it only natively supports Linux.  There is a version for Macs which uses a light-weight virtual machine  but it is not being actively developed anymore.  You can run Singularity on Windows within a virtual environment  like Docker does  but would have to set that up yourself.  As with Docker  GPUs are only accessible on Linux.  To use SongExplorer with a GPU on Windows one must install it manually  without the convenience of a container.  We're looking for volunteers to write a Conda recipe to make this easy.   provide your manual annotations.  These manual annotations will serve to   snippets signifies your computer terminal's command line.  Square brackets   Platform-specific installation instructions can be found at  Sylabs.  SongExplorer has been tested with version 3.4 on   On Linux you'll also need to install the CUDA and CUDNN drivers from   click the Download button  or equivalently use the command line (for which you   Platform-specific installation instructions can be found at   Prompt on Windows  or the Terminal on Mac  and download the SongExplorer image       -e SONGEXPLORER_BIN -h=`hostname` -p 5006:5006 ^       -e SONGEXPLORER_BIN -h=`hostname` -p 5006:5006 \   userid.  Optionally specify the current working directory with the -w flag.   definition (e.g.  ""%HOMEPATH%/songexplorer/configuration.pysh"" on Windows  or   $ $SONGEXPLORER_BIN cp /opt/songexplorer/configuration.pysh $PWD [%CD% on Windows]   Note that ""configuration.pysh"" must be a valid Python and Bash file.  Hence  the unusual "".pysh"" extension.   SongExplorer provides the option to use a GPU or not with the train_gpu variable.   train_{cpu gpu}_{ncpu_cores ngpu_cards ngigabytes_memory} variables specify   without a GPU.   underestimate the resources required  particularly memory consumption.  To make   Use the nvidia-smi command to similarly monitor the GPU card.  The same   the GPU cores.  Use the watch command to receive repeated updates (i.e.   | NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |   | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |   | Processes:                                                       GPU Memory |  |  GPU       PID   Type   Process name                             Usage      |   Set the SONGEXPLORER environment variable plus the songexplorer alias on both   cluster also requires that the cluster be configured to permit hosting a web   train_gpu_cluster_flags=""-n 2 -gpu 'num=1' -q gpu_rtx""   printed to the terminal.  In the output above this is ""arthurb-ws2:5006""  which   ""configuration.pysh"".  Now press DoIt!.  Output into the log directory are   fields.  Now train a classifier on your annotations using the Train button.   thresholds that one can use to achieve a specified precision-recall ratio.  Use           $PWD/groundtruth-data/round2 [%CD%/... on Windows]   precision-recall ratios using the Ethogram button.  Choose one of the   ""ckpt-*"".  You'll also need to specify which "".tf"" files to threshold using the   set you specify  click on Omit One.  Select the set as described above for   Take care to choose the correct one.   The following Bash code directly calls this script to make predictions on a set   The above workflow could also easily be performed in Julia  Python  Matlab  or   One can also supply your own tensorflow code that implements a   $ git clone https://github.com/JaneliaSciComp/SongExplorer.git  $ rm -rf songexplorer/.git  $ sudo singularity build -s songexplorer.img songexplorer/containers/singularity.def   $ sudo singularity build songexplorer.sif songexplorer.img   To build an image without GPU support  comment out the section titled ""install   your shell environment.  source_path in ""configuration.pysh"" must be set   $ cd songexplorer   well as the Linux Bash interfaces.  To run them  simply execute ""runtests.sh"":   Let's walk through the steps needed to train a classifier completely from scratch.  Recordings need to be monaural 16-bit little-endian PCM-encoded WAV files. They should all be sampled at the same rate  which can be anything.  For this tutorial we supply you with *Drosophila melanogaster* data sampled at 2500 Hz.  First  let's get some data bundled with SongExplorer into your home directory.      $ $SONGEXPLORER_BIN ls -1 /opt/songexplorer/data     20161207T102314_ch1-annotated-person1.csv     20161207T102314_ch1.wav*     20190122T093303a-7-annotated-person2.csv*     20190122T093303a-7-annotated-person3.csv*     20190122T093303a-7.wav*     20190122T132554a-14-annotated-person2.csv*     20190122T132554a-14-annotated-person3.csv*     20190122T132554a-14.wav*     Antigua_20110313095210_ch26.wav     my_frozen_graph_1k_0.pb*     PS_20130625111709_ch3-annotated-person1.csv     PS_20130625111709_ch3.wav*     vgg_labels.txt*      $ mkdir -p groundtruth-data/round1      $ $SONGEXPLORER_BIN cp /opt/songexplorer/data/PS_20130625111709_ch3.wav \           $PWD/groundtruth-data/round1 [%CD%/... on Windows]   corresponding stages in `kernel_sizes`.  See [LeCun *et al* (1989; Neural Computation)](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf).  * `dilate after` specifies the first layer  starting from zero  at which to start dilating the convolutional kernels.  See [Yu and Koltun (2016; arXiv)](https://arxiv.org/pdf/1511.07122.pdf).  * `stride after` specifies the first layer  starting from zero  at which to start striding the convolutional kernels by two.  * `connection` specifies whether to use identity bypasses  which can help models with many layers converge.  See [He  Zhang  Ren  and Sun (2015; arXiv](https://arxiv.org/abs/1512.03385).  * `dropout` is the fraction of hidden units on each forward pass to omit during training.  See [Srivastava  Hinton  *et al* (2014; J. Machine Learning Res.)](http://jmlr.org/papers/v15/srivastava14a.html).  There is also:  * `weights seed` specifies whether to randomize the initial weights or not.  A value of -1 results in different values for each fold.  They are also different each time you run `x-validate`.  Any other number results in a set of initial weights that is unique to that number across all folds and repeated runs.  * `batch seed` similarly specifies whether to randomize the order in which samples are drawn from the ground-truth data set during training.  A value of -1 results in a different order for each fold and run;  any other number results in a unique order specific to that number across folds and runs.  To perform a simple grid search for the optimal value for a particular hyperparameter  first choose how many folds you want to partition your ground-truth data into using `k-fold`.  Then set the hyperparameter of interest to the first value you want to try and choose a name for the `Logs Folder` such that its prefix will be shared across all of the hyperparameter values you plan to validate.  Suffix any additional hyperparameters of interest using underscores.  (For example  to search mini-batch and keep track of kernel size and feature maps  use ""mb-64_ks129_fm64"".)  If your models is small  use `models_per_job` in ""configuration.pysh"" to train multiple folds on a GPU. Click the `X-Validate` button and then `DoIt!`.  One classifier will be trained for each fold  using it as the validation set and the remaining folds for training.  Separate files and subdirectories are created in the `Logs Folder` that are suffixed by the fold number and the letter ""k"".  Plot overlayed training curves with the `Accuracy` button  as before.  Repeat the above procedure for each of remaining hyperparameter values you want to try (e.g. ""mb-128_ks129_fm64""  ""mb-256_ks129_fm64""  etc.).  Then use the `Compare` button to create a figure of the cross-validation data over the hyperparameter values  specifying the prefix that the logs folders have in common (""mb"" in this case). Output are three files:  * ""[suffix]-compare-confusion-matrices.pdf"" contains the summed confusion matrix for each of the values tested.  * ""[suffix]-compare-overall-params-speed.pdf"" plots the accuracy  number of trainable parameters  and training time for each model.  * ""[suffix]-compare-precision-recall.pdf"" shows the final error rates for each model and wanted word.       M.init(""configuration.pysh"")     V.init(None)     C.init(None)           run([""hetero""  ""start""  str(M.local_ncpu_cores)           str(M.local_ngpu_cards)  str(M.local_ngigabytes_memory)])       """;General;https://github.com/JaneliaSciComp/SongExplorer
"""You have an audio recording  and you want to know where certain classes of sounds are.  SongExplorer is trained to recognize such words by manually giving it a few examples.  It will then automatically calculate the probability  over time  of when those words occur in all of your recordings.  Applications suitable for SongExplorer include quantifying the rate or pattern of words emitted by a particular species  distinguishing a recording of one species from another  and discerning whether individuals of the same species produce different song.  Underneath the hood is a deep convolutional neural network.  The input is the raw audio stream  and the output is a set of mutually-exclusive probability waveforms corresponding to each word of interest.  Training begins by first thresholding one of your recordings in the time- and frequency-domains to find sounds that exceed the ambient noise. These sounds are then clustered into similar categories for you to manually annotate with however many word labels naturally occur.  A classifier is then trained on this corpus of ground truth  and a new recording is analyzed by it.  The words it automatically finds are then clustered as before  but this time are displayed with predicted labels.  You manually correct the mistakes  both re-labeling words that it got wrong  as well as labeling words it missed.  These new annotations are added to the ground truth  and the process of retraining the classifier and analyzing and correcting new recordings is repeated until the desired accuracy is reached.        V.time_sigma_string.value = ""4 2""     V.time_smooth_ms_string.value = ""6.4""     V.frequency_n_ms_string.value = ""25.6""     V.frequency_nw_string.value = ""4""     V.frequency_p_string.value = ""0.1 1.0""     V.frequency_smooth_ms_string.value = ""25.6""       SongExplorer can be run on all three major platforms.  The installation procedure is different on each due to various support of the technologies used. We recommend using Singularity on Linux  and Docker on Microsoft Windows and Apple Macintosh.  Training your own classifier is fastest with an Nvidia graphics processing unit (GPU).  TensorFlow  the machine learning framework from Google that SongExplorer uses  supports Ubuntu  Windows and Mac.  The catch is that Nvidia (and hence TensorFlow) currently doesn't support GPUs on Macs.  So while using a pre-trained classifier would be fine on a Mac  because inference is just as fast on the CPU  training your own would take longer.  Docker  a popular container framework which provides an easy way to deploy software across platforms  supports Linux  Windows and Mac  but only supports GPUs on Linux.  Moreover  on Windows and Mac it runs within a heavy-weight virtual machine  and on all platforms it requires administrator privileges to both install and run.  Singularity is an alternative to Docker that does not require root access. For this reason it is required in certain high-performance computing (HPC) environments.  Currently it only natively supports Linux.  There is a version for Macs which uses a light-weight virtual machine  but it is not being actively developed anymore.  You can run Singularity on Windows within a virtual environment  like Docker does  but would have to set that up yourself.  As with Docker  GPUs are only accessible on Linux.  To use SongExplorer with a GPU on Windows one must install it manually  without the convenience of a container.  We're looking for volunteers to write a Conda recipe to make this easy.   provide your manual annotations.  These manual annotations will serve to   snippets signifies your computer terminal's command line.  Square brackets   Platform-specific installation instructions can be found at  Sylabs.  SongExplorer has been tested with version 3.4 on   On Linux you'll also need to install the CUDA and CUDNN drivers from   click the Download button  or equivalently use the command line (for which you   Platform-specific installation instructions can be found at   Prompt on Windows  or the Terminal on Mac  and download the SongExplorer image       -e SONGEXPLORER_BIN -h=`hostname` -p 5006:5006 ^       -e SONGEXPLORER_BIN -h=`hostname` -p 5006:5006 \   userid.  Optionally specify the current working directory with the -w flag.   definition (e.g.  ""%HOMEPATH%/songexplorer/configuration.pysh"" on Windows  or   $ $SONGEXPLORER_BIN cp /opt/songexplorer/configuration.pysh $PWD [%CD% on Windows]   Note that ""configuration.pysh"" must be a valid Python and Bash file.  Hence  the unusual "".pysh"" extension.   SongExplorer provides the option to use a GPU or not with the train_gpu variable.   train_{cpu gpu}_{ncpu_cores ngpu_cards ngigabytes_memory} variables specify   without a GPU.   underestimate the resources required  particularly memory consumption.  To make   Use the nvidia-smi command to similarly monitor the GPU card.  The same   the GPU cores.  Use the watch command to receive repeated updates (i.e.   | NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |   | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |   | Processes:                                                       GPU Memory |  |  GPU       PID   Type   Process name                             Usage      |   Set the SONGEXPLORER environment variable plus the songexplorer alias on both   cluster also requires that the cluster be configured to permit hosting a web   train_gpu_cluster_flags=""-n 2 -gpu 'num=1' -q gpu_rtx""   printed to the terminal.  In the output above this is ""arthurb-ws2:5006""  which   ""configuration.pysh"".  Now press DoIt!.  Output into the log directory are   fields.  Now train a classifier on your annotations using the Train button.   thresholds that one can use to achieve a specified precision-recall ratio.  Use           $PWD/groundtruth-data/round2 [%CD%/... on Windows]   precision-recall ratios using the Ethogram button.  Choose one of the   ""ckpt-*"".  You'll also need to specify which "".tf"" files to threshold using the   set you specify  click on Omit One.  Select the set as described above for   Take care to choose the correct one.   The following Bash code directly calls this script to make predictions on a set   The above workflow could also easily be performed in Julia  Python  Matlab  or   One can also supply your own tensorflow code that implements a   $ git clone https://github.com/JaneliaSciComp/SongExplorer.git  $ rm -rf songexplorer/.git  $ sudo singularity build -s songexplorer.img songexplorer/containers/singularity.def   $ sudo singularity build songexplorer.sif songexplorer.img   To build an image without GPU support  comment out the section titled ""install   your shell environment.  source_path in ""configuration.pysh"" must be set   $ cd songexplorer   well as the Linux Bash interfaces.  To run them  simply execute ""runtests.sh"":   Let's walk through the steps needed to train a classifier completely from scratch.  Recordings need to be monaural 16-bit little-endian PCM-encoded WAV files. They should all be sampled at the same rate  which can be anything.  For this tutorial we supply you with *Drosophila melanogaster* data sampled at 2500 Hz.  First  let's get some data bundled with SongExplorer into your home directory.      $ $SONGEXPLORER_BIN ls -1 /opt/songexplorer/data     20161207T102314_ch1-annotated-person1.csv     20161207T102314_ch1.wav*     20190122T093303a-7-annotated-person2.csv*     20190122T093303a-7-annotated-person3.csv*     20190122T093303a-7.wav*     20190122T132554a-14-annotated-person2.csv*     20190122T132554a-14-annotated-person3.csv*     20190122T132554a-14.wav*     Antigua_20110313095210_ch26.wav     my_frozen_graph_1k_0.pb*     PS_20130625111709_ch3-annotated-person1.csv     PS_20130625111709_ch3.wav*     vgg_labels.txt*      $ mkdir -p groundtruth-data/round1      $ $SONGEXPLORER_BIN cp /opt/songexplorer/data/PS_20130625111709_ch3.wav \           $PWD/groundtruth-data/round1 [%CD%/... on Windows]   corresponding stages in `kernel_sizes`.  See [LeCun *et al* (1989; Neural Computation)](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf).  * `dilate after` specifies the first layer  starting from zero  at which to start dilating the convolutional kernels.  See [Yu and Koltun (2016; arXiv)](https://arxiv.org/pdf/1511.07122.pdf).  * `stride after` specifies the first layer  starting from zero  at which to start striding the convolutional kernels by two.  * `connection` specifies whether to use identity bypasses  which can help models with many layers converge.  See [He  Zhang  Ren  and Sun (2015; arXiv](https://arxiv.org/abs/1512.03385).  * `dropout` is the fraction of hidden units on each forward pass to omit during training.  See [Srivastava  Hinton  *et al* (2014; J. Machine Learning Res.)](http://jmlr.org/papers/v15/srivastava14a.html).  There is also:  * `weights seed` specifies whether to randomize the initial weights or not.  A value of -1 results in different values for each fold.  They are also different each time you run `x-validate`.  Any other number results in a set of initial weights that is unique to that number across all folds and repeated runs.  * `batch seed` similarly specifies whether to randomize the order in which samples are drawn from the ground-truth data set during training.  A value of -1 results in a different order for each fold and run;  any other number results in a unique order specific to that number across folds and runs.  To perform a simple grid search for the optimal value for a particular hyperparameter  first choose how many folds you want to partition your ground-truth data into using `k-fold`.  Then set the hyperparameter of interest to the first value you want to try and choose a name for the `Logs Folder` such that its prefix will be shared across all of the hyperparameter values you plan to validate.  Suffix any additional hyperparameters of interest using underscores.  (For example  to search mini-batch and keep track of kernel size and feature maps  use ""mb-64_ks129_fm64"".)  If your models is small  use `models_per_job` in ""configuration.pysh"" to train multiple folds on a GPU. Click the `X-Validate` button and then `DoIt!`.  One classifier will be trained for each fold  using it as the validation set and the remaining folds for training.  Separate files and subdirectories are created in the `Logs Folder` that are suffixed by the fold number and the letter ""k"".  Plot overlayed training curves with the `Accuracy` button  as before.  Repeat the above procedure for each of remaining hyperparameter values you want to try (e.g. ""mb-128_ks129_fm64""  ""mb-256_ks129_fm64""  etc.).  Then use the `Compare` button to create a figure of the cross-validation data over the hyperparameter values  specifying the prefix that the logs folders have in common (""mb"" in this case). Output are three files:  * ""[suffix]-compare-confusion-matrices.pdf"" contains the summed confusion matrix for each of the values tested.  * ""[suffix]-compare-overall-params-speed.pdf"" plots the accuracy  number of trainable parameters  and training time for each model.  * ""[suffix]-compare-precision-recall.pdf"" shows the final error rates for each model and wanted word.       M.init(""configuration.pysh"")     V.init(None)     C.init(None)           run([""hetero""  ""start""  str(M.local_ncpu_cores)           str(M.local_ngpu_cards)  str(M.local_ngigabytes_memory)])       """;Computer Vision;https://github.com/JaneliaSciComp/SongExplorer
"""""";Graphs;https://github.com/SpaceLearner/DisenGraphRep
"""The creates a directory ER_20spin/eco/network to store following.   """;Graphs;https://github.com/ilBarbara/BioRL
"""In this example  we will use [WikiText-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) to train and evaluate a language model. tfDlg provides a simple script to download the corpus from the official web site in the [example/wikitext](example/benchmark-wikitext) directory. Use the script [examples/benchmark-wikitext/get_wikitext.py](tfdlg/examples/benchmark-wikitext/get_wikitext.py) to download the data first.  ```sh $ python ../examples/benchmark-wikitext/get_wikitext.py 2_raw ```  This command downloads WikiText-2 consisting of raw level tokens. You can find the corpus under the `wikitext-2-raw` directory.  ```sh $ ls wikitext-2-raw wiki.test.raw  wiki.train.raw wiki.valid.raw ```   Prepare your environment with Python >= 3.8  < 3.9 first.  Then run `pip` to install this package from GitHub.  ```sh $ pip install git+https://github.com/colorfulscoop/tfdlg ```  You can run tests with [pytest](https://docs.pytest.org/en/stable/) to make sure your installtion succeeds.  ```sh $ pip install pytest==6.1.1 $ pytest tests/ ```  :memo: If you install tfDlg in a container environment  use the corresponded container.  | GPU use | Container | Command example | | --- | --- | --- | | Yes | tensorflow/tensorflow:2.4.1-gpu bash | `docker container run --gpus all -v $(pwd):/work -w /work --rm -it tensorflow/tensorflow:2.4.1-gpu bash` | | No | python:3.8.7-buster | `docker container run -v $(pwd):/work -w /work --rm -it python:3.8.7-buster bash` |   tfDlg provides two ways to use in ways of **script-based** and **package-based**.  tfDlg is a Python package to enable you to use all the functionalities from your Python scripts. This usual way to use tfDlg as a Python package is called a package-based usage here.  On the other hand  script-based utilizes the pacakge to provide fundamental scripts for training  evaluation and serving your models. In this viewpoint  script-based can be considered as examples of how to use tfDlg as a Python package.  Take a look at the script-based usage first.   Get scripts from GitHub first. Then install dependencies.  ```sh $ git clone https://github.com/colorfulscoop/tfdlg $ cd scripts $ pip install -r requirements.txt ```   As a basic usage  you first need to import config files and models from the package.  ```py from tfdlg.configs import GPT2SmallConfig from tfdlg.models import PreLNDecoder  config = GPT2SmallConfig() model = PreLNDecoder(config) ```  Then you can train here in the usual manner of training Tensorflow Keras models by `fit` method.  After training your model  `save_model` saves the model parameter as well as the model hyper parameters which are specified in `tfdlg.configs.Config` class.  ```py from tfdlg.utils import save_model save_model(""path/to/save/dir""  model  config) ```  `load_model` can be used to load your model from the directory where you saved it.  ```py from tfdlg.utils import load_model model = load_model(""path/to/save/dir"") ```  Check more details in the scripts which are used for script-based usage.   """;General;https://github.com/colorfulscoop/tfdlg
"""In this example  we will use [WikiText-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) to train and evaluate a language model. tfDlg provides a simple script to download the corpus from the official web site in the [example/wikitext](example/benchmark-wikitext) directory. Use the script [examples/benchmark-wikitext/get_wikitext.py](tfdlg/examples/benchmark-wikitext/get_wikitext.py) to download the data first.  ```sh $ python ../examples/benchmark-wikitext/get_wikitext.py 2_raw ```  This command downloads WikiText-2 consisting of raw level tokens. You can find the corpus under the `wikitext-2-raw` directory.  ```sh $ ls wikitext-2-raw wiki.test.raw  wiki.train.raw wiki.valid.raw ```   Prepare your environment with Python >= 3.8  < 3.9 first.  Then run `pip` to install this package from GitHub.  ```sh $ pip install git+https://github.com/colorfulscoop/tfdlg ```  You can run tests with [pytest](https://docs.pytest.org/en/stable/) to make sure your installtion succeeds.  ```sh $ pip install pytest==6.1.1 $ pytest tests/ ```  :memo: If you install tfDlg in a container environment  use the corresponded container.  | GPU use | Container | Command example | | --- | --- | --- | | Yes | tensorflow/tensorflow:2.4.1-gpu bash | `docker container run --gpus all -v $(pwd):/work -w /work --rm -it tensorflow/tensorflow:2.4.1-gpu bash` | | No | python:3.8.7-buster | `docker container run -v $(pwd):/work -w /work --rm -it python:3.8.7-buster bash` |   tfDlg provides two ways to use in ways of **script-based** and **package-based**.  tfDlg is a Python package to enable you to use all the functionalities from your Python scripts. This usual way to use tfDlg as a Python package is called a package-based usage here.  On the other hand  script-based utilizes the pacakge to provide fundamental scripts for training  evaluation and serving your models. In this viewpoint  script-based can be considered as examples of how to use tfDlg as a Python package.  Take a look at the script-based usage first.   Get scripts from GitHub first. Then install dependencies.  ```sh $ git clone https://github.com/colorfulscoop/tfdlg $ cd scripts $ pip install -r requirements.txt ```   As a basic usage  you first need to import config files and models from the package.  ```py from tfdlg.configs import GPT2SmallConfig from tfdlg.models import PreLNDecoder  config = GPT2SmallConfig() model = PreLNDecoder(config) ```  Then you can train here in the usual manner of training Tensorflow Keras models by `fit` method.  After training your model  `save_model` saves the model parameter as well as the model hyper parameters which are specified in `tfdlg.configs.Config` class.  ```py from tfdlg.utils import save_model save_model(""path/to/save/dir""  model  config) ```  `load_model` can be used to load your model from the directory where you saved it.  ```py from tfdlg.utils import load_model model = load_model(""path/to/save/dir"") ```  Check more details in the scripts which are used for script-based usage.   """;Natural Language Processing;https://github.com/colorfulscoop/tfdlg
"""""";Computer Vision;https://github.com/zhiggins11/Semantic-Segmentation
"""""";General;https://github.com/wondonghyeon/protest-detection-violence-estimation
"""""";Computer Vision;https://github.com/wondonghyeon/protest-detection-violence-estimation
"""     sh       cd ./data        sh       cd ./data        sh       cd ./utils   Commit your Changes (git commit -m 'Add some AmazingFeature')   """;Computer Vision;https://github.com/donshen/pointnet.phasedetection
"""Install PyTorch and download the ImageNet dataset following the [official PyTorch ImageNet training code](https://github.com/pytorch/examples/tree/master/imagenet). Similar to [MoCo](https://github.com/facebookresearch/moco)  the code release contains minimal modifications for both unsupervised pre-training and linear classification to that code.   In addition  install [apex](https://github.com/NVIDIA/apex) for the [LARS](https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py) implementation needed for linear classification.     --pretrained [your checkpoint path]/checkpoint_0099.pth.tar \   Settings for the above: 8 NVIDIA V100 GPUs  CUDA 10.1/CuDNN 7.6.5  PyTorch 1.7.0.   """;General;https://github.com/facebookresearch/simsiam
"""You can build the repo through the following commands: ``` $ git clone https://github.com/alexandrosstergiou/SoftPool.git $ cd SoftPool-master/pytorch $ make install --- (optional) --- $ make test ```    ImageNet weight can be downloaded from the following links:   You can load any of the 1D  2D or 3D variants after the installation with:  ```python import softpool_cuda from SoftPool import soft_pool1d  SoftPool1d from SoftPool import soft_pool2d  SoftPool2d from SoftPool import soft_pool3d  SoftPool3d ```  + `soft_poolxd`: Is a functional interface for SoftPool. + `SoftPoolxd`: Is the class-based version which created an object that can be referenced later in the code.   """;Computer Vision;https://github.com/alexandrosstergiou/SoftPool
"""Get started with disent by installing it with $pip install disent or cloning this repository.   are not available from pip install.   - A pytorch version of disentanglement_lib.   The following is a basic working example of disent that trains a BetaVAE with a cyclic beta schedule and evaluates the trained model with various metrics.  <details><summary><b>üíæ Basic Example</b></summary> <p>  ```python3 import os import pytorch_lightning as pl import torch from torch.utils.data import DataLoader  from disent.dataset import DisentDataset from disent.dataset.data import XYObjectData from disent.dataset.sampling import SingleSampler from disent.dataset.transform import ToImgTensorF32 from disent.frameworks.vae import BetaVae from disent.metrics import metric_dci from disent.metrics import metric_mig from disent.model import AutoEncoder from disent.model.ae import DecoderConv64 from disent.model.ae import EncoderConv64 from disent.schedule import CyclicSchedule  #: create the dataset & dataloaders #: - ToImgTensorF32 transforms images from numpy arrays to tensors and performs checks data = XYObjectData() dataset = DisentDataset(dataset=data  sampler=SingleSampler()  transform=ToImgTensorF32()) dataloader = DataLoader(dataset=dataset  batch_size=128  shuffle=True  num_workers=os.cpu_count())  #: create the BetaVAE model #: - adjusting the beta  learning rate  and representation size. module = BetaVae(   model=AutoEncoder(     #: z_multiplier is needed to output mu & logvar when parameterising normal distribution     encoder=EncoderConv64(x_shape=data.x_shape  z_size=10  z_multiplier=2)      decoder=DecoderConv64(x_shape=data.x_shape  z_size=10)    )    cfg=BetaVae.cfg(     optimizer='adam'      optimizer_kwargs=dict(lr=1e-3)      loss_reduction='mean_sum'      beta=4    ) )  #: cyclic schedule for target 'beta' in the config/cfg. The initial value from the #: config is saved and multiplied by the ratio from the schedule on each step. #: - based on: https://arxiv.org/abs/1903.10145 module.register_schedule(   'beta'  CyclicSchedule(     period=1024   #: repeat every: trainer.global_step % period   ) )  #: train model #: - for 2048 batches/steps trainer = pl.Trainer(   max_steps=2048  gpus=1 if torch.cuda.is_available() else None  logger=False  checkpoint_callback=False ) trainer.fit(module  dataloader)  #: compute disentanglement metrics #: - we cannot guarantee which device the representation is on #: - this will take a while to run get_repr = lambda x: module.encode(x.to(module.device))  metrics = {   **metric_dci(dataset  get_repr  num_train=1000  num_test=500  show_progress=True)    **metric_mig(dataset  get_repr  num_train=2000)  }  #: evaluate print('metrics:'  metrics) ```  </p> </details>  Visit the [docs](https://disent.dontpanic.sh) for more examples!    The entrypoint for basic experiments is `experiment/run.py`.  Some configuration will be required  but basic experiments can be adjusted by modifying the [Hydra Config 1.0](https://github.com/facebookresearch/hydra) files in `experiment/config` (Please note that hydra 1.1 is not yet supported).  Modifying the main `experiment/config/config.yaml` is all you need for most basic experiments. The main config file contains a defaults list with entries corresponding to yaml configuration files (config options) in the subfolders (config groups) in `experiment/config/<config_group>/<option>.yaml`.  <details><summary><b>üíæ Config Defaults Example</b></summary> <p>  ```yaml defaults:   #: data   - sampling: default__bb   - dataset: xyobject   - augment: none   #: system   - framework: adavae_os   - model: vae_conv64   #: training   - optimizer: adam   - schedule: beta_cyclic   - metrics: fast   - run_length: short   #: logs   - run_callbacks: vis   - run_logging: wandb   #: runtime   - run_location: local   - run_launcher: local   - run_action: train  #: <rest of config.yaml left out> ... ```  </p> </details>  Easily modify  any of these values to adjust how the basic experiment will be run. For example  change `framework: adavae` to `framework: betavae`  or change the dataset from `xyobject` to `shapes3d`. Add new options by adding new yaml files in the config group folders.  [Weights and Biases](https://docs.wandb.ai/quickstart) is supported by changing `run_logging: none` to `run_logging: wandb`. However  you will need to login from the command line. W&B logging supports visualisations of latent traversals.   ----------------------   """;Computer Vision;https://github.com/nmichlo/disent
"""---  >**Question answering**  (**QA**) is a computer science discipline within the fields of  [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval ""Information retrieval"")  and  [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing ""Natural language processing"")  (NLP)  which is concerned with building systems that automatically answer questions posed by humans in a  [natural language](https://en.wikipedia.org/wiki/Natural_language ""Natural language""). > >Source: [Wikipedia](https://en.wikipedia.org/wiki/Question_answering)  >**Extractive QA** is a popular task for natural language processing (NLP) research  where models must extract a short snippet from a document in order to answer a natural language question. > >  Source: [Facebook AI](https://ai.facebook.com/blog/research-in-brief-unsupervised-question-answering-by-cloze-translation/)    **Context-Based Question Answering (CBQA)** is an inference web-based Extractive QA search engine  mainly dependent on [Haystack](https://github.com/deepset-ai/haystack) and [Transformers](https://github.com/huggingface/transformers) library.  The CBQA application allows the user to add context and perform Question Answering(QA) in that context.  The main components in this application use [Haystack's](https://github.com/deepset-ai/haystack) core components   > - **FileConverter**: Extracts pure text from files (pdf  docx  pptx  html and many more). > -  **PreProcessor**: Cleans and splits texts into smaller chunks. > -   **DocumentStore**: Database storing the documents  metadata and vectors for our search. We recommend Elasticsearch or FAISS  but have also more light-weight options for fast prototyping (SQL or In-Memory). >   - **Retriever**: Fast algorithms that identify candidate documents for a given query from a large collection of documents. Retrievers narrow down the search space significantly and are therefore key for scalable QA. Haystack supports sparse methods (TF-IDF  BM25  custom Elasticsearch queries) and state of the art dense methods (e.g. sentence-transformers and Dense Passage Retrieval) > - **Reader**: Neural network (e.g. BERT or RoBERTA) that reads through texts in detail to find an answer. The Reader takes multiple passages of text as input and returns top-n answers. Models are trained via  [FARM](https://github.com/deepset-ai/FARM)  or [Transformers](https://github.com/huggingface/transformers)  on SQuAD like tasks. You can just load a pretrained model from  [Hugging Face's model hub](https://huggingface.co/models)  or fine-tune it on your own domain data. > >Source: [Haystack's Key Components docs](https://github.com/deepset-ai/haystack/#key-components)  In CBQA the allowed formats for adding the context are    - Textual Context (Using the TextBox field)  - File Uploads (.pdf  .txt  and .docx)  These contexts are uploaded to a temporary directory for each user for pre-processing and deletes after uploading them to **Elasticsearch ** which is the only **DocumentStore type** used in this system.  Each user has a separate Elasticsearch index to store the context documents. Using the **PreProcessor** and the **FileConverter** modules from [Haystack](https://github.com/deepset-ai/haystack)  the pre-processing and the extraction of text from context files is done.  **Elasticsearcher Retriever** is used in CBQA to retrieve relevant documents based on the search query.  **Transformers-based Readers** are used to extracting answers from the retrieved documents. The Readers used in CBQA are the pre-trained Transformers models hosted on the [Hugging Face's model hub](https://huggingface.co/models) .  Currently  CBQA provides the interface to perform QA in **English** and **French**  using four Transformers based models    - BERT  - RoBERTa  - DistilBERT  - CamemBERT  The interface provides an option to choose the inference device between CPU and GPU.  The output is in a tabular form containing the following headers   -   **Answers** (Extracted answers based on the question and context) -   **Context** (Specifies the context window  related to the answer) -   **Document Title** (Specifies the title of context file  related to the answer)   ---   The main dependencies required to run the CBQA application is in the  [requirements.txt](./requirements.txt)  To install the dependencies   1.  Create a Python virtual environment with Python version 3.7 and activate it 2.  Install dependency libraries using the following command in the terminal. ```console   $ pip install -r requirements.txt ```  Before executing the CBQA application  please start the Elasticsearch server. Elasticsearch is the DocumentStore type used in this application. To download and install the Elasticsearch  please check [here](https://www.elastic.co/downloads/elasticsearch).  In case you are using the docker environment  run Elasticsearch on docker using the following commands in the terminal. If you want to install the docker engine on your machine  please check [here](https://docs.docker.com/get-docker/). ```console $ docker pull docker.elastic.co/elasticsearch/elasticsearch:7.10.0 ```` ```console $ docker run -p 9200:9200 -e ""discovery.type=single-node"" docker.elastic.co/elasticsearch/elasticsearch:7.10.0 ```  Make sure the Elasticsearch server is running using the following command in the new terminal. ```console $ curl http://localhost:9200 ```  You should get a response like the one below. ```console { ""name"" : ""facddac422e8""  ""cluster_name"" : ""docker-cluster""  ""cluster_uuid"" : ""a4A3-yBhQdKBlpSDkpRelg""  ""version"" : { ""number"" : ""7.10.0""  ""build_flavor"" : ""default""  ""build_type"" : ""docker""  ""build_hash"" : ""51e9d6f22758d0374a0f3f5c6e8f3a7997850f96""  ""build_date"" : ""2020-11-09T21:30:33.964949Z""  ""build_snapshot"" : false  ""lucene_version"" : ""8.7.0""  ""minimum_wire_compatibility_version"" : ""6.8.0""  ""minimum_index_compatibility_version"" : ""6.0.0-beta1"" }  ""tagline"" : ""You Know  for Search"" } ```   After installing the dependency libraries and starting the Elasticsearch server  you are good to go.  - _To run the application using a WSGI server like [Gunicorn](https://gunicorn.org)._ Use the following command in the new terminal.   ```console   $ gunicorn -w 1 --threads 4 app:app   ```    This runs the application in Gunicorn server on [http://localhost:8000](http://localhost:8000/) with a single worker and 4 threads.  - _To run the application in the [flask](https://flask.palletsprojects.com/en/1.1.x/) development server (Not recommended using this in production)_. Use the following command in the new terminal.   ```console   $ python app.py   ```    Now the application will be running in the flask development server on [http://localhost/5000](http://localhost/5000).  In the application execution cases above  you should see the below statement (date and time will be different) in the terminal after the application has started. ```console User tracker thread started @  2021-03-04 18:25:20.803277 ```  The above statement means that the thread handling the user connection has started  and the application is ready to accept users.   **Note**: When performing QA using pre-trained models  at first use  the selected model gets downloaded from the [Hugging Face's model hub](https://huggingface.co/models) this may take a while depending on your internet speed. If you are re-starting the application while you are testing  make sure to remove the auto-created temporary user directories in the project and the user index on Elasticsearch (Will be fixed soon)    ---  Before starting the installation  clone this repository using the following commands in the terminal.  ```console $ git clone https://github.com/Karthik-Bhaskar/Context-Based-Question-Answering.git ````  ```console $ cd Context-Based-Question-Answering/ ```  You can get started using one of the two options    1. [Installation using pip](#installation-using-pip)  2. [Running as a docker container ](#running-as-a-docker-container )   Python (version 3.7)  Haystack  (version 0.7.0)  Transformers (version 4.3.1)   Flask  (version 1.1.2)  Gunicorn (version 20.0.4)  Bootstrap (version 4.5.3)  jQuery (version 3.5.1)   --- *Full demo video on YouTube.*  [![Alt text](https://img.youtube.com/vi/vqy5XmDwKMQ/0.jpg)](https://youtu.be/vqy5XmDwKMQ)   """;Natural Language Processing;https://github.com/Karthik-Bhaskar/Context-Based-Question-Answering
"""""";Computer Vision;https://github.com/FreddeFrallan/Multilingual-CLIP
"""LibriTTS train-clean-360 split tar.gz link   Following the format from NVIDIA/tacotron2  the metadata should be formatted as:   """;Computer Vision;https://github.com/mindslab-ai/univnet
"""""";Computer Vision;https://github.com/birosjh/pytorch_ssd
"""This repository is fully built upon the [OpenMMLab](https://openmmlab.com/) toolkits. For each individual task  the config and model files follow the same directory organization as [mmcls](https://github.com/open-mmlab/mmclassification)  [mmdet](https://github.com/open-mmlab/mmdetection)  and [mmseg](https://github.com/open-mmlab/mmsegmentation) respectively  so just copy-and-paste them to the corresponding locations to get started.  For example  in terms of evaluating detectors ```shell git clone https://github.com/open-mmlab/mmdetection #: and install  #: copy model files cp det/mmdet/models/backbones/* mmdetection/mmdet/models/backbones cp det/mmdet/models/necks/* mmdetection/mmdet/models/necks cp det/mmdet/models/dense_heads/* mmdetection/mmdet/models/dense_heads cp det/mmdet/models/roi_heads/* mmdetection/mmdet/models/roi_heads cp det/mmdet/models/roi_heads/mask_heads/* mmdetection/mmdet/models/roi_heads/mask_heads cp det/mmdet/models/utils/* mmdetection/mmdet/models/utils cp det/mmdet/datasets/* mmdetection/mmdet/datasets  #: copy config files cp det/configs/_base_/models/* mmdetection/configs/_base_/models cp det/configs/_base_/schedules/* mmdetection/configs/_base_/schedules cp det/configs/involution mmdetection/configs -r  #: evaluate checkpoints cd mmdetection bash tools/dist_test.sh ${CONFIG_FILE} ${CHECKPOINT_FILE} ${GPU_NUM} [--out ${RESULT_FILE}] [--eval ${EVAL_METRICS}] ```  For more detailed guidance  please refer to the original [mmcls](https://github.com/open-mmlab/mmclassification)  [mmdet](https://github.com/open-mmlab/mmdetection)  and [mmseg](https://github.com/open-mmlab/mmsegmentation) tutorials.  Currently  we provide an memory-efficient implementation of the involuton operator based on [CuPy](https://cupy.dev/). Please install this library in advance. A customized CUDA kernel would bring about further acceleration on the hardware. Any contribution from the community regarding this is welcomed!   """;Computer Vision;https://github.com/d-li14/involution
""" If you want to reproduce my imagenet pretrained models you need download ILSVRC2012 dataset and make sure the folder architecture as follows: ``` ILSVRC2012 | |-----train----1000 sub classes folders | |-----val------1000 sub classes folders Please make sure the same class has same class folder name in train and val folders. ```  If you want to reproduce my COCO pretrained models you need download COCO2017 dataset and make sure the folder architecture as follows: ``` COCO2017 | |-----annotations----all label jsons |                  |                |----train2017 |----images------|----val2017                  |----test2017 ```  If you want to reproduce my VOC pretrained models you need download VOC2007+VOC2012 dataset and make sure the folder architecture as follows: ``` VOCdataset |                 |----Annotations |                 |----ImageSets |----VOC2007------|----JPEGImages |                 |----SegmentationClass |                 |----SegmentationObject |         |                 |----Annotations |                 |----ImageSets |----VOC2012------|----JPEGImages |                 |----SegmentationClass |                 |----SegmentationObject ```   If you want to reproduce my model you need enter a training folder directory then run train.sh and test.sh.  For example you can enter classification_training/imagenet/resnet_vovnet_darknet_example.   """;Computer Vision;https://github.com/zgcr/simpleAICV-pytorch-ImageNet-COCO-training
"""Thanks to Kaggle and a lot of amazing data enthusiasm people sharing their notebooks so I had a chance to learn Transformer and really use it to a real-world task!         Saint+ is a **Transformer** based knowledge-tracing model which takes students' exercise history information to predict future performance. As classical Transformer  it has an Encoder-Decoder structure that Encoder applied self-attention to a stream of exercise embeddings; Decoder applied self-attention to responses embeddings and encoder-decoder attention to encoder output.  git clone https://github.com/Chang-Chia-Chi/SaintPlus-Knowledge-Tracing-Pytorch.git      cd SaintPlus-Knowledge-Tracing-Pytorch      """;General;https://github.com/Chang-Chia-Chi/SaintPlus-Knowledge-Tracing-Pytorch
"""Thanks to Kaggle and a lot of amazing data enthusiasm people sharing their notebooks so I had a chance to learn Transformer and really use it to a real-world task!         Saint+ is a **Transformer** based knowledge-tracing model which takes students' exercise history information to predict future performance. As classical Transformer  it has an Encoder-Decoder structure that Encoder applied self-attention to a stream of exercise embeddings; Decoder applied self-attention to responses embeddings and encoder-decoder attention to encoder output.  git clone https://github.com/Chang-Chia-Chi/SaintPlus-Knowledge-Tracing-Pytorch.git      cd SaintPlus-Knowledge-Tracing-Pytorch      """;Natural Language Processing;https://github.com/Chang-Chia-Chi/SaintPlus-Knowledge-Tracing-Pytorch
"""We use datasets in LDMB format for faster IO speed. Please refer to [wiki](https://github.com/xinntao/EDVR/wiki/Prepare-datasets-in-LMDB-format) for more details.   - Python 3 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux)) - [PyTorch >= 1.0](https://pytorch.org/) - NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads) - [Deformable Convolution](https://arxiv.org/abs/1703.06211). We use [Charles Shang](https://github.com/CharlesShang)'s [DCNv2](https://github.com/CharlesShang/DCNv2) implementation. Please first compile it.    ```   cd ./codes/models/modules/DCNv2   bash make.sh   ``` - Python packages: `pip install numpy opencv-python lmdb pyyaml` - TensorBoard:    - PyTorch >= 1.1: `pip install tb-nightly future`   - PyTorch == 1.0: `pip install tensorboardX`   Please see [wiki](https://github.com/xinntao/EDVR/wiki/Testing-and-Training) for the basic usage  *i.e. * training and testing.  """;Computer Vision;https://github.com/zhusiling/EDVR
"""""";Computer Vision;https://github.com/devanshpratapsingh/U-net_for_Multiclass_Segmentation
"""``` yolo.ai ‚îú‚îÄ‚îÄ cfg                 #: The directory where model config file is located (darknet  efficientnet  etc) ‚îú‚îÄ‚îÄ tests               #: Implmentation test cases ‚îú‚îÄ‚îÄ yolo                #: YOLO implementation code bases ‚îÇ   ‚îú‚îÄ‚îÄ data            #: Base data directory ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasets    #: Contain datasets such as pascal-voc ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transforms  #: Custom transforms for Dataset ‚îÇ   ‚îú‚îÄ‚îÄ models          #: Base model directory ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ arch        #: YOLO model assembly place ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backbones   #: All backbone network gathering here ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectors   #: Assembly of all types of detectors ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ losses      #: The gathering place of all loss functions ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics     #: Metrics functions for bounding boxes and losses ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ modules     #: Individuals modules for network building ‚îÇ   ‚îî‚îÄ‚îÄ utils           #: Utilites file for visualization and network ```  """;Computer Vision;https://github.com/DavianYang/yolo.ai
"""""";General;https://github.com/sjoshi804/hierarchical-darts-neural-architecture-search
"""Datasets are stored as uncompressed ZIP archives containing uncompressed PNG files and a metadata file `dataset.json` for labels.  Custom datasets can be created from a folder containing images; see [`python dataset_tool.py --help`](./docs/dataset-tool-help.txt) for more information. Alternatively  the folder can also be used directly as a dataset  without running it through `dataset_tool.py` first  but doing so may lead to suboptimal performance.  Legacy TFRecords datasets are not supported &mdash; see below for instructions on how to convert them.  **FFHQ**:  Step 1: Download the [Flickr-Faces-HQ dataset](https://github.com/NVlabs/ffhq-dataset) as TFRecords.  Step 2: Extract images from TFRecords using `dataset_tool.py` from the [TensorFlow version of StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada/):  ```.bash #: Using dataset_tool.py from TensorFlow version at #: https://github.com/NVlabs/stylegan2-ada/ python ../stylegan2-ada/dataset_tool.py unpack \     --tfrecord_dir=~/ffhq-dataset/tfrecords/ffhq --output_dir=/tmp/ffhq-unpacked ```  Step 3: Create ZIP archive using `dataset_tool.py` from this repository:  ```.bash #: Original 1024x1024 resolution. python dataset_tool.py --source=/tmp/ffhq-unpacked --dest=~/datasets/ffhq.zip  #: Scaled down 256x256 resolution. python dataset_tool.py --source=/tmp/ffhq-unpacked --dest=~/datasets/ffhq256x256.zip \     --width=256 --height=256 ```  **MetFaces**: Download the [MetFaces dataset](https://github.com/NVlabs/metfaces-dataset) and create ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/metfaces/images --dest=~/datasets/metfaces.zip ```  **AFHQ**: Download the [AFHQ dataset](https://github.com/clovaai/stargan-v2/blob/master/README.md#animal-faces-hq-dataset-afhq) and create ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/afhq/train/cat --dest=~/datasets/afhqcat.zip python dataset_tool.py --source=~/downloads/afhq/train/dog --dest=~/datasets/afhqdog.zip python dataset_tool.py --source=~/downloads/afhq/train/wild --dest=~/datasets/afhqwild.zip ```  **CIFAR-10**: Download the [CIFAR-10 python version](https://www.cs.toronto.edu/~kriz/cifar.html) and convert to ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/cifar-10-python.tar.gz --dest=~/datasets/cifar10.zip ```  **LSUN**: Download the desired categories from the [LSUN project page](https://www.yf.io/p/lsun/) and convert to ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/lsun/raw/cat_lmdb --dest=~/datasets/lsuncat200k.zip \     --transform=center-crop --width=256 --height=256 --max_images=200000  python dataset_tool.py --source=~/downloads/lsun/raw/car_lmdb --dest=~/datasets/lsuncar200k.zip \     --transform=center-crop-wide --width=512 --height=384 --max_images=200000 ```  **BreCaHAD**:  Step 1: Download the [BreCaHAD dataset](https://figshare.com/articles/BreCaHAD_A_Dataset_for_Breast_Cancer_Histopathological_Annotation_and_Diagnosis/7379186).  Step 2: Extract 512x512 resolution crops using `dataset_tool.py` from the [TensorFlow version of StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada/):  ```.bash #: Using dataset_tool.py from TensorFlow version at #: https://github.com/NVlabs/stylegan2-ada/ python dataset_tool.py extract_brecahad_crops --cropsize=512 \     --output_dir=/tmp/brecahad-crops --brecahad_dir=~/downloads/brecahad/images ```  Step 3: Create ZIP archive using `dataset_tool.py` from this repository:  ```.bash python dataset_tool.py --source=/tmp/brecahad-crops --dest=~/datasets/brecahad.zip ```   * GPU memory usage is comparable to the TensorFlow version.   | stylegan2-ada-pytorch | Main directory hosted on Amazon S3       --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl       --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl   The above code requires torch_utils and dnnlib to be accessible via PYTHONPATH. It does not need source code for the networks themselves &mdash; their class definitions are loaded from the pickle via torch_utils.persistence.   The name of the output directory reflects the training configuration. For example  00000-mydataset-auto1 indicates that the base configuration was auto1  meaning that the hyperparameters were selected automatically for training on one GPU. The base configuration is controlled by --cfg:       --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl   We employ the following metrics in the ADA paper. Execution time and GPU memory usage is reported for one NVIDIA Tesla V100 GPU at 1024x1024 resolution:   Pre-trained networks are stored as `*.pkl` files that can be referenced using local filenames or URLs:  ```.bash #: Generate curated MetFaces images without truncation (Fig.10 left) python generate.py --outdir=out --trunc=1 --seeds=85 265 297 849 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl  #: Generate uncurated MetFaces images with truncation (Fig.12 upper left) python generate.py --outdir=out --trunc=0.7 --seeds=600-605 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl  #: Generate class conditional CIFAR-10 images (Fig.17 left  Car) python generate.py --outdir=out --seeds=0-35 --class=1 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl  #: Style mixing example python style_mixing.py --outdir=out --rows=85 100 75 458 1500 --cols=55 821 1789 293 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl ```  Outputs from the above commands are placed under `out/*.png`  controlled by `--outdir`. Downloaded network pickles are cached under `$HOME/.cache/dnnlib`  which can be overridden by setting the `DNNLIB_CACHE_DIR` environment variable. The default PyTorch extension build directory is `$HOME/.cache/torch_extensions`  which can be overridden by setting `TORCH_EXTENSIONS_DIR`.  **Docker**: You can run the above curated image example using Docker as follows:  ```.bash docker build --tag sg2ada:latest . ./docker_run.sh python3 generate.py --outdir=out --trunc=1 --seeds=85 265 297 849 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl ```  Note: The Docker image requires NVIDIA driver release `r455.23` or later.  **Legacy networks**: The above commands can load most of the network pickles created using the previous TensorFlow versions of StyleGAN2 and StyleGAN2-ADA. However  for future compatibility  we recommend converting such legacy pickles into the new format used by the PyTorch version:  ```.bash python legacy.py \     --source=https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl \     --dest=stylegan2-cat-config-f.pkl ```   """;General;https://github.com/XieBaoshi/stylegan2
"""Datasets are stored as uncompressed ZIP archives containing uncompressed PNG files and a metadata file `dataset.json` for labels.  Custom datasets can be created from a folder containing images; see [`python dataset_tool.py --help`](./docs/dataset-tool-help.txt) for more information. Alternatively  the folder can also be used directly as a dataset  without running it through `dataset_tool.py` first  but doing so may lead to suboptimal performance.  Legacy TFRecords datasets are not supported &mdash; see below for instructions on how to convert them.  **FFHQ**:  Step 1: Download the [Flickr-Faces-HQ dataset](https://github.com/NVlabs/ffhq-dataset) as TFRecords.  Step 2: Extract images from TFRecords using `dataset_tool.py` from the [TensorFlow version of StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada/):  ```.bash #: Using dataset_tool.py from TensorFlow version at #: https://github.com/NVlabs/stylegan2-ada/ python ../stylegan2-ada/dataset_tool.py unpack \     --tfrecord_dir=~/ffhq-dataset/tfrecords/ffhq --output_dir=/tmp/ffhq-unpacked ```  Step 3: Create ZIP archive using `dataset_tool.py` from this repository:  ```.bash #: Original 1024x1024 resolution. python dataset_tool.py --source=/tmp/ffhq-unpacked --dest=~/datasets/ffhq.zip  #: Scaled down 256x256 resolution. python dataset_tool.py --source=/tmp/ffhq-unpacked --dest=~/datasets/ffhq256x256.zip \     --width=256 --height=256 ```  **MetFaces**: Download the [MetFaces dataset](https://github.com/NVlabs/metfaces-dataset) and create ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/metfaces/images --dest=~/datasets/metfaces.zip ```  **AFHQ**: Download the [AFHQ dataset](https://github.com/clovaai/stargan-v2/blob/master/README.md#animal-faces-hq-dataset-afhq) and create ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/afhq/train/cat --dest=~/datasets/afhqcat.zip python dataset_tool.py --source=~/downloads/afhq/train/dog --dest=~/datasets/afhqdog.zip python dataset_tool.py --source=~/downloads/afhq/train/wild --dest=~/datasets/afhqwild.zip ```  **CIFAR-10**: Download the [CIFAR-10 python version](https://www.cs.toronto.edu/~kriz/cifar.html) and convert to ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/cifar-10-python.tar.gz --dest=~/datasets/cifar10.zip ```  **LSUN**: Download the desired categories from the [LSUN project page](https://www.yf.io/p/lsun/) and convert to ZIP archive:  ```.bash python dataset_tool.py --source=~/downloads/lsun/raw/cat_lmdb --dest=~/datasets/lsuncat200k.zip \     --transform=center-crop --width=256 --height=256 --max_images=200000  python dataset_tool.py --source=~/downloads/lsun/raw/car_lmdb --dest=~/datasets/lsuncar200k.zip \     --transform=center-crop-wide --width=512 --height=384 --max_images=200000 ```  **BreCaHAD**:  Step 1: Download the [BreCaHAD dataset](https://figshare.com/articles/BreCaHAD_A_Dataset_for_Breast_Cancer_Histopathological_Annotation_and_Diagnosis/7379186).  Step 2: Extract 512x512 resolution crops using `dataset_tool.py` from the [TensorFlow version of StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada/):  ```.bash #: Using dataset_tool.py from TensorFlow version at #: https://github.com/NVlabs/stylegan2-ada/ python dataset_tool.py extract_brecahad_crops --cropsize=512 \     --output_dir=/tmp/brecahad-crops --brecahad_dir=~/downloads/brecahad/images ```  Step 3: Create ZIP archive using `dataset_tool.py` from this repository:  ```.bash python dataset_tool.py --source=/tmp/brecahad-crops --dest=~/datasets/brecahad.zip ```   * GPU memory usage is comparable to the TensorFlow version.   | stylegan2-ada-pytorch | Main directory hosted on Amazon S3       --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl       --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl   The above code requires torch_utils and dnnlib to be accessible via PYTHONPATH. It does not need source code for the networks themselves &mdash; their class definitions are loaded from the pickle via torch_utils.persistence.   The name of the output directory reflects the training configuration. For example  00000-mydataset-auto1 indicates that the base configuration was auto1  meaning that the hyperparameters were selected automatically for training on one GPU. The base configuration is controlled by --cfg:       --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl   We employ the following metrics in the ADA paper. Execution time and GPU memory usage is reported for one NVIDIA Tesla V100 GPU at 1024x1024 resolution:   Pre-trained networks are stored as `*.pkl` files that can be referenced using local filenames or URLs:  ```.bash #: Generate curated MetFaces images without truncation (Fig.10 left) python generate.py --outdir=out --trunc=1 --seeds=85 265 297 849 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl  #: Generate uncurated MetFaces images with truncation (Fig.12 upper left) python generate.py --outdir=out --trunc=0.7 --seeds=600-605 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl  #: Generate class conditional CIFAR-10 images (Fig.17 left  Car) python generate.py --outdir=out --seeds=0-35 --class=1 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl  #: Style mixing example python style_mixing.py --outdir=out --rows=85 100 75 458 1500 --cols=55 821 1789 293 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl ```  Outputs from the above commands are placed under `out/*.png`  controlled by `--outdir`. Downloaded network pickles are cached under `$HOME/.cache/dnnlib`  which can be overridden by setting the `DNNLIB_CACHE_DIR` environment variable. The default PyTorch extension build directory is `$HOME/.cache/torch_extensions`  which can be overridden by setting `TORCH_EXTENSIONS_DIR`.  **Docker**: You can run the above curated image example using Docker as follows:  ```.bash docker build --tag sg2ada:latest . ./docker_run.sh python3 generate.py --outdir=out --trunc=1 --seeds=85 265 297 849 \     --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl ```  Note: The Docker image requires NVIDIA driver release `r455.23` or later.  **Legacy networks**: The above commands can load most of the network pickles created using the previous TensorFlow versions of StyleGAN2 and StyleGAN2-ADA. However  for future compatibility  we recommend converting such legacy pickles into the new format used by the PyTorch version:  ```.bash python legacy.py \     --source=https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl \     --dest=stylegan2-cat-config-f.pkl ```   """;Computer Vision;https://github.com/XieBaoshi/stylegan2
"""""";Reinforcement Learning;https://github.com/Medabid1/RL_Project
"""git clone https://github.com/NVIDIA/trt-samples-for-hackathon-cn.git  cd build   """;General;https://github.com/trthackthonFighters/ConvBert
"""git clone https://github.com/NVIDIA/trt-samples-for-hackathon-cn.git  cd build   """;Natural Language Processing;https://github.com/trthackthonFighters/ConvBert
"""git clone https://github.com/NVIDIA/trt-samples-for-hackathon-cn.git  cd build   """;Sequential;https://github.com/trthackthonFighters/ConvBert
"""__[step 1.] Prepare dataset__    The author of progressive GAN released CelebA-HQ dataset  and which Nash is working on over on the branch that i forked this from. For my version just make sure that all images are the children of that folder that you declare in Config.py. Also i warn you that if you use multiple classes  they should be similar as to not end up with attrocities.   ~~~ --------------------------------------------- The training data folder should look like :  <train_data_root>                 |--Your Folder                         |--image 1                         |--image 2                         |--image 3 ... --------------------------------------------- ~~~  __[step 2.] Prepare environment using virtualenv__      + you can easily set PyTorch (v0.3) and TensorFlow environment using virtualenv.   + CAUTION: if you have trouble installing PyTorch  install it mansually using pip. [[PyTorch Install]](http://pytorch.org/)   + For install please take your time and install all dependencies of PyTorch and also install tensorflow      ~~~   $ virtualenv --python=python2.7 venv   $ . venv/bin/activate   $ pip install -r requirements.txt   $ conda install pytorch torchvision -c pytorch   ~~~    __[step 3.] Run training__       + edit `config.py` to change parameters. (don't forget to change path to training images) + specify which gpu devices to be used  and change ""n_gpu"" option in `config.py` to support Multi-GPU training. + run and enjoy!    ~~~~   (example)   If using Single-GPU (device_id = 0):   $ vim config.py   -->   change ""n_gpu=1""  ¬†$ CUDA_VISIBLE_DEVICES=0 python trainer.py      If using Multi-GPUs (device id = 1 3 7):   $ vim config.py   -->   change ""n_gpu=3""  ¬†$ CUDA_VISIBLE_DEVICES=1 3 7 python trainer.py ~~~~      __[step 4.] Display on tensorboard__   (At the moment skip this part) + you can check the results on tensorboard.  <p align=""center""><img src=""https://puu.sh/ympU0/c38f4e7d33.png"" width=""700""></p>    <p align=""center""><img src=""https://puu.sh/ympUe/bf9b53dea8.png"" width=""700"" align=""center""></p>       ~~~   $ tensorboard --logdir repo/tensorboard --port 8888   $ <host_ip>:8888 at your browser.   ~~~       __[step 5.] Generate fake images using linear interpolation__    ~~~ CUDA_VISIBLE_DEVICES=0 python generate_interpolated.py ~~~        """;Computer Vision;https://github.com/nashory/pggan-pytorch
"""Manual: https://github.com/AlexeyAB/darknet/wiki   Requirements (and how to install dependencies)   Requirements for Windows  Linux and macOS   How to compile on Linux/macOS (using CMake)   How to compile on Linux (using make)  How to compile on Windows (using CMake)  How to compile on Windows (using vcpkg)  How to train with multi-GPU   tkDNN: https://github.com/ceccocats/tkDNN  OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile   - `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   - `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/  Pytorch - Scaled-YOLOv4: https://github.com/WongKinYiu/ScaledYOLOv4  TensorFlow: pip install yolov4 YOLOv4 on TensorFlow 2.0 / TFlite / Android: https://github.com/hunglc007/tensorflow-yolov4-tflite   OpenCV the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example  Intel OpenVINO 2021.2: supports YOLOv4 (NPU Myriad X / USB Neural Compute Stick / Arria FPGA): https://devmesh.intel.com/projects/openvino-yolov4-49c756 read this manual (old manual ) (for Scaled-YOLOv4 models use https://github.com/Chen-MingChang/pytorch_YOLO_OpenVINO_demo )  PyTorch > ONNX:   DirectML https://github.com/microsoft/DirectML/tree/master/Samples/yolov4  OpenCL (Intel  AMD  Mali GPUs for macOS & GNU/Linux) https://github.com/sowson/darknet   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like CUDA  cudnn  ZED and build against those. It will also create a shared object library file to use darknet for code development.  To update CMake on Ubuntu  it's better to follow guide here: https://apt.kitware.com/ or https://cmake.org/download/  git clone https://github.com/AlexeyAB/darknet  cd darknet  mkdir build_release  cd build_release   cmake --build . --target install --parallel 8  Install: Cmake  CUDA  cuDNN How to install dependencies  Install powershell for your OS (Linux or MacOS) (guide here).   git clone https://github.com/AlexeyAB/darknet  cd darknet  ./build.ps1 -UseVCPKG -EnableOPENCV -EnableCUDA -EnableCUDNN   remove option -UseVCPKG if you plan to manually provide OpenCV library to darknet or if you do not want to enable OpenCV integration  add option -EnableOPENCV_CUDA if you want to build OpenCV with CUDA support - very slow to build! (requires -UseVCPKG)  If you open the build.ps1 script at the beginning you will find all available switches.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   You also need to specify for which graphics card the code is generated. This is done by setting ARCH=. If you use a never version than CUDA 11 you further need to edit line 20 from Makefile and remove -gencode arch=compute_30 code=sm_30 \ as Kepler GPU support was dropped in CUDA 11. You can also drop the general ARCH= and just uncomment ARCH= for your graphics card.   MSVC: https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community  CMake GUI: Windows win64-x64 Installerhttps://cmake.org/download/   find the executable file darknet.exe in the output path to the binaries you specified  This is the recommended approach to build Darknet on Windows.  Install Visual Studio 2017 or 2019. In case you need to download it  please go here: Visual Studio Community. Remember to install English language pack  this is mandatory for vcpkg!  Install CUDA enabling VS Integration during installation.   git clone https://github.com/AlexeyAB/darknet  cd darknet  .\build.ps1 -UseVCPKG -EnableOPENCV -EnableCUDA -EnableCUDNN  (add option -EnableOPENCV_CUDA if you want to build OpenCV with CUDA support - very slow to build! - or remove options like -EnableCUDA or -EnableCUDNN if you are not interested in them). If you open the build.ps1 script at the beginning you will find all available switches.  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   in Python: https://github.com/tzutalin/labelImg  in Python: https://github.com/Cartucho/OpenLabeling   in JavaScript: https://github.com/opencv/cvat   If you use `build.ps1` script or the makefile (Linux only) you will find `darknet` in the root directory.  If you use the deprecated Visual Studio solutions  you will find `darknet` in the directory `\build\darknet\x64`.  If you customize build with CMake GUI  darknet executable will be installed in your preferred folder.  - Yolo v4 COCO - **image**: `./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` - **Output coordinates** of objects: `./darknet detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` - Yolo v4 COCO - **video**: `./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` - Yolo v4 COCO - **WebCam 0**: `./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` - Yolo v4 COCO for **net-videocam** - Smart WebCam: `./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` - Yolo v4 - **save result videofile res.avi**: `./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` - Yolo v3 **Tiny** COCO - video: `./darknet detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` - **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` - Yolo v3 Tiny **on GPU #1**: `./darknet detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` - Alternative method Yolo v3 COCO - image: `./darknet detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` - Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):     `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` - 186 MB Yolo9000 - image: `./darknet detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` - Remember to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app - To process a list of images `data/train.txt` and save results of detection to `result.json` file use:     `./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` - To process a list of images `data/train.txt` and save results of detection to `result.txt` use:     `./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` - Pseudo-labelling - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` - To calculate anchors: `./darknet detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` - To check accuracy mAP@IoU=50: `./darknet detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` - To check accuracy mAP@IoU=75: `./darknet detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   - on Linux   - using `build.sh` or   - build `darknet` using `cmake` or   - set `LIBSO=1` in the `Makefile` and do `make` - on Windows   - using `build.ps1` or   - build `darknet` using `cmake` or   - compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs:  - C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h   - Python examples using the C API:     - https://github.com/AlexeyAB/darknet/blob/master/darknet.py     - https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py  - C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp   - C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp  ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     - You should have installed **CUDA 10.2**     - To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      - you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`      - after launching your console application and entering the image file name - you will see info for each object:     `<obj_id> <left_x> <top_y> <width> <height> <probability>`     - to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     - you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)  `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42)  ```cpp struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false);         std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```   """;General;https://github.com/AlexeyAB/darknet
"""""";Reinforcement Learning;https://github.com/pathway/alphaxos
"""""";General;https://github.com/04RR/SOTA-Vision
"""""";Natural Language Processing;https://github.com/04RR/SOTA-Vision
"""Please see example commands in run.sh for training the vavae model.  Or you can use python train.py -m [model type]. The -m option can be used to tell the the script to train a different model.  [model type] can be: - 'sys2': train original VQVAE - 'sys3': train a self-supervised VQVAE with dual encoders - 'sys4': train a semi-supervised VQVAE with dual encoders - 'sys4a': train system 4a but use angular softmax - 'sys5': train a semi-supervised VQVAE with dual encoders  and gradient reversal - 'sys5a': train system 5a but use angular softmax   Please modify sampling rate and other parameters in [config.py](https://github.com/rhoposit/icassp2021/blob/main/config.py) before training.    """;Computer Vision;https://github.com/rhoposit/icassp2021
"""""";General;https://github.com/lancopku/Explicit-Sparse-Transformer
"""""";Natural Language Processing;https://github.com/lancopku/Explicit-Sparse-Transformer
"""```python import torch import numpy as np from resmlp import ResMLP  img = torch.ones([1  3  224  224])  model = ResMLP(in_channels=3  image_size=224  patch_size=16  num_classes=1000                   dim=384  depth=12  mlp_dim=384*4)  parameters = filter(lambda p: p.requires_grad  model.parameters()) parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000 print('Trainable Parameters: %.3fM' % parameters)  out_img = model(img)  print(""Shape of out :""  out_img.shape)  #: [B  in_channels  image_size  image_size] ```   """;Computer Vision;https://github.com/rishikksh20/ResMLP-pytorch
"""```python import torch import numpy as np from resmlp import ResMLP  img = torch.ones([1  3  224  224])  model = ResMLP(in_channels=3  image_size=224  patch_size=16  num_classes=1000                   dim=384  depth=12  mlp_dim=384*4)  parameters = filter(lambda p: p.requires_grad  model.parameters()) parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000 print('Trainable Parameters: %.3fM' % parameters)  out_img = model(img)  print(""Shape of out :""  out_img.shape)  #: [B  in_channels  image_size  image_size] ```   """;General;https://github.com/rishikksh20/ResMLP-pytorch
"""""";Computer Vision;https://github.com/yashgarg98/GAN
"""""";General;https://github.com/pralab/Fast-Minimum-Norm-FMN-Attack
"""""";Computer Vision;https://github.com/amamov/vgg16-vs-resnet
"""It is advisable to use the `augmented_conv2d(...)` function directly to build an attention augmented convolution block.  ```python from attn_augconv import augmented_conv2d  ip = Input(...) x = augmented_conv2d(ip  ...) ... ```  If you wish to add the attention module seperately  you can do so using the `AttentionAugmentation1D` layer as well.  ```python from attn_augconv import AttentionAugmentation1D  ip = Input(...)  #: make sure that input to the AttentionAugmentation1D layer has (2 * depth_k + depth_v) filters. x = Conv2D(2 * depth_k + depth_v  ...)(ip) x = AttentionAugmentation1D(depth_k  depth_v  num_heads)(x) ... ```   """;General;https://github.com/titu1994/keras-attention-augmented-convs
"""""";General;https://github.com/mitran27/Attention-is-all-you-Need
"""""";Natural Language Processing;https://github.com/mitran27/Attention-is-all-you-Need
"""We now provide a *clean* version of GFPGAN  which does not require customized CUDA extensions. <br> If you want want to use the original model in our paper  please see [PaperModel.md](PaperModel.md) for installation.  1. Clone repo      ```bash     git clone https://github.com/TencentARC/GFPGAN.git     cd GFPGAN     ```  1. Install dependent packages      ```bash     #: Install basicsr - https://github.com/xinntao/BasicSR     #: We use BasicSR for both training and inference     pip install basicsr      #: Install facexlib - https://github.com/xinntao/facexlib     #: We use face detection and face restoration helper in the facexlib package     pip install facexlib      pip install -r requirements.txt     python setup.py develop      #: If you want to enhance the background (non-face) regions with Real-ESRGAN      #: you also need to install the realesrgan package     pip install realesrgan     ```   - Python >= 3.7 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html)) - [PyTorch >= 1.7](https://pytorch.org/) - Option: NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads) - Option: Linux   We provide a clean version of GFPGAN  which can run without CUDA extensions. So that it can run in Windows or on CPU mode.   wget https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth -P experiments/pretrained_models   (You can try a simple version ( options/train_gfpgan_v1_simple.yml) that does not require face component landmarks.)   """;Computer Vision;https://github.com/TencentARC/GFPGAN
"""Manual: https://github.com/AlexeyAB/darknet/wiki   Requirements (and how to install dependencies)   How to compile on Linux/macOS (using CMake)   How to compile on Linux (using make)  How to compile on Windows (using CMake)  How to compile on Windows (using vcpkg)  How to train with multi-GPU   * tkDNN: https://github.com/ceccocats/tkDNN  * OpenCV: https://gist.github.com/YashasSamaga/48bdb167303e10f4d07b754888ddbdcf   train  = &lt;replace with your path&gt;/trainvalno5k.txt   Compile Darknet with GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 in the Makefile   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/  Pytorch - Scaled-YOLOv4: https://github.com/WongKinYiu/ScaledYOLOv4  TensorFlow: pip install yolov4 YOLOv4 on TensorFlow 2.0 / TFlite / Android: https://github.com/hunglc007/tensorflow-yolov4-tflite   OpenCV the fastest implementation of YOLOv4 for CPU (x86/ARM-Android)  OpenCV can be compiled with OpenVINO-backend for running on (Myriad X / USB Neural Compute Stick / Arria FPGA)  use yolov4.weights/cfg with: C++ example or Python example  Intel OpenVINO 2021.2: supports YOLOv4 (NPU Myriad X / USB Neural Compute Stick / Arria FPGA): https://devmesh.intel.com/projects/openvino-yolov4-49c756 read this manual (old manual ) (for Scaled-YOLOv4 models use https://github.com/Chen-MingChang/pytorch_YOLO_OpenVINO_demo )  PyTorch > ONNX:    DirectML https://github.com/microsoft/DirectML/tree/master/Samples/yolov4  OpenCL (Intel  AMD  Mali GPUs for macOS & GNU/Linux) https://github.com/sowson/darknet   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   The CMakeLists.txt will attempt to find installed optional dependencies like CUDA  cudnn  ZED and build against those. It will also create a shared object library file to use darknet for code development.  Install powershell if you do not already have it (guide here).  To update CMake on Ubuntu  it's better to follow guide here: https://apt.kitware.com/   PS Code/&gt;              git clone https://github.com/AlexeyAB/darknet  PS Code/&gt;              cd darknet   (add option -EnableOPENCV_CUDA if you want to build OpenCV with CUDA support - very slow to build!)  If you open the build.ps1 script at the beginning you will find all available switches.   PS Code/&gt;              git clone https://github.com/AlexeyAB/darknet  PS Code/&gt;              cd darknet   If you open the build.ps1 script at the beginning you will find all available switches.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   You also need to specify for which graphics card the code is generated. This is done by setting ARCH=. If you use a never version than CUDA 11 you further need to edit line 20 from Makefile and remove -gencode arch=compute_30 code=sm_30 \ as Kepler GPU support was dropped in CUDA 11. You can also drop the general ARCH= and just uncomment ARCH= for your graphics card.   MSVC: https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community  CMake GUI: Windows win64-x64 Installerhttps://cmake.org/download/   find the executable file darknet.exe in the output path to the binaries you specified  This is the recommended approach to build Darknet on Windows.  Install Visual Studio 2017 or 2019. In case you need to download it  please go here: Visual Studio Community  Install CUDA (at least v10.0) enabling VS Integration during installation.   PS Code/&gt;              git clone https://github.com/AlexeyAB/darknet  PS Code/&gt;              cd darknet   (add option -EnableOPENCV_CUDA if you want to build OpenCV with CUDA support - very slow to build! - or remove options like -EnableCUDA or -EnableCUDNN if you are not interested in them). If you open the build.ps1 script at the beginning you will find all available switches.  Train it first on 1 GPU for like 1000 iterations: darknet.exe detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137   for enet-coco.cfg (EfficientNetB0-Yolov3) (14 MB): enetb0-coco.conv.132   Also you can get result earlier than all 45000 iterations.   in Python: https://github.com/tzutalin/labelImg  in Python: https://github.com/Cartucho/OpenLabeling   in JavaScript: https://github.com/opencv/cvat   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v4 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov4.cfg yolov4.weights -ext_output dog.jpg` * Yolo v4 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output test.mp4` * Yolo v4 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -c 0` * Yolo v4 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v4 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov4.cfg yolov4.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov4.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remember to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-labelling - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux   * using `build.sh` or   * build `darknet` using `cmake` or   * set `LIBSO=1` in the `Makefile` and do `make` * on Windows   * using `build.ps1` or   * build `darknet` using `cmake` or   * compile `build\darknet\yolo_cpp_dll.sln` solution or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs:  * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h   * Python examples using the C API:     * https://github.com/AlexeyAB/darknet/blob/master/darknet.py     * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py  * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp   * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp  ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov4.cfg yolov4.weights test.mp4`      * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)  `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42)  ```cpp struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false);         std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/CRIGIM/darknet
"""Tested on CartPole & Pendulum   """;Reinforcement Learning;https://github.com/liampetti/DDPG
"""""";Audio;https://github.com/basameera/NIPS_week_4_5
"""""";Sequential;https://github.com/basameera/NIPS_week_4_5
"""""";Audio;https://github.com/keonlee9420/VAENAR-TTS
"""""";Computer Vision;https://github.com/keonlee9420/VAENAR-TTS
"""""";General;https://github.com/keonlee9420/VAENAR-TTS
"""```python train.py```   """;Graphs;https://github.com/Sean-Bin-Yang/Path-InfoMax
"""""";General;https://github.com/SaverioMonaco/Image-Colorization-through-Segmentation
"""""";Computer Vision;https://github.com/Mnpr/Thoractic-Diseases-Detection
"""""";General;https://github.com/Mnpr/Thoractic-Diseases-Detection
"""``` git clone git@github.com:domluna/memn2n.git  mkdir ./memn2n/data/ cd ./memn2n/data/ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz tar xzvf ./tasks_1-20_v1-2.tar.gz  cd ../ python single.py ```   Running a [single bAbI task](./single.py)  Running a [joint model on all bAbI tasks](./joint.py)  These files are also a good example of usage.   """;General;https://github.com/domluna/memn2n
"""Create Environment: `conda create --name nerbert python=3.6`  Activate Environment: `conda activate nerbert`  Make Install Executable: `chmod +x install.sh`  Install Requirements: `./install.sh`  Save All Models To `saved_models` Directory From: https://drive.google.com/drive/folders/1HgHJtuW1fOuO8bWxSAxTZZQL48FW-rRI?usp=sharing   """;Natural Language Processing;https://github.com/AhmedYounes94/Semi-supervised-BERT-NER
"""‰∏ãËΩΩÂú∞ÂùÄÔºöÈìæÊé•Ôºöhttps://pan.baidu.com/s/17UPXPI3WU2EekAtpfP-sug      ÊèêÂèñÁ†ÅÔºö9812  ÈìæÊé•Ôºöhttps://pan.baidu.com/s/13g0jTZOu6zL9rB2znndYqA    python 3.7   """;Computer Vision;https://github.com/Atmosphere-art/Self-Attention-GAN
"""‰∏ãËΩΩÂú∞ÂùÄÔºöÈìæÊé•Ôºöhttps://pan.baidu.com/s/17UPXPI3WU2EekAtpfP-sug      ÊèêÂèñÁ†ÅÔºö9812  ÈìæÊé•Ôºöhttps://pan.baidu.com/s/13g0jTZOu6zL9rB2znndYqA    python 3.7   """;General;https://github.com/Atmosphere-art/Self-Attention-GAN
"""""";General;https://github.com/safi842/Microstructure-GAN
"""""";Computer Vision;https://github.com/safi842/Microstructure-GAN
"""GPUs: K80 ($0.14/hr)  T4 ($0.11/hr)  V100 ($0.74/hr) CUDA with Nvidia Apex FP16/32     Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   $ git clone https://github.com/ultralytics/yolov3 && cd yolov3   Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB'  total_memory=16130MB)   To access an up-to-date working environment (with all dependencies including CUDA/CUDNN  Python and PyTorch preinstalled)  consider a:   * [Notebook](https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb) <a href=""https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a> * [Train Custom Data](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) << highly recommended * [GCP Quickstart](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart) * [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart)  ![Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/yolov3?logo=docker) * [A TensorRT Implementation of YOLOv3 and YOLOv4](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp)     """;Computer Vision;https://github.com/AdolfoMatias/rede_neural_laranja_maca_adolfo
"""""";General;https://github.com/PaddlePaddle/Research
"""""";Computer Vision;https://github.com/PaddlePaddle/Research
"""""";Computer Vision;https://github.com/KiUngSong/Generative-Models
"""""";General;https://github.com/KiUngSong/Generative-Models
"""2) One-Stage Detectors   To download a dataset from OID:  Step 1: Install git            git clone https://github.com/EscVM/OIDv4_Toolkit.git          !git clone https://github.com/EscVM/OIDv4_Toolkit.git   One will get the OID repo here: https://g.co/dataset/open-images   !pip install icrawler   The first thing we have to do is cloning and building the darknet.The following cells will clone darknet from AlexeyAB's famous repository  adjust the Makefile to enable OPENCV and GPU for darknet and then build darknet.  !git clone https://github.com/AlexeyAB/darknet   !wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137   Note that you would have to change your class names and paths given according to your model. Now remember that you have to give permission to your YOLO model for execution. That can be done using the following code:   """;Computer Vision;https://github.com/Abhi-899/YOLOV4-Custom-Object-Detection
"""Install the requirements using the following commands in your Python Environment:  ```bash pip install PyYAML pip install h5py pip install numpy pip install opencv-contrib-python pip install imageio  pip3 install torch torchvision torchaudio pip install pytorch-lightning ```   """;Computer Vision;https://github.com/adipandas/torch_shnet
"""- MXNet   """;General;https://github.com/UnofficialJuliaMirror/DeepMark-deepmark
"""git clone https://github.com/Megvii-BaseDetection/YOLOX  cd YOLOX   source yoloxenv/bin/activate   pip install -r requirements.txt   :pip install -v -e .   git clone https://github.com/NVIDIA/apex  cd apex  : pip install -v --disable-pip-version-check --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./   python setup.py install   pip install cython; pip3 install 'git+https://github.com/cocodataset/cocoapi.git#:subdirectory=PythonAPI'       #:         os.path.join(rootpath  ""ImageSets""  ""Main""  name + "".txt"")       name = self.name       imagesetfile = os.path.join(rootpath  ""ImageSets""  ""Main""  name + "".txt"")           os.mkdir(output_dir)           os.mkdir(cachedir)   [08/16/2021-13:11:15] [I] GPU Compute   : ËøôÈáåÂÅáËÆæÂ∑≤ÁªèÂú®ÂÆπÂô®ÂÜÖË¢´ÊåáÂíåÂÆâË£Ö‰∫ÜÂøÖË¶ÅÁöÑÂ∫ìÊØîÂ¶Ç make  opencv   cd yolox_linux   -- Found OpenCV: /workspace/opencv-4.5.2/build (found version ""4.5.2"")    -- Found CUDA: /usr/local/cuda (found version ""11.2"")    : make   """;Computer Vision;https://github.com/DataXujing/YOLOX-
"""""";Computer Vision;https://github.com/Sakib1263/VGG-1D-2D-Tensorflow-Keras
"""""";General;https://github.com/Sakib1263/VGG-1D-2D-Tensorflow-Keras
"""git clone https://github.com/HaijunMa/GAN.git   """;Computer Vision;https://github.com/HaijunMa/GAN
"""""";Computer Vision;https://github.com/04RR/SOTA-Vision
""" - Notebook: folder ch·ª©a c√°c notebook ƒë·ªÉ train    - Run c√°c cell trong notebook   pip install -r requirements.txt. ƒë·ªÉ c√†i ƒë·∫∑t c√°c packages c·∫ßn thi·∫øt   """;Computer Vision;https://github.com/vieduy/Neural-Style-Transfer
"""```bash $ pip install fast-transformer-pytorch ```   ```python import torch from fast_transformer_pytorch import FastTransformer  model = FastTransformer(     num_tokens = 20000      dim = 512      depth = 2      max_seq_len = 4096      absolute_pos_emb = True   #: default uses relative positional encoding  but if that isn't working  then turn on absolute positional embedding by setting this to True )  x = torch.randint(0  20000  (1  4096)) mask = torch.ones(1  4096).bool()  logits = model(x  mask = mask) #: (1  4096  20000) ```   """;Natural Language Processing;https://github.com/lucidrains/fast-transformer-pytorch
"""""";General;https://github.com/Friger/GAGNN
"""Pytorch TabNet   cd adsb-flight-localization  mkdir mysql     - `cd adsb-flight-localization`   Apache Spark and Kafka run on JAVA 8/11. Hence  we will start by installing the Java SE Development Kit 8:   2. Create the directory for JDK:   - sudo mkdir /usr/lib/jvm   - cd /usr/lib/jvm  - sudo tar -xvzf jdk-8u281-linux-x64.tar.gz   5. Verify the version of the JDK with the following command:  - java -version      - cd kafka_2.12-2.7.0   Update the apt packages index:  sudo apt update  Install MariaDB by running the following command:  sudo apt install mariadb-server  Install all packages included in requirements.txt  Create a virtual environment (conda  virtualenv etc.).  conda create -n &lt;env_name&gt; python=3.7  Activate your environment.  conda activate &lt;env_name&gt;  Install requirements.  pip install -r requirements.txt  Restart your environment.  conda deactivate  conda activate &lt;env_name&gt;  To install Node.js run the following commands:  curl -fsSL https://deb.nodesource.com/setup_14.x | bash -  apt-get install -y nodejs  Verify that the Node.js and npm were successfully installed:  node --version  npm --version  Install Leaflet.js using npm package manager:  npm install leaflet   ![demo gif](https://github.com/radoslawkrolikowski/adsb-flight-localization/blob/main/assets/demo.gif)    A. Data inspection and preprocessing as well as training of the ML models. 1. Specify your configuration by modifying config.py file:    - MariaDB properties    - Kafka brokers addresses and topics 2. Run and follow the [data_inspection](https://nbviewer.jupyter.org/github/radoslawkrolikowski/adsb-flight-localization/blob/main/data_inspection.ipynb) notebook to get an insight into the nature of the data. 3. Create the MariaDB database by running the createDB.py script (not necessary if you want to store preprocessed data in the HDF5 file) 4. Use the [data_preprocessing](https://nbviewer.jupyter.org/github/radoslawkrolikowski/adsb-flight-localization/blob/main/data_preprocessing.ipynb) notebook to perform the preprocessing of the entire training dataset (consists of 3 files). 5. Run the [prepare_eval_test_datasets](https://nbviewer.jupyter.org/github/radoslawkrolikowski/adsb-flight-localization/blob/main/prepare_eval_test_datasets.ipynb) notebook to make the evaluation and test sets ready. 6. Run the [training_ensemble](https://nbviewer.jupyter.org/github/radoslawkrolikowski/adsb-flight-localization/blob/main/training_ensemble.ipynb) notebook to build the Random forest regressor and the Gradient-boosted trees estimators from the training set: 7. Use the [training_TabNet](https://nbviewer.jupyter.org/github/radoslawkrolikowski/adsb-flight-localization/blob/main/training_TabNet.ipynb) notebook to train the TabNet neural network model.  B. Real-time data preprocessing  prediction and visualization. 1. Before each run of the application we have to start the ZooKeeper and Kafka brokers:      1. Start the ZooKeeper:         - `cd zookeeper/`         - `bin/zkServer.sh start conf/zookeeper.properties`     2. Check if it started correctly:         - `bin/zkServer.sh status conf/zookeeper.properties`      3. Start the Kafka nodes:        - `cd kafka/`        - `bin/kafka-server-start.sh config/server.properties`        - `bin/kafka-server-start.sh config/server-1.properties`        - `bin/kafka-server-start.sh config/server-2.properties`  2. Create the Kafka topics if you run the application for the first time (list of sample topics can be found in config.py file):    	1. Create topic: 		- `bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 3 --partitions 1 --topic topic_name`   	2. List available topics: 		- `bin/kafka-topics.sh --list --bootstrap-server localhost:9092`  3. Run the [flights_map](https://github.com/radoslawkrolikowski/adsb-flight-localization/blob/main/flights_map.py) Flask application and then go to the http://localhost:5001/ to access the map.   4. Then we can run the ADSB_producer.py to preprocess and publish the real-time ADS-B data to the Kafka topic. 5. To make a real-time prediction run predict.py file (only data that comes after predict.py is launched is going to be considered). 6. Observe the real-time aircraft localization predictions using the Flight Radar map (http://localhost:5001/). You can click on the plane icon to visualize its route and depict the altitude graph.     """;General;https://github.com/radoslawkrolikowski/adsb-flight-localization
"""This project is based on [Detectron2](https://github.com/facebookresearch/detectron2)  which can be constructed as follows. * Install Detectron2 following [the instructions](https://detectron2.readthedocs.io/tutorials/install.html). * Setup the dataset following [the structure](https://github.com/facebookresearch/detectron2/blob/master/datasets/README.md). * Install DCNv2 following [Install DCNv2.md](./DCNv2/README.md).   """;Computer Vision;https://github.com/ShihuaHuang95/FaPN-full
"""For DS-TD3: Dynamic Sparse training of TD3 algorithm  ``` python main.py --env HalfCheetah-v3 --policy DS-TD3 ```  For Static-TD3 ``` python main.py --env HalfCheetah-v3 --policy StaticSparseTD3 ```  For TD3 ``` python main.py --env HalfCheetah-v3 --policy TD3 ```   """;General;https://github.com/GhadaSokar/Dynamic-Sparse-Training-for-Deep-Reinforcement-Learning
"""For DS-TD3: Dynamic Sparse training of TD3 algorithm  ``` python main.py --env HalfCheetah-v3 --policy DS-TD3 ```  For Static-TD3 ``` python main.py --env HalfCheetah-v3 --policy StaticSparseTD3 ```  For TD3 ``` python main.py --env HalfCheetah-v3 --policy TD3 ```   """;Reinforcement Learning;https://github.com/GhadaSokar/Dynamic-Sparse-Training-for-Deep-Reinforcement-Learning
"""This can be downloaded using the following command (within the docker container above)   cd scripts; python download_imagenet.py --data_dir &lt;DATASET_DIR&gt;   [from pipedream/profiler/image_classification; you will need to have the changes to PyTorch listed above]  Note that the profiling step must be run with only a single GPU (hence the CUDA_VISIBLE_DEVICES=0 before the command).   """;General;https://github.com/msr-fiddle/pipedream
"""""";General;https://github.com/emanuele-progr/PSSP
"""""";Natural Language Processing;https://github.com/emanuele-progr/PSSP
"""<a href=""https://pypi.org/project/gans-implementations/"">PyPi Installation</a>  ```bash $ pip install gans-implementations ```  Local Install and Run:   ```bash $ cd {PROJECT_DIRECTORY} $ pip install -e . ```           <img alt=""PyPi Version"" src=""https://img.shields.io/pypi/v/GANs-Implementations"">           <img alt=""PyPi Downloads"" src=""https://img.shields.io/pypi/dm/GANs-Implementations"">           <img alt=""Package Status"" src=""https://img.shields.io/pypi/status/GANs-Implementations"">   In notebooks directory there is a notebook on how to use each of these models for their intented use case; such as image  generation for StyleGAN and others. Check them out!  ```python from gans_package.models import StyleGAN_Generator  StyleGAN_Discriminator  in_channels = 256 out_channels = 3 hidden_channels = 512 z_dim = 128 mapping_hidden_size = 256 w_dim = 512 synthesis_layers = 5 kernel_size=3  in_size = 3 d_hidden_size = 16  g = StyleGAN_Generator(in_channels                          out_channels                          hidden_channels                          z_dim                          mapping_hidden_size                          w_dim                          synthesis_layers                          kernel_size                          device=DEVICE).to(DEVICE)  d = StyleGAN_Discriminator(in_size  d_hidden_size).to(DEVICE)  import torch  noise = torch.randn(BATCH_SIZE  z_dim).to(DEVICE)  fake = g(noise) pred = d(fake) ```   """;General;https://github.com/UdbhavPrasad072300/GANs-Implementations
"""<a href=""https://pypi.org/project/gans-implementations/"">PyPi Installation</a>  ```bash $ pip install gans-implementations ```  Local Install and Run:   ```bash $ cd {PROJECT_DIRECTORY} $ pip install -e . ```           <img alt=""PyPi Version"" src=""https://img.shields.io/pypi/v/GANs-Implementations"">           <img alt=""PyPi Downloads"" src=""https://img.shields.io/pypi/dm/GANs-Implementations"">           <img alt=""Package Status"" src=""https://img.shields.io/pypi/status/GANs-Implementations"">   In notebooks directory there is a notebook on how to use each of these models for their intented use case; such as image  generation for StyleGAN and others. Check them out!  ```python from gans_package.models import StyleGAN_Generator  StyleGAN_Discriminator  in_channels = 256 out_channels = 3 hidden_channels = 512 z_dim = 128 mapping_hidden_size = 256 w_dim = 512 synthesis_layers = 5 kernel_size=3  in_size = 3 d_hidden_size = 16  g = StyleGAN_Generator(in_channels                          out_channels                          hidden_channels                          z_dim                          mapping_hidden_size                          w_dim                          synthesis_layers                          kernel_size                          device=DEVICE).to(DEVICE)  d = StyleGAN_Discriminator(in_size  d_hidden_size).to(DEVICE)  import torch  noise = torch.randn(BATCH_SIZE  z_dim).to(DEVICE)  fake = g(noise) pred = d(fake) ```   """;Computer Vision;https://github.com/UdbhavPrasad072300/GANs-Implementations
"""A common test of language competence is to identify a word which doesn't belong in a list with several other words.   As an example  in the list: car  boat  plane  train  microwave  all the words except microwave are modes of transportation  so the answer would be microwave.  Until recently  such a task would have been nearly impossible for a computer to solve without extreme effort on behalf of the programmer. A tool called word2vec [https://code.google.com/p/word2vec/] was released a few days ago  which allows for efficient computation of distributed representations of words as real-valued vectors. Feature vectors are learned by using recent advances in deep learning and neural networks  and have been shown to learn very rich representations of word meaning and usage. See this paper for more information on how the vector representations are learned: http://arxiv.org/pdf/1301.3781.pdf  With this new tool  it is possible to examine a range of previously difficult NLP tasks  one of which is identifying a word which doesn't belong in a list. This program demonstrates this capability. Some samples:  ->staple hammer saw drill  I think staple doesnt belong in this list!  ->math shopping reading science  I think shopping doesnt belong in this list!  ->rain snow sleet sun  I think sun doesnt belong in this list!  ->eight six seven five three owe nine  I think owe doesnt belong in this list!  ->breakfast cereal dinner lunch  I think cereal doesnt belong in this list!  ->england spain france italy greece germany portugal australia  I think australia doesnt belong in this list!  etc.  The vector representations were learned from 1GB of wikipedia text  which if I remember correctly amounted to about 100-200 million words. If you're looking to download and try it out  the file which holds the vectors is pretty large - about 500M. I chunked it up into smaller files so that GitHub would let me push.  If you decide to try it out  keep in mind that the longer the list  the better it will perform. Feel free to check it out  pull  modify  anything. word2vec is really an amazing tool which has the potential to make our NLP systems incredibly more intelligent!  If you want to see some visualizations of the representations  see the /visualizations directory. t-SNE was used to generate the 2D scatterplot.  """;Natural Language Processing;https://github.com/dhammack/Word2VecExample
"""We make VERSE available in two forms: fast  optimized C++ code that was used in the experiments  and more convenient python wrapper. Note that wrapper is still experimental and may not provide optimal performance.  For C++ executables: ```bash cd src && make; ``` should be enough on most platforms. If you need to change the default compiler (i.e. to Intel)  use: ```bash make CXX=icpc ```  VERSE is able to encompass diverse similarity measures under its model. For performance reasons  we have implemented three different similarities separately.  Use the command ```bash verse -input data/karate.bcsr -output karate.bin -dim 128 -alpha 0.85 -threads 4 -nsamples 3 ``` to run the default version (that corresponds to PPR similarity) with 128 embedding dimension  PPR alpha 0.85  using 3 negative samples.   1 2 3 4 5 6 7 8 9 11 12 13 14 18 20 22 32       (note  you must also specify the variable name of the adjacency matrix --matfile-variable-name)   We make VERSE available in two forms: fast  optimized C++ code that was used in the experiments  and more convenient python wrapper. Note that wrapper is still experimental and may not provide optimal performance.  For C++ executables: ```bash cd src && make; ``` should be enough on most platforms. If you need to change the default compiler (i.e. to Intel)  use: ```bash make CXX=icpc ```  VERSE is able to encompass diverse similarity measures under its model. For performance reasons  we have implemented three different similarities separately.  Use the command ```bash verse -input data/karate.bcsr -output karate.bin -dim 128 -alpha 0.85 -threads 4 -nsamples 3 ``` to run the default version (that corresponds to PPR similarity) with 128 embedding dimension  PPR alpha 0.85  using 3 negative samples.   """;Graphs;https://github.com/xgfs/verse
"""To run this code  you need to install the t5   instructions in the T5 repo   python -m t5.models.mesh_transformer_main \   python -m t5.models.mesh_transformer_main \   """;Natural Language Processing;https://github.com/google-research/multilingual-t5
"""I decided to use Google Cloud Platform compute with ¬£230.58 free credit for 1 year. [This is a wonderful tutorial](https://course.fast.ai/start_gcp.html) to get started and refer back to.  My notes are [here](https://github.com/datalass1/fastai/issues/18)   My notes are [here](https://github.com/datalass1/fastai/issues/20)   - [Neural Network playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-gauss&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4 2&seed=0.44189&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=regression&initZero=false&hideText=false)  """;General;https://github.com/datalass1/fastai
"""	- First download the multicased BERT model and change the path in the ipynb file and use google colab 	  to train the model  It will take nearly 1-2 hours for training then download those trained models 	  and use them with <code>telugu_ner.py<code> which is just a prediction file. 	    """;Natural Language Processing;https://github.com/Zeeshan75/Bert_Telugu_Ner
"""    z: white noise  z~logistic distribution LÔºà0 1Ôºâ  one mixture   """;Audio;https://github.com/kensun0/Parallel-Wavenet
"""    z: white noise  z~logistic distribution LÔºà0 1Ôºâ  one mixture   """;Sequential;https://github.com/kensun0/Parallel-Wavenet
"""Run `python main.py --help` for full detail.  Example: ``` python main.py --batch_size 128 --imsize 64 --dataset mura --adv_loss inverse --version sabigan_wrist --image_path ~/datasets/ --use_tensorboard true --mura_class XR_WRIST --mura_type negative ```   """;Computer Vision;https://github.com/pavasgdb/Anomaly-detector-using-GAN
"""Follow experimental summary [here](https://bit.ly/3arUw9q).   """;General;https://github.com/sayakpaul/EvoNorms-in-TensorFlow-2
"""faster_rcnn_pytorch | Faster RCNN with PyTorch   """;Computer Vision;https://github.com/busyboxs/Some-resources-useful-for-me
"""1. relu 2. sgd 3. xavier    """;Computer Vision;https://github.com/bettersituation/rigid_batch_norm
"""1. relu 2. sgd 3. xavier    """;General;https://github.com/bettersituation/rigid_batch_norm
"""To compute the FID score between two datasets and get gradient for the first dataset  where images of each dataset are contained in an individual folder: ``` python ./fid_score.py path/to/dataset1 path/to/dataset2 ```   ``` python ./fid_score.py cifar/dev1 cifar/dev2 ```   """;General;https://github.com/yenchenlin/fid
"""Install PyTorch following the instuctions on the [official website] (https://pytorch.org/). The code has been tested over PyTorch 0.4.1 and 1.0.0 versions.  Then install the other dependencies. ``` pip install -r requirements.txt ```   """;Graphs;https://github.com/weihua916/powerful-gnns
"""See https://github.com/bioinf-jku/TTUR for the original implementation using Tensorflow.   You can still use this version if you want a quick FID estimate without installing Tensorflow.   Requirements: - python3 - MXNet - GluonCV - numpy - scipy  To compute the FID score between two datasets  where images of each dataset are contained in an individual folder: ``` ./fid_score.py path/to/dataset1 path/to/dataset2 ```  To run the evaluation on GPU  use the flag `--gpu`.    """;General;https://github.com/djl11/mxnet-fid
"""**Requirements**: Visual Studio 2013   |  Linux (CPU)   |  Windows (CPU) |   Installation instructions   Copy .\windows\CommonSettings.props.example to .\windows\CommonSettings.props  By defaults Windows build requires CUDA and cuDNN libraries.  Both can be disabled by adjusting build variables in .\windows\CommonSettings.props.   3rd party dependencies required by Caffe are automatically resolved via NuGet.  Download CUDA Toolkit 7.5 from nVidia website.  If you don't have CUDA installed  you can experiment with CPU_ONLY build.   Download cuDNN v4 or cuDNN v5 from nVidia website.  Unpack downloaded zip to %CUDA_PATH% (environment variable set by CUDA installer).   Install for all users and add Python to PATH (through installer).  Run the following commands from elevated command prompt:  conda install --yes numpy scipy matplotlib scikit-image pip  pip install protobuf   * set PythonPath environment variable to point to &lt;caffe_root&gt;\Build\x64\Release\pycaffe  or   To build Caffe Matlab wrapper set MatlabSupport to true and MatlabDir to the root of your Matlab installation in .\windows\CommonSettings.props.   """;Computer Vision;https://github.com/Code-0x00/caffe_windows
"""""";Computer Vision;https://github.com/modelhub-ai/vgg-19
"""* An example call for the synthetic tasks follows.  We trained these models on CPUs.  Please see the docstring for further details ``` python Run_Gin_Experiment.py --cv-fold 0 --model-type rpGin --out-weight-dir /some/path --out-log-dir /another/path/ --onehot-id-dim 10 ``` * Now we show examples for the molecular tasks.  The Tox 21 dataset is smaller so we demonstrate with that. For the molecular k-ary tasks: ``` python Duvenaud-kary.py 'tox_21' 20 ``` * For the molecular RP-Duvenaud tasks: ``` python rp_duvenaud.py 'tox_21' 'unique_local' 0 ``` * For the molecular RNN task: ``` python RNN-DFS.py 'tox_21' ```   """;Graphs;https://github.com/PurdueMINDS/RelationalPooling
"""""";Computer Vision;https://github.com/m1lhaus/SimpleSqueezeNet
"""code at terminal.   """;Computer Vision;https://github.com/tatsuyaokunaga/Simpsons_Detection
"""""";General;https://github.com/OoO256/cifar10-resnet
"""""";Computer Vision;https://github.com/OoO256/cifar10-resnet
"""Spell corrector.ipynb   We tackle the problem of OCR post processing. In OCR  we map the image form of the document into the text domain. This is done first using an CNN+LSTM+CTC model  in our case based on tesseract. Since this output maps only image to text  we need something on top to validate and correct language semantics.  The idea is to build a language model  that takes the OCRed text and corrects it based on language knowledge. The langauge model could be: - Char level: the aim is to capture the word morphology. In which case it's like a spelling correction system. - Word level: the aim is to capture the sentence semnatics. But such systems suffer from the OOV problem. - Fusion: to capture semantics and morphology language rules. The output has to be at char level  to avoid the OOV. However  the input can be char  word or both.   Pre-train on a noisy  medical sentences   Spell corrector.ipynb   """;Natural Language Processing;https://github.com/ahmadelsallab/spell_corrector
""" **BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.    In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant    See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/thanhlong1997/bert_quora
"""""";General;https://github.com/google-research/valan
"""""";Reinforcement Learning;https://github.com/google-research/valan
"""Import `optimizer.py`  then add the prefix `Lookahead` before the name of [arbitrary optimizer](http://mxnet.incubator.apache.org/api/python/optimization/optimization.html?highlight=opt#module-mxnet.optimizer).  ```python import optimizer optimizer.LookaheadSGD(k=5  alpha=0.5  learning_rate=1e-3) ```   ```bash python mnist.py --optimizer sgd --seed 42 python mnist.py --optimizer lookaheadsgd --seed 42 ```  """;General;https://github.com/mnikitin/LookaheadOptimizer-mx
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;Natural Language Processing;https://github.com/meizi1114/bert
"""""";General;https://github.com/agassi4013/Log-DenseNet-Tensorflow
"""""";Computer Vision;https://github.com/agassi4013/Log-DenseNet-Tensorflow
"""""";Reinforcement Learning;https://github.com/YuriCat/MuZeroJupyterExample
"""```bash $ python demo.py ```   Image | True | Out | Plot | |---|---|---|---| |![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/0_raw.jpg)|look_vec: 0.2209 0.4703 -0.8544<br>pupil_size: 0.0233|look_vec: 0.1917 0.4689 -0.8622<br>pupil_size: 0.0301|![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/0_angle.jpg)| |![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/1_raw.jpg)|look_vec: 0.1296 -0.0196 -0.9914<br>pupil_size: -0.3122|look_vec: 0.2225 0.0327 -0.9744<br>pupil_size: -0.2559|![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/1_angle.jpg)| |![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/2_raw.jpg)|look_vec: -0.6035 0.0833 -0.7930<br>pupil_size: 0.2096|look_vec: -0.6263 0.0704 -0.7764<br>pupil_size: 0.2214|![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/2_angle.jpg)| |![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/3_raw.jpg)|look_vec: -0.5143 -0.2162 -0.8299<br>pupil_size: 0.0997|look_vec: -0.5563 -0.2640 -0.7879<br>pupil_size: 0.0541|![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/3_angle.jpg)| |![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/4_raw.jpg)|look_vec: 0.3604 0.0302 -0.9323<br>pupil_size: 0.1686|look_vec: 0.3937 0.0203 -0.9190<br>pupil_size: 0.2245|![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/4_angle.jpg)| |![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/5_raw.jpg)|look_vec: 0.6021 0.6270 -0.4943<br>pupil_size: 0.1146|look_vec: 0.5902 0.6401 -0.4918<br>pupil_size: 0.0198|![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/5_angle.jpg)| |![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/6_raw.jpg)|look_vec: -0.0691 -0.2175 -0.9736<br>pupil_size: -0.0675|look_vec: -0.0955 -0.1726 -0.9804<br>pupil_size: -0.0526|![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/6_angle.jpg)| |![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/7_raw.jpg)|look_vec: -0.4191 0.3765 -0.8262<br>pupil_size: -0.3669|look_vec: -0.3618 0.3485 -0.8646<br>pupil_size: -0.2192|![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/7_angle.jpg)| |![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/8_raw.jpg)|look_vec: 0.0711 -0.1398 -0.9876<br>pupil_size: 0.2395|look_vec: 0.0902 -0.1491 -0.9847<br>pupil_size: 0.2621|![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/8_angle.jpg)| |![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/9_raw.jpg)|look_vec: 0.3328 -0.4387 -0.8347<br>pupil_size: -0.2609|look_vec: 0.3543 -0.4874 -0.7981<br>pupil_size: -0.2140|![image](https://github.com/foamliu/Gaze-Estimation/raw/master/images/9_angle.jpg)| """;General;https://github.com/foamliu/Gaze-Estimation
"""The .sh files need to be run first to download data and install tools.   """;Natural Language Processing;https://github.com/Somefive/XNLI
"""- Revised the first four modules of Intro to DL with PyTorch as I need to help on PyTorch Beast Weak.   Working on the Thank you Initiative for udacity-facebook. Would love to see everyone's participation. ^_^ Here: https://secureprivataischolar.slack.com/archives/CJSCX4WAZ/p1566660348242000   """;General;https://github.com/rupaai/60DaysOfUdacity
"""""";General;https://github.com/SaremS/MANN
"""""";Sequential;https://github.com/SaremS/MANN
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/meizi1114/bert
"""High localization accuracy is crucial in many real-world applications. We propose a novel single stage end-to-end object detection network (RRC) to produce high accuracy detection results. You can use the code to train/evaluate a network for object detection task. For more details  please refer to our paper (https://arxiv.org/abs/1704.05776).  | method | KITTI test *mAP* car (moderate)| | :-------: | :-----: | | [Mono3D](http://3dimage.ee.tsinghua.edu.cn/cxz/mono3d)| 88.66% | | [SDP+RPN](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Exploit_All_the_CVPR_2016_paper.pdf)| 88.85% | | [MS-CNN](https://github.com/zhaoweicai/mscnn) | 89.02% | | [Sub-CNN](https://arxiv.org/pdf/1604.04693.pdf) | 89.04% | | RRC (single model) | **89.85%** |    [KITTI ranking](http://www.jimmyren.com/papers/rrc_kitti.pdf)   1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6).    By default  we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`.  2. Download the KITTI dataset(http://www.cvlibs.net/datasets/kitti/eval_object.php).    By default  we assume the data is stored in `$HOME/data/KITTI/`         ¬† Unzip the training images  testing images and the labels in `$HOME/data/KITTI/`.  3. Create the LMDB file.    For training .    As only the images contain cars are adopted as training set for car detection   the labels for cars should be extracted.          We have provided the list of images contain cars in `$CAFFE_ROOT/data/KITTI-car/`.    ```Shell    #: extract the labels for cars    cd $CAFFE_ROOT/data/KITTI-car/    ./extract_car_label.sh    ```     Before create the LMDB files. The labels should be converted to VOC type. We provide some matlab scripts.         The scripts are in `$CAFFE_ROOT/data/convert_labels/`. Just modify `converlabels.m`.    ```Shell    line 4: root_dir = '/your/path/to/KITTI/';    ```    VOC type labels will be generated in `$KITTI_ROOT/training/labels_2car/xml/`.    ```Shell    cd $CAFFE_ROOT/data/KITTI-car/    #: Create the trainval.txt  test.txt  and test_name_size.txt in data/KITTI-car/    ./create_list.sh    #: You can modify the parameters in create_data.sh if needed.    #: It will create lmdb files for trainval and test with encoded original image:    #:   - $HOME/data/KITTI/lmdb/KITTI-car_training_lmdb/    #:   - $HOME/data/KITTI/lmdb/KITTI-car_testing_lmdb/    #: and make soft links at data/KITTI-car/lmdb     ./data/KITTI-car/create_data.sh    ```  1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`    ```Shell    https://github.com/xiaohaoChen/rrc_detection.git    cd rrc_detection    ``` 2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.    Before build it  you should install CUDA and CUDNN(v5.0).        CUDA 7.5 and CUDNN v5.0 were adapted in our computer.    ```Shell    #: Modify Makefile.config according to your Caffe installation.    cp Makefile.config.example Makefile.config    make -j8    #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.    make py    make test -j8    make runtest -j8    ```     If you only have one TITAN X card  you should modify the script rrc_kitti.py.        #: Make sure that the work directory is caffe_root     cd $caffe_root   """;Computer Vision;https://github.com/xiaohaoChen/rrc_detection
"""""";General;https://github.com/ericjang/gumbel-softmax
"""""";General;https://github.com/Kokonut133/MagicDraw
"""Anaconda Python distribution   - Create environment and install requirements - Clone this repository  ```bash git clone https://github.com/tbullmann/imagesegmentation-tensorflow.git ```  - Create directories  ```bash mkdir datasets   mkdir temp  ```  - Download Drosophila VNC dataset ```bash git clone https://github.com/tbullmann/groundtruth-drosophila-vnc datasets/vnc ```  - Download Mouse Cortex dataset ```bash git clone https://github.com/tbullmann/groundtruth-mouse-cortex datasets/cortex ```  - [Run the examples](docs/README.md)   """;Computer Vision;https://github.com/tbullmann/imagesegmentation-tensorflow
"""Anaconda Python distribution   - Create environment and install requirements - Clone this repository  ```bash git clone https://github.com/tbullmann/imagesegmentation-tensorflow.git ```  - Create directories  ```bash mkdir datasets   mkdir temp  ```  - Download Drosophila VNC dataset ```bash git clone https://github.com/tbullmann/groundtruth-drosophila-vnc datasets/vnc ```  - Download Mouse Cortex dataset ```bash git clone https://github.com/tbullmann/groundtruth-mouse-cortex datasets/cortex ```  - [Run the examples](docs/README.md)   """;General;https://github.com/tbullmann/imagesegmentation-tensorflow
"""""";Computer Vision;https://github.com/daveboat/pytorch_focal_loss
"""""";General;https://github.com/daveboat/pytorch_focal_loss
"""Follow the instructions for installing SpinningUp  you might also need environment libraries such as Gym and MuJoCo.   To run Peng's Q(lambda) with delayed environment (with k=3)  n-step buffer with n=5  run the following   To run n-step with delayed environment (with k=3)  n-step buffer with n=5  run the following   To run Retrace with delayed environment (with k=3)  n-step buffer with n=5  just set lambda=1.0 and run the following   """;General;https://github.com/robintyh1/icml2021-pengqlambda
"""- Install tensorflow/pytorch  git clone git@github.com:yaxingwang/MineGAN.git to get MineGA   """;General;https://github.com/yaxingwang/MineGAN
"""""";Reinforcement Learning;https://github.com/snjstudent/MyMuzero
"""This repository is for the CVPR 2018 Spotlight paper  '[Path Aggregation Network for Instance Segmentation](https://arxiv.org/abs/1803.01534)'  which ranked 1st place of [COCO Instance Segmentation Challenge 2017](http://cocodataset.org/#detections-leaderboard)   2nd place of [COCO Detection Challenge 2017](http://cocodataset.org/#detections-leaderboard) (Team Name: [UCenter](https://places-coco2017.github.io/#winners)) and 1st place of 2018 [Scene Understanding Challenge for Autonomous Navigation in Unstructured Environments](http://cvit.iiit.ac.in/scene-understanding-challenge-2018/benchmarks.php#instance) (Team Name: TUTU).   For environment requirements  data preparation and compilation  please refer to [Detectron.pytorch](https://github.com/roytseng-tw/Detectron.pytorch).  WARNING: pytorch 0.4.1 is broken  see https://github.com/pytorch/pytorch/issues/8483. Use pytorch 0.4.0   by [Shu Liu](http://shuliu.me)  Lu Qi  Haifang Qin  [Jianping Shi](https://shijianping.me/)  [Jiaya Jia](http://jiaya.me/).   For training and testing  we keep the same as the one in [Detectron.pytorch](https://github.com/roytseng-tw/Detectron.pytorch). To train and test PANet  simply use corresponding config files. For example  to train PANet on COCO:  ```shell python tools/train_net_step.py --dataset coco2017 --cfg configs/panet/e2e_panet_R-50-FPN_2x_mask.yaml ```  To evaluate the model  simply use:  ```shell python tools/test_net.py --dataset coco2017 --cfg configs/panet/e2e_panet_R-50-FPN_2x_mask.yaml --load_ckpt {path/to/your/checkpoint} ```   """;Computer Vision;https://github.com/ShuLiu1993/PANet
"""--name ffhq_aegan_wplus_decoupled \   --name ffhq_aegan_wplus_joint \   --name ffhq_gan \   --name ffhq_alae_wtied_recw=1_mlpd=4 \   """;General;https://github.com/phymhan/stylegan2-pytorch
"""--name ffhq_aegan_wplus_decoupled \   --name ffhq_aegan_wplus_joint \   --name ffhq_gan \   --name ffhq_alae_wtied_recw=1_mlpd=4 \   """;Computer Vision;https://github.com/phymhan/stylegan2-pytorch
"""""";Computer Vision;https://github.com/zxr931120/-
"""Check `README.md` under `data` for more details.     1. Create a fresh conda environment  and install all dependencies.  ```text conda create -n vilbert-mt python=3.6 conda activate vilbert-mt git clone --recursive https://github.com/facebookresearch/vilbert-multi-task.git cd vilbert-multi-task pip install -r requirements.txt ```  2. Install pytorch ``` conda install pytorch torchvision cudatoolkit=10.0 -c pytorch ```  3. Install apex  follows https://github.com/NVIDIA/apex  4. Install this codebase as a package in this environment. ```text python setup.py develop ```   Download link   Download link   """;Natural Language Processing;https://github.com/johntiger1/multitask_multimodal
"""$ pip install mkdocs   $ git clone https://github.com/zjZSTU/Fast-R-CNN.git   """;Computer Vision;https://github.com/zjZSTU/Fast-R-CNN
"""""";Computer Vision;https://github.com/tianyu-tristan/Visual-Attention-Model
"""""";Computer Vision;https://github.com/sd2001/UNets
"""``` from optimizers import DemonRanger from dataloader import batcher #: some random function to batch data  class config:    def __init__(self):        self.batch_size = ...        self.wd = ...        self.lr = ...        self.epochs = ...                 config = config()      train_data = ... step_per_epoch = count_step_per_epoch(train_data config.batch_size)  model = module(stuff)  optimizer = DemonRanger(params=model.parameters()                          lr=config.lr                          weight_decay=config.wd                          epochs=config.epochs                          step_per_epoch=step_per_epoch                          IA_cycle=step_per_epoch) IA_activate = False                       for epoch in range(config.epochs):     batches = batcher(train_data  config.batch_size)          for batch in batches:         loss = do stuff         loss.backward()         optimizer.step(IA_activate=IA_activate)          #: automatically enable IA (Iterate Averaging) near the end of training (when metric of your choice not improving for a while)     if (IA_patience running low) and IA_activate is False:         IA_activate = True            ```   ``` optimizer = DemonRanger(params=model.parameters()                          lr=config.lr                          weight_decay=config.wd                          epochs=config.epochs                          step_per_epoch=step_per_epoch                          IA_cycle=step_per_epoch)  #: just do optimizer.step(IA_activate=IA_activate) when necessary (change IA_activate to True near the end of training based on some scheduling scheme or tuned hyperparameter--- alternative to learning rate scheduling)  ```    """;General;https://github.com/JRC1995/DemonRangerOptimizer
"""GCP (or any other cloud services that provide a robust environment)   """;General;https://github.com/lucashueda/long_sentence_transformer
"""Add AUROC generated metric for tensorboard  update env.yml   """;Computer Vision;https://github.com/pdoyle5000/simple_net
"""**BERT**  or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).  To give a few numbers  here are the results on the [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering task:  SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1 ------------------------------------- | :------: | :------: 1st Place Ensemble - BERT             | **87.4** | **93.2** 2nd Place Ensemble - nlnet            | 86.0     | 91.7 1st Place Single Model - BERT         | **85.1** | **91.8** 2nd Place Single Model - nlnet        | 83.5     | 90.1  And several natural language inference tasks:  System                  | MultiNLI | Question NLI | SWAG ----------------------- | :------: | :----------: | :------: BERT                    | **86.7** | **91.1**     | **86.3** OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0  Plus many other tasks.  Moreover  these results were all obtained with almost no task-specific neural network architecture design.  If you already know what BERT is and you just want to get started  you can [download the pre-trained models](#pre-trained-models) and [run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few minutes.   In certain cases  rather than fine-tuning the entire pre-trained model end-to-end  it can be beneficial to obtained *pre-trained contextual embeddings*  which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.  As an example  we include the script `extract_features.py` which can be used like this:  ```shell #: Sentence A and Sentence B are separated by the ||| delimiter for sentence #: pair tasks like question answering and entailment. #: For single sentence inputs  put one sentence per line and DON'T use the #: delimiter. echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt  python extract_features.py \   --input_file=/tmp/input.txt \   --output_file=/tmp/output.jsonl \   --vocab_file=$BERT_BASE_DIR/vocab.txt \   --bert_config_file=$BERT_BASE_DIR/bert_config.json \   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \   --layers=-1 -2 -3 -4 \   --max_seq_length=128 \   --batch_size=8 ```  This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by `layers` (-1 is the final hidden layer of the Transformer  etc.)  Note that this script will produce very large output files (by default  around 15kb for every input token).  If you need to maintain alignment between the original and tokenized words (for projecting training labels)  see the [Tokenization](#tokenization) section below.  **Note:** You may see a message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c  running initialization to predict.` This message is expected  it just means that we are using the `init_from_checkpoint()` API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint  this script will complain.   PyTorch version of BERT available   We are releasing the following:   The links to the models are here (right-click  'Save link as...' on the name):   The fine-tuning examples which use BERT-Base should be able to run on a GPU   on your local machine  using a GPU like a Titan X or GTX 1080.   might use the following flags instead:   You should see output like this:   Once you have trained your classifier you can use it in inference mode by using   To run on SQuAD  you will first need to download the dataset. The   Download these to some directory $SQUAD_DIR.   not seem to fit on a 12GB GPU using BERT-Large). However  a reasonably strong   For example  one random run with these parameters produces the following Dev   To run on SQuAD 2.0  you will first need to download the dataset. The necessary   Download these to some directory $SQUAD_DIR.   We assume you have copied everything from the output directory to a local   derived threshold or alternatively you can extract the appropriate answers from   effective batch sizes to be used on the GPU. The code will be based on one (or  both) of the following techniques:   Punctuation splitting: Split all punctuation characters on both sides       on the one from tensor2tensor  which is linked). E.g.  john johanson ' s   If you have a pre-tokenized representation with word-level annotations  you can   to both scripts).       you will likely get NaNs when training on GPU or TPU due to unchecked   If you want to use BERT with Colab  you can  get started with the notebook   PyTorch version of BERT available   possible that we will release larger models if we are able to obtain significant   See the section on [out-of-memory issues](#out-of-memory-issues) for more information.   """;General;https://github.com/zhang-huihui/git-repository
"""bash ./download_dataset.sh horse2zebra   """;Computer Vision;https://github.com/SarderLab/WSI-cycleGAN
"""bash ./download_dataset.sh horse2zebra   """;General;https://github.com/SarderLab/WSI-cycleGAN
"""""";Computer Vision;https://github.com/ruyueshuo/YOLOv4_Deployment
"""- pytorch.   ``` bash   ``` bash   """;Natural Language Processing;https://github.com/bino282/bert4news
"""""";Sequential;https://github.com/chris-tng/semi-supervised-nlp
"""""";Graphs;https://github.com/zfjsail/gae-pytorch
"""* Does not work on windows due to WSL not supporting host mode on the port   git clone https://github.com/vinayak19th/MDD_Project   If you have an NVIDA CUDA supported GPU  you can run the server for GPU runtime  docker run --runtime=nvidia -d -p 8501:8501 -p 8500:8500 --name bart vinayak1998th/bart_serve:gpu   """;Sequential;https://github.com/vinayak19th/Brevis-2.0
""" If you want to reproduce my imagenet pretrained models you need download ILSVRC2012 dataset and make sure the folder architecture as follows: ``` ILSVRC2012 | |-----train----1000 sub classes folders | |-----val------1000 sub classes folders Please make sure the same class has same class folder name in train and val folders. ```  If you want to reproduce my COCO pretrained models you need download COCO2017 dataset and make sure the folder architecture as follows: ``` COCO2017 | |-----annotations----all label jsons |                  |                |----train2017 |----images------|----val2017                  |----test2017 ```  If you want to reproduce my VOC pretrained models you need download VOC2007+VOC2012 dataset and make sure the folder architecture as follows: ``` VOCdataset |                 |----Annotations |                 |----ImageSets |----VOC2007------|----JPEGImages |                 |----SegmentationClass |                 |----SegmentationObject |         |                 |----Annotations |                 |----ImageSets |----VOC2012------|----JPEGImages |                 |----SegmentationClass |                 |----SegmentationObject ```   If you want to reproduce my model you need enter a training folder directory then run train.sh and test.sh.  For example you can enter classification_training/imagenet/resnet_vovnet_darknet_example.   """;General;https://github.com/zgcr/simpleAICV-pytorch-ImageNet-COCO-training
"""""";General;https://github.com/AlexHeyman/PopulationBasedTraining
"""""";Natural Language Processing;https://github.com/SindhuMadi/FakeNewsDetection
"""python ÏïÑÏ£º ÏÇ¥Ïßù  Í∏∞Î≥∏ linux Î™ÖÎ†πÏñ¥ : linux.md  Ïã§ÏäµÎÇ¥Ïö©    ÏúàÎèÑÏö∞ ÌôòÍ≤ΩÏóêÏÑú linux command HowTo : how_to_linux_command_on_windows.md   dynamic robotics 1 : https://www.youtube.com/watch?v=_sBBaNYex3E  dynamic robotics 2 : https://www.youtube.com/watch?v=94nnAOZRg8k  cart pole : https://www.youtube.com/watch?v=XiigTGKZfks  bidirectional RNN : https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66   ÎèôÏòÅÏÉÅ Ïä§ÌÉÄÏùº Î≥ÄÌôò : https://www.youtube.com/watch?v=Khuj4ASldmU   Papers with code : https://paperswithcode.com/       jupyter           cd  pwd  ls          mkdir  rm  cp           git  wget       gpu           python           pip       numpy       pandas       compile   """;Computer Vision;https://github.com/dhrim/cau_2021
"""""";General;https://github.com/narutomst/Auto-CNN-HSI-Classification
"""Packaged developed to work with Python 3+. Some examples require Python 3.6+ and nltk (<http://www.nltk.org/>) installed.  tqdm (<https://github.com/tqdm/tqdm>) not mandatory to have installed but is recommended to track the progress  especially for jobs with several hundreds of gigabytes of text.  ```bash pip install overlapy ```   It follows the contents of an usage example from one of our examples found [here](examples/).  ```python from overlapy import OverlapyTestSet  Overlapy  pretraining_dataset = [     ""A B A C D E F G""      ""A C F J K H E""      ""V L N M Q""      ""A B A C √á T Z V E""      ""L M N O P""  ]  testset_examples = [     ""B A B A C O Q W R""   #: Match A B A C with #:1  #:4 from pretraining_dataset     ""O P Q F J K H""   #: Match F J K H with #:2 from pretraining_dataset     ""W E R E""   #: No match     ""I E T Z V E L""   #: Match T Z V E with #:4 from pretraining_dataset     ""K E K W""   #: No match ] #: Total examples matched: 3   def tokenizer(s):     #: Simple tokenization by whitespace.     return s.split()   #: We'll override the parameter min_n and set it to 1 as we want the ngram value to be allowed #: to be less than 8. The testset examples were constructed for it to be 4  actually. testset = OverlapyTestSet(     ""test""  min_n=1  examples=[tokenizer(s) for s in testset_examples] ) print(f""N value: {testset.compute_n()}"") print(f""#: NGrams: {len(set(map(tuple  list(testset.ngrams()))))}"")  #: We create an Overlapy object  handing three arguments: #:   * Testsets: A list of OverlapyTestSet objects that we want to study. #:   * Dataset: Dataset we want to calculate collisions with #:   * n_workers: Number of worker processes to use overlapy = Overlapy(     testsets=[testset]      dataset=[tokenizer(s) for s in pretraining_dataset]      n_workers=2  ) #: Let's run and get the matches matches = overlapy.run()  #: We should be getting 3 testset examples that have been flagged for matches. #:    #:0 matches on A B A C #:    #:1 matches on F J K H #:    #:3 matches on T V Z E #: As we had noted above print(f""Matches: {list(testset.get_matches(matches))}"") ```   """;Natural Language Processing;https://github.com/nlx-group/overlapy
"""""";Computer Vision;https://github.com/SupreethRao99/VisionTransformer
"""""";Natural Language Processing;https://github.com/RonRaifer/BERT-Ghazali
"""üëØ Clone this repo:  $ git clone https://github.com/saimj7/People-Counting-in-Real-Time.git   """;General;https://github.com/vietvulong/Python-People-Counting-in-Real-Time-master
"""üëØ Clone this repo:  $ git clone https://github.com/saimj7/People-Counting-in-Real-Time.git   """;Computer Vision;https://github.com/vietvulong/Python-People-Counting-in-Real-Time-master
"""""";Reinforcement Learning;https://github.com/s-sd/task-amenability
"""* Training ```python CUDA_VISIBLE_DEVICES=<cuda_indice> python train.py --config config/vgg_transformer.yml ```  * Testing ```python CUDA_VISIBLE_DEVICES=<cuda_indice> python test.py --config config/vgg_transformer.yml ```  * Predicting ```python CUDA_VISIBLE_DEVICES=<cuda_indice> python predict.py --config config/vgg_transformer.yml --image <image_path> ```    """;General;https://github.com/truclycs/ocr_seq2seq
"""* Training ```python CUDA_VISIBLE_DEVICES=<cuda_indice> python train.py --config config/vgg_transformer.yml ```  * Testing ```python CUDA_VISIBLE_DEVICES=<cuda_indice> python test.py --config config/vgg_transformer.yml ```  * Predicting ```python CUDA_VISIBLE_DEVICES=<cuda_indice> python predict.py --config config/vgg_transformer.yml --image <image_path> ```    """;Natural Language Processing;https://github.com/truclycs/ocr_seq2seq
"""```bash kedro run ```   """;General;https://github.com/marnixkoops/kendrite
"""Make the following directory tree for your dataset on the project root and place original images in `train/high_resolution` and `validate/high_resolution/` directories.  ``` .datasets ‚îî‚îÄ‚îÄ (your dataset name)     ‚îú‚îÄ‚îÄ test     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ high_resolution     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ low_resolution     ‚îú‚îÄ‚îÄ train     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ high_resolution     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ low_resolution     ‚îî‚îÄ‚îÄ validate         ‚îú‚îÄ‚îÄ high_resolution         ‚îî‚îÄ‚îÄ low_resolution ```  Next  make low resolution images which have quarter size of original ones and place them in `low_resolution` directories.  This program request TFRecords as dataset. I prepare a function for you. Fix `dataset_name` and `extension` in the src/datasets.py and execute it from project root.  ```bash python src/dataset.py ```  Make sure there exists `train.tfrecords` and `valid.tfrecords` in the `datasets/(your dataset name)` directory.   Python 3.9.6   Start training with the following command   $ pipenv run train  #: If you use pipenv   Start training with the following command   You can download pre-trained weight from [here](https://drive.google.com/drive/folders/1LE1AK0HVHN-_x9S3-aCUeA0mFT-D68VW?usp=sharing). These weights are trained with 32x32 images.  ![](https://github.com/Hayashi-Yudai/SRGAN/blob/main/assets/test_data.png)  """;General;https://github.com/Hayashi-Yudai/SRGAN
"""Make the following directory tree for your dataset on the project root and place original images in `train/high_resolution` and `validate/high_resolution/` directories.  ``` .datasets ‚îî‚îÄ‚îÄ (your dataset name)     ‚îú‚îÄ‚îÄ test     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ high_resolution     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ low_resolution     ‚îú‚îÄ‚îÄ train     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ high_resolution     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ low_resolution     ‚îî‚îÄ‚îÄ validate         ‚îú‚îÄ‚îÄ high_resolution         ‚îî‚îÄ‚îÄ low_resolution ```  Next  make low resolution images which have quarter size of original ones and place them in `low_resolution` directories.  This program request TFRecords as dataset. I prepare a function for you. Fix `dataset_name` and `extension` in the src/datasets.py and execute it from project root.  ```bash python src/dataset.py ```  Make sure there exists `train.tfrecords` and `valid.tfrecords` in the `datasets/(your dataset name)` directory.   Python 3.9.6   Start training with the following command   $ pipenv run train  #: If you use pipenv   Start training with the following command   You can download pre-trained weight from [here](https://drive.google.com/drive/folders/1LE1AK0HVHN-_x9S3-aCUeA0mFT-D68VW?usp=sharing). These weights are trained with 32x32 images.  ![](https://github.com/Hayashi-Yudai/SRGAN/blob/main/assets/test_data.png)  """;Computer Vision;https://github.com/Hayashi-Yudai/SRGAN
"""[2] Mish https://github.com/digantamisra98/Mish    """;General;https://github.com/ThanhPham1987/ANN
"""[2] Mish https://github.com/digantamisra98/Mish    """;Computer Vision;https://github.com/ThanhPham1987/ANN
"""go to cd cls_seg_mt. Run python train_classification.py --input_path=path_to_data_from_step2 --outf=models/cls.   1. Clone the repository: `git clone https://github.com/sarthakTUM/roofn3d.git` 2. go to `cd roofn3d`. Install the requirements: `pip install -r requirements.txt`. It is recommended to perform this step in a separate virtual environment. If using Conda  then `conda create --name roofn3d`. 3. For classification  segmentation  and multi-task demo: `cd cls_seg_mt`.  4. Run `python demo.py`. Different examples can be seen by changing the `--idx`parameter.  For example  `python demo.py --idx=15`. The `--idx` parameter can be a maximum of 23. 5. For Roof Completion  go to `cd completion` from the roof directory. 6. Run `python demo.py`. Different example can be seen by changing the `--input_path` parameter. For example: `python demo.py --input_path=demo_data/saddleback_roof.pcd`  or `python demo.py --input_path=demo_data/twosidedhip_roof.pcd`. 7. Demo data for both the tasks have been added to demo:data directories.   The models are cloned along with the repository. If there are any difficulties in cloning the models  please download the models from: 1. https://drive.google.com/open?id=1C8X4O9SnNmvmJbzpqx_2YJR7sYTxZ3Tm for Classification  segmentation  and Multi-Task networks. Place the models in the `roofn3d/cls_seg_mt/models` directory. 2. https://drive.google.com/open?id=15r54fXLjZcFuL2ok35M1_hXxw_Fn7LJu for Completion. Place the model in `completion/log` directory.    """;Computer Vision;https://github.com/sarthakTUM/roofn3d
"""""";Computer Vision;https://github.com/RanBezen/cycleGan_handwriting
"""""";General;https://github.com/RanBezen/cycleGan_handwriting
"""```python import torch  def check_annealing(model  optimizer  param_dict):     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(         optimizer  T_max=param_dict['t_max']  eta_min=param_dict['eta_min']  last_epoch=-1)      lr_list = [0. for i in range(param_dict['epochs']) for j in range(param_dict['steps'])]     for epoch in range(param_dict['epochs']):         for idx in range(param_dict['steps']):                      now_itr = epoch * param_dict['steps'] + idx             now_lr = scheduler.get_lr()              lr_list[epoch*steps+idx] = now_lr             optimizer.step()              scheduler.step()             if optimizer.param_groups[0]['lr'] == param_dict['eta_min']:                 if param_dict['whole_decay']:                     annealed_lr = param_dict['lr'] * (1 + math.cos(                         math.pi * now_itr / (param_dict['epochs'] * param_dict['steps']) )) / 2                     optimizer.param_groups[0]['initial_lr'] = annealed_lr                 param_dict['t_max'] *= param_dict['t_mult']                 scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(                     optimizer  T_max=param_dict['t_max']  eta_min=param_dict['eta_min']  last_epoch=-1)                      return lr_list  epochs = 100 steps = 200 lr = 1.  t01_tmult2 = {     'epochs':       epochs      'steps':        steps      't_max':        steps*1      't_mult':       2      'eta_min':      0      'lr':           lr      'whole_decay':  False      'out_name':     ""T_0={}-T_mult={}"".format(steps*1  2)      }  model = torch.nn.Linear(10  2) optimizer = torch.optim.SGD(model.parameters()  lr=lr)  #: Run t01_tmult2_out = check_annealing(model  optimizer  t01_tmult2)  #: Visualize def show_graph(lr_lists  epochs  steps  out_name):     plt.clf()     plt.rcParams['figure.figsize'] = [20  5]     x = list(range(epochs * steps))     plt.plot(x  lr_lists  label=""line L"")     plt.plot()      plt.ylim(10e-5  1)     plt.yscale(""log"")     plt.xlabel(""iterations"")     plt.ylabel(""learning rate"")     plt.title(""Check Cosine Annealing Learing Rate with {}"".format(out_name))     plt.legend()     plt.show()  show_graph(t01_tmult2_out  epochs  steps  t01_tmult2['out_name']) ```   """;General;https://github.com/yumatsuoka/check_cosine_annealing_lr
"""""";Computer Vision;https://github.com/Jay0505/CycleGAN
"""""";General;https://github.com/Jay0505/CycleGAN
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;Computer Vision;https://github.com/toufiksk/darknet
"""* To run on a single task  use `babi_runner.py` with `-t` followed by task's id. For example     ``` python babi_runner.py -t 1 ``` The output will look like: ``` Using data from data/tasks_1-20_v1-2/en Train and test for task 1 ... 1 | train error: 0.876116 | val error: 0.75 |===================================               | 71% 0.5s ``` * To run on 20 tasks: ``` python babi_runner.py -a ``` * To train using all training data from 20 tasks  use the joint mode: ``` python babi_runner.py -j ```   * In order to run the Web-based demo using the pretrained model `memn2n_model.pklz` in `trained_model/`  run: ``` python -m demo.qa ```  * Alternatively  you can try the console-based demo: ``` python -m demo.qa -console ```  * The pretrained model `memn2n_model.pklz` can be created by running: ``` python -m demo.qa -train ```  * To show all options  run `python -m demo.qa -h`   """;General;https://github.com/vinhkhuc/MemN2N-babi-python
"""    export CAFFE_ROOT=$your_caffe_root      python decode.py /ABSOLUTE_PATH_TO/SqueezeNet_deploy.prototxt /ABSOLUTE_PATH_TO/compressed_SqueezeNet.net /ABSOLUTE_PATH_TO/decompressed_SqueezeNet.caffemodel      note: decompressed_SqueezeNet.caffemodel is the output  can be any name.      $CAFFE_ROOT/build/tools/caffe test --model=SqueezeNet_trainval.prototxt --weights=decompressed_SqueezeNet.caffemodel --iterations=1000 --gpu 0   """;Computer Vision;https://github.com/songhan/SqueezeNet-Deep-Compression
"""1. clone the repo and make sure you have installed `tensorflow` or `tensorflow-gpu` on your local machine.  2. run following commands ```bash python setup.py install cd examples python deepwalk_wiki.py ```   The design and implementation follows simple principles(**graph in embedding out**) as much as possible.  """;Graphs;https://github.com/shenweichen/GraphEmbedding
"""It is tested with pytorch-1.0.   """;Computer Vision;https://github.com/fxia22/pointnet.pytorch
"""pip install params-flow   .. |Python Versions| image:: https://img.shields.io/pypi/pyversions/params-flow.svg   .. kpe/bert-for-tf2: https://github.com/kpe/bert-for-tf2   """;General;https://github.com/kpe/params-flow
"""- eval_input_reader ÈáåÈù¢ÁöÑshuffleÔºå Ëøô‰∏™ÊòØË∑üevalÊ≠•È™§ÁöÑÊï∞ÊçÆreaderÊúâÂÖ≥ÔºåÂ¶ÇÊûú‰∏ç‰ΩøÁî®GPUËøõË°åËÆ≠ÁªÉÁöÑËØùÔºåËøôÈáåÈúÄË¶Å‰ªéfalseÊîπÊàêtrueÔºå‰∏çÁÑ∂‰ºöÂØºËá¥ÈîôËØØÔºåËØ¶ÁªÜÂÜÖÂÆπÂèÇÈòÖ https://github.com/tensorflow/models/issues/1936   windows ÂëΩ‰ª§Ë°å‰∏çÊîØÊåÅ * Êìç‰ΩúÁ¨¶„ÄÇ   """;General;https://github.com/pessimiss/ai100-w8-master
"""- eval_input_reader ÈáåÈù¢ÁöÑshuffleÔºå Ëøô‰∏™ÊòØË∑üevalÊ≠•È™§ÁöÑÊï∞ÊçÆreaderÊúâÂÖ≥ÔºåÂ¶ÇÊûú‰∏ç‰ΩøÁî®GPUËøõË°åËÆ≠ÁªÉÁöÑËØùÔºåËøôÈáåÈúÄË¶Å‰ªéfalseÊîπÊàêtrueÔºå‰∏çÁÑ∂‰ºöÂØºËá¥ÈîôËØØÔºåËØ¶ÁªÜÂÜÖÂÆπÂèÇÈòÖ https://github.com/tensorflow/models/issues/1936   windows ÂëΩ‰ª§Ë°å‰∏çÊîØÊåÅ * Êìç‰ΩúÁ¨¶„ÄÇ   """;Computer Vision;https://github.com/pessimiss/ai100-w8-master
"""    make -j8                  #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.        make test -j8             #: (Optional)              make runtest -j8   cd caffe   \#: You can modify the parameters in create_data.sh if needed.                                """;Computer Vision;https://github.com/tchernitski/caffe.deploy
"""The goal of Detectron is to provide a high-quality  high-performance codebase for object detection *research*. It is designed to be flexible in order to support rapid implementation and evaluation of novel research. Detectron includes implementations of the following object detection algorithms:  - [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017* - [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017* - [Faster R-CNN](https://arxiv.org/abs/1506.01497) - [RPN](https://arxiv.org/abs/1506.01497) - [Fast R-CNN](https://arxiv.org/abs/1504.08083) - [R-FCN](https://arxiv.org/abs/1605.06409)  using the following backbone network architectures:  - [ResNeXt{50 101 152}](https://arxiv.org/abs/1611.05431) - [ResNet{50 101 152}](https://arxiv.org/abs/1512.03385) - [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt) - [VGG16](https://arxiv.org/abs/1409.1556)  Additional backbone architectures may be easily implemented. For more details about these models  please see [References](#references) below.   Please find installation instructions for Caffe2 and Detectron in [`INSTALL.md`](INSTALL.md).   After installation  please see [`GETTING_STARTED.md`](GETTING_STARTED.md) for brief tutorials covering inference and training with Detectron.   To start  please check the [troubleshooting](INSTALL.md#troubleshooting) section of our installation instructions as well as our [FAQ](FAQ.md). If you couldn't find help there  try searching our GitHub issues. We intend the issues page to be a forum in which the community collectively troubleshoots problems.  If bugs are found  **we appreciate pull requests** (including adding Q&A's to `FAQ.md` and improving our installation instructions and troubleshooting documents). Please see [CONTRIBUTING.md](CONTRIBUTING.md) for more information about contributing to Detectron.   """;Computer Vision;https://github.com/jiajunhua/facebookresearch-Detectron
"""other cDDGANS implemenatations https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN and https://github.com/togheppi/cDCGAN   <td><img src=""https://github.com/DanielLongo/cGANs/blob/master/generated_images/0-20.gif""/>   """;General;https://github.com/DanielLongo/cGANs
"""Requirement **Java 1.8+** The repository already contains **WordNet 2.1**  **VerbNet 3.0** and **StanfordCoreNLP API 4.0**. Language model must be downloaded. * Copy `NLPPipeline/src/` on your project * Go to `src/lib/` and follow `""IMPORTANT -model download.txt""` to download the language model  Requirements: - Python 3.5+ - NLTK 3.0+  Requirements: - Python 3.5+ - NLTK 3.0+  The experiments in BabyAI require Python 3.6.9 (the latest version of Python supported by Google Colab). In order to install the required dependencies call pip install . inside the babyai_rb folder.    * --rb: the required bolt    To run the translator  in the `Cfg` folder enter:  ``` python ./CFG2LTLf.py --pathNL './cfg_nl.txt' --pathLTLf './cfg.ltlf' --sentence 'Go to a red ball' ``` use `--sentence` to input the sentence to translate  `--pathNL` to specify the path to the NL CFG and `--pathLTLf` to specify the path to the LTLf CFG.   To run the translator  in the `LambdaCalculus` folder enter:  ``` python ./NL2LTLf.py --path './mappings.csv' --sentence 'Go to a red ball' --set_pronouns 'True' ``` use `--sentence`  to input the sentence to translate  `--path` to specify the path to the mapping `.csv` file and `--set_pronouns` to enable/disable pronoun handling.   All auxiliary classes contains JavaDocs. To translate a sentence use `NL2LTLTranslator.translate(sentence)`. The class `NL2LTLTranslator` contains a main method with some examples.   To visualize a demo of the agent execute `babyai_rb/scripts/enjoy.py` specifying the environment  the model and the  restraining bolt (and additionally `rb-prop`). For example to visualize the model trained previously (Experiment 1) call ``` python babyai_rb/scripts/enjoy.py --env BabyAI-GoToRedBall-v0 --model $MODEL --rb VisitBoxAndPickRestrainingBolt --rb-prop 0 ``` where `$MODEL` is the name of the trained model in `babyai_rb/scripts/models/`.   """;Reinforcement Learning;https://github.com/GiadaSimionato/Reasoning_Agents_2020
"""The goal of Detectron is to provide a high-quality  high-performance codebase for object detection *research*. It is designed to be flexible in order to support rapid implementation and evaluation of novel research. Detectron includes implementations of the following object detection algorithms:  - [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017* - [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017* - [Faster R-CNN](https://arxiv.org/abs/1506.01497) - [RPN](https://arxiv.org/abs/1506.01497) - [Fast R-CNN](https://arxiv.org/abs/1504.08083) - [R-FCN](https://arxiv.org/abs/1605.06409)  using the following backbone network architectures:  - [ResNeXt{50 101 152}](https://arxiv.org/abs/1611.05431) - [ResNet{50 101 152}](https://arxiv.org/abs/1512.03385) - [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt) - [VGG16](https://arxiv.org/abs/1409.1556)  Additional backbone architectures may be easily implemented. For more details about these models  please see [References](#references) below.   Please find installation instructions for Caffe2 and Detectron in [`INSTALL.md`](INSTALL.md).   After installation  please see [`GETTING_STARTED.md`](GETTING_STARTED.md) for brief tutorials covering inference and training with Detectron.   To start  please check the [troubleshooting](INSTALL.md#troubleshooting) section of our installation instructions as well as our [FAQ](FAQ.md). If you couldn't find help there  try searching our GitHub issues. We intend the issues page to be a forum in which the community collectively troubleshoots problems.  If bugs are found  **we appreciate pull requests** (including adding Q&A's to `FAQ.md` and improving our installation instructions and troubleshooting documents). Please see [CONTRIBUTING.md](CONTRIBUTING.md) for more information about contributing to Detectron.   """;General;https://github.com/jiajunhua/facebookresearch-Detectron
"""You can download all variants by running   $ ./download_model.sh   You can change that path in the environment variable.   * AlexNet-PyTorch   Both these scripts download this code.   You can run these transfer tasks using:  $ ./eval_linear.sh   From the visu folder you can run   """;General;https://github.com/facebookresearch/deepcluster
"""The NTM can be used as a reusable module  currently not packaged though.  1. Clone repository 2. Install [PyTorch](http://pytorch.org/) 3. pip install -r requirements.txt   Execute ./train.py  ``` usage: train.py [-h] [--seed SEED] [--task {copy repeat-copy}] [-p PARAM]                 [--checkpoint-interval CHECKPOINT_INTERVAL]                 [--checkpoint-path CHECKPOINT_PATH]                 [--report-interval REPORT_INTERVAL]  optional arguments:   -h  --help            show this help message and exit   --seed SEED           Seed value for RNGs   --task {copy repeat-copy}                         Choose the task to train (default: copy)   -p PARAM  --param PARAM                         Override model params. Example: ""-pbatch_size=4                         -pnum_heads=2""   --checkpoint-interval CHECKPOINT_INTERVAL                         Checkpoint interval (default: 1000). Use 0 to disable                         checkpointing   --checkpoint-path CHECKPOINT_PATH                         Path for saving checkpoint data (default: './')   --report-interval REPORT_INTERVAL                         Reporting interval ```  """;General;https://github.com/loudinthecloud/pytorch-ntm
"""The NTM can be used as a reusable module  currently not packaged though.  1. Clone repository 2. Install [PyTorch](http://pytorch.org/) 3. pip install -r requirements.txt   Execute ./train.py  ``` usage: train.py [-h] [--seed SEED] [--task {copy repeat-copy}] [-p PARAM]                 [--checkpoint-interval CHECKPOINT_INTERVAL]                 [--checkpoint-path CHECKPOINT_PATH]                 [--report-interval REPORT_INTERVAL]  optional arguments:   -h  --help            show this help message and exit   --seed SEED           Seed value for RNGs   --task {copy repeat-copy}                         Choose the task to train (default: copy)   -p PARAM  --param PARAM                         Override model params. Example: ""-pbatch_size=4                         -pnum_heads=2""   --checkpoint-interval CHECKPOINT_INTERVAL                         Checkpoint interval (default: 1000). Use 0 to disable                         checkpointing   --checkpoint-path CHECKPOINT_PATH                         Path for saving checkpoint data (default: './')   --report-interval REPORT_INTERVAL                         Reporting interval ```  """;Sequential;https://github.com/loudinthecloud/pytorch-ntm
"""<a href='g3doc/installation.md'>Installation.</a><br>   To get help with issues you may encounter while using the DeepLab Tensorflow implementation  create a new question on [StackOverflow](https://stackoverflow.com/) with the tags ""tensorflow"" and ""deeplab"".  Please report bugs (i.e.  broken code  not usage questions) to the tensorflow/models GitHub [issue tracker](https://github.com/tensorflow/models/issues)  prefixing the issue name with ""deeplab"".   """;Computer Vision;https://github.com/Syarujianai/deeplab-commented
"""""";Computer Vision;https://github.com/lim0606/caffe-googlenet-bn
"""""";General;https://github.com/lim0606/caffe-googlenet-bn
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.  git clone https://github.com/affinelayer/pix2pix-tensorflow.git  cd pix2pix-tensorflow   ```sh #: clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git cd pix2pix-tensorflow #: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir facades_train \   --max_epochs 200 \   --input_dir facades/train \   --which_direction BtoA #: test the model python pix2pix.py \   --mode test \   --output_dir facades_test \   --input_dir facades/val \   --checkpoint facades_train ```  The test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.  If you have Docker installed  you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:  ```sh #: train the model python tools/dockrun.py python pix2pix.py \       --mode train \       --output_dir facades_train \       --max_epochs 200 \       --input_dir facades/train \       --which_direction BtoA #: test the model python tools/dockrun.py python pix2pix.py \       --mode test \       --output_dir facades_test \       --input_dir facades/val \       --checkpoint facades_train ```   <img src=""docs/combine.png"" width=""900px""/>  ```sh #: Resize source images python tools/process.py \   --input_dir photos/original \   --operation resize \   --output_dir photos/resized #: Create images with blank centers python tools/process.py \   --input_dir photos/resized \   --operation blank \   --output_dir photos/blank #: Combine resized images with blanked images python tools/process.py \   --input_dir photos/resized \   --b_dir photos/blank \   --operation combine \   --output_dir photos/combined #: Split into train/val set python tools/split.py \   --dir photos/combined ```  The folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.   """;Computer Vision;https://github.com/mgrcar/pix2pix
"""Linux with Tensorflow GPU edition + cuDNN   For colorization  your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:   Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint  this should point to the output_dir that you created previously with --mode train:   Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.  git clone https://github.com/affinelayer/pix2pix-tensorflow.git  cd pix2pix-tensorflow   ```sh #: clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git cd pix2pix-tensorflow #: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades #: train the model (this may take 1-8 hours depending on GPU  on CPU you will be waiting for a bit) python pix2pix.py \   --mode train \   --output_dir facades_train \   --max_epochs 200 \   --input_dir facades/train \   --which_direction BtoA #: test the model python pix2pix.py \   --mode test \   --output_dir facades_test \   --input_dir facades/val \   --checkpoint facades_train ```  The test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.  If you have Docker installed  you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:  ```sh #: train the model python tools/dockrun.py python pix2pix.py \       --mode train \       --output_dir facades_train \       --max_epochs 200 \       --input_dir facades/train \       --which_direction BtoA #: test the model python tools/dockrun.py python pix2pix.py \       --mode test \       --output_dir facades_test \       --input_dir facades/val \       --checkpoint facades_train ```   <img src=""docs/combine.png"" width=""900px""/>  ```sh #: Resize source images python tools/process.py \   --input_dir photos/original \   --operation resize \   --output_dir photos/resized #: Create images with blank centers python tools/process.py \   --input_dir photos/resized \   --operation blank \   --output_dir photos/blank #: Combine resized images with blanked images python tools/process.py \   --input_dir photos/resized \   --b_dir photos/blank \   --operation combine \   --output_dir photos/combined #: Split into train/val set python tools/split.py \   --dir photos/combined ```  The folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.   """;General;https://github.com/mgrcar/pix2pix
"""""";Computer Vision;https://github.com/manicman1999/CycleGAN256-BR
"""""";General;https://github.com/manicman1999/CycleGAN256-BR
"""Read the code first in the Jupyter Notebook  and create an images folders to store your image dataset (with the names renamed to 5-digit numbers starting from 0  images resized to 256x256). Then just run it and let the community know what you came up with!   """;Computer Vision;https://github.com/t0nberryking/DCGAN256
"""Read the code first in the Jupyter Notebook  and create an images folders to store your image dataset (with the names renamed to 5-digit numbers starting from 0  images resized to 256x256). Then just run it and let the community know what you came up with!   """;General;https://github.com/t0nberryking/DCGAN256
"""pytorch-generative supports the following algorithms.    Notebook: https://github.com/EugenHotaj/pytorch-generative/blob/master/notebooks/style_transfer.ipynb <br>   Notebook: https://github.com/EugenHotaj/pytorch-generative/blob/master/notebooks/cppn.ipynb <br>   Supported models are implemented as PyTorch Modules and are easy to use:  ```python from pytorch_generative import models  model = models.ImageGPT(in_channels=1  in_size=28) ... model(data) ```  Alternatively  lower level building blocks in [pytorch_generative.nn](https://github.com/EugenHotaj/pytorch-generative/blob/master/pytorch_generative/nn.py) can be used to write models from scratch. For example  we implement a convolutional [ImageGPT](https://openai.com/blog/image-gpt/)-like model below:  ```python  from torch import nn  from pytorch_generative import nn as pg_nn   class TransformerBlock(nn.Module):   """"""An ImageGPT Transformer block.""""""    def __init__(self                  n_channels                  n_attention_heads):     """"""Initializes a new TransformerBlock instance.          Args:       n_channels: The number of input and output channels.       n_attention_heads: The number of attention heads to use.     """"""     super().__init__()     self._attn = pg_nn.MaskedAttention(         in_channels=n_channels          embed_channels=n_channels          out_channels=n_channels          n_heads=n_attention_heads          is_causal=False)     self._out = nn.Sequential(         nn.Conv2d(             in_channels=n_channels               out_channels=4*n_channels               kernel_size=1)          nn.GELU()          nn.Conv2d(             in_channels=4*n_channels               out_channels=n_channels               kernel_size=1))    def forward(self  x):     x = x + self._attn(x)     return x + self._out(x)   class ImageGPT(nn.Module):   """"""The ImageGPT Model.      Note that we don't use LayerNorm because it would break the model's    autoregressive property.   """"""      def __init__(self                        in_channels                 in_size                 n_transformer_blocks=8                 n_attention_heads=4                 n_embedding_channels=16):     """"""Initializes a new ImageGPT instance.          Args:       in_channels: The number of input channels.       in_size: Size of the input images. Used to create positional encodings.       n_transformer_blocks: Number of TransformerBlocks to use.       n_attention_heads: Number of attention heads to use.       n_embedding_channels: Number of attention embedding channels to use.     """"""     super().__init__()     self._pos = nn.Parameter(torch.zeros(1  in_channels  in_size  in_size))     self._input = pg_nn.MaskedConv2d(         is_causal=True          in_channels=in_channels          out_channels=n_embedding_channels          kernel_size=3          padding=1)     self._transformer = nn.Sequential(         *[TransformerBlock(n_channels=n_embedding_channels                           n_attention_heads=n_attention_heads)           for _ in range(n_transformer_blocks)])     self._out = nn.Conv2d(in_channels=n_embedding_channels                            out_channels=in_channels                            kernel_size=1)    def forward(self  x):     x = self._input(x + self._pos)     x = self._transformer(x)     return self._out(x) ```   """;Computer Vision;https://github.com/eyalbetzalel/pytorch-generative-v2
"""""";General;https://github.com/seriousssam/deep-image-prior-transfer
"""```python    This repo. is absolutely based on official impl. from https://github.com/quark0/darts with trivial modificatio to make it run on pytorch 0.4+ version.   Instructions for acquiring PTB and WT2 can be found here. While CIFAR-10 can be automatically downloaded by torchvision  ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions here.   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/sinamoradian/SNIP-NAS-2
"""- The requred libraries are opencv  numpy and keras (any recent versions would be fine). You can install all the dependencies by: ```bash pip install -r requirements.txt ``` - Clone this repo: ```bash git clone https://github.com/andrearama/Deep-Auxiliary-Classifier-GAN cd pytorch-CycleGcd Deep-Auxiliary-Classifier-GAN ```   """;Computer Vision;https://github.com/andrearama/Deep-Auxiliary-Classifier-GAN
"""- The requred libraries are opencv  numpy and keras (any recent versions would be fine). You can install all the dependencies by: ```bash pip install -r requirements.txt ``` - Clone this repo: ```bash git clone https://github.com/andrearama/Deep-Auxiliary-Classifier-GAN cd pytorch-CycleGcd Deep-Auxiliary-Classifier-GAN ```   """;General;https://github.com/andrearama/Deep-Auxiliary-Classifier-GAN
"""""";Computer Vision;https://github.com/ArkaJU/U-Net-Satellite
"""1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   Requirements: hardware  Basic installation   1. Clone the Faster R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git   ```  2. We'll call the directory that you cloned Faster R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*     **Note 1:** If you didn't clone Faster R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `faster-rcnn` branch (or equivalent detached state). This will happen automatically *if you followed step 1 instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```  4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```  5. Download pre-computed Faster R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_faster_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `faster_rcnn_models`. See `data/README.md` for details.     These models were trained on VOC 2007 trainval.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	```  2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```  4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. [Optional] If you want to use COCO  please see some notes under `data/README.md` 7. Follow the next sections to download pre-trained ImageNet models   To train and test a Faster R-CNN detector using the **alternating optimization** algorithm from our NIPS 2015 paper  use `experiments/scripts/faster_rcnn_alt_opt.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  (""alt opt"" refers to the alternating optimization training algorithm described in the NIPS paper.)  To train and test a Faster R-CNN detector using the **approximate joint training** method  use `experiments/scripts/faster_rcnn_end2end.sh`. Output is written underneath `$FRCN_ROOT/output`.  ```Shell cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...] #: GPU_ID is the GPU you want to train on #: NET in {ZF  VGG_CNN_M_1024  VGG16} is the network arch to use #: --set ... allows you to specify fast_rcnn.config options  e.g. #:   --set EXP_DIR seed_rng1701 RNG_SEED 1701 ```  This method trains the RPN module jointly with the Fast R-CNN network  rather than alternating between training the two. It results in faster (~ 1.5x speedup) training times and similar detection accuracy. See these [slides](https://www.dropbox.com/s/xtr4yd4i5e0vw8g/iccv15_tutorial_training_rbg.pdf?dl=0) for more details.  Artifacts generated by the scripts in `tools` are written in this directory.  Trained Fast R-CNN networks are saved under:  ``` output/<experiment directory>/<dataset name>/ ```  Test outputs are saved under:  ``` output/<experiment directory>/<dataset name>/<network snapshot name>/ ```  """;Computer Vision;https://github.com/xzgz/faster-rcnn
"""""";Computer Vision;https://github.com/darr/spatial_transformer_networks
"""In this repository  we provide training data  network settings and loss designs for deep face recognition. The training data includes the normalised MS1M  VGG2 and CASIA-Webface datasets  which were already packed in MXNet binary format. The network backbones include ResNet  MobilefaceNet  MobileNet  InceptionResNet_v2  DenseNet  DPN. The loss functions include Softmax  SphereFace  CosineFace  ArcFace and Triplet (Euclidean/Angular) Loss.   ![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)  Our method  ArcFace  was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository  you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.   Install MXNet with GPU support (Python 2.7).  pip install mxnet-cu90   git clone --recursive https://github.com/deepinsight/insightface.git   For training with m1=1.0  m2=0.3  m3=0.2  run following command:   PyTorch: InsightFace_Pytorch  PyTorch: arcface-pytorch   [![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)  Please click the image to watch the Youtube video. For Bilibili users  click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).   """;General;https://github.com/Soldie/insightface-Rec.Face
"""""";General;https://github.com/igul222/improved_wgan_training
"""""";Computer Vision;https://github.com/igul222/improved_wgan_training
"""The Progressive GAN code repository contains a command-line tool for recreating bit-exact replicas of the datasets that we used in the paper. The tool also provides various utilities for operating on the datasets:  ``` usage: dataset_tool.py [-h] <command> ...      display             Display images in dataset.     extract             Extract images from dataset.     compare             Compare two datasets.     create_mnist        Create dataset for MNIST.     create_mnistrgb     Create dataset for MNIST-RGB.     create_cifar10      Create dataset for CIFAR-10.     create_cifar100     Create dataset for CIFAR-100.     create_svhn         Create dataset for SVHN.     create_lsun         Create dataset for single LSUN category.     create_celeba       Create dataset for CelebA.     create_celebahq     Create dataset for CelebA-HQ.     create_from_images  Create dataset from a directory full of images.     create_from_hdf5    Create dataset from legacy HDF5 archive.  Type ""dataset_tool.py <command> -h"" for more information. ```  The datasets are represented by directories containing the same image data in several resolutions to enable efficient streaming. There is a separate `*.tfrecords` file for each resolution  and if the dataset contains labels  they are stored in a separate file as well:  ``` > python dataset_tool.py create_cifar10 datasets/cifar10 ~/downloads/cifar10 > ls -la datasets/cifar10 drwxr-xr-x  2 user user         7 Feb 21 10:07 . drwxrwxr-x 10 user user        62 Apr  3 15:10 .. -rw-r--r--  1 user user   4900000 Feb 19 13:17 cifar10-r02.tfrecords -rw-r--r--  1 user user  12350000 Feb 19 13:17 cifar10-r03.tfrecords -rw-r--r--  1 user user  41150000 Feb 19 13:17 cifar10-r04.tfrecords -rw-r--r--  1 user user 156350000 Feb 19 13:17 cifar10-r05.tfrecords -rw-r--r--  1 user user   2000080 Feb 19 13:17 cifar10-rxx.labels ```  The ```create_*``` commands take the standard version of a given dataset as input and produce the corresponding `*.tfrecords` files as output. Additionally  the ```create_celebahq``` command requires a set of data files representing deltas with respect to the original CelebA dataset. These deltas (27.6GB) can be downloaded from [`datasets/celeba-hq-deltas`](https://drive.google.com/open?id=0B4qLcYyJmiz0TXY1NG02bzZVRGs).  **Note about module versions**: Some of the dataset commands require specific versions of Python modules and system libraries (e.g. pillow  libjpeg)  and they will give an error if the versions do not match. Please heed the error messages ‚Äì there is **no way** to get the commands to work other than installing these specific versions.   | Feature                           | TensorFlow version                            | Original Theano version   |   Pull the Progressive GAN code repository and add it to your PYTHONPATH environment variable.  Install the required Python packages with pip install -r requirements-pip.txt   Download karras2018iclr-celebahq-1024x1024.pkl from networks/tensorflow-version and place it in the same directory as the script.   """;Computer Vision;https://github.com/thenhz/progressive_growing_of_gan
"""sketch.js   the main code for placing the image as input for classifier and steps of prediction   sketch.js   the main code for placing the image as input for classifier and steps of prediction   """;General;https://github.com/JagadishSivakumar/Image-Classifier-ml5.js
"""sketch.js   the main code for placing the image as input for classifier and steps of prediction   sketch.js   the main code for placing the image as input for classifier and steps of prediction   """;Computer Vision;https://github.com/JagadishSivakumar/Image-Classifier-ml5.js
"""""";Computer Vision;https://github.com/cokowpublic/Nuclei-Segmentation
"""""";Reinforcement Learning;https://github.com/KAIST-AILab/deeprl_practice_colab
"""""";Reinforcement Learning;https://github.com/ksajan/DDPG-MAPE
"""To the best of our knowledge there was no available dataset compiled from academic papers. Therefore we decided to prepare a dataset from academic papers on arxiv.org.   All scripts related to the dataset preparation can be found in the **[dataset_generation](dataset_generation)** directory.   """;Natural Language Processing;https://github.com/inzva/fake-academic-paper-generation
"""To run the code  see the usage instructions at the top of `main.py`.   """;General;https://github.com/ArnoutDevos/maml-cifar-fs
"""""";Sequential;https://github.com/trichtu/ConvLSTM-RAU-net
"""""";Computer Vision;https://github.com/nasirtrekker/personcounter
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/fuadkhairi/darknet_yolo_train
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights  yolov2.cfg (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   yolov2-tiny.cfg (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/fuadkhairi/darknet_yolo_train
"""| Installation   Disambiguation: if you are looking for Haiku the operating system then   ""impure"" modules into pure functions that can be used with jax.jit    Because JAX installation is different depending on your CUDA version  Haiku does   to install JAX with the relevant accelerator support.  Then  install Haiku using pip:  $ pip install git+https://github.com/deepmind/dm-haiku  Alternatively  you can install via PyPI:  $ pip install -U dm-haiku  Our examples rely on additional libraries (e.g. bsuite). You can install the full set of additional requirements using pip:  $ pip install -r examples/requirements.txt   becomes my_linear). Modules can have named parameters that are accessed     #: Update parameters using SGD or Adam or ...   """;General;https://github.com/deepmind/dm-haiku
"""| Installation   Disambiguation: if you are looking for Haiku the operating system then   ""impure"" modules into pure functions that can be used with jax.jit    Because JAX installation is different depending on your CUDA version  Haiku does   to install JAX with the relevant accelerator support.  Then  install Haiku using pip:  $ pip install git+https://github.com/deepmind/dm-haiku  Alternatively  you can install via PyPI:  $ pip install -U dm-haiku  Our examples rely on additional libraries (e.g. bsuite). You can install the full set of additional requirements using pip:  $ pip install -r examples/requirements.txt   becomes my_linear). Modules can have named parameters that are accessed     #: Update parameters using SGD or Adam or ...   """;Computer Vision;https://github.com/deepmind/dm-haiku
"""  ![Summary](/images/summary.PNG)    """;Natural Language Processing;https://github.com/Raman-Raje/Machine-Reading-Comprehension-Neural-Question-Answer-
"""* This guide assumes you are using a linux computer. It also assumes that if you downloaded the json file from the internet and it was stored in your `Downloads` folder  that you have moved it to the root of your home directory.  * For convenience I made a folder in my home directory called `bin`. This will be the folder for the json file on my  regular computer. * On the Raspberry Pi I navigated to the `/opt` directory and made a folder called `bot`. I placed the json file at `/opt/bot/`. * For simplicity I will refer to the json file on my regular computer as `awesome-sr-XXXXXX.json`. In this scheme `awesome-sr` is the name of my project and `XXXXXX` is the hexadecimal number that google appends to the json file name. Because this name is long and the hex digits are hard to type I will copy and paste them when possible as I set up the Bash shell variable. * Edit the `.bashrc` file with your favorite editor. * Add the following to the  last line of the `.bashrc` file: `export GOOGLE_APPLICATION_CREDENTIALS=/path/to/json/awesome-sr-XXXXXX.json` A link follows that might be helpful: https://cloud.google.com/docs/authentication/getting-started#setting_the_environment_variable * Save the changes. * You must exit and re-enter the bash shell in a new terminal for the changes to take effect. After that you should be able to run the `game_sr.py` file. You will be charged for the service. * On the Raspberry Pi use the same general technique as above. Edit the `.basshrc` file to contain the line `export GOOGLE_APPLICATION_CREDENTIALS=/opt/bot/awesome-sr-XXXXXX.json` where `XXXXXX` is the hexadecimal label on the json file on the Rapberry Pi. This number will be different from the one on your regular computer. Here is a list of scripts and their description and possibly their location. You must execute them in order. It is recommended that you install all the packages in the `requirements.txt` file. You can do this with the command `pip3 install -r requirements.txt` 1. `do_make_glove_download.sh` This script is located in the root folder of the repository. It takes no arguments. Execute this command and the GloVe word embeddings will be downloaded on your computer. This download could take several minutes. The file is found in the `raw` folder. In order to continue to later steps you must unpack the file. In the `raw` directory  execute the command `unzip glove.6B.zip`.  2. `do_make_reddit_download.sh` This script is located in the root folder of the repository. It takes no arguments. Execute this command and the Reddit Comments JSON file will be downloaded on your computer. This download could take several hours and requires several gigabytes of space. The file is found in the `raw` folder. In order to continue to later steps you must unpack the file. In the `raw` directory execute the command `bunzip2 RC_2017-11.bz2`. Unzipping this file takes hours and consumes 30 to 50 gigabytes of space on your hard drive. 3. `do_make_db_from_reddit.py` This script is located in the root folder of the repository. It takes one argument  a specification of the location of the uunpacked Reddit Comments JSON file. Typically you would execute the command as `./do_make_db_from_reddit.py raw/RC_2017-11`. Executing this file takes several hours and outputs a sqlite data base called `input.db` in the root directory or your repository. There should be 5.9 Million paired rows of comments in the final db file. You can move the file or rename it for convenience. I typically put it in the `raw` folder. This python script uses `sqlite3`. 4. `do_make_train_test_from_db.py` This file is not located in the root folder of the repository. It is in the subfolder that the `model.py` file is found in. Execute this file with one argument  the location of the `input.db` file. The script takes several hours and creates many files in the `data` folder that the `model.py` file will later use for training. These data files are also used to create the vocabulary files that are essential for the model. 5. `do_make_vocab.py` This file is located in the directory  that the `do_make_train_test_from_db.py` is found in. It takes no arguments. It proceeds to find the most popular words in the training files and makes them into a list of vocabulary words of the size specified by the `settings.py` file. It also adds a token for unknown words and for the start and end of each sentence. If word embeddings are enabled  it will prepare the word embeddings from the GloVe download. The GloVe download does not include contractions  so if it is used no contractions will appear in the `vocab.big.txt` file. The embeddings can be disabled by specifying 'None' for `embed_size` in the `model/settings.py` file. Embeddings can be enabled with some versions of the keras model. The pytorch model is to be used without pre-set embeddings. This script could take hours to run. It puts its vocabulary list in the `data` folder  along with a modified GloVe word embeddings file. 6. `do_make_rename_train.sh` This file should be called once after the data folder is set up to create some important symbolic links that will allow the `model.py` file to find the training data. If your computer has limited resources this method can be called with a single integer  `n`  as the first argument. This sets up the symbolic links to piont the `model.py` file at the `n`th training file. It should be noted that there are about 80 training files in the `RC_2017-11` download  but these training files are simply copies of the larger training file  called `train.big.from` and `train.big.to`  split up into smaller pieces. When strung together they are identical to the bigger file. If your computer can use the bigger file it is recommended that you do so. If you are going to use the larger file  call the script withhout any arguments. If you are going to use the smaller files  call the script with the number associated with the file you are interested in. This call woudl look like this: `./do_make_rename_train.sh 1`  * --mode=MODENAME This sets the mode for the program. It can be one of the following:   * https://gist.github.com/fgolemo/b973a3fa1aaa67ac61c480ae8440e754   Download and install the Google-Cloud-Sdk. This package has the gcloud command.    You must also restart your terminal.   """;General;https://github.com/Asteur/someChatbot
"""To run a VAE model with 3 layers of stochastic units  each connected by a two-layer MLP:  *VAE: X->MLP->Z1->MLP->Z2->MLP->Z3->MLP->Z2->MLP->Z1->MLP->Xrecon*  ``` python run_models.py \ 	-lr 0.00020 \ 	-modeltype VAE \ 	-batch_size 256 \ 	-dataset mnistresample \ 	-mlp_layers 2 \ 	-latent_sizes 64 32 16 \ 	-hidden_sizes 512 256 128 \ 	-nonlin_dec leaky_rectify \ 	-nonlin_enc leaky_rectify \ 	-only_mu_up True \ 	-eq_samples 1 \ 	-iw_samples 1 \ 	-ramp_n_samples True \ 	-batch_norm True \ 	-batch_norm_output False \ 	-temp_start 0.00 -temp_epochs 200 \ 	-num_epochs 2000 -eval_epochs 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 \ 	-ladder_share_params False \ 	-only_mu_up True \ 	-lv_eps_z 1e-5 \ 	-lv_eps_out 1e-5 \ 	-outfolder results/mnistresample_VAE ```  Corresponding ladderVAE model  ``` python run_models.py \ 	-lr 0.00020 \ 	-modeltype ladderVAE \ 	-batch_size 256 \ 	-dataset mnistresample \ 	-mlp_layers 2 \ 	-latent_sizes 64 32 16 \ 	-hidden_sizes 512 256 128 \ 	-nonlin_dec leaky_rectify \ 	-nonlin_enc leaky_rectify \ 	-only_mu_up True \ 	-eq_samples 1 \ 	-iw_samples 1 \ 	-ramp_n_samples True \ 	-batch_norm True \ 	-batch_norm_output False \ 	-temp_start 0.00 -temp_epochs 200 \ 	-num_epochs 2000 -eval_epochs 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 \ 	-ladder_share_params False \ 	-only_mu_up True \ 	-lv_eps_z 1e-5 \ 	-lv_eps_out 1e-5 \ 	-outfolder results/mnistresample_ladderVAE ```  """;Computer Vision;https://github.com/casperkaae/LVAE
"""To compile this code  cuda-5.0 or cuda-5.5 is required.   Setup the paths in the build.sh script under Kernel and PluginSrc.  Run ./build.sh under the main directory. A dist directory will be created with all python codes and built shared libraries inside.   """;General;https://github.com/mavenlin/cuda-convnet
"""""";General;https://github.com/graykode/nlp-tutorial
"""""";Sequential;https://github.com/graykode/nlp-tutorial
"""""";Computer Vision;https://github.com/trichtu/ConvLSTM-RAU-net
"""The training and evaluation scripts operate on datasets stored as multi-resolution TFRecords. Each dataset is represented by a directory containing the same image data in several resolutions to enable efficient streaming. There is a separate *.tfrecords file for each resolution  and if the dataset contains labels  they are stored in a separate file as well. By default  the scripts expect to find the datasets at `datasets/<NAME>/<NAME>-<RESOLUTION>.tfrecords`. The directory can be changed by editing [config.py](./config.py):  ``` result_dir = 'results' data_dir = 'datasets' cache_dir = 'cache' ```  To obtain the FFHQ dataset (`datasets/ffhq`)  please refer to the [Flickr-Faces-HQ repository](http://stylegan.xyz/ffhq).  To obtain the CelebA-HQ dataset (`datasets/celebahq`)  please refer to the [Progressive GAN repository](https://github.com/tkarras/progressive_growing_of_gans).  To obtain other datasets  including LSUN  please consult their corresponding project pages. The datasets can be converted to multi-resolution TFRecords using the provided [dataset_tool.py](./dataset_tool.py):  ``` > python dataset_tool.py create_lsun datasets/lsun-bedroom-full ~/lsun/bedroom_lmdb --resolution 256 > python dataset_tool.py create_lsun_wide datasets/lsun-car-512x384 ~/lsun/car_lmdb --width 512 --height 384 > python dataset_tool.py create_lsun datasets/lsun-cat-full ~/lsun/cat_lmdb --resolution 256 > python dataset_tool.py create_cifar10 datasets/cifar10 ~/cifar10 > python dataset_tool.py create_from_images datasets/custom-dataset ~/custom-images ```   More examples you can find in the Jupyter notebook   3) Then you can play with Jupyter notebook   """;Computer Vision;https://github.com/Puzer/stylegan-encoder
"""This repository is for '[Pyramid Scene Parsing Network](https://arxiv.org/abs/1612.01105)'  which ranked 1st place in [ImageNet Scene Parsing Challenge 2016](http://image-net.org/challenges/LSVRC/2016/results). The code is modified from Caffe version of  [DeepLab v2](https://bitbucket.org/aquariusjay/deeplab-public-ver2) and [yjxiong](https://github.com/yjxiong/caffe/tree/mem) for evaluation. We merge the batch normalization layer named 'bn_layer' in the former one into the later one while keep the original 'batch_norm_layer' in the later one unchanged for compatibility. The difference is that 'bn_layer' contains four parameters as 'slope bias mean variance' while 'batch_norm_layer' contains two parameters as 'mean variance'. Several evaluation code is borrowed from [MIT Scene Parsing](https://github.com/CSAILVision/sceneparsing).   For installation  please follow the instructions of [Caffe](https://github.com/BVLC/caffe) and [DeepLab v2](https://bitbucket.org/aquariusjay/deeplab-public-ver2). To enable cuDNN for GPU acceleration  cuDNN v4 is needed. If you meet error related with 'matio'  please download and install [matio](https://sourceforge.net/projects/matio/files/matio/1.5.2) as required in 'DeepLab v2'.  The code has been tested successfully on Ubuntu 14.04 and 12.04 with CUDA 7.0.   1. Clone the repository:     ```shell    git clone https://github.com/hszhao/PSPNet.git    ```  2. Build Caffe and matcaffe:     ```shell    cd $PSPNET_ROOT    cp Makefile.config.example Makefile.config    vim Makefile.config    make -j8 && make matcaffe    ```  3. Evaluation:     - Evaluation code is in folder 'evaluation'.    - Download trained models and put them in folder 'evaluation/model':      - pspnet50\_ADE20K.caffemodel: [GoogleDrive](https://drive.google.com/file/d/0BzaU285cX7TCN1R3QnUwQ0hoMTA/view?usp=sharing&resourcekey=0-L6OpeoyQEhFhmmoYPmYcZA)      - pspnet101\_VOC2012.caffemodel: [GoogleDrive](https://drive.google.com/file/d/0BzaU285cX7TCNVhETE5vVUdMYk0/view?usp=sharing&resourcekey=0-xFuZ3szHOg4z0eYBhuJDSA)      - pspnet101\_cityscapes.caffemodel: [GoogleDrive](https://drive.google.com/file/d/0BzaU285cX7TCT1M3TmNfNjlUeEU/view?usp=sharing&resourcekey=0-7zvi4rARgQoXa6_AvZ17qg)    - Modify the related paths in 'eval_all.m':      - Mainly variables 'data_root' and 'eval_list'  and your image list for evaluation should be similarity to that in folder 'evaluation/samplelist' if you use this evaluation code structure.       - Matlab 'parfor' evaluation is used and the default GPUs are with ID [0:3]. Modify variable 'gpu_id_array' if needed. We assume that number of images can be divided by number of GPUs; if not  you can just pad your image list or switch to single GPU evaluation by set 'gpu_id_array' be length of one  and change 'parfor' to 'for' loop.     ```shell    cd evaluation    vim eval_all.m    ```     - Run the evaluation scripts:     ```    ./run.sh    ```  4. Results:      Prediction results will show in folder 'evaluation/mc_result' and the expected scores are:     (single scale testing denotes as 'ss' and multiple scale testing denotes as 'ms')     - PSPNet50 on ADE20K valset (mIoU/pAcc): 41.68/80.04 (ss) and 42.78/80.76 (ms)     - PSPNet101 on VOC2012 testset (mIoU): 85.41 (ms)    - PSPNet101 on cityscapes valset (mIoU/pAcc): 79.70/96.38 (ss) and 80.91/96.59 (ms)  5. Demo video:     Video processed by PSPNet101 on cityscapes dataset:     Merge with colormap on side: [Video1](https://youtu.be/rB1BmBOkKTw)     Alpha blending with value as 0.5: [Video2](https://youtu.be/HYghTzmbv6Q)   """;Computer Vision;https://github.com/hszhao/PSPNet
"""The only dependencies are h5py  Theano and Keras. Run the following commands ``` pip install --user cython h5py pip install --user git+https://github.com/Theano/Theano.git pip install --user git+https://github.com/fchollet/keras.git ```  Then  you need to install the convnetskeras module : ``` git clone https://github.com/heuritech/convnets-keras.git cd convnets-keras sudo python setup.py install ```   The weights can be found here :  * <a href=""http://files.heuritech.com/weights/alexnet_weights.h5"">AlexNet weights</a> * <a href=""http://files.heuritech.com/weights/vgg16_weights.h5"">VGG16 weights</a> * <a href=""http://files.heuritech.com/weights/vgg19_weights.h5"">VGG19 weights</a>     **BEWARE** !! : Since the networks have been trained in different settings  the preprocessing is different for the differents networks :  * For the AlexNet  the images (for the mode without the heatmap) have to be of shape (227 227). It is recommended to resize the images with a size of (256 256)  and then do a crop of size (227 227). The colors are in RGB order. ```python from keras.optimizers import SGD from convnetskeras.convnets import preprocess_image_batch  convnet  im = preprocess_image_batch(['examples/dog.jpg'] img_size=(256 256)  crop_size=(227 227)  color_mode=""rgb"")  sgd = SGD(lr=0.1  decay=1e-6  momentum=0.9  nesterov=True) model = convnet('alexnet' weights_path=""weights/alexnet_weights.h5""  heatmap=False) model.compile(optimizer=sgd  loss='mse')  out = model.predict(im) ```  * For the VGG  the images (for the mode without the heatmap) have to be of shape (224 224). It is recommended to resize the images with a size of (256 256)  and then do a crop of size (224 224). The colors are in BGR order. ```python from keras.optimizers import SGD from convnetskeras.convnets import preprocess_image_batch  convnet  im = preprocess_image_batch(['examples/dog.jpg'] img_size=(256 256)  crop_size=(224 224)  color_mode=""bgr"")  sgd = SGD(lr=0.1  decay=1e-6  momentum=0.9  nesterov=True) #:#: For the VGG16  use this command model = convnet('vgg_16' weights_path=""weights/vgg16_weights.h5""  heatmap=False) #:#: For the VGG19  use this one instead #: model = convnet('vgg_19' weights_path=""weights/vgg19_weights.h5""  heatmap=False) model.compile(optimizer=sgd  loss='mse')  out = model.predict(im)  ```    The heatmap are produced by converting the model into a fully convolutionize model. The fully connected layers are transformed into convolution layers (by using the same weights)  so we are able to compute the output of the network on each sub-frame of size (227 227) (or (224 224)) of a bigger picture. This produces a heatmap for each label of the classifier.  Using the heatmap is almost the same thing than directly classify. We suppose that we want the heatmap of the all the synsets linked with dogs  which are all the children in Wordnet of the synset ""n02084071"" (see next section to know how to find how we can get all the labels linked with a given synset) :  ```python from keras.optimizers import SGD from convnetskeras.convnets import preprocess_image_batch  convnet from convnetskeras.imagenet_tool import synset_to_dfs_ids  im = preprocess_image_batch(['examples/dog.jpg']  color_mode=""bgr"")  sgd = SGD(lr=0.1  decay=1e-6  momentum=0.9  nesterov=True) model = convnet('alexnet' weights_path=""weights/alexnet_weights.h5""  heatmap=True) model.compile(optimizer=sgd  loss='mse')  out = model.predict(im)  s = ""n02084071"" ids = synset_to_dfs_ids(s) heatmap = out[0 ids].sum(axis=0)  #: Then  we can get the image import matplotlib.pyplot as plt plt.imsave(""heatmap_dog.png"" heatmap) ``` <img src=https://raw.githubusercontent.com/heuritech/convnets-keras/master/examples/dog.jpg width=""400px"">  <img src=https://raw.githubusercontent.com/heuritech/convnets-keras/master/examples/heatmap_dog.png width=""400px"">   If you want to detect all cars  you might need to have a classification of higher level than the one given by the wordnets of ImageNet. Indeed  a lot of different synsets are present for different kinds of cars. We can then choose a synset in the tree  and select all the ids of its children :  ```python >>>synset_to_dfs_ids(""n04576211"") [670  870  880  444  671  565  705  428  791  561  757  829  866  847  547  820  408  573  575  803  407  436  468  511  609  627  656  661  751  817  665  555  569  717  864  867  675  734  656  586  847  802  660  603  612  690] ```   """;Computer Vision;https://github.com/heuritech/convnets-keras
"""**Fast R-CNN** is a fast framework for object detection with deep ConvNets. Fast R-CNN  - trains state-of-the-art models  like VGG16  9x faster than traditional R-CNN and 3x faster than SPPnet   - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time   - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet   - and is written in Python and C++/Caffe.  Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083) and later published at ICCV 2015.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	``` 	 2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```   	 4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. Follow the next sections to download pre-computed object proposals and pre-trained ImageNet models   1. Clone the Fast R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/fast-rcnn.git   ```    2. We'll call the directory that you cloned Fast R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*        **Note 1:** If you didn't clone Fast R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `fast-rcnn` branch (or equivalent detached state). This will happen automatically *if you follow these instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```      4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```      5. Download pre-computed Fast R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_fast_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `fast_rcnn_models`. See `data/README.md` for details.   Requirements: hardware  Basic installation   1. Clone the Fast R-CNN repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/fast-rcnn.git   ```    2. We'll call the directory that you cloned Fast R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*        **Note 1:** If you didn't clone Fast R-CNN with the `--recursive` flag  then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `fast-rcnn` branch (or equivalent detached state). This will happen automatically *if you follow these instructions*.  3. Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```      4. Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html      #: If you're experienced with Caffe and have all of the requirements installed     #: and your Makefile.config in place  then simply do:     make -j8 && make pycaffe     ```      5. Download pre-computed Fast R-CNN detectors     ```Shell     cd $FRCN_ROOT     ./data/scripts/fetch_fast_rcnn_models.sh     ```      This will populate the `$FRCN_ROOT/data` folder with `fast_rcnn_models`. See `data/README.md` for details.   *After successfully completing [basic installation](#installation-sufficient-for-the-demo)*  you'll be ready to run the demo.  **Python**  To run the demo ```Shell cd $FRCN_ROOT ./tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007. The object proposals are pre-computed in order to reduce installation requirements.  **Note:** If the demo crashes Caffe because your GPU doesn't have enough memory  try running the demo with a small network  e.g.  `./tools/demo.py --net caffenet` or with `--net vgg_cnn_m_1024`. Or run in CPU mode `./tools/demo.py --cpu`. Type `./tools/demo.py -h` for usage.  **MATLAB**  There's also a *basic* MATLAB demo  though it's missing some minor bells and whistles compared to the Python version. ```Shell cd $FRCN_ROOT/matlab matlab #: wait for matlab to start...  #: At the matlab prompt  run the script: >> fast_rcnn_demo ```  Fast R-CNN training is implemented in Python only  but test-time detection functionality also exists in MATLAB. See `matlab/fast_rcnn_demo.m` and `matlab/fast_rcnn_im_detect.m` for details.  **Computing object proposals**  The demo uses pre-computed selective search proposals computed with [this code](https://github.com/rbgirshick/rcnn/blob/master/selective_search/selective_search_boxes.m). If you'd like to compute proposals on your own images  there are many options. Here are some pointers; if you run into trouble using these resources please direct questions to the respective authors.  1. Selective Search: [original matlab code](http://disi.unitn.it/~uijlings/MyHomepage/index.php#page=projects1)  [python wrapper](https://github.com/sergeyk/selective_search_ijcv_with_python) 2. EdgeBoxes: [matlab code](https://github.com/pdollar/edges) 3. GOP and LPO: [python code](http://www.philkr.net/) 4. MCG: [matlab code](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/) 5. RIGOR: [matlab code](http://cpl.cc.gatech.edu/projects/RIGOR/)  Apologies if I've left your method off this list. Feel free to contact me and ask for it to be included.   1. Download the training  validation  test data and VOCdevkit  	```Shell 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 	wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 	``` 	 2. Extract all of these tars into one directory named `VOCdevkit`  	```Shell 	tar xvf VOCtrainval_06-Nov-2007.tar 	tar xvf VOCtest_06-Nov-2007.tar 	tar xvf VOCdevkit_08-Jun-2007.tar 	```  3. It should have this basic structure  	```Shell   	$VOCdevkit/                           #: development kit   	$VOCdevkit/VOCcode/                   #: VOC utility code   	$VOCdevkit/VOC2007                    #: image sets  annotations  etc.   	#: ... and several other directories ...   	```   	 4. Create symlinks for the PASCAL VOC dataset  	```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5. [Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6. Follow the next sections to download pre-computed object proposals and pre-trained ImageNet models   **Train** a Fast R-CNN detector. For example  train a VGG16 network on VOC 2007 trainval:  ```Shell ./tools/train_net.py --gpu 0 --solver models/VGG16/solver.prototxt \ 	--weights data/imagenet_models/VGG16.v2.caffemodel ```  If you see this error  ``` EnvironmentError: MATLAB command 'matlab' not found. Please add 'matlab' to your PATH. ```  then you need to make sure the `matlab` binary is in your `$PATH`. MATLAB is currently required for PASCAL VOC evaluation.  **Test** a Fast R-CNN detector. For example  test the VGG 16 network on VOC 2007 test:  ```Shell ./tools/test_net.py --gpu 1 --def models/VGG16/test.prototxt \ 	--net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel ```  Test output is written underneath `$FRCN_ROOT/output`.  **Compress** a Fast R-CNN model using truncated SVD on the fully-connected layers:  ```Shell ./tools/compress_net.py --def models/VGG16/test.prototxt \ 	--def-svd models/VGG16/compressed/test.prototxt \     --net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel #: Test the model you just compressed ./tools/test_net.py --gpu 0 --def models/VGG16/compressed/test.prototxt \ 	--net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000_svd_fc6_1024_fc7_256.caffemodel ```   """;Computer Vision;https://github.com/beassssry/U
"""1. Clone the repository ``` $ git clone https://github.com/veegee82/DARTS.git ``` 2. Go to folder ``` $ cd DARTS ``` 3. Install with pip3 ``` $ pip3 install -e . ```   1. Clone the repository ``` $ git clone https://github.com/veegee82/tf_base.git ``` 2. Go to folder ``` $ cd tf_base ``` 3. Install with pip3 ```  $ pip3 install -e . ```   """;General;https://github.com/veegee82/DARTS
"""Refer to [INSTRUCTIONS](INSTRUCTIONS.md) to install and use the code in this repo.   """;Computer Vision;https://github.com/baldassarreFe/deep-koalarization
"""Refer to [INSTRUCTIONS](INSTRUCTIONS.md) to install and use the code in this repo.   """;General;https://github.com/baldassarreFe/deep-koalarization
"""""";Computer Vision;https://github.com/yhenon/keras-spp
"""```python from ntm import NTMCell  cell = NTMCell(num_controller_layers  num_controller_units  num_memory_locations  memory_size      num_read_heads  num_write_heads  shift_range=3  output_dim=num_bits_per_output_vector      clip_value=clip_controller_output_to_value)  outputs  _ = tf.nn.dynamic_rnn(     cell=cell      inputs=inputs      time_major=False) ```  The implementation is derived from https://github.com/snowkylin/ntm  another open source NTM implementation. We make small but meaningful changes to the linked code that have a large effect on making our implementation more reliable to train and faster to converge as well as being easier to integrate with Tensorflow. Our contribution is: - We compare three different memory initialization schemes and find that initializing the memory contents of a Neural Turing Machine to small constant values works much better than random initilization or backpropagating through memory initialization. - We clip the outputs from the NTM controller into a range  which helps with optimization difficulties. - The NTMCell implements the [Tensorflow RNNCell interface](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell) so can be used directly with [tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)  etc. - We never see loss go to NaN as some other implementations report. - We implement 3 of the 5 tasks from the NTM paper. We run many experiments and report convergence speed and generalization performance for our implementation  compared to an LSTM  a DNC and for 3 memory contents initialization schemes.   ![Neural Turing Machine Copy Task - Seq len=20](/img/copy_ntm_20_0.png)   ![Neural Turing Machine Copy Task - Seq len=40](/img/copy_ntm_40_1.png)   ![Neural Turing Machine Associate Recall Task - Seq len=6 items](/img/associative_recall_ntm_6_0.png)   ![Neural Turing Machine Associate Recall Task - Seq len=12 items](/img/associative_recall_ntm_12_0.png)  In order to interpret how the NTM used its external memory we trained a network with 32 memory locations on the Copy task and graphed the read and write head address locations over time.  As you can see from the below graphs  the network first writes the sequence to memory and then reads it back in the same order it wrote it to memory. This uses both the content and location based addressing capabilities of the NTM. The pattern of writes followed by reads is what we would expect of a reasonable solution to the Copy task.   """;General;https://github.com/MarkPKCollier/NeuralTuringMachine
"""```python from ntm import NTMCell  cell = NTMCell(num_controller_layers  num_controller_units  num_memory_locations  memory_size      num_read_heads  num_write_heads  shift_range=3  output_dim=num_bits_per_output_vector      clip_value=clip_controller_output_to_value)  outputs  _ = tf.nn.dynamic_rnn(     cell=cell      inputs=inputs      time_major=False) ```  The implementation is derived from https://github.com/snowkylin/ntm  another open source NTM implementation. We make small but meaningful changes to the linked code that have a large effect on making our implementation more reliable to train and faster to converge as well as being easier to integrate with Tensorflow. Our contribution is: - We compare three different memory initialization schemes and find that initializing the memory contents of a Neural Turing Machine to small constant values works much better than random initilization or backpropagating through memory initialization. - We clip the outputs from the NTM controller into a range  which helps with optimization difficulties. - The NTMCell implements the [Tensorflow RNNCell interface](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell) so can be used directly with [tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)  etc. - We never see loss go to NaN as some other implementations report. - We implement 3 of the 5 tasks from the NTM paper. We run many experiments and report convergence speed and generalization performance for our implementation  compared to an LSTM  a DNC and for 3 memory contents initialization schemes.   ![Neural Turing Machine Copy Task - Seq len=20](/img/copy_ntm_20_0.png)   ![Neural Turing Machine Copy Task - Seq len=40](/img/copy_ntm_40_1.png)   ![Neural Turing Machine Associate Recall Task - Seq len=6 items](/img/associative_recall_ntm_6_0.png)   ![Neural Turing Machine Associate Recall Task - Seq len=12 items](/img/associative_recall_ntm_12_0.png)  In order to interpret how the NTM used its external memory we trained a network with 32 memory locations on the Copy task and graphed the read and write head address locations over time.  As you can see from the below graphs  the network first writes the sequence to memory and then reads it back in the same order it wrote it to memory. This uses both the content and location based addressing capabilities of the NTM. The pattern of writes followed by reads is what we would expect of a reasonable solution to the Copy task.   """;Sequential;https://github.com/MarkPKCollier/NeuralTuringMachine
"""""";Computer Vision;https://github.com/ANIME305/Anime-GAN-tensorflow
"""""";General;https://github.com/ANIME305/Anime-GAN-tensorflow
"""The validation set is real captcha image crawl from the railway booking website and labeled manually. Load the data as same as above  and X(feature(image)) put in ```vali_data```  Y(label) in ```vali_label```.   Before train the model  we have to load the data into memory.  Firstly we have to process X part: feature(our captcha image). The data we input to CNN should be numpy array type  so we use Pillow to read image and convert it to numpy array.  ```python for index in range(1  50001  1)     image = Image.open(""./data/train_set/"" + str(index) + "".jpg"") #:Load our image     nparr = np.array(image) #: Convert to numpy array     nparr = nparr / 255.0 ```  The shape of nparr is ```(60  200  3)```  it's same as the input we just designed in the model. And we plan to use 50 000 captcha image to train the model  so the input shape to CNN will be ```(50000  60  200  3)```. Use numpy.stack to merge them all:  ```python train_data = np.stack([np.array(Image.open(""./data/train_set/"" + str(index) + "".jpg""))/255.0 for index in range(1  50001  1)]) ```  Now  the shape of train_data is ```(50000  60  200  3)```„ÄÇ  The next is Y part  label: the answer of the training set. Because the model is multi-output(6 softmax regression classifier)  so the Y should be a list containing 6 numpy array  like this: ``` [[First digit of first image ...  First digit of last image]  [Second digit of first image ...  Second digit of last image]  [...]  [...]  [...]  [...]] ``` And every digit is present as one-hot encoding  for example 0 is ```[1  0  0  0  .... 0]```  2 is```[0  0  1  0  .... 0]```  ```python traincsv = open('./data/train_set/train.csv'  'r'  encoding = 'utf8') read_label = [toonehot(row[1]) for row in csv.reader(traincsv)] train_label = [[] for _ in range(6)] for arr in read_label:     for index in range(6):         train_label[index].append(arr[index]) train_label = [arr for arr in np.asarray(train_label)] ```   Firstly we have to observe the captcha  it's easy to find that the captcha is made up of two primary elements: - ```5 ~ 6 digits``` number and the text size is not same. Furthermore  they are being rotated  and the color is floating. - The color of background is floating  and there have some white and black interference lines  and some of them will overlay on the number.  And more...: - The angle of rotation is between about ```-55 ~ 55 degrees```  and the size is about ```25 ~ 27pt```. - We can found that one number has not only one style  so we guess that there have two fonts randomly in change. The first one obviously is ```Courier New-Bold```  and the second one is ```Times New Roman-Bold```.(You can use software such as Photoshop to cross-comparison.) - About the range of background and text color  we can through the color quantization such as k-means to get color of every background and text  and so we can calculate the color range.(I used k-means in opencv to implement.) - The color range(R/G/B) of the background is between about ```180 ~ 250```  and text is between ```10 ~ 140```. - Those interference lines form a rectangle  they have two styles: left and up sides are black  right and down sides are white  and vice versa.(you can also treat them as be rotated 180 degrees). - The number of the rectangle is between about ```30 ~ 32```  randomly distribute on captcha image  and the width and height is between about ```5 ~ 21px```. Besides  there has 40% white line will overlay on the number  and about 20% by the black line.  With these observation  we are about to generate training set! Now  let's observe where the number place on the image:  ![image](./readme_img/old/5.PNG)![image](./readme_img/old/6.PNG)![image](./readme_img/old/7.PNG)  From these images we can find that the text(number) are not regularly distributed on the image  we can guess that the text is randomly moved left or right after a rotation. There has even some text overlap together  so we can't crop the image and process only one number at a time.  Above is the captcha rule we simply observed. The implement of training set generate is in ```captcha_gen.py```  you can try to implement it in your own way.  ![image](./readme_img/old/8.jpg)  The generator finally will output 50 000 captcha image and a csv labeled answer.  ![image](./readme_img/old/9.PNG)![image](./readme_img/old/10.PNG)    It is not difficult for building a CNN model to solve a captcha  but where and how do we get a labeled training set?  ![image](./readme_img/old/2.jpeg)![image](./readme_img/old/3.jpeg)![image](./readme_img/old/4.jpeg)  We can write a program to crawl thousands of captcha image  and labeled it manually  but it's a time-consuming job! Maybe we can try to generate some captcha image by imitating it. But of course  the image we generate should be really close to the real  otherwise  the accuracy on validation set will really bad.   ‰ª•6Á¢ºÁöÑÁÇ∫‰æãÔºåÊàëÂÄëÁèæÂú®Êúâ75000ÂºµÈ©óË≠âÁ¢ºÂúñÁâáÔºåÊàëÂÄëÂèñÂÖ∂‰∏≠Ââç60000ÂºµÁÇ∫Ë®ìÁ∑¥ÈõÜÔºåÂæå15000ÂºµÁÇ∫È©óË≠âÈõÜ‰æÜË®ìÁ∑¥„ÄÇ(È©óË≠âÈõÜ‰πüÊòØ‰∏ÄÊ®£ÔºåÂè™ÊòØÊîπÊàêÂèñÂæå15000Âºµ„ÄÇ)  ```python traincsv = open('./data/6_real_train_set/captcha_train.csv'  'r'  encoding = 'utf8') train_data = np.stack([np.array(Image.open(""./data/6_real_train_set/"" + row[0] + "".jpg""))/255.0 for row in csv.reader(traincsv)][:60000]) traincsv = open('./data/6_real_train_set/captcha_train.csv'  'r'  encoding = 'utf8') read_label = [toonehot(row[1]) for row in csv.reader(traincsv)][:60000] train_label = [[] for _ in range(6)] for arr in read_label:     for index in range(6):         train_label[index].append(arr[index]) train_label = [arr for arr in np.asarray(train_label)] ```  Âè¶Â§ñÂà§Êñ∑ÊòØ5Á¢ºor6Á¢ºÁöÑÊ®°ÂûãÔºåÂâáÊòØÂêÑ‰ª•5/6Á¢ºÁöÑÂâç60000ÂºµÂêÑÈö®Ê©üÊåëÈÅ∏20000Âºµ=40000ÂºµÁï∂Ë®ìÁ∑¥ÈõÜÔºåÂâ©‰∏ã15000ÂºµÂêÑÈö®Ê©üÊåëÈÅ∏5000Âºµ=10000ÂºµÁï∂È©óË≠âÈõÜ„ÄÇ   È©óË≠âÈõÜÁöÑËºâÂÖ•ÊñπÂºèË∑üË®ìÁ∑¥ÈõÜÁõ∏ÂêåÔºåÈÄôÈÇäÁï•ÈÅé„ÄÇ   Âú®Ë®ìÁ∑¥‰πãÂâçÊàëÂÄëË¶ÅÂÖàÂ∞áË≥áÊñôËºâÂÖ•Âà∞Ë®òÊÜ∂È´î‰∏≠ÔºåÂâçÈù¢Áî¢ÁîüË®ìÁ∑¥ÈõÜÂíåÈ©óË≠âÈõÜÁöÑÊôÇÂÄôÔºåÊàëÂÄëÊòØÂ∞áÈ©óË≠âÁ¢ºÂ≠òÊàê‰∏ÄÂºµÂºµÁ∑®ËôüÂ•ΩÁöÑÂúñÁâáÔºå‰∏¶Áî®csvÊ™îË®òÈåÑ‰∏ã‰∫ÜÁ≠îÊ°à„ÄÇ  ÈÄôÈÇä‰∏ÄÊ®£‰ª•6Á¢ºÁöÑÁÇ∫‰æãÔºåÈ¶ñÂÖàÊàëÂÄëÂÖàËôïÁêÜXÁöÑÈÉ®ÂàÜÔºå‰πüÂ∞±ÊòØÁâπÂæµÂÄºÔºåÈÄôÈÇäÂ∞±ÊòØÊåáÊàëÂÄëÁöÑÂúñÁâá„ÄÇ ËÄåË¶ÅËº∏ÂÖ•ÈÄ≤CNNÁöÑË≥áÊñôÂøÖÈ†àÊòØnumpy arrayÁöÑÂΩ¢ÂºèÔºåÊâÄ‰ª•ÊàëÂÄëÁî®Pillow‰æÜËÆÄÂèñÂúñÁâá‰∏¶ËΩâÁÇ∫numpyÊ†ºÂºèÔºö  ```python traincsv = open('./data/6_imitate_train_set/captcha_train.csv'  'r'  encoding = 'utf8') for row in csv.reader(traincsv):  ¬† ¬†image = Image.open(""./data/6_imitate_train_set/"" + row[0] + "".jpg"") #: ËÆÄÂèñÂúñÁâá  ¬† ¬†nparr = np.array(image) #: ËΩâÊàênp array  ¬† ¬†nparr = nparr / 255.0 ```  ÈÄôÊôÇÊàëÂÄë‰∏ã```nparr.shape```ÔºåÂèØ‰ª•ÁúãÂà∞Áü©Èô£ÁöÑÂ§ßÂ∞èÊòØ```(60  200  3)```ÔºåË∑üÂâçÈù¢Ê®°ÂûãË®≠Ë®àÁöÑInputÊòØÁõ∏ÂêåÁöÑ„ÄÇ  ËÄåÊàëÂÄëË®àÂäÉ‰ΩøÁî®50000ÂºµÂúñÁâá‰æÜË®ìÁ∑¥ÔºåÊâÄ‰ª•ÊúÄÂæåËº∏ÂÖ•Áµ¶CNNÁöÑÁü©Èô£Â§ßÂ∞èÊúÉÊòØ```(50000  60  200  3)```ÔºåÈÄôÈÉ®ÂàÜÂè™Ë¶ÅÂà©Áî®stackÂ∞±ÂèØ‰ª•ÊääÂÆÉÂÄëÂêà‰ΩµÔºåÊï¥ÁêÜÊàê‰∏ãÈù¢:  ```python train_data = np.stack([np.array(Image.open(""./data/6_imitate_train_set/"" + row[0] + "".jpg""))/255.0 for row in csv.reader(traincsv)]) ```  ÊúÄÂæåtrain_dataÁöÑshapeÂ∞±ÊúÉÊòØ```(50000  60  200  3)```„ÄÇ  Êé•‰∏ã‰æÜYÂâáÊòØË®ìÁ∑¥ÈõÜÁöÑÊ®ôË®òÔºå‰πüÂ∞±ÊòØÊàëÂÄëË®ìÁ∑¥ÈõÜÁöÑÁ≠îÊ°à„ÄÇ  Âõ†ÁÇ∫ÊàëÂÄëÁöÑÊ®°ÂûãÊòØÂ§öËº∏Âá∫ÁöÑÁµêÊßã(6ÁµÑsoftmaxÂáΩÊï∏ÂàÜÈ°ûÂô®)ÔºåÊâÄ‰ª•YË¶ÅÊòØ‰∏ÄÂÄãÂê´Êúâ6ÂÄãnumpy arrayÁöÑlistÔºåÂ§ßÊ¶ÇÂÉèÊòØÈÄôÊ®£Ôºö ``` [[Á¨¨‰∏ÄÂºµÁ¨¨1ÂÄãÊï∏Â≠ó ... ÊúÄÂæå‰∏ÄÂºµÁ¨¨1ÂÄãÊï∏Â≠ó]  [Á¨¨‰∏ÄÂºµÁ¨¨2ÂÄãÊï∏Â≠ó ... ÊúÄÂæå‰∏ÄÂºµÁ¨¨2ÂÄãÊï∏Â≠ó]  [...]  [...]  [...]  [...]] ``` ËÄåÂÖ∂‰∏≠ÊØèÂÄãÊï∏Â≠óÈÉΩÊòØ‰ª•one-hot encodingË°®Á§∫Ôºå‰æãÂ¶Ç0Â∞±ÊòØ```[1  0  0  0  .... 0]```Ôºå2Â∞±ÊòØ```[0  0  1  0  .... 0]```  ```python traincsv = open('./data/6_imitate_train_set/captcha_train.csv'  'r'  encoding = 'utf8') #: ËÆÄÂèñË®ìÁ∑¥ÈõÜÁöÑÊ®ôË®ò read_label = [toonehot(row[1]) for row in csv.reader(traincsv)] #: Â∞áÊØè‰∏ÄË°åÁöÑÊñáÂ≠óËΩâÊàêone-hot encoding train_label = [[] for _ in range(6)] #: ÂêÑÁµÑËº∏Âá∫ÁöÑÁ≠îÊ°àË¶ÅÊîæÂà∞train_label  for arr in read_label:     for index in range(6):  ¬† ¬† ¬† ¬†train_label[index].append(arr[index]) #: ÂéüÊú¨ÊòØ[[Á¨¨1Â≠óÁ≠îÊ°à  ...  Á¨¨6Â≠óÁ≠îÊ°à] ......  [Á¨¨1Â≠óÁ≠îÊ°à  ...  Á¨¨6Â≠óÁ≠îÊ°à]]  ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†#: Ë¶ÅËΩâÊàê[[Á¨¨1Â≠óÁ≠îÊ°à ...  Á¨¨1Â≠óÁ≠îÊ°à] ...  [Á¨¨6Â≠óÁ≠îÊ°à ...  Á¨¨6Â≠óÁ≠îÊ°à]]ÊâçÁ¨¶ÂêàYÁöÑËº∏ÂÖ• train_label = [arr for arr in np.asarray(train_label)] #: ÊúÄÂæåË¶ÅÊää6ÂÄãnumpy array ÊîæÂú®‰∏ÄÂÄãlist ```   ËÆìÊàëÂÄë‰æÜÊ®°‰ªøÁî¢Áîü‰∏Ä‰∫õÈ©óË≠âÁ¢ºÂêßÔºÅ È¶ñÂÖàÊàëÂÄëË¶ÅÂÖàËßÄÂØüÈ©óË≠âÁ¢ºÔºå‰Ω†ÂèØ‰ª•ÂØ´‰∏ÄÊîØÁà¨Ëü≤Á®ãÂºè(eg.```captcha_scrawl.py```)ÂéªÊì∑Âèñ‰∏ÄÂÖ©ÁôæÂºµÈ©óË≠âÁ¢ºÂõû‰æÜÁ¥∞Á¥∞ÊØîÂ∞ç„ÄÇÊàëÂÄë‰∏çÈõ£ÁôºÁèæÂè∞ÈêµÁöÑÈ©óË≠âÁ¢º‰∏çÂ§ñ‰πéÁî±ÂÖ©ÂÄã‰∏ªË¶ÅÂÖÉÁ¥†ÁµÑÊàêÔºö - ```5 ~ 6Á¢º```ÁöÑÊï∏Â≠óÂèäËã±Êñá(‰∏çÂåÖÂê´OÂíåI)ÔºåÂ§ßÂ∞è‰ºº‰πé‰∏ç‰∏ÄËá¥ÔºåËÄå‰∏îÈÉΩÊúâÁ∂ìÈÅéÊóãËΩâÔºåÂè¶Â§ñÈ°èËâ≤ÊòØÊµÆÂãïÁöÑ„ÄÇ - ËÉåÊôØÊòØÊµÆÂãïÁöÑÈ°èËâ≤ÔºåÂè¶Â§ñÈÇÑÊúâ‰∏çÂ∞ëÂπ≤ÊìæÁöÑÁ∑öÊ¢ùÔºåÁúãËµ∑‰æÜÊáâË©≤ÊòØÁü©ÂΩ¢ÔºåÁî±ÈªëÁ∑öÂíåÁôΩÁ∑öÁµÑÊàêÔºå‰∏îÊúâÈÉ®ÂàÜÊúÉËìãÂà∞Êï∏Â≠ó‰∏äÈù¢„ÄÇ  ÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÊúÉÁôºÁèæ: - Êï∏Â≠óÁöÑÊóãËΩâËßíÂ∫¶Á¥ÑÂú®```-55 ~ 55Â∫¶```ÈñìÔºåÂ§ßÂ∞èÁ¥Ñ```25 ~ 27pt```„ÄÇ - Â≠óÂûãÁöÑÈÉ®ÂàÜÔºå‰ªîÁ¥∞ËßÄÂØüÊúÉÁôºÁèæÂêå‰∏ÄÂÄãÂ≠óÊúÉÊúâÂÖ©Á®Æ‰∏ç‰∏ÄÊ®£ÁöÑÊ®£ÂºèÔºåÊé®Ê∏¨ÊòØÊúâÂÖ©Á®ÆÂ≠óÂûãÈö®Ê©üÊõ¥ÊõøÔºåÂÖ∂‰∏≠‰∏ÄÂÄãÂæàÊòéÈ°ØÊòØ```Courier New-Bold```ÔºåÂè¶‰∏ÄÂÄãÊØîÂ∞ç‰∏Ä‰∏ã‰πü‰∏çÈõ£ÁôºÁèæÂç≥ÊòØ```Times New Roman-Bold```„ÄÇ - ËÉåÊôØÂíåÂ≠óÂûãÈ°èËâ≤ÁöÑÈÉ®ÂàÜÔºåÂèØ‰ª•Áî®‰∏Ä‰∫õËâ≤ÂΩ©ÂùáÂÄºÂåñÁöÑÊâãÊ≥ïÂø´ÈÄüÁöÑÂæûÊï∏ÁôæÂºµÁöÑÈ©óË≠âÁ¢º‰∏≠ÂæóÂá∫ÊØè‰∏ÄÂºµÁöÑËÉåÊôØÂèäÊï∏Â≠óÁöÑÈ°èËâ≤ÔºåÈÄ≤ËÄåÊàëÂÄëÂ∞±ËÉΩÁÆóÂá∫È°èËâ≤ÁöÑÁØÑÂúç„ÄÇÈÄôÈÉ®ÂàÜÂèØ‰ª•Áî®OpenCVÁöÑk-means‰æÜÂØ¶‰ΩúÔºåÈÄôÈÇäÂ∞±‰∏çÂÜçË¥ÖËø∞„ÄÇ  ËÉåÊôØÁöÑR/G/BÁØÑÂúçÁ¥ÑÊòØÂú®```180 ~ 250```ÈñìÔºåÊñáÂ≠óÁöÑÈÉ®ÂàÜÂâáÊòØ```10 ~ 140```Èñì„ÄÇ - Âπ≤ÊìæÁöÑÁ∑öÊ¢ùÊòØÁü©ÂΩ¢ÔºåÊúâÂ∑¶„ÄÅ‰∏äÊòØÈªëÁ∑öÊ¢ù‰∏îÂè≥„ÄÅ‰∏ãÊòØÁôΩÁ∑öÊ¢ùÂíåÂÄíÈÅé‰æÜÔºåÂÖ±ÂÖ©Á®ÆÊ®£Âºè(‰πüÂèØ‰ª•Áï∂‰ΩúÊòØÊóãËΩâ180Â∫¶)ÔºåÂπ≥ÂùáÂ§ßÁ¥ÑÊúÉÂá∫Áèæ```30 ~ 32ÂÄã```Èö®Ê©üÂàÜÂ∏ÉÂú®Âúñ‰∏≠ÔºåÈï∑ÂØ¨ÈÉΩÂ§ßÁ¥ÑËêΩÂú®```5 ~ 21px```Èñì„ÄÇ Âè¶Â§ñÔºåÂ§ßÁ¥ÑÊúâ4ÊàêÁöÑÊ©üÊúÉÁôΩÁ∑öÊúÉËìãÂú®Êï∏Â≠ó‰∏äÔºåÈªëÁ∑öËìãÂú®ÊñáÂ≠ó‰∏äÁöÑÊ©üÁéáÂâáÊõ¥‰Ωé„ÄÇ  Êúâ‰∫ÜÈÄô‰∫õËßÄÂØüÔºåÂè™Â∑Æ‰∏ÄÈªûÈªûÂ∞±ÂèØ‰ª•Áî¢ÁîüË®ìÁ∑¥ÈõÜ‰∫ÜÔºåÊàëÂÄëÁèæÂú®‰æÜËßÄÂØüÊñáÂ≠óÈÉΩËêΩÂú®ÂúñÁâá‰∏äÁöÑÁîöÈ∫º‰ΩçÁΩÆ‰∏ä:  ![image](./readme_img/captcha_seperate1.png)![image](./readme_img/captcha_seperate2.png)![image](./readme_img/captcha_seperate3.png)  ÂæûÈÄôÂπæÂºµÂúñ‰∏≠‰∏çÈõ£ÁúãÂá∫ÊñáÂ≠ó‰∏¶ÈùûË¶èÂâáÂú∞ÂàÜÂ∏ÉÂú®ÂúñÁâá‰∏äÔºåÊàëÂÄëÂèØ‰ª•ÁåúÊ∏¨ÊñáÂ≠óÊòØÊóãËΩâÂæåË¢´Èö®Ê©üÂ∑¶ÁßªÊàñÂè≥Áßª‰∫ÜÔºåÁîöËá≥ÈÇÑÊúÉÊúâÈáçÁñäÁöÑÊÉÖÊ≥ÅÔºåÊâÄ‰ª•Ê≤íËæ¶Ê≥ïÁî®ÂàáÂâ≤ÁöÑÊñπÂºè‰∏ÄÊ¨°ËôïÁêÜ‰∏ÄÂÄãÊñáÂ≠ó„ÄÇ  ‰ª•‰∏äÂ∞±ÊòØÊàëÂÄëÁ∞°ÂñÆËßÄÂØüÂà∞ÁöÑÈ©óË≠âÁ¢ºË¶èÂâáÔºåË®ìÁ∑¥ÈõÜÁî¢ÁîüÁöÑÈÉ®ÂàÜÂØ¶‰ΩúÂú®```captcha_gen.py```‰∏≠ÔºåÈõñÁÑ∂ÂØ´ÂæóÊúâÈªûÈõú‰∫ÇÔºå‰∏çÈÅéÊ≤íÁîöÈ∫ºÁâπÂà•ÁöÑÂú∞ÊñπÔºåÂ∞±ÊòØÁÖßËëó‰∏äÈù¢ÁöÑË¶èÂâáÁî¢ÁîüÔºåÂèØ‰ª•Ë©¶Ëëó‰ª•Ëá™Â∑±ÁöÑÊñπÂºèÂØ¶‰ΩúÁúãÁúã„ÄÇ  ![image](./readme_img/captcha_sample4.jpg)  ```python if __name__ == ""__main__"":     generate(50000  ""./data/56_imitate_train_set/""   ENGP=100  FIVEP=50  ENGNOLIMIT=True  filename=""train"")     generate(10240  ""./data/56_imitate_vali_set/""   ENGP=100  FIVEP=50  ENGNOLIMIT=True  filename=""vali"")     generate(50000  ""./data/5_imitate_train_set/""   ENGP=100  FIVEP=100  ENGNOLIMIT=True  filename=""train"")     generate(10240  ""./data/5_imitate_vali_set/""   ENGP=100  FIVEP=100  ENGNOLIMIT=True  filename=""vali"")     generate(50000  ""./data/6_imitate_train_set/""   ENGP=100  FIVEP=0  ENGNOLIMIT=True  filename=""train"")     generate(10240  ""./data/6_imitate_vali_set/""   ENGP=100  FIVEP=0  ENGNOLIMIT=True  filename=""vali"") ```  ÊúÄÂæåÊúÉÁÇ∫ÊàëÂÄëÈ†êË®àÂª∫Á´ãÁöÑ‰∏âÂÄãCNN(2.2.1ÊúÉÊèêÂà∞)ÂêÑÂàÜÂà•Áî¢Áîü50000Á≠ÜTraining dataÂíå10240Á≠ÜValidate dataÔºåÁ≠îÊ°àÂâáÊ®ôË®òÂú®csvÊ™î‰∏≠„ÄÇ  ![image](./readme_img/csv.png)![image](./readme_img/generate.png)    Ë¶ÅÂª∫Á´ã‰∏ÄÂÄãËæ®Ë≠òÈ©óË≠âÁ¢ºÁöÑCNNÊ®°ÂûãÂÖ∂ÂØ¶‰∏¶ÈùûÈõ£‰∫ãÔºåÈõ£ÁöÑÊòØË¶ÅÂ¶Ç‰ΩïÂèñÂæóÊ®ôË®òÂ•ΩÁöÑË®ìÁ∑¥ÈõÜÂë¢?  ![image](./readme_img/captcha_sample1.jpg)![image](./readme_img/captcha_sample2.jpg)![image](./readme_img/captcha_sample3.jpg)  Âú®ÈÄôÈÇäÊàëÂÄëÊúÉÂòóË©¶ÂÖ©Á®ÆÊñπÊ≥ï(2.Ëàá3.)Ôºö  Click here or scroll down for english version   ÂàÜÈ°ûÈ©óË≠âÁ¢ºÊòØ5Á¢º or 6Á¢ºÁöÑÊ®°Âûã(train_cnn_imitate_56)ÂâáÈÅîÂà∞Á¥Ñ98.13%„ÄÇ   """;Computer Vision;https://github.com/JasonLiTW/simple-railway-captcha-solver
"""Two experiments are included in this repository  where benchmarks are from the paper [Generalized Sliced Wasserstein Distances](http://papers.nips.cc/paper/8319-generalized-sliced-wasserstein-distances) and the paper [Distributional Sliced-Wasserstein and Applications to Generative Modeling](https://arxiv.org/pdf/2002.07367.pdf)  respectively. The first one is on the task of sliced Wasserstein flow  and the second one is on generative modellings with GANs. For more details and setups  please refer to the original paper **[Augmented Sliced Wasserstein Distances](https://arxiv.org/abs/2006.08812)**.  To install the required python packages  run the following command:  pip install -r requirements.txt   The pytorch code for calculating the FID score is from https://github.com/mseitzer/pytorch-fid.   The generative modelling experiment evaluates the performances of GANs trained with different sliced-based Wasserstein metrics. To train and evaluate the model  run the following command:  ``` python main.py  --model-type ASWD --dataset CIFAR --epochs 200 --num-projection 1000 --batch-size 512 --lr 0.0005 ```   """;General;https://github.com/xiongjiechen/ASWD
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-91-download-archive  OpenCV 3.3.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.3.0/opencv-3.3.0-vc14.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;Computer Vision;https://github.com/wpsliu123/yolo-windows
"""1. Follow the instructions for [installing Sonnet](https://github.com/deepmind/sonnet).  2. clone this repository using  `$ git clone https://github.com/vijayvee/behavior_recognition`  3. Add the cloned repository's parent path to $PYTHONPATH as follows  `cd <parent_dir>/behavior_recognition; export PYTHONPATH=$PYTHONPATH:<parent_dir>`   """;Computer Vision;https://github.com/vijayvee/behavior_recognition
"""""";Computer Vision;https://github.com/martinarjovsky/WassersteinGAN
"""<a href='g3doc/installation.md'>Installation.</a><br>   To get help with issues you may encounter while using the DeepLab Tensorflow implementation  create a new question on [StackOverflow](https://stackoverflow.com/) with the tags ""tensorflow"" and ""deeplab"".  Please report bugs (i.e.  broken code  not usage questions) to the tensorflow/models GitHub [issue tracker](https://github.com/tensorflow/models/issues)  prefixing the issue name with ""deeplab"".   """;General;https://github.com/Syarujianai/deeplab-commented
"""release of this package.) Requires python >= 2.7  cython  numpy  scipy.   """;General;https://github.com/weiwang2330/sparse-structured-attention
"""""";General;https://github.com/asvcode/1_cycle
"""The demos in this folder are designed to give straightforward samples of using TensorFlow in mobile applications.  Inference is done using the [TensorFlow Android Inference Interface](../../../tensorflow/contrib/android)  which may be built separately if you want a standalone library to drop into your existing application. Object tracking and YUV -> RGB conversion is handled by libtensorflow_demo.so.  A device running Android 5.0 (API 21) or higher is required to run the demo due to the use of the camera2 API  although the native libraries themselves can run on API >= 14 devices.   Make sure that adb debugging is enabled on your Android 5.0 (API 21) or later device  then after building use the following command from your workspace root to install the APK:  ```bash adb install -r bazel-bin/tensorflow/examples/android/tensorflow_demo.apk ```   The TensorFlow `GraphDef`s that contain the model definitions and weights are not packaged in the repo because of their size. They are downloaded automatically and packaged with the APK by Bazel via a new_http_archive defined in `WORKSPACE` during the build process  and by Gradle via download-models.gradle.  **Optional**: If you wish to place the models in your assets manually  remove all of the `model_files` entries from the `assets` list in `tensorflow_demo` found in the `[BUILD](BUILD)` file. Then download and extract the archives yourself to the `assets` directory in the source tree:  ```bash BASE_URL=https://storage.googleapis.com/download.tensorflow.org/models for MODEL_ZIP in inception5h.zip mobile_multibox_v1a.zip stylize_v1.zip do   curl -L ${BASE_URL}/${MODEL_ZIP} -o /tmp/${MODEL_ZIP}   unzip /tmp/${MODEL_ZIP} -d tensorflow/examples/android/assets/ done ```  This will extract the models and their associated metadata files to the local assets/ directory.  If you are using Gradle  make sure to remove download-models.gradle reference from build.gradle after your manually download models; otherwise gradle might download models again and overwrite your models.   Bazel is the primary build system for TensorFlow. To build with Bazel  it and the Android NDK and SDK must be installed on your system.  1. Install the latest version of Bazel as per the instructions [on the Bazel website](https://bazel.build/versions/master/docs/install.html). 2. The Android NDK is required to build the native (C/C++) TensorFlow code.         The current recommended version is 12b  which may be found         [here](https://developer.android.com/ndk/downloads/older_releases.html#ndk-12b-downloads). 3. The Android SDK and build tools may be obtained         [here](https://developer.android.com/tools/revisions/build-tools.html)          or alternatively as part of         [Android Studio](https://developer.android.com/studio/index.html). Build         tools API >= 23 is required to build the TF Android demo (though it will         run on API >= 21 devices).   nightly build   that Windows users download the  prebuilt binaries   you may build the APK. Run this from your workspace root:   If you get build errors about protocol buffers  run  git submodule update --init and make sure that you've modified your WORKSPACE   make sure that you can build with Bazel following the above directions. Then   look at build.gradle and make sure that the path to Bazel   Android Studio project. Click through installing all the Gradle extensions it  requests  and you should be able to have Android Studio build the demo like any   This folder contains an example application utilizing TensorFlow for Android devices.   Once the app is installed it can be started via the ""TF Classify""  ""TF Detect"" and ""TF Stylize"" icons  which have the orange TensorFlow logo as their icon.  While running the activities  pressing the volume keys on your device will toggle debug visualizations on/off  rendering additional info to the screen that may be useful for development purposes.   Pick your preferred approach below. At the moment  we have full support for Bazel  and partial support for gradle  cmake  make  and Android Studio.  As a first step for all build types  clone the TensorFlow repo with:  ``` git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git ```  Note that `--recurse-submodules` is necessary to prevent some issues with protobuf compilation.   """;Computer Vision;https://github.com/GenesisDCarmen/C_Reconocimiento_Facial
"""The demos in this folder are designed to give straightforward samples of using TensorFlow in mobile applications.  Inference is done using the [TensorFlow Android Inference Interface](../../../tensorflow/contrib/android)  which may be built separately if you want a standalone library to drop into your existing application. Object tracking and YUV -> RGB conversion is handled by libtensorflow_demo.so.  A device running Android 5.0 (API 21) or higher is required to run the demo due to the use of the camera2 API  although the native libraries themselves can run on API >= 14 devices.   Make sure that adb debugging is enabled on your Android 5.0 (API 21) or later device  then after building use the following command from your workspace root to install the APK:  ```bash adb install -r bazel-bin/tensorflow/examples/android/tensorflow_demo.apk ```   The TensorFlow `GraphDef`s that contain the model definitions and weights are not packaged in the repo because of their size. They are downloaded automatically and packaged with the APK by Bazel via a new_http_archive defined in `WORKSPACE` during the build process  and by Gradle via download-models.gradle.  **Optional**: If you wish to place the models in your assets manually  remove all of the `model_files` entries from the `assets` list in `tensorflow_demo` found in the `[BUILD](BUILD)` file. Then download and extract the archives yourself to the `assets` directory in the source tree:  ```bash BASE_URL=https://storage.googleapis.com/download.tensorflow.org/models for MODEL_ZIP in inception5h.zip mobile_multibox_v1a.zip stylize_v1.zip do   curl -L ${BASE_URL}/${MODEL_ZIP} -o /tmp/${MODEL_ZIP}   unzip /tmp/${MODEL_ZIP} -d tensorflow/examples/android/assets/ done ```  This will extract the models and their associated metadata files to the local assets/ directory.  If you are using Gradle  make sure to remove download-models.gradle reference from build.gradle after your manually download models; otherwise gradle might download models again and overwrite your models.   Bazel is the primary build system for TensorFlow. To build with Bazel  it and the Android NDK and SDK must be installed on your system.  1. Install the latest version of Bazel as per the instructions [on the Bazel website](https://bazel.build/versions/master/docs/install.html). 2. The Android NDK is required to build the native (C/C++) TensorFlow code.         The current recommended version is 12b  which may be found         [here](https://developer.android.com/ndk/downloads/older_releases.html#ndk-12b-downloads). 3. The Android SDK and build tools may be obtained         [here](https://developer.android.com/tools/revisions/build-tools.html)          or alternatively as part of         [Android Studio](https://developer.android.com/studio/index.html). Build         tools API >= 23 is required to build the TF Android demo (though it will         run on API >= 21 devices).   nightly build   that Windows users download the  prebuilt binaries   you may build the APK. Run this from your workspace root:   If you get build errors about protocol buffers  run  git submodule update --init and make sure that you've modified your WORKSPACE   make sure that you can build with Bazel following the above directions. Then   look at build.gradle and make sure that the path to Bazel   Android Studio project. Click through installing all the Gradle extensions it  requests  and you should be able to have Android Studio build the demo like any   This folder contains an example application utilizing TensorFlow for Android devices.   Once the app is installed it can be started via the ""TF Classify""  ""TF Detect"" and ""TF Stylize"" icons  which have the orange TensorFlow logo as their icon.  While running the activities  pressing the volume keys on your device will toggle debug visualizations on/off  rendering additional info to the screen that may be useful for development purposes.   Pick your preferred approach below. At the moment  we have full support for Bazel  and partial support for gradle  cmake  make  and Android Studio.  As a first step for all build types  clone the TensorFlow repo with:  ``` git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git ```  Note that `--recurse-submodules` is necessary to prevent some issues with protobuf compilation.   """;General;https://github.com/GenesisDCarmen/C_Reconocimiento_Facial
"""""";General;https://github.com/jiye-ML/Classify_FRACTALNET
"""""";Computer Vision;https://github.com/jiye-ML/Classify_FRACTALNET
"""The validation set is real captcha image crawl from the railway booking website and labeled manually. Load the data as same as above  and X(feature(image)) put in ```vali_data```  Y(label) in ```vali_label```.   Before train the model  we have to load the data into memory.  Firstly we have to process X part: feature(our captcha image). The data we input to CNN should be numpy array type  so we use Pillow to read image and convert it to numpy array.  ```python for index in range(1  50001  1)     image = Image.open(""./data/train_set/"" + str(index) + "".jpg"") #:Load our image     nparr = np.array(image) #: Convert to numpy array     nparr = nparr / 255.0 ```  The shape of nparr is ```(60  200  3)```  it's same as the input we just designed in the model. And we plan to use 50 000 captcha image to train the model  so the input shape to CNN will be ```(50000  60  200  3)```. Use numpy.stack to merge them all:  ```python train_data = np.stack([np.array(Image.open(""./data/train_set/"" + str(index) + "".jpg""))/255.0 for index in range(1  50001  1)]) ```  Now  the shape of train_data is ```(50000  60  200  3)```„ÄÇ  The next is Y part  label: the answer of the training set. Because the model is multi-output(6 softmax regression classifier)  so the Y should be a list containing 6 numpy array  like this: ``` [[First digit of first image ...  First digit of last image]  [Second digit of first image ...  Second digit of last image]  [...]  [...]  [...]  [...]] ``` And every digit is present as one-hot encoding  for example 0 is ```[1  0  0  0  .... 0]```  2 is```[0  0  1  0  .... 0]```  ```python traincsv = open('./data/train_set/train.csv'  'r'  encoding = 'utf8') read_label = [toonehot(row[1]) for row in csv.reader(traincsv)] train_label = [[] for _ in range(6)] for arr in read_label:     for index in range(6):         train_label[index].append(arr[index]) train_label = [arr for arr in np.asarray(train_label)] ```   Firstly we have to observe the captcha  it's easy to find that the captcha is made up of two primary elements: - ```5 ~ 6 digits``` number and the text size is not same. Furthermore  they are being rotated  and the color is floating. - The color of background is floating  and there have some white and black interference lines  and some of them will overlay on the number.  And more...: - The angle of rotation is between about ```-55 ~ 55 degrees```  and the size is about ```25 ~ 27pt```. - We can found that one number has not only one style  so we guess that there have two fonts randomly in change. The first one obviously is ```Courier New-Bold```  and the second one is ```Times New Roman-Bold```.(You can use software such as Photoshop to cross-comparison.) - About the range of background and text color  we can through the color quantization such as k-means to get color of every background and text  and so we can calculate the color range.(I used k-means in opencv to implement.) - The color range(R/G/B) of the background is between about ```180 ~ 250```  and text is between ```10 ~ 140```. - Those interference lines form a rectangle  they have two styles: left and up sides are black  right and down sides are white  and vice versa.(you can also treat them as be rotated 180 degrees). - The number of the rectangle is between about ```30 ~ 32```  randomly distribute on captcha image  and the width and height is between about ```5 ~ 21px```. Besides  there has 40% white line will overlay on the number  and about 20% by the black line.  With these observation  we are about to generate training set! Now  let's observe where the number place on the image:  ![image](./readme_img/old/5.PNG)![image](./readme_img/old/6.PNG)![image](./readme_img/old/7.PNG)  From these images we can find that the text(number) are not regularly distributed on the image  we can guess that the text is randomly moved left or right after a rotation. There has even some text overlap together  so we can't crop the image and process only one number at a time.  Above is the captcha rule we simply observed. The implement of training set generate is in ```captcha_gen.py```  you can try to implement it in your own way.  ![image](./readme_img/old/8.jpg)  The generator finally will output 50 000 captcha image and a csv labeled answer.  ![image](./readme_img/old/9.PNG)![image](./readme_img/old/10.PNG)    It is not difficult for building a CNN model to solve a captcha  but where and how do we get a labeled training set?  ![image](./readme_img/old/2.jpeg)![image](./readme_img/old/3.jpeg)![image](./readme_img/old/4.jpeg)  We can write a program to crawl thousands of captcha image  and labeled it manually  but it's a time-consuming job! Maybe we can try to generate some captcha image by imitating it. But of course  the image we generate should be really close to the real  otherwise  the accuracy on validation set will really bad.   ‰ª•6Á¢ºÁöÑÁÇ∫‰æãÔºåÊàëÂÄëÁèæÂú®Êúâ75000ÂºµÈ©óË≠âÁ¢ºÂúñÁâáÔºåÊàëÂÄëÂèñÂÖ∂‰∏≠Ââç60000ÂºµÁÇ∫Ë®ìÁ∑¥ÈõÜÔºåÂæå15000ÂºµÁÇ∫È©óË≠âÈõÜ‰æÜË®ìÁ∑¥„ÄÇ(È©óË≠âÈõÜ‰πüÊòØ‰∏ÄÊ®£ÔºåÂè™ÊòØÊîπÊàêÂèñÂæå15000Âºµ„ÄÇ)  ```python traincsv = open('./data/6_real_train_set/captcha_train.csv'  'r'  encoding = 'utf8') train_data = np.stack([np.array(Image.open(""./data/6_real_train_set/"" + row[0] + "".jpg""))/255.0 for row in csv.reader(traincsv)][:60000]) traincsv = open('./data/6_real_train_set/captcha_train.csv'  'r'  encoding = 'utf8') read_label = [toonehot(row[1]) for row in csv.reader(traincsv)][:60000] train_label = [[] for _ in range(6)] for arr in read_label:     for index in range(6):         train_label[index].append(arr[index]) train_label = [arr for arr in np.asarray(train_label)] ```  Âè¶Â§ñÂà§Êñ∑ÊòØ5Á¢ºor6Á¢ºÁöÑÊ®°ÂûãÔºåÂâáÊòØÂêÑ‰ª•5/6Á¢ºÁöÑÂâç60000ÂºµÂêÑÈö®Ê©üÊåëÈÅ∏20000Âºµ=40000ÂºµÁï∂Ë®ìÁ∑¥ÈõÜÔºåÂâ©‰∏ã15000ÂºµÂêÑÈö®Ê©üÊåëÈÅ∏5000Âºµ=10000ÂºµÁï∂È©óË≠âÈõÜ„ÄÇ   È©óË≠âÈõÜÁöÑËºâÂÖ•ÊñπÂºèË∑üË®ìÁ∑¥ÈõÜÁõ∏ÂêåÔºåÈÄôÈÇäÁï•ÈÅé„ÄÇ   Âú®Ë®ìÁ∑¥‰πãÂâçÊàëÂÄëË¶ÅÂÖàÂ∞áË≥áÊñôËºâÂÖ•Âà∞Ë®òÊÜ∂È´î‰∏≠ÔºåÂâçÈù¢Áî¢ÁîüË®ìÁ∑¥ÈõÜÂíåÈ©óË≠âÈõÜÁöÑÊôÇÂÄôÔºåÊàëÂÄëÊòØÂ∞áÈ©óË≠âÁ¢ºÂ≠òÊàê‰∏ÄÂºµÂºµÁ∑®ËôüÂ•ΩÁöÑÂúñÁâáÔºå‰∏¶Áî®csvÊ™îË®òÈåÑ‰∏ã‰∫ÜÁ≠îÊ°à„ÄÇ  ÈÄôÈÇä‰∏ÄÊ®£‰ª•6Á¢ºÁöÑÁÇ∫‰æãÔºåÈ¶ñÂÖàÊàëÂÄëÂÖàËôïÁêÜXÁöÑÈÉ®ÂàÜÔºå‰πüÂ∞±ÊòØÁâπÂæµÂÄºÔºåÈÄôÈÇäÂ∞±ÊòØÊåáÊàëÂÄëÁöÑÂúñÁâá„ÄÇ ËÄåË¶ÅËº∏ÂÖ•ÈÄ≤CNNÁöÑË≥áÊñôÂøÖÈ†àÊòØnumpy arrayÁöÑÂΩ¢ÂºèÔºåÊâÄ‰ª•ÊàëÂÄëÁî®Pillow‰æÜËÆÄÂèñÂúñÁâá‰∏¶ËΩâÁÇ∫numpyÊ†ºÂºèÔºö  ```python traincsv = open('./data/6_imitate_train_set/captcha_train.csv'  'r'  encoding = 'utf8') for row in csv.reader(traincsv):  ¬† ¬†image = Image.open(""./data/6_imitate_train_set/"" + row[0] + "".jpg"") #: ËÆÄÂèñÂúñÁâá  ¬† ¬†nparr = np.array(image) #: ËΩâÊàênp array  ¬† ¬†nparr = nparr / 255.0 ```  ÈÄôÊôÇÊàëÂÄë‰∏ã```nparr.shape```ÔºåÂèØ‰ª•ÁúãÂà∞Áü©Èô£ÁöÑÂ§ßÂ∞èÊòØ```(60  200  3)```ÔºåË∑üÂâçÈù¢Ê®°ÂûãË®≠Ë®àÁöÑInputÊòØÁõ∏ÂêåÁöÑ„ÄÇ  ËÄåÊàëÂÄëË®àÂäÉ‰ΩøÁî®50000ÂºµÂúñÁâá‰æÜË®ìÁ∑¥ÔºåÊâÄ‰ª•ÊúÄÂæåËº∏ÂÖ•Áµ¶CNNÁöÑÁü©Èô£Â§ßÂ∞èÊúÉÊòØ```(50000  60  200  3)```ÔºåÈÄôÈÉ®ÂàÜÂè™Ë¶ÅÂà©Áî®stackÂ∞±ÂèØ‰ª•ÊääÂÆÉÂÄëÂêà‰ΩµÔºåÊï¥ÁêÜÊàê‰∏ãÈù¢:  ```python train_data = np.stack([np.array(Image.open(""./data/6_imitate_train_set/"" + row[0] + "".jpg""))/255.0 for row in csv.reader(traincsv)]) ```  ÊúÄÂæåtrain_dataÁöÑshapeÂ∞±ÊúÉÊòØ```(50000  60  200  3)```„ÄÇ  Êé•‰∏ã‰æÜYÂâáÊòØË®ìÁ∑¥ÈõÜÁöÑÊ®ôË®òÔºå‰πüÂ∞±ÊòØÊàëÂÄëË®ìÁ∑¥ÈõÜÁöÑÁ≠îÊ°à„ÄÇ  Âõ†ÁÇ∫ÊàëÂÄëÁöÑÊ®°ÂûãÊòØÂ§öËº∏Âá∫ÁöÑÁµêÊßã(6ÁµÑsoftmaxÂáΩÊï∏ÂàÜÈ°ûÂô®)ÔºåÊâÄ‰ª•YË¶ÅÊòØ‰∏ÄÂÄãÂê´Êúâ6ÂÄãnumpy arrayÁöÑlistÔºåÂ§ßÊ¶ÇÂÉèÊòØÈÄôÊ®£Ôºö ``` [[Á¨¨‰∏ÄÂºµÁ¨¨1ÂÄãÊï∏Â≠ó ... ÊúÄÂæå‰∏ÄÂºµÁ¨¨1ÂÄãÊï∏Â≠ó]  [Á¨¨‰∏ÄÂºµÁ¨¨2ÂÄãÊï∏Â≠ó ... ÊúÄÂæå‰∏ÄÂºµÁ¨¨2ÂÄãÊï∏Â≠ó]  [...]  [...]  [...]  [...]] ``` ËÄåÂÖ∂‰∏≠ÊØèÂÄãÊï∏Â≠óÈÉΩÊòØ‰ª•one-hot encodingË°®Á§∫Ôºå‰æãÂ¶Ç0Â∞±ÊòØ```[1  0  0  0  .... 0]```Ôºå2Â∞±ÊòØ```[0  0  1  0  .... 0]```  ```python traincsv = open('./data/6_imitate_train_set/captcha_train.csv'  'r'  encoding = 'utf8') #: ËÆÄÂèñË®ìÁ∑¥ÈõÜÁöÑÊ®ôË®ò read_label = [toonehot(row[1]) for row in csv.reader(traincsv)] #: Â∞áÊØè‰∏ÄË°åÁöÑÊñáÂ≠óËΩâÊàêone-hot encoding train_label = [[] for _ in range(6)] #: ÂêÑÁµÑËº∏Âá∫ÁöÑÁ≠îÊ°àË¶ÅÊîæÂà∞train_label  for arr in read_label:     for index in range(6):  ¬† ¬† ¬† ¬†train_label[index].append(arr[index]) #: ÂéüÊú¨ÊòØ[[Á¨¨1Â≠óÁ≠îÊ°à  ...  Á¨¨6Â≠óÁ≠îÊ°à] ......  [Á¨¨1Â≠óÁ≠îÊ°à  ...  Á¨¨6Â≠óÁ≠îÊ°à]]  ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†#: Ë¶ÅËΩâÊàê[[Á¨¨1Â≠óÁ≠îÊ°à ...  Á¨¨1Â≠óÁ≠îÊ°à] ...  [Á¨¨6Â≠óÁ≠îÊ°à ...  Á¨¨6Â≠óÁ≠îÊ°à]]ÊâçÁ¨¶ÂêàYÁöÑËº∏ÂÖ• train_label = [arr for arr in np.asarray(train_label)] #: ÊúÄÂæåË¶ÅÊää6ÂÄãnumpy array ÊîæÂú®‰∏ÄÂÄãlist ```   ËÆìÊàëÂÄë‰æÜÊ®°‰ªøÁî¢Áîü‰∏Ä‰∫õÈ©óË≠âÁ¢ºÂêßÔºÅ È¶ñÂÖàÊàëÂÄëË¶ÅÂÖàËßÄÂØüÈ©óË≠âÁ¢ºÔºå‰Ω†ÂèØ‰ª•ÂØ´‰∏ÄÊîØÁà¨Ëü≤Á®ãÂºè(eg.```captcha_scrawl.py```)ÂéªÊì∑Âèñ‰∏ÄÂÖ©ÁôæÂºµÈ©óË≠âÁ¢ºÂõû‰æÜÁ¥∞Á¥∞ÊØîÂ∞ç„ÄÇÊàëÂÄë‰∏çÈõ£ÁôºÁèæÂè∞ÈêµÁöÑÈ©óË≠âÁ¢º‰∏çÂ§ñ‰πéÁî±ÂÖ©ÂÄã‰∏ªË¶ÅÂÖÉÁ¥†ÁµÑÊàêÔºö - ```5 ~ 6Á¢º```ÁöÑÊï∏Â≠óÂèäËã±Êñá(‰∏çÂåÖÂê´OÂíåI)ÔºåÂ§ßÂ∞è‰ºº‰πé‰∏ç‰∏ÄËá¥ÔºåËÄå‰∏îÈÉΩÊúâÁ∂ìÈÅéÊóãËΩâÔºåÂè¶Â§ñÈ°èËâ≤ÊòØÊµÆÂãïÁöÑ„ÄÇ - ËÉåÊôØÊòØÊµÆÂãïÁöÑÈ°èËâ≤ÔºåÂè¶Â§ñÈÇÑÊúâ‰∏çÂ∞ëÂπ≤ÊìæÁöÑÁ∑öÊ¢ùÔºåÁúãËµ∑‰æÜÊáâË©≤ÊòØÁü©ÂΩ¢ÔºåÁî±ÈªëÁ∑öÂíåÁôΩÁ∑öÁµÑÊàêÔºå‰∏îÊúâÈÉ®ÂàÜÊúÉËìãÂà∞Êï∏Â≠ó‰∏äÈù¢„ÄÇ  ÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÊúÉÁôºÁèæ: - Êï∏Â≠óÁöÑÊóãËΩâËßíÂ∫¶Á¥ÑÂú®```-55 ~ 55Â∫¶```ÈñìÔºåÂ§ßÂ∞èÁ¥Ñ```25 ~ 27pt```„ÄÇ - Â≠óÂûãÁöÑÈÉ®ÂàÜÔºå‰ªîÁ¥∞ËßÄÂØüÊúÉÁôºÁèæÂêå‰∏ÄÂÄãÂ≠óÊúÉÊúâÂÖ©Á®Æ‰∏ç‰∏ÄÊ®£ÁöÑÊ®£ÂºèÔºåÊé®Ê∏¨ÊòØÊúâÂÖ©Á®ÆÂ≠óÂûãÈö®Ê©üÊõ¥ÊõøÔºåÂÖ∂‰∏≠‰∏ÄÂÄãÂæàÊòéÈ°ØÊòØ```Courier New-Bold```ÔºåÂè¶‰∏ÄÂÄãÊØîÂ∞ç‰∏Ä‰∏ã‰πü‰∏çÈõ£ÁôºÁèæÂç≥ÊòØ```Times New Roman-Bold```„ÄÇ - ËÉåÊôØÂíåÂ≠óÂûãÈ°èËâ≤ÁöÑÈÉ®ÂàÜÔºåÂèØ‰ª•Áî®‰∏Ä‰∫õËâ≤ÂΩ©ÂùáÂÄºÂåñÁöÑÊâãÊ≥ïÂø´ÈÄüÁöÑÂæûÊï∏ÁôæÂºµÁöÑÈ©óË≠âÁ¢º‰∏≠ÂæóÂá∫ÊØè‰∏ÄÂºµÁöÑËÉåÊôØÂèäÊï∏Â≠óÁöÑÈ°èËâ≤ÔºåÈÄ≤ËÄåÊàëÂÄëÂ∞±ËÉΩÁÆóÂá∫È°èËâ≤ÁöÑÁØÑÂúç„ÄÇÈÄôÈÉ®ÂàÜÂèØ‰ª•Áî®OpenCVÁöÑk-means‰æÜÂØ¶‰ΩúÔºåÈÄôÈÇäÂ∞±‰∏çÂÜçË¥ÖËø∞„ÄÇ  ËÉåÊôØÁöÑR/G/BÁØÑÂúçÁ¥ÑÊòØÂú®```180 ~ 250```ÈñìÔºåÊñáÂ≠óÁöÑÈÉ®ÂàÜÂâáÊòØ```10 ~ 140```Èñì„ÄÇ - Âπ≤ÊìæÁöÑÁ∑öÊ¢ùÊòØÁü©ÂΩ¢ÔºåÊúâÂ∑¶„ÄÅ‰∏äÊòØÈªëÁ∑öÊ¢ù‰∏îÂè≥„ÄÅ‰∏ãÊòØÁôΩÁ∑öÊ¢ùÂíåÂÄíÈÅé‰æÜÔºåÂÖ±ÂÖ©Á®ÆÊ®£Âºè(‰πüÂèØ‰ª•Áï∂‰ΩúÊòØÊóãËΩâ180Â∫¶)ÔºåÂπ≥ÂùáÂ§ßÁ¥ÑÊúÉÂá∫Áèæ```30 ~ 32ÂÄã```Èö®Ê©üÂàÜÂ∏ÉÂú®Âúñ‰∏≠ÔºåÈï∑ÂØ¨ÈÉΩÂ§ßÁ¥ÑËêΩÂú®```5 ~ 21px```Èñì„ÄÇ Âè¶Â§ñÔºåÂ§ßÁ¥ÑÊúâ4ÊàêÁöÑÊ©üÊúÉÁôΩÁ∑öÊúÉËìãÂú®Êï∏Â≠ó‰∏äÔºåÈªëÁ∑öËìãÂú®ÊñáÂ≠ó‰∏äÁöÑÊ©üÁéáÂâáÊõ¥‰Ωé„ÄÇ  Êúâ‰∫ÜÈÄô‰∫õËßÄÂØüÔºåÂè™Â∑Æ‰∏ÄÈªûÈªûÂ∞±ÂèØ‰ª•Áî¢ÁîüË®ìÁ∑¥ÈõÜ‰∫ÜÔºåÊàëÂÄëÁèæÂú®‰æÜËßÄÂØüÊñáÂ≠óÈÉΩËêΩÂú®ÂúñÁâá‰∏äÁöÑÁîöÈ∫º‰ΩçÁΩÆ‰∏ä:  ![image](./readme_img/captcha_seperate1.png)![image](./readme_img/captcha_seperate2.png)![image](./readme_img/captcha_seperate3.png)  ÂæûÈÄôÂπæÂºµÂúñ‰∏≠‰∏çÈõ£ÁúãÂá∫ÊñáÂ≠ó‰∏¶ÈùûË¶èÂâáÂú∞ÂàÜÂ∏ÉÂú®ÂúñÁâá‰∏äÔºåÊàëÂÄëÂèØ‰ª•ÁåúÊ∏¨ÊñáÂ≠óÊòØÊóãËΩâÂæåË¢´Èö®Ê©üÂ∑¶ÁßªÊàñÂè≥Áßª‰∫ÜÔºåÁîöËá≥ÈÇÑÊúÉÊúâÈáçÁñäÁöÑÊÉÖÊ≥ÅÔºåÊâÄ‰ª•Ê≤íËæ¶Ê≥ïÁî®ÂàáÂâ≤ÁöÑÊñπÂºè‰∏ÄÊ¨°ËôïÁêÜ‰∏ÄÂÄãÊñáÂ≠ó„ÄÇ  ‰ª•‰∏äÂ∞±ÊòØÊàëÂÄëÁ∞°ÂñÆËßÄÂØüÂà∞ÁöÑÈ©óË≠âÁ¢ºË¶èÂâáÔºåË®ìÁ∑¥ÈõÜÁî¢ÁîüÁöÑÈÉ®ÂàÜÂØ¶‰ΩúÂú®```captcha_gen.py```‰∏≠ÔºåÈõñÁÑ∂ÂØ´ÂæóÊúâÈªûÈõú‰∫ÇÔºå‰∏çÈÅéÊ≤íÁîöÈ∫ºÁâπÂà•ÁöÑÂú∞ÊñπÔºåÂ∞±ÊòØÁÖßËëó‰∏äÈù¢ÁöÑË¶èÂâáÁî¢ÁîüÔºåÂèØ‰ª•Ë©¶Ëëó‰ª•Ëá™Â∑±ÁöÑÊñπÂºèÂØ¶‰ΩúÁúãÁúã„ÄÇ  ![image](./readme_img/captcha_sample4.jpg)  ```python if __name__ == ""__main__"":     generate(50000  ""./data/56_imitate_train_set/""   ENGP=100  FIVEP=50  ENGNOLIMIT=True  filename=""train"")     generate(10240  ""./data/56_imitate_vali_set/""   ENGP=100  FIVEP=50  ENGNOLIMIT=True  filename=""vali"")     generate(50000  ""./data/5_imitate_train_set/""   ENGP=100  FIVEP=100  ENGNOLIMIT=True  filename=""train"")     generate(10240  ""./data/5_imitate_vali_set/""   ENGP=100  FIVEP=100  ENGNOLIMIT=True  filename=""vali"")     generate(50000  ""./data/6_imitate_train_set/""   ENGP=100  FIVEP=0  ENGNOLIMIT=True  filename=""train"")     generate(10240  ""./data/6_imitate_vali_set/""   ENGP=100  FIVEP=0  ENGNOLIMIT=True  filename=""vali"") ```  ÊúÄÂæåÊúÉÁÇ∫ÊàëÂÄëÈ†êË®àÂª∫Á´ãÁöÑ‰∏âÂÄãCNN(2.2.1ÊúÉÊèêÂà∞)ÂêÑÂàÜÂà•Áî¢Áîü50000Á≠ÜTraining dataÂíå10240Á≠ÜValidate dataÔºåÁ≠îÊ°àÂâáÊ®ôË®òÂú®csvÊ™î‰∏≠„ÄÇ  ![image](./readme_img/csv.png)![image](./readme_img/generate.png)    Ë¶ÅÂª∫Á´ã‰∏ÄÂÄãËæ®Ë≠òÈ©óË≠âÁ¢ºÁöÑCNNÊ®°ÂûãÂÖ∂ÂØ¶‰∏¶ÈùûÈõ£‰∫ãÔºåÈõ£ÁöÑÊòØË¶ÅÂ¶Ç‰ΩïÂèñÂæóÊ®ôË®òÂ•ΩÁöÑË®ìÁ∑¥ÈõÜÂë¢?  ![image](./readme_img/captcha_sample1.jpg)![image](./readme_img/captcha_sample2.jpg)![image](./readme_img/captcha_sample3.jpg)  Âú®ÈÄôÈÇäÊàëÂÄëÊúÉÂòóË©¶ÂÖ©Á®ÆÊñπÊ≥ï(2.Ëàá3.)Ôºö  Click here or scroll down for english version   ÂàÜÈ°ûÈ©óË≠âÁ¢ºÊòØ5Á¢º or 6Á¢ºÁöÑÊ®°Âûã(train_cnn_imitate_56)ÂâáÈÅîÂà∞Á¥Ñ98.13%„ÄÇ   """;Computer Vision;https://github.com/tiger154/captcha-solver-custom
"""In this research project  to solve real world problems with machine learning  I noted that there is a limit to the traditional Deep Learning application  which is highly dependent on existing datasets because it is still difficult to obtain enough labled data.  The basis for judgment must be clear in the biomedical field  so I decided to use image data among various types for the reason of being visualized intuitively.  Using just one labeled image data for training  I wanted to categorize a lot of unseen data based on it by the basic concept of one shot learning through reinforcement learning.  In this project  I redefined the one shot image segmentation problem as a reinforcement learning and solved it using PPO. I found that there was actually a dramatic performance.  <p align=""center""> <img src=""oneshotgo/data/res/un.png"" width=70%/> </p>   ``` git clone https://github.com/decoderkurt/research_project_school_of_ai_2019.git cd research_project_school_of_ai_2019 pip install -e . ```  python -m baselines.run --alg=ppo2 --env=OneShotGo-v0 --save_path=""YourOwnOneShotGo10M""  python -m baselines.run --alg=ppo2 --env=OneShotGo-v0 --load_path=""OneShotGo10M""   """;Reinforcement Learning;https://github.com/decoderkurt/research_project_school_of_ai_2019
"""""";Natural Language Processing;https://github.com/graykode/nlp-tutorial
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;Computer Vision;https://github.com/aakashjhawar/traffic-sign-detection
"""Requirements (and how to install dependecies)   How to compile on Linux  How to compile on Windows   How to train with multi-GPU:   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   yolov3-openimages.cfg (247 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-openimages.weights  yolov3-spp.cfg (240 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3-spp.weights  yolov3.cfg (236 MB COCO Yolo v3) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov3.weights  yolov3-tiny.cfg (34 MB COCO Yolo v3 tiny) - requires 1 GB GPU-RAM:  https://pjreddie.com/media/files/yolov3-tiny.weights   * `yolov2.cfg` (194 MB COCO Yolo v2) - requires 4 GB GPU-RAM: https://pjreddie.com/media/files/yolov2.weights   * `yolov2-tiny.cfg` (43 MB COCO Yolo v2) - requires 1 GB GPU-RAM: https://pjreddie.com/media/files/yolov2-tiny.weights   You can get cfg-files by path: darknet/cfg/   MS COCO: use ./scripts/get_coco_dataset.sh to get labeled MS COCO detection dataset   ILSVRC2012 (ImageNet classification): use ./scripts/get_imagenet_train.sh (also imagenet_label.sh for labeling valid set)   Start Smart WebCam on your phone   Just do make in the darknet directory.   GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   ZED_CAMERA=1 to build a library with ZED-3D-camera support (should be ZED SDK installed)  then run   If you have already installed Visual Studio 2015/2017/2019  CUDA > 10.0  cuDNN > 7.0  OpenCV > 2.4  then compile Darknet by using C:\Program Files\CMake\bin\cmake-gui.exe as on this IMAGE: Configure -> Optional platform for generator (Set: x64) -> Finish -> Generate -> Open Project -> x64 & Release -> Build -> Build solution   Install or update Visual Studio to at least version 2017  making sure to have it fully patched (run again the installer if not sure to automatically update to latest version). If you need to install from scratch  download VS from here: Visual Studio Community  Install CUDA and cuDNN  Install git and cmake. Make sure they are on the Path at least for the current account  Install vcpkg and try to install a test library to make sure everything is working  for example vcpkg install opengl  Define an environment variables  VCPKG_ROOT  pointing to the install path of vcpkg  Define another environment variable  with name VCPKG_DEFAULT_TRIPLET and value x64-windows   PS \&gt;                  cd $env:VCPKG_ROOT  PS Code\vcpkg&gt;         .\vcpkg install pthreads opencv[ffmpeg] #:replace with opencv[cuda ffmpeg] in case you want to use cuda-accelerated openCV  Open Powershell  go to the darknet folder and build with the command .\build.ps1. If you want to use Visual Studio  you will find two custom solutions created for you by CMake after the build  one in build_win_debug and the other in build_win_release  containing all the appropriate config flags for your system.  If you have CUDA 10.0  cuDNN 7.4 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. Also add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN v7.4.1 for CUDA 10.0: https://developer.nvidia.com/rdp/cudnn-archive  add Windows system variable CUDNN with path to CUDNN: https://user-images.githubusercontent.com/4096485/53249764-019ef880-36ca-11e9-8ffe-d9cf47e7e462.jpg   If you don't have GPU  but have OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu  If you have OpenCV 2.4.13 instead of 3.0 then you should change paths after \darknet.sln is opened   Note: CUDA must be installed only after Visual Studio has been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Yolo v3](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=MPU2HistivI ""Yolo v3"")  Others: https://www.youtube.com/user/pjreddie/videos   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  On Linux find executable file `./darknet` in the root directory  while on Windows find it in the directory `\build\darknet\x64`   * Yolo v3 COCO - **image**: `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25` * **Output coordinates** of objects: `darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg` * Yolo v3 COCO - **video**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4` * Yolo v3 COCO - **WebCam 0**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0` * Yolo v3 COCO for **net-videocam** - Smart WebCam: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg` * Yolo v3 - **save result videofile res.avi**: `darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi` * Yolo v3 **Tiny** COCO - video: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4` * **JSON and MJPEG server** that allows multiple connections from your soft or Web-browser `ip-address:8070` and 8090: `./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output` * Yolo v3 Tiny **on GPU #1**: `darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Train on **Amazon EC2**  to see mAP & Loss-chart using URL like: `http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090` in the Chrome/Firefox (**Darknet should be compiled with OpenCV**):      `./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.json` file use:      `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json < data/train.txt` * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output < data/train.txt > result.txt` * Pseudo-lableing - to process a list of images `data/new_train.txt` and save results of detection in Yolo training format for each image as label `<image_name>.txt` (in this way you can increase the amount of training data) use:     `darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels < data/new_train.txt` * To calculate anchors: `darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416` * To check accuracy mAP@IoU=50: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights` * To check accuracy mAP@IoU=75: `darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75`   * on Linux - set `LIBSO=1` in the `Makefile` and do `make` * on Windows - compile `build\darknet\yolo_cpp_dll.sln` or `build\darknet\yolo_cpp_dll_no_gpu.sln` solution  There are 2 APIs: * C API: https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h     * Python examples using the C API::               * https://github.com/AlexeyAB/darknet/blob/master/darknet.py	          * https://github.com/AlexeyAB/darknet/blob/master/darknet_video.py      * C++ API: https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp     * C++ example that uses C++ API: https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp      ----  1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open the solution `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 10.0**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open the solution `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` struct bbox_t {     unsigned int x  y  w  h;    // (x y) - top-left corner  (w  h) - width & height of bounded box     float prob;                    // confidence - probability that the object was found correctly     unsigned int obj_id;        // class of object - from range [0  classes-1]     unsigned int track_id;        // tracking id for video (0 - untracked  1 - inf - tracked object)     unsigned int frames_counter;// counter of frames on which the object was detected };  class Detector { public:         Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0);         ~Detector();          std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false);         std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false);         static image_t load_image(std::string image_filename);         static void free_image(image_t m);  #:ifdef OPENCV         std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); 	std::shared_ptr<image_t> mat_to_image_resize(cv::Mat mat) const; #:endif }; ```  """;General;https://github.com/aakashjhawar/traffic-sign-detection
""" The following example can be found in `test/example.py`. It demonstrates a few different variations of how to use the pretrained MTLSTM class that generates contextualized word vectors (CoVe) programmatically.   """;Natural Language Processing;https://github.com/salesforce/cove
"""See more details  including system dependencies  python requirements and setups in [install.md](./docs/install.md). Please follows the instructions in [install.md](./docs/install.md) to install this firstly.  **Notice that `imags_size=512` need at least 9.8GB GPU memory.** if you are using a middle-level GPU(e.g. RTX 2060)  you should change the `image_size` to 384 or 256. The following table can be used as a reference:  | image_size | preprocess | personalize | run_imitator | recommended gpu                    | | ---------- | ---------- | ----------- | ------------ | ---------------------------------- | | 256x256    | 3.1 GB     | 4.3 GB      | 1.1 GB       | RTX 2060 / RTX 2070                | | 384x384    | 3.1 GB     | 7.9 GB      | 1.5 GB       | GTX 1080Ti / RTX 2080Ti / Titan Xp | | 512x512    | 3.1 GB     | 9.8 GB      | 2 GB         | GTX 1080Ti / RTX 2080Ti / Titan Xp | | 1024x1024  | 3.1 GB     | 20 GB       | -            | RTX Titan / P40 / V100 32G         |    [x] 12/20/2020  A precompiled version on Windows has been released! [Usage]   """;Computer Vision;https://github.com/justinjohn0306/iPERCore-Windows10
"""To avoid any conflict with your existing Python setup  and to keep this project self-contained  it is suggested to work in a virtual environment with [`virtualenv`](http://docs.python-guide.org/en/latest/dev/virtualenvs/). To install `virtualenv`: ``` pip install --upgrade virtualenv ``` Create a virtual environment  activate it and install the requirements in [`requirements.txt`](requirements.txt). ``` virtualenv venv source venv/bin/activate pip install -r requirements.txt ```   You can use the [`main.py`](main.py) script in order to run reinforcement learning experiments with MAML. This script was tested with Python 3.5. Note that some environments may also work with Python 2.7 (all experiments besides MuJoCo-based environments). ``` python main.py --env-name HalfCheetahDir-v1 --num-workers 8 --fast-lr 0.1 --max-kl 0.01 --fast-batch-size 20 --meta-batch-size 40 --num-layers 2 --hidden-size 100 --num-batches 1000 --gamma 0.99 --tau 1.0 --cg-damping 1e-5 --ls-max-steps 15 --output-folder maml-halfcheetah-dir --device cuda ```   """;General;https://github.com/shunzh/pytorch-maml-rl
"""Download the GAIA dataset with the notebook 'Create_CELEBA-HQ.ipynb'   """;Computer Vision;https://github.com/timsainb/GAIA
"""There are numerous other things that I have learned and failed through so far. I went into this semester with barely any knowledge of anything in this realm  and I really feel like I have learned a lot up to now  and am excited to continue to learn more and more.  The first few weeks of my semester were spent trying to learn and understand the mathematics and qualities that modern day style transfer is based off of. I read a few of the dominant papers in the field that discussed the methods.  To summarize  when creating a stylized image  there are three main players. The content photo (c)  the style photo (s)  and the new pistache (p).  At a high level  we want to make (p) as similar in content to (c)  and as similar in style to (s). So how can we define  what ‚Äòcontent‚Äô and ‚Äòstyle‚Äô are? [4] found that Convolutional Neural Networks (CNN‚Äôs)  when trained for object recognition  extract different types of information at different layers. If you then try to minimize the feature reconstruction for particular layers  you can extract different information. Importantly  minimizing early layers in the CNN seems to capture the texture and color images  whereas  minimizing higher layers in the CNN seem to capture image content and overall spatial structure [5].  There is a lot of math that is involved in reconstructing those layers  and minimizing the differences between images  that I won‚Äôt get into here. After the initial paper was published  an important addition was made that enabled the creation of the Android application that I ended up editing. [5] demonstrated a way of not only creating stylized images  but also creating them in real time. To create the pistache in [4]  there was both a forward and backward pass through a pretrained network. To fix the problem of speed  [5] trained a different neural network to quickly approximate solutions to their problem.  Finally  [6] resolved an issue that allowed you to use the same network for N distinct styles instead of 1  thus saving a ton of space. This allowed the Google Codelab to contain so many distinct styles  as well as allow some of the more well known style transfer apps  like Prisma to work.    A pistache is an artistic work that imitates the style of another one.  This git repo is the culumination of half a semester of work for my independent study. To date  I have edited android app  by implemented image-segmented background blurring  and increased luminance matching to a Google Codelab [1]  that focuses on creating pistaches.  After getting to this point  and remembering that this independent study was supposed to be about machine learning and art  and I hadn‚Äôt made any art projects  I began to think about what I wanted to make with this unique application.  I quickly realized that my moms birthday was coming up  and that I have historically never gotten her anything. For those who don‚Äôt know my mom  I personally believe that she has a manic addiction to her children. That belief is entirely centered around the life she wishes to portray through her Facebook.  Facebook photos tell your story  either intentionally or unintentionally  to everyone that kinda knows you.  Either way  in deciding how to celebrate the birth of my mom  I came to the conclusion that I wanted to tell her story. The one she tells through Facebook that is. It isn‚Äôt necessarily the story I would have chosen to portray  but it‚Äôs the one she has.  So  I had the pleasure of scouring Janice‚Äôs photos to build my collage  and I grew increasingly fearful for my privacy in the process. Regardless  I grabbed about 60 photos from her Facebook (and some others I wanted to throw in)  and created 26 different pistaches with styles ranging from Picasso to Van Gogh  resulting in over 1500 total photos.  Then  I went through the 26 pistaches for each of the 60 images  and chose the one that I thought most truly represented that memory to me. The photo that I think best captures that frozen moment. Thus  trying to portray how I view the world that she is portraying. Seeing her world through my eyes.  Also  for fun  I took all 1500 photos and put it in a super-collage  I call the meg. That photo is 170MB. That image is fucking awesome! But I had to scale down the image so I can have it on github. So the resolution is wack. But I have the real version and its literally the best thing ever.  <p align=""center"">  <img src=""scaled_down_mega_collage.jpg?""> </p>   """;General;https://github.com/chrismgeorge/Artistic_Additions_To_Style_Transfer
"""**Deformable ConvNets** is initially described in an [ICCV 2017 oral paper](https://arxiv.org/abs/1703.06211). (Slides at [ICCV 2017 Oral](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf))  **R-FCN** is initially described in a [NIPS 2016 paper](https://arxiv.org/abs/1605.06409).   <img src='demo/deformable_conv_demo1.png' width='800'> <img src='demo/deformable_conv_demo2.png' width='800'> <img src='demo/deformable_psroipooling_demo.png' width='800'>   For R-FCN/Faster R-CNN\: 1. Please download COCO and VOC 2007+2012 datasets  and make sure it looks like this:  	``` 	./data/coco/ 	./data/VOCdevkit/VOC2007/ 	./data/VOCdevkit/VOC2012/ 	```  2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  For DeepLab\: 1. Please download Cityscapes and VOC 2012 datasets and make sure it looks like this:  	``` 	./data/cityscapes/ 	./data/VOCdevkit/VOC2012/ 	``` 2. Please download argumented VOC 2012 annotations/image lists  and put the argumented annotations and the argumented train/val lists into:  	``` 	./data/VOCdevkit/VOC2012/SegmentationClass/ 	./data/VOCdevkit/VOC2012/ImageSets/Main/ 	```      Respectively.     2. Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMEtxf1Ciym8uZ8sg)  and put it under folder `./model`. Make sure it looks like this: 	``` 	./model/pretrained_model/resnet_v1_101-0000.params 	```  1. Clone the Deformable ConvNets repository  and we'll call the directory that you cloned Deformable-ConvNets as ${DCN_ROOT}. ``` git clone https://github.com/msracver/Deformable-ConvNets.git ```  2. For Windows users  run ``cmd .\init.bat``. For Linux user  run `sh ./init.sh`. The scripts will build cython module automatically and create some folders.  3. Install MXNet: 	 	**Note: The MXNet's Custom Op cannot execute parallelly using multi-gpus after this [PR](https://github.com/apache/incubator-mxnet/pull/6928). We strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.5).**  	***Quick start***  	3.1 Install MXNet and all dependencies by  	``` 	pip install -r requirements.txt 	``` 	If there is no other error message  MXNet should be installed successfully.  	 	***Build from source (alternative way)***  	3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by 	``` 	git clone --recursive https://github.com/dmlc/mxnet.git 	git checkout 998378a 	git submodule update 	#: if it's the first time to checkout  just use: git submodule update --init --recursive 	``` 	3.3 Compile MXNet 	``` 	cd ${MXNET_ROOT} 	make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1 	``` 	3.4 Install the MXNet Python binding by 	 	***Note: If you will actively switch between different versions of MXNet  please follow 3.5 instead of 3.4*** 	``` 	cd python 	sudo python setup.py install 	``` 	3.5 For advanced users  you may put your Python packge into `./external/mxnet/$(YOUR_MXNET_PACKAGE)`  and modify `MXNET_VERSION` in `./experiments/rfcn/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`. Thus you can switch among different versions of MXNet quickly.  4. For Deeplab  we use the argumented VOC 2012 dataset. The argumented annotations are provided by [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html) dataset. For convenience  we provide the converted PNG annotations and the lists of train/val images  please download them from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMRhVImMI1jRrsxDg).   We provide trained deformable convnet models  including the deformable R-FCN & Faster R-CNN models trained on COCO trainval  and the deformable DeepLab model trained on CityScapes train.  1. To use the demo with our pre-trained deformable models  please download manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMSjehIcCgAhvEAHw) or [BaiduYun](https://pan.baidu.com/s/1dFlPFED)  and put it under folder `model/`.  	Make sure it looks like this: 	``` 	./model/rfcn_dcn_coco-0000.params 	./model/rfcn_coco-0000.params 	./model/fpn_dcn_coco-0000.params 	./model/fpn_coco-0000.params 	./model/rcnn_dcn_coco-0000.params 	./model/rcnn_coco-0000.params 	./model/deeplab_dcn_cityscapes-0000.params 	./model/deeplab_cityscapes-0000.params 	./model/deform_conv-0000.params 	./model/deform_psroi-0000.params 	``` 2. To run the R-FCN demo  run 	``` 	python ./rfcn/demo.py 	``` 	By default it will run Deformable R-FCN and gives several prediction results  to run R-FCN  use 	``` 	python ./rfcn/demo.py --rfcn_only 	``` 3. To run the DeepLab demo  run 	``` 	python ./deeplab/demo.py 	``` 	By default it will run Deformable Deeplab and gives several prediction results  to run DeepLab  use 	``` 	python ./deeplab/demo.py --deeplab_only 	``` 4. To visualize the offset of deformable convolution and deformable psroipooling  run 	``` 	python ./rfcn/deform_conv_demo.py 	python ./rfcn/deform_psroi_demo.py 	```    1. All of our experiment settings (GPU #  dataset  etc.) are kept in yaml config files at folder `./experiments/rfcn/cfgs`  `./experiments/faster_rcnn/cfgs` and `./experiments/deeplab/cfgs/`. 2. Eight config files have been provided so far  namely  R-FCN for COCO/VOC  Deformable R-FCN for COCO/VOC  Faster R-CNN(2fc) for COCO/VOC  Deformable Faster R-CNN(2fc) for COCO/VOC  Deeplab for Cityscapes/VOC and Deformable Deeplab for Cityscapes/VOC  respectively. We use 8 and 4 GPUs to train models on COCO and on VOC for R-FCN  respectively. For deeplab  we use 4 GPUs for all experiments.  3. To perform experiments  run the python scripts with the corresponding config file as input. For example  to train and test deformable convnets on COCO with ResNet-v1-101  use the following command     ```     python experiments\rfcn\rfcn_end2end_train_test.py --cfg experiments\rfcn\cfgs\resnet_v1_101_coco_trainval_rfcn_dcn_end2end_ohem.yaml     ```     A cache folder would be created automatically to save the model and the log under `output/rfcn_dcn_coco/`. 4. Please find more details in config files and in our code.   """;Computer Vision;https://github.com/zengzhaoyang/trident
"""Keras 2.2.0  Tensorflow 1.8.0  Ubuntu 16.04  Python 3.5   """;Computer Vision;https://github.com/VinGPan/paper_implementations
"""cd project directory  Run python setup.py build_ext --inplace   """;Computer Vision;https://github.com/jahongir7174/EfficientDet-tf
"""InfoGAN is an information-theoretic extension to the simple Generative Adversarial Networks that is able to learn disentangled representations in a completely unsupervised manner. What this means is that InfoGAN successfully disentangle wrirting styles from digit shapes on th MNIST dataset and discover visual concepts such as hair styles and gender on the CelebA dataset. To achieve this an information-theoretic regularization is added to the loss function that enforces the maximization of mutual information between latent codes  c  and the generator distribution G(z  c).   cuda 9.0  Python 3.6.5  PyTorch 1.0.0   matplotlib 2.2.2   Edit the **`config.py`** file to select training parameters and the dataset to use. Choose *`dataset`* from **['MNIST'  'FashionMNIST'  'SVHN'  'CelebA']**  To train the model run **`train.py`**: ```sh python3 train.py ``` After training the network to experiment with the latent code for the `MNIST` dataset run **`mnist_generate.py`**: ```sh python3 mnist_generate.py --load_path /path/to/pth/checkpoint ```   """;Computer Vision;https://github.com/Natsu6767/InfoGAN-PyTorch
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-91-download-archive  OpenCV 3.3.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.3.0/opencv-3.3.0-vc14.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     **use this command**: `yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4`          * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;General;https://github.com/wpsliu123/yolo-windows
"""""";Computer Vision;https://github.com/li012589/NeuralRG
"""- Download the **compatible** few-shot data at here: [download](https://atmahou.github.io/attachments/new_FewShotNLU_data(ACL20).zip)  - Set test  train  dev data file path in `./scripts/run_1_shot_slot_tagging.sh` to your setting.    > For simplicity  your only need to set the root path for data as follow: ```bash base_data_dir=/your_dir/ACL2020data/ ```   - Download the pytorch bert model  or convert tensorflow param by yourself with [scripts](https://github.com/huggingface/transformers/blob/master/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py). - Set BERT path in the `./scripts/run_1_shot_slot_tagging.sh` to your setting: ```bash bert_base_uncased=/your_dir/uncased_L-12_H-768_A-12/ bert_base_uncased_vocab=/your_dir/uncased_L-12_H-768_A-12/vocab.txt ```   Here  we take the few-shot slot tagging and NER task from [(Hou et al.  2020)](https://arxiv.org/abs/2006.05702) as quick start examples.   ```bash source ./scripts/run_1_shot_slot_tagging.sh 0 snips ```     ```bash source ./scripts/run_1_shot_slot_tagging.sh 0 ner ```  > To run 5-shots experiments  use `./scripts/run_5_shot_slot_tagging.sh`   ```json {   ""domain_name"": [     {  // episode       ""support"": {  // support set         ""seq_ins"": [[""we""  ""are""  ""friends""  "".""]  [""how""  ""are""  ""you""  ""?""]]   // input sequence         ""seq_outs"": [[""O""  ""O""  ""O""  ""O""]  [""O""  ""O""  ""O""  ""O""]]   // output sequence in sequence labeling task         ""labels"": [[""statement""]  [""query""]]  // output labels in classification task       }        ""query"": {  // query set         ""seq_ins"": [[""we""  ""are""  ""friends""  "".""]  [""how""  ""are""  ""you""  ""?""]]          ""seq_outs"": [[""O""  ""O""  ""O""  ""O""]  [""O""  ""O""  ""O""  ""O""]]          ""labels"": [[""statement""]  [""query""]]       }     }      ...   ]    ... }  ```     """;Natural Language Processing;https://github.com/AtmaHou/MetaDialog
"""Please follow the instructions of py-faster-rcnn [here](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to setup VOC and COCO datasets (Part of COCO is done). The steps involve downloading data and optionally creating soft links in the ``data`` folder. Since faster RCNN does not rely on pre-computed proposals  it is safe to ignore the steps that setup proposals.  If you find it useful  the ``data/cache`` folder created on my side is also shared [here](http://ladoga.graphics.cs.cmu.edu/xinleic/tf-faster-rcnn/cache.tgz).   1. Clone the repository   ```Shell   git clone https://github.com/endernewton/tf-faster-rcnn.git   ```  2. Update your -arch in setup script to match your GPU   ```Shell   cd tf-faster-rcnn/lib   #: Change the GPU architecture (-arch) if necessary   vim setup.py   ```    | GPU model  | Architecture |   | ------------- | ------------- |   | TitanX (Maxwell/Pascal) | sm_52 |   | GTX 960M | sm_50 |   | GTX 1080 (Ti) | sm_61 |   | Grid K520 (AWS g2.2xlarge) | sm_30 |   | Tesla K80 (AWS p2.xlarge) | sm_37 |    **Note**: You are welcome to contribute the settings on your end if you have made the code work properly on other GPUs. Also even if you are only using CPU tensorflow  GPU based code (for NMS) will be used by default  so please set **USE_GPU_NMS False** to get the correct output.   3. Build the Cython modules   ```Shell   make clean   make   cd ..   ```  4. Install the [Python COCO API](https://github.com/pdollar/coco). The code requires the API to access COCO dataset.   ```Shell   cd data   git clone https://github.com/pdollar/coco.git   cd coco/PythonAPI   make   cd ../../..   ```     - Due to the randomness in GPU training with Tensorflow especially for VOC  the best numbers are reported (with 2-3 attempts) here. According to my experience  for COCO you can almost always get a very close number (within ~0.2%) despite the randomness.      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz     tar -xzvf vgg_16_2016_08_28.tar.gz      cd ../..     For Resnet101  you can set up like:      cd data/imagenet_weights     wget -v http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz     tar -xzvf resnet_v1_101_2016_08_28.tar.gz      cd ../..     ./experiments/scripts/train_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/train_faster_rcnn.sh 1 coco res101     ./experiments/scripts/test_faster_rcnn.sh [GPU_ID] [DATASET] [NET]    #: GPU_ID is the GPU you want to test on     ./experiments/scripts/test_faster_rcnn.sh 1 coco res101   1. Download pre-trained model   ```Shell   #: Resnet101 for voc pre-trained on 07+12 set   ./data/scripts/fetch_faster_rcnn_models.sh   ```   **Note**: if you cannot download the models through the link  or you want to try more models  you can check out the following solutions and optionally update the downloading script:   - Another server [here](http://xinlei.sp.cs.cmu.edu/xinleic/tf-faster-rcnn/).   - Google drive [here](https://drive.google.com/open?id=0B1_fAEgxdnvJSmF3YUlZcHFqWTQ).  2. Create a folder and a soft link to use the pre-trained model   ```Shell   NET=res101   TRAIN_IMDB=voc_2007_trainval+voc_2012_trainval   mkdir -p output/${NET}/${TRAIN_IMDB}   cd output/${NET}/${TRAIN_IMDB}   ln -s ../../../data/voc_2007_trainval+voc_2012_trainval ./default   cd ../../..   ```  3. Demo for testing on custom images   ```Shell   #: at repository root   GPU_ID=0   CUDA_VISIBLE_DEVICES=${GPU_ID} ./tools/demo.py   ```   **Note**: Resnet101 testing probably requires several gigabytes of memory  so if you encounter memory capacity issues  please install it with CPU support only. Refer to [Issue 25](https://github.com/endernewton/tf-faster-rcnn/issues/25).  4. Test with pre-trained Resnet101 models   ```Shell   GPU_ID=0   ./experiments/scripts/test_faster_rcnn.sh $GPU_ID pascal_voc_0712 res101   ```   **Note**: If you cannot get the reported numbers (79.8 on my side)  then probably the NMS function is compiled improperly  refer to [Issue 5](https://github.com/endernewton/tf-faster-rcnn/issues/5).   """;Computer Vision;https://github.com/zxqcreations/faster-rcnn-tf
"""Add named folder dataA  dataB  valA and valB. dataA and dataB are used for training. valA and valB are used for test.  like this ``` main.py pred.py dataA   ‚îú 000.jpg   ‚îú aaa.png   ...   ‚îî zzz.jpg dataB   ‚îú 111.jpg   ‚îú bbb.png   ...   ‚îî xxx.jpg valA   ‚îú 222.jpg   ‚îú ccc.png   ...   ‚îî yyy.jpg  valB   ‚îú 333.jpg   ‚îú ddd.png   ...   ‚îî www.jpg  ```  To train  ``` python main.py ```  To Validate  ``` python pred.py valA valB ```   upper left: inputA  upper right: converted AtoB  under left: inputB  under right; converted BtoA   """;Computer Vision;https://github.com/itsuki8914/CycleGAN-TF
"""Add named folder dataA  dataB  valA and valB. dataA and dataB are used for training. valA and valB are used for test.  like this ``` main.py pred.py dataA   ‚îú 000.jpg   ‚îú aaa.png   ...   ‚îî zzz.jpg dataB   ‚îú 111.jpg   ‚îú bbb.png   ...   ‚îî xxx.jpg valA   ‚îú 222.jpg   ‚îú ccc.png   ...   ‚îî yyy.jpg  valB   ‚îú 333.jpg   ‚îú ddd.png   ...   ‚îî www.jpg  ```  To train  ``` python main.py ```  To Validate  ``` python pred.py valA valB ```   upper left: inputA  upper right: converted AtoB  under left: inputB  under right; converted BtoA   """;General;https://github.com/itsuki8914/CycleGAN-TF
"""First we need a tool to encode a surface weather field   (2m air temperature anomaly  mean-sea-level-pressure  and precipitation rate)   as an image. [Script](./weather2image//make.3var.plot.R)  Then we need a set of pairs of such images - a source image  and a target image from 6-hours later. Each pair should be separated by at least 5 days  so they are independent states. [Script](./weather2image//make.training.batch.R)  Then we need to take a training set (400) of those pairs of images and pack them into the 512x256 side-by-side format used by pix2pix (source in the left half  and target in the right half). [Script](./weather2image/make_p2p_training_images.R)  Alternatively  you can get the set of training and test images I used from [Dropbox](https://www.dropbox.com/s/0knxkll2btjjnyl/weather2weather_imgs.tar).  Then train a model on this set for 200 epochs - with a fast GPU this should take about 1 hour  but  CPU-only  it takes a bit over 24 hours on my 4-core iMac. (It took about 2 hours on one gpu-node of [Isambard](http://gw4.ac.uk/isambard/)).  ```sh python weather2weather.py \   --mode train \   --output_dir $SCRATCH/weather2weather/model_train \   --max_epochs 200 \   --input_dir $SCRATCH/weather2weather/p2p_format_images_for_training \   --which_direction AtoB ``` Now make some more pairs of images (100) to test the model on - same format as the training set  but must be different weather states (times). [Script](./weather2image/make_p2p_validation_images.R)  Use the trained model to make predictions from the validation set sources and compare those predictions to the validation set targets.  ```sh python weather2weather.py \   --mode test \   --output_dir $SCRATCH/weather2weather/model_test \   --input_dir $SCRATCH/weather2weather/p2p_format_images_for_validation \   --checkpoint $SCRATCH/weather2weather/model_train ```  The test run will output an HTML file at `$SCRATCH/weather2weather/model_test/index.html` that shows input/output/target image sets. This is good for a first glance  but those images are in a packed analysis form. So we need a tool to convert the packed image pairs to a clearer image format: [Script](./weather2image/replot.p2p.image.R). This shows target weather (top left)  model output weather (top right)  target pressure increment (bottom left)  and model output pressure increment (bottom right).  To postprocess all the test cases run: ```sh ./weather2image/replot_all_validation.R \   --input.dir=$SCRATCH/weather2weather/model_test/images \   --output.dir=$SCRATCH/weather2weather/model_test/postprocessed ```  This will produce an HTML file at  `$SCRATCH/weather2weather/model_test/index.html` showing results of all the test cases.  This clearly does have skill at 6-hour weather forecasts - it gets the semi-diurnal oscillation  and some of the extratropical structure. The final step is to use the model on it's own output - by making repeated 6-hour forecasts we can make a forecast as far into the future as we like. [This is less successful](https://vimeo.com/275778137).   """;Computer Vision;https://github.com/philip-brohan/weather2weather
"""First we need a tool to encode a surface weather field   (2m air temperature anomaly  mean-sea-level-pressure  and precipitation rate)   as an image. [Script](./weather2image//make.3var.plot.R)  Then we need a set of pairs of such images - a source image  and a target image from 6-hours later. Each pair should be separated by at least 5 days  so they are independent states. [Script](./weather2image//make.training.batch.R)  Then we need to take a training set (400) of those pairs of images and pack them into the 512x256 side-by-side format used by pix2pix (source in the left half  and target in the right half). [Script](./weather2image/make_p2p_training_images.R)  Alternatively  you can get the set of training and test images I used from [Dropbox](https://www.dropbox.com/s/0knxkll2btjjnyl/weather2weather_imgs.tar).  Then train a model on this set for 200 epochs - with a fast GPU this should take about 1 hour  but  CPU-only  it takes a bit over 24 hours on my 4-core iMac. (It took about 2 hours on one gpu-node of [Isambard](http://gw4.ac.uk/isambard/)).  ```sh python weather2weather.py \   --mode train \   --output_dir $SCRATCH/weather2weather/model_train \   --max_epochs 200 \   --input_dir $SCRATCH/weather2weather/p2p_format_images_for_training \   --which_direction AtoB ``` Now make some more pairs of images (100) to test the model on - same format as the training set  but must be different weather states (times). [Script](./weather2image/make_p2p_validation_images.R)  Use the trained model to make predictions from the validation set sources and compare those predictions to the validation set targets.  ```sh python weather2weather.py \   --mode test \   --output_dir $SCRATCH/weather2weather/model_test \   --input_dir $SCRATCH/weather2weather/p2p_format_images_for_validation \   --checkpoint $SCRATCH/weather2weather/model_train ```  The test run will output an HTML file at `$SCRATCH/weather2weather/model_test/index.html` that shows input/output/target image sets. This is good for a first glance  but those images are in a packed analysis form. So we need a tool to convert the packed image pairs to a clearer image format: [Script](./weather2image/replot.p2p.image.R). This shows target weather (top left)  model output weather (top right)  target pressure increment (bottom left)  and model output pressure increment (bottom right).  To postprocess all the test cases run: ```sh ./weather2image/replot_all_validation.R \   --input.dir=$SCRATCH/weather2weather/model_test/images \   --output.dir=$SCRATCH/weather2weather/model_test/postprocessed ```  This will produce an HTML file at  `$SCRATCH/weather2weather/model_test/index.html` showing results of all the test cases.  This clearly does have skill at 6-hour weather forecasts - it gets the semi-diurnal oscillation  and some of the extratropical structure. The final step is to use the model on it's own output - by making repeated 6-hour forecasts we can make a forecast as far into the future as we like. [This is less successful](https://vimeo.com/275778137).   """;General;https://github.com/philip-brohan/weather2weather
"""you can save your adjacent matrix using the code below   noted: The nodeID start from 0.<br>   ``` $ python main.py -c config/xx.ini ``` >noted: your can just checkout and modify config file or main.py to get what you want.  """;Graphs;https://github.com/suanrong/SDNE
"""* PyTorch 1.2 + TorchText   * 2.1 Build Transformer-XL + MultiAttention heads   * Requirements: Python 3 + Pytorch v1.2   * TODO: Add GPU support  * Build Bert - Bidirectional Transformer   * Build Bert - Bidirectional Transformer   * pip install transformers tb-nightly   """;Natural Language Processing;https://github.com/fanchenyou/transformer-study
"""This is a GAN demo for creating anime character faces from random noise.   """;Computer Vision;https://github.com/RikoLi/gan-acgface
"""<img src=""./instruction_images/Windows/Step_1_DD_Win.png"" width=""480px""></img>  Presuming Python is installed:  - Open command prompt and navigate to the directory of your current version of Python ```bash   pip install deep-daze ```   ```bash $ pip install deep-daze ```     This will require that you have an Nvidia GPU or AMD GPU   - Minimum Requirements: 4GB VRAM (Using VERY LOW settings  see usage instructions below)           (required) A phrase less than 77 tokens which you would like to visualize.   We get:   ```bash $ imagine ""a house in the forest"" ``` For Windows:  <img src=""./instruction_images/Windows/Step_2_DD_Win.png"" width=""480px""></img>  - Open command prompt as administrator ```bash   imagine ""a house in the forest"" ```  That's it.   If you have enough memory  you can get better quality by adding a `--deeper` flag  ```bash $ imagine ""shattered plates on the ground"" --deeper ```   If you have at least 16 GiB of vram available  you should be able to run these settings with some wiggle room. ```python imagine = Imagine(     text=text      num_layers=42      batch_size=64      gradient_accumulate_every=1  ) ```   ```python imagine = Imagine(     text=text      num_layers=24      batch_size=16      gradient_accumulate_every=2 ) ```   If you are desperate to run this on a card with less than 8 GiB vram  you can lower the image_width. ```python imagine = Imagine(     text=text      image_width=256      num_layers=16      batch_size=1      gradient_accumulate_every=16 #: Increase gradient_accumulate_every to correct for loss in low batch sizes ) ```   This is just a teaser. We will be able to generate images  sound  anything at will  with natural language. The holodeck is about to become real in our lifetimes.  Please join replication efforts for DALL-E for <a href=""https://github.com/lucidrains/dalle-pytorch"">Pytorch</a> or <a href=""https://github.com/EleutherAI/DALLE-mtf"">Mesh Tensorflow</a> if you are interested in furthering this technology.   """;General;https://github.com/lucidrains/deep-daze
""" These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. For our model to work all of the dependencies are on the requirements.txt. Run the following command line on your local repository. ``` pip install -r requirements.txt ```   """;Computer Vision;https://github.com/upc-postgrads/BrainScan
"""'--gpu'  '-g'  type=int  default=0      - to be updated   Segment the objects in the input image by executing:    ``` python demo.py --image <input image> --modelfile result/snapshot_model.npz --contour ```   """;Computer Vision;https://github.com/DeNA/Chainer_Mask_R-CNN
"""""";General;https://github.com/sanju-27/Video-Super-Resolution
"""""";Computer Vision;https://github.com/sanju-27/Video-Super-Resolution
"""![Alt text](cot.png)   """;Computer Vision;https://github.com/Rakeshpavan333/oct_dil
"""PATH/class1 <br/>  PATH/class2 <br/>   Code for evaulate FID score came from https://github.com/bioinf-jku/TTUR   """;Computer Vision;https://github.com/rosinality/sagan-pytorch
"""PATH/class1 <br/>  PATH/class2 <br/>   Code for evaulate FID score came from https://github.com/bioinf-jku/TTUR   """;General;https://github.com/rosinality/sagan-pytorch
"""TensorFlow version == 1.4   """;Computer Vision;https://github.com/SeonbeomKim/TensorFlow-pix2pix
"""TensorFlow version == 1.4   """;General;https://github.com/SeonbeomKim/TensorFlow-pix2pix
"""- Source code:https://github.com/AlexeyAB/darknet   Download weight   For coco dataset you can use tool/coco_annotatin.py.   ONNX and TensorRT models are converted from Pytorch (TianXiaomo): Pytorch->ONNX->TensorRT.   | Pytorch (TianXiaomo)|       0.466 |       0.704 |       0.505 |       0.267 |       0.524 |       0.629 |   | Pytorch (TianXiaomo)|       0.404 |       0.615 |       0.436 |       0.196 |       0.438 |       0.552 |   Pytorch version Recommended:   Install onnxruntime  pip install onnxruntime   Pytorch version Recommended:   Install onnxruntime  pip install onnxruntime   TensorRT version Recommended: 7.0  7.1   1: Thanks:github:https://github.com/onnx/onnx-tensorflow  2: Run git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow  Run pip install -e .  Note:Errors will occur when using ""pip install onnx-tf""  at least for me it is recommended to use source code installation  1. Compile the DeepStream Nvinfer Plugin   cd DeepStream        make   """;Computer Vision;https://github.com/weidalin/yolov4_mixup
"""git clone https://github.com/kklipski/ALHE-projekt.git   Zainstaluj w swoim projekcie pakiety wymienione w pliku requirements.txt.   W terminalu w PyCharm (warunek: otwarty projekt) u≈ºyj (pierwszej) komendy wygenerowanej na stronie: https://pytorch.org/get-started/locally/  Przyk≈Çad dla konfiguracji: PyTorch Build: Stable (1.1)  Your OS: Windows  Package: Pip  Language: Python 3.7  CUDA: 10.0:  pip3 install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp37-cp37m-win_amd64.whl   """;General;https://github.com/kklipski/ALHE-projekt
"""Instructions for acquiring PTB and WT2 can be found here. While CIFAR-10 can be automatically downloaded by torchvision  ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions here.   cd rnn &amp;&amp; python train.py                                 #: PTB   """;General;https://github.com/HeungChangLee/AutoML
"""""";Reinforcement Learning;https://github.com/kshitij-ingale/Reinforcement-Learning
"""- Clone this repo: ```bash git clone https://github.com/ImagingLab/Colorizing-with-GANs.git cd Colorizing-with-GANs ``` - Install Tensorflow and dependencies from https://www.tensorflow.org/install/ - Install python requirements: ```bash pip install -r requirements.txt ```   """;Computer Vision;https://github.com/ImagingLab/Colorizing-with-GANs
"""Follow the instructions below to get our project running on your local machine.  1. Clone the repository and make sure you have prerequisites below to run the code. 2. Run `python src/main.py --help` to see the various options available to specify. 3. To train the model  run the command `python src/main.py ...` along with the flags. For example  to run on the maps (map-to-satellite) dataset  you may run  ```bash python src/main.py --mode train --data_root '../datasets/maps' --num_epochs 100 --data_invert ```  4. All the outputs will be saved to `src/output/[timestamp]` where `[timestamp]` is the time of start of training.   """;Computer Vision;https://github.com/vamsi3/pix2pix
"""Follow the instructions below to get our project running on your local machine.  1. Clone the repository and make sure you have prerequisites below to run the code. 2. Run `python src/main.py --help` to see the various options available to specify. 3. To train the model  run the command `python src/main.py ...` along with the flags. For example  to run on the maps (map-to-satellite) dataset  you may run  ```bash python src/main.py --mode train --data_root '../datasets/maps' --num_epochs 100 --data_invert ```  4. All the outputs will be saved to `src/output/[timestamp]` where `[timestamp]` is the time of start of training.   """;General;https://github.com/vamsi3/pix2pix
"""- Clone this repo: ```bash git clone https://github.com/ImagingLab/Colorizing-with-GANs.git cd Colorizing-with-GANs ``` - Install Tensorflow and dependencies from https://www.tensorflow.org/install/ - Install python requirements: ```bash pip install -r requirements.txt ```   """;General;https://github.com/ImagingLab/Colorizing-with-GANs
"""- TODO: 	- Load vocabulary. 	- Perform decoding after the translation. ---  """;General;https://github.com/jadore801120/attention-is-all-you-need-pytorch
"""- TODO: 	- Load vocabulary. 	- Perform decoding after the translation. ---  """;Natural Language Processing;https://github.com/jadore801120/attention-is-all-you-need-pytorch
"""""";General;https://github.com/owruby/shake-shake_pytorch
"""Siamese Mask R-CNN is designed as a minimal variation of Mask R-CNN which can perform the visual search task described above. For more details please read the [paper](https://arxiv.org/abs/1811.11507).   <p align=""center"">  <img src=""figures/siamese-mask-rcnn-sketch.png"" width=50%> </p>   The model requires [MS COCO](http://cocodataset.org/#home) and the [CocoAPI](https://github.com/waleedka/coco) to be added to `/data`. ``` cd data git clone https://github.com/cocodataset/cocoapi.git ``` It is recommended to symlink the dataset root of MS COCO.  ``` ln -s $PATH_TO_COCO$/coco coco ``` If unsure follow the instructions of the [Matterport Mask R-CNN implementation](https://github.com/matterport/Mask_RCNN#ms-coco-requirements).   1. Clone this repository 2. Prepare COCO dataset as described below 3. Run the [install_requirements.ipynb](install_requirements.ipynb) notebook to install all relevant dependencies.   One-shot instance segmentation can be summed up as: Given a query image and a reference image showing an object of a novel category  we seek to detect and segment all instances of the corresponding category (in the image above ‚Äòperson‚Äô on the left  ‚Äòcar‚Äô on the right). Note that no ground truth annotations of reference categories are used during training. This type of visual search task creates new challenges for computer vision algorithms  as methods from metric and few-shot learning have to be incorporated into the notoriously hard tasks ofobject identification and segmentation.  Siamese Mask R-CNN extends Mask R-CNN - a state-of-the-art object detection and segmentation system - with a Siamese backbone and a matching procedure to perform this type of visual search.   Get the pretrained weights from the [releases menu](https://github.com/bethgelab/siamese-mask-rcnn/releases) and save them to `/checkpoints`.   """;Computer Vision;https://github.com/bethgelab/siamese-mask-rcnn
"""Also see: https://www.bioneurotech.com/iwa-dml   Code source from:   """;General;https://github.com/Harry-Muzart/harry-muzart.github.io
"""Also see: https://www.bioneurotech.com/iwa-dml   Code source from:   """;Natural Language Processing;https://github.com/Harry-Muzart/harry-muzart.github.io
"""""";General;https://github.com/IrishCoffee/cudnnMultiHeadAttention
"""""";Natural Language Processing;https://github.com/IrishCoffee/cudnnMultiHeadAttention
"""Install Niftynet (https://niftynet.readthedocs.io/en/dev/installation.html)   Train from niftynet/extensions with command line:      - outdirname: the absolute path of the directory you want your output goes to   a directory with all the encoded files   """;General;https://github.com/blackPacha/VAE_ABIDE1
"""``` #: clone git clone https://github.com/leon-liangwu/MaskYolo_Caffe.git --recursive  #: install requirements cd ROOT_MaskYolo pip install -r requirements.txt  #: compile box_utils cd lib/box_utils python setup.py build_ext --inplace  #: compile caffe cd caffe-maskyolo cp Makefile.config.example Makefile.config make -j make pycaffe ```   cd ROOT_MaskYolo   sh ./scripts/convert_detection.sh  /path/to/train.txt /path/to/lmdb      cd ./models/mb_v2_t4_cls5_yolo/  sh train_yolo.sh   cd ROOT_MaskYolo/lib/cocoapi/PythonAPI  make -j   cd ROOT_MaskYolo  python scripts/createdata_mask.py --coco_dir=/path/to/coco --lmdb_dir=/path/to/lmdb   cd ./models_maskyolo/mb_body_mask   sh train_maskyolo_step1.sh   sh train_maskyolo_step2.sh   ``` cd tools python yolo_inference.py [--img_path=xxx.jpg] [--model=xxx.prototxt] [--weights=xxx.caffemodel] #: Net forward time consumed: 3.96ms ``` The demo result is shown below.  ![](assets/detection1.png)   ``` cd tools python mask_inference.py [--img_path=xxx.jpg] [--model=xxx.prototxt] [--weights=xxx.caffemodel]  #: Net forward time consumed: 8.67ms  ```   ``` cd tools  python kps_inference.py [--img_path=xxx.jpg] [--model=xxx.prototxt] [--weights=xxx.caffemodel]  #: Net forward time consumed: 5.58ms ```     """;Computer Vision;https://github.com/leon-liangwu/MaskYolo_Caffe
"""Also see: https://www.bioneurotech.com/iwa-dml   Code source from:   """;Computer Vision;https://github.com/Harry-Muzart/harry-muzart.github.io
"""""";General;https://github.com/KAIST-AILab/deeprl_practice_colab
"""""";Natural Language Processing;https://github.com/drewwilimitis/hyperbolic-learning
"""One example is shown below:  ```python {     ""content"": ""‰∏ñÈî¶ËµõÁöÑÊï¥‰ΩìÊ∞¥Âπ≥ËøúÈ´ò‰∫é‰∫öÊ¥≤ÊùØÔºåË¶ÅÂ¶ÇÂêå‰∫öÊ¥≤ÊùØÈÇ£Ê†∑‚ÄúÈ±º‰∏éÁÜäÊéåÂÖºÂæó‚ÄùÔºåÂ∞±ÈúÄË¶ÅÂêÑÊñπÈù¢ÂØÜÂàáÈÖçÂêà„ÄÅ#:idiom#:„ÄÇ‰Ωú‰∏∫‰∏ªÂ∏ÖÁöÑ‰øûËßâÊïèÔºåÈô§‰∫ÜÂæóÊâìÁ†¥‰øùÂÆàÊÄùÊÉ≥ÔºåÊï¢‰∫éÁ†¥Ê†ºÁî®‰∫∫ÔºåËøòÂæóÂ∑ß‰∫éÁî®ÂÖµ„ÄÅ#:idiom#:„ÄÅÁÅµÊ¥ªÊéíÈòµÔºåÊåáÊå•ÂæóÂΩìÔºåÂäõ‰∫âÈÄöËøáÊØîËµõÊé®Êñ∞‰∫∫„ÄÅÂá∫‰Ω≥Áª©„ÄÅÂá∫Êñ∞ÁöÑÊàòÊñóÂäõ„ÄÇ""       ""realCount"": 2      ""groundTruth"": [""ÈÄöÂäõÂêà‰Ωú""  ""ÊúâÁöÑÊîæÁü¢""]       ""candidates"": [         [""Âá≠Á©∫ÊçèÈÄ†""  ""È´òÂ§¥Â§ßÈ©¨""  ""ÈÄöÂäõÂêà‰Ωú""  ""ÂêåËàüÂÖ±Êµé""  ""ÂíåË°∑ÂÖ±Êµé""  ""Ëì¨Â§¥Âû¢Èù¢""  ""Á¥ßÈî£ÂØÜÈºì""]           [""Âè´Ëã¶ËøûÂ§©""  ""Èáè‰ΩìË£ÅË°£""  ""ÈáëÊ¶úÈ¢òÂêç""  ""ÁôæÊàò‰∏çÊÆÜ""  ""Áü•ÂΩºÁü•Â∑±""  ""ÊúâÁöÑÊîæÁü¢""  ""È£éÊµÅÊâçÂ≠ê""]     ] } ```  - `content`: The given passage where the original idioms are replaced by placeholders `#idiom#` - `realCount`: The number of placeholders or blanks - `groundTruth`: The golden answers in the order of blanks - `candidates`: The given candidates in the order of blanks   """;Natural Language Processing;https://github.com/chujiezheng/ChID-Dataset
"""This allows you to greatly simplify the model  since it does not have to deal with the manual placement of tensors.  Instead  you just specify which GPU you'd like to use in the beginning of your script.   : Horovod: pin GPU to be used to process local rank (one GPU per process)   Congratulations!  If you made it this far  your fashion_mnist.py should now be fully distributed.  To verify  you can run the following command in the terminal  which should produce no output:   In this tutorial  you will learn how to apply Horovod to a [WideResNet](https://arxiv.org/abs/1605.07146) model  trained on the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset.   In `fashion_mnist.py`  we're using the filename of the last checkpoint to determine the epoch to resume training from in case of a failure:  ![image](https://user-images.githubusercontent.com/16640218/54185268-d35d3f00-4465-11e9-99eb-96d4b99f1d38.png)  As you scale your workload to multi-node  some of your workers may not have access to the filesystem containing the checkpoint.  For that reason  we make the first worker to determine the epoch to restart from  and *broadcast* that information to the rest of the workers.  To broadcast the starting epoch from the first worker  add the following code:  ```python #: Horovod: broadcast resume_from_epoch from rank 0 (which will have #: checkpoints) to other ranks. resume_from_epoch = hvd.broadcast(resume_from_epoch  0  name='resume_from_epoch') ```  ![image](https://user-images.githubusercontent.com/16640218/53534072-2de3bc00-3ab2-11e9-8cf1-7531542e3202.png) (see line 52-54)   """;Computer Vision;https://github.com/darkreapyre/HaaS
"""* PyTorch 1.2 + TorchText   * 2.1 Build Transformer-XL + MultiAttention heads   * Requirements: Python 3 + Pytorch v1.2   * TODO: Add GPU support  * Build Bert - Bidirectional Transformer   * Build Bert - Bidirectional Transformer   * pip install transformers tb-nightly   """;General;https://github.com/fanchenyou/transformer-study
"""This implementation is based on [mmdetection](https://github.com/open-mmlab/mmdetection)(v1.0.0). Please refer to [INSTALL.md](docs/INSTALL.md) for installation and dataset preparation.   For your convenience  we provide the following trained models on COCO (more models are coming soon).   Once the installation is done  you can download the provided models and use [inference_demo.py](demo/inference_demo.py) to run a quick demo.   """;Computer Vision;https://github.com/MY-Swich/SOLO_my
""" |Backbone|Detectors|AP|AP<sub>50</sub>|AP<sub>75</sub>|AP<sub>S</sub>|AP<sub>M</sub>|AP<sub>L</sub>|Weights| |:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:| |ResNet-50 + Triplet Attention (k = 7)|Mask RCNN|**35.8**|**57.8**|**38.1**|**18**|**38.1**|**50.7**|[Google Drive](https://drive.google.com/file/d/18hFlWdziAsK-FB_GWJk3iBRrtdEJK7lf/view?usp=sharing)|    ``` usage: train_imagenet.py  [-h] [--arch ARCH] [-j N] [--epochs N] [--start-epoch N] [-b N]                           [--lr LR] [--momentum M] [--weight-decay W] [--print-freq N]                           [--resume PATH] [-e] [--pretrained] [--world-size WORLD_SIZE]                           [--rank RANK] [--dist-url DIST_URL]                           [--dist-backend DIST_BACKEND] [--seed SEED] [--gpu GPU]                           [--multiprocessing-distributed]                           DIR  PyTorch ImageNet Training  positional arguments:   DIR                   path to dataset  optional arguments:   -h  --help            show this help message and exit   --arch ARCH  -a ARCH  model architecture: alexnet | densenet121 |                         densenet161 | densenet169 | densenet201 |                         resnet101 | resnet152 | resnet18 | resnet34 |                         resnet50 | squeezenet1_0 | squeezenet1_1 | vgg11 |                         vgg11_bn | vgg13 | vgg13_bn | vgg16 | vgg16_bn | vgg19                         | vgg19_bn (default: resnet18)   -j N  --workers N     number of data loading workers (default: 4)   --epochs N            number of total epochs to run   --start-epoch N       manual epoch number (useful on restarts)   -b N  --batch-size N  mini-batch size (default: 256)  this is the total                         batch size of all GPUs on the current node when using                         Data Parallel or Distributed Data Parallel   --lr LR  --learning-rate LR                         initial learning rate   --momentum M          momentum   --weight-decay W  --wd W                         weight decay (default: 1e-4)   --print-freq N  -p N  print frequency (default: 10)   --resume PATH         path to latest checkpoint (default: none)   -e  --evaluate        evaluate model on validation set   --pretrained          use pre-trained model   --world-size WORLD_SIZE                         number of nodes for distributed training   --rank RANK           node rank for distributed training   --dist-url DIST_URL   url used to set up distributed training   --dist-backend DIST_BACKEND                         distributed backend   --seed SEED           seed for initializing training.   --gpu GPU             GPU id to use.   --multiprocessing-distributed                         Use multi-processing distributed training to launch N                         processes per node  which has N GPUs. This is the                         fastest way to use PyTorch for either single node or                         multi node data parallel training ```   """;General;https://github.com/landskape-ai/triplet-attention
"""Given an image of a person‚Äôs face  the task of classifying the ID of the face is known as **face classification**. Whereas the problem of determining whether two face images are of the same person is known as **face verification** and this has several important applications. This mini-project uses convolutional neural networks (CNNs) to design an end-to-end system for face classification and face verification.   """;Computer Vision;https://github.com/anjandeepsahni/face_classification
"""Given an image of a person‚Äôs face  the task of classifying the ID of the face is known as **face classification**. Whereas the problem of determining whether two face images are of the same person is known as **face verification** and this has several important applications. This mini-project uses convolutional neural networks (CNNs) to design an end-to-end system for face classification and face verification.   """;General;https://github.com/anjandeepsahni/face_classification
"""The project is based on PyTorch 1.5+ and Python 3.6+  because method signatures and type hints are beautiful. If you do not have Python 3.6  install it first. [Here is how for Ubuntu 16.04](https://vsupalov.com/developing-with-python3-6-on-ubuntu-16-04/). Then  in your favorite virtual environment  simply do:  ``` pip install flair ```   Let's run named entity recognition (NER) over an example sentence. All you need to do is make a `Sentence`  load a pre-trained model and use it to predict tags for the sentence:  ```python from flair.data import Sentence from flair.models import SequenceTagger  #: make a sentence sentence = Sentence('I love Berlin .')  #: load the NER tagger tagger = SequenceTagger.load('ner')  #: run NER over sentence tagger.predict(sentence) ```  Done! The `Sentence` now has entity annotations. Print the sentence to see what the tagger found.  ```python print(sentence) print('The following NER tags are found:')  #: iterate over entities and print for entity in sentence.get_spans('ner'):     print(entity) ```  This should print:  ```console Sentence: ""I love Berlin ."" - 4 Tokens  The following NER tags are found:  Span [3]: ""Berlin""   [‚àí Labels: LOC (0.9992)] ```   We provide a set of quick tutorials to get you started with the library:  * [Tutorial 1: Basics](/resources/docs/TUTORIAL_1_BASICS.md) * [Tutorial 2: Tagging your Text](/resources/docs/TUTORIAL_2_TAGGING.md) * [Tutorial 3: Embedding Words](/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md) * [Tutorial 4: List of All Word Embeddings](/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md) * [Tutorial 5: Embedding Documents](/resources/docs/TUTORIAL_5_DOCUMENT_EMBEDDINGS.md) * [Tutorial 6: Loading a Dataset](/resources/docs/TUTORIAL_6_CORPUS.md) * [Tutorial 7: Training a Model](/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md) * [Tutorial 8: Training your own Flair Embeddings](/resources/docs/TUTORIAL_9_TRAINING_LM_EMBEDDINGS.md) * [Tutorial 9: Training a Zero Shot Text Classifier (TARS)](/resources/docs/TUTORIAL_10_TRAINING_ZERO_SHOT_MODEL.md)  The tutorials explain how the base NLP classes work  how you can load pre-trained models to tag your text  how you can embed your text with different word or document embeddings  and how you can train your own language models  sequence labeling models  and text classification models. Let us know if anything is unclear.  There is also a dedicated landing page for our **[biomedical NER and datasets](/resources/docs/HUNFLAIR.md)** with installation instructions and tutorials.  There are also good third-party articles and posts that illustrate how to use Flair: * [How to build a text classifier with Flair](https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f) * [How to build a microservice with Flair and Flask](https://shekhargulati.com/2019/01/04/building-a-sentiment-analysis-python-microservice-with-flair-and-flask/) * [A docker image for Flair](https://towardsdatascience.com/docker-image-for-nlp-5402c9a9069e) * [Great overview of Flair functionality and how to use in Colab](https://www.analyticsvidhya.com/blog/2019/02/flair-nlp-library-python/) * [Visualisation tool for highlighting the extracted entities](https://github.com/lunayach/visNER) * [Practical approach of State-of-the-Art Flair in Named Entity Recognition](https://medium.com/analytics-vidhya/practical-approach-of-state-of-the-art-flair-in-named-entity-recognition-46a837e25e6b) * [Benchmarking NER algorithms](https://towardsdatascience.com/benchmark-ner-algorithm-d4ab01b2d4c3) * [Training a Flair text classifier on Google Cloud Platform (GCP) and serving predictions on GCP](https://github.com/robinvanschaik/flair-on-gcp) * [Model Interpretability for transformer-based Flair models](https://github.com/robinvanschaik/interpret-flair)   """;Sequential;https://github.com/flairNLP/flair
"""```python3 X = load_data() #: This is assumed to be a 2-dimensional numpy array  where rows represent data samples. ``` Basic DISCERN instance (numpy-based): ```python3 from DISCERN import DISCERN  di = DISCERN() di.fit(X)  clustering_labels = di.labels_ cluster_centers = di.cluster_centers_ sse_loss = di.inertia_ ``` Fix the number of clusters to a specific number (only use DISCERN to initialize K-Means) ```python3 di = DISCERN(n_clusters=K) ``` Use Spherical K-Means ```python3 di = DISCERN(metric='cosine') ``` Specify an upper bound for the number of clusters ```python3 di = DISCERN(max_n_clusters=1000) ```    """;General;https://github.com/alihassanijr/DISCERN
"""The project is based on PyTorch 1.5+ and Python 3.6+  because method signatures and type hints are beautiful. If you do not have Python 3.6  install it first. [Here is how for Ubuntu 16.04](https://vsupalov.com/developing-with-python3-6-on-ubuntu-16-04/). Then  in your favorite virtual environment  simply do:  ``` pip install flair ```   Let's run named entity recognition (NER) over an example sentence. All you need to do is make a `Sentence`  load a pre-trained model and use it to predict tags for the sentence:  ```python from flair.data import Sentence from flair.models import SequenceTagger  #: make a sentence sentence = Sentence('I love Berlin .')  #: load the NER tagger tagger = SequenceTagger.load('ner')  #: run NER over sentence tagger.predict(sentence) ```  Done! The `Sentence` now has entity annotations. Print the sentence to see what the tagger found.  ```python print(sentence) print('The following NER tags are found:')  #: iterate over entities and print for entity in sentence.get_spans('ner'):     print(entity) ```  This should print:  ```console Sentence: ""I love Berlin ."" - 4 Tokens  The following NER tags are found:  Span [3]: ""Berlin""   [‚àí Labels: LOC (0.9992)] ```   We provide a set of quick tutorials to get you started with the library:  * [Tutorial 1: Basics](/resources/docs/TUTORIAL_1_BASICS.md) * [Tutorial 2: Tagging your Text](/resources/docs/TUTORIAL_2_TAGGING.md) * [Tutorial 3: Embedding Words](/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md) * [Tutorial 4: List of All Word Embeddings](/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md) * [Tutorial 5: Embedding Documents](/resources/docs/TUTORIAL_5_DOCUMENT_EMBEDDINGS.md) * [Tutorial 6: Loading a Dataset](/resources/docs/TUTORIAL_6_CORPUS.md) * [Tutorial 7: Training a Model](/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md) * [Tutorial 8: Training your own Flair Embeddings](/resources/docs/TUTORIAL_9_TRAINING_LM_EMBEDDINGS.md) * [Tutorial 9: Training a Zero Shot Text Classifier (TARS)](/resources/docs/TUTORIAL_10_TRAINING_ZERO_SHOT_MODEL.md)  The tutorials explain how the base NLP classes work  how you can load pre-trained models to tag your text  how you can embed your text with different word or document embeddings  and how you can train your own language models  sequence labeling models  and text classification models. Let us know if anything is unclear.  There is also a dedicated landing page for our **[biomedical NER and datasets](/resources/docs/HUNFLAIR.md)** with installation instructions and tutorials.  There are also good third-party articles and posts that illustrate how to use Flair: * [How to build a text classifier with Flair](https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f) * [How to build a microservice with Flair and Flask](https://shekhargulati.com/2019/01/04/building-a-sentiment-analysis-python-microservice-with-flair-and-flask/) * [A docker image for Flair](https://towardsdatascience.com/docker-image-for-nlp-5402c9a9069e) * [Great overview of Flair functionality and how to use in Colab](https://www.analyticsvidhya.com/blog/2019/02/flair-nlp-library-python/) * [Visualisation tool for highlighting the extracted entities](https://github.com/lunayach/visNER) * [Practical approach of State-of-the-Art Flair in Named Entity Recognition](https://medium.com/analytics-vidhya/practical-approach-of-state-of-the-art-flair-in-named-entity-recognition-46a837e25e6b) * [Benchmarking NER algorithms](https://towardsdatascience.com/benchmark-ner-algorithm-d4ab01b2d4c3) * [Training a Flair text classifier on Google Cloud Platform (GCP) and serving predictions on GCP](https://github.com/robinvanschaik/flair-on-gcp) * [Model Interpretability for transformer-based Flair models](https://github.com/robinvanschaik/interpret-flair)   """;Natural Language Processing;https://github.com/flairNLP/flair
"""NOTE: We do NOT generate the whole LSTM/Bi-LSTM architecture using Pytorch. Instead  we just use           self.name = name     The principle says: ""Everyone must be able to run everything by one click!"". So you see pretty much everything in one   the previously processed sequence. It can be seen in the following Python script:   Output:  i m glad i invited you  EOS   """;Natural Language Processing;https://github.com/astorfi/sequence-to-sequence-from-scratch
"""How to compile on Linux  How to compile on Windows   Yolo v3 source chart for the RetinaNet on MS COCO got from Table 1 (e): https://arxiv.org/pdf/1708.02002.pdf   A Yolo cross-platform Windows and Linux version (for object detection). Contributtors: https://github.com/pjreddie/darknet/graphs/contributors  This repository is forked from Linux-version: https://github.com/pjreddie/darknet   both Windows and Linux   both cuDNN v5-v7  CUDA >= 7.5   Linux GCC>=4.9 or Windows MS Visual Studio 2015 (v140): https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409  (or offline ISO image)  CUDA 9.1: https://developer.nvidia.com/cuda-downloads  OpenCV 3.4.0: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.0/opencv-3.4.0-vc14_vc15.exe/download  or OpenCV 2.4.13: https://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.13/opencv-2.4.13.2-vc14.exe/download   GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported   Start Smart WebCam on your phone   Just do make in the darknet directory.   * GPU=1 to build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)  * CUDNN=1 to build with cuDNN v5-v7 to accelerate training by using GPU (cuDNN should be in /usr/local/cudnn)   If you have MSVS 2015  CUDA 9.1  cuDNN 7.0 and OpenCV 3.x (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet.sln  set x64 and Release https://hsto.org/webt/uh/fk/-e/uhfk-eb0q-hwd9hsxhrikbokd6u.jpeg and do the: Build -> Build darknet. NOTE: If installing OpenCV  use OpenCV 3.4.0 or earlier. This is a bug in OpenCV 3.4.1 in the C API (see #500).   1.2 Check that there are bin and include folders in the C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1 if aren't  then copy them to this folder from the path where is CUDA installed  1.3. To install CUDNN (speedup neural network)  do the following:  download and install cuDNN 7.0 for CUDA 9.1: https://developer.nvidia.com/cudnn  add Windows system variable cudnn with path to CUDNN: https://hsto.org/files/a49/3dc/fc4/a493dcfc4bd34a1295fd15e0e2e01f26.jpg   If you have other version of CUDA (not 9.1) then open build\darknet\darknet.vcxproj by using Notepad  find 2 places with ""CUDA 9.1"" and change it to your CUDA-version  then do step 1  If you don't have GPU  but have MSVS 2015 and OpenCV 3.0 (with paths: C:\opencv_3.0\opencv\build\include & C:\opencv_3.0\opencv\build\x64\vc14\lib)  then start MSVS  open build\darknet\darknet_no_gpu.sln  set x64 and Release  and do the: Build -> Build darknet_no_gpu   Note: CUDA must be installed only after that MSVS2015 had been installed.  Also  you can to create your own darknet.sln & darknet.vcxproj  this example for CUDA 9.1 and OpenCV 3.0   - (right click on project) -> Build dependecies -> Build Customizations -> set check on CUDA 9.1 or what version you have - for example as here: http://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/01/VS2013-R-5.jpg   cusolver64_91.dll  curand64_91.dll  cudart64_91.dll  cublas64_91.dll - 91 for CUDA 9.1 or your version  from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1\bin   For OpenCV 2.4.13: opencv_core2413.dll  opencv_highgui2413.dll and opencv_ffmpeg2413_64.dll from  C:\opencv_2.4.13\opencv\build\x64\vc14\bin   Download and install Python for Windows: https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe   (Note: To disable Loss-Window use flag -dont_show. If you are using CPU  try darknet_no_gpu.exe instead of darknet.exe.)   Also you can get result earlier than all 45000 iterations.   Download PascalVOC dataset  install Python 3.x and get file 2007_test.txt as described here: https://github.com/AlexeyAB/darknet#how-to-train-pascal-voc-data   [![Everything Is AWESOME](http://img.youtube.com/vi/VOC3huqHrss/0.jpg)](https://www.youtube.com/watch?v=VOC3huqHrss ""Everything Is AWESOME"")  Others: https://www.youtube.com/channel/UC7ev3hNVkx4DzZ3LO19oebg   * `darknet_yolo_v3.cmd` - initialization with 236 MB **Yolo v3** COCO-model yolov3.weights & yolov3.cfg and show detection on the image: dog.jpg  * `darknet_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and waiting for entering the name of the image file * `darknet_demo_voc.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4 * `darknet_demo_store.cmd` - initialization with 194 MB VOC-model yolo-voc.weights & yolo-voc.cfg and play your video file which you must rename to: test.mp4  and store result to: res.avi * `darknet_net_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from network video-camera mjpeg-stream (also from you phone) * `darknet_web_cam_voc.cmd` - initialization with 194 MB VOC-model  play video from Web-Camera number #0 * `darknet_coco_9000.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the image: dog.jpg * `darknet_coco_9000_demo.cmd` - initialization with 186 MB Yolo9000 COCO-model  and show detection on the video (if it is present): street4k.mp4  and store result to: res.avi   On Linux use `./darknet` instead of `darknet.exe`  like this:`./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights`  * **Yolo v3** COCO - image: `darknet.exe detector test data/coco.data cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Alternative method Yolo v3 COCO - image: `darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25` * Output coordinates of objects: `darknet.exe detector test data/coco.data yolov3.cfg yolov3.weights -thresh 0.25 dog.jpg -ext_output` * 194 MB VOC-model - image: `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -i 0` * 194 MB VOC-model - video: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 194 MB VOC-model - **save result to the file res.avi**: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights test.mp4 -i 0 -out_filename res.avi` * Alternative method 194 MB VOC-model - video: `darknet.exe yolo demo yolo-voc.cfg yolo-voc.weights test.mp4 -i 0` * 43 MB VOC-model for video: `darknet.exe detector demo data/coco.data cfg/yolov2-tiny.cfg yolov2-tiny.weights test.mp4 -i 0` * **Yolo v3** 236 MB COCO for net-videocam - Smart WebCam: `darknet.exe detector demo data/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model for net-videocam - Smart WebCam: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights http://192.168.0.80:8080/video?dummy=param.mjpg -i 0` * 194 MB VOC-model - WebCamera #0: `darknet.exe detector demo data/voc.data yolo-voc.cfg yolo-voc.weights -c 0` * 186 MB Yolo9000 - image: `darknet.exe detector test cfg/combine9k.data yolo9000.cfg yolo9000.weights` * Remeber to put data/9k.tree and data/coco9k.map under the same folder of your app if you use the cpp api to build an app * To process a list of images `data/train.txt` and save results of detection to `result.txt` use:                                  `darknet.exe detector test data/voc.data yolo-voc.cfg yolo-voc.weights -dont_show -ext_output < data/train.txt > result.txt`   1. To compile Yolo as C++ DLL-file `yolo_cpp_dll.dll` - open in MSVS2015 file `build\darknet\yolo_cpp_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_cpp_dll     * You should have installed **CUDA 9.1**     * To use cuDNN do: (right click on project) -> properties -> C/C++ -> Preprocessor -> Preprocessor Definitions  and add at the beginning of line: `CUDNN;`  2. To use Yolo as DLL-file in your C++ console application - open in MSVS2015 file `build\darknet\yolo_console_dll.sln`  set **x64** and **Release**  and do the: Build -> Build yolo_console_dll      * you can run your console application from Windows Explorer `build\darknet\x64\yolo_console_dll.exe`     * or you can run from MSVS2015 (before this - you should copy 2 files `yolo-voc.cfg` and `yolo-voc.weights` to the directory `build\darknet\` )     * after launching your console application and entering the image file name - you will see info for each object:      `<obj_id> <left_x> <top_y> <width> <height> <probability>`     * to use simple OpenCV-GUI you should uncomment line `//#define OPENCV` in `yolo_console_dll.cpp`-file: [link](https://github.com/AlexeyAB/darknet/blob/a6cbaeecde40f91ddc3ea09aa26a03ab5bbf8ba8/src/yolo_console_dll.cpp#L5)     * you can see source code of simple example for detection on the video file: [link](https://github.com/AlexeyAB/darknet/blob/ab1c5f9e57b4175f29a6ef39e7e68987d3e98704/src/yolo_console_dll.cpp#L75)     `yolo_cpp_dll.dll`-API: [link](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_v2_class.hpp#L42) ``` class Detector { public: 	Detector(std::string cfg_filename  std::string weight_filename  int gpu_id = 0); 	~Detector();  	std::vector<bbox_t> detect(std::string image_filename  float thresh = 0.2  bool use_mean = false); 	std::vector<bbox_t> detect(image_t img  float thresh = 0.2  bool use_mean = false); 	static image_t load_image(std::string image_filename); 	static void free_image(image_t m);  #:ifdef OPENCV 	std::vector<bbox_t> detect(cv::Mat mat  float thresh = 0.2  bool use_mean = false); #:endif }; ```  """;General;https://github.com/toufiksk/darknet
"""In `data/custom/train.txt` and `data/custom/valid.txt`  add paths to images that will be used as train and validation data respectively.   ```bash $ git clone https://github.com/Lornatang/YOLOv3-PyTorch.git $ cd YOLOv3-PyTorch/ $ pip3 install -r requirements.txt ```   Clone and install requirements   Download COCO2014   Using CUDA    Webcam:  --source 0  HTTP stream:  --source https://v.qq.com/x/page/x30366izba3.html   Note: All commands use the following parameters.   $ cd cfgs/   $ bash create_model.sh your-dataset-num-classes   To train on the custom dataset run:   $ git clone https://github.com/Lornatang/YOLOv3-PyTorch && cd YOLOv3-PyTorch   """;Computer Vision;https://github.com/Lornatang/YOLOv3-PyTorch
"""""";Computer Vision;https://github.com/Stick-To/Inception-tensorflow
"""""";General;https://github.com/Stick-To/Inception-tensorflow
"""NOTE: We do NOT generate the whole LSTM/Bi-LSTM architecture using Pytorch. Instead  we just use           self.name = name     The principle says: ""Everyone must be able to run everything by one click!"". So you see pretty much everything in one   the previously processed sequence. It can be seen in the following Python script:   Output:  i m glad i invited you  EOS   """;Sequential;https://github.com/astorfi/sequence-to-sequence-from-scratch
"""You will need to install cmake first (required for dlib  which is used for face alignment). Currently the code only works with CUDA installed (and therefore requires an appropriate GPU) and has been tested on Linux and Windows. For the full set of required Python packages  create a Conda environment from the provided YAML  e.g.  conda create -f pulse.yml  or (Anaconda on Windows):  conda env create -n pulse -f pulse.yml  conda activate pulse   The main file of interest for applying PULSE is `run.py`. A full list of arguments with descriptions can be found in that file; here we describe those relevant to getting started.   """;Computer Vision;https://github.com/marcin-laskowski/Pulse
"""NOTE: We do NOT generate the whole LSTM/Bi-LSTM architecture using Pytorch. Instead  we just use           self.name = name     The principle says: ""Everyone must be able to run everything by one click!"". So you see pretty much everything in one   the previously processed sequence. It can be seen in the following Python script:   Output:  i m glad i invited you  EOS   """;General;https://github.com/astorfi/sequence-to-sequence-from-scratch
"""""";Natural Language Processing;https://github.com/bharat3012/Word-Representation-Vector-Space
"""An unofficical low-resolution (32 x 32) implementation of BigBiGAN   Paper: https://arxiv.org/abs/1907.02544  Python 3.6   Matplotlib 3.1.1   Change them to fit in your GPU according to your VRAM(>=6 GB recommended).   Set up the flags in ```main.py```. In terminal  enter ```python3 main.py``` to execute the training.  MNIST ![mnist](https://github.com/LEGO999/BIgBiGAN/blob/master/fig/mnist2.png) Fashion-MNIST ![fmnist](https://github.com/LEGO999/BIgBiGAN/blob/master/fig/fmnist-22.png) CIFAR10 ![cifar10](https://github.com/LEGO999/BigBiGAN-TensorFlow2.0/blob/master/fig/cifar10-con-49.png)  """;General;https://github.com/LEGO999/BigBiGAN-TensorFlow2.0
"""An unofficical low-resolution (32 x 32) implementation of BigBiGAN   Paper: https://arxiv.org/abs/1907.02544  Python 3.6   Matplotlib 3.1.1   Change them to fit in your GPU according to your VRAM(>=6 GB recommended).   Set up the flags in ```main.py```. In terminal  enter ```python3 main.py``` to execute the training.  MNIST ![mnist](https://github.com/LEGO999/BIgBiGAN/blob/master/fig/mnist2.png) Fashion-MNIST ![fmnist](https://github.com/LEGO999/BIgBiGAN/blob/master/fig/fmnist-22.png) CIFAR10 ![cifar10](https://github.com/LEGO999/BigBiGAN-TensorFlow2.0/blob/master/fig/cifar10-con-49.png)  """;Computer Vision;https://github.com/LEGO999/BigBiGAN-TensorFlow2.0
"""For setup  a Makefile is provided:  ```shell script make install ```  Or  you can manually run:  ```shell script #: Install System Requirements python3 -m pip install -r requirements.txt ```  In either case  you should delete the `.cloud` directory. Either the Makefile will do it for you or  you can manually delete it. It contains operations that I use personally when working with the Google API  so unless you are working with any of the same Google APIs  you should delete it.   To use the repository  it can be directly cloned from the command line:  ```shell script git clone --recurse-submodules https://github.com/amogh7joshi/engagement-detection.git ```   Then  use the scripts provided in the scripts directory to install the necessary data:  1. To install the model and caffemodel files for the DNN  use the getdata.sh script.    Once the datasets are preprocessed  they can be called through the following functions:  ```python from data.load_data import get_fer2013_data from data.load_data import get_ckplus_data  #: Load the training  validation  and testing data (repeat with other datasets). X_train  X_validation  X_test  y_train  y_validation  y_test = get_fer2013_data() ```  For more information  visit the `data` subdirectory.  The other tools in the Makefile are for convenience purposes only when committing to this repository   in addition to the `editconstant.sh` script. Do not use them unless you are committing to your own repository.  The `info.json` file contains the relevant locations of the cascade classifiers and DNN model files. You can replace the current locations with those on your computer  and then load the detectors as follows.  ```python from util.info import load_info  #: Set the `eyes` option to true if you want to load the eye cascade. cascade_face  cascade_eyes  net = load_info(eyes = True) ```   ![GitHub Workflow Status](https://img.shields.io/github/workflow/status/amogh7joshi/chemsolve/CodeQL)  Currently  all models have been configured to work with the `fer2013` and `ck+` datasets.  **Model Training**: Run the `trainmodel.py` script. You can edit the number of epochs in the argparse argument at the top of the file. Alternatively  you can run itt from the command line using the flags as mentioned by the  argparse arguments. Model weights will be saved to the `data/model` directory  and at the completion of the training  the best model will be moved into the `data/savedmodels` directory. The json file containing the model architecture will also be saved there. You can control what models to keep in the `data/savedmodels` directory manually.  **Model Testing**: Run the `testmodel.py` script. You can edit which model weights and architecture you want to use at the  location at the top of the file. From there  you can run `model.evaluate` on the pre-loaded training and testing data   you can run `model.predict` on any custom images you want to test  or run any other operations with the model.  A confusion matrix is also present  which will display if `plt.show()` is uncommented.  **Live Emotion Detection**: Run the `videoclassification.py` script. If you already have a trained model  set it at the top of the  script  and it will detect emotions live. For just facial detection  run the `facevideo.py` script. You can choose which detector you want to use  as described at the top of the file. If you want to save images  set the `-s` flag to `True`  and they will save to a  custom directory `imageruntest` at the top-level. More information is included at the top of the file.   **Image Emotion Detection**: Run the `emotionclassification.py` script. Choose the images you want to detect emotions on and place their paths in  the `userimages` variable. If running from the command line  then write out the paths to each of the images when running the script. Optionally  if you just want facial detection   run the `facedetect.py` script. If running from the command line  then read the argument information at the top of the file.  Otherwise  insert the paths of the images that you want to detect faces from into a list called `user_images` midway through the file. The changed images will save to a custom directory called `modded`  but you can change that from the `savedir` variable. For each image inputted  the script will output the same image with a bounding box around the faces detected from the image.   """;Computer Vision;https://github.com/amogh7joshi/engagement-detection
"""For setup  a Makefile is provided:  ```shell script make install ```  Or  you can manually run:  ```shell script #: Install System Requirements python3 -m pip install -r requirements.txt ```  In either case  you should delete the `.cloud` directory. Either the Makefile will do it for you or  you can manually delete it. It contains operations that I use personally when working with the Google API  so unless you are working with any of the same Google APIs  you should delete it.   To use the repository  it can be directly cloned from the command line:  ```shell script git clone --recurse-submodules https://github.com/amogh7joshi/engagement-detection.git ```   Then  use the scripts provided in the scripts directory to install the necessary data:  1. To install the model and caffemodel files for the DNN  use the getdata.sh script.    Once the datasets are preprocessed  they can be called through the following functions:  ```python from data.load_data import get_fer2013_data from data.load_data import get_ckplus_data  #: Load the training  validation  and testing data (repeat with other datasets). X_train  X_validation  X_test  y_train  y_validation  y_test = get_fer2013_data() ```  For more information  visit the `data` subdirectory.  The other tools in the Makefile are for convenience purposes only when committing to this repository   in addition to the `editconstant.sh` script. Do not use them unless you are committing to your own repository.  The `info.json` file contains the relevant locations of the cascade classifiers and DNN model files. You can replace the current locations with those on your computer  and then load the detectors as follows.  ```python from util.info import load_info  #: Set the `eyes` option to true if you want to load the eye cascade. cascade_face  cascade_eyes  net = load_info(eyes = True) ```   ![GitHub Workflow Status](https://img.shields.io/github/workflow/status/amogh7joshi/chemsolve/CodeQL)  Currently  all models have been configured to work with the `fer2013` and `ck+` datasets.  **Model Training**: Run the `trainmodel.py` script. You can edit the number of epochs in the argparse argument at the top of the file. Alternatively  you can run itt from the command line using the flags as mentioned by the  argparse arguments. Model weights will be saved to the `data/model` directory  and at the completion of the training  the best model will be moved into the `data/savedmodels` directory. The json file containing the model architecture will also be saved there. You can control what models to keep in the `data/savedmodels` directory manually.  **Model Testing**: Run the `testmodel.py` script. You can edit which model weights and architecture you want to use at the  location at the top of the file. From there  you can run `model.evaluate` on the pre-loaded training and testing data   you can run `model.predict` on any custom images you want to test  or run any other operations with the model.  A confusion matrix is also present  which will display if `plt.show()` is uncommented.  **Live Emotion Detection**: Run the `videoclassification.py` script. If you already have a trained model  set it at the top of the  script  and it will detect emotions live. For just facial detection  run the `facevideo.py` script. You can choose which detector you want to use  as described at the top of the file. If you want to save images  set the `-s` flag to `True`  and they will save to a  custom directory `imageruntest` at the top-level. More information is included at the top of the file.   **Image Emotion Detection**: Run the `emotionclassification.py` script. Choose the images you want to detect emotions on and place their paths in  the `userimages` variable. If running from the command line  then write out the paths to each of the images when running the script. Optionally  if you just want facial detection   run the `facedetect.py` script. If running from the command line  then read the argument information at the top of the file.  Otherwise  insert the paths of the images that you want to detect faces from into a list called `user_images` midway through the file. The changed images will save to a custom directory called `modded`  but you can change that from the `savedir` variable. For each image inputted  the script will output the same image with a bounding box around the faces detected from the image.   """;General;https://github.com/amogh7joshi/engagement-detection
"""""";Computer Vision;https://github.com/Sakib1263/Inception-InceptionResNet-1D-2D-Tensorflow-Keras
"""""";General;https://github.com/Sakib1263/Inception-InceptionResNet-1D-2D-Tensorflow-Keras
"""ËØ•Â∑•‰ΩúÂ§ÑÁêÜÁöÑÊòØÂõæÂÉèÁöÑ‰∫åÂàÜÁ±ªÈóÆÈ¢ò„ÄÇÂà§Êñ≠ÊòØÂê¶‰∏∫‰∫∫ËÑ∏ÔºåÂÅö‰∏∫‰∫∫ËÑ∏Ê£ÄÊµãÁöÑ‰∏Ä‰∏™Ê®°Âùó„ÄÇ‰∏ª‰ΩìÊ®°ÂûãÊòØ[ResNet](https://arxiv.org/abs/1512.03385)   1. ‰∏ãËΩΩLFW Face DatabaseÊï∞ÊçÆÈõÜÔºåËß£ÂéãÂêéÊîæÁΩÆ‰∫é`data`‰∏ã 2. ËøêË°å`data/prepare_data.py` 4. Ê≥®ÔºöÂú®`data/prepare_data.py`Á¨¨105Ë°åÂ¢ûÂä†ÂèÇÊï∞`download=True`ÔºåÂèØ‰ª•Ëá™Âä®‰∏ãËΩΩcifar100 3. ËøêË°å`gen_train_loader_data.py`   """;General;https://github.com/xcmyz/FaceDetection
"""ËØ•Â∑•‰ΩúÂ§ÑÁêÜÁöÑÊòØÂõæÂÉèÁöÑ‰∫åÂàÜÁ±ªÈóÆÈ¢ò„ÄÇÂà§Êñ≠ÊòØÂê¶‰∏∫‰∫∫ËÑ∏ÔºåÂÅö‰∏∫‰∫∫ËÑ∏Ê£ÄÊµãÁöÑ‰∏Ä‰∏™Ê®°Âùó„ÄÇ‰∏ª‰ΩìÊ®°ÂûãÊòØ[ResNet](https://arxiv.org/abs/1512.03385)   1. ‰∏ãËΩΩLFW Face DatabaseÊï∞ÊçÆÈõÜÔºåËß£ÂéãÂêéÊîæÁΩÆ‰∫é`data`‰∏ã 2. ËøêË°å`data/prepare_data.py` 4. Ê≥®ÔºöÂú®`data/prepare_data.py`Á¨¨105Ë°åÂ¢ûÂä†ÂèÇÊï∞`download=True`ÔºåÂèØ‰ª•Ëá™Âä®‰∏ãËΩΩcifar100 3. ËøêË°å`gen_train_loader_data.py`   """;Computer Vision;https://github.com/xcmyz/FaceDetection
"""""";Graphs;https://github.com/ds4dm/sparse-gcn
"""IMPORTANT: For each experiment  contents of its `.yaml` file is stored in *Text* section in tensorboard!  There are multiple metrics stored for each model and in this section you can find their description. * **accuracy measured at N**: after learning N-th task  evaluation on all previous tasks is performed. In other words *accuracy measured at 10* is PermutedMNIST-10 and *accuracy measured at 100* is PermutedMNIST-100 * **task 0 accuracy**: as we don't want to evaluate all tasks too often  to check if the catastrophic forgetting happened  after learning each task we just check what is its accuracy at task number 0  **References:** 1. Continual learning with hypernetworks  <https://arxiv.org/abs/1906.00695> 2. Three scenarios for continual learning  <https://arxiv.org/abs/1904.07734> 3. HyperNetworks  <https://arxiv.org/abs/1609.09106>  """;General;https://github.com/gahaalt/continual-learning-with-hypernets
"""Skeleton code from https://github.com/ricardorei/lightning-text-classification   - Then  pull the model asRobertaLongForMaskedLM.from_pretrained('simonlevine/bioclinical-roberta-long')- Now  it can be used as usual. Note you may get untrained weights warnings.   """;Computer Vision;https://github.com/simonlevine/clinical-longformer
"""Skeleton code from https://github.com/ricardorei/lightning-text-classification   - Then  pull the model asRobertaLongForMaskedLM.from_pretrained('simonlevine/bioclinical-roberta-long')- Now  it can be used as usual. Note you may get untrained weights warnings.   """;General;https://github.com/simonlevine/clinical-longformer
"""An example can be found in `vgg.py`.  1) Copy the common/normalization.py to your root directory and import it. 2) Build a DecorelationNormalization layer to replace the batch normalization layer.  ```python from common import normalization  ... feature = normalization.DecorelationNormalization(decomposition='iter_norm_wm'                                                    iter_num=5)(feature) ```   """;General;https://github.com/bhneo/decorrelated_bn
"""""";General;https://github.com/ajithcodesit/lstm_copy_task
"""""";Sequential;https://github.com/ajithcodesit/lstm_copy_task
"""[PyTorch] (by Pavel Yakubovskiy)  [PyTorch] (by 4ui_iurz1)   [PyTorch] (by ZJUGiveLab)  [PyTorch] (by MontaEllis)   """;Computer Vision;https://github.com/MrGiovanni/UNetPlusPlus
"""""";Computer Vision;https://github.com/MNaplesDevelopment/DCGAN-PyTorch
"""""";Computer Vision;https://github.com/neptune-ai/open-solution-mapping-challenge
"""My platform is:  * 2080ti gpu * ubuntu-16.04 * python3.6.9 * pytorch-1.3.1 installed via conda * cudatoolkit-10.1.243  * cudnn-7.6.3 in /usr/lib/x86_64-linux-gpu    $ mkdir -p dataset &amp;&amp; cd dataset   Note:    """;General;https://github.com/CoinCheung/fixmatch-pytorch
"""The implementation is based on two papers & Github Repository: - Object Tracking(https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch) - Simple Online and Realtime Tracking with a Deep Association Metric https://arxiv.org/abs/1703.07402 - YOLOv4: Optimal Speed and Accuracy of Object Detection https://arxiv.org/pdf/2004.10934.pdf   Webcam:  --source 0  RTSP stream:  --source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa   """;Computer Vision;https://github.com/JisuHann/Object-Tracking
""" Taken single channels of both image and mask for training.    ![](https://github.com/sagnik1511/U-Net-Reduced-with-keras/blob/main/images/train_data.png)    ![](https://github.com/sagnik1511/U-Net-Reduced-with-keras/blob/main/images/test_data.png)    """;Computer Vision;https://github.com/sagnik1511/U-Net-Reduced-with-TF-keras
"""""";Computer Vision;https://github.com/scut-aitcm/Competitive-Inner-Imaging-SENet
"""""";Computer Vision;https://github.com/secretlyvogon/Neural-Network-Implementations
"""""";General;https://github.com/0xPr0xy/MobileNet-CoreML
"""""";Computer Vision;https://github.com/0xPr0xy/MobileNet-CoreML
"""""";Computer Vision;https://github.com/Stick-To/WideResNet-tensorflow
"""""";Computer Vision;https://github.com/Qengineering/TensorFlow_Lite_Segmentation_RPi_64-bit
"""    $ git clone https://github.com/Neptune-Trojans/GANs   """;Computer Vision;https://github.com/Neptune-Trojans/GANs
"""$ mkdir MyDir <br/>  $ cd MyDir <br/>  $ wget https://github.com/Qengineering/TensorFlow_Lite_Segmentation_RPi_32/archive/refs/heads/master.zip <br/>   """;Computer Vision;https://github.com/Qengineering/TensorFlow_Lite_Segmentation_RPi_32-bit
"""""";Computer Vision;https://github.com/GivralNguyen/Ultra-light-Vehicle-Detection-using-Tiny-Mobilenet-SSD-real-time-on-CPU
"""PYTHON VERSION: 3.6.8  OPENCV VERSION: 3.4.4.19  OPENCV CONTRIBUTION VERSION: 3.4.4.19   SegNet(modified version of caffe) installation  https://github.com/alexgkendall/caffe-segnet 'for cudNN v2'  https://github.com/navganti/caffe-segnet-cudnn7 'for cudNN v7'   """;Computer Vision;https://github.com/HAN-ARK/GVSS-S.A.Drone
"""If you would like to quickly start using the AI explainability 360 toolkit without cloning this repository  then you can install the [aix360 pypi package](https://pypi.org/project/aix360/) as follows.   ```bash (your environment)$ pip install aix360 ```  If you follow this approach  you may need to download the notebooks in the [examples](./examples) folder separately.     Clone the latest version of this repository:  ```bash (aix360)$ git clone https://github.com/Trusted-AI/AIX360 ```  If you'd like to run the examples and tutorial notebooks  download the datasets now and place them in their respective folders as described in [aix360/data/README.md](aix360/data/README.md).  Then  navigate to the root directory of the project which contains `setup.py` file and run:  ```bash (aix360)$ pip install -e . ```   Supported Configurations:  | OS      | Python version | | ------- | -------------- | | macOS   | 3.6  | | Ubuntu  | 3.6  | | Windows | 3.6  |   with other projects on your system. A virtual environment manager is strongly  recommended to ensure dependencies may be installed safely. If you have trouble installing the toolkit  try this first.   if you are curious) and can be installed from   Then  to create a new Python 3.6 environment  run:  conda create --name aix360 python=3.6  conda activate aix360  The shell should now look like (aix360) $. To deactivate the environment  run:  (aix360)$ conda deactivate   Note: Older versions of conda may use source activate aix360 and source  deactivate (activate aix360 and deactivate on Windows).   """;General;https://github.com/Trusted-AI/AIX360
"""""";Computer Vision;https://github.com/Stick-To/PyramidNet-tensorflow
"""""";General;https://github.com/Stick-To/PyramidNet-tensorflow
"""If you're on macOS and you have Homebrew installed  you can install ChromeDriver by running:  brew install chromedriver  You can download ChromeDriver from the official download page.  The scripts are written for Python 3. To install the Python dependencies  run:       pip3 install -r ./requirements.txt   """;General;https://github.com/benjaminvdb/DBRD
"""If you're on macOS and you have Homebrew installed  you can install ChromeDriver by running:  brew install chromedriver  You can download ChromeDriver from the official download page.  The scripts are written for Python 3. To install the Python dependencies  run:       pip3 install -r ./requirements.txt   """;Natural Language Processing;https://github.com/benjaminvdb/DBRD
"""Human visual system starts from lower visual area and proceed to the higher areas. However  it is not a full story. Our lower visual areas are largely affected by various higher visual area interactively.   ![Retino and Non-retino images][incongOccluded]    """;Computer Vision;https://github.com/Ohyeon5/MultiscaleSegmentation
"""ReAgent can be installed via. Docker or manually. Detailed instructions on how to install ReAgent can be found [here](docs/installation.rst).   ReAgent is designed for large-scale  distributed recommendation/optimization tasks where we don‚Äôt have access to a simulator. In this environment  it is typically better to train offline on batches of data  and release new policies slowly over time. Because the policy updates slowly and in batches  we use off-policy algorithms. To test a new policy without deploying it  we rely on counter-factual policy evaluation (CPE)  a set of techniques for estimating a policy based on the actions of another policy.  We also have a set of tools to facilitate applying RL in real-world applications: - Domain Analysis Tool  which analyzes state/action feature importance and identifies whether the problem is a suitable for applying batch RL - Behavior Cloning  which clones from the logging policy to bootstrap the learning policy safely  Detailed instructions on how to use ReAgent can be found [here](docs/usage.rst).    """;General;https://github.com/facebookresearch/ReAgent
"""ReAgent can be installed via. Docker or manually. Detailed instructions on how to install ReAgent can be found [here](docs/installation.rst).   ReAgent is designed for large-scale  distributed recommendation/optimization tasks where we don‚Äôt have access to a simulator. In this environment  it is typically better to train offline on batches of data  and release new policies slowly over time. Because the policy updates slowly and in batches  we use off-policy algorithms. To test a new policy without deploying it  we rely on counter-factual policy evaluation (CPE)  a set of techniques for estimating a policy based on the actions of another policy.  We also have a set of tools to facilitate applying RL in real-world applications: - Domain Analysis Tool  which analyzes state/action feature importance and identifies whether the problem is a suitable for applying batch RL - Behavior Cloning  which clones from the logging policy to bootstrap the learning policy safely  Detailed instructions on how to use ReAgent can be found [here](docs/usage.rst).    """;Reinforcement Learning;https://github.com/facebookresearch/ReAgent
"""""";Computer Vision;https://github.com/hamidriasat/Computer-Vision-and-Deep-Learning
"""cd feature\ extraction   cd feature\ extraction                    cd  fine\ tuning      1- **install python3**     2- **install requirements**:   ```   pip install -r requirements.txt   ```      3- **Download and Extract dataset**:  ``` curl https://raw.githubusercontent.com/EdenMelaku/Transfer-Learning-Pytorch-Implmentation/master/mnist_png.tar.gz | tar xzC /temp  ```           """;General;https://github.com/EdenMelaku/Transfer-Learning-Pytorch-Implementation
"""cd feature\ extraction   cd feature\ extraction                    cd  fine\ tuning      1- **install python3**     2- **install requirements**:   ```   pip install -r requirements.txt   ```      3- **Download and Extract dataset**:  ``` curl https://raw.githubusercontent.com/EdenMelaku/Transfer-Learning-Pytorch-Implmentation/master/mnist_png.tar.gz | tar xzC /temp  ```           """;Computer Vision;https://github.com/EdenMelaku/Transfer-Learning-Pytorch-Implementation
"""Navigate to the folder Sentiment-Analysis-with-BERT and run the command below:  pip install -r requirements.txt  Note that pytorch will have some issues of installing. Make sure you install pytorch separately if you have any issues.  If you find any issues/bugs with the code. I am available on email at venkyr91193@gmail.com  using the pytorch implementation with the help of the libary tranformers. (https://huggingface.co/transformers/)   """;Natural Language Processing;https://github.com/venkyr91193/Sentiment-Analysis-with-BERT
"""For the experiment  we will use Pommerman environment. This has relatively strict constraints on environment settings and simple to deploy algorithms.   """;Reinforcement Learning;https://github.com/tegg89/magnet
""" Once you've finished training your model using affinelayer's **pixpix.py** script  you can use the **export** mode to export only the generator to a new folder:  ``` python pix2pix.py --mode export --output_dir exported_model --input_dir your_model --checkpoint your_model ```  To create a .pict file and quantize your model  use the **export-checkpoint.py** script  ``` python tools/export-checkpoint.py --checkpoint exported_model --output_file model.pict ```  The default setting for number of filters in the first convolutional layer of your generator (--ngf) and your discriminator (--ndf) is set to 64. When using these settings  the quantized model will be around 50 MB in size and require significant processing power.  For simple use in javascript I recommend setting -ngf to 32 or 16 before training  which will result in a final model size of around 13 MB and 3 MB. This will significantly increase the generator's speed  but also reduce the quality of the generated image.   Original pix2pix paper: https://arxiv.org/abs/1611.07004  """;Computer Vision;https://github.com/AlliBalliBaba/PixFace
""" Once you've finished training your model using affinelayer's **pixpix.py** script  you can use the **export** mode to export only the generator to a new folder:  ``` python pix2pix.py --mode export --output_dir exported_model --input_dir your_model --checkpoint your_model ```  To create a .pict file and quantize your model  use the **export-checkpoint.py** script  ``` python tools/export-checkpoint.py --checkpoint exported_model --output_file model.pict ```  The default setting for number of filters in the first convolutional layer of your generator (--ngf) and your discriminator (--ndf) is set to 64. When using these settings  the quantized model will be around 50 MB in size and require significant processing power.  For simple use in javascript I recommend setting -ngf to 32 or 16 before training  which will result in a final model size of around 13 MB and 3 MB. This will significantly increase the generator's speed  but also reduce the quality of the generated image.   Original pix2pix paper: https://arxiv.org/abs/1611.07004  """;General;https://github.com/AlliBalliBaba/PixFace
"""Use python to run run_cnn_test_cifar10.py for experiments on [Cifar10](https://www.cs.toronto.edu/~kriz/cifar.html) and run_cnn_test_cifar100.py for experiments on [Cifar100](https://www.cs.toronto.edu/~kriz/cifar.html)   * Run experiments on Cifar10: ```bash   -  python run_cnn_test_cifar10.py  --lr 0.1 --method ""padam"" --net ""vggnet""  --partial 0.125 --wd 5e-4 ``` * Run experiments on Cifar100: ```bash   -  python run_cnn_test_cifar100.py  --lr 0.1 --method ""padam"" --net ""resnet""  --partial 0.125 --wd 5e-4 ```  """;General;https://github.com/uclaml/Padam
"""Use python to run run_cnn_test_cifar10.py for experiments on [Cifar10](https://www.cs.toronto.edu/~kriz/cifar.html) and run_cnn_test_cifar100.py for experiments on [Cifar100](https://www.cs.toronto.edu/~kriz/cifar.html)   * Run experiments on Cifar10: ```bash   -  python run_cnn_test_cifar10.py  --lr 0.1 --method ""padam"" --net ""vggnet""  --partial 0.125 --wd 5e-4 ``` * Run experiments on Cifar100: ```bash   -  python run_cnn_test_cifar100.py  --lr 0.1 --method ""padam"" --net ""resnet""  --partial 0.125 --wd 5e-4 ```  """;Computer Vision;https://github.com/uclaml/Padam
"""<a name='gan'></a>   ```bash $ python GAN/GAN.py ```  <hr>  <a name='dcgan'></a>   ```bash $ python DCGAN/DCGAN.py ```  <hr>   <a name='lsgan'></a>   ```bash $ python LSGAN/LSGAN.py ```  <hr>  <a name='wgan'></a>   ```bash $ python WGAN/WGAN.py ```  <hr>  <a name='wgan-gp'></a>   ```bash $ python WGAN-GP/WGAN-GP.py ```  <hr>  <a name='dragan'></a>   ```bash $ python DRAGAN/DRAGAN.py ```  <hr>   """;Computer Vision;https://github.com/marload/GANs-TensorFlow2
"""<a name='gan'></a>   ```bash $ python GAN/GAN.py ```  <hr>  <a name='dcgan'></a>   ```bash $ python DCGAN/DCGAN.py ```  <hr>   <a name='lsgan'></a>   ```bash $ python LSGAN/LSGAN.py ```  <hr>  <a name='wgan'></a>   ```bash $ python WGAN/WGAN.py ```  <hr>  <a name='wgan-gp'></a>   ```bash $ python WGAN-GP/WGAN-GP.py ```  <hr>  <a name='dragan'></a>   ```bash $ python DRAGAN/DRAGAN.py ```  <hr>   """;General;https://github.com/marload/GANs-TensorFlow2
"""This part is the easiest. I guess we just write out some text files containing the training data? We can let the usual LM preprocessing pipeline handle it from there.  With the data downloader in place  we simply need to (1) expose the val/test examples  and (2) remove them from the training set.  * Arguably  (2) should be handled by LM preprocessing in a more general way. There are probably non-NLU-eval cases where we want to remove some specific data from training. * Depending on how exactly we do the val/test removal  we may want to format the same example multiple ways to ensure that they don't get leaked into the training set in a slightly tweaked format. * Thought experiment: SQuAD is based largely on Wikipedia. What exactly would we want to remove from the LM? * [GPT-3]: In GPT-3  they attempted to remove val/test from their LM set  but there was a bug that caused leakage. So they ended up doing the opposite: removing overlaps from the LM set from the val/test. Funky. * [GPT-3]: See page 30 and Appendix C for details. They do some funky n-gram based search and removal. We should think about whether we want to follow their protocol exactly   ```bash pip install lm-eval ```   To inspect what the LM inputs look like  you can run the following command:   To evaluate a model  (e.g. GPT-2) on NLU tasks (e.g. LAMBADA  HellaSwag)  you can run the following command.  ```bash python main.py \ 	--model gpt2 \ 	--device cuda:0 \ 	--tasks lambada hellaswag ``` (This uses gpt2-117M by default as per HF defaults  use --model_args to specify other gpt2 sizes)  Additional arguments can be provided to the model constructor using the `--model_args` flag. Most importantly  the `gpt2` model can be used to load an arbitrary HuggingFace model. For example  to run GPTNeo use the following:  ```bash python main.py \ 	--model gpt2 \ 	--model_args pretrained=EleutherAI/gpt-neo-2.7B \ 	--device cuda:0 \ 	--tasks lambada hellaswag ```  If you have access to the OpenAI API  you can also evaluate GPT-3:  ```bash export OPENAI_API_SECRET_KEY=YOUR_KEY_HERE python main.py \ 	--model gpt3 \ 	--model_args engine=davinci \ 	--tasks lambada hellaswag ```  To evaluate mesh-transformer-jax models that are not available on HF  please invoke eval harness through [this script](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/eval_harness.py).   """;Natural Language Processing;https://github.com/EleutherAI/lm-evaluation-harness
"""I leave my own environment below. I tested it out on a single GPU.       * Linux(Ubuntu 18.04.5 LTS)   * GPU:   * nvidia-docker2(for GPU)   and nvidia-docker2(for GPU)   1. build or pull docker image  build image(this might take some time) ```bash ./docker.sh build ``` pull image from [dockerhub](https://hub.docker.com/repository/docker/docker4rintarooo/tspdrl/tags?page=1&ordering=last_updated) ```bash docker pull docker4rintarooo/tspdrl:latest ```  2. run container using docker image(-v option is to mount directory) ```bash ./docker.sh run ``` If you don't have a GPU  you can run ```bash ./docker.sh run_cpu ``` <br><br>   """;Sequential;https://github.com/Rintarooo/TSP_DRL_PtrNet
"""other cDDGANS implemenatations https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN and https://github.com/togheppi/cDCGAN   <td><img src=""https://github.com/DanielLongo/cGANs/blob/master/generated_images/0-20.gif""/>   """;Computer Vision;https://github.com/DanielLongo/cGANs
"""En distribuant le travail en diff√©rentes partitions et sur diff√©rents noeuds  avec ce qu'on appelle le *Resilient Distributed Dataset (RDD)*  Spark est jusqu'√† 30 fois plus rapide que Hadoop MapReduce pour ex√©cuter un tri par exemple.  Spark fonctionne en 4 grandes √©tapes : - on cr√©e un RDD √† partir de notre jeu de donn√©es  - on applique diff√©rentes transformations pour en cr√©er de nouveaux ; r√©sultants de fonctions dites 'immutables' telles que `.map` ou `.filter`  - on d√©cide quels RDDs garder en m√©moire avec les fonctions `.persist` ou `.unpersist`  - et on peut ensuite appliquer des fonctions plus classiques √† nos RDDs comme `.count` ou `.collect` qui modifie le RDD directement  sans en cr√©er un nouveau.  Essayons de reproduire l'algorithme de MapReduce pour compter les mots.  ```python from pyspark import SparkContext  sc = pyspark.SparkContext() file = sc.textfile(""data/count.txt"")              #:split words on each line count = file.flatMap(lambda line: line.split("" ""))             #:add 1 for each occurence of a word             .map(lambda word: (word  1))             #:aggregate the number of occurences of each word             .reduceByKey(lambda a  b: a + b)              count.persist() count.saveAsTextFile(""data/count.txt"") ```    """;General;https://github.com/qmonmous/BigData-X-Python
"""Named entity recognition (NER) is a central component in natural language processing tasks. Identifying named entities is a key part in systems e.g. for question answering or entity linking. Traditionally  NER systems are built using conditional random fields (CRFs). Recent systems are using neural network architectures like bidirectional LSTM with a CRF-layer ontop and pre-trained word embeddings ([Ma and Hovy  2016](http://aclweb.org/anthology/P16-1101); [Lample et al.  2016a](http://aclweb.org/anthology/N16-1030); [Reimers and Gurevych  2017](http://aclweb.org/anthology/D17-1035); [Lin et al.  2017](http://aclweb.org/anthology/W17-4421)).  Pre-trained word embeddings have been shown to be of great use for downstream NLP tasks ([Mikolov et al.  2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf); [Pennington et al.  2014](https://www.aclweb.org/anthology/D14-1162)). Many recently proposed approaches go beyond these pre-trained embeddings. Recent works have proposed methods that produce different representations for the same word depending on its contextual usage ([Peters et al.  2017](http://aclweb.org/anthology/P17-1161) [2018a](https://aclweb.org/anthology/N18-1202); [Akbik et al.  2018](https://www.aclweb.org/anthology/C18-1139); [Devlin et al.  2018](https://arxiv.org/abs/1810.04805)). These methods have shown to be very powerful in the fields of named entity recognition  coreference resolution  part-of-speech tagging and question answering  especially in combination with classic word embeddings.  Our paper is based on the work of [Riedl and Pad√≥ (2018)](http://aclweb.org/anthology/P18-2020). They showed how to build a model for German named entity recognition (NER) that performs at the state of the art for both contemporary and historical texts. Labeled historical texts for German named entity recognition are a low-resource domain. In order to achieve robust state-of-the-art results for historical texts they used transfer-learning with labeled data from other high-resource domains like CoNLL-2003 ([Tjong Kim Sang and De Meulder  2003](http://aclweb.org/anthology/W03-0419)) or GermEval ([Benikova et al.  2014](http://www.lrec-conf.org/proceedings/lrec2014/pdf/276_Paper.pdf)). They showed that using Bi-LSTM with a CRF as the top layer and word embeddings outperforms CRFs with hand-coded features in a big-data situation.  We build up upon their work and use the same low-resource datasets for Historic German. Furthermore  we show how to achieve new state-of-the-art results for Historic German named entity recognition by using only unlabeled data via pre-trained language models and word embeddings. We also introduce a novel language model pre-training objective  that uses only contemporary texts for training to achieve comparable state-of-the-art results on historical texts.   preprocessing steps like tokenization are needed. We use 1/500 of the complete   With the --number argument you should define a unique id for your experiment.   This sections shows how to use one of our trained models with Flair in order to perform NER on a sentence.  Then you can use the following code to perform NER:  ```python from flair.data import Sentence from flair.models import SequenceTagger  #: Noisy OCR :) sentence = Sentence(""April Martin Ansclm   K. Gefan - gen-Auffehers Georg Sausgruber ."")  tagger: SequenceTagger = SequenceTagger.load(""dbmdz/flair-historic-ner-onb"") tagger.predict(sentence)  sentence.to_tagged_string() ```  This outputs:  ```python 'April Martin <B-PER> Ansclm <E-PER>   K. Gefan - gen-Auffehers Georg <B-PER> Sausgruber <E-PER> .' ```   """;Natural Language Processing;https://github.com/dbmdz/historic-ner
"""Upload an image from the left bar  and the result will be output automatically.   Click the check box (Result semantic segmentation)  and you can see reslut of semantic segmentation. ![demo_img1](https://user-images.githubusercontent.com/64745286/119369441-24784700-bcef-11eb-982e-e8d61d497b01.png)    ```bash git clone https://github.com/ikuto056/Estimating_Distance.git streamlit run app.py ```   """;Computer Vision;https://github.com/oki5656/Estimating_Distance
"""Upload an image from the left bar  and the result will be output automatically.   Click the check box (Result semantic segmentation)  and you can see reslut of semantic segmentation. ![demo_img1](https://user-images.githubusercontent.com/64745286/119369441-24784700-bcef-11eb-982e-e8d61d497b01.png)    ```bash git clone https://github.com/ikuto056/Estimating_Distance.git streamlit run app.py ```   """;General;https://github.com/oki5656/Estimating_Distance
"""Upload an image from the left bar  and the result will be output automatically.   Click the check box (Result semantic segmentation)  and you can see reslut of semantic segmentation. ![demo_img1](https://user-images.githubusercontent.com/64745286/119369441-24784700-bcef-11eb-982e-e8d61d497b01.png)    ```bash git clone https://github.com/ikuto056/Estimating_Distance.git streamlit run app.py ```   """;Sequential;https://github.com/oki5656/Estimating_Distance
"""better. For the more experienced  we recommend using the models that Auto-Keras   """;Computer Vision;https://github.com/collectionslab/Omniscribe
"""``` conda env create -f environment.yml conda activate hovernet pip install torch==1.6.0 torchvision==0.7.0 ```  Above  we install PyTorch version 1.6 with CUDA 10.2.    | PyTorch    | 0.8211     | 0.5904     | 0.6321    |   | PyTorch    | 0.8504     | 0.5464     | 0.6009    |   | PyTorch    | 0.756          | 0.636          | 0.559          | 0.557          | 0.348          |     Usage: <br /> ```   python run_train.py [--gpu=<id>] [--view=<dset>]   python run_train.py (-h | --help)   python run_train.py --version ```  Options: ```   -h --help       Show this string.   --version       Show version.   --gpu=<id>      Comma separated GPU list.     --view=<dset>   Visualise images after augmentation. Choose 'train' or 'valid'. ```  Examples:  To visualise the training dataset as a sanity check before training use: ``` python run_train.py --view='train' ```  To initialise the training script with GPUs 0 and 1  the command is: ``` python run_train.py --gpu='0 1'  ```   Usage: <br /> ```   run_infer.py [options] [--help] <command> [<args>...]   run_infer.py --version   run_infer.py (-h | --help) ```  Options: ```   -h --help                   Show this string.   --version                   Show version.    --gpu=<id>                  GPU list. [default: 0]   --nr_types=<n>              Number of nuclei types to predict. [default: 0]   --type_info_path=<path>     Path to a json define mapping between type id  type name                                 and expected overlay color. [default: '']    --model_path=<path>         Path to saved checkpoint.   --model_mode=<mode>         Original HoVer-Net or the reduced version used in PanNuke / MoNuSAC  'original' or 'fast'. [default: fast]   --nr_inference_workers=<n>  Number of workers during inference. [default: 8]   --nr_post_proc_workers=<n>  Number of workers during post-processing. [default: 16]   --batch_size=<n>            Batch size. [default: 128] ```  Tile Processing Options: <br /> ```    --input_dir=<path>     Path to input data directory. Assumes the files are not nested within directory.    --output_dir=<path>    Path to output directory..     --draw_dot             To draw nuclei centroid on overlay. [default: False]    --save_qupath          To optionally output QuPath v0.2.3 compatible format. [default: False]    --save_raw_map         To save raw prediction or not. [default: False] ```  WSI Processing Options: <br /> ```     --input_dir=<path>      Path to input data directory. Assumes the files are not nested within directory.     --output_dir=<path>     Path to output directory.     --cache_path=<path>     Path for cache. Should be placed on SSD with at least 100GB. [default: cache]     --mask_dir=<path>       Path to directory containing tissue masks.                              Should have the same name as corresponding WSIs. [default: '']      --proc_mag=<n>          Magnification level (objective power) used for WSI processing. [default: 40]     --ambiguous_size=<int>  Define ambiguous region along tiling grid to perform re-post processing. [default: 128]     --chunk_shape=<n>       Shape of chunk for processing. [default: 10000]     --tile_shape=<n>        Shape of tiles for processing. [default: 2048]     --save_thumb            To save thumb. [default: False]     --save_mask             To save mask. [default: False] ```  The above command can be used from the command line or via an executable script. We supply two example executable scripts: one for tile processing and one for WSI processing. To run the scripts  first make them executable by using `chmod +x run_tile.sh` and `chmod +x run_tile.sh`. Then run by using `./run_tile.sh` and `./run_wsi.sh`.  Intermediate results are stored in cache. Therefore ensure that the specified cache location has enough space! Preferably ensure that the cache location is SSD.  Note  it is important to select the correct model mode when running inference. 'original' model mode refers to the method described in the original medical image analysis paper with a 270x270 patch input and 80x80 patch output. 'fast' model mode uses a 256x256 patch input and 164x164 patch output. Model checkpoints trained on Kumar  CPM17 and CoNSeP are from our original publication and therefore the 'original' mode **must** be used. For PanNuke and MoNuSAC  the 'fast' mode **must** be selected. The model mode for each checkpoint that we provide is given in the filename. Also  if using a model trained only for segmentation  `nr_types` must be set to 0.  `type_info.json` is used to specify what RGB colours are used in the overlay. Make sure to modify this for different datasets and if you would like to generally control overlay boundary colours.  As part of our tile processing implementation  we add an option to save the output in a form compatible with QuPath.   Take a look on how to utilise the output in `examples/usage.ipynb`.    """;Computer Vision;https://github.com/vqdang/hover_net
"""In this work  we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN including only the most essential component in GCN‚Äîneighborhood aggregation‚Äîfor collaborative filtering     * Add Cpp Extension in  code/sources/  for negative sampling. To use the extension  please install pybind11 and cppimport under your environment   pytorch version results (stop at 1000 epochs):   run LightGCN on **Gowalla** dataset:  * change base directory  Change `ROOT_PATH` in `code/world.py`  * command  ` cd code && python main.py --decay=1e-4 --lr=0.001 --layer=3 --seed=2020 --dataset=""gowalla"" --topks=""[20]"" --recdim=64`  * log output  ```shell ... ====================== EPOCH[5/1000] BPR[sample time][16.2=15.84+0.42] [saved][[BPR[aver loss1.128e-01]] [0;30;43m[TEST][0m {'precision': array([0.03315359])  'recall': array([0.10711388])  'ndcg': array([0.08940792])} [TOTAL TIME] 35.9975962638855 ... ====================== EPOCH[116/1000] BPR[sample time][16.9=16.60+0.45] [saved][[BPR[aver loss2.056e-02]] [TOTAL TIME] 30.99874997138977 ... ```  *NOTE*:  1. Even though we offer the code to split user-item matrix for matrix multiplication  we strongly suggest you don't enable it since it will extremely slow down the training speed. 2. If you feel the test process is slow  try to increase the ` testbatch` and enable `multicore`(Windows system may encounter problems with `multicore` option enabled) 3. Use `tensorboard` option  it's good. 4. Since we fix the seed(`--seed=2020` ) of `numpy` and `torch` in the beginning  if you run the command as we do above  you should have the exact output log despite the running time (check your output of *epoch 5* and *epoch 116*).    """;Graphs;https://github.com/gusye1234/LightGCN-PyTorch
"""Training GAN is hard. Models may never converge and mode collapses are common. <br>  When learning generative models  we assume the data we have comes from some unknown distribution <img src='./readme_images/pr.png' />. (The r stands for real) We want to learn a distribution <img src='./readme_images/ptheta.png' />‚Äã‚Äã that approximates <img src='./readme_images/pr.png' />  where Œ∏ are the parameters of the distribution. <br> You can imagine two approaches for doing this. <br> - Directly learn the probability density function <img src='./readme_images/ptheta.png' />‚Äã‚Äã. We optimize <img src='./readme_images/ptheta.png' />‚Äã‚Äã through maximum likelihood estimation.  - Learn a function that transforms an existing distribution Z into <img src='./readme_images/ptheta.png' />‚Äã‚Äã.   The first approach runs into problems. Given function <img src='./readme_images/ptheta.png' />‚Äã‚Äã‚Äã‚Äã  the MLE objective is <br> <img src='./readme_images/eqn13.png' />‚Äã‚Äã <br> In the limit  this is equivalent to minimizing the KL-divergence. <br> <img src='./readme_images/eqn14.png' />‚Äã‚Äã <br> <img src='./readme_images/eqn15.png' />‚Äã‚Äã <br>  Variational Auto-Encoders (VAEs) and Generative Adversarial Networks (GANs) are well known examples of this approach.   """;Computer Vision;https://github.com/Mohammad-Rahmdel/WassersteinGAN-Tensorflow
"""Please refer to [INSTALL.md](docs/INSTALL.md) for the installation of `OpenPCDet`.    * Support lastest PyTorch 1.1~1.10 and spconv 1.0~2.x  where spconv 2.x should be easy to install with pip and faster than previous version (see the official update of spconv here).   so you could also play with WOD by setting a smaller DATA_CONFIG.SAMPLED_INTERVAL even if you only have limited GPU resources.    Please refer to [DEMO.md](docs/DEMO.md) for a quick demo to test with a pretrained model and  visualize the predicted results on your custom data or the original KITTI data.   Please refer to [GETTING_STARTED.md](docs/GETTING_STARTED.md) to learn more usage about this project.    """;Computer Vision;https://github.com/open-mmlab/OpenPCDet
"""To train SIG and GIS  run the following commands:   """;Computer Vision;https://github.com/biweidai/SINF
