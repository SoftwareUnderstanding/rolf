{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.05952"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9325182562707431
      ],
      "excerpt": "    beta_rate (float): Rate (0 to 1) for increasing beta to 1 as per Schauel et al. (https://arxiv.org/abs/1511.05952) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/snhwang/p3_collab-compet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-02T08:45:50Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-02T10:53:22Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9225986093322734,
        0.9499762948676401
      ],
      "excerpt": "This project was one of the requirements for completing the Deep Reinforcement Learning Nanodegree (DRLND) course at Udacity.com. \n\u200b   This project uitlizes the Tennis environment. In this environment, two agents play tennis with each other. Each agent is represented by a racket and they try to learn to hit a ball back and forth between each other across a net. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9943514637499181,
        0.997703384068781,
        0.9027848376464269,
        0.9249573740071321
      ],
      "excerpt": "During one episode of play, an agent earns a reward of +0.1 every time it hits the ball over the net. A negative reward of -0.01 is given if the ball hits the ground or goes out of bounds. Ideally, the two agents should learn how to keep the ball in play to earn a high total reward. \n\u200b   The introduction to the course project indicates the state space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. On examination of the environment, it indicates that it's state size is 24 for each agent, so there must be other parameters in the state space. \n\u200b   The action space consists of two possible continuous actions, corresponding to movement towards (or away from) the net and jumping. \nThe environment is episodic. Each agent earns a score in one episode and the episode is then characterized by the maximum score between the agents. The maximum score between the two agents during one episode is average over 100 consecutive episodes. This 100-episode average of the maximum agent score must exceed +0.5 in order for the environment to be considered solved. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8063210365442327,
        0.9340374756884892
      ],
      "excerpt": "To train the the agent(s) with the provided parameters, just \"run all\" under the Cell drop down menu of the Jupyter notebook.  \nThe parameters of the learning agent can be changed in Section 4 of the notebook. The parameters for running the simulation and training the agent can be modified in Section 5. The parameters are described below. During training, multiple checkpoints are saved for running the trained agent later: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.885011431229347
      ],
      "excerpt": "checkpoint_actor_best_avg_max.pth and checkpoint_critic_best_avg_max.pth: The actor and critic network weights for the model that achieve the highest 100-episode average of the maximum episode score.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.974457182520471
      ],
      "excerpt": "The default is 'RELU' if not specified and I did not get 'SELU' to work. I recommend just using 'RELU,' or not specifying so that it just always uses the default. The name of the checkpoint can be changed in Section 4 of the notebook. The following examples shows how to run the agents using the checkpoint files for the neural networks which achieved the highest maximum agent score in a single episode. which a agent through 100 episodes and provide scores as well as the final average score. The final parameter is the number of episodes to run and can also be changed: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9087772838050556
      ],
      "excerpt": "Tennis-SNH.ipynb: Jupyter notebook to train the agent(s) and to save the trained the neural network weights as checkpoints. This notebook is set up for version 2 with multiple agents. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.955413554779079
      ],
      "excerpt": "These parameters and the implementation are discussed more in the file Report.md. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9177674984672025,
        0.939452146173442
      ],
      "excerpt": "update_every: The number of time steps between each updating of the neural networks  \nnum_updates: The number of times to update the networks at every update_every interval \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9203431506843246
      ],
      "excerpt": "network (string): The name of the neural networks that are used for learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8350360121109714
      ],
      "excerpt": "    Default is \"RELU.\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8966822276720685
      ],
      "excerpt": "    epsilon_initial (float): Initial value of epsilon for epsilon-greedy selection of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8730153461966612
      ],
      "excerpt": "        Higher is faster decay. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8726031645600245
      ],
      "excerpt": "    beta_initial (float): For prioritized replay. Corrects bias induced by weighted \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.891455462061632
      ],
      "excerpt": "        unless  prioritized experience replay is used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481044516665399
      ],
      "excerpt": "        targets instead of soft updating. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Project 3 of the Udacity Deep Reinforcement Learning Nanodegree",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "http://ipython.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Download the environment from one of the links below.  You need only select the environment that matches your operating system:\n\n- **_Version 1: One (1) Agent_**\n  - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Linux.zip)\n  - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher.app.zip)\n  - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Windows_x86.zip)\n  - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Windows_x86_64.zip)\n- **_Version 2: Twenty (20) Agents_**\n  - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux.zip)\n  - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher.app.zip)\n  - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86.zip)\n  - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86_64.zip)\n\nUnzip (or decompress) the file which provides a folder.  Copy folder into the folder p3_collab-compet-SNH. The Jupyter notebook for running the code is called `Tennis-SNH.ipynb`. The folder name indicated in Section1 of the notebook for starting the environment must match one of the folder you copied the environment into.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/snhwang/p3_collab-compet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 05 Dec 2021 07:30:24 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/snhwang/p3_collab-compet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "snhwang/p3_collab-compet",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/snhwang/p3_collab-compet/master/Tennis-SNH-pretrained.ipynb",
      "https://raw.githubusercontent.com/snhwang/p3_collab-compet/master/Tennis-SNH.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment:\n\n```\npython -m ipykernel install --user --name drlnd --display-name \"drlnd\"\n```\n\nThese steps only need to be performed once. \n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\u200b\tThe instructions at https://github.com/udacity/deep-reinforcement-learning indicate that you should enter the following on the command line to clone the the repository and install the dependencies:\n\n```\ngit clone https://github.com/udacity/deep-reinforcement-learning.git\ncd deep-reinforcement-learning/python\npip install .\n```\n\nHowever, for Windows 10, this did not work for me. The pip command fails when it tries to install torch 0.4.0. This version may no longer be available. I edited the dependencies shown in the requirements.txt file in the directory and changed the line for torch from\n\n `torch==0.4.0` to `torch==0.4.1`. \n\nThe pip command worked after the change. Otherwise you can install the required packages in the requirements folder manually. Sometimes these software packages change and you may need to refer to the specific instructions for an individual package. For example, https://pytorch.org/get-started/locally/ may be helpful for installing PyTorch. \n\nIf you clone the DRLND repository, the original files from the project can be found in the folder deep-reinforcement-learning/p3_collab-compet\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- **Linux** or **Mac**:\n\n  In a terminal window, perform the following commands:\n\n```\nconda create --name drlnd python=3.6\nsource activate drlnd\n```\n\n- **Windows**:\n\n  Make sure you are using the anaconda command line rather than the usual windows cmd.exe. \n\n```\nconda create --name drlnd python=3.6 \nactivate drlnd\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\u200b\tThe installation of the software is accomplished with the package manager, conda. Installing Anaconda (https://www.anaconda.com/) will include conda as well as facilitate the installation of other data science software packages. The Jupyter Notebook App is also required for running this project and is installed automatically with Anaconda.\n\n \tThe dependencies for this project can be installed by following the instructions at https://github.com/udacity/deep-reinforcement-learning#dependencies.  Required components include but are are not limited to Python 3.6 (I specifically used 3.6.6), and PyTorch v0.4, and a version of the Unity ML-Agents toolkit. Note that ML-Agents are only supported on Microsoft Windows 10. I only used Windows 10, so cannot vouch for the accuracy of the instructions for other operating systems.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8129053173698598,
        0.9890348583315558,
        0.869220493838264,
        0.9554169436001461,
        0.9172361032193227,
        0.8712279230539444,
        0.9116099320754565,
        0.8820234121644812,
        0.8142695800724485,
        0.9167203705163228
      ],
      "excerpt": "The github repository is https://github.com/snhwang/p3-collab-compet-SNH.git. \nIn a terminal window, specifically an Anaconda terminal window for Microsoft Windows, activate the conda environment if not already done: \nLinux or Mac: \nsource activate drlnd \nMake sure you are using the anaconda command line rather than the usual windows cmd.exe.  \nactivate drlnd \nChange directory to the p1_navigate_SNH folder. Run Jupyter Notebook: \njupyter notebook \nOpen the notebook Tennis-SNH.ipynb to train with the multi-agent environment. Before running code in a notebook, change the kernel to match the drlnd environment by using the drop-down Kernelmenu: \n(taken from the Udacity instructions) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8416647351223449
      ],
      "excerpt": "Change directory to the p1_navigate_SNH folder. Run Jupyter Notebook: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8224627008955988
      ],
      "excerpt": "checkpoint_actor.pth and checkpoint_critic.pth: The first time the agents achieve a 100-episode-average maximum score of >0.5. Keep in mind, that the agent's neural networks were changing during training during those 100 episodes. After training, a run of 100 episodes without any training can be performed using one of the checkpoints to see how well it performs.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8110457067478052
      ],
      "excerpt": "buffer_size: Buffer size for experience replay. Default 2e6. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8102192919040947
      ],
      "excerpt": "    tau_initial (float): Initial value for tau, the weighting factor for soft updating \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/snhwang/p3_collab-compet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "HTML",
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Project 2: Continuous Control",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "p3_collab-compet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "snhwang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/snhwang/p3_collab-compet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\u200b\tThe instructions at https://github.com/udacity/deep-reinforcement-learning indicate that you should enter the following on the command line to clone the the repository and install the dependencies:\n\n```\ngit clone https://github.com/udacity/deep-reinforcement-learning.git\ncd deep-reinforcement-learning/python\npip install .\n```\n\nHowever, for Windows 10, this did not work for me. The pip command fails when it tries to install torch 0.4.0. This version may no longer be available. I edited the dependencies shown in the requirements.txt file in the directory and changed the line for torch from\n\n `torch==0.4.0` to `torch==0.4.1`. \n\nThe pip command worked after the change. Otherwise you can install the required packages in the requirements folder manually. Sometimes these software packages change and you may need to refer to the specific instructions for an individual package. For example, https://pytorch.org/get-started/locally/ may be helpful for installing PyTorch. \n\nIf you clone the DRLND repository, the original files from the project can be found in the folder deep-reinforcement-learning/p3_collab-compet\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment:\n\n```\npython -m ipykernel install --user --name drlnd --display-name \"drlnd\"\n```\n\nThese steps only need to be performed once. \n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 05 Dec 2021 07:30:24 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\u200b\tThe installation of the software is accomplished with the package manager, conda. Installing Anaconda (https://www.anaconda.com/) will include conda as well as facilitate the installation of other data science software packages. The Jupyter Notebook App is also required for running this project and is installed automatically with Anaconda.\n\n \tThe dependencies for this project can be installed by following the instructions at https://github.com/udacity/deep-reinforcement-learning#dependencies.  Required components include but are are not limited to Python 3.6 (I specifically used 3.6.6), and PyTorch v0.4, and a version of the Unity ML-Agents toolkit. Note that ML-Agents are only supported on Microsoft Windows 10. I only used Windows 10, so cannot vouch for the accuracy of the instructions for other operating systems.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment:\n\n```\npython -m ipykernel install --user --name drlnd --display-name \"drlnd\"\n```\n\nThese steps only need to be performed once. \n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}