{
  "citation": [
    {
      "confidence": [
        0.8944178096468923,
        0.9554441738822752
      ],
      "excerpt": "  none'>#total pages</td> \n  <td class=xl66 style='border-top:none;border-left:none'>#sem pages</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554441738822752,
        0.9554441738822752
      ],
      "excerpt": "  <td class=xl66 style='border-top:none;border-left:none'>#total pages</td> \n  <td class=xl66 style='border-top:none;border-left:none'>#sem pages</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906174419333412
      ],
      "excerpt": "  <td class=xl67 align=right style='border-top:none;border-left:none'>0.12</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "  <td class=xl67 align=right style='border-top:none;border-left:none'>0.32</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "  <td class=xl66 align=right style='border-top:none;border-left:none'>11</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.976240086309536
      ],
      "excerpt": "Hellman et al. [2]; https://www.assembla.com/spaces/commondata/subversion/source/HEAD/extractorNutch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998970783524171,
        0.9908796903780731
      ],
      "excerpt": "[1]. Meusel, Robert, Peter Mika, and Roi Blanco. \"Focused Crawling for Structured Data.\" Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. ACM, 2014. \n[2]. Hellmann, Sebastian, et al. \"Knowledge Base Creation, Enrichment and Repair.\" Linked Open Data--Creating Knowledge Out of Interlinked Data. Springer International Publishing, 2014. 45-69. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/YahooArchive/anthelion",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2015-10-27T00:42:01Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-07T09:41:39Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9936040965115267,
        0.9726538374166586,
        0.8793207751493706,
        0.9968029537584643
      ],
      "excerpt": "Anthelion is a Nutch plugin for focused crawling of semantic data. \nThe project is an open-source project released under the Apache License 2.0. \nNote: This project contains the complete Nutch 1.6 distribution. The plugin itself can be found in /src/plugin/parse-anth \nTable of Contents \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9850593677657423,
        0.8785496437839839,
        0.986893173220416,
        0.9922438601682737
      ],
      "excerpt": "The plugin uses an online learning approach to predict data-rich web pages based on the context of the page as well as using feedback from the extraction of metadata from previously seen pages [1]. \nTo perform the focused crawling the plugin implements three extensions: \nAnthelionScoringFilter (implements the ScoringFilter interface): wraps around the Anthelion online classifier to classify newly discovered outlinks, as relevant or not. This extension gives score to each outlink, which is then used in the Generate stage, i.e., the URLs for the next fetch cycle are selected based on the score. This extension also pushes feedback to the classifier for the already parsed web pages. The online classifier can be configured and tuned (see Usage and Development). \nWdcParser (implements the Parser interface): This extension parses the web page content and tries to extract semantic data. The parser is adaptation of an already existing Nutch parser plugin implemented in [2]. The parser is based on the any23 library and is able to extract Microdata, Microformats and RDFa annotation from HTML. The extracted triples are stored in the Content field. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.96524852178682
      ],
      "excerpt": "An overview of the complete crawling process using the Anthelion plugin is given in the following figure. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9687483227302245,
        0.9182506955805886
      ],
      "excerpt": "In order to evaluate the focused crawler we measure the precision of the crawled pages, i.e., the ratio of the number of crawled web pages that contain semantic data and the total number of crawled web pages. \nSo far, we have evaluated using three different seeds sample, and several different configurations. An overview is given in the following table. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9068757214452807
      ],
      "excerpt": "The pairwise comparison is given in the following chart: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9276269251488765,
        0.9129599301323724
      ],
      "excerpt": "Here we summarize the tools used, their purpose, and the licenses under which they're released. \nThis project includes the sources of Apache Nutch 1.6 (Apache License 2.0 - http://www.apache.org/licenses/LICENSE-2.0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8709360900398824
      ],
      "excerpt": "Used for parsing the crawled web pages \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8303565654660905
      ],
      "excerpt": "For the libraries and tools used in Anthelion, please check the Anthelion [README file] (https://github.com/yahoo/anthelion/blob/master/anthelion/README.md).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9272923913550547
      ],
      "excerpt": "For more details about the Anthelion project please check the Anthelion [README file] (https://github.com/yahoo/anthelion/blob/master/anthelion/README.md). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Anthelion is a plugin for Apache Nutch to crawl semantic annotations within HTML pages.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yahoo/anthelion/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 704,
      "date": "Mon, 20 Dec 2021 20:21:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/YahooArchive/anthelion/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "YahooArchive/anthelion",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/yahoo/anthelion/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8219817549150983,
        0.8002641752693935
      ],
      "excerpt": " <tr height=15 style='height:15.0pt'> \n  <td rowspan=2 height=30 class=xl65 width=65 style='height:30.0pt;width:65pt'>#seeds</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8219817549150983,
        0.8383366974549007
      ],
      "excerpt": " <tr height=15 style='height:15.0pt'> \n  <td height=15 class=xl66 style='height:15.0pt;border-top:none;border-left: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8219817549150983,
        0.827601411354893
      ],
      "excerpt": " <tr height=15 style='height:15.0pt'> \n  <td height=15 class=xl66 align=right style='height:15.0pt;border-top:none'>2</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8219817549150983,
        0.847890471910126
      ],
      "excerpt": " <tr height=15 style='height:15.0pt'> \n  <td height=15 class=xl66 align=right style='height:15.0pt;border-top:none'>10</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8219817549150983,
        0.827601411354893
      ],
      "excerpt": " <tr height=15 style='height:15.0pt'> \n  <td height=15 class=xl66 align=right style='height:15.0pt;border-top:none'>1000</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8219817549150983,
        0.827601411354893
      ],
      "excerpt": " <tr height=15 style='height:15.0pt'> \n  <td height=15 class=xl66 align=right style='height:15.0pt;border-top:none'>1000</td> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/YahooArchive/anthelion/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Java",
      "HTML",
      "Shell",
      "XSLT"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "nutch-anth",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "anthelion",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "YahooArchive",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/YahooArchive/anthelion/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2875,
      "date": "Mon, 20 Dec 2021 20:21:06 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "As mentioned in the beginning of the document this project contains the complete Nutch 1.6 code, including the plugin. If you download the complete project, there is no need for any changes and settings. If you want to download only the plugin, please download only the nutch-anth.zip from the root of the folder and go to step 2 of the configuration. If you want to contribute to the plugin and/or want to use the sources with another version of Nutch, please follow the following instructions:\n\n1. Download and copy the /src/plugin/parse-anth folder into your Nutch's plugins directory.\n\n2. Enable the plugin in conf/nutch-site.xml by adding *parse-anth* in the *plugin.includes* property.\n\n3. Copy the properties from nutch-anth.xml to conf/nutch-site.xml.\n\n\t3.1. Download the baseline.properties file and set the property *anth.scoring.classifier.PropsFilePath* conf/nutch-site.xml to point to the file. This file contains all configurations for the online classifier.\n\n4. In order for ant to compile and deploy the plugin you need to edit the src/plugin/build.xml, by adding the following line in the *deploy* target:\n\t```xml\n\t<ant dir=\"parse-anth\" target=\"deploy\"/>\n\t```\n5. Add the following lines in conf/parse-plugins.xml:\n\t```xml\n\t<mimeType name=\"text/html\">\n\t\t\t<plugin id=\"parse-anth\" />\n\t\t</mimeType>\n\t\n\t        <mimeType name=\"application/xhtml+xml\">\n\t\t\t<plugin id=\"parse-anth\" />\n\t\t</mimeType>\n\t```\n6. Add the following line in the *alias* property in conf/parse-plugins.xml:\n\t\n\t```xml\n\t<alias name=\"parse-anth\" extension-id=\"com.yahoo.research.parsing.WdcParser\" />\n\t```\n7. Copy the *lib* folder into the root of the Nutch distribution.\n\n8. Run `mvn package` inside the *anthelion* folder. This will create the jar \"Anthelion-1.0.0-jar-with-dependencies.jar\". Copy the jar to src/plugin/parse-anth/lib.\n\n9. Add the following field in conf/schema.xml (also add it to the Solr schema.xml, if you are using Solr):\n\t```xml\n\t<field name=\"containsSem\" type=\"text_general\" stored=\"true\" indexed=\"true\"/>\n\t```\n10. Run ant in the root of your folder.\n\n",
      "technique": "Header extraction"
    }
  ]
}