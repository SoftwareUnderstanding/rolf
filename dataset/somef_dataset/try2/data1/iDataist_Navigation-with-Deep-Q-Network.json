{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.05952\n4. Wang, Schaul, et al. \"Dueling Network Architectures for Deep Reinforcement Learning.\" 2015. https://arxiv.org/abs/1511.06581\n\n",
      "https://arxiv.org/abs/1511.06581\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[image1]: https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif \"Trained Agent\"\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/iDataist/Navigation-with-Deep-Q-Network",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-11T01:54:43Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-07T21:53:36Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. [requirements.txt](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/requirements.txt) - Includes all the required libraries for the Conda Environment.\n2. [model.py](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/model.py) - Defines the QNetwork which is the nonlinear function approximator to calculate the value actions based directly on observation from the environment.\n3. [dqn_agent.py](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/dqn_agent.py) -  Defines the Agent that uses Deep Learning to find the optimal parameters for the function approximators, determines the best action to take and maximizes the overall or total reward.\n4. [Navigation.ipynb](https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/Navigation.ipynb) - The main file that trains the Deep Q-Network and shows the trained agent in action. This file can be run in the Conda environment.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9555428005455339,
        0.9790854969012777
      ],
      "excerpt": "In this project, I trained an agent to navigate and collect bananas in a large, square world. \nUnity Machine Learning Agents (ML-Agents) is an open-source Unity plugin that enables games and simulations to serve as environments for training intelligent agents. The gif below shows the environment for this project. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9892200161992917,
        0.9769036443022715
      ],
      "excerpt": "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana.  Thus, the goal of the agent is to collect as many yellow bananas as possible while avoiding blue bananas. \nThe state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  Given this information, the agent has to learn how to best select actions.  Four discrete actions are available, corresponding to: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9872784369298436,
        0.8195822652330542,
        0.9573645096329255
      ],
      "excerpt": "- Reinforcement Learning (RL) is a branch of Machine Learning, where an agent outputs an action and the environment returns an observation (the state of the system) and a reward. The goal of an agent is to determine the best action to take and maximizes the overall or total reward. \n- Value-based Deep RL uses nonlinear function approximators (Deep Neural Network) to calculate the value actions based directly on observation from the environment. Deep Learning can be used to find the optimal parameters for these function approximators. \n- I created a ReplayBuffer Class to enable experience replay&lt;sup&gt;1, 2&lt;/sup&gt;. Using the replay pool, the behavior distribution is averaged over many of its previous states, smoothing out learning and avoiding oscillations. The advantage is that each step of the experience is potentially used in many weight updates. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9160361972414905,
        0.9637326854959563,
        0.9116718613112708
      ],
      "excerpt": "The environment was solved in 463 episodes, with the average reward score of 13 to indicate solving the environment. \nPrioritized Experience Replay<sup>3</sup>: I have adopted experience replay in the DQN. But some of these experiences may be more important for learning than others. Moreover, these important experiences might occur infrequently. If we sample the batches uniformly, then these experiences have a very small chance of getting selected. Since buffers are practically limited in capacity, older important experiences may get lost. I will implement prioritized experience replay<sup>4</sup> will help to optimize the selection of experiences. \nDueling Networks<sup>4</sup>: Dueling networks use two streams, one that estimates the state value function and one that estimates the advantage for each action.These streams may share some layers in the beginning, then branch off with their own fully-connected layers. The desired Q values are obtained by combining the state and advantage values. The value of most states don't vary a lot across actions. So, it makes sense to try and directly estimate them. But we still need to capture the difference actions make in each state. This is where the advantage function comes in. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/iDataist/Navigation-with-Deep-Q-Network/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 02:12:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/iDataist/Navigation-with-Deep-Q-Network/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "iDataist/Navigation-with-Deep-Q-Network",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/iDataist/Navigation-with-Deep-Q-Network/main/Navigation.ipynb",
      "https://raw.githubusercontent.com/iDataist/Navigation-with-Deep-Q-Network/main/.ipynb_checkpoints/Navigation-checkpoint.ipynb",
      "https://raw.githubusercontent.com/iDataist/Navigation-with-Deep-Q-Network/main/.ipynb_checkpoints/Navigation-Copy1-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    - I adopted Double Deep Q-Network structure<sup>1, 2</sup> with three fully connected layers. If a single network is used, the Q-functions values change at each step of training, and then the value estimates can quickly spiral out of control. I used a target network to represent the old Q-function, which is used to compute the loss of every action during training.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8516088754469572
      ],
      "excerpt": "The environment was solved in 463 episodes, with the average reward score of 13 to indicate solving the environment. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/iDataist/Navigation-with-Deep-Q-Network/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "ASP.NET",
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) Udacity\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "(Image References)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Navigation-with-Deep-Q-Network",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "iDataist",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/iDataist/Navigation-with-Deep-Q-Network/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sun, 26 Dec 2021 02:12:13 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Create the Conda Environment\n\n    a. Install [`miniconda`](http://conda.pydata.org/miniconda.html) on your computer, by selecting the latest Python version for your operating system. If you already have `conda` or `miniconda` installed, you should be able to skip this step and move on to step b.\n\n    **Download** the latest version of `miniconda` that matches your system.\n\n    |        | Linux | Mac | Windows |\n    |--------|-------|-----|---------|\n    | 64-bit | [64-bit (bash installer)][lin64] | [64-bit (bash installer)][mac64] | [64-bit (exe installer)][win64]\n    | 32-bit | [32-bit (bash installer)][lin32] |  | [32-bit (exe installer)][win32]\n\n    [win64]: https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86_64.exe\n    [win32]: https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86.exe\n    [mac64]: https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\n    [lin64]: https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n    [lin32]: https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86.sh\n\n    **Install** [miniconda](http://conda.pydata.org/miniconda.html) on your machine. Detailed instructions:\n\n    - **Linux:** http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install\n    - **Mac:** http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install\n    - **Windows:** http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install\n\n    b. Install git and clone the repository.\n\n    For working with Github from a terminal window, you can download git with the command:\n    ```\n    conda install git\n    ```\n    To clone the repository, run the following command:\n    ```\n    cd PATH_OF_DIRECTORY\n    git clone https://github.com/iDataist/Navigation-with-Deep-Q-Network\n    ```\n    c. Create local environment\n\n    - Create (and activate) a new environment, named `dqn-env` with Python 3.7. If prompted to proceed with the install `(Proceed [y]/n)` type y.\n\n        - __Linux__ or __Mac__:\n        ```\n        conda create -n dqn-env python=3.7\n        conda activate dqn-env\n        ```\n        - __Windows__:\n        ```\n        conda create --name dqn-env python=3.7\n        conda activate dqn-env\n        ```\n\n        At this point your command line should look something like: `(dqn-env) <User>:USER_DIR <user>$`. The `(dqn-env)` indicates that your environment has been activated, and you can proceed with further package installations.\n\n    - Install a few required pip packages, which are specified in the requirements text file. Be sure to run the command from the project root directory since the requirements.txt file is there.\n        ```\n        pip install -r requirements.txt\n        ipython3 kernel install --name dqn-env --user\n        ```\n    - Open Jupyter Notebook, and open the Navigation.ipynb file.\n        ```\n        jupyter notebook\n        ```\n2. Download the Unity Environment\n\n   Download the environment from one of the links below.  You need only select the environment that matches your operating system:\n    - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip)\n    - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip)\n    - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip)\n    - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)\n\n    (_For Windows users_) Check out [this link](https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64) if you need help with determining if your computer is running a 32-bit version or 64-bit version of the Windows operating system.\n\n    (_For AWS_) If you'd like to train the agent on AWS (and have not [enabled a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md)), then please use [this link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux_NoVis.zip) to obtain the environment.\n\n",
      "technique": "Header extraction"
    }
  ]
}