{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1609.02907",
      "https://arxiv.org/abs/1706.02216",
      "https://arxiv.org/abs/1710.10903",
      "https://arxiv.org/abs/1809.00130",
      "https://arxiv.org/abs/1905.05460",
      "https://arxiv.org/abs/1905.01669",
      "https://arxiv.org/abs/1806.02623",
      "https://arxiv.org/abs/1609.02907",
      "https://arxiv.org/abs/1710.10903",
      "https://arxiv.org/abs/1801.10247",
      "https://arxiv.org/abs/1907.02237"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find *CogDL* is useful for your research, please consider citing the following papers:\n```\n@inproceedings{Fey/Lenssen/2019,\n  title={Fast Graph Representation Learning with {PyTorch Geometric}},\n  author={Fey, Matthias and Lenssen, Jan E.},\n  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},\n  year={2019},\n}\n\n@inproceedings{perozzi2014deepwalk,\n  title={Deepwalk: Online learning of social representations},\n  author={Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},\n  booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},\n  pages={701--710},\n  year={2014},\n  organization={ACM}\n}\n\n@inproceedings{tang2015line,\n  title={Line: Large-scale information network embedding},\n  author={Tang, Jian and Qu, Meng and Wang, Mingzhe and Zhang, Ming and Yan, Jun and Mei, Qiaozhu},\n  booktitle={Proceedings of the 24th International Conference on World Wide Web},\n  pages={1067--1077},\n  year={2015},\n  organization={ACM}\n}\n\n@inproceedings{grover2016node2vec,\n  title={node2vec: Scalable feature learning for networks},\n  author={Grover, Aditya and Leskovec, Jure},\n  booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\n  pages={855--864},\n  year={2016},\n  organization={ACM}\n}\n\n@inproceedings{cao2015grarep,\n  title={Grarep: Learning graph representations with global structural information},\n  author={Cao, Shaosheng and Lu, Wei and Xu, Qiongkai},\n  booktitle={Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},\n  pages={891--900},\n  year={2015},\n  organization={ACM}\n}\n\n@inproceedings{Ou2016Asymmetric,\n  title={Asymmetric Transitivity Preserving Graph Embedding},\n  author={Ou, Mingdong and Cui, Peng and Pei, Jian and Zhang, Ziwei and Zhu, Wenwu},\n  booktitle={The  ACM SIGKDD International Conference},\n  pages={1105-1114},\n  year={2016},\n}\n\n@inproceedings{qiu2018network,\n  title={Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec},\n  author={Qiu, Jiezhong and Dong, Yuxiao and Ma, Hao and Li, Jian and Wang, Kuansan and Tang, Jie},\n  booktitle={Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},\n  pages={459--467},\n  year={2018},\n  organization={ACM}\n}\n\n@inproceedings{qiu2019netsmf,\n  title={NetSMF: Large-Scale Network Embedding as Sparse Matrix Factorization},\n  author={Qiu, Jiezhong and Dong, Yuxiao and Ma, Hao and Li, Jian and Wang, Chi and Wang, Kuansan and Tang, Jie},\n  booktitle={The World Wide Web Conference},\n  pages={1509--1520},\n  year={2019},\n  organization={ACM}\n}\n\n@article{zhang2018spectral,\n  title={Spectral Network Embedding: A Fast and Scalable Method via Sparsity},\n  author={Zhang, Jie and Wang, Yan and Tang, Jie},\n  journal={arXiv preprint arXiv:1806.02623},\n  year={2018}\n}\n\n@article{kipf2016semi,\n\ttitle={Semi-supervised classification with graph convolutional networks},\n\tauthor={Kipf, Thomas N and Welling, Max},\n\tjournal={arXiv:1609.02907},\n\tyear={2016}\n}\n\n@article{Velickovic:17GAT,\n\tauthor    = {Petar Velickovic and\n\tGuillem Cucurull and\n\tArantxa Casanova and\n\tAdriana Romero and\n\tPietro Li{\\`{o}} and\n\tYoshua Bengio},\n\ttitle     = {Graph Attention Networks},\n\tjournal   = {arXiv:1710.10903},\n\tyear      = {2017},\n}\n\n@inproceedings{hamilton2017inductive,\n\ttitle={Inductive representation learning on large graphs},\n\tauthor={Hamilton, Will and Ying, Zhitao and Leskovec, Jure},\n\tbooktitle={NIPS'17},\n\tpages={1025--1035},\n\tyear={2017}\n}\n\n@article{chen2018fastgcn,\n\ttitle={FastGCN: fast learning with graph convolutional networks via importance sampling},\n\tauthor={Chen, Jie and Ma, Tengfei and Xiao, Cao},\n\tjournal={arXiv:1801.10247},\n\tyear={2018}\n}\n\n@article{zou2019dimensional,\n\ttitle={Dimensional Reweighting Graph Convolutional Networks},\n\tauthor={Zou, Xu and Jia, Qiuye and Zhang, Jianwei and Zhou, Chang and Yang, Hongxia and Tang, Jie},\n\tjournal={arXiv preprint arXiv:1907.02237},\n\tyear={2019}\n}\n\n```\n<!--\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zou2019dimensional,\n    title={Dimensional Reweighting Graph Convolutional Networks},\n    author={Zou, Xu and Jia, Qiuye and Zhang, Jianwei and Zhou, Chang and Yang, Hongxia and Tang, Jie},\n    journal={arXiv preprint arXiv:1907.02237},\n    year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{chen2018fastgcn,\n    title={FastGCN: fast learning with graph convolutional networks via importance sampling},\n    author={Chen, Jie and Ma, Tengfei and Xiao, Cao},\n    journal={arXiv:1801.10247},\n    year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{hamilton2017inductive,\n    title={Inductive representation learning on large graphs},\n    author={Hamilton, Will and Ying, Zhitao and Leskovec, Jure},\n    booktitle={NIPS'17},\n    pages={1025--1035},\n    year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Velickovic:17GAT,\n    author    = {Petar Velickovic and\n    Guillem Cucurull and\n    Arantxa Casanova and\n    Adriana Romero and\n    Pietro Li{`{o}} and\n    Yoshua Bengio},\n    title     = {Graph Attention Networks},\n    journal   = {arXiv:1710.10903},\n    year      = {2017},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{kipf2016semi,\n    title={Semi-supervised classification with graph convolutional networks},\n    author={Kipf, Thomas N and Welling, Max},\n    journal={arXiv:1609.02907},\n    year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhang2018spectral,\n  title={Spectral Network Embedding: A Fast and Scalable Method via Sparsity},\n  author={Zhang, Jie and Wang, Yan and Tang, Jie},\n  journal={arXiv preprint arXiv:1806.02623},\n  year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{qiu2019netsmf,\n  title={NetSMF: Large-Scale Network Embedding as Sparse Matrix Factorization},\n  author={Qiu, Jiezhong and Dong, Yuxiao and Ma, Hao and Li, Jian and Wang, Chi and Wang, Kuansan and Tang, Jie},\n  booktitle={The World Wide Web Conference},\n  pages={1509--1520},\n  year={2019},\n  organization={ACM}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{qiu2018network,\n  title={Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec},\n  author={Qiu, Jiezhong and Dong, Yuxiao and Ma, Hao and Li, Jian and Wang, Kuansan and Tang, Jie},\n  booktitle={Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},\n  pages={459--467},\n  year={2018},\n  organization={ACM}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{Ou2016Asymmetric,\n  title={Asymmetric Transitivity Preserving Graph Embedding},\n  author={Ou, Mingdong and Cui, Peng and Pei, Jian and Zhang, Ziwei and Zhu, Wenwu},\n  booktitle={The  ACM SIGKDD International Conference},\n  pages={1105-1114},\n  year={2016},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{cao2015grarep,\n  title={Grarep: Learning graph representations with global structural information},\n  author={Cao, Shaosheng and Lu, Wei and Xu, Qiongkai},\n  booktitle={Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},\n  pages={891--900},\n  year={2015},\n  organization={ACM}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{grover2016node2vec,\n  title={node2vec: Scalable feature learning for networks},\n  author={Grover, Aditya and Leskovec, Jure},\n  booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\n  pages={855--864},\n  year={2016},\n  organization={ACM}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{tang2015line,\n  title={Line: Large-scale information network embedding},\n  author={Tang, Jian and Qu, Meng and Wang, Mingzhe and Zhang, Ming and Yan, Jun and Mei, Qiaozhu},\n  booktitle={Proceedings of the 24th International Conference on World Wide Web},\n  pages={1067--1077},\n  year={2015},\n  organization={ACM}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{perozzi2014deepwalk,\n  title={Deepwalk: Online learning of social representations},\n  author={Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},\n  booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},\n  pages={701--710},\n  year={2014},\n  organization={ACM}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{Fey/Lenssen/2019,\n  title={Fast Graph Representation Learning with {PyTorch Geometric}},\n  author={Fey, Matthias and Lenssen, Jan E.},\n  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},\n  year={2019},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9748040881275956,
        0.842086723209586,
        0.9409537240827404
      ],
      "excerpt": "Perozzi et al. (2014): Deepwalk: Online learning of social representations \nTang et al. (2015): Line: Large-scale informa- tion network embedding \nGrover and Leskovec. (2016): node2vec: Scalable feature learning for networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8953967942217171,
        0.9244828239290314
      ],
      "excerpt": "Ou et al. (2016): Asymmetric transitivity preserving graph em- bedding \nQiu et al. (2017): Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9992038792692737
      ],
      "excerpt": "Zhang et al. (2019): Spectral Network Embedding: A Fast and Scalable Method via Sparsity \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968632307650429,
        0.9874849040232603,
        0.9989927189173851,
        0.969954047509243,
        0.9964316885842295,
        0.9006009944770582
      ],
      "excerpt": "Hamilton et al. (2017): Inductive Representation Learning on Large Graphs \nVeli\u010dkovi\u0107 et al. (2017): Graph Attention Networks \nNew! CIKM 2018 Ding et al. (2018): Semi-supervised Learning on Graphs with Generative Adversarial Nets \nNew Han et al. (2019): GroupRep: Unsupervised Structural Representation Learning for Groups in Networks \nNew Zhang et al. (2019): Revisiting Graph Convolutional Networks: Neighborhood Aggregation and Network Sampling \nNew Zhang et al. (2019): Co-training Graph Convolutional Networks with Network Redundancy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998988387194209
      ],
      "excerpt": "New! IJCAI 2019 Zhang et al. (2019): ProNE: Fast and Scalable Network Representation Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.99435051842229
      ],
      "excerpt": "New! ACL 2019 Ding et al. (2019): Cognitive Graph for Multi-Hop Reading Comprehension at Scale \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9969239209639227
      ],
      "excerpt": "New! KDD 2019 Cen et al. (2019): Representation Learning for Attributed Multiplex Heterogeneous Network \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9775223411068394
      ],
      "excerpt": "New! IJCAI 2019 Zhao et al. (2019): Large Scale Evolving Graphs with Burst Detection \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zxhhh97/ABot",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-23T01:16:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-07T17:06:41Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9770605124066899,
        0.8930901044020226,
        0.9503594243669721,
        0.9330587134134407
      ],
      "excerpt": "CogDL is a graph representation learning toolkit that allows researchers and developers to easily train and compare baseline or custom models for node classification, link prediction and other tasks on graphs. It provides implementations of many popular models, including: non-GNN Baselines like Deepwalk, LINE, NetMF,  GNN Baselines like GCN, GAT, GraphSAGE. \nCogDL features: \nSparsification: fast network embedding on large-scale networks with tens of millions of nodes \nArbitrary: dealing with different graph strucutures: attributed, multiplex and heterogeneous networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9069705047827991,
        0.8160399844520364
      ],
      "excerpt": "Extensible: easily register new datasets, models, criterions and tasks \n--task, Downsteam tasks to evaluate representation like node_classification, unsupervised_node_classification, link_prediction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978027802044922
      ],
      "excerpt": "for DeepWalk and node2vec: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8158152404758665
      ],
      "excerpt": "- --walk-length, Length of walk start at each node. Default is 50; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8659145744056659
      ],
      "excerpt": "- --window-size, Window size of skip-gram model. Default is 10; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9196130948353296,
        0.9196130948353296
      ],
      "excerpt": "- --q, Parameter in node2vec. Default is 1.0; \n- --p, Parameter in node2vec. Default is 1.0; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8307619419334493
      ],
      "excerpt": "- --alpha, Initial earning rate of SGD. Default is 0.025; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095427408726869
      ],
      "excerpt": "- --negative, Number of negative node in sampling. Default is 5; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9681086934503496,
        0.9394449182630016,
        0.8815993122879207,
        0.9394449182630016
      ],
      "excerpt": "- --beta, Parameter of katz for HOPE. Default is 0.01; \nfor Grarep: \n- --step, Number of matrix step in GraRep and ProNE. Default is 5; \nfor NetMF: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9272222998050552,
        0.9095427408726869,
        0.915155158481337,
        0.9394449182630016
      ],
      "excerpt": "- --is-large, Large or small for NetMF; \n- --negative, Number of negative node in sampling. Default is 5; \n- --rank, Number of Eigenpairs in NetMF, default is 256; \nfor NetSMF: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095427408726869,
        0.8499141350783344
      ],
      "excerpt": "- --negative, Number of negative node in sampling. Default is 5; \n- --round, Number of round in NetSMF. Default is 100; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9286708101352581,
        0.9556888967046722,
        0.9556888967046722,
        0.978027802044922
      ],
      "excerpt": "- --step, Number of items in the chebyshev expansion. Default is 5; \n- --theta, Parameter of ProNE. Default is 0.5; \n- --mu, Parameter of ProNE. Default is 0.2; \nfor GCN and DR-GCN: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978027802044922
      ],
      "excerpt": "for GAT and DR-GAT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "for Graphsage: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9514577666764522,
        0.9201648473115546,
        0.8138697811220386
      ],
      "excerpt": "If you have a well-perform algorithm and are willing to public it, you can submit your implementation via opening an issue or join our slack group. After evaluating its originality, creativity and efficiency, we will add your method's performance into our leaderboard. \nIf you have a unique and interesting and are willing to public it, you can submit your dataset via opening an issue in our repository or commenting on slack group, we will run all suitable methods on your dataset and update our leaderboard. \nIf you have a well-perform algorithm and are willing to implement it in our toolkit to help more people, you can create a pull request,  detail information can be found here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8108656450817829
      ],
      "excerpt": "New Han et al. (2019): GroupRep: Unsupervised Structural Representation Learning for Groups in Networks \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zxhhh97/ABot/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Thu, 30 Dec 2021 03:02:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/zxhhh97/ABot/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "zxhhh97/ABot",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/zxhhh97/ABot/master/Debug.ipynb",
      "https://raw.githubusercontent.com/zxhhh97/ABot/master/API/Predict.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- PyTorch version >= 1.0.0\n- Python version >= 3.6\n\nPlease follow the instructions here to install PyTorch and other dependencies: https://github.com/pytorch/pytorch#installation, https://github.com/rusty1s/pytorch_geometric/#installation\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8853446714167591
      ],
      "excerpt": "For example, if you want to run GCN on Cora with node classification task , you should use following operation: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8186049246948932
      ],
      "excerpt": "If you want to run parallel experiments on your server with multiple GPUs like multiple models gcn, gat on multiple datasets Cora, Citeseer with node classification task, you should use following operation: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8117672144873463
      ],
      "excerpt": "If you have ANY difficulties to get things working in the above steps, feel free to open an issue. You can expect a reply within 24 hours. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9369836482498836
      ],
      "excerpt": "$ python train.py --task node_classification --dataset cora --model gcn \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9255786428639698
      ],
      "excerpt": "$ python train.py --task node_classification --dataset cora citeseer --model gcn gat --device-id 0 1 2 3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8201310054593968
      ],
      "excerpt": "- --walk-num, the number of random walks to start at each node; the default is 10; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8283739620733388
      ],
      "excerpt": "- --batch-size, Batch size in SGD training process. Default is 100; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.804445265428105,
        0.8303750407087301
      ],
      "excerpt": "- --hidden-size, The size of hidden layer. Default=16; \n- --num-layers, The number of GCN layer. Default=2; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8197456072002501
      ],
      "excerpt": "- --sample-size, The List of number of neighbor samples for each node in Graphsage. Default=10, 10; \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/zxhhh97/ABot/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Qibin Chen\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "CogDL",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ABot",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "zxhhh97",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zxhhh97/ABot/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- PyTorch version >= 1.0.0\n- Python version >= 3.6\n\nPlease follow the instructions here to install PyTorch and other dependencies: https://github.com/pytorch/pytorch#installation, https://github.com/rusty1s/pytorch_geometric/#installation\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Thu, 30 Dec 2021 03:02:29 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can use `python train.py --task example_task --dataset example_dataset --model example_method` to run example_method on example_data and evaluate it via example_task.\n\n",
      "technique": "Header extraction"
    }
  ]
}