{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1804.10959",
      "https://arxiv.org/abs/1804.10959",
      "https://arxiv.org/abs/1804.10959",
      "https://arxiv.org/abs/1910.13267",
      "https://arxiv.org/abs/1804.10959",
      "https://arxiv.org/abs/1804.10959",
      "https://arxiv.org/abs/1910.13267"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9334202873386838
      ],
      "excerpt": "subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/google/sentencepiece",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Want to contribute? Great! First, read this page (including the small print at the end).\nBefore you contribute\nBefore we can use your code, you must sign the\nGoogle Individual Contributor License Agreement\n(CLA), which you can do online. The CLA is necessary mainly because you own the\ncopyright to your changes even after your contribution becomes part of our\ncodebase, so we need your permission to use and distribute your code. We also\nneed to be sure of various other things\u2014for instance, that you'll tell us if you\nknow that your code infringes on other people's patents. You don't have to sign\nthe CLA until after you've submitted your code for review and a member has\napproved it, but you must do it before we can put your code into our codebase.\nBefore you start working on a larger contribution, you should get in touch with\nus first through the issue tracker with your idea so that we can help out and\npossibly guide you. Coordinating up-front makes it much easier to avoid\nfrustration later on.\nCode reviews\nAll submissions, including submissions by project members, require review. We\nuse Github pull requests for this purpose.\nThe small print\nContributions made by corporations are covered by a different agreement than\nthe one above, the Software Grant and Corporate Contributor License Agreement.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-03-07T10:03:48Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T04:33:43Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9613228156128432
      ],
      "excerpt": "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9593057906299017
      ],
      "excerpt": "is predetermined prior to the neural model training. SentencePiece implements \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9106050150634234,
        0.963223279240027,
        0.9570283842826875,
        0.8213898602040274
      ],
      "excerpt": "unigram language model [Kudo.]) \nwith the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing. \nThis is not an official Google product. \nPurely data driven: SentencePiece trains tokenization and detokenization \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9612393015127212,
        0.869076188274171,
        0.9801149241036725
      ],
      "excerpt": "Language independent: SentencePiece treats the sentences just as sequences of Unicode characters. There is no language-dependent logic. \nMultiple subword algorithms: BPE  [Sennrich et al.] and unigram language model [Kudo.] are supported. \nSubword regularization: SentencePiece implements subword sampling for subword regularization and BPE-dropout which help to improve the robustness and accuracy of NMT models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8570371137988008
      ],
      "excerpt": "Self-contained: The same tokenization/detokenization is obtained as long as the same model file is used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8434752303618664
      ],
      "excerpt": "For those unfamiliar with SentencePiece as a software/algorithm, one can read a gentle introduction here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9111075737924987,
        0.959014453690365,
        0.9175759324519379
      ],
      "excerpt": "Note that BPE algorithm used in WordPiece is slightly different from the original BPE. \nSentencePiece is a re-implementation of sub-word units, an effective way to alleviate the open vocabulary \n  problems in neural machine translation. SentencePiece supports two segmentation algorithms, byte-pair-encoding (BPE) [Sennrich et al.] and unigram language model [Kudo.]. Here are the high level differences from other implementations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8297870794878889,
        0.921906028351103,
        0.9051072378825961,
        0.8390769249569314,
        0.9602516668099382,
        0.9110338511942152,
        0.958661784453656,
        0.9427198540830855
      ],
      "excerpt": "assume an infinite vocabulary, SentencePiece trains the segmentation model such \nthat the final vocabulary size is fixed, e.g., 8k, 16k, or 32k. \nNote that SentencePiece specifies the final vocabulary size for training, which is different from \nsubword-nmt that uses the number of merge operations. \nThe number of merge operations is a BPE-specific parameter and not applicable to other segmentation algorithms, including unigram, word and character. \nPrevious sub-word implementations assume that the input sentences are pre-tokenized. This constraint was required for efficient training, but makes the preprocessing complicated as we have to run language dependent tokenizers in advance. \nThe implementation of SentencePiece is fast enough to train the model from raw sentences. This is useful for training the tokenizer and detokenizer for Chinese and Japanese where no explicit spaces exist between words. \nThe first step of Natural Language processing is text tokenization. For \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8914352125789499,
        0.9606233778232257
      ],
      "excerpt": "One observation is that the original input and tokenized sequence are NOT \nreversibly convertible. For instance, the information that is no space between \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.937657710292822
      ],
      "excerpt": "SentencePiece treats the input text just as a sequence of Unicode characters. Whitespace is also handled as a normal symbol. To handle the whitespace as a basic token explicitly, SentencePiece first escapes the whitespace with a meta symbol \"\u2581\" (U+2581) as follows. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9570702857571073
      ],
      "excerpt": "Since the whitespace is preserved in the segmented text, we can detokenize the text without any ambiguities. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639117573155299,
        0.8352516423883767,
        0.8231600123431251
      ],
      "excerpt": "This feature makes it possible to perform detokenization without relying on language-specific resources. \nNote that we cannot apply the same lossless conversions when splitting the \nsentence with standard word segmenters, since they treat the whitespace as a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8695698106429259
      ],
      "excerpt": "(ja) \u3053\u3093\u306b\u3061\u306f\u4e16\u754c\u3002  \u2192 [\u3053\u3093\u306b\u3061\u306f] [\u4e16\u754c] [\u3002] (No space between \u3053\u3093\u306b\u3061\u306f and \u4e16\u754c) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9816364253038098
      ],
      "excerpt": "that virtually augment training data with on-the-fly subword sampling, which helps to improve the accuracy as well as robustness of NMT models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8638927777920314
      ],
      "excerpt": "(C++/Python) into the NMT system to sample one segmentation for each parameter update, which is different from the standard off-line data preparations. Here's the example of Python library. You can find that 'New York' is segmented differently on each SampleEncode (C++) or encode with enable_sampling=True (Python) calls. The details of sampling parameters are found in sentencepiece_processor.h. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "s = spm.SentencePieceProcessor(model_file='spm.model') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9727691148460018
      ],
      "excerpt": "* --character_coverage: amount of characters covered by the model, good defaults are: 0.9995 for languages with rich character set like Japanese or Chinese and 1.0 for other languages with small character set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.934436459397268
      ],
      "excerpt": "Use --help flag to display all parameters for training, or see here for an overview. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8404012333870805
      ],
      "excerpt": "When setting -1 id e.g., bos_id=-1, this special token is disabled. Note that the unknow id cannot be disabled.  We can define an id for padding (&lt;pad&gt;) as --pad_id=3. \u00a0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9897254487371785,
        0.9288698161557342
      ],
      "excerpt": "spm_encode accepts a --vocabulary and a --vocabulary_threshold option so that spm_encode will only produce symbols which also appear in the vocabulary (with at least some frequency). The background of this feature is described in subword-nmt page. \nThe usage is basically the same as that of subword-nmt. Assuming that L1 and L2 are the two languages (source/target languages), train the shared spm model, and get resulting vocabulary for each: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8244811873107655,
        0.8244811873107655
      ],
      "excerpt": "% spm_encode --model=spm.model --vocabulary={vocab_file}.L1 --vocabulary_threshold=50 &lt; {test_file}.L1 &gt; {test_file}.seg.L1 \n% spm_encode --model=spm.model --vocabulary={vocab_file}.L2 --vocabulary_threshold=50 &lt; {test_file}.L2 &gt; {test_file}.seg.L2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Unsupervised text tokenizer for Neural Network-based text generation.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/google/sentencepiece/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 767,
      "date": "Wed, 29 Dec 2021 09:18:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/google/sentencepiece/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "google/sentencepiece",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/google/sentencepiece/master/python/add_new_vocab.ipynb",
      "https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_python_module_example.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/google/sentencepiece/master/test.sh",
      "https://raw.githubusercontent.com/google/sentencepiece/master/python/make_py_wheel_mac.sh",
      "https://raw.githubusercontent.com/google/sentencepiece/master/python/make_py_wheel.sh",
      "https://raw.githubusercontent.com/google/sentencepiece/master/python/build_bundled.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can download and install sentencepiece using the [vcpkg](https://github.com/Microsoft/vcpkg) dependency manager:\n\n    git clone https://github.com/Microsoft/vcpkg.git\n    cd vcpkg\n    ./bootstrap-vcpkg.sh\n    ./vcpkg integrate install\n    ./vcpkg install sentencepiece\n\nThe sentencepiece port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The following tools and libraries are required to build SentencePiece:\n\n* [cmake](https://cmake.org/)\n* C++11 compiler\n* [gperftools](https://github.com/gperftools/gperftools) library (optional, 10-40% performance improvement can be obtained.)\n\nOn Ubuntu, the build tools can be installed with apt-get:\n```\n% sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev\n```\n\nThen, you can build and install command line tools as follows.\n```\n% git clone https://github.com/google/sentencepiece.git \n% cd sentencepiece\n% mkdir build\n% cd build\n% cmake ..\n% make -j $(nproc)\n% sudo make install\n% sudo ldconfig -v\n```\nOn OSX/macOS, replace the last command with `sudo update_dyld_shared_cache`\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9880134483351434,
        0.999746712887969
      ],
      "excerpt": "You can install Python binary package of SentencePiece with. \n% pip install sentencepiece \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8481421161266481
      ],
      "excerpt": "SentencePiece treats the input text just as a sequence of Unicode characters. Whitespace is also handled as a normal symbol. To handle the whitespace as a basic token explicitly, SentencePiece first escapes the whitespace with a meta symbol \"\u2581\" (U+2581) as follows. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.838731791265562
      ],
      "excerpt": "[Hello] [\u2581Wor] [ld] [.] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9012248701992861
      ],
      "excerpt": "import sentencepiece as spm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9066770435424679
      ],
      "excerpt": "* --input: one-sentence-per-line raw corpus file. No need to run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207406275993623
      ],
      "excerpt": "* --model_type: model type. Choose from unigram (default), bpe, char, or word. The input sentence must be pretokenized when using word type. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8602157090787687
      ],
      "excerpt": "% spm_train --input=train --model_prefix=spm --vocab_size=8000 --character_coverage=0.9995 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.85072036011545
      ],
      "excerpt": "Then segment train/test corpus with --vocabulary option \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/google/sentencepiece/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Jupyter Notebook",
      "Python",
      "SWIG",
      "CMake",
      "Perl",
      "Shell",
      "Batchfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'This is the esaxx copyright.\\n\\nCopyright (c) 2010 Daisuke Okanohara All Rights Reserved.\\n\\nPermission is hereby granted, free of charge, to any person\\nobtaining a copy of this software and associated documentation\\nfiles (the \"Software\"), to deal in the Software without\\nrestriction, including without limitation the rights to use,\\ncopy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the\\nSoftware is furnished to do so, subject to the following\\nconditions:\\n\\nThe above copyright notice and this permission notice shall be\\nincluded in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\\nWHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\\nOTHER DEALINGS IN THE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SentencePiece",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sentencepiece",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "google",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/google/sentencepiece/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "## Updates\r\n* Improves the performance of unigram training\r\n* Updated the nfkc normalization with the latest ICU module.\r\n* Stop handling zero-width-joiner string as whitespace.\r\n\r\n## New features\r\n* added new sampling algorithm without replacement.\r\n* added API for new sampling and perplexity calculation.\r\n* added ``allow_whitespace_only_pieces`` mode.\r\n\r\n",
        "dateCreated": "2021-06-17T16:10:09Z",
        "datePublished": "2021-06-17T16:55:39Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.96",
        "name": "v0.1.96",
        "tag_name": "v0.1.96",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.96",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/44812531",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.96"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "##  Updates\r\n- support to build sentencepiece with the external (official) abseil library.\r\n- upgraded protobuf 3.14.0\r\n- changed the type of input_sentence_size from int32 to uint64.",
        "dateCreated": "2021-01-10T05:49:13Z",
        "datePublished": "2021-01-10T06:02:51Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.95",
        "name": "v0.1.95",
        "tag_name": "v0.1.95",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.95",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/36189535",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.95"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "##  Updates\r\n- added SetRandomGeneratorSeed function to set the seed value for random generator. This can allow to make reproducible sampling.\r\n- Validate the range of the vocab id in Python module.\r\n- Change the directory arrangement of python module.\r\n- Added protobuf python module.\r\n\r\n## Bug fixes\r\n- Support to build python wheel from source package.",
        "dateCreated": "2020-10-24T01:12:46Z",
        "datePublished": "2020-10-24T02:01:03Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.94",
        "name": "v0.1.94",
        "tag_name": "v0.1.94",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.94",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/33003991",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.94"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "## Bug fix\r\n- Fixed the regression bug around the flag --minloglevel\r\n- Fixed minor bugs.\r\n\r\n##  Updates\r\n- Used manylinux2014 to build pypi packages\r\n- Support arm64, ppc64le, s390x architectures in pypi packages\r\n- Support Python 3.9\r\n\r\n## Removed\r\n- Stopped tf-sentencepiece.\r\n- Stopped the support of Python 2.x and Python 3.4\r\n",
        "dateCreated": "2020-10-14T02:57:36Z",
        "datePublished": "2020-10-14T04:38:14Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.93",
        "name": "v0.1.93",
        "tag_name": "v0.1.93",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.93",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/32543327",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.93"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "## Bug fix\r\n- Fixed the regression bug around the flag --minloglevel\r\n- Fixed build break on Solaris.\r\n\r\n## Minor upgrade\r\n- upgrade builtin protobuf to 3.12.3\r\n- Implmeneted absl::flags port.\r\n",
        "dateCreated": "2020-06-08T08:25:01Z",
        "datePublished": "2020-06-08T09:05:45Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.92",
        "name": "v0.1.92",
        "tag_name": "v0.1.92",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.92",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/27317142",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.92"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "## New API\r\n- [Python] Added a feature to feed training data as Python's iterable object.\r\nhttps://github.com/google/sentencepiece/tree/master/python#training-without-local-filesystem\r\n- [Python] Added a feature to set model writer to emit the output model to any non-local devices. \r\nhttps://github.com/google/sentencepiece/tree/master/python#training-without-local-filesystem\r\n- [C++] Add an API to returns the trained model directly as std::string.\r\n\r\n## Bug Fix\r\n- Ignores nbest parameter in BPE-dropout\r\n- fixed build error when SPM_ENABLE_NFKC_COMPILE=ON\r\n- fixed the cost computation around user_defined_symbol and faster encoding introduced in the previous release.\r\n",
        "dateCreated": "2020-05-21T03:12:53Z",
        "datePublished": "2020-05-21T03:25:09Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.91",
        "name": "v0.1.91",
        "tag_name": "v0.1.91",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.91",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/26743534",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.91"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "Renamed v0.1.9 to v0.1.90 because PyPI doesn't recognize 0.1.9 as the latest release.",
        "dateCreated": "2020-05-13T06:10:48Z",
        "datePublished": "2020-05-13T06:20:09Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.90",
        "name": "v0.1.90",
        "tag_name": "v0.1.90",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.90",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/26453418",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.90"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "## Features:\r\n- **--byte_fallback:** fallback UNK token into UTF-8 byte sequences. 256 byte symbols are reserved in advance.\r\nhttps://arxiv.org/pdf/1909.03341.pdf Note that  you need to set --character_coverage less than 1.0, otherwise byte-fall-backed token may not appear in the training data.\r\n- **BPE-dropout:** Implemented BPE dropout. https://arxiv.org/abs/1910.13267 \r\nSampling API is available for the BPE.\r\nhttps://github.com/google/sentencepiece/blob/master/src/sentencepiece_processor.h#L287\r\n- **--required_chars=chars:** Specify the set of Unicode chars that must be included in the final vocab.\r\n- **--split_digits:** Split all digits (0-9) into separate pieces (disabled by default)\r\n- **Denormalization:** Apply extra normalization rule after decoding. We can specify the rule as TSV via --denormalization_rule_tsv=file flag. Note that offset information may not always be preserved.\r\n- -**-train_extremely_large_corpus:** Train the unigram model from extremely large corpus (> 10M sentences) to avoid integer overflow. Note that it will increase the memory usage. 300GB or larger memory might be necessary.\r\n\r\n\r\n## Performance improvement:\r\n- 30%-50% performance improvement is obtained in the default unigram one-best tokenization.\r\n\r\n## New API\r\n- [Python] Added Python friendly API. New API allows to feed any chars to user_defined_symbols during the training. The old methods are still available.\r\nhttps://github.com/google/sentencepiece/tree/master/python#segmentation\r\n- [C++] Added the interface to feed training data via arbitrary iterator object. \r\nhttps://github.com/google/sentencepiece/blob/master/src/sentencepiece_trainer.h#L40\r\n- [C++] Added the interface to set set a pre-tokenizer to specify the word boundary. This is used as a word-boundary constraint to set the seed vocabulary, and not used in the inference time.\r\nhttps://github.com/google/sentencepiece/blob/master/src/pretokenizer_for_training.h\r\n",
        "dateCreated": "2020-05-12T18:06:12Z",
        "datePublished": "2020-05-13T02:52:05Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.9",
        "name": "v0.1.9",
        "tag_name": "v0.1.9",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.9",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/26449879",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.9"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "- Support tf 1.5.1 2.0.0 2.0.1 2.1.0 and 2.2.0rc3\r\n- Added python wrapper for Python3.8 on Mac",
        "dateCreated": "2020-04-24T09:15:14Z",
        "datePublished": "2020-04-24T09:34:11Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.86",
        "name": "v0.1.86",
        "tag_name": "v0.1.86",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.86",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/25843040",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.86"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "Support tf 1.15 and Python3.8 on Windows",
        "dateCreated": "2019-12-15T14:52:31Z",
        "datePublished": "2019-12-15T15:39:09Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.85",
        "name": "v0.1.85",
        "tag_name": "v0.1.85",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.85",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/22238399",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.85"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "- Support tf 2.0.0",
        "dateCreated": "2019-10-12T08:24:36Z",
        "datePublished": "2019-10-12T09:01:47Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.84",
        "name": "v0.1.84",
        "tag_name": "v0.1.84",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.84",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/20657593",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.84"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "- Use the official docker image to build tf_sentencepiece ops\r\n- support tf 1.14.0 and tf 2.0.0-beta1.",
        "dateCreated": "2019-08-16T14:38:19Z",
        "datePublished": "2019-08-16T15:09:27Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.83",
        "name": "v0.1.83",
        "tag_name": "v0.1.83",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.83",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/19341506",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.83"
      },
      {
        "authorType": "User",
        "author_name": "fozziethebeat",
        "body": "Releases a new version of Sentencepiece with major refactorings:\r\n\r\n  * Builds with Bazel\r\n  * Re-uses existing open source libraries whenever possible\r\n  * Refactors internal dependencies\r\n  * New sets of features for configuring tokenizers\r\n  * Separation from Tensorflow",
        "dateCreated": "2019-06-11T05:59:45Z",
        "datePublished": "2019-06-24T02:55:05Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/1.0.0",
        "name": "Sentencepiece re-release",
        "tag_name": "1.0.0",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/1.0.0",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/18170843",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/1.0.0"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "Bug fix:  fixed the behavior of is_unknown method in Python module.",
        "dateCreated": "2019-04-13T16:21:50Z",
        "datePublished": "2019-04-13T16:36:00Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.82",
        "name": "v0.1.82",
        "tag_name": "v0.1.82",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.82",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/16744096",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.82"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "Fix: support tensorflow 0.13.1",
        "dateCreated": "2019-03-22T16:06:31Z",
        "datePublished": "2019-03-22T16:39:57Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.81",
        "name": "v0.1.81",
        "tag_name": "v0.1.81",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.81",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/16289619",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.81"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "Feature: Get rid of the dependency to external protobuf\r\nFeature: added (Encode|Decode)AsSerializedProto interface so Python module can get full access to the SentencePieceText proto including the byte offsets/aligments\r\nFeature: added --treat_whitespace_as_suffix option to make _ be a suffix of word.\r\nFeature: Added normalization rules to remove control characters in the default nmt_* normalizers\r\nMinor fix: simplify the error messager\r\nMinor fix:  do not emit full source path in LOG(INFO)\r\n\r\nFor more detail: https://github.com/google/sentencepiece/compare/v0.1.7...v0.1.8",
        "dateCreated": "2019-01-10T08:16:53Z",
        "datePublished": "2019-01-10T08:49:45Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.8",
        "name": "v0.1.8",
        "tag_name": "v0.1.8",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.8",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/14897440",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.8"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "Deprecated: `--mining_sentence_size` and `--training_sentence_size`. Load all sentences by default. `--input_sentence_size` can be specified to limit the sentences to be loaded\r\nFeature: added `--unk_piece/--bos_piece/--eos_piece/--pad_piece` flags to change the surface representations of these special symbols.\r\nBug fix: added third_party directory for cmake's subdirectory. \r\n\r\nFor more detail: \r\nhttps://github.com/google/sentencepiece/compare/v0.1.6...v0.1.7",
        "dateCreated": "2018-12-25T02:38:07Z",
        "datePublished": "2018-12-25T06:08:17Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.7",
        "name": "v0.1.7",
        "tag_name": "v0.1.7",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.7",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/14690156",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.7"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "SentencePiece Windows release",
        "dateCreated": "2018-11-11T13:05:29Z",
        "datePublished": "2018-11-11T15:39:36Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.6pre1",
        "name": "v0.1.6pre1",
        "tag_name": "v0.1.6pre1",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.6pre1",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/13940688",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.6pre1"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "* Bug fix: do not apply normalization to the user-defined-symbols.\r\n* Bug fix: stop adding extra whitespaces before user-defined symbols\r\n* Feature: added --minloglevel flag to suppress LOG(INFO) message\r\n* Feature: added --split_by_number flag to allow numbers to attach other symbols.\r\n* Feature: added --max_sentence_length flag to control the maximum byte length of input sentence for training.\r\n* used tf-versioned so file for _sentencepiece_processor_ops to minimize ABI incompatibility for tf wapper.\r\n\r\nFor more detail: https://github.com/google/sentencepiece/compare/v0.1.5...master",
        "dateCreated": "2018-11-11T13:05:29Z",
        "datePublished": "2018-11-11T15:39:03Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.6",
        "name": "v0.1.6",
        "tag_name": "v0.1.6",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.6",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/13940686",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.6"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "",
        "dateCreated": "2018-10-28T16:29:21Z",
        "datePublished": "2018-10-28T17:03:56Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.5",
        "name": "v0.1.5",
        "tag_name": "v0.1.5",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.5",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/13697393",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.5"
      },
      {
        "authorType": "User",
        "author_name": "taku910",
        "body": "Initial SentencePiece releases",
        "dateCreated": "2018-08-26T08:37:39Z",
        "datePublished": "2018-08-26T15:31:28Z",
        "html_url": "https://github.com/google/sentencepiece/releases/tag/v0.1.4",
        "name": "v0.1.4",
        "tag_name": "v0.1.4",
        "tarball_url": "https://api.github.com/repos/google/sentencepiece/tarball/v0.1.4",
        "url": "https://api.github.com/repos/google/sentencepiece/releases/12579781",
        "zipball_url": "https://api.github.com/repos/google/sentencepiece/zipball/v0.1.4"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5563,
      "date": "Wed, 29 Dec 2021 09:18:04 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "neural-machine-translation",
      "natural-language-processing",
      "word-segmentation"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n% spm_train --input=data/botchan.txt --model_prefix=m --vocab_size=1000\nunigram_model_trainer.cc(494) LOG(INFO) Starts training with :\ninput: \"../data/botchan.txt\"\n... <snip>\nunigram_model_trainer.cc(529) LOG(INFO) EM sub_iter=1 size=1100 obj=10.4973 num_tokens=37630 num_tokens/piece=34.2091\ntrainer_interface.cc(272) LOG(INFO) Saving model: m.model\ntrainer_interface.cc(281) LOG(INFO) Saving vocabs: m.vocab\n\n% echo \"I saw a girl with a telescope.\" | spm_encode --model=m.model\n\u2581I \u2581saw \u2581a \u2581girl \u2581with \u2581a \u2581 te le s c o pe .\n\n% echo \"I saw a girl with a telescope.\" | spm_encode --model=m.model --output_format=id\n9 459 11 939 44 11 4 142 82 8 28 21 132 6\n\n% echo \"9 459 11 939 44 11 4 142 82 8 28 21 132 6\" | spm_decode --model=m.model --input_format=id\nI saw a girl with a telescope.\n```\nYou can find that the original input sentence is restored from the vocabulary id sequence.\n\n",
      "technique": "Header extraction"
    }
  ]
}