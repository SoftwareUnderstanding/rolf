{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We are thankful to developers of these tools for inspiring this implementation.\n\n- [pycma](https://github.com/CMA-ES/pycma)\n- [evolution-strategies-starter](https://github.com/openai/evolution-strategies-starter)\n- [estool](https://github.com/hardmaru/estool)\n- [ARS](https://github.com/modestyachts/ARS)\n\n[Back to Contents](#contents)\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2008.02387",
      "https://arxiv.org/abs/2008.02387",
      "https://arxiv.org/abs/1703.03864",
      "https://arxiv.org/abs/1412.6980",
      "https://arxiv.org/abs/1803.07055",
      "https://arxiv.org/abs/1703.03864.\n\n`[4]`\nHa, D. (2017). [A Visual Guide to Evolution Strategies](https://blog.otoro.net/2017/10/29/visual-evolution-strategies/).\n\n`[5]`\nKingma, D.P., & Ba, J. (2015). [Adam: A method for stochastic optimization](https://arxiv.org/abs/1412.6980).\nIn Proceedings of 3rd International Conference on Learning Representations (ICLR 2015).\n\n`[6]`\nHansen, N., Akimoto, Y., Baudis, P. (2019). [CMA-ES/pycma on Github](https://github.com/CMA-ES/pycma). Zenodo, DOI:10.5281/zenodo.2559634, February 2019.\n\n`[7]`\nMania, H., Guy, A., & Recht, B. (2018). [Simple random search provides a competitive approach to reinforcement learning](https://arxiv.org/abs/1803.07055) \narXiv preprint https://arxiv.org/abs/1803.07055. \n\n[Back to Contents](#contents)\n\n## Citation\n\nIf you use this code, please cite us in your repository/paper as:\n \n```\nToklu, N. E., Liskowski, P., & Srivastava, R. K. (2020, September). ClipUp: A Simple and Powerful Optimizer for Distribution-Based Policy Evolution. In International Conference on Parallel Problem Solving from Nature (pp. 515-527). Springer, Cham.\n```\n\nBibtex:\n```\n@inproceedings{toklu2020clipup,\n  title={ClipUp: A Simple and Powerful Optimizer for Distribution-Based Policy Evolution",
      "https://arxiv.org/abs/1803.07055. \n\n[Back to Contents](#contents)\n\n## Citation\n\nIf you use this code, please cite us in your repository/paper as:\n \n```\nToklu, N. E., Liskowski, P., & Srivastava, R. K. (2020, September). ClipUp: A Simple and Powerful Optimizer for Distribution-Based Policy Evolution. In International Conference on Parallel Problem Solving from Nature (pp. 515-527). Springer, Cham.\n```\n\nBibtex:\n```\n@inproceedings{toklu2020clipup,\n  title={ClipUp: A Simple and Powerful Optimizer for Distribution-Based Policy Evolution"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code, please cite us in your repository/paper as:\n \n```\nToklu, N. E., Liskowski, P., & Srivastava, R. K. (2020, September). ClipUp: A Simple and Powerful Optimizer for Distribution-Based Policy Evolution. In International Conference on Parallel Problem Solving from Nature (pp. 515-527). Springer, Cham.\n```\n\nBibtex:\n```\n@inproceedings{toklu2020clipup,\n  title={ClipUp: A Simple and Powerful Optimizer for Distribution-Based Policy Evolution},\n  author={Toklu, Nihat Engin and Liskowski, Pawe{\\l} and Srivastava, Rupesh Kumar},\n  booktitle={International Conference on Parallel Problem Solving from Nature},\n  pages={515--527},\n  year={2020},\n  organization={Springer}\n}\n```\n\n[Back to Contents](#contents)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "`[1]`\nSehnke, F., Osendorfer, C., R\u00fcckstie\u00df, T., Graves, A., Peters, J., & Schmidhuber, J. (2010).\n[Parameter-exploring policy gradients](http://people.idsia.ch/~juergen/nn2010.pdf). \nNeural Networks, 23(4), 551-559.\n\n`[2]`\nToklu, N.E., Liskowski, P., & Srivastava, R.K. (2020).\n[ClipUp: A Simple and Powerful Optimizer for Distribution-based Policy Evolution](https://arxiv.org/abs/2008.02387).\n16th International Conference on Parallel Problem Solving from Nature (PPSN 2020).\n\n`[3]`\nSalimans, T., Ho, J., Chen, X., Sidor, S., & Sutskever, I. (2017).\n[Evolution strategies as a scalable alternative to reinforcement learning](https://arxiv.org/abs/1703.03864). \narXiv preprint arXiv:1703.03864.\n\n`[4]`\nHa, D. (2017). [A Visual Guide to Evolution Strategies](https://blog.otoro.net/2017/10/29/visual-evolution-strategies/).\n\n`[5]`\nKingma, D.P., & Ba, J. (2015). [Adam: A method for stochastic optimization](https://arxiv.org/abs/1412.6980).\nIn Proceedings of 3rd International Conference on Learning Representations (ICLR 2015).\n\n`[6]`\nHansen, N., Akimoto, Y., Baudis, P. (2019). [CMA-ES/pycma on Github](https://github.com/CMA-ES/pycma). Zenodo, DOI:10.5281/zenodo.2559634, February 2019.\n\n`[7]`\nMania, H., Guy, A., & Recht, B. (2018). [Simple random search provides a competitive approach to reinforcement learning](https://arxiv.org/abs/1803.07055) \narXiv preprint arXiv:1803.07055. \n\n[Back to Contents](#contents)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{toklu2020clipup,\n  title={ClipUp: A Simple and Powerful Optimizer for Distribution-Based Policy Evolution},\n  author={Toklu, Nihat Engin and Liskowski, Pawe{\\l} and Srivastava, Rupesh Kumar},\n  booktitle={International Conference on Parallel Problem Solving from Nature},\n  pages={515--527},\n  year={2020},\n  organization={Springer}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9959074629570259
      ],
      "excerpt": "Please see the following animation for a visual explanation of how PGPE works. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nnaisense/pgpelib",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-07T09:20:16Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T23:39:05Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "PGPE is an algorithm for computing approximate policy gradients for Reinforcement Learning (RL) problems.\n`pgpelib` provides a clean, scalable and easily extensible implementation of PGPE, and also serves as a reference (re)implementation of **ClipUp** [[2](#references)], an optimizer designed to work specially well with PGPE-style gradient estimation.\nAlthough they were developed in the context of RL, both PGPE and ClipUp are general purpose tools for solving optimization problems.\n\nHere are some interesting RL agents trained in simulation with the [PGPE+ClipUp](#what-is-clipup) implementation in `pgpelib`.\n\n<table>\n    <tr>\n        <td>HumanoidBulletEnv-v0<br />Score: 4853</td>\n        <td>\n            <img src=\"images/pgpelib_HumanoidBulletEnv_score4853.gif\" alt=\"HumanoidBulletEnv-v0\" />\n        </td>\n    <tr>\n    </tr>\n        <td>Humanoid-v2<br />Score: 10184</td>\n        <td>\n            <img src=\"images/pgpelib_Humanoid_score10184.gif\" alt=\"Humanoid-v2\" />\n        </td>\n    <tr>\n    </tr>\n        <td>Walker2d-v2<br />Score: 5232</td>\n        <td>\n            <img src=\"images/pgpelib_Walker2d_score5232.gif\" alt=\"Walker2d-v2\" />\n        </td>\n    </tr>\n</table>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.946885993958996,
        0.9597309189031906
      ],
      "excerpt": "A mini library for Policy Gradients with Parameter-based Exploration [1] and friends.  \nThis library serves as a clean re-implementation of the algorithms used in our relevant paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8656499619292638,
        0.8656499619292638
      ],
      "excerpt": "What is PGPE? \nWhat is ClipUp? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421830362470589,
        0.9514670180646083,
        0.839316437009443,
        0.9169171748385223
      ],
      "excerpt": "PGPE is a derivative-free policy gradient estimation algorithm.  \nMore generally, it can be seen as a distribution-based evolutionary algorithm suitable for optimization in the domain of real numbers. \nWith simple modifications to PGPE, one can also obtain similar algorithms like OpenAI-ES [3] and Augmented Random Search [7]. \nPlease see the following animation for a visual explanation of how PGPE works. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9061193103044464,
        0.8513636255343446
      ],
      "excerpt": "ClipUp is a new optimizer (a gradient following algorithm) that we propose in [2] for use within distribution-based evolutionary algorithms such as PGPE. \nIn [3, 4], it was shown that distribution-based evolutionary algorithms work well with adaptive optimizers.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9861594469395789,
        0.9908924108724612
      ],
      "excerpt": "We argue that ClipUp is simpler and more intuitive, yet competitive with Adam. \nPlease see our blog post and paper [2] for more details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9563295248328837,
        0.9962970918763038,
        0.852466080131688
      ],
      "excerpt": "Ease of parallelization is a massive benefit of evolutionary search techniques. \npgpelib is thoughtfully agnostic when it comes to parallelization: the choice of tool used for parallelization is left to the user. \nWe provide thoroughly documented examples of using either multiprocessing or ray for parallelizing evaluations across multiple cores on a single machine or across multiple machines. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8101272050064657
      ],
      "excerpt": "This repository also contains a Python script for training RL agents. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A mini library for Policy Gradients with Parameter-based Exploration, with reference implementation of the ClipUp optimizer (https://arxiv.org/abs/2008.02387) from NNAISENSE.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nnaisense/pgpelib/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 28 Dec 2021 11:40:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nnaisense/pgpelib/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "nnaisense/pgpelib",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/nnaisense/pgpelib/release/examples/01-rastrigin.ipynb",
      "https://raw.githubusercontent.com/nnaisense/pgpelib/release/examples/02-rl.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/nnaisense/pgpelib/release/train_agents/train_humanoid.sh",
      "https://raw.githubusercontent.com/nnaisense/pgpelib/release/train_agents/train_lunarlandercontinuous.sh",
      "https://raw.githubusercontent.com/nnaisense/pgpelib/release/train_agents/train_walker2d.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**Pre-requisites**: `swig` is a pre-requisite for _Box2D_, a simple physics engine used for some RL examples.\nIt can be installed either system-wide (using a package manager like `apt`) or using `conda`.\nThen you can install `pgpelib` using following commands:\n\n```bash\n#: Install directly from GitHub\npip install git+https://github.com/nnaisense/pgpelib.git#:egg=pgpelib\n\n#: Or install from source in editable mode (to run examples or to modify code)\ngit clone https://github.com/nnaisense/pgpelib.git\ncd pgpelib\npip install -e .\n```\n\nIf you wish to run experiments based on _MuJoCo_, you will need some additional setup.\nSee [this link](https://spinningup.openai.com/en/latest/user/installation.html#installing-mujoco-optional) for setup instructions.\n\n[Back to Contents](#contents)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8686409898177407
      ],
      "excerpt": "See the train_agents directory. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.897176225947782
      ],
      "excerpt": "Basic usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "Training RL agents \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nnaisense/pgpelib/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright 2020, NNAISENSE\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification,\\nare permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this\\nlist of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright notice,\\nthis list of conditions and the following disclaimer in the documentation and/or\\nother materials provided with the distribution.\\n\\n3. Neither the name of the copyright holder nor the names of its contributors\\nmay be used to endorse or promote products derived from this software without\\nspecific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "PGPElib",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pgpelib",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "nnaisense",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nnaisense/pgpelib/blob/release/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 36,
      "date": "Tue, 28 Dec 2021 11:40:01 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning",
      "artificial-intelligence",
      "evolutionary-algorithms",
      "evolution-strategies"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To dive into executable code examples, please see [the `examples` directory](examples/).\nBelow we give a very quick tutorial on how to use `pgpelib` for optimization.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "`pgpelib` provides an ask-and-tell interface for optimization, similar to [[4, 6](#references)].\nThe general principle is to repeatedly _ask_ the optimizer for candidate solutions to evaluate, and then _tell_ it the corresponding fitness values so it can update the current solution or population.\nUsing this interface, a typical communication with the solver is as follows:\n\n```python\nfrom pgpelib import PGPE\nimport numpy as np\n\npgpe = PGPE(\n    solution_length=5,   #: A solution vector has the length of 5\n    popsize=20,          #: Our population size is 20\n\n    #:optimizer='clipup',          #: Uncomment these lines if you\n    #:optimizer_config = dict(     #: would like to use the ClipUp\n    #:    max_speed=...,           #: optimizer.\n    #:    momentum=0.9\n    #:),\n\n    #:optimizer='adam',            #: Uncomment these lines if you\n    #:optimizer_config = dict(     #: would like to use the Adam\n    #:    beta1=0.9,               #: optimizer.\n    #:    beta2=0.999,\n    #:    epsilon=1e-8\n    #:),\n\n    ...\n)\n\n#: Let us run the evolutionary computation for 1000 generations\nfor generation in range(1000):\n\n    #: Ask for solutions, which are to be given as a list of numpy arrays.\n    #: In the case of this example, solutions is a list which contains\n    #: 20 numpy arrays, the length of each numpy array being 5.\n    solutions = pgpe.ask()\n\n    #: This is the phase where we evaluate the solutions\n    #: and prepare a list of fitnesses.\n    #: Make sure that fitnesses[i] stores the fitness of solutions[i].\n    fitnesses = [...]  #: compute the fitnesses here\n\n    #: Now we tell the result of our evaluations, fitnesses,\n    #: to our solver, so that it updates the center solution\n    #: and the spread of the search distribution.\n    pgpe.tell(fitnesses)\n\n#: After 1000 generations, we print the center solution.\nprint(pgpe.center)\n```\n\n`pgpelib` also supports **adaptive population sizes**, where additional solutions are sampled from the current search distribution and evaluated until a certain number of total simulator interactions (i.e. timesteps) is reached.\nUse of this technique can be enabled by specifying the `num_interactions` parameter, as demonstrated by the following snippet:\n\n```python\npgpe = PGPE(\n    solution_length=5,      #: Our RL policy has 5 trainable parameters.\n    popsize=20,             #: Our base population size is 20.\n                            #: After evaluating a batch of 20 policies,\n                            #: if we do not reach our threshold of\n                            #: simulator interactions, we will keep sampling\n                            #: and evaluating more solutions, 20 at a time,\n                            #: until the threshold is finally satisfied.\n\n    num_interactions=17500, #: Threshold for simulator interactions.\n    ...\n)\n\n#: Let us run the evolutionary computation for 1000 generations\nfor generation in range(1000):\n\n    #: We begin the inner loop of asking for new solutions,\n    #: until the threshold of interactions count is reached.\n    while True:\n\n        #: ask for new policies to evaluate in the simulator\n        solutions = pgpe.ask()\n\n        #: This is the phase where we evaluate the policies,\n        #: and prepare a list of fitnesses and a list of\n        #: interaction counts.\n        #: Make sure that:\n        #:   fitnesses[i] stores the fitness of solutions[i];\n        #:   interactions[i] stores the number of interactions\n        #:       made with the simulator while evaluating the\n        #:       i-th solution.\n        fitnesses = [...]\n        interactions = [...]\n\n        #: Now we tell the result of our evaluations\n        #: to our solver, so that it updates the center solution\n        #: and the spread of the search distribution.\n        interaction_limit_reached = pgpe.tell(fitnesses, interactions)\n\n        #: If the limit on number of interactions per generation is reached,\n        #: pgpelib has already updated the search distribution internally.\n        #: So we can stop creating new solutions and end this generation.\n        if interaction_limit_reached:\n            break\n\n#: After 1000 generations, we print the center solution (policy).\nprint(pgpe.center)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}