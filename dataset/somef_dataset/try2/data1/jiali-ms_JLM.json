{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1608.05859",
      "https://arxiv.org/abs/1512.04906",
      "https://arxiv.org/abs/1609.04309"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jiali-ms/JLM",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-01-08T09:20:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-02T07:27:47Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9421795587532031,
        0.996633470054476
      ],
      "excerpt": "A fast LSTM Language Model for large vocabulary language like Japanese and Chinese. \nIt focuses on accelerating inference time and reducing model size to fit requirement of real-time applications especially in client side. It is 85% smaller, and are 50x faster than standard LSTM solution with softmax. See the paper JLM - Fast RNN Language Model with Large Vocabulary for performance detail. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9740461504158936,
        0.8191039133336676
      ],
      "excerpt": "A standard Viterbi decoder with beam search is implemented. It batches prediction to save decoding time. We also implemented Enabling Real-time Neural IME with Incremental Vocabulary Selection at NAACL 2019. It further more reduced the softmax cost during decoding by ~95%, reaching real-time in commodity CPU. \nRun the test.py in train folder, it will auto generate a random sentence with the trained model. If the sentence doesn't make sense to your language knowledge at all. One of the stage is not correctly setup. Here is an example of random generated sentence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8255271196999419
      ],
      "excerpt": "It is a common issue to bring trained model into clients like mobile device or Windows environment. We recommend you use ONNX for the purpose. But for this example, we want full control and want to cutoff dependency to TF. Run the weights.py in the train folder to get a pickle of all the trainable parameters in the numpy format. You can use the following command to even dump the txt format for other usage. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9231036970204287
      ],
      "excerpt": "Run eval.py to see the conversion accuracy of your trained model with offline model.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A fast LSTM Language Model for large vocabulary language like Japanese and Chinese",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jiali-ms/JLM/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 24,
      "date": "Thu, 23 Dec 2021 17:53:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jiali-ms/JLM/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jiali-ms/JLM",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "> python data.py -v 50000\n\nMake sure you put your corpus.txt in the data folder first. The script will generate a encoded corpus and a bunch of pickles for later usage.  The lexicon is also generated in this stage. Words in lexicon are ordered by their frequency for the convenience of vocabulary segmentation.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We use [BCCWJ](http://pj.ninjal.ac.jp/corpus_center/bccwj/en/) Japanese corpus for example. \n\nMake sure you have a txt file with a white space segmented sentence per line.\n> If not, use tools like [MeCab](http://taku910.github.io/mecab/) to get the job done first.\n\nIn JLM, the unit **word** is the in the format of **display/reading/POS**.  For example \"\u54c1\u5ddd/\u30b7\u30ca\u30ac\u30ef/\u540d\u8a5e-\u56fa\u6709\u540d\u8a5e-\u5730\u540d-\u4e00\u822c\". \n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8529608171421074
      ],
      "excerpt": "Run the test.py in train folder, it will auto generate a random sentence with the trained model. If the sentence doesn't make sense to your language knowledge at all. One of the stage is not correctly setup. Here is an example of random generated sentence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9483418594369847,
        0.8004040272594708,
        0.8943096850060217
      ],
      "excerpt": "python weiths.py -e 1 -v True \nRun eval.py to see the conversion accuracy of your trained model with offline model.  \npython eval.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jiali-ms/JLM/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Jerry Yao\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "JLM",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "JLM",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jiali-ms",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jiali-ms/JLM/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The training part is done with TensorFlow. Instead of depending on a big dynamic library of TF to run in client app, we dumped the trained weights out. The inference and decoding can be done by python with numpy or C++ with Eigen. There is no black box.\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "> python data.py -v 50000\n\nMake sure you put your corpus.txt in the data folder first. The script will generate a encoded corpus and a bunch of pickles for later usage.  The lexicon is also generated in this stage. Words in lexicon are ordered by their frequency for the convenience of vocabulary segmentation.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Config the experiment folder path (put the corpus with vocab you would like to use in the config)\n> data_path = os.path.abspath(os.path.join(root_path, \"data/corpus_50000\"))\n\n- Enter the train folder, and edit the run file [train.py](https://github.com/jiali-ms/JLM/blob/master/train/train.py) file\n\n```python\nparameters = {\n    \"is_debug\": False,\n    \"batch_size\": 128 * 3,\n    \"embed_size\": 256,\n    \"hidden_size\": 512,\n    \"num_steps\": 20,\n    \"max_epochs\": 10,\n    \"early_stopping\": 1,\n    \"dropout\": 0.9,\n    \"lr\": 0.001,\n    \"share_embedding\": True,\n    \"gpu_id\": 0,\n    \"tf_random_seed\": 101,\n    \"D_softmax\": False,\n    \"V_table\": True,\n    \"embedding_seg\": [(200, 0, 12000), (100, 12000, 30000), (50, 30000, None)]\n}\n```\n **V_table** is the D-softmax* here. The **embedding_seg** means how you want to separate your vocabulary. Make your own best result by tuning the hyper parameters. The results will be saved to folder \"experiments\" with a ID, **sacred** framework will take care of all the experiment indexing.\n > A small hint here, use up your GPU memory by setting a bigger **num_steps** and **batch_size** to best reduce training time.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 109,
      "date": "Thu, 23 Dec 2021 17:53:59 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tensorflow",
      "language-modeling",
      "lstm",
      "decoder",
      "deep-neural-networks",
      "beam-search",
      "viterbi"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "go to http://nlpfun.com/ime\n",
      "technique": "Header extraction"
    }
  ]
}