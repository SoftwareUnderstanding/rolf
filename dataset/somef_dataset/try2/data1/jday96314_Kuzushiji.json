{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jday96314/Kuzushiji",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-20T01:58:25Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-20T03:11:12Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9087926180575034,
        0.9800750302958444,
        0.8979411005071259,
        0.8149449514447441
      ],
      "excerpt": "This is the 13th place solution to the Kuzushiji Recognition Kaggle competition. \nThanks to Kaggle and the organizers for creating such an interesting competition. Congrats to everyone who finished. \nData preprocessing \nI resized all images to 512x512. Some of my images were converted to greyscale. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8855755189674606,
        0.8718854135150749
      ],
      "excerpt": "My solution was based on Faster-RCNN, but with several small modifications. \n1.  Instead of choosing anchor sizes arbitrarily, I selected them by running k-means clustering on the widths and heights of the ground truth bounding boxes. I used the cluster centers as my anchor box sizes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9877175872034002,
        0.9542404560029221
      ],
      "excerpt": "3. No layers are shared between the network which proposes regions of interest and the network which classifies them.  This allowed for easier experimentation because it allowed the networks to be modified independently of one another. It also simplified the training procedure. However, the inference efficiency of my solution could likely be improved by sharing all of the residual blocks. \nMy region proposal network and my character classification network both used wide ResNet-34 backbones. My region proposal network used twice as many convolutional filters as the ResNet-34 configuration described in the original ResNet paper. My character classification network used roughly three times as many convolutional filters as the original ResNet-34 configuration. I experimented with deeper ResNet-50 and ResNet-101 based networks, but I found wider, shallower networks worked better. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9501725741309865,
        0.9708538319145353,
        0.9494531992587789,
        0.912026086664785,
        0.9926481926315863,
        0.959920616563336
      ],
      "excerpt": "Failed attempt at using a language model for post-processing the detection results \nI was very interested in using a language model to correct my image processing code's incorrect detections. I spent over a month trying different approaches for this but was unable to get it working well. \nTo get the characters into approximately the order in which a human would read them, I used DBSCAN to group the characters into columns, sorted the columns by the mean horizontal coordinates of their characters, and then sorted the characters within each column by their vertical coordinate. This worked well for most images. \nTo train my correction networks, I used the ground truth labels to generate a large number of synthetic submission files that contained known errors. These errors were fairly realistic and were randomly added based on statistics from how my Faster-RCNN based model performs on a cross-validation dataset. \nTo perform the character label corrections, I initially tried network architectures that are meant to performing language translations, such as encoder-decoder LSTMs with an attention mechanism or a transformer network. I favored these network architectures over passing characters into a single recurrent network because in theory, they should be able to elegantly handle cases in which the number of ground truth characters is different from the number of characters which was detected. Unfortunately, these networks introduced slightly more errors than they corrected. I eventually abandoned the translation based network architectures and switched over to using a simple bi-directional GRU network. It was able to improve my score by around .01, but only on submission files which were generated by weak label detection models. If I run it on my best submission file, then it will lower my score by ~.003. \nI think the reason I had so much difficulty getting a language model working well is that my Faster-RCNN based models do a better job considering a character's context than I initially expected. I think in order to get the correction network working well I would have needed to pull in an outside text corpus. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "1. Data augmentation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Kuzushiji Recognition Kaggle competition solution.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jday96314/Kuzushiji/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 12:11:22 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jday96314/Kuzushiji/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jday96314/Kuzushiji",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jday96314/Kuzushiji/master/ImageProcessing/DualNetworkApproach/SubmissionVisualization.ipynb",
      "https://raw.githubusercontent.com/jday96314/Kuzushiji/master/ImageProcessing/InitialExploration/CsvExploration.ipynb",
      "https://raw.githubusercontent.com/jday96314/Kuzushiji/master/ImageProcessing/InitialExploration/ImageExploration.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jday96314/Kuzushiji/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Kuzushiji",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Kuzushiji",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jday96314",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jday96314/Kuzushiji/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 12:11:22 GMT"
    },
    "technique": "GitHub API"
  }
}