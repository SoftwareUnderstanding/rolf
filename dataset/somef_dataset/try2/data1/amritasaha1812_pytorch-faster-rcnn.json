{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1612.03144",
      "https://arxiv.org/abs/1612.03144",
      "https://arxiv.org/abs/1702.02138"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this implementation or the analysis conducted in our report helpful, please consider citing:\n\n    @article{chen17implementation,\n        Author = {Xinlei Chen and Abhinav Gupta},\n        Title = {An Implementation of Faster RCNN with Study for Region Sampling},\n        Journal = {arXiv preprint arXiv:1702.02138},\n        Year = {2017}\n    }\n\nFor convenience, here is the faster RCNN citation:\n\n    @inproceedings{renNIPS15fasterrcnn,\n        Author = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},\n        Title = {Faster {R-CNN}: Towards Real-Time Object Detection\n                 with Region Proposal Networks},\n        Booktitle = {Advances in Neural Information Processing Systems ({NIPS})},\n        Year = {2015}\n    }\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{renNIPS15fasterrcnn,\n    Author = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},\n    Title = {Faster {R-CNN}: Towards Real-Time Object Detection\n             with Region Proposal Networks},\n    Booktitle = {Advances in Neural Information Processing Systems ({NIPS})},\n    Year = {2015}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{chen17implementation,\n    Author = {Xinlei Chen and Abhinav Gupta},\n    Title = {An Implementation of Faster RCNN with Study for Region Sampling},\n    Journal = {arXiv preprint arXiv:1702.02138},\n    Year = {2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.816073034659289
      ],
      "excerpt": "If you used the master branch before Sep. 26 2017 and its corresponding pretrained model, PLEASE PAY ATTENTION: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8943024519418389
      ],
      "excerpt": "  - Train Resnet101 on COCO 2014 trainval35k and test on minival (900k/1190k), ~~37.4~~. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amritasaha1812/pytorch-faster-rcnn",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-16T09:09:11Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-16T09:10:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9109736702314366
      ],
      "excerpt": "The old master branch in now under old_master, you can still run the code and download the pretrained model, but the pretrained model for that old master is not compatible to the current master! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9708478168973925,
        0.9510214564882173,
        0.9913168973017327,
        0.9600101414244504,
        0.9670415052119818
      ],
      "excerpt": "The change is related to this issue; master now matches all the details in tf-faster-rcnn so that we can now convert pretrained tf model to pytorch model. \nA pytorch implementation of faster RCNN detection framework based on Xinlei Chen's tf-faster-rcnn. Xinlei Chen's repository is based on the python Caffe implementation of faster RCNN available here. \nNote: Several minor modifications are made when reimplementing the framework, which give potential improvements. For details about the modifications and ablative analysis, please refer to the technical report An Implementation of Faster RCNN with Study for Region Sampling. If you are seeking to reproduce the results in the original paper, please use the official code or maybe the semi-official code. For details about the faster RCNN architecture please refer to the paper Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. \nThe current code supports VGG16, Resnet V1 and Mobilenet V1 models. We mainly tested it on plain VGG16 and Resnet101 architecture. As the baseline, we report numbers using a single model on a single convolution layer, so no multi-scale, no multi-stage bounding box regression, no skip-connection, no extra input is used. The only data augmentation technique is left-right flipping during training following the original Faster RCNN. All models are released. \nWith VGG16 (conv5_3): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8019629248370846
      ],
      "excerpt": "With Resnet101 (last conv4): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714172048282937
      ],
      "excerpt": "More Results: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8191521690591188
      ],
      "excerpt": "  - Due to the randomness in GPU training especially for VOC, the best numbers are reported (with 2-3 attempts) here. According to Xinlei's experience, for COCO you can almost always get a very close number (within ~0.2%) despite the randomness. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9383667034651,
        0.821398214001502,
        0.9439099324423044,
        0.9037475352530888
      ],
      "excerpt": "  - Since we keep the small proposals (\\< 16 pixels width/height), our performance is especially good for small objects. \n  - We do not set a threshold (instead of 0.05) for a detection to be included in the final result, which increases recall. \n  - Weight decay is set to 1e-4. \n  - For other minor modifications, please check the report. Notable ones include using crop_and_resize, and excluding ground truth boxes in RoIs during training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9736187111884275,
        0.9086856961213169,
        0.8290056923818434,
        0.8380990540926213
      ],
      "excerpt": "  - For Resnets, we fix the first block (total 4) when fine-tuning the network, and only use crop_and_resize to resize the RoIs (7x7) without max-pool (which Xinlei finds useless especially for COCO). The final feature maps are average-pooled for classification and regression. All batch normalization parameters are fixed. Learning rate for biases is not doubled. \n  - For Mobilenets, we fix the first five layers when fine-tuning the network. All batch normalization parameters are fixed. Weight decay for Mobilenet layers is set to 4e-5. \nFor approximate FPN baseline setup we simply resize the image with 800 pixels, add 32^2 anchors, and take 1000 proposals during testing. \nCheck out here/here/here for the latest models, including longer COCO VGG16 models and Resnet ones. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411427044783083,
        0.978822405335542,
        0.9390434731139468,
        0.968966523355069
      ],
      "excerpt": "Additional features not mentioned in the report are added to make research life easier: \n  - Support for train-and-validation. During training, the validation data will also be tested from time to time to monitor the process and check potential overfitting. Ideally training and validation should be separate, where the model is loaded every time to test on validation. However Xinlei have implemented it in a joint way to save time and GPU memory. Though in the default setup the testing data is used for validation, no special attempts is made to overfit on testing set. \n  - Support for resuming training. Xinlei tried to store as much information as possible when snapshoting, with the purpose to resume training from the latest snapshot properly. The meta information includes current image index, permutation of images, and random state of numpy. However, when you resume training the random seed for tensorflow will be reset (not sure how to save the random state of tensorflow now), so it will result in a difference. Note that, the current implementation still cannot force the model to behave deterministically even with the random seeds set. Suggestion/solution is welcome and much appreciated. \n  - Support for visualization. The current implementation will summarize ground truth boxes, statistics of losses, activations and variables during training, and dump it to a separate folder for tensorboard visualization. The computing graph is also saved for debugging. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.965651939988263
      ],
      "excerpt": "  #: NET in {vgg16, res50, res101, res152} is the network arch to use \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.965651939988263
      ],
      "excerpt": "  #: NET in {vgg16, res50, res101, res152} is the network arch to use \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.989203723074911
      ],
      "excerpt": "The default number of training iterations is kept the same to the original faster RCNN for VOC 2007, however Xinlei finds it is beneficial to train longer (see report for COCO), probably due to the fact that the image batch size is one. For VOC 07+12 we switch to a 80k/110k schedule following R-FCN. Also note that due to the nondeterministic nature of the current implementation, the performance can vary a bit, but in general it should be within ~1% of the reported numbers for VOC, and ~0.2% of the reported numbers for COCO. Suggestions/Contributions are welcome. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amritasaha1812/pytorch-faster-rcnn/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 04:51:53 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/amritasaha1812/pytorch-faster-rcnn/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "amritasaha1812/pytorch-faster-rcnn",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/amritasaha1812/pytorch-faster-rcnn/master/tools/demo.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/amritasaha1812/pytorch-faster-rcnn/master/experiments/scripts/convert_vgg16.sh",
      "https://raw.githubusercontent.com/amritasaha1812/pytorch-faster-rcnn/master/experiments/scripts/train_faster_rcnn_notime.sh",
      "https://raw.githubusercontent.com/amritasaha1812/pytorch-faster-rcnn/master/experiments/scripts/test_faster_rcnn.sh",
      "https://raw.githubusercontent.com/amritasaha1812/pytorch-faster-rcnn/master/experiments/scripts/test_faster_rcnn_notime.sh",
      "https://raw.githubusercontent.com/amritasaha1812/pytorch-faster-rcnn/master/experiments/scripts/train_faster_rcnn.sh",
      "https://raw.githubusercontent.com/amritasaha1812/pytorch-faster-rcnn/master/data/scripts/fetch_faster_rcnn_models.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please follow the instructions of py-faster-rcnn [here](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to setup VOC and COCO datasets (Part of COCO is done). The steps involve downloading data and optionally creating soft links in the ``data`` folder. Since faster RCNN does not rely on pre-computed proposals, it is safe to ignore the steps that setup proposals.\n\nIf you find it useful, the ``data/cache`` folder created on Xinlei's side is also shared [here](https://drive.google.com/drive/folders/0B1_fAEgxdnvJSmF3YUlZcHFqWTQ).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Clone the repository\n  ```Shell\n  git clone https://github.com/ruotianluo/pytorch-faster-rcnn.git\n  ```\n\n2. Install the [Python COCO API](https://github.com/pdollar/coco). The code requires the API to access COCO dataset.\n  ```Shell\n  cd data\n  git clone https://github.com/pdollar/coco.git\n  cd coco/PythonAPI\n  make\n  cd ../../..\n  ```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8026995432291087
      ],
      "excerpt": "  - Due to the randomness in GPU training especially for VOC, the best numbers are reported (with 2-3 attempts) here. According to Xinlei's experience, for COCO you can almost always get a very close number (within ~0.2%) despite the randomness. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8254213883331442,
        0.935187272858883,
        0.8837680365796365
      ],
      "excerpt": "   cd data/imagenet_weights \n   python #: open python in terminal and run the following Python code \n   ```Python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466,
        0.8633405492125716
      ],
      "excerpt": "   cd ../.. \n   For Resnet101, you can set up like:Shell \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8254213883331442,
        0.8636108802707678
      ],
      "excerpt": "   cd data/imagenet_weights \n   #: download from my gdrive (link in pytorch-resnet) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "   cd ../.. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8544778391397956
      ],
      "excerpt": "For Mobilenet V1, you can set up like: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8254213883331442
      ],
      "excerpt": "   cd data/imagenet_weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "   cd ../.. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.884067387307346,
        0.9069687384322455
      ],
      "excerpt": "  ./experiments/scripts/train_faster_rcnn.sh [GPU_ID] [DATASET] [NET] \n  #: GPU_ID is the GPU you want to test on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9549997561619052
      ],
      "excerpt": "  ./experiments/scripts/train_faster_rcnn.sh 1 coco res101 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.884067387307346,
        0.9069687384322455
      ],
      "excerpt": "  ./experiments/scripts/test_faster_rcnn.sh [GPU_ID] [DATASET] [NET] \n  #: GPU_ID is the GPU you want to test on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9549997561619052
      ],
      "excerpt": "  ./experiments/scripts/test_faster_rcnn.sh 1 coco res101 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8078910944880745,
        0.8251831069890266
      ],
      "excerpt": "  - Train on VOC 2007 trainval and test on VOC 2007 test, 71.22(from scratch) 70.75(converted) (70.8 for tf-faster-rcnn). \n  - Train on VOC 2007+2012 trainval and test on VOC 2007 test (R-FCN schedule), 75.33(from scratch) 75.27(converted) (75.7 for tf-faster-rcnn). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8449972127073169,
        0.8078079626793083
      ],
      "excerpt": "  - Train on VOC 2007 trainval and test on VOC 2007 test, 75.29(from scratch) 75.76(converted) (75.7 for tf-faster-rcnn). \n  - Train on VOC 2007+2012 trainval and test on VOC 2007 test (R-FCN schedule), 79.26(from scratch) 79.78(converted) (79.8 for tf-faster-rcnn). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8606681050589102,
        0.8900486270063179
      ],
      "excerpt": "   from torch.utils.model_zoo import load_url \n   from torchvision import models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8275093558259294
      ],
      "excerpt": "Train (and test, evaluation) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838148168639296
      ],
      "excerpt": "  #: Examples: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838148168639296
      ],
      "excerpt": "  #: Examples: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869,
        0.8133316360607923
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.312 \n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.128 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.312 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/amritasaha1812/pytorch-faster-rcnn/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell",
      "MATLAB",
      "Roff"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 Xinlei Chen\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Important notice:",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-faster-rcnn",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "amritasaha1812",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amritasaha1812/pytorch-faster-rcnn/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  - A basic pytorch installation. The code follows **1.0**. If you are using old **0.1.12** or **0.2** or **0.3** or **0.4**, you can checkout the corresponding branch.\n  - Torchvision **0.3**. This code uses `torchvision.ops` for `nms`, `roi_pool` and `roi_align`\n  - Python packages you might not have: `opencv-python`, `easydict` (similar to [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn)). For `easydict` make sure you have the right version. Xinlei uses 1.6.\n  - [tensorboard-pytorch](https://github.com/lanpa/tensorboard-pytorch) to visualize the training and validation curve. Please build from source to use the latest tensorflow-tensorboard.\n  - ~~Docker users: Since the recent upgrade, the docker image on docker hub (https://hub.docker.com/r/mbuckler/tf-faster-rcnn-deps/) is no longer valid. However, you can still build your own image by using dockerfile located at `docker` folder (cuda 8 version, as it is required by Tensorflow r1.0.) And make sure following Tensorflow installation to install and use nvidia-docker[https://github.com/NVIDIA/nvidia-docker]. Last, after launching the container, you have to build the Cython modules within the running container.~~\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 04:51:53 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Download pre-trained model (only google drive works)\n  <!-- ```Shell\n  #: Resnet101 for voc pre-trained on 07+12 set\n  #: ./data/scripts/fetch_faster_rcnn_models.sh\n  ```\n  **Note**: if you cannot download the models through the link, or you want to try more models, you can check out the following solutions and optionally update the downloading script: -->\n  - ~~Another server [here](http://gs11655.sp.cs.cmu.edu/xinleic/tf-faster-rcnn/).~~\n  - Google drive [here](https://drive.google.com/open?id=0B7fNdx_jAqhtNE10TDZDbFRuU0E).\n\n**(Optional)**\nInstead of downloading my pretrained or converted model, you can also convert from tf-faster-rcnn model.\nYou can download the tensorflow pretrained model from [tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn/#demo-and-test-with-pre-trained-models).\nThen run:\n```Shell\npython tools/convert_from_tensorflow.py --tensorflow_model resnet_model.ckpt \npython tools/convert_from_tensorflow_vgg.py --tensorflow_model vgg_model.ckpt\n```\n\nThis script will create a `.pth` file with the same name in the same folder as the tensorflow model.\n\n2. Create a folder and a soft link to use the pre-trained model\n  ```Shell\n  NET=res101\n  TRAIN_IMDB=voc_2007_trainval+voc_2012_trainval\n  mkdir -p output/${NET}/${TRAIN_IMDB}\n  cd output/${NET}/${TRAIN_IMDB}\n  ln -s ../../../data/voc_2007_trainval+voc_2012_trainval ./default\n  cd ../../..\n  ```\n\n3. Demo for testing on custom images\n  ```Shell\n  #: at repository root\n  GPU_ID=0\n  CUDA_VISIBLE_DEVICES=${GPU_ID} ./tools/demo.py\n  ```\n  **Note**: Resnet101 testing probably requires several gigabytes of memory, so if you encounter memory capacity issues, please install it with CPU support only. Refer to [Issue 25](https://github.com/endernewton/tf-faster-rcnn/issues/25).\n\n4. Test with pre-trained Resnet101 models\n  ```Shell\n  GPU_ID=0\n  ./experiments/scripts/test_faster_rcnn.sh $GPU_ID pascal_voc_0712 res101\n  ```\n  **Note**: If you cannot get the reported numbers (79.8 on my side), then probably the NMS function is compiled improperly, refer to [Issue 5](https://github.com/endernewton/tf-faster-rcnn/issues/5).\n\n",
      "technique": "Header extraction"
    }
  ]
}