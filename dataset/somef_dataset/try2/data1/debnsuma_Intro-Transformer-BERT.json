{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\nhttps://arxiv.org/pdf/1810.04805.pdf\n\n[2] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and\nSanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. In Proceedings of the IEEE international conference on computer vision,\npages 19\u201327.\n\n[3] Getting Started with Google BERT\nhttps://www.packtpub.com/product/getting-started-with-google-bert/9781838821593\n\n[4] Data Science on AWS\nhttps://www.oreilly.com/library/view/data-science-on/9781492079385/\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/debnsuma/Intro-Transformer-BERT",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-05T06:17:07Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-02T22:04:18Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9707544566117037,
        0.9942901542058173,
        0.9831037127098904,
        0.972674909588609,
        0.8181055182438216
      ],
      "excerpt": "Text classification is a technique for putting text into different categories and has a wide range of applications: email providers use text classification to detect to spam emails, marketing agencies use it for sentiment analysis of customer reviews, and moderators of discussion forums use it to detect inappropriate comments. \nIn the past, data scientists used methods such as tf-idf, word2vec, or bag-of-words (BOW) to generate features for training classification models. While these techniques have been very successful in many NLP tasks, they don't always capture the meanings of words accurately when they appear in different contexts. Recently, we see increasing interest in using Bidirectional Encoder Representations from Transformers (BERT) to achieve better results in text classification tasks, due to its ability more accurately encode the meaning of words in different contexts. \nBERT was trained on BookCorpus and English Wikipedia data, which contain 800 million words and 2,500 million words, respectively. Training BERT from scratch would be prohibitively expensive. By taking advantage of transfer learning, one can quickly fine tune BERT for another use case with a relatively small amount of training data to achieve state-of-the-art results for common NLP tasks, such as text classification and question answering.  \nAmazon SageMaker is a fully managed service that provides developers and data scientists with the ability to build, train, and deploy machine learning (ML) models quickly. Amazon SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models. The SageMaker Python SDK provides open source APIs and containers that make it easy to train and deploy models in Amazon SageMaker with several different machine learning and deep learning frameworks. \nIn this example, we walk through our dataset, the training process, and finally model deployment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8625148601934314,
        0.8517713677169102,
        0.941382166229458,
        0.9515455699751012,
        0.8460093578091802,
        0.9330537515440218,
        0.9706683053334487,
        0.9266783079280485,
        0.8893448875268035,
        0.8909017175738522,
        0.8442636515317746,
        0.9821916110674341
      ],
      "excerpt": "sentences are intentionally masked. BERT takes in these masked sentences as input and trains itself \nto predict the masked word. In addition, BERT uses a \"next sentence prediction\" task that pre-trains \ntext-pair representations. BERT is a substantial breakthrough and has helped researchers and data \nengineers across industry to achieve state-of-art results in many Natural Language Processing (NLP) \ntasks. BERT offers representation of each word conditioned on its context (rest of the sentence). \nFor more information about BERT, please refer to [1]. \nOne of the biggest challenges data scientists face for NLP projects is lack of training data; they \noften have only a few thousand pieces of human-labeled text data for their model training. However, \nmodern deep learning NLP tasks require a large amount of labeled data. One way to solve this problem \nis to use transfer learning. \nTransfer learning is a machine learning method where a pre-trained model, such as a pre-trained \nResNet model for image classification, is reused as the starting point for a different but related \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8705036638742941,
        0.8705098048032058
      ],
      "excerpt": "time and cost. \nBERT was trained on BookCorpus and English Wikipedia data, which contain 800 million words and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8651971903865145,
        0.9764717602226664
      ],
      "excerpt": "By taking advantage of transfer learning, one can quickly fine tune BERT for another use case with a \nrelatively small amount of training data to achieve state-of-the-art results for common NLP tasks, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Introduction to Transformers and BERT",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://sagemaker.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/debnsuma/Intro-Transformer-BERT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Thu, 30 Dec 2021 07:50:54 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/debnsuma/Intro-Transformer-BERT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "debnsuma/Intro-Transformer-BERT",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/debnsuma/Intro-Transformer-BERT/main/BERT-Disaster-Tweets-Prediction.ipynb",
      "https://raw.githubusercontent.com/debnsuma/Intro-Transformer-BERT/main/.ipynb_checkpoints/BERT-Disaster-Tweets-Prediction-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/debnsuma/Intro-Transformer-BERT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT No Attribution",
      "url": "https://api.github.com/licenses/mit-0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of\\nthis software and associated documentation files (the \"Software\"), to deal in\\nthe Software without restriction, including without limitation the rights to\\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\\nthe Software, and to permit persons to whom the Software is furnished to do so.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Fine Tuning BERT for Disaster Tweets Classification",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Intro-Transformer-BERT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "debnsuma",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/debnsuma/Intro-Transformer-BERT/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Thu, 30 Dec 2021 07:50:54 GMT"
    },
    "technique": "GitHub API"
  }
}