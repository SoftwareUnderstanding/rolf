{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2111.08960",
      "https://arxiv.org/abs/2111.08960](https://arxiv.org/abs/2111.08960",
      "https://arxiv.org/abs/2111.08960",
      "https://arxiv.org/abs/1406.2661",
      "https://arxiv.org/abs/1912.04958",
      "https://arxiv.org/abs/1810.10340"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{hudson2021ganformer2,\n  title={Compositional Transformers for Scene Generation},\n  author={Hudson, Drew A and Zitnick, C. Lawrence},\n  journal={Advances in Neural Information Processing Systems {NeurIPS} 2021},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{hudson2021ganformer,\n  title={Generative Adversarial Transformers},\n  author={Hudson, Drew A and Zitnick, C. Lawrence},\n  journal={Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9586556701111782
      ],
      "excerpt": "  <b><a href=\"https://cs.stanford.edu/~dorarad/\">Drew A. Hudson</a>* & <a href=\"http://larryzitnick.org/\">C. Lawrence Zitnick</a></b></span> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977994744046882,
        0.9977994744046882
      ],
      "excerpt": "1st Paper: https://arxiv.org/pdf/2103.01209 \n2nd Paper: https://arxiv.org/abs/2111.08960 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "| FFHQ          | 70,000    | 256&times;256 | 13GB           | 13GB             | 10    | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9321597499656605
      ],
      "excerpt": "| GAN        | 25.02        | 12.16        | 13.18      | 11.57      | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "| VQGAN      | 32.60        | 59.63        | 63.12      | 173.80     | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dorarad/gansformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-01T13:39:07Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T02:37:39Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9937135836879591
      ],
      "excerpt": "*I wish to thank Christopher D. Manning for the fruitful discussions and constructive feedback in developing the Bipartite Transformer, especially when explored within the language representation area and also in the visual context, as well as for providing the kind financial support that allowed this work to happen! :sunflower: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9909851986635139,
        0.997792365443614,
        0.9363491143781869
      ],
      "excerpt": "This is an implementation of the GANformer model, a novel and efficient type of transformer, explored for the task of image generation. The network employs a bipartite structure that enables long-range interactions across the image, while maintaining computation of linearly efficiency, that can readily scale to high-resolution synthesis.  \nThe model iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes.  \nIn contrast to the classic transformer architecture, it utilizes multiplicative integration that allows flexible region-based modulation, and can thus be seen as a generalization of the successful StyleGAN network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9348349526221301
      ],
      "excerpt": ":white_check_mark: Pretrained networks for all datasets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9302287581602335
      ],
      "excerpt": "\u2b1c\ufe0f Releasing the GANformer2 model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8029332580996773
      ],
      "excerpt": "Using the pre-trained models (generated after training for 5-7x less steps than StyleGAN2 models! Training our models for longer will improve the image quality further): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9378529984850988,
        0.8904368148079543
      ],
      "excerpt": "We recommend setting it to values in the range of 0.6-1.0. \nWe currently provide pretrained models for resolution 256&times;256 but keep training them and will release newer checkpoints as well as pretrained models for resolution 1024&times;1024 soon! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9784182589953068
      ],
      "excerpt": "See table below for details about the datasets in the catalog. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9457936607993204,
        0.9030727783299551,
        0.9301607930424819
      ],
      "excerpt": " --shards-num to select the number of shards for the data (default: adapted to each dataset) \n* --max-images to store only a subset of the dataset, in order to reduce the size of the stored tfrecord files (default: max). \nThis can be particularly useful to save space in case of large datasets, such as LSUN-bedrooms (originaly contains 3M images) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9180194321940001
      ],
      "excerpt": "We provide pretrained models for bedrooms, cityscapes, clevr and ffhq. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9225443627168868
      ],
      "excerpt": "For comparing to state-of-the-art, we compute metric scores using 50,000 sample imaegs. To expedite training though, we recommend settings --eval-images-num to a lower number. Note though that this can impact the precision of the metrics, so we recommend using a lower value during training, and increasing it back up in the final evaluation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9317275706870117,
        0.8949296155660326
      ],
      "excerpt": "Tensorboard logs are also created (--summarize) that track the metrics, loss values for the generator and discriminator, and other useful statistics over the course of training. \nThe codebase suppors multiple baselines in addition to the GANformer. For instance, to run a vanilla GAN model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.83884057343124
      ],
      "excerpt": "* StyleGAN2: --baseline StyleGAN2, with one global latent that modulates the image features. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8575406830195649
      ],
      "excerpt": "* SAGAN: --baseline SAGAN, which performs self-attention between all image features in low-resolution layer (e.g. 32x32). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9172854772981287,
        0.948886630621922,
        0.860059181823877
      ],
      "excerpt": "Below we provide the FID-50k scores for the GANformer (using the pretrained checkpoints above) as well as baseline models. \nNote that these scores are different than the scores reported in the StyleGAN2 paper since they run experiments for up to 7x more training steps (5k-15k kimg-steps in our experiments over all models, which takes about 3-4 days with 4 GPUs, vs 50-70k kimg-steps in their experiments, which take over 90 GPU-days). \n| Model          | CLEVR        | LSUN-Bedroom | FFHQ       | Cityscapes | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9677952850427715
      ],
      "excerpt": "Compared to the original GANformer depicted in the paper, this repository make several additional improvments that contributed to the performance: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9546298681201231,
        0.850925700462443,
        0.9362955217160405,
        0.963589492747104
      ],
      "excerpt": "* Add an additional global latent (--style) to the k latent components, such that first the global latent modulates all the image features uniformly, and then the k latents modulate different regions based on the bipartite transformer's attention. \nThe global latent is useful for coordinating holistic aspects of the image such as global lighting conditions, global style properties for e.g. faces, etc. \n* After making these changes, we observed no additional benefit from adding the transformer to the discriminator, and therefore for simplicity we disabled that. \nThe code supports producing qualitative results and visualizations. For instance, to create attention maps for each layer: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481351141551896
      ],
      "excerpt": "In the following we list some of the most useful model options. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8766695640661054,
        0.9058495649153525
      ],
      "excerpt": "--transformer: To add transformer layers to the generator (GANformer) \n--components-num: Number of latent components, which will attend to the image. We recommend values in the range of 8-16 (default: 1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9385709310680446
      ],
      "excerpt": "--kmeans: Track and update image-to-latents assignment centroids, used in the duplex attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8736845542430962
      ],
      "excerpt": "--vis-layer-maps: Visualize attention maps of all layer and heads \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9147727806601766
      ],
      "excerpt": "--interpolation-density: Number of samples in between two end points of an interpolation (default: 8) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8831302818695869,
        0.9287862843996904,
        0.9599003890177379,
        0.9827266294258676
      ],
      "excerpt": "The GANformer consists of two networks: \nGenerator: which produces the images (x) given randomly sampled latents (z). The latent z has a shape [batch_size, component_num, latent_dim], where component_num = 1 by default (Vanilla GAN, StyleGAN) but is > 1 for the GANformer model. We can define the latent components by splitting z along the second dimension to obtain z_1,...,z_k latent components. The generator likewise consists of two parts: \n* Mapping network: converts sampled latents from a normal distribution (z) to the intermediate space (w). A series of Feed-forward layers. The k latent components either are mapped independently from the z space to the w space or interact with each other through self-attention (optional flag). \n* Synthesis network: the intermediate latents w are used to guide the generation of new images. Images features begin from a small constant/sampled grid of 4x4, and then go through multiple layers of convolution and up-sampling until reaching the desirable resolution (e.g. 256x256). After each convolution, the image features are modulated (meaning that their variance and bias are controlled) by the intermediate latent vectors w. While in the StyleGAN model there is one global w vectors that controls all the features equally. The GANformer uses attention so that the k latent components specialize to control different regions in the image to create it cooperatively, and therefore perform better especially in generating images depicting multi-object scenes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9686435567001186,
        0.9745588311293095
      ],
      "excerpt": "  * Simplex Attention: when attention is applied in one direction only from the latents to the image features (top-down). \n  * Duplex Attention: when attention is applied in the two directions: latents to image features (top-down) and then image features back to latents (bottom-up), so that each representation informs the other iteratively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9823421117457984,
        0.9907069686888111,
        0.9567440361415417,
        0.9452129883848231
      ],
      "excerpt": "  * Self Attention between image features (SAGAN model): prior approaches used attention directly between the image features, but this method does not scale well due to the quadratic number of features which becomes very high for high-resolutions. \nDiscriminator: Receives and image and has to predict whether it is real or fake \u2013 originating from the dataset or the generator. The model perform multiple layers of convolution and downsampling on the image, reducing the representation's resolution gradually until making final prediction. Optionally, attention can be incorporated into the discriminator as well where it has multiple (k) aggregator variables, that use attention to adaptively collect information from the image while being processed. We observe small improvements in model performance when attention is used in the discriminator, although note that most of the gain in using attention based on our observations arises from the generator. \nThis codebase builds on top of and extends the great StyleGAN2 repository by Karras et al.   \nThe GANformer model can also be seen as a generalization of StyleGAN: while StyleGAN has one global latent vector that control the style of all image features globally, the GANformer has k latent vectors, that cooperate through attention to control regions within the image, and thereby better modeling images of multi-object and compositional scenes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Generative Adversarial Transformers",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dorarad/gansformer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 105,
      "date": "Thu, 23 Dec 2021 16:27:09 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dorarad/gansformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dorarad/gansformer",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The model relies on custom TensorFlow ops that are compiled on the fly using [NVCC](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html). \n\nTo set up the environment e.g. for cuda-10.0:\n```python\nexport PATH=/usr/local/cuda-10.0/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda10.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n```\n\nTo test that your NVCC installation is working correctly, run:\n```python\nnvcc test_nvcc.cu -o test_nvcc -run\n| CPU says hello.\n| GPU says hello.\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We explored the GANformer model on 4 datasets for images and scenes: [CLEVR](https://cs.stanford.edu/people/jcjohns/clevr/), [LSUN-Bedrooms](https://www.yf.io/p/lsun), [Cityscapes](https://www.cityscapes-dataset.com/) and [FFHQ](https://github.com/NVlabs/ffhq-dataset). The model can be trained on other datasets as well.\nWe trained the model on `256x256` resolution. Higher resolutions are supported too. The model will automatically adapt to the resolution of the images in the dataset.\n\nThe [`prepare_data.py`](prepare_data.py) can either prepare the datasets from our catalog or create new datasets.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9328790330963598
      ],
      "excerpt": "\u2b1c\ufe0f Releasing Pytorch version (coming soon!) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9204101912277954
      ],
      "excerpt": "To prepare the datasets from the catalog, run the following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8116186927000949
      ],
      "excerpt": "| GAN        | 25.02        | 12.16        | 13.18      | 11.57      | \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.810114716705777
      ],
      "excerpt": ":white_check_mark: Training and data-prepreation intructions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.807839746566916
      ],
      "excerpt": "A minimal example of using a pre-trained GANformer can be found at generate.py. When executed, the 10-lines program downloads a pre-trained modle and uses it to generate some images: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9327544134343694
      ],
      "excerpt": "python generate.py --gpus 0 --model gdrive:bedrooms-snapshot.pkl --output-dir images --images-num 32 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9204066422405223
      ],
      "excerpt": "python prepare_data.py --ffhq --cityscapes --clevr --bedrooms --max-images 100000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9012658462549894,
        0.8262810700698885
      ],
      "excerpt": " --data-dir the output data directory (default: datasets) \n --shards-num to select the number of shards for the data (default: adapted to each dataset) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.850355295951606
      ],
      "excerpt": "Models are trained by using the --train option. To fine-tune a pretrained GANformer model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9492530196120881
      ],
      "excerpt": "python run_network.py --train --gpus 0 --ganformer-default --expname clevr-pretrained --dataset clevr \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9305730466472419
      ],
      "excerpt": "python run_network.py --train --gpus 0 --ganformer-default --expname clevr-scratch --dataset clevr \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.840256250272169
      ],
      "excerpt": "During training, sample images and attention maps will be generated and stored at results/<expname>-<run-id> (--keep-samples). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024935320214266
      ],
      "excerpt": "python run_network.py --train --gpus 0 --baseline GAN --expname clevr-gan --dataset clevr \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9072404628740258
      ],
      "excerpt": "python run_network.py --eval --gpus 0 --expname clevr-exp --dataset clevr \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8542745444758697
      ],
      "excerpt": "| GANformer | 9.24   | 6.15   | 7.42 | 5.23 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8871512941865499
      ],
      "excerpt": "python run_network.py --gpus 0 --eval --expname clevr-exp --dataset clevr --vis-layer-maps \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8430435057735859
      ],
      "excerpt": "--data-dir and --result-dir: Directory names for the datasets (tfrecords) and logging/results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8058557744993204
      ],
      "excerpt": "--num-heads: Number of attention heads (default: 1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003751159116907
      ],
      "excerpt": "Sample imaegs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8512296093570266
      ],
      "excerpt": "Run python run_network.py -h for the full options list. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dorarad/gansformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Drew Arad Hudson\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "GANformer: Generative Adversarial Transformers",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "gansformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dorarad",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dorarad/gansformer/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "dorarad",
        "body": "Official Implementation of the Generative Adversarial Transformers paper for image and compositional scene generation. \r\nThe codebase supports training, evaluation, image sampling, and variety of visualizations.",
        "dateCreated": "2021-03-17T14:08:41Z",
        "datePublished": "2021-03-17T14:11:44Z",
        "html_url": "https://github.com/dorarad/gansformer/releases/tag/v1.0",
        "name": "Generative Adversarial Transformers ",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/dorarad/gansformer/tarball/v1.0",
        "url": "https://api.github.com/repos/dorarad/gansformer/releases/39944500",
        "zipball_url": "https://api.github.com/repos/dorarad/gansformer/zipball/v1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<img align=\"right\" src=\"https://cs.stanford.edu/people/dorarad/dia.png\" width=\"190px\">\n\n- Python 3.6 or 3.7 are supported.\n- We recommend TensorFlow 1.14 which was used for development, but TensorFlow 1.15 is also supported.\n- The code was tested with CUDA 10.0 toolkit and cuDNN 7.5.\n- We have performed experiments on Titan V GPU. We assume 12GB of GPU memory (more memory can expedite training).\n- See [`requirements.txt`](requirements.txt) for the required python packages and run `pip install -r requirements.txt` to install them.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1031,
      "date": "Thu, 23 Dec 2021 16:27:09 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "transformers",
      "gans",
      "generative-adversarial-networks",
      "image-generation",
      "scene-generation",
      "compositionality",
      "attention"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<div align=\"center\">\n  <img src=\"https://cs.stanford.edu/people/dorarad/faces.png\" style=\"float:left\" width=\"750px\">\n  <br>\n  <img src=\"https://cs.stanford.edu/people/dorarad/bedroom.png\" style=\"float:left\" width=\"750px\">\n  <br>\n  <img src=\"https://cs.stanford.edu/people/dorarad/clevr_new.png\" style=\"float:left\" width=\"750px\">\n  <br>\n  <img src=\"https://cs.stanford.edu/people/dorarad/cities_small.png\" style=\"float:left\" width=\"750px\">\n</div>\n\n",
      "technique": "Header extraction"
    }
  ]
}