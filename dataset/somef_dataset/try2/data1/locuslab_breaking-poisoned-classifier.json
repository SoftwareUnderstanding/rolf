{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2010.09080\n\nIn this paper, we demonstrate a novel attack against backdoor poisoned classifiers.\n<p>\n<img src=\"github_figures/pipeline.png\" width=\"1000\">\nFigure 1: Overview of our attack. Given a poisoned classifier, we construct a *robustified smoothed*\nclassifier using *Denoised Smoothing* (Salman et al., 2020",
      "https://arxiv.org/abs/2003.01908",
      "https://arxiv.org/abs/1708.06733",
      "https://arxiv.org/abs/1910.00033",
      "https://arxiv.org/abs/2010.09080"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this repository useful for your research, please consider citing our work:\n```\n@inproceedings{sun2020poisoned,\n  title={Poisoned classifiers are not only backdoored, they are fundamentally broken},\n  author={Sun, Mingjie and Agarwal, Siddhant and Kolter, J. Zico},\n  booktitle={arXiv:2010.09080},\n  year={2020}\n}\n```",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{sun2020poisoned,\n  title={Poisoned classifiers are not only backdoored, they are fundamentally broken},\n  author={Sun, Mingjie and Agarwal, Siddhant and Kolter, J. Zico},\n  booktitle={arXiv:2010.09080},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8714162992508173,
        0.9991295298092048
      ],
      "excerpt": "Mingjie Sun, Siddhant Agarwal, J. Zico Kolter <br> \nPaper: https://arxiv.org/abs/2010.09080 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/locuslab/breaking-poisoned-classifier",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-16T07:12:40Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-10T01:21:31Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.985581620198092
      ],
      "excerpt": "This repository contains the code and pretrained models necessary to replicate the results in the following paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9139625887959111
      ],
      "excerpt": "Figure 1: Overview of our attack. Given a poisoned classifier, we construct a *robustified smoothed* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9359258979362727,
        0.9891785691729262
      ],
      "excerpt": "Figure 2: Results of attacking a poisoned classifier on ImageNet, where the attack success rate of the original backdoor  \nis 72.60%. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8502098582046952
      ],
      "excerpt": "To start with, clone the repository: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9869541257352838,
        0.835768273537832,
        0.8912833970835525,
        0.8038538505914371,
        0.9305185017214919
      ],
      "excerpt": "Our code is based on the open source code of Saha et al (2020) and Salman et al. (2020). The contents of this repository are as follows: \n<!-- * [code](code) contains the base code for attacking smoothed classifier. --> \nDirectory denoised-smoothing is the open source repository of the paper Denoised Smoothing.   \nDirectory code contains the code for breaking poisoned classifiers with three backdoor attack methods: BadNet, HTBA, CLBD, as well as attacking poisoned classifiers from the TrojAI dataset. \nIn code, there are four directories: badnet, clbd, htba and trojAI, where we attack different poisoned classifiers. In each directory, we provide a notebook breaking-poisoned-classifier.ipynb which contains a demonstration of our attack method. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for paper \"Poisoned classifiers are not only backdoored, they are fundamentally broken\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/locuslab/breaking-poisoned-classifier/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 28 Dec 2021 15:51:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/locuslab/breaking-poisoned-classifier/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "locuslab/breaking-poisoned-classifier",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/locuslab/breaking-poisoned-classifier/main/code/badnet/breaking-poisoned-classifier.ipynb",
      "https://raw.githubusercontent.com/locuslab/breaking-poisoned-classifier/main/code/clbd/breaking-poisoned-classifier.ipynb",
      "https://raw.githubusercontent.com/locuslab/breaking-poisoned-classifier/main/code/trojAI/breaking-poisoned-classifier.ipynb",
      "https://raw.githubusercontent.com/locuslab/breaking-poisoned-classifier/main/code/htba/breaking-poisoned-classifier.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8705869480997346,
        0.9798376452272237,
        0.9906248903846466,
        0.8270538936472002,
        0.8509099983432364
      ],
      "excerpt": "To start with, clone the repository: \ngit clone git@github.com:Eric-mingjie/breaking-poisoned-classifier.git \ncd breaking-poisoned-classifier \ngit submodule update --init --recursive  \nRemember to download the pretrained denoisers from Denoised Smoothing (Follow step 4 here). \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8509751115390626
      ],
      "excerpt": "<img src=\"github_figures/pipeline.png\" width=\"1000\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8799140251064418
      ],
      "excerpt": "<img src=\"github_figures/result.png\" width=\"1000\" align=\"center\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/locuslab/breaking-poisoned-classifier/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Breaking Poisoned Classifier",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "breaking-poisoned-classifier",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "locuslab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/locuslab/breaking-poisoned-classifier/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 17,
      "date": "Tue, 28 Dec 2021 15:51:02 GMT"
    },
    "technique": "GitHub API"
  }
}