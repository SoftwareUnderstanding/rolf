{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Data from [TartanAir](https://theairlab.org/tartanair-dataset/) was used to train our model. We additionally use evaluation tools from [evo](https://github.com/MichaelGrupp/evo) and [tartanair_tools](https://github.com/castacks/tartanair_tools).\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2108.10869"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{teed2021droid,\n  title={{DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras}},\n  author={Teed, Zachary and Deng, Jia},\n  journal={Advances in neural information processing systems},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.965348805649728,
        0.8100090190274344,
        0.9440663002812514
      ],
      "excerpt": "<!-- <center><img src=\"misc/DROID.png\" width=\"640\" style=\"center\"></center> --> \nDROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras \nZachary Teed and Jia Deng \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/princeton-vl/DROID-SLAM",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-24T22:41:58Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-26T09:57:47Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8148538177646754,
        0.908925214220865,
        0.9590334450977244
      ],
      "excerpt": "DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras \nZachary Teed and Jia Deng \nInitial Code Release: This repo currently provides a single GPU implementation of our monocular, stereo, and RGB-D SLAM systems. It currently contains demos, training, and evaluation scripts. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/princeton-vl/droid-slam/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 65,
      "date": "Sun, 26 Dec 2021 17:50:57 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/princeton-vl/DROID-SLAM/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "princeton-vl/DROID-SLAM",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/princeton-vl/droid-slam/main/thirdparty/tartanair_tools/TartanAir_Sample.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/princeton-vl/droid-slam/main/tools/evaluate_eth3d.sh",
      "https://raw.githubusercontent.com/princeton-vl/droid-slam/main/tools/evaluate_tum.sh",
      "https://raw.githubusercontent.com/princeton-vl/droid-slam/main/tools/validate_tartanair.sh",
      "https://raw.githubusercontent.com/princeton-vl/droid-slam/main/tools/download_sample_data.sh",
      "https://raw.githubusercontent.com/princeton-vl/droid-slam/main/tools/evaluate_euroc.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8949319496846044
      ],
      "excerpt": "Download the TartanAir dataset using the script thirdparty/tartanair_tools/download_training.py and put them in datasets/TartanAir \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8073179533444517
      ],
      "excerpt": "Download the EuRoC sequences (ASL format) and put them in datasets/EuRoC \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8309391638231081
      ],
      "excerpt": "Download the ETH3D dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8964642402448546,
        0.9089386425266153
      ],
      "excerpt": "First download the TartanAir dataset. The download script can be found in thirdparty/tartanair_tools/download_training.py. You will only need the rgb and depth data. \npython download_training.py --rgb --depth \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/princeton-vl/DROID-SLAM/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2020, Carnegie Mellon University\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n\\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DROID-SLAM",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DROID-SLAM",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "princeton-vl",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/princeton-vl/DROID-SLAM/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run the code you will need ...\n* **Inference:** Running the demos will require a GPU with at least 11G of memory. \n\n* **Training:** Training requires a GPU with at least 24G of memory. We train on 4 x RTX-3090 GPUs.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 531,
      "date": "Sun, 26 Dec 2021 17:50:57 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Clone the repo using the `--recursive` flag\n```Bash\ngit clone --recursive https://github.com/princeton-vl/DROID-SLAM.git\n```\n\n2. Creating a new anaconda environment using the provided .yaml file. Use `environment_novis.yaml` to if you do not want to use the visualization\n```Bash\nconda env create -f environment.yaml\npip install evo --upgrade --no-binary evo\npip install gdown\n```\n\n3. Compile the extensions (takes about 10 minutes)\n```Bash\npython setup.py install\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Download the model from google drive: [droid.pth](https://drive.google.com/file/d/1PpqVt1H4maBa_GbPJp4NwxRsd9jk-elh/view?usp=sharing)\n\n2. Download some sample videos using the provided script.\n```Bash\n./tools/download_sample_data.sh\n```\n\nRun the demo on any of the samples (all demos can be run on a GPU with 11G of memory). While running, press the \"s\" key to increase the filtering threshold (= more points) and \"a\" to decrease the filtering threshold (= fewer points).\n\n\n```Python\npython demo.py --imagedir=data/abandonedfactory --calib=calib/tartan.txt --stride=2\n```\n\n```Python\npython demo.py --imagedir=data/sfm_bench/rgb --calib=calib/eth.txt\n```\n\n```Python\npython demo.py --imagedir=data/Barn --calib=calib/barn.txt --stride=1 --backend_nms=4\n```\n\n```Python\npython demo.py --imagedir=data/mav0/cam0/data --calib=calib/euroc.txt --t0=150\n```\n\n```Python\npython demo.py --imagedir=data/rgbd_dataset_freiburg3_cabinet/rgb --calib=calib/tum3.txt\n```\n\n\n**Running on your own data:** All you need is a calibration file. Calibration files are in the form \n```\nfx fy cx cy [k1 k2 p1 p2 [ k3 [ k4 k5 k6 ]]]\n```\nwith parameters in brackets optional.\n\n",
      "technique": "Header extraction"
    }
  ]
}