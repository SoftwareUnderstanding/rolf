{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2010.01412",
      "https://arxiv.org/abs/2102.11600",
      "https://arxiv.org/abs/2010.01412"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/davda54/sam",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-25T20:46:29Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-25T06:18:33Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9069023766694615
      ],
      "excerpt": "  inputs, targets = (b.to(device) for b in batch) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8669833762783922
      ],
      "excerpt": "  with model.no_sync():  #: <- this is the important line \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9769792774831499
      ],
      "excerpt": "| params (iterable) | iterable of parameters to optimize or dicts defining parameter groups | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9783235481401541
      ],
      "excerpt": "| **kwargs | keyword arguments passed to the __init__ method of base_optimizer | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156070444464878
      ],
      "excerpt": "Performs the first optimization step that finds the weights with the highest loss in the local rho-neighborhood. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023204600127737
      ],
      "excerpt": "Performs the second optimization step that updates the original weights with the gradient from the (locally) highest point in the loss landscape. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9228325167585233
      ],
      "excerpt": "| closure (callable) | the closure should do an additional full forward and backward pass on the optimized model (default: None) | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8719081547437213
      ],
      "excerpt": "I've verified that SAM works on a simple WRN 16-8 model run on CIFAR10; you can replicate the experiment by running train.py. The Wide-ResNet is enhanced only by label smoothing and the most basic image augmentations with cutout, so the errors are higher than those in the SAM paper. Theoretically, you can get even lower errors by running for longer (1800 epochs instead of 200), because SAM shouldn't be as prone to overfitting. SAM uses rho=0.05, while ASAM is set to rho=2.0, as suggested by its authors. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "SAM: Sharpness-Aware Minimization (PyTorch)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/davda54/sam/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 99,
      "date": "Sat, 25 Dec 2021 22:01:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/davda54/sam/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "davda54/sam",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8013441476332323
      ],
      "excerpt": "@hjq133: The suggested usage can potentially cause problems if you use batch normalization. The running statistics are computed in both forward passes, but they should be computed only for the first one. A possible solution is to set BN momentum to zero (kindly suggested by @ahmdtaha) to bypass the running statistics during the second pass. An example usage is on lines 51 and 58 in example/train.py: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8347931097375236
      ],
      "excerpt": "for batch in dataset.train: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  optimizer.first_step(zero_grad=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  optimizer.second_step(zero_grad=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8452751543426027
      ],
      "excerpt": "for input, output in data: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8310380106999183
      ],
      "excerpt": "  loss = loss_function(output, model(input)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  optimizer.first_step(zero_grad=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8719491278656056,
        0.8594142235991984
      ],
      "excerpt": "  loss_function(output, model(input)).backward() \n  optimizer.second_step(zero_grad=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8018472484793263
      ],
      "excerpt": "| Optimizer             | Test error rate | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/davda54/sam/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 David Samuel\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Usage",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sam",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "davda54",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/davda54/sam/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 817,
      "date": "Sat, 25 Dec 2021 22:01:45 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "optimizer",
      "pytorch",
      "sam",
      "sharpness-aware"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "It should be straightforward to use SAM in your training pipeline. Just keep in mind that the training will run twice as slow, because SAM needs two forward-backward passes to estime the \"sharpness-aware\" gradient. If you're using gradient clipping, make sure to change only the magnitude of gradients, not their direction.\n\n```python\nfrom sam import SAM\n...\n\nmodel = YourModel()\nbase_optimizer = torch.optim.SGD  #: define an optimizer for the \"sharpness-aware\" update\noptimizer = SAM(model.parameters(), base_optimizer, lr=0.1, momentum=0.9)\n...\n\nfor input, output in data:\n\n  #: first forward-backward pass\n  loss = loss_function(output, model(input))  #: use this loss for any training statistics\n  loss.backward()\n  optimizer.first_step(zero_grad=True)\n  \n  #: second forward-backward pass\n  loss_function(output, model(input)).backward()  #: make sure to do a full forward pass\n  optimizer.second_step(zero_grad=True)\n...\n```\n\n<br>\n\n**Alternative usage with a single closure-based `step` function**. This alternative offers similar API to native PyTorch optimizers like LBFGS (kindly suggested by [@rmcavoy](https://github.com/rmcavoy)):\n\n```python\nfrom sam import SAM\n...\n\nmodel = YourModel()\nbase_optimizer = torch.optim.SGD  #: define an optimizer for the \"sharpness-aware\" update\noptimizer = SAM(model.parameters(), base_optimizer, lr=0.1, momentum=0.9)\n...\n\nfor input, output in data:\n  def closure():\n    loss = loss_function(output, model(input))\n    loss.backward()\n    return loss\n\n  loss = loss_function(output, model(input))\n  loss.backward()\n  optimizer.step(closure)\n  optimizer.zero_grad()\n...\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}