{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* I want to thank Frank, Kayla and Danny for the guidance and support.\n\n* Dataset is from [Kaggle](https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics).\n\n* Thanks for the tutorial [GPU-accelerated Deep Learning on Windows 10](https://github.com/philferriere/dlwin).\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. [Distributed Representations of Sentences and Documents](https://arxiv.org/pdf/1405.4053.pdf)\n2. [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)\n3. [Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf)\n4. [Attention Is All You Need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\n5. [Importance Sampling](https://arxiv.org/pdf/1803.00942.pdf)\n6. [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9479903223591607
      ],
      "excerpt": "| index | song            | year | artist  | genre | lyrics                                            | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9431939338322127
      ],
      "excerpt": "| 0     | ego-remix       | 2009 | beyonce | Pop   | Oh baby, how you doing?\\nYou know I'm gonna cu... | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9263184920282503
      ],
      "excerpt": "| 2     | honesty         | 2009 | beyonce | Pop   | If you search\\nFor tenderness\\nIt isn't hard t... | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266,
        0.9030859728368266
      ],
      "excerpt": "lstm_1 (LSTM)                (None, 10, 200)           230400     \ndropout_1 (Dropout)          (None, 10, 200)           0          \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266,
        0.9030859728368266,
        0.9030859728368266
      ],
      "excerpt": "embedding_1 (Embedding)      (None, 10, 100)           600500 \nlstm_1 (LSTM)                (None, 10, 300)           481200 \ndropout_1 (Dropout)          (None, 10, 300)           0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266,
        0.9030859728368266,
        0.9030859728368266
      ],
      "excerpt": "embedding_1 (Embedding)      (None, 10, 100)           600500 \nlstm_1 (LSTM)                (None, 10, 250)           351000 \ndropout_1 (Dropout)          (None, 10, 250)           0 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/slme1109/Lyrics_Generator_Using_LSTM",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-22T07:08:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-01T07:06:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8767627939209865
      ],
      "excerpt": "2. Exploratory data analysis \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9280836676224651
      ],
      "excerpt": "1. How to prepare the data and train the model? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8450908969520674
      ],
      "excerpt": "3. How to improve word-level model? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.959928425515791
      ],
      "excerpt": "Natural language processing is among the most attractive and difficult field in machine learning. Different from computer vision and other machine learning tasks, NLP does not convey meaning through any physical manifestation. By the virtue of deep learning, NLP achieved tremendous progress in keyword search, machine translation, semantic analysis and etc. In this project, I would like to make a lyrics generator by using both character level and word level RNN(recurrent neural network). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9962888911994802
      ],
      "excerpt": "Unlike other machine learning tasks, there is not much visualization we can do with NLP. And data cleaning will be based on the model that we want to explore. For character-based model, it is necessay to keep punctuations as they are part of the characters. However, for word-based model, punctuations are supposed to be removed. To briefly explore the dataset, we will count most frequently used words for different artists (bag of words or unigram). Consequently, very crude sentiment score can be obtained from it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9516668253507258
      ],
      "excerpt": "Sentiment score base on bag-of-words \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9785414701151913,
        0.9877086543275304
      ],
      "excerpt": "Based on bag of words, one can do naive bayes to predict the genre of songs, but we will not cover it here. Let us go deeper to deep learning. \nWhy recurrent? Different from vanilla neural network, RNN (see below, pic from wikipedia) is able to process sequences of inputs (such as words and sentences) by utilizing the internel state (memory state). Hence, it is regarded as a very promising candidate to solve NLP tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9809770153105825
      ],
      "excerpt": "Inspired by the minimal character-level Vanilla RNN model from Andrej Karpathy, we decided to build a more complicated RNN model to generate lyrics. Below is the summary of my model: 2 LSTM layers and 1 dense layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577302739126875
      ],
      "excerpt": "dense_1 (Dense)              (None, 87)                17487 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8208303634388457
      ],
      "excerpt": "After 600 epochs, the model achieved 64% accuracy for validation set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369972944948917
      ],
      "excerpt": "Feed the new seed lyrics into the neural network and iterate the above process as many as you want. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8687304021723276,
        0.8646641170395889
      ],
      "excerpt": "In the end, you might get something like 'I want to be king of pop' \nThis process is known as teacher forcing: training neural network that uses model output from a prior time step as an input. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8006638920984674
      ],
      "excerpt": "Not smart, it repeats the same words over and over. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9293471104180626
      ],
      "excerpt": "input: 'not afraid' output: ') To take a stand) Maybe a dontte the back of the way that I know what the fuck I say to the motherf' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9772994063039651,
        0.9908559032645379
      ],
      "excerpt": "It is amazing as the generator can spell the word correctly and it is not hard to tell that they have eminem style. \nInstead of letting the model learning how to spell words. One can upgrade the model from character-level to word-level. Correspondingly, this endows model the ability to learning semantics from the corpus. Since the number of unique words is much larger than that of characters, it is necessay to introduce a new representation: word embedding. This is basically the only difference from character-based model. However, there is much more wisdom than just dimension reduction. The notion was first referred by Misolov, et al.,2013. Word embedding rotates word vector from one-hot representation to word2vec representation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577302739126875,
        0.8577302739126875
      ],
      "excerpt": "dense_1 (Dense)              (None, 300)               90300 \ndense_2 (Dense)              (None, 6005)              1807505 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9621652820971586
      ],
      "excerpt": "Training is much harder than character-base model. Only 32% accuracy is obtained from this model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8454109668480412,
        0.8050228133281858
      ],
      "excerpt": "my heart in pieces lost my heart on the carousel to \nGenerate 40-word: a circus girl who left my heart in pieces lost my heart on the carousel to a circus girl who left my heart in pieces lost my heart on the carousel to a circus girl who left my heart in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9713038117334064,
        0.8362128967031464
      ],
      "excerpt": "It generates something with correct grammar at some parts but it is hard to understand the meaning. \nWord-level RNN is essentially concatenation of two neural networks. If we train two parts separately, we should achieve better accuracy. However, after using a pretrained word embedding, the accuracy of validation set decreases to 20%. Such a counterintuitive result! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = Sequential() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577302739126875
      ],
      "excerpt": "dense_1 (Dense)              (None, 6005)              1507255 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8237192770859669,
        0.993714522805055
      ],
      "excerpt": "There are 600,500 non-trainable parameters which are from pretrained word embedding. Using non-trainable embedding seems not working well. Maybe it would be better to train the word embedding particularlly for the dataset by using CBOW or skip-gram. \nSeq2Seq model was widely used in neural tranlation machine. But there is nothing wrong to apply it to lyrics generator. The basic idea is to process input in the encoder end and generate a memory state (a vector) that represents the whole input message. Decoder take the memory state and SOS token to generate one token. Generated token becomes the next input for decoder to predict the next token. The process iterates many cycles until EOS is generated or max length of output is reached. Accordingly, unlike many-to-one model, seq2seq model has advantage of generating more than one token. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8857236748614811,
        0.9394449182630016
      ],
      "excerpt": "However, the performance does not change by much. But from recently research, inplementing attention mechanism could significantly improve the performance. Attention mechanism not only resolves long-term dependency problem of vanilla Seq2Seq languange model but also speeds up training process as it dicarded RNN which disfavors parallel computation. \nTakeaways for tunning hyperparameters: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9558839113428323,
        0.9419716403454557,
        0.936495401433279,
        0.849763978479079
      ],
      "excerpt": "It is easy to overfit the model. It is necessay to add Dropout layer after each LSTM layer. \nSometimes GRU is better than LSTM and computationally cheaper. \nInitialization and luck are very important. Try to restart kernel if model is stuck at local minimum. \nTry importance sampling which randomly takes samples from distrution instead of feeding datapoints in order. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826112424467009
      ],
      "excerpt": "Character-based models perform better than word-base. Even though word embedding is a very innovative method in NLP, word vectors by itself hardly convey semantics efficiently. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8685734587188249,
        0.8619804226218288
      ],
      "excerpt": "Try negative sampling to boost the training and improve the metrics (better than softmax). \nImplement attention mechanism to seq2seq model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Data Science Immersive - Galvanize Capstone Project II",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/slme1109/Lyrics_Generator_Using_LSTM/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 22:10:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/slme1109/Lyrics_Generator_Using_LSTM/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "slme1109/Lyrics_Generator_Using_LSTM",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* One-hot encode all characters ( a-z, 0-9 and punctuations ``` !\"#:$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n ``` )\n    ```python\n    from keras.utils import to_categorical\n    #:[1,2,3] -> [[1,0,0], [0,1,0], [0,0,1]]\n    one-hot_X = to_categorical(X, num_classes=vocab_size)    \n    ```\n* Make a sliding window that collects 10 characters as input. Model only generates one character.\n    ```\n    input: 'hello, world' -> output: 'n', true output: 'p'\n    ```\n* Calculate cross entropy and backpropagate the neural network to update 568,687 parameters.\n* Slide the window to the right by one character. Extract new input and iterate above process until cost function reaches the minimum.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8687931612094828
      ],
      "excerpt": "| 3     | you-are-my-rock | 2009 | beyonce | Pop   | Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote... | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8876600310044139
      ],
      "excerpt": "input: 'lose myself'    output: ' in the music, the moment You own it, you better never let it go You only get o' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8032028198730533
      ],
      "excerpt": "Generate 100-word: little start i go a i am away im dreams of this life girl girl tell me fall im together so much i feel you all i say that you say into into i lost my heart i can and you game on me baby ill see i can be far away today i love you from your truth cause youre across the bitch baby does it feel it needs me from a door start that not here there was ghost of moon aint a you better he made the you she win your dreams off the madness i never \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8337521426974137
      ],
      "excerpt": "<img src=\"images/neural.jpeg\" ></img> | <img src=\"images/eminem.jpg\" ></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9295932733615537
      ],
      "excerpt": "<img src=\"images/genre.jpg\" > \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9190443536166801
      ],
      "excerpt": "from sklearn.feature_extraction.text import CountVectorizer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8831347796305735
      ],
      "excerpt": "<img src=\"images/mj.jpg\"  ></img> | <img src=\"images/eminem2.jpg\"  ></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8819156704466504
      ],
      "excerpt": "<div align=center><img src=\"images/rnn.png\" width=\"100%\" ></div> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.906614234580968
      ],
      "excerpt": "Layer (type)                 Output Shape              Param #:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934745846565956
      ],
      "excerpt": "Total params: 568,687 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.875232135894237
      ],
      "excerpt": "<div align=center><img src=\"images/word2vec.png\" width=\"80%\" ></div> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.906614234580968
      ],
      "excerpt": "Layer (type)                 Output Shape              Param #: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934745846565956
      ],
      "excerpt": "Total params: 3,700,705 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8227903402298582
      ],
      "excerpt": "model.add(Embedding(num_of_tokens, latent_dim, input_length=seq_length, weights=[pretrained_embedding], train = False)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.906614234580968
      ],
      "excerpt": "Layer (type)                 Output Shape              Param #: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934745846565956
      ],
      "excerpt": "Total params: 2,959,755 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.875232135894237
      ],
      "excerpt": "<div align=center><img src=\"images/seq2seq.png\" width=\"80%\" ></div> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/slme1109/Lyrics_Generator_Using_LSTM/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Lyrics generator\n\n## Table of Contents\n\n<!-- vim-markdown-toc GFM -->\n\n* [Introduction](#introduction)\n    * [1. Motivation](#1-motivation)\n    * [2. Exploratory data analysis](#2-exploratory-data-analysis)\n        * [Bag-of-words](#bag-of-words)\n* [Character-level recurrent neural network(RNN)](#character-level-recurrent-neural-networkrnn)\n    * [1. How to prepare the data and train the model?](#1-how-to-prepare-the-data-and-train-the-model)\n    * [2. How to generate lyrics? Teacher forcing](#2-how-to-generate-lyrics-teacher-forcing)\n    * [3. Eminem's lyrics generator](#3-eminems-lyrics-generator)\n* [Word-level RNN](#word-level-rnn)\n    * [1. Word embedding and word2vec](#1-word-embedding-and-word2vec)\n    * [2. Michael Jackson's lyrics generator based on word-level RNN](#2-michael-jacksons-lyrics-generator-based-on-word-level-rnn)\n    * [3. How to improve word-level model?](#3-how-to-improve-word-level-model)\n        * [a. Using pretrained word embedding](#a-using-pretrained-word-embedding)\n        * [b. Word-level seq2seq model](#b-word-level-seq2seq-model)\n* [Concluding remarks](#concluding-remarks)\n* [Reference](#reference)\n* [Acknowledgement](#acknowledgement)\n\n<!-- vim-markdown-toc -->\n\n## Introduction\n\n### 1. Motivation\n\nNatural language processing is among the most attractive and difficult field in machine learning. Different from computer vision and other machine learning tasks, NLP does not convey meaning through any physical manifestation. By the virtue of deep learning, NLP achieved tremendous progress in keyword search, machine translation, semantic analysis and etc. In this project, I would like to make a lyrics generator by using both character level and word level RNN(recurrent neural network).\n\n\nNeural network                        | Lyric generator\n:-------------------------:           | :-------------------------:\n<img src=\"images/neural.jpeg\" ></img> | <img src=\"images/eminem.jpg\" ></img>\n\nThe dataset is from kaggle with 3.8 million song lyrics from various artist.\n\n| index | song            | year | artist  | genre | lyrics                                            |\n|-------|-----------------|------|---------|-------|---------------------------------------------------|\n| 0     | ego-remix       | 2009 | beyonce | Pop   | Oh baby, how you doing?\\nYou know I'm gonna cu... |\n| 1     | then-tell-me    | 2009 | beyonce | Pop   | playin' everything so easy,\\nit's like you see... |\n| 2     | honesty         | 2009 | beyonce | Pop   | If you search\\nFor tenderness\\nIt isn't hard t... |\n| 3     | you-are-my-rock | 2009 | beyonce | Pop   | Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote... |\n| 4     | black-culture   | 2009 | beyonce | Pop   | Party the people, the people the party it's po... |\n\n<img src=\"images/genre.jpg\" >\n\nBut I will only use lyrics from Eminem and Michael Jackson. Because they have around 400 songs, it is easier to extract regular patterns from them.\n\n### 2. Exploratory data analysis\n\nUnlike other machine learning tasks, there is not much visualization we can do with NLP. And data cleaning will be based on the model that we want to explore. For character-based model, it is necessay to keep punctuations as they are part of the characters. However, for word-based model, punctuations are supposed to be removed. To briefly explore the dataset, we will count most frequently used words for different artists (bag of words or unigram). Consequently, very crude sentiment score can be obtained from it.\n\n#### Bag-of-words\nRemove stop words and punctuations from lyrics\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(stop_words='english')\nX = vectorizer.fit_transform([eminem_lyrics])\n```\n\nSentiment score base on bag-of-words\n```python\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\ndef sentiment(eminem_lyrics):\n    sentiment = SentimentIntensityAnalyzer()\n    score = sentiment.polarity_scores(eminem_lyrics)\n    return score\n```\n\nMichael Jackson                   | Eminem\n:-------------------------:       | :-------------------------:\n<img src=\"images/mj.jpg\"  ></img> | <img src=\"images/eminem2.jpg\"  ></img>\nsentiment score: 1.0              | sentiment score: -1.0\n\nBased on bag of words, one can do naive bayes to predict the genre of songs, but we will not cover it here. Let us go deeper to deep learning.\n\n\n## Character-level recurrent neural network(RNN)\n\nWhy recurrent? Different from vanilla neural network, RNN (see below, pic from wikipedia) is able to process sequences of inputs (such as words and sentences) by utilizing the internel state (memory state). Hence, it is regarded as a very promising candidate to solve NLP tasks.\n\n<div align=center><img src=\"images/rnn.png\" width=\"100%\" ></div>\n\nInspired by the [minimal character-level Vanilla RNN model](https://gist.github.com/karpathy/d4dee566867f8291f086) from Andrej Karpathy, we decided to build a more complicated RNN model to generate lyrics. Below is the summary of my model: 2 LSTM layers and 1 dense layer.\n```\nLayer (type)                 Output Shape              Param #",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Lyrics_Generator_Using_LSTM",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "slme1109",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/slme1109/Lyrics_Generator_Using_LSTM/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 22:10:33 GMT"
    },
    "technique": "GitHub API"
  }
}