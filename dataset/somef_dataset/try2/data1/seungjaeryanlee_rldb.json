{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1312.5602",
      "https://arxiv.org/abs/1507.06527",
      "https://arxiv.org/abs/1507.04296",
      "https://arxiv.org/abs/1509.06461",
      "https://arxiv.org/abs/1511.05952",
      "https://arxiv.org/abs/1511.06581",
      "https://arxiv.org/abs/1706.10295",
      "https://arxiv.org/abs/1707.06887",
      "https://arxiv.org/abs/1710.02298",
      "https://arxiv.org/abs/1710.10044",
      "https://arxiv.org/abs/1806.06923",
      "https://arxiv.org/abs/1803.00933",
      "https://arxiv.org/abs/1602.01783",
      "https://arxiv.org/abs/1502.05477",
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1708.05144",
      "https://arxiv.org/abs/1802.09477",
      "https://arxiv.org/abs/1802.01561",
      "https://arxiv.org/abs/1704.04651",
      "https://arxiv.org/abs/1810.12894",
      "https://arxiv.org/abs/1707.01891"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DBLP:journals/corr/FortunatoAPMOGM17,\n    author    = {Meire Fortunato and\n                 Mohammad Gheshlaghi Azar and\n                 Bilal Piot and\n                 Jacob Menick and\n                 Ian Osband and\n                 Alex Graves and\n                 Vlad Mnih and\n                 R{\\'{e}}mi Munos and\n                 Demis Hassabis and\n                 Olivier Pietquin and\n                 Charles Blundell and\n                 Shane Legg},\n    title     = {Noisy Networks for Exploration},\n    journal   = {CoRR},\n    volume    = {abs/1706.10295},\n    year      = {2017},\n    url       = {http://arxiv.org/abs/1706.10295},\n    archivePrefix = {arXiv},\n    eprint    = {1706.10295},\n    timestamp = {Mon, 13 Aug 2018 16:46:11 +0200},\n    biburl    = {https://dblp.org/rec/bib/journals/corr/FortunatoAPMOGM17},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DBLP:journals/corr/WangFL15,\\n'\n                     '    author    = {Ziyu Wang and\\n'\n                     '                 Nando de Freitas and\\n'\n                     '                 Marc Lanctot},\\n'\n                     '    title     = {Dueling Network Architectures for Deep '\n                     'Reinforcement Learning},\\n'\n                     '    journal   = {CoRR},\\n'\n                     '    volume    = {abs/1511.06581},\\n'\n                     '    year      = {2015},\\n'\n                     '    url       = {http://arxiv.org/abs/1511.06581},\\n'\n                     '    archivePrefix = {arXiv},\\n'\n                     '    eprint    = {1511.06581},\\n'\n                     '    timestamp = {Mon, 13 Aug 2018 16:48:17 +0200},\\n'\n                     '    biburl    = '\n                     '{https://dblp.org/rec/bib/journals/corr/WangFL15},\\n'\n                     '    bibsource = {dblp computer science bibliography, '\n                     'https://dblp.org}\\n'\n                     '}',\n    'source-nickname': 'DuDQN',\n    'source-title': 'Dueling Network Architectures for Deep Reinforcement '\n                    'Learning'\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9906856324374771
      ],
      "excerpt": "    \"source-title\": \"\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9536736855944256
      ],
      "excerpt": "\"source-bibtex\": \"\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9813161029725735
      ],
      "excerpt": "\"algo-title\": \"\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906856324374771
      ],
      "excerpt": "\"algo-source-title\": \"\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9813161029725735
      ],
      "excerpt": "\"env-title\": \"\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9717931768858122
      ],
      "excerpt": "source-bibtex is a BibTeX-format citation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8561235956062729
      ],
      "excerpt": "[x] Human-level control through deep reinforcement learning (Mnih et al., 2015) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452861872232974,
        0.8889430772208565,
        0.9712438395444837,
        0.9941546430985363,
        0.9939812286046852,
        0.9962337287403669,
        0.9723838714490726,
        0.9935874209287872,
        0.9911488790035612,
        0.9861586135483766,
        0.9810716336804474,
        0.9951559417369137,
        0.9876379512277085,
        0.9989372956866087,
        0.9392298261051337,
        0.9911959320294913,
        0.9987287711509865,
        0.9466778778845187,
        0.9876995381561243
      ],
      "excerpt": "[x] Massively Parallel Methods for Deep Reinforcement Learning (Nair et al., 2015) \n[x] Deep Reinforcement Learning with Double Q-learning (Hasselt et al., 2015) \n[x] Prioritized Experience Replay (Schaul et al., 2015) \n[x] Dueling Network Architectures for Deep Reinforcement Learning (Wang et al., 2015) \n[x] Noisy Networks for Exploration (Fortunato et al., 2017) \n[x] A Distributional Perspective on Reinforcement Learning (Bellemare et al., 2017) \n[x] Rainbow: Combining Improvements in Deep Reinforcement Learning (Hessel et al., 2017) \n[x] Distributional Reinforcement Learning with Quantile Regression (Dabney et al., 2017) \n[x] Implicit Quantile Networks for Distributional Reinforcement Learning (Dabney et al., 2018) \n[x] Distributed Prioritized Experience Replay (Horgan et al., 2018) \n[x] Asynchronous Methods for Deep Reinforcement Learning (Mnih et al., 2016) \n[x] Trust Region Policy Optimization (Schulman et al., 2015) \n[x] Proximal Policy Optimization Algorithms (Schulman et al., 2017) \n[x] Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (Wu et al., 2017) \n[x] Addressing Function Approximation Error in Actor-Critic Methods (Fujimoto et al., 2018) \n[x] IMPALA: Importance Weighted Actor-Learner Architectures (Espeholt et al., 2018) \n[x] The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning (Gruslys et al., 2017) \n[x] Exploration by Random Network Distillation (Burda et al., 2018) \n[x] Trust-PCL: An Off-Policy Trust Region Method for Continuous Control (Nachum et al., 2017) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/seungjaeryanlee/rldb",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "How to Contribute\nThank you for your interest in helping us improve rldb!\nTesting\nWe use pytest to verify our package. Note that we separate our tests from our application code. Thus, you have two ways of running tests.\nThe first is installing rldb with the --editable flag. This enables you to use the pytest command.\nbash\npip install --editable .\npytest\nThe second is running pytest with the following command.\nbash\npython -m pytest",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-18T08:54:24Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-04T12:46:15Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8784978581893831
      ],
      "excerpt": "Database of RL algorithms \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9100563403471226
      ],
      "excerpt": "Here is the format of every entry: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051947561933855
      ],
      "excerpt": "algo-title is the full title of the algorithm used. algo-nickname is the nickname or acronym for that algorithm if it exists, otherwise it is the same as algo-nickname. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9753618544476719
      ],
      "excerpt": "For example, the Space Invaders score of Asynchronous Advantage Actor Critic (A3C) algorithm in the Noisy Networks for Exploration (NoisyNet) paper is represented by the following entry: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8728807487029934
      ],
      "excerpt": "[x] Playing Atari with Deep Reinforcement Learning (Mnih et al., 2013) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9667645468005098,
        0.827385793411461,
        0.8777359802304662
      ],
      "excerpt": "[x] Deep Recurrent Q-Learning for Partially Observable MDPs (Hausknecht and Stone, 2015) \n[x] Massively Parallel Methods for Deep Reinforcement Learning (Nair et al., 2015) \n[x] Deep Reinforcement Learning with Double Q-learning (Hasselt et al., 2015) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8371338456321892
      ],
      "excerpt": "[x] Dueling Network Architectures for Deep Reinforcement Learning (Wang et al., 2015) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8252085679092339
      ],
      "excerpt": "[x] Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (Wu et al., 2017) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Performances of Reinforcement Learning Agents",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/seungjaeryanlee/rldb/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 22 Dec 2021 00:33:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/seungjaeryanlee/rldb/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "seungjaeryanlee/rldb",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/seungjaeryanlee/rldb/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9043929270298233
      ],
      "excerpt": "    \"source-nickname\": \"\", \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/seungjaeryanlee/rldb/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Seungjae Ryan Lee\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "rldb",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "rldb",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "seungjaeryanlee",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/seungjaeryanlee/rldb/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 39,
      "date": "Wed, 22 Dec 2021 00:33:03 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "python3",
      "reinforcement-learning",
      "state-of-the-art",
      "database"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can use `rldb.find_all({})` to retrieve all existing entries in `rldb`.\n\n```python\nimport rldb\n\n\nall_entries = rldb.find_all({})\n```\n\nYou can also filter entries by specifying key-value pairs that the entry must match:\n\n```python\nimport rldb\n\n\ndqn_entries = rldb.find_all({'algo-nickname': 'DQN'})\nbreakout_noop_entries = rldb.find_all({\n    'env-title': 'atari-breakout',\n    'env-variant': 'No-op start',\n})\n```\n\nYou can also use `rldbl.find_one(filter_dict)` to find one entry that matches the key-value pair specified in `filter_dict`:\n\n```python\nimport rldb\nimport pprint\n\n\nentry = rldb.find_one({\n    'env-title': 'atari-pong',\n    'algo-title': 'Human',\n})\npprint.pprint(entry)\n```\n\n\n<details><summary>Output</summary>\n<p>\n\n```python\n{\n    'algo-nickname': 'Human',\n    'algo-title': 'Human',\n    'env-title': 'atari-pong',\n    'env-variant': 'No-op start',\n    'score': 14.6,\n    'source-arxiv-id': '1511.06581',\n    'source-arxiv-version': 3,\n    'source-authors': [   'Ziyu Wang',\n                          'Tom Schaul',\n                          'Matteo Hessel',\n                          'Hado van Hasselt',\n                          'Marc Lanctot',\n                          'Nando de Freitas'],\n    'source-bibtex': '@article{DBLP:journals/corr/WangFL15,\\n'\n                     '    author    = {Ziyu Wang and\\n'\n                     '                 Nando de Freitas and\\n'\n                     '                 Marc Lanctot},\\n'\n                     '    title     = {Dueling Network Architectures for Deep '\n                     'Reinforcement Learning},\\n'\n                     '    journal   = {CoRR},\\n'\n                     '    volume    = {abs/1511.06581},\\n'\n                     '    year      = {2015},\\n'\n                     '    url       = {http://arxiv.org/abs/1511.06581},\\n'\n                     '    archivePrefix = {arXiv},\\n'\n                     '    eprint    = {1511.06581},\\n'\n                     '    timestamp = {Mon, 13 Aug 2018 16:48:17 +0200},\\n'\n                     '    biburl    = '\n                     '{https://dblp.org/rec/bib/journals/corr/WangFL15},\\n'\n                     '    bibsource = {dblp computer science bibliography, '\n                     'https://dblp.org}\\n'\n                     '}',\n    'source-nickname': 'DuDQN',\n    'source-title': 'Dueling Network Architectures for Deep Reinforcement '\n                    'Learning'\n}\n```\n\n</p>\n</details>\n\n",
      "technique": "Header extraction"
    }
  ]
}