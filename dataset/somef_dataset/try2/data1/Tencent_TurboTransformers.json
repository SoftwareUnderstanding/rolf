{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1909.11942",
      "https://arxiv.org/abs/1907.11692"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Cite this paper, if you use TurboTransformers in your research publication.\n\n```\n@inproceedings{fang2021turbotransformers,\n  title={TurboTransformers: an efficient GPU serving system for transformer models},\n  author={Fang, Jiarui and Yu, Yang and Zhao, Chengduo and Zhou, Jie},\n  booktitle={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},\n  pages={389--402},\n  year={2021}\n}\n```\n\nThe artifacts of the paper can be found at branch `ppopp21_artifact_centos`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{fang2021turbotransformers,\n  title={TurboTransformers: an efficient GPU serving system for transformer models},\n  author={Fang, Jiarui and Yu, Yang and Zhao, Chengduo and Zhou, Jie},\n  booktitle={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},\n  pages={389--402},\n  year={2021}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Tencent/TurboTransformers",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Although we recommend you post your problem with github issues, you can also join in our Turbo user group.\n1. Scan this [QR code](./images/namecode.pdf \"qrcode\") and add our contactor as your WeChat friend.\n2. QQ Group, Name: TurboTransformers, Number : 1109315167.\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-20T09:44:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T02:53:16Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8633383641178435
      ],
      "excerpt": "The WeChat AI open-sourced TurboTransformers with the following characteristics. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9696251463858967,
        0.9321024341072279
      ],
      "excerpt": "Smart Batching. Minimize zero-padding overhead for a batch of requests of different lengths. \nIt can be used as a plugin for pytorch. Tthe end-to-end acceleration is obtained by adding a few lines of python code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8874843065430452
      ],
      "excerpt": "For example, It brings 1.88x acceleration to the WeChat FAQ service, 2.11x acceleration to the public cloud sentiment analysis service, and 13.6x acceleration to the QQ recommendation system. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9957373231550063
      ],
      "excerpt": "The following table is a comparison of TurboTransformers and related work. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8666924731293799
      ],
      "excerpt": "We currently support the following transformer models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.860059181823877
      ],
      "excerpt": "    model = transformers.BertModel.from_pretrained(model_id) \n    model.eval() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "torch_res = model( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "tt_model = turbo_transformers.BertModel.from_torch(model) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9574563696575022
      ],
      "excerpt": "We also prepared a docker image containing CPU version of TurboTransformers, as well as other related works, i.e. onnxrt v1.2.0 and pytorch-jit on dockerhub \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8520424472112704
      ],
      "excerpt": "We also prepared a docker image containing GPU version of TurboTransformers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8340899861438997,
        0.9493367265096909,
        0.9264470070734269
      ],
      "excerpt": "The first step in using turbo is to load a pre-trained model. We provide a way to load pytorch and tensorflow pre-trained models in huggingface/transformers. \nThe specific conversion method is to use the corresponding script in ./tools to convert the pre-trained model into an npz format file, and turbo uses the C ++ or python interface to load the npz format model. \nIn particular, we consider that most of the pre-trained models are in PyTorch format and used with python. We provide a shortcut for calling directly in python for the PyTorch saved model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9526779556000062
      ],
      "excerpt": "Since the user of BERT acceleration always requires a customized post-processing process for the task, we provide an example of how to write a sequence classification application. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9356440314947314
      ],
      "excerpt": "Our example provides the GPU and two CPU multi-thread calling methods. One is to do one BERT inference using multiple threads; the other is to do multiple BERT inference, each of which using one thread. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9853420056461544,
        0.8905648692553323
      ],
      "excerpt": "Usually, feeding a batch of requests of different lengths into a bert model for inference, \nzero-padding is required to make all the requests have the same length. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9519615418655403
      ],
      "excerpt": "In this way, 90% and 50% of the last two sequence's computation are wasted. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9165062428025715
      ],
      "excerpt": "it is not necessary to pad the input tensors. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9799398346403132
      ],
      "excerpt": "which accouts to a small propation of the entire BERT computation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8856117202160433
      ],
      "excerpt": "Turbo provides a model as BertModelSmartBatch including a smart batching technique. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936741198757286,
        0.8178978967023248,
        0.8174905709856339,
        0.8700521074575059,
        0.9262546282847312
      ],
      "excerpt": "How to know hotspots of your code? \nHow to add a new layer? \nCurrently (June 2020), In the near future, we will add support for low-precision models (CPU int8, GPU FP16). \nLooking forwards to your contribution! \nThe results of Turbo Transformers may be different from the results of PyTorch after 2 digits behind the decimal point. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8148248351071872
      ],
      "excerpt": "Turbo and PyTorch share the same MKL. MKL of PyTorch 1.5.0 may slow in Turbo. Reasons need to be determined. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "a fast and user-friendly runtime for transformer inference (Bert, Albert, GPT2, Decoders, etc) on CPU and GPU.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Tencent/TurboTransformers/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 143,
      "date": "Tue, 21 Dec 2021 05:22:55 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Tencent/TurboTransformers/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Tencent/TurboTransformers",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/Tencent/TurboTransformers/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Tencent/TurboTransformers/master/benchmark/run_gpu_variable_benchmark.sh",
      "https://raw.githubusercontent.com/Tencent/TurboTransformers/master/benchmark/run_cpu_fixed_benchmark.sh",
      "https://raw.githubusercontent.com/Tencent/TurboTransformers/master/benchmark/run_quantization_benchmark.sh",
      "https://raw.githubusercontent.com/Tencent/TurboTransformers/master/benchmark/run_cpu_variable_benchmark.sh",
      "https://raw.githubusercontent.com/Tencent/TurboTransformers/master/benchmark/run_gpu_fixed_benchmark.sh",
      "https://raw.githubusercontent.com/Tencent/TurboTransformers/master/benchmark/mem_watcher.sh",
      "https://raw.githubusercontent.com/Tencent/TurboTransformers/master/tools/build_conda_package.sh",
      "https://raw.githubusercontent.com/Tencent/TurboTransformers/master/tools/build_docker_cpu.sh",
      "https://raw.githubusercontent.com/Tencent/TurboTransformers/master/tools/build_docker_gpu.sh",
      "https://raw.githubusercontent.com/Tencent/TurboTransformers/master/tools/build_and_run_unittests.sh",
      "https://raw.githubusercontent.com/Tencent/TurboTransformers/master/tools/compile.sh",
      "https://raw.githubusercontent.com/Tencent/TurboTransformers/master/tools/ci_check.sh",
      "https://raw.githubusercontent.com/Tencent/TurboTransformers/master/tools/conda/build.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Note that the building scripts only apply to specific OS and software (Pytorch, OpenNMT, transformers, etc.) versions.\nPlease adjust them according to your needs.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8124596844784701
      ],
      "excerpt": "Excellent CPU / GPU performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8893265044554576
      ],
      "excerpt": "| pytorch (CPU/GPU) | Medium/Medium | No | Yes | Easy | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365,
        0.8837680365796365,
        0.8837680365796365,
        0.8837680365796365,
        0.8837680365796365
      ],
      "excerpt": "BERT [Python] [C++] \nALBERT [Python] \nRoberta [Python] \nTransformer Decoder [Python] \nGPT2 [Python] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9708885120774579
      ],
      "excerpt": "git clone https://github.com/Tencent/TurboTransformers --recursive \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8461017519497106
      ],
      "excerpt": "sh tools/build_docker_cpu.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8896827939633698
      ],
      "excerpt": "env BUILD_TYPE=dev sh tools/build_docker_cpu.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd /workspace \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466,
        0.9935309876697507
      ],
      "excerpt": "cd /workspace \nmkdir -p build && cd build \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8474895321345809,
        0.9888365753844199
      ],
      "excerpt": "make -j 4 \npip install find . -name *whl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8721423083407055,
        0.9465718491881494,
        0.9416661168854227,
        0.8717051273527449,
        0.849187264122653,
        0.8314637087111192
      ],
      "excerpt": "cd benchmark \nbash run_benchmark.sh \n4. Install conda packages in docker (optional) \nsh tool/build_conda_package.sh \n: The conda package will be in /workspace/dist/*.tar.bz2 \n: When using turbo_transformers in other environments outside this container: conda install your_root_path/dist/*.tar.bz2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9708885120774579
      ],
      "excerpt": "git clone https://github.com/Tencent/TurboTransformers --recursive \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9745416555157295,
        0.8142362916922588
      ],
      "excerpt": ": You can modify the environment variables in the script to specify the cuda version and operating system version \nsh tools/build_docker_gpu.sh $PWD \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8662782423560728
      ],
      "excerpt": ": for example: nvidia-docker run --gpus all --net=host --rm -it -v $PWD:/workspace -v /etc/passwd:/etc/passwd --name=turbo_gpu_env thufeifeibear:0.1.1-cuda9.0-ubuntu16.04-gpu-dev \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466,
        0.8450297549904826
      ],
      "excerpt": "cd /workspace \nsh tools/build_and_run_unittests.sh $PWD -DWITH_GPU=ON \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8721423083407055,
        0.9465718491881494
      ],
      "excerpt": "cd benchmark \nbash gpu_run_benchmark.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8597379971626529
      ],
      "excerpt": "Tensor Core  can accelerate computing on GPU. It is disabled by default in TurboTransformers. If you want to turn it on, before compiling code, set option WITH_MODULE_BENCHMAKR ON in CMakeLists.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8361380687903786
      ],
      "excerpt": "zero-padding is required to make all the requests have the same length. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9517294411942204
      ],
      "excerpt": "Download PyTorch version to 1.1.0 will improve Turbo's Performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8526256549469337,
        0.9235327951698699
      ],
      "excerpt": "July 2020 v0.3.1, TurboTransformers added support for ALbert, Roberta on CPU/GPU. \nJune 2020 v0.3.0, TurboTransformers added support for Transformer Decoder on CPU/GPU. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9133368656218674,
        0.9133368656218674,
        0.8458751354831934
      ],
      "excerpt": "import transformers \nimport turbo_transformers \nif name == \"main\": \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090089860393823
      ],
      "excerpt": "    cfg = model.config \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8645297786635092
      ],
      "excerpt": "docker run -it --rm --name=turbort -v $PWD:/workspace your_image_name /bin/bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9368325597523536,
        0.8219014031567595
      ],
      "excerpt": "<img width=\"700\" height=\"150\" src=\"./images/pretrainmodelload.jpg\" alt=\"pretrained\"> \nRefer to examples of supported models in ./example/python. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8502404760932294
      ],
      "excerpt": "The example is presented in ./example/python/bert_smart_pad.py. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Tencent/TurboTransformers/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Python",
      "Cuda",
      "CMake",
      "Shell",
      "C"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/Tencent/TurboTransformers/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (C) 2020 THL A29 Limited, a Tencent company.\\nAll rights reserved.\\nLicensed under the BSD 3-Clause License (the \"License\"); you may\\nnot use this file except in compliance with the License. You may\\nobtain a copy of the License at\\nhttps://opensource.org/licenses/BSD-3-Clause\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" basis,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\nimplied. See the License for the specific language governing\\npermissions and limitations under the License.\\nSee the AUTHORS file for names of contributors.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# TurboTransformers: a fast and user-friendly runtime for transformer inference on CPU and GPU",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "TurboTransformers",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Tencent",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Tencent/TurboTransformers/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "fangjiarui",
        "body": "Albert Model uses the model-aware-allocator.",
        "dateCreated": "2020-11-25T05:05:18Z",
        "datePublished": "2020-11-25T09:46:57Z",
        "html_url": "https://github.com/Tencent/TurboTransformers/releases/tag/v0.5.1",
        "name": "TurboTransformers v0.5.1",
        "tag_name": "v0.5.1",
        "tarball_url": "https://api.github.com/repos/Tencent/TurboTransformers/tarball/v0.5.1",
        "url": "https://api.github.com/repos/Tencent/TurboTransformers/releases/34405051",
        "zipball_url": "https://api.github.com/repos/Tencent/TurboTransformers/zipball/v0.5.1"
      },
      {
        "authorType": "User",
        "author_name": "fangjiarui",
        "body": "Add Model Aware Allocator for Bert Model.",
        "dateCreated": "2020-11-19T12:14:04Z",
        "datePublished": "2020-11-19T12:18:06Z",
        "html_url": "https://github.com/Tencent/TurboTransformers/releases/tag/v0.5.0",
        "name": "TurboTransformers v0.5.0",
        "tag_name": "v0.5.0",
        "tarball_url": "https://api.github.com/repos/Tencent/TurboTransformers/tarball/v0.5.0",
        "url": "https://api.github.com/repos/Tencent/TurboTransformers/releases/34170320",
        "zipball_url": "https://api.github.com/repos/Tencent/TurboTransformers/zipball/v0.5.0"
      },
      {
        "authorType": "User",
        "author_name": "feifeibear",
        "body": "Add Quantized Bert using onnxruntime.",
        "dateCreated": "2020-08-19T09:09:34Z",
        "datePublished": "2020-08-19T09:12:33Z",
        "html_url": "https://github.com/Tencent/TurboTransformers/releases/tag/v0.4.2",
        "name": "TurboTransformers v0.4.2",
        "tag_name": "v0.4.2",
        "tarball_url": "https://api.github.com/repos/Tencent/TurboTransformers/tarball/v0.4.2",
        "url": "https://api.github.com/repos/Tencent/TurboTransformers/releases/29853993",
        "zipball_url": "https://api.github.com/repos/Tencent/TurboTransformers/zipball/v0.4.2"
      },
      {
        "authorType": "User",
        "author_name": "feifeibear",
        "body": "Using onnxruntime-cpu as CPU backend, parallel to our own home-grown implementation.",
        "dateCreated": "2020-07-29T13:07:12Z",
        "datePublished": "2020-08-12T02:23:59Z",
        "html_url": "https://github.com/Tencent/TurboTransformers/releases/tag/v0.4.1",
        "name": "TurboTransformers v0.4.1",
        "tag_name": "v0.4.1",
        "tarball_url": "https://api.github.com/repos/Tencent/TurboTransformers/tarball/v0.4.1",
        "url": "https://api.github.com/repos/Tencent/TurboTransformers/releases/29587324",
        "zipball_url": "https://api.github.com/repos/Tencent/TurboTransformers/zipball/v0.4.1"
      },
      {
        "authorType": "User",
        "author_name": "feifeibear",
        "body": "Support Transformer decoder used in OpenNMT-py.\r\nNew GPU memory allocator.\r\nBe Compatible with Pytorch v1.5.0.",
        "dateCreated": "2020-06-28T08:22:20Z",
        "datePublished": "2020-06-30T04:09:13Z",
        "html_url": "https://github.com/Tencent/TurboTransformers/releases/tag/v0.3.0",
        "name": "TurboTransformer v0.3.0",
        "tag_name": "v0.3.0",
        "tarball_url": "https://api.github.com/repos/Tencent/TurboTransformers/tarball/v0.3.0",
        "url": "https://api.github.com/repos/Tencent/TurboTransformers/releases/28050319",
        "zipball_url": "https://api.github.com/repos/Tencent/TurboTransformers/zipball/v0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "feifeibear",
        "body": "Add blis to BLAS options.",
        "dateCreated": "2020-06-11T03:56:18Z",
        "datePublished": "2020-06-11T03:58:30Z",
        "html_url": "https://github.com/Tencent/TurboTransformers/releases/tag/0.2.1",
        "name": "TurboTransformer v0.2.1",
        "tag_name": "0.2.1",
        "tarball_url": "https://api.github.com/repos/Tencent/TurboTransformers/tarball/0.2.1",
        "url": "https://api.github.com/repos/Tencent/TurboTransformers/releases/27437301",
        "zipball_url": "https://api.github.com/repos/Tencent/TurboTransformers/zipball/0.2.1"
      },
      {
        "authorType": "User",
        "author_name": "feifeibear",
        "body": "Bert Acceleration on CPU and GPU.",
        "dateCreated": "2020-04-25T14:12:51Z",
        "datePublished": "2020-04-25T14:34:24Z",
        "html_url": "https://github.com/Tencent/TurboTransformers/releases/tag/v0.0.1",
        "name": "TurboTransformer v0.0.1",
        "tag_name": "v0.0.1",
        "tarball_url": "https://api.github.com/repos/Tencent/TurboTransformers/tarball/v0.0.1",
        "url": "https://api.github.com/repos/Tencent/TurboTransformers/releases/25880502",
        "zipball_url": "https://api.github.com/repos/Tencent/TurboTransformers/zipball/v0.0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1064,
      "date": "Tue, 21 Dec 2021 05:22:55 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "nlp",
      "transformer",
      "bert",
      "decoder",
      "gpu",
      "machine-translation",
      "inference",
      "huggingface-transformers",
      "pytorch",
      "albert",
      "roberta",
      "gpt2"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "TurboTransformers provides C++ / python API interfaces. We hope to do our best to adapt to a variety of online environments to reduce the difficulty of development for users.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}