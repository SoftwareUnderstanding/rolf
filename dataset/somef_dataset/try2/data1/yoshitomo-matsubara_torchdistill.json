{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2011.12913",
      "https://arxiv.org/abs/1412.6550",
      "https://arxiv.org/abs/2007.15818"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/image_classification.py) [pytorch/vision/references/classification/](https://github.com/pytorch/vision/blob/master/references/classification/)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/object_detection.py) [pytorch/vision/references/detection/](https://github.com/pytorch/vision/tree/master/references/detection/)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/semantic_segmentation.py) [pytorch/vision/references/segmentation/](https://github.com/pytorch/vision/tree/master/references/segmentation/)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/hf_transformers/text_classification.py) [huggingface/transformers/examples/pytorch/text-classification](https://github.com/huggingface/transformers/tree/master/examples/pytorch/text-classification)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/kd) Geoffrey Hinton, Oriol Vinyals and Jeff Dean. [\"Distilling the Knowledge in a Neural Network\"](https://fb56552f-a-62cb3a1a-s-sites.googlegroups.com/site/deeplearningworkshopnips2014/65.pdf?attachauth=ANoY7co8sQACDsEYLkP11zqEAxPgYHLwkdkDP9NHfEB6pzQOUPmfWf3cVrL3WE7PNyed-lrRsF7CY6Tcme5OEQ92CTSN4f8nDfJcgt71fPtAvcTvH5BpzF-2xPvLkPAvU9Ub8XvbySAPOsMKMWmGsXG2FS1_X1LJsUfuwKdQKYVVTtRfG5LHovLHIwv6kXd3mOkDKEH7YdoyYQqjSv6ku2KDjOpVQBt0lKGVPXeRdwUcD0mxDqCe4u8%3D&attredirects=1) (Deep Learning and Representation Learning Workshop: NeurIPS 2014)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/multi_stage/fitnet) Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta and Yoshua Bengio. [\"FitNets: Hints for Thin Deep Nets\"](https://arxiv.org/abs/1412.6550) (ICLR 2015)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/multi_stage/fsp) Junho Yim, Donggyu Joo, Jihoon Bae and Junmo Kim. [\"A Gift From Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning\"](http://openaccess.thecvf.com/content_cvpr_2017/html/Yim_A_Gift_From_CVPR_2017_paper.html) (CVPR 2017)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/at) Sergey Zagoruyko and Nikos Komodakis. [\"Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer\"](https://openreview.net/forum?id=Sks9_ajex) (ICLR 2017)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/pkt) Nikolaos Passalis and Anastasios Tefas. [\"Learning Deep Representations with Probabilistic Knowledge Transfer\"](http://openaccess.thecvf.com/content_ECCV_2018/html/Nikolaos_Passalis_Learning_Deep_Representations_ECCV_2018_paper.html) (ECCV 2018)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/multi_stage/ft) Jangho Kim, Seonguk Park and Nojun Kwak. [\"Paraphrasing Complex Network: Network Compression via Factor Transfer\"](http://papers.neurips.cc/paper/7541-paraphrasing-complex-network-network-compression-via-factor-transfer) (NeurIPS 2018)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/multi_stage/dab) Byeongho Heo, Minsik Lee, Sangdoo Yun and Jin Young Choi. [\"Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons\"](https://aaai.org/ojs/index.php/AAAI/article/view/4264) (AAAI 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/coco2017/multi_stage/ktaad) Tong He, Chunhua Shen, Zhi Tian, Dong Gong, Changming Sun, Youliang Yan. [\"Knowledge Adaptation for Efficient Semantic Segmentation\"](https://openaccess.thecvf.com/content_CVPR_2019/html/He_Knowledge_Adaptation_for_Efficient_Semantic_Segmentation_CVPR_2019_paper.html) (CVPR 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/rkd) Wonpyo Park, Dongju Kim, Yan Lu and Minsu Cho. [\"Relational Knowledge Distillation\"](http://openaccess.thecvf.com/content_CVPR_2019/html/Park_Relational_Knowledge_Distillation_CVPR_2019_paper.html) (CVPR 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/vid) Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D. Lawrence and Zhenwen Dai. [\"Variational Information Distillation for Knowledge Transfer\"](http://openaccess.thecvf.com/content_CVPR_2019/html/Ahn_Variational_Information_Distillation_for_Knowledge_Transfer_CVPR_2019_paper.html) (CVPR 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/hnd) Yoshitomo Matsubara, Sabur Baidya, Davide Callegaro, Marco Levorato and Sameer Singh. [\"Distilled Split Deep Neural Networks for Edge-Assisted Real-Time Systems\"](https://dl.acm.org/doi/10.1145/3349614.3356022) (Workshop on Hot Topics in Video Analytics and Intelligent Edges: MobiCom 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/cckd) Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou and Zhaoning Zhang. [\"Correlation Congruence for Knowledge Distillation\"](http://openaccess.thecvf.com/content_ICCV_2019/html/Peng_Correlation_Congruence_for_Knowledge_Distillation_ICCV_2019_paper.html) (ICCV 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/spkd) Frederick Tung and Greg Mori. [\"Similarity-Preserving Knowledge Distillation\"](http://openaccess.thecvf.com/content_ICCV_2019/html/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html) (ICCV 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/crd) Yonglong Tian, Dilip Krishnan and Phillip Isola. [\"Contrastive Representation Distillation\"](https://openreview.net/forum?id=SkgpBJrtvS) (ICLR 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/coco2017/single_stage/ghnd) Yoshitomo Matsubara and Marco Levorato. [\"Neural Compression and Filtering for Edge-assisted Real-time Object Detection in Challenged Networks\"](https://arxiv.org/abs/2007.15818) (ICPR 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/tfkd) Li Yuan, Francis E.H.Tay, Guilin Li, Tao Wang and Jiashi Feng. [\"Revisiting Knowledge Distillation via Label Smoothing Regularization\"](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf) (CVPR 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/multi_stage/sskd) Guodong Xu, Ziwei Liu, Xiaoxiao Li and Chen Change Loy. [\"Knowledge Distillation Meets Self-Supervision\"](http://www.ecva.net/papers/eccv_2020/papers_ECCV/html/898_ECCV_2020_paper.php) (ECCV 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/multi_stage/pad) Youcai Zhang, Zhonghao Lan, Yuchen Dai, Fangao Zeng, Yan Bai, Jie Chang and Yichen Wei. [\"Prime-Aware Adaptive Distillation\"](http://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3317_ECCV_2020_paper.php) (ECCV 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/kr) Pengguang Chen, Shu Liu, Hengshuang Zhao, Jiaya Jia. [\"Distilling Knowledge via Knowledge Review\"](https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Distilling_Knowledge_via_Knowledge_Review_CVPR_2021_paper.html) (CVPR 2021)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use ***torchdistill*** in your research, please cite the following paper.  \n[[Paper](https://link.springer.com/chapter/10.1007/978-3-030-76423-4_3)] [[Preprint](https://arxiv.org/abs/2011.12913)]  \n```bibtex\n@inproceedings{matsubara2021torchdistill,\n  title={torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation},\n  author={Matsubara, Yoshitomo},\n  booktitle={International Workshop on Reproducible Research in Pattern Recognition},\n  pages={24--44},\n  year={2021},\n  organization={Springer}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{matsubara2021torchdistill,\n  title={torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation},\n  author={Matsubara, Yoshitomo},\n  booktitle={International Workshop on Reproducible Research in Pattern Recognition},\n  pages={24--44},\n  year={2021},\n  organization={Springer}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9960569962187636
      ],
      "excerpt": "When you refer to torchdistill in your paper, please cite this paper  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9287913210266059
      ],
      "excerpt": "| S: ResNet-18    | 69.76*    | 71.37 | 70.90 | 71.56      | 70.93 | 70.52 | 70.09 | 71.08 | 71.71  | 71.64 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yoshitomo-matsubara/torchdistill",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-18T17:40:32Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T08:17:53Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.819492477633405
      ],
      "excerpt": "you will NOT need to reimplement the models, that often change the interface of the forward, but instead  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8166105735911152
      ],
      "excerpt": "In addition to knowledge distillation, this framework helps you design and perform general deep learning experiments \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9606494464711743,
        0.8638961150472936
      ],
      "excerpt": "instead of this GitHub repository. \nYour citation is appreciated and motivates me to maintain and upgrade this framework! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9158383603086133,
        0.8988538253245169
      ],
      "excerpt": "will give you a better idea of the usage such as knowledge distillation and analysis of intermediate representations. \nIn torchdistill, many components and PyTorch modules are abstracted e.g., models, datasets, optimizers, losses,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9934100942994086,
        0.9627955472729255
      ],
      "excerpt": "** FT is assessed with ILSVRC 2015 in the original work. \nFor the 2nd row (S: ResNet-18), most of the results are reported in this paper,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9806349336031971
      ],
      "excerpt": "and the configurations reuse the hyperparameters such as number of epochs used in the original work except for KD. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A coding-free framework built on PyTorch for reproducible deep learning studies. \ud83c\udfc620 knowledge distillation methods presented at CVPR, ICLR, ECCV, NeurIPS, ICCV, etc are implemented so far. \ud83c\udf81 Trained models, training logs and configurations are available for ensuring the reproducibiliy and benchmark.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yoshitomo-matsubara/torchdistill/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 50,
      "date": "Mon, 27 Dec 2021 20:31:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "yoshitomo-matsubara/torchdistill",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/yoshitomo-matsubara/torchdistill/main/demo/glue_kd_and_submission.ipynb",
      "https://raw.githubusercontent.com/yoshitomo-matsubara/torchdistill/main/demo/cifar_training.ipynb",
      "https://raw.githubusercontent.com/yoshitomo-matsubara/torchdistill/main/demo/extract_intermediate_representations.ipynb",
      "https://raw.githubusercontent.com/yoshitomo-matsubara/torchdistill/main/demo/cifar_kd.ipynb",
      "https://raw.githubusercontent.com/yoshitomo-matsubara/torchdistill/main/demo/glue_finetuning_and_submission.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\ngit clone https://github.com/yoshitomo-matsubara/torchdistill.git\ncd torchdistill/\npip3 install -e .\n#: or use pipenv\npipenv install \"-e .\"\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```\npip3 install torchdistill\n#: or use pipenv\npipenv install torchdistill\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3.6 >=\n- pipenv (optional)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8875231531362491
      ],
      "excerpt": "If you find models on PyTorch Hub or GitHub repositories supporting PyTorch Hub, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "    name: 'resnest50d' \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8262749871847298
      ],
      "excerpt": "This example notebook   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779754821585296
      ],
      "excerpt": "| T: ResNet-34*  | Pretrained | KD    | AT    | FT         | CRD   | Tf-KD | SSKD  | L2    | PAD-L2 | KR    | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8222772563163673
      ],
      "excerpt": "These examples write out test prediction files for you to see the test performance at the GLUE leaderboard system. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "    name: 'resnest50d' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060372108626078
      ],
      "excerpt": "    params: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8869264826052469
      ],
      "excerpt": "      pretrained: True \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Yoshitomo Matsubara\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "torchdistill",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "yoshitomo-matsubara",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yoshitomo-matsubara/torchdistill/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# Bug fix\r\n- `strict` should not be used here (PR #202)\r\n\r\n# Minor update\r\n- Update version (PRs #201, #203)",
        "dateCreated": "2021-12-26T02:06:14Z",
        "datePublished": "2021-12-26T02:07:31Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.3.0",
        "name": "Bug fix",
        "tag_name": "v0.3.0",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.3.0",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/55988947",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# Example updates\r\n- Restructure and make download=True (PR #190)\r\n- Make log_freq configurable for test (PR #191)\r\n- Refactor (PR #192)\r\n- Probably torch.cuda.synchronize() is no longer needed (PR #194)\r\n- Add an option to use teacher output (PR #195)\r\n- Replace no_grad with inference_mode (PR #199)\r\n\r\n# Minor updates\r\n- Add strict arg (PR #193)\r\n- Add assert error message (PR #196)\r\n- Check if ckpt file path is string (PR #197)\r\n- Check if batch images are instance of Tensor (PR #198)\r\n- Update version (PRs #189, #200)",
        "dateCreated": "2021-12-26T01:35:58Z",
        "datePublished": "2021-12-26T01:43:26Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.2.9",
        "name": "Example and minor updates",
        "tag_name": "v0.2.9",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.2.9",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/55988645",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.2.9"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# New features\r\n- Add wrapped resize to enable specifying interpolation for resize (PR #182)\r\n- Add wrapped random crop resize to enable specifying interpolation for random crop resize (PR #183)\r\n- Enable to load ckpt containing only specific module and via URL (PR #187)\r\n\r\n# New examples and trained models\r\n- Add examples for PASCAL VOC 2012 (PRs #184, #186)\r\n- Update README (PR #185)\r\n- Add model weights of DeepLabv3 with ResNet-50/101  fine-tuned on PASCAL VOC 2012 (Segmentation)\r\n\r\n\r\n|                         | mean IoU | global pixelwise acc |\r\n|-------------------------|---------:|---------------------:|\r\n| DeepLabv3 w/ ResNet-50  |     80.6 |                 95.7 |\r\n| DeepLabv3 w/ ResNet-101 |     82.4 |                 96.2 |\r\n\r\nModel implementations are available in [torchvision](https://github.com/pytorch/vision). These model weights are originally pretrained on COCO 2017 dataset (available in [torchvision](https://github.com/pytorch/vision)) and then fine-tuned on PASCAL VOC 2012 (Segmentation) dataset.\r\n\r\n# Minor updates\r\n- Add a version constant (PR #175)\r\n- Rename and add functions for ResNet-50 and ResNet-101 (PR #176)\r\n- Add CITATION file (PR #178)\r\n- Update version (PRs #174, #188)\r\n- Update README (PRs #179, #180)",
        "dateCreated": "2021-12-17T04:08:49Z",
        "datePublished": "2021-12-17T04:30:51Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.2.8",
        "name": "Add new features, PASCAL examples and pretrained models",
        "tag_name": "v0.2.8",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.2.8",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/55512202",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.2.8"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# Minor updates\r\n- Update version (PRs #161, #162, #173)\r\n- Update README (PRs #163, #164)\r\n- Add an option to log config (PR #169)\r\n\r\n# Bug fix\r\n- In PyTorch v.1.10, `load_state_dict_from_url` is no longer available in `torchvision.models.utils` (PR: #172)",
        "dateCreated": "2021-10-28T00:44:39Z",
        "datePublished": "2021-10-28T00:50:31Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.2.7",
        "name": "Minor updates and bug fix to support PyTorch v1.10",
        "tag_name": "v0.2.7",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.2.7",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/52192034",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.2.7"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# New method\r\n- Add knowledge translation and adaptation + affinity distillation for semantic segmentation (PR #158)\r\n\r\n# Minor updates\r\n- Update version (PRs #151, #160)\r\n- Update README (PRs #153, #159)\r\n- Stop training when facing NaN or Infinity (PR #157)",
        "dateCreated": "2021-09-14T09:50:30Z",
        "datePublished": "2021-09-14T09:58:59Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.2.6",
        "name": "Add KTAAD method and improve exampes",
        "tag_name": "v0.2.6",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.2.6",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/49515370",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.2.6"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# New method\r\n- Add knowledge review method (PRs #141, #145, #146)\r\n\r\n# New features\r\n- Make nn.ModuleList hookable (PR #139)\r\n- Support negative index in module path (PR #144)\r\n\r\n# Minor updates\r\n- Update version (PRs #137, #148)\r\n- Update README (PRs #138, #147)\r\n- Fix a typo (PR #142)\r\n",
        "dateCreated": "2021-08-01T01:06:43Z",
        "datePublished": "2021-08-01T01:22:39Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.2.5",
        "name": "Add knowledge review method and new features",
        "tag_name": "v0.2.5",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.2.5",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/47102090",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.2.5"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# Minor updates\r\n- Update version (PRs #128, #136)\r\n- Fix a typo (PR #130)\r\n- Make pin_memory configurable (PR #134)\r\n\r\n# Bug fix\r\n- Clear io_dict in pre-process (Issue #132, PR #135)",
        "dateCreated": "2021-07-15T11:17:12Z",
        "datePublished": "2021-07-15T11:28:37Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.2.4",
        "name": "Minor updates and potential bug fix in package",
        "tag_name": "v0.2.4",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.2.4",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/46248315",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.2.4"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# Bug fixes\r\n- `DistributedDataParallel` is no longer allowed for wrapping models with no updatable parameters (Issue #122 PRs #124, #125)\r\n- Fix a bug in detecting collate function type (Issue #123 PR #126)\r\n\r\n# Misc\r\n- Update version (PR #127)",
        "dateCreated": "2021-06-30T12:10:36Z",
        "datePublished": "2021-06-30T12:16:26Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.2.3",
        "name": "Bug fixes in package",
        "tag_name": "v0.2.3",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.2.3",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/45486628",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.2.3"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# Examples\r\n- Improve log format (PR #111)\r\n- Tune hyperparameters for GLUE tasks (PRs #112, #113)\r\n- Add sample KD configs for GLUE tasks (PR #114)\r\n\r\n# Misc\r\n- Update notebooks (PR #115)\r\n- Update README (PR #116)\r\n- Update version (PRs #118, #120)\r\n- Support PyTorch v1.9.0 (PR #119)",
        "dateCreated": "2021-06-23T10:41:57Z",
        "datePublished": "2021-06-23T10:49:05Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.2.2",
        "name": "Update examples and support PyTorch v1.9.0",
        "tag_name": "v0.2.2",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.2.2",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/45089282",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.2.2"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# Examples\r\n- Update GLUE example (PRs #97, #98, #99, #104, #106, #108)\r\n- Enable test prediction to make a submission for GLUE leaderboard (PR #102)\r\n- Add notebook (PRs #105, #109)\r\n\r\n# Bug fixes\r\n- Provide kwargs (PR #94)\r\n- Enable teacher to run in fp16 mode (PR #110)\r\n\r\n# Minor updates\r\n- Update README (PRs #93, #101, #102, #103 #107, #108)\r\n- Refactor / Fix typos (PRs #95, #96, #100, #101, #104)",
        "dateCreated": "2021-05-25T02:52:28Z",
        "datePublished": "2021-05-25T03:10:45Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.2.1",
        "name": "Update HF support, examples and notebooks",
        "tag_name": "v0.2.1",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.2.1",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/43498283",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.2.1"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# New features and example\r\n- Introduce Hugging Face's Accelerate to better collaborate with their Transformers package (PR #91)\r\n- Introduce example of text classification (GLUE tasks) with Hugging Face's Transformers and datasets (PR #92)\r\n\r\n# Minor updates\r\n- Update README (PR #93)\r\n- Allow non-function collator and make filtering optimizer's params optional (PR #89)",
        "dateCreated": "2021-05-05T03:48:33Z",
        "datePublished": "2021-05-05T03:57:28Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.2.0",
        "name": "Support Hugging Face Transformers and Accelerate",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.2.0",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/42451903",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# Example updates\r\n- Add an example to show how to import models via PyTorch Hub (PR #83)\r\n- Add an option to set random seed for reproducibility (PR #85)\r\n- Add an example of segmentation model training (PR #86)\r\n\r\n# Restructuring\r\n- Refactor function util (PR #84)\r\n\r\n# Typo fixes\r\n- Fix typos in dataset util and examples (PR #88)\r\n",
        "dateCreated": "2021-04-09T05:55:55Z",
        "datePublished": "2021-04-09T05:58:14Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.1.6",
        "name": "Example updates",
        "tag_name": "v0.1.6",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.1.6",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/41163241",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.1.6"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# Minor updates\r\n- Make IoU type selection model-free (PR #74)\r\n- Update loss string (PR #74)\r\n- Disable DDP when no params are updatable (PR #77)\r\n- Update README (PR #78)\r\n\r\n# Bug/Typo fixes\r\n- Fix typos in example commands (PR #76)\r\n- Fix typos in sample configs (PR #79)\r\n- Fix bugs for clip grad norm (PR #80)\r\n\r\n",
        "dateCreated": "2021-03-22T05:51:48Z",
        "datePublished": "2021-03-22T05:58:55Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.1.5",
        "name": "Minor updates and bug fixes",
        "tag_name": "v0.1.5",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.1.5",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/40140199",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.1.5"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# Minor updates\r\n- Update functions for object detection  models (PR #59)\r\n- Update README (PRs #61 #62)\r\n\r\n# Minor bug fixes\r\n- Rename (PR #60)\r\n- Bug fixes (PR #73)",
        "dateCreated": "2021-02-21T22:54:32Z",
        "datePublished": "2021-02-21T23:00:47Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.1.4",
        "name": "Minor updates and bug fixes",
        "tag_name": "v0.1.4",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.1.4",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/38399153",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.1.4"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# Updated official README and configs\r\n- More detailed instructions (PRs #55, #56)\r\n- Restructured official configs (PR #55)\r\n- Updated FT config for ImageNet (PR #55)\r\n\r\n# Support detailed training configurations\r\n- Step-wise parameter update besides epoch-wise parameter update (PR #58)\r\n- Gradient accumulation (PR #58)\r\n- Max gradient norm (PR #58)\r\n\r\n# Bug/Typo fixes\r\n- Bug fixes (PRs #54, #57)\r\n- Typo fixes (PRs #53, #58)",
        "dateCreated": "2021-02-06T23:27:11Z",
        "datePublished": "2021-02-06T23:42:58Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.1.3",
        "name": "Support more detailed training configs and update official configs",
        "tag_name": "v0.1.3",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.1.3",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/37528493",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.1.3"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# New examples\r\n- Added sample configs  for CIFAR-10 and CIFAR-100 datasets\r\n1. Training without teacher (i.e., using `TrainingBox`) for [CIFAR-10](https://github.com/yoshitomo-matsubara/torchdistill/tree/master/configs/sample/cifar10/ce) and [CIFAR-100](https://github.com/yoshitomo-matsubara/torchdistill/tree/master/configs/sample/cifar100/ce) (PR #48)\r\n2. Knowledge distillation for [CIFAR-10](https://github.com/yoshitomo-matsubara/torchdistill/tree/master/configs/sample/cifar10/kd) and [CIFAR-100](https://github.com/yoshitomo-matsubara/torchdistill/tree/master/configs/sample/cifar100/kd) (PR #50)\r\n- Added Google Colab examples (PR #51)\r\n1. [Training without teacher for CIFAR-10 and CIFAR-100](https://colab.research.google.com/github/yoshitomo-matsubara/torchdistill/blob/master/demo/cifar_training.ipynb)\r\n2. [Knowledge distillation for CIFAR-10 and CIFAR-100](https://colab.research.google.com/github/yoshitomo-matsubara/torchdistill/blob/master/demo/cifar_kd.ipynb)\r\n\r\n# Bug fixes\r\n- Fixed a bug in init of DenseNet-BC  (PR #48)\r\n- Resolved checkpoint name conflicts (PR #49)",
        "dateCreated": "2021-01-11T02:05:10Z",
        "datePublished": "2021-01-11T02:07:59Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.1.2",
        "name": "Google Colab Examples and bug fixes",
        "tag_name": "v0.1.2",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.1.2",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/36206218",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.1.2"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# New features\r\n- Added TrainingBox to train models without teachers (PR #39)\r\n- Supported PyTorch Hub in registry (PR #40)\r\n- Supported random split e.g., split training dataset into training and validation datasets (PR #41)\r\n- Added reimplemented models for CIFAR-10 and CIFAR-100 datasets (PR #41)\r\n\r\n# Pretrained models\r\nReferred to the following repositories for training methods.\r\n- ResNet: https://github.com/facebookarchive/fb.resnet.torch\r\n- WRN (Wide ResNet): https://github.com/szagoruyko/wide-residual-networks\r\n- DenseNet-BC: https://github.com/liuzhuang13/DenseNet\r\n\r\nNote that there are some accuracy gaps between these and those reported in their original studies.\r\n\r\n|                               | CIFAR-10 | CIFAR-100 |\r\n|-------------------------------|---------:|----------:|\r\n| ResNet-20                     |    91.92 |       N/A |\r\n| ResNet-32                     |    93.03 |       N/A |\r\n| ResNet-44                     |    93.20 |       N/A |\r\n| ResNet-56                     |    93.57 |       N/A |\r\n| ResNet-110                    |    93.50 |       N/A |\r\n| WRN-40-4                      |    95.24 |     79.44 |\r\n| WRN-28-10                     |    95.53 |     81.27 |\r\n| WRN-16-8                      |    94.76 |     79.26 |\r\n| DenseNet-BC (k=12, depth=100) |    95.53 |     77.14 |\r\n",
        "dateCreated": "2020-12-27T19:21:10Z",
        "datePublished": "2020-12-27T19:21:54Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.1.1",
        "name": "TrainingBox, PyTorch Hub, random split, pretrained models for CIFAR-10 and CIFAR-100 datasets",
        "tag_name": "v0.1.1",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.1.1",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/35753402",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.1.1"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "- Extended ForwardHookManager (Issue #32 PR #33)\r\n- Fixed bugs around post_forward function caused by a gathering paradigm introduced to I/O dict (Issue #34 PR #35)",
        "dateCreated": "2020-12-04T18:35:10Z",
        "datePublished": "2020-12-04T18:39:13Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.0.2",
        "name": "Extended ForwardHookManager and bug fix",
        "tag_name": "v0.0.2",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.0.2",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/34842036",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.0.2"
      },
      {
        "authorType": "User",
        "author_name": "yoshitomo-matsubara",
        "body": "# torchdistill\r\n\r\nThe first lease of ***torchdistill*** with code and assets for \"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation\"",
        "dateCreated": "2020-11-25T07:54:45Z",
        "datePublished": "2020-11-25T07:56:10Z",
        "html_url": "https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.0.1",
        "name": "The first release of torchdistill",
        "tag_name": "v0.0.1",
        "tarball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/tarball/v0.0.1",
        "url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/releases/34400754",
        "zipball_url": "https://api.github.com/repos/yoshitomo-matsubara/torchdistill/zipball/v0.0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 497,
      "date": "Mon, 27 Dec 2021 20:31:06 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "knowledge-distillation",
      "pytorch",
      "image-classification",
      "imagenet",
      "object-detection",
      "coco",
      "semantic-segmentation",
      "cifar10",
      "cifar100",
      "colab-notebook",
      "google-colab",
      "pascal-voc",
      "nlp",
      "natural-language-processing",
      "transformer",
      "glue"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Executable code can be found in [examples/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/) such as\n- [Image classification](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/image_classification.py): ImageNet (ILSVRC 2012), CIFAR-10, CIFAR-100, etc\n- [Object detection](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/object_detection.py): COCO 2017, etc\n- [Semantic segmentation](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/semantic_segmentation.py): COCO 2017, PASCAL VOC, etc\n- [Text classification](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/hf_transformers/text_classification.py): GLUE, etc\n\nFor CIFAR-10 and CIFAR-100, some models are reimplemented and available as pretrained models in ***torchdistill***. \nMore details can be found [here](https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.1.1).  \n\nSome Transformer models fine-tuned by ***torchdistill*** for GLUE tasks are available at [Hugging Face Model Hub](https://huggingface.co/yoshitomo-matsubara). \nSample GLUE benchmark results and details can be found [here](https://github.com/yoshitomo-matsubara/torchdistill/tree/master/examples/hf_transformers#sample-benchmark-results-and-fine-tuned-models).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The following examples are available in [demo/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/demo/). \nNote that the examples are for Google Colab users. Usually, [examples/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/) would be a better reference \nif you have your own GPU(s).\n\n",
      "technique": "Header extraction"
    }
  ]
}