{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1703.10135",
      "https://arxiv.org/abs/1703.10135](https://arxiv.org/abs/1703.10135) [cs.CL]\n- [The LJ Speech Dataset](https://keithito.com/LJ-Speech-Dataset/)\n- [Recurrent Neural Networks](https://d2l.ai/chapter_recurrent-neural-networks/index.html) @ [Dive into Deep Learning](https://d2l.ai/index.html) interactive book\n- [Text to Speech Deep Learning Architectures](http://www.erogol.com/text-speech-deep-learning-architectures/)\n- [Deep Learning for Audio](http://slazebni.cs.illinois.edu/spring17/lec26_audio.pdf) Y. Fan, M. Potok, C. Shroba\n- [Deep Learning for Text-to-Speech Synthesis, using the Merlin toolkit](http://www.speech.zone/courses/one-off/merlin-interspeech2017/)\n- [Babble-rnn: Generating speech from speech with LSTM networks](http://babble-rnn.consected.com/docs/babble-rnn-generating-speech-from-speech-post.html)\n- https://github.com/r9y9/tacotron_pytorch\n- https://github.com/keithito/tacotron\n- https://github.com/mozilla/TTS\n- https://github.com/Kyubyong/tacotron\n- [The Centre for Speech Technology Research](http://www.cstr.ed.ac.uk/)\n- [Preparing Data for Training an HTS Voice](http://www.cs.columbia.edu/~ecooper/tts/data.html)\n- [awesome speech synthesis/recognition papers](http://rodrigo.ebrmx.com/github_/zzw922cn/awesome-speech-recognition-speech-synthesis-papers)"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Tacotron: Towards End-to-End Speech Synthesis\t[arXiv:1703.10135](https://arxiv.org/abs/1703.10135) [cs.CL]\n- [The LJ Speech Dataset](https://keithito.com/LJ-Speech-Dataset/)\n- [Recurrent Neural Networks](https://d2l.ai/chapter_recurrent-neural-networks/index.html) @ [Dive into Deep Learning](https://d2l.ai/index.html) interactive book\n- [Text to Speech Deep Learning Architectures](http://www.erogol.com/text-speech-deep-learning-architectures/)\n- [Deep Learning for Audio](http://slazebni.cs.illinois.edu/spring17/lec26_audio.pdf) Y. Fan, M. Potok, C. Shroba\n- [Deep Learning for Text-to-Speech Synthesis, using the Merlin toolkit](http://www.speech.zone/courses/one-off/merlin-interspeech2017/)\n- [Babble-rnn: Generating speech from speech with LSTM networks](http://babble-rnn.consected.com/docs/babble-rnn-generating-speech-from-speech-post.html)\n- https://github.com/r9y9/tacotron_pytorch\n- https://github.com/keithito/tacotron\n- https://github.com/mozilla/TTS\n- https://github.com/Kyubyong/tacotron\n- [The Centre for Speech Technology Research](http://www.cstr.ed.ac.uk/)\n- [Preparing Data for Training an HTS Voice](http://www.cs.columbia.edu/~ecooper/tts/data.html)\n- [awesome speech synthesis/recognition papers](http://rodrigo.ebrmx.com/github_/zzw922cn/awesome-speech-recognition-speech-synthesis-papers)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8490817347094297
      ],
      "excerpt": "Literature and references \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/IvKosar/text2speech",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-11T12:00:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-22T19:32:29Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9967438111161513
      ],
      "excerpt": "Stages of project \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042186185714213
      ],
      "excerpt": "[x] Create architecture of the model from the paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8414515174211293,
        0.889159397971678
      ],
      "excerpt": "To use the model we created jupyter notebook (inference.ipynb). \nHere is the example of usage: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9518471196073566,
        0.9855384329503567,
        0.8384279707796628,
        0.9791410068004924,
        0.9189420056909479,
        0.8207193647074673
      ],
      "excerpt": "For our project we choose to use LJ Speech Dataset. \nIt consists of 13100 audio clips of a single speaker with transcriptions to every clip. \nTotal number of words is 225715 and total length of all audio is almost 24 hours. \nThe model takes characters as input and outputs the corresponding raw spectrogram, which is then fed to the Griffin-Lim reconstruction algorithm to synthesize speech. \nCBHG  consists  of  a bank of 1-D convolutional filters,  followed by highway networks and bidirectional gated recurrent unit (GRU)  recurrent neural net (RNN).  \nCBHG is a powerful module for extracting representations from sequences. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/IvKosar/text2speech/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 28 Dec 2021 20:21:16 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/IvKosar/text2speech/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "IvKosar/text2speech",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/IvKosar/text2speech/master/inference.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9538870279784434
      ],
      "excerpt": "The output is generated audio. It has taken ~2 seconds for generation on GPU GeForce GTX 1060. You can listen to it in the ipynb notebook or here. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "[x] Training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737873112808705
      ],
      "excerpt": "[x] Train model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8949899472983357
      ],
      "excerpt": "$ python train.py --config configs/config.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8835225572704719
      ],
      "excerpt": "Here is the example of usage: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/IvKosar/text2speech/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Sequence translation: text-to-speech",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "text2speech",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "IvKosar",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/IvKosar/text2speech/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can install all required dependencies with: \n```bash\n$ pip install -r requirements.txt\n```\n\nYou can also install the latest packages manually with:\n\n  - [![Anaconda-Server Badge](https://anaconda.org/anaconda/numpy/badges/version.svg)](https://anaconda.org/anaconda/numpy): `conda install numpy`\n  - [![Anaconda-Server Badge](https://anaconda.org/anaconda/scipy/badges/version.svg)](https://anaconda.org/anaconda/scipy): `conda install scipy`\n  - [![Anaconda-Server Badge](https://anaconda.org/conda-forge/tqdm/badges/version.svg)](https://anaconda.org/conda-forge/tqdm): `conda install -c conda-forge tqdm`\n  - [![PyPI version](https://badge.fury.io/py/tensorboardX.svg)](https://badge.fury.io/py/tensorboardX): `pip install tensorboardX`\n  - [![Anaconda-Server Badge](https://anaconda.org/pytorch/pytorch/badges/version.svg)](https://anaconda.org/pytorch/pytorch):  `conda install -c pytorch pytorch`\n  - [![Anaconda-Server Badge](https://anaconda.org/conda-forge/matplotlib/badges/version.svg)](https://anaconda.org/conda-forge/matplotlib): `conda install -c conda-forge matplotlib`\n  - [![Anaconda-Server Badge](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/conda-forge/pandas): ` conda install -c conda-forge pandas`\n  - [![Anaconda-Server Badge](https://anaconda.org/conda-forge/librosa/badges/version.svg)](https://anaconda.org/conda-forge/librosa): `conda install -c conda-forge librosa`\n  - [![Anaconda-Server Badge](https://anaconda.org/conda-forge/unidecode/badges/version.svg)](https://anaconda.org/conda-forge/unidecode): `conda install -c conda-forge unidecode`\n  - [![Anaconda-Server Badge](https://anaconda.org/conda-forge/yaml/badges/version.svg)](https://anaconda.org/conda-forge/yaml) : `conda install -c conda-forge yaml`\n  - [![PyPI version](https://badge.fury.io/py/SoundFile.svg)](https://badge.fury.io/py/SoundFile): `pip install SoundFile`\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 20:21:16 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "python"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "| Text| `It was a great day` | `I love Machine Learning` | `My name is Pytorch and I live on cuda` | `I gonna take my horse to the old town road` |\n|-------|-------|-------|-------|-------|\n| Generated speech in wav | [link](http://marianpetruk.github.com/projects/text2speech/generated/itwaagrda.wav) , [alt_link](generated_audio/wav/itwaagrda.wav) | [link](http://marianpetruk.github.com/projects/text2speech/generated/ilomale.wav) , [alt_link](generated_audio/wav/ilomale.wav) | [link](http://marianpetruk.github.com/projects/text2speech/generated/mynaispyanilioncu.wav) , [alt_link](generated_audio/wav/mynaispyanilioncu.wav) | [link](http://marianpetruk.github.com/projects/text2speech/generated/igotamyhototholtoro.wav) , [alt_link](generated_audio/wav/igotamyhototholtoro.wav) |\n| Converted wav to mp3 | [link](generated_audio/converted_to_mp3/itwaagrda.mp3) | [link](generated_audio/converted_to_mp3/ilomale.mp3) | [link](generated_audio/converted_to_mp3/mynaispyanilioncu.mp3) | [link](generated_audio/converted_to_mp3/igotamyhototholtoro.mp3) |\n\n",
      "technique": "Header extraction"
    }
  ]
}