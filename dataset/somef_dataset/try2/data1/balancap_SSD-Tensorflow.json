{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/balancap/SSD-Tensorflow",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-01-18T13:45:38Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T12:48:40Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9979384154819404,
        0.9906476027551646,
        0.9318645966586429,
        0.9869424008355877,
        0.8742276827696599,
        0.9659949367043451
      ],
      "excerpt": "This repository contains a TensorFlow re-implementation of the original Caffe code. At present, it only implements VGG-based SSD networks (with 300 and 512 inputs), but the architecture of the project is modular, and should make easy the implementation and training of other SSD variants (ResNet or Inception based for instance). Present TF checkpoints have been directly converted from SSD Caffe models. \nThe organisation is inspired by the TF-Slim models repository containing the implementation of popular architectures (ResNet, Inception and VGG). Hence, it is separated in three main parts: \n* datasets: interface to popular datasets (Pascal VOC, COCO, ...) and scripts to convert the former to TF-Records; \n* networks: definition of SSD networks, and common encoding and decoding methods (we refer to the paper on this precise topic); \n* pre-processing: pre-processing and data augmentation routines, inspired by original VGG and Inception implementations. \nThe current version only supports Pascal VOC datasets (2007 and 2012). In order to be used for training a SSD model, the former need to be converted to TF-Records using the tf_convert_data.py script: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8845842945976086,
        0.9142358151634348
      ],
      "excerpt": "Note the previous command generated a collection of TF-Records instead of a single file in order to ease shuffling during training. \nThe present TensorFlow implementation of SSD models have the following performances: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8559145927229403
      ],
      "excerpt": "We are working hard at reproducing the same performance as the original Caffe implementation! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9015015192794673
      ],
      "excerpt": "The evaluation script provides estimates on the recall-precision curve and compute the mAP metrics following the Pascal VOC 2007 and 2012 guidelines. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8941481833273803,
        0.9173306509858092
      ],
      "excerpt": "The script train_ssd_network.py is in charged of training the network. Similarly to TF-Slim models, one can pass numerous options to the training process (dataset, optimiser, hyper-parameters, model, ...). In particular, it is possible to provide a checkpoint file which can be use as starting point in order to fine-tune a network. \nThe easiest way to fine the SSD model is to use as pre-trained SSD network (VGG-300 or VGG-512). For instance, one can fine a model starting from the former as following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9710593504794622
      ],
      "excerpt": "One can also try to build a new SSD model based on standard architecture (VGG, ResNet, Inception, ...) and set up on top of it the multibox layers (with specific anchors, ratios, ...). For that purpose, you can fine-tune a network by only loading the weights of the original architecture, and initialize randomly the rest of network. For instance, in the case of the VGG-16 architecture, one can train a new model as following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8710328629680585
      ],
      "excerpt": "Hence, in the former command, the training script randomly initializes the weights belonging to the checkpoint_exclude_scopes and load from the checkpoint file vgg_16.ckpt the remaining part of the network. Note that we also specify with the trainable_scopes parameter to first only train the new SSD components and left the rest of VGG network unchanged. Once the network has converged to a good first result (~0.5 mAP for instance), you can fine-tuned the complete network as following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Single Shot MultiBox Detector in TensorFlow",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/balancap/SSD-Tensorflow/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1950,
      "date": "Sun, 26 Dec 2021 07:16:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/balancap/SSD-Tensorflow/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "balancap/SSD-Tensorflow",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/balancap/SSD-Tensorflow/master/notebooks/ssd_notebook.ipynb",
      "https://raw.githubusercontent.com/balancap/SSD-Tensorflow/master/notebooks/ssd_tests.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8170749643624405
      ],
      "excerpt": "In addition, if one wants to experiment/test a different Caffe SSD checkpoint, the former can be converted to TensorFlow checkpoints as following: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8302572593676458
      ],
      "excerpt": "The current version only supports Pascal VOC datasets (2007 and 2012). In order to be used for training a SSD model, the former need to be converted to TF-Records using the tf_convert_data.py script: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python tf_convert_data.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8776541719781755
      ],
      "excerpt": "| Model | Training data  | Testing data | mAP | FPS  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python eval_ssd_network.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "    --dataset_split_name=test \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python caffe_to_tensorflow.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8018709005434865
      ],
      "excerpt": "The script train_ssd_network.py is in charged of training the network. Similarly to TF-Slim models, one can pass numerous options to the training process (dataset, optimiser, hyper-parameters, model, ...). In particular, it is possible to provide a checkpoint file which can be use as starting point in order to fine-tune a network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_ssd_network.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "    --dataset_split_name=train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python eval_ssd_network.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "    --dataset_split_name=test \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    --wait_for_checkpoints=True \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_ssd_network.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "    --dataset_split_name=train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_ssd_network.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "    --dataset_split_name=train \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/balancap/SSD-Tensorflow/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SSD: Single Shot MultiBox Detector in TensorFlow",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SSD-Tensorflow",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "balancap",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/balancap/SSD-Tensorflow/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4067,
      "date": "Sun, 26 Dec 2021 07:16:31 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tensorflow",
      "ssd",
      "deep-learning",
      "yolo",
      "object-detection"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The [SSD Notebook](notebooks/ssd_notebook.ipynb) contains a minimal example of the SSD TensorFlow pipeline. Shortly, the detection is made of two main steps: running the SSD network on the image and post-processing the output using common algorithms (top-k filtering and Non-Maximum Suppression algorithm).\n\nHere are two examples of successful detection outputs:\n![](pictures/ex1.png \"SSD anchors\")\n![](pictures/ex2.png \"SSD anchors\")\n\nTo run the notebook you first have to unzip the checkpoint files in ./checkpoint\n```bash\nunzip ssd_300_vgg.ckpt.zip\n```\nand then start a jupyter notebook with\n```bash\njupyter notebook notebooks/ssd_notebook.ipynb\n```\n\n\n",
      "technique": "Header extraction"
    }
  ]
}