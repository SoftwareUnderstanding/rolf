{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2003.09044\n[3]:https://sites.google.com/view/fabric-vsf\n[4]:https://github.com/ryanhoque/fabric-vsf"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{fabric_vsf_2020,\n    author = {Ryan Hoque and Daniel Seita and Ashwin Balakrishna and Aditya Ganapathi and Ajay Tanwani and Nawid Jamali and Katsu Yamane and Soshi Iba and Ken Goldberg},\n    title = {{VisuoSpatial Foresight for Multi-Step, Multi-Task Fabric Manipulation}},\n    booktitle = {Robotics: Science and Systems (RSS)},\n    Year = {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9761627744986326
      ],
      "excerpt": "our RSS 2020 paper \"VisuoSpatial Foresight (VSF) for Multi-Step, Multi-Task \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9256420083044248
      ],
      "excerpt": "ability. If you have questions on how to use the code, please contact Daniel \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8257265130638167
      ],
      "excerpt": "source /data/envs/visual/bin/activate \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BerkeleyAutomation/dvrk-vismpc",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-14T23:59:29Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-12T11:40:09Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9150376520283997,
        0.9033133496204337
      ],
      "excerpt": "Update May 2020: this is the code we used for the physical fabrics \nexperiments with the dVRK. The master branch has the code for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9216986150913619,
        0.9383713747076485
      ],
      "excerpt": "The paper is [available on arXiv][2] and we have additional information on the \n[project website][3], along with [code for the simulator and training video \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9783438343530207
      ],
      "excerpt": "Disclaimer: some parts of the code might not be documented to the best of our \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8225470811430978
      ],
      "excerpt": "Seita (seita@berkeley.edu) with details on what you want to do, and I shall \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.98554665547266
      ],
      "excerpt": "This is not for the original smoothing paper. For that, [see this code][1]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9421061192921603
      ],
      "excerpt": "Before using the robot, we need to calibrate it to ensure that we have a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9262129370876665
      ],
      "excerpt": "frame. Put a 5x5 checkerboard on top of the foam rubber. Then go into the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.987469443235764
      ],
      "excerpt": "For quick testing of the da vinci with resorting to the machinery of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254749566248652
      ],
      "excerpt": "On the DGX: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9461458258960069
      ],
      "excerpt": "(i.e., \"recipe\") where containers are instances of an image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "scp dir_for_imgs/000-* seita@jensen.ist.berkeley.edu:/raid/for-daniel/dir_for_imgs/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "scp dir_for_imgs/001-* seita@jensen.ist.berkeley.edu:/raid/for-daniel/dir_for_imgs/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8116774108731166,
        0.8995688143898415
      ],
      "excerpt": "and so on! I modified Ryan's script so that it looks at /data/dir_for_images/ \nwhich is where /raid/for-daniel goes. Well, actually to make things simple, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "DVRK code for VisMPC + Fabric project",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BerkeleyAutomation/dvrk-vismpc/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 03:13:42 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/BerkeleyAutomation/dvrk-vismpc/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "BerkeleyAutomation/dvrk-vismpc",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We're using davinci-arm machine (formerly named davinci0).\n\nWe use a mix of the system Python 2.7 on there, and a Python 3.5 virtual env.\nUnlike in the original smoothing paper (from fall 2019) the camera code has\nbeen updated to use Python 3. However, the neural network code (as before)\nstill requires Python 3.\n\nTo install, first clone this repository. Then make a new Python 3.5 virtualenv\nfrom a reference virtualenv on davinci0:\n\n```\nsource ~/venv/bin/activate\n```\n\nI did `pip freeze` and put this in a file, and then made a new Python3 virtualenv:\n\n```\nvirtualenv venvs/py3-vismpc-dvrk  --python=python3\n```\n\nand then did `pip install -r [filename].txt` to get all the packages updated.\nAfter that, running `import zivid` should work. This should be the same\nvirtualenv that we use for loading TensorFlow.\n\n**Update: not sure what happened but we may have to install a Python2\nvirtualenv as well that uses the system site packages, so that we can do `pip\ninstall scikit-learn` and get the structural similarity code ... stay tuned!**\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8862302242936231
      ],
      "excerpt": "calibration. Use the system Python (it's Python 2.7). Once it looks good, save \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9130252313557959
      ],
      "excerpt": "Run something like this (assuming GPU 5 is available, check with nvidia-smi): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.830124526564826,
        0.8356154076599529,
        0.8104062105145108
      ],
      "excerpt": "IF MAKING CHANGES TO THE VISMPC CODE BE SURE TO COMPILE VIA python setup.py \ninstall, EVEN IF WE ARE JUST CHANGING C.E.M. HYPERPARAMETERS. \nIt takes a few seconds to start up. Eventually, this will run continuously like \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8649465909321873
      ],
      "excerpt": "at one number next. Get this back from the DGX: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9468595068638951
      ],
      "excerpt": "you can make the commands the same both ways: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.936948401893365
      ],
      "excerpt": "tests/ folder and run python test_03_checkerboard_pick_test.py to check the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8232501065682251
      ],
      "excerpt": "To test the camera code, just run python ZividCapture.py and adjust the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9075764168941234
      ],
      "excerpt": "rm dir_for_imgs/*.png ; rm dir_for_imgs/*.txt ; python ZividCapture.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372565577971909
      ],
      "excerpt": "python run.py --tier 1 --vf \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/BerkeleyAutomation/dvrk-vismpc/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Da Vinci Research Kit (dVRK) Code for Fabrics and Visual MPC",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "dvrk-vismpc",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "BerkeleyAutomation",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BerkeleyAutomation/dvrk-vismpc/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 03:13:42 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Performing our experiments involves several steps, due to loading a separate\nneural network and having it run continuously. Roughly, the steps are:\n\n0. Check the configuration file in `config.py` which will contain a bunch of\nsettings we can adjust. *In particular, adjust which calibration data we should\nbe using*. They are in the form of text files. There is a SECOND config file in\nthe network loading folder.\n\n1. Activate the robot via `roscore`, then (in a separate tab) run `./teleop` in\n`~/catkin_ws` and click HOME. This gets the dvrk setup.\n\n2. In another tab, *activate the Python 3 virtualenv above*, and run\n\n   ```\n   rm dir_for_imgs/*.png ; rm dir_for_imgs/result*.txt ; python call_network/load_net.py\n   ```\n   See `call_network/README.md` for detailed instructions.  This script runs\n   continuously in the background and checks for any new images in the target\n   directory. Removes images in the calibration directory.\n\n3. In another tab, *activate the Python 3 virtualenv above*, and run `python\nZividCapture.py`. This script runs continuously and will activate with a\nkeyboard command. Whenever we need a new set of images, we need to press enter\nat this TAB. Then that will take a picture, and save images indexed by a\nleading number. The neural net code will detect that number and load a specific\nfile w/that index.\n\n4. Finally, run `python run.py --tier X` for experiments, using the system\nPython. This requires some keyboard strokes. *The code runs one episode* and\nthen terminates.  Repeat for more episodes.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Here's a minimal working example, e.g., could be in `tests/test_01_positions.py`:\n\n```python\nimport sys\nsys.path.append('.')\nimport dvrkArm\n\nif __name__ == \"__main__\":\n    p = dvrkArm.dvrkArm('/PSM1')\n    pose_frame = p.get_current_pose_frame()\n    pose_rad = p.get_current_pose()\n    pose_deg = p.get_current_pose(unit='deg')\n\n    print('pose_frame:\\n{}'.format(pose_frame))\n    print('pose_rad: {}'.format(pose_rad))\n    print('pose_deg: {}'.format(pose_deg))\n\n    targ_pos = [0.04845971,  0.05464517, -0.12231114]\n    targ_rot = [4.65831872, -0.69974499,  0.87412989]\n    p.set_pose(pos=targ_pos, rot=targ_rot, unit='rad')\n```\n\nand run `python tests/test_01_positions.py`. This will print out pose\ninformation about the dvrk arm, which can then be used to hard code some\nactions. For example, above we show how to set the pose of the arm given\npre-computed `targ_pos` and `targ_rot`.\n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}