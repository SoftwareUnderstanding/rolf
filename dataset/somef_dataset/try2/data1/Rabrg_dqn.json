{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1312.5602 [cs]",
      "https://arxiv.org/abs/ 1312.5602",
      "https://arxiv.org/abs/1509.06461 [cs]",
      "https://arxiv.org/abs/ 1509.06461"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{van_hasselt_deep_2015,\n    title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},\n    url = {http://arxiv.org/abs/1509.06461},\n    abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},\n    urldate = {2021-08-21},\n    journal = {arXiv:1509.06461 [cs]},\n    author = {van Hasselt, Hado and Guez, Arthur and Silver, David},\n    month = dec,\n    year = {2015},\n    note = {arXiv: 1509.06461},\n    keywords = {Computer Science - Machine Learning},\n    file = {arXiv Fulltext PDF:/Users/ryangr/Zotero/storage/Q42ZS9F7/van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf:application/pdf;arXiv.org Snapshot:/Users/ryangr/Zotero/storage/NKNFH2K8/1509.html:text/html},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{mnih_playing_2013,\n    title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},\n    url = {http://arxiv.org/abs/1312.5602},\n    abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},\n    urldate = {2021-08-21},\n    journal = {arXiv:1312.5602 [cs]},\n    author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},\n    month = dec,\n    year = {2013},\n    note = {arXiv: 1312.5602},\n    keywords = {Computer Science - Machine Learning},\n    file = {arXiv Fulltext PDF:/Users/ryangr/Zotero/storage/ANV64XSM/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/ryangr/Zotero/storage/WDPY5Y6P/1312.html:text/html},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9256351921816347,
        0.9960946386010011
      ],
      "excerpt": "    title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning}, \n    url = {http://arxiv.org/abs/1509.06461}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9967495074413848,
        0.9986097054491982
      ],
      "excerpt": "    journal = {arXiv:1509.06461 [cs]}, \n    author = {van Hasselt, Hado and Guez, Arthur and Silver, David}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9864698047705643,
        0.887167692142383,
        0.9942872316509309,
        0.9977056916365334
      ],
      "excerpt": "    year = {2015}, \n    note = {arXiv: 1509.06461}, \n    keywords = {Computer Science - Machine Learning}, \n    file = {arXiv Fulltext PDF:/Users/ryangr/Zotero/storage/Q42ZS9F7/van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf:application/pdf;arXiv.org Snapshot:/Users/ryangr/Zotero/storage/NKNFH2K8/1509.html:text/html}, \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rabrg/dqn",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-21T02:05:17Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-16T05:37:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9846597680703448
      ],
      "excerpt": "A PyTorch implementation of DeepMind's DQN algorithm with the Double DQN (DDQN) improvement. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9922772475720741
      ],
      "excerpt": "    abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A PyTorch implementation of DeepMind's DQN algorithm with the Double DQN (DDQN) improvement.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rabrg/dqn/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 01:54:56 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Rabrg/dqn/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rabrg/dqn",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Rabrg/dqn/main/demo.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Rabrg/dqn/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "dqn",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "dqn",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rabrg",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rabrg/dqn/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Thu, 23 Dec 2021 01:54:56 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning",
      "dqn",
      "pytorch"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nimport gym\nfrom torch import nn\n\n\nenv = gym.make(\"LunarLander-v2\")\nmodel = nn.Sequential(\n    nn.Linear(env.observation_space.shape[0], 64),\n    nn.ReLU(),\n    nn.Linear(64, 32),\n    nn.ReLU(),\n    nn.Linear(32, env.action_space.n),\n)\n\ndqn = DQN(env, model)\ndqn.learn(n_episodes=300)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}