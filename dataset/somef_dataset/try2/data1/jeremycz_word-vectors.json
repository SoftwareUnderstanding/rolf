{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n1. [2013, Mikolov et al. Efficient Estimation of Word Representations in Vector Space. arxiv:1307.3781v3](https://arxiv.org/pdf/1301.3781.pdf)\r\n2. [2013, Mikolov et al. Distributed Representations of Words and Phrases and their Compositionality. NIPS 2013.](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jeremycz/word-vectors",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-20T09:08:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-01-05T21:51:32Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n- Most words are symbols for an extra-linguistic entity - a word is a signifier that maps to a signified (idea/thing)\r\n- Approx. 13m words in English language\r\n  - There is probably some N-dimensional space (such that N << 13m) that is sufficient to encode all semantics of our language\r\n- Most simple word vector - one-hot encoding\r\n  - Denotational semantics - the concept of representing an idea as a symbol - a word or one-hot vector - sparse, cannot capture similarity - localist encoding\r\n\r\nEvaluation\r\n\r\n- Intrinsic - evaluation on a specific, intermediate task\r\n  - Fast to compute\r\n  - Aids with understanding of the system\r\n  - Needs to be correlated with real task to provide a good measure of usefulness\r\n  - Word analogies - popular intrinsic evaluation method for word vectors\r\n    - Semantic - e.g. King/Man | Queen/Woman\r\n    - Syntactic - e.g. big/biggest | fast/fastest\r\n- Extrinsic - evaluation on a real task\r\n  - Slow\r\n  - May not be clear whether the problem with low performance is related to a particular subsystem, other subsystems, or interactions between subsystems\r\n  - If a subsystem is replaced and performance improves, the change is likely to be good\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9852445704213684
      ],
      "excerpt": "A repository to explore dense representations of words. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9017465361165276
      ],
      "excerpt": "Perform a SVD on $X$ to get a $USV^T$ decomposition \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9283259893626524,
        0.8145191933020692
      ],
      "excerpt": "$X \\in \\mathcal{R}^{V \\times M}$, where $M$ is the number of documents \nWindow-based co-occurrence \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8719725240229104,
        0.8361165816151138
      ],
      "excerpt": "Matrix is high-dimensional (quadratic cost for SVD) \nNeed to perform some hacks to adjust for imbalanced word frequencies \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8949977453539416,
        0.8785389384626203,
        0.8513052285016568,
        0.803956797799767
      ],
      "excerpt": "Weight co-occurrence counts based on distance between words in the document \nUse Pearson correlation and set negative counts to 0 instead of using just the raw count \nTwo model architectures: \nContinuous Bag-of-Words (CBOW) - uses context words to predict target word \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8145409713294852,
        0.8744074273077124,
        0.8254417868083708
      ],
      "excerpt": "Better for frequent words and lower dimensional vectors \nHierarchical softmax - define objective using an efficient tree structure to compute probabilities for the complete vocabulary \nBetter for infrequent words \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9822284681774022
      ],
      "excerpt": "Since $y_j = 1$ for the target word and $y_j = 0$ for all other words, the equation reduces to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9616345870710619
      ],
      "excerpt": "where $\\hat{y}$ is the predicted probability of the target word. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9191790640028146
      ],
      "excerpt": "$c$ is the size of the training context (which can be a function of the center word $w$) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8432645815537804
      ],
      "excerpt": "The probability $P(w_{t+j}|w_t)$ is calculated using the softmax function: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9897006022487285,
        0.8969512574138234
      ],
      "excerpt": "$u_w$ and $v_w$ are the 'input' and 'output' vector representations of $w$, and $W$ is the size of the vocabulary \nThis formulation is impractical computationally because it requires computing the softmax over all the representations in the vocabulary \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8432645815537804
      ],
      "excerpt": "The probability $P(w_t|w_c)$ is calculated using the softmax function: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9730051716472665
      ],
      "excerpt": "where $v_C$ is the sum of 'output' representations of all words in the context window: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for exploring word vectors",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jeremycz/word-vectors/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 18:36:56 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jeremycz/word-vectors/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jeremycz/word-vectors",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jeremycz/word-vectors/master/svd_word_vectors.ipynb",
      "https://raw.githubusercontent.com/jeremycz/word-vectors/master/tf_idf_word_vectors.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jeremycz/word-vectors/master/a2/get_datasets.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8060949797375969
      ],
      "excerpt": "Larger $c$ - more training examples, higher accuracy, increased training time \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jeremycz/word-vectors/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell",
      "PowerShell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Word Vectors",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "word-vectors",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jeremycz",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jeremycz/word-vectors/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 18:36:56 GMT"
    },
    "technique": "GitHub API"
  }
}