{
  "citation": [
    {
      "confidence": [
        0.999760757257886,
        0.9896534279187987,
        0.9999894746916713,
        0.9896534279187987
      ],
      "excerpt": "[1] Han Zhang, Ian Goodfellow, Dimitris Metaxas, Augustus Odena: \u201cSelf-Attention Generative Adversarial Networks\u201d, 2018; \nFull paper: https://arxiv.org/pdf/1805.08318.pdf \n[2] Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He: \u201cNon-local Neural Networks\u201d, 2017;  \nFull paper: https://arxiv.org/pdf/1711.07971.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/franknb/Self-attention-DCGAN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-23T00:05:08Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-03T16:08:22Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9426862576298026,
        0.9207834987361264,
        0.9611903772595157,
        0.9082282409081271,
        0.9312202276556264
      ],
      "excerpt": "An idea that has been out there for years, until Google spoke for it in 2017 as mentioned below in paper [3]. It worked as the following steps in the case of images: \nUsing a kernel size 1 convo to generate Query, Key and Value layers, with the shape of Channels * N, N = Width * Height. \nGenerate attention map by the matrix dot product of Query and Key, with the shape of N * N. The N * N attention map describe each pixel's attention score on every other pixels, hence the name \"self-attention\". Pixels here mean data points in input matrices. \nGet attention weights by the matrix dot product of Value and attention map, with the shape of C * N. The attention weights describe each pixel's total attention score throughout all pixels. We then reshape the attention weights into C * W * H. \nAdd the attention weights back onto input layer it self with a weight of Gamma, a learning parameter initializing at 0. It means that the self-attention module does not do anything initially. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9435314860644959
      ],
      "excerpt": "For the 1 * 28 * 28 input case, I built such model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "An application of Self-Attention GANs and DCGAN on mnist dataset.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/franknb/Self-attention-DCGAN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Sun, 26 Dec 2021 12:06:56 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/franknb/Self-attention-DCGAN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "franknb/Self-attention-DCGAN",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/franknb/Self-attention-DCGAN/master/Self_attention.ipynb",
      "https://raw.githubusercontent.com/franknb/Self-attention-DCGAN/master/Self_attention2.ipynb",
      "https://raw.githubusercontent.com/franknb/Self-attention-DCGAN/master/SAGAN_mnist.ipynb",
      "https://raw.githubusercontent.com/franknb/Self-attention-DCGAN/master/SAGAN_celeba.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8712457860056126
      ],
      "excerpt": "Using a kernel size 1 convo to generate Query, Key and Value layers, with the shape of Channels * N, N = Width * Height. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/franknb/Self-attention-DCGAN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Self-attention-DCGAN",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Self-attention-DCGAN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "franknb",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/franknb/Self-attention-DCGAN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. SAGAN_mnist.ipynb & SAGAN_celeba.ipynb: Notebooks interface containing codes for loading datasets, utility functions and training function.\n\n2. model_28.py & model_64.py: Contains self-attention, Generator and Discriminator modules for 28 by 28 version and 64 by 64 version, respectively.\n\n3. spectral.py: Containing spectral normalization module. Borrowed from https://github.com/heykeetae/Self-Attention-GAN. Can be removed given minor change in model.py.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 22,
      "date": "Sun, 26 Dec 2021 12:06:56 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "dcgan",
      "pytorch",
      "self-attention",
      "deep-learning",
      "sagan",
      "mnist"
    ],
    "technique": "GitHub API"
  }
}