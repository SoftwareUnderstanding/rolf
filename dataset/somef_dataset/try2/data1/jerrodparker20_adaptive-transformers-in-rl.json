{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1910.06764",
      "https://arxiv.org/abs/1905.07799",
      "https://arxiv.org/abs/1802.01561"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this repository useful, do cite it with,\n```\n@article{kumar2020adaptive,\n    title={Adaptive Transformers in RL},\n    author={Shakti Kumar and Jerrod Parker and Panteha Naderian},\n    year={2020},\n    eprint={2004.03761},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{kumar2020adaptive,\n    title={Adaptive Transformers in RL},\n    author={Shakti Kumar and Jerrod Parker and Panteha Naderian},\n    year={2020},\n    eprint={2004.03761},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jerrodparker20/adaptive-transformers-in-rl",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-10T00:24:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-09T09:06:55Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8362387677358714,
        0.9173037373998963,
        0.9692165454862999
      ],
      "excerpt": "Official implementation of Adaptive Transformers in RL \nIn this work we replicate several results from Stabilizing Transformers for RL on both Pong and rooms_select_nonmatching_object from DMLab30.  \nWe also extend the Stable Transformer architecture with Adaptive Attention Span on a partially observable (POMDP) setting of Reinforcement Learning. To our knowledge this is one of the first attempts to stabilize and explore Adaptive Attention Span in an RL domain. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8596185136483986
      ],
      "excerpt": "Downloading Atari: Getting Started with Gym\u2013 http://gym.openai.com/docs/#getting-started-with-gym \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.937931815470356
      ],
      "excerpt": "For more details on other flags, see the top of train.py (include a link to this file) which has descriptions for each. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Adaptive Attention Span for Reinforcement Learning",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jerrodparker20/adaptive-transformers-in-rl/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 13,
      "date": "Mon, 27 Dec 2021 17:17:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jerrodparker20/adaptive-transformers-in-rl/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jerrodparker20/adaptive-transformers-in-rl",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jerrodparker20/adaptive-transformers-in-rl/master/adaptive_span2/get_pretrained.sh",
      "https://raw.githubusercontent.com/jerrodparker20/adaptive-transformers-in-rl/master/adaptive_span2/get_data.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9665215968409407,
        0.9825729248922788
      ],
      "excerpt": "Build DMLab package with Bazel\u2013 https://github.com/deepmind/lab/blob/master/docs/users/build.md \nInstall the python module for DMLab\u2013 https://github.com/deepmind/lab/tree/master/python/pip_package \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8689545651528952
      ],
      "excerpt": "To run without a GPU, use the flag \u201c--disable_cuda\u201d. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8543074238920702
      ],
      "excerpt": "All experiments use a slightly revised version of IMPALA from torchbeast \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py --total_steps 20000000 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py --total_steps 10000000 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8200248990569099
      ],
      "excerpt": "--sleep_length 5 --atari True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py --total_steps 20000000 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jerrodparker20/adaptive-transformers-in-rl/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Adaptive Transformers  in RL",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "adaptive-transformers-in-rl",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jerrodparker20",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jerrodparker20/adaptive-transformers-in-rl/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 83,
      "date": "Mon, 27 Dec 2021 17:17:31 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "rl",
      "pomdp",
      "transformer",
      "attention-mechanism",
      "stabilizing-transformers-for-rl",
      "adaptive-attention",
      "transformers-in-rl",
      "torchbeast",
      "impala",
      "monobeast"
    ],
    "technique": "GitHub API"
  }
}