{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/cs/9905014\">Dietterich</a>.\n </td>\n  <td><img src='gifs/example.gif' width='120' height='185.25'></td>\n</tr>\n</tbody>\n</table>\n\n-------------------------------------------------------------------------------------\n### Motivation\n\nIt is a well known empirical fact in reinforcement learning that\nmodel-based approaches (e.g., <i>R</i><sub><font\nsize=\"4\">max</font></sub>",
      "https://arxiv.org/abs/1312.5602",
      "https://arxiv.org/abs/cs/9905014"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.895872907375662
      ],
      "excerpt": "An Object-Oriented Representation for Efficient Reinforcement Learning (Paper by C. Diuk et al.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9475303146148999
      ],
      "excerpt": "    This task was introduced by <a href=\"https://arxiv.org/abs/cs/9905014\">Dietterich</a>. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8370263667029935
      ],
      "excerpt": "Motivated by human intelligence, Diuk et al. introduce a new \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9227447097351363
      ],
      "excerpt": "Diuk et al. provide a learning algorithm for deterministic \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "    <td align=\"center\">11.8s</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "    <td align=\"center\">30.5ms</td> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/borea17/efficient_rl",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-06T11:01:50Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-25T19:07:42Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This part shall give an overview about the different reimplemented\nalgorithms. These can be divided into *model-free* and *model-based* approaches.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8022095138919544,
        0.9658290498293729
      ],
      "excerpt": "Motivation | Summary | Results | How to use this repository \nThis is a Python reimplementation for the Taxi domain of  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8267699898514631,
        0.9612098438341031
      ],
      "excerpt": "  <td>In the <i>Taxi domain</i> the goal is to navigate the <i>taxi</i> (initially yellow box) towards<br> \n    the <i>passenger</i> (blue letter), take a <i>Pickup</i> action and then deliver the <i>taxi with<br>passenger inside</i> (green box) towards the <i>destination</i> (magenta letter) and perform <br> a <i>Dropoff</i> action. A reward of -1 is obtained for every time step it takes until delivery. <br>Successful <i>Dropoff</i> results in +20 reward, while non-successful <i>Dropoff</i> or <i>Pickup</i> is<br> penalized with -10.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9420975812182235,
        0.9274831846976022,
        0.8584363617125468
      ],
      "excerpt": "It is a well known empirical fact in reinforcement learning that \nmodel-based approaches (e.g., <i>R</i><sub><font \nsize=\"4\">max</font></sub>) are more sample-efficient than model-free \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9539266921082785,
        0.8540792871602216
      ],
      "excerpt": "that model-based learning tackles the exploration-exploitation dilemma \nin a smarter way by using the accumulated experience to build an \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8238352260910513,
        0.8541280207593405
      ],
      "excerpt": "make model-based learning even more sample-efficient. Factored MDPs \nenable an effective parametrization of transition and reward dynamics \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9162975614847789
      ],
      "excerpt": "can be learned with less samples. A major downside of these approaches \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8895765753029327,
        0.8675659764345022
      ],
      "excerpt": "environments and their dynamics. As it turns out, humans are way more \nsample-efficient than state-of-the-art algorithms when playing games  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9326855949193057
      ],
      "excerpt": "needs the objects and relations to consider, which seems more natural \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8717528069336231,
        0.978009645774756,
        0.8862171444326763
      ],
      "excerpt": "without having an actual model of the environment. Probably the most \nfamous model-free algorithm is Q-learning which also builds the \nbasis for the (perhaps even more famous) DQN paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.961353936299998,
        0.8420624757759757
      ],
      "excerpt": "reward r and new state s<sup>'</sup>, the update rule is very \nsimple and is derived from Bellman's optimality equation: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9381892816937946,
        0.8151969665782826,
        0.8628410838468509
      ],
      "excerpt": "where &alpha; is the learning rate. To allow for exploration, \nQ-learning commonly uses &epsi;-greedy exploration or the Greedy \nin the Limit with Infinite Exploration approach (see David Silver, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9003641194062313
      ],
      "excerpt": "Diuk uses two variants of Q-learning: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9752614162543028,
        0.9794246386128456
      ],
      "excerpt": "* Q-learning with optimistic initialization: instead of some \n  random initialization of the Q-table a smart initialization to an \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8746167805262706,
        0.836456888157749
      ],
      "excerpt": "  is used. Thereby unvisited state-action pairs become more likely to be \n  visited. Here, &alpha; was is to 1 (deterministic environment) and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.968697251457692
      ],
      "excerpt": "In model-based approaches the agent learns a model of the environment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8725126410489348
      ],
      "excerpt": "between the following three algorithms lies in the way they learn the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.95988858813294
      ],
      "excerpt": "state-of-the-art algorithm to surpass the exploration-exploitation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.922222549190672,
        0.8709177184239919
      ],
      "excerpt": "is known, the algorithm uses the empirical transition and reward \nfunction for planning. In case a state is unknown,  R<sub><font \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8073594483647484
      ],
      "excerpt": "that for planning. Therefore, actions which have not been tried out \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8829928996177823,
        0.9056416910031123,
        0.8680610840337273
      ],
      "excerpt": "known action also leads to maximal return. The parameter M defines the number of \nobservations the agent has to see until it considers a \ntransition/reward to be known, in a deterministic case such as the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016,
        0.9394449182630016
      ],
      "excerpt": " - 5 x positions for taxi \n - 5 y positions for taxi \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8503218362707826
      ],
      "excerpt": "number of states |S|, the number of actions |A|), the states are \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8281771127222491
      ],
      "excerpt": "grid and learns the state transition (more precisely it would learn \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8963971600246392
      ],
      "excerpt": "To address this shortcoming, the agent needs a different \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8294802782677463,
        0.8092080807452401
      ],
      "excerpt": "Factored R<sub><font size=\"4\">max</font></sub> is a R<sub><font \nsize=\"4\">max</font></sub> adaptation that builds on a factored MDP \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9322195442003052
      ],
      "excerpt": "location at time t+1 under action North is independent of the y \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8297406623395921,
        0.8358357707912569,
        0.9739105370286502,
        0.8391020571491679,
        0.9739220713386969
      ],
      "excerpt": "different DBN) and it enables Factored R<sub><font \nsize=\"4\">max</font></sub> to much more sample-efficient learning.  \nThe downside of this approach is that this kind of prior knowledge may not \nbe available and that it lacks some generalization, e.g., although \nFactored R<sub><font size=\"4\">max</font></sub> knows that the x location is independent of all other state \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8042644517065922
      ],
      "excerpt": "to perfom action North at each x location to learn the outcome. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8092080807452401
      ],
      "excerpt": "size=\"4\">max</font></sub> adaptation that builds on a deterministic (propositional) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9772536027953318,
        0.8124103805305432
      ],
      "excerpt": "is based on objects and their interactions, a state is presented as  \nthe union of all (object) attribute values. Additionally, each state \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8569925547571362,
        0.9674723420172031,
        0.9116887321107938,
        0.8164739969375759,
        0.973879600651369,
        0.8077234200607534,
        0.8731423113472382,
        0.8611492861655003,
        0.883520940627667
      ],
      "excerpt": "enabled and which are not in that state. During a transition each \nattribute of the state may exert some kind of effect which results in \nan attribute change. There are some limitations to the effects that can \noccur which are well explained in Diuk's dissertation. The basic idea \nof DOOR<sub><font size=\"4\">max</font></sub> is to recover the \ndeterministic OO MDP using condition-effect learners (in these \nlearners conditions are basically the relations that need to hold in \norder for an effect to occur). \nThe paper results show that in DOOR<sub><font size=\"4\">max</font></sub> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8759958869802035,
        0.8847951695929761
      ],
      "excerpt": "offers better generalization. Another feature is that the learned transition \ndynamics is easy to interpret, e.g., DOOR<sub><font \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8330436302601804
      ],
      "excerpt": "There are some differences between this reimplementation and Diuk's \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9050717092348886
      ],
      "excerpt": "1) For educational purposes, the reward function is also learned in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "   transition model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9533050500984324
      ],
      "excerpt": "   the same setting, there is a discrepancy for Q-learning. When the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8911209728809975,
        0.948462427630391,
        0.9671716218961514
      ],
      "excerpt": "   not be the same (these are the results in brackets), similar results to Diuk could be obtained. \n3) Some implementation details are different such as the update procedure \n   of the empirical transition and reward functions or the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9396051868482842,
        0.9542338946496343,
        0.8924714785095612
      ],
      "excerpt": "The dissertation results align with the reimplementation results. Clearly, DOOR<sub><font size=\"4\">max</font></sub> outperforms the other algorithms in terms of sample-efficiency. \nFor the differences in Q-Learning and the values in brackets, refer to \n2) of Differences between Reimplementation and Diuk. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702819545651609
      ],
      "excerpt": "    <td>|<i>A</i>|, visualization of game</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702819545651609
      ],
      "excerpt": "    <td>|<i>A</i>|, visualization of game</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9895781324478673,
        0.954085103420836
      ],
      "excerpt": "The paper results align with the reimplementation results. These results show that DOOR<sub><font size=\"4\">max</font></sub>  not only outperforms Factored R<sub><font size=\"4\">max</font></sub> in terms of sample-efficiency, but also scales much better to larger problems. Note that the number of states increases by a factor of more than 14.  \nThe results were obtained on a cluster from which I do not know the CPU specifics (this is not too important since the focus lies on the comparison). Note that Diuk et al. used a more powerful machine for the paper result: the average step times are notably smaller compared to the dissertation results.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8606489350925353
      ],
      "excerpt": "Defaultly, each agent runs only once. To increase the number of repetitions change n_repetitions in the scripts.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8535349753820591
      ],
      "excerpt": "are extensions to the gym Taxi environment in the corresponding folders.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Reimplementation of \"An Object-Oriented Representation for Efficient RL\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/borea17/efficient_rl/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 13:22:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/borea17/efficient_rl/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "borea17/efficient_rl",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The experimental setup is described on p.31 of Diuk's Dissertation or\np.7 of the paper. It consists of testing against six probe states and reporting the number\nof steps the agent had to take until the optimal policy for these 6\nstart states was reached. Since there is some randomness in the\ntrials, each algorithm runs 100 times and the results are then averaged. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.849438557486644
      ],
      "excerpt": "dependency relations between state variables, thereby the environment dynamics \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9504234035981496
      ],
      "excerpt": "environment dynamics. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8600260492236664
      ],
      "excerpt": "steps until it has fully learned the 5x5 Taxi transition dynamics. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9049577316825731
      ],
      "excerpt": "relations for the environment dynamics between variables using \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9593182947734069,
        0.9906248903846466,
        0.9820226428242687,
        0.999746712887969
      ],
      "excerpt": "git clone --depth 1 https://github.com/borea17/efficient_rl/ \ncd efficient_rl \npython setup.py install \npip install efficient_rl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8032069344308966
      ],
      "excerpt": "are extensions to the gym Taxi environment in the corresponding folders.  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9235277997891415
      ],
      "excerpt": "  <td><img src='gifs/example.gif' width='120' height='185.25'></td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8030898217399196
      ],
      "excerpt": "The basic idea is to start with a random action-value function and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8228532310995463
      ],
      "excerpt": "After successful installation, download dissertation_script.py and paper_script.py (which are in folder efficient_rl), then run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.9246227682586091
      ],
      "excerpt": "python dissertation_script.py  \npython paper_script.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/borea17/efficient_rl/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 borea17\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Efficient Reinforcement Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "efficient_rl",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "borea17",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/borea17/efficient_rl/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Wed, 29 Dec 2021 13:22:31 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reproducible-research",
      "reinforcement-learning",
      "model-based-rl",
      "efficient-rl",
      "door-max"
    ],
    "technique": "GitHub API"
  }
}