{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use (part of) my code or find my work helpful, please consider citing\n```\n@misc{queianchen_nerf,\n  author={Quei-An, Chen},\n  title={Nerf_pl: a pytorch-lightning implementation of NeRF},\n  url={https://github.com/kwea123/nerf_pl/},\n  year={2020},\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{queianchen_nerf,\n  author={Quei-An, Chen},\n  title={Nerf_pl: a pytorch-lightning implementation of NeRF},\n  url={https://github.com/kwea123/nerf_pl/},\n  year={2020},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "Reconstruct colored mesh! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "   --num_epochs 30 --batch_size 1024 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "   --lr_scheduler steplr --decay_step 10 20 --decay_gamma 0.5 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kwea123/nerf_pl",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-12T14:12:06Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T22:03:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8014819820619186
      ],
      "excerpt": "Please see each subsection for training on different datasets. Available training datasets: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9459307401409588,
        0.9790032325133918
      ],
      "excerpt": "These parameters are chosen to best mimic the training settings in the original repo. See opt.py for all configurations. \nNOTE: the above configuration doesn't work for some scenes like drums, ship. In that case, consider increasing the batch_size or change the optimizer to radam. I managed to train on all scenes with these modifications. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9459307401409588
      ],
      "excerpt": "These parameters are chosen to best mimic the training settings in the original repo. See opt.py for all configurations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8741273755876782
      ],
      "excerpt": "2. Prepare your images in a folder (around 20 to 30 for forward facing, and 40 to 50 for 360 inward-facing) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9700922920366112
      ],
      "excerpt": "For more details of training a good model, please see the video [here](#colab). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8822652778461221,
        0.956754664158235
      ],
      "excerpt": "The speed is measured on 1 RTX2080Ti. Detailed profile can be found in release. \nTraining memory is largely reduced, since the original repo loads the whole data to GPU at the beginning, while we only pass batches to GPU every step. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8158980963454899
      ],
      "excerpt": "IMPORTANT : Don't forget to add --spheric_poses if the model is trained under --spheric setting! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108882284243888
      ],
      "excerpt": "Example of lego scene using pretrained model and the reconstructed colored mesh: (PSNR=31.39, paper=32.54) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.859374229253056,
        0.8289139949900708
      ],
      "excerpt": "Example of fern scene using pretrained model: \nExample of own scene (Silica GGO figure) and the reconstructed colored mesh. Click to link to youtube video. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.95670568713337,
        0.8074594787847433,
        0.9873693692003337,
        0.9508027831780367,
        0.8914456829156556
      ],
      "excerpt": "The concept of NeRF is that the whole scene is compressed into a NeRF model, then we can render from any pose we want. To render from plausible poses, we can leverage the training poses; therefore, you can generate video with only the trained model and the poses (hence the name of portable scenes). I provided my silica model in release, feel free to play around with it! \nIf you trained some interesting scenes, you are also welcomed to share the model (and the poses_bounds.npy) by sending me an email, or post in issues! After all, a model is just around 5MB! Please run python utils/save_weights_only.py --ckpt_path $YOUR_MODEL_PATH to extract the final model. \nSee README_mesh for reconstruction of colored mesh. Only supported for blender dataset and 360 inward-facing data! \nThe learning rate decay in the original repo is by step, which means it decreases every step, here I use learning rate decay by epoch, which means it changes only at the end of 1 epoch. \nThe validation image for LLFF dataset is chosen as the most centered image here, whereas the original repo chooses every 8th image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8800822185810714
      ],
      "excerpt": "colmap to prepare camera poses for your own training data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9331441137122523,
        0.8833216497565767,
        0.8526396414490823
      ],
      "excerpt": "Please see this playlist for the detailed tutorials. \nWe can incorporate ray tracing techniques into the volume rendering pipeline, and realize realistic scene editing (following is the materials scene with an object removed, and a mesh is inserted and rendered with ray tracing). The code will not be released. \nWith my integration in Unity, I can realize realistic mixed reality photos (note my character casts shadow on the scene, zero post- image editing required): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "NeRF (Neural Radiance Fields) and NeRF in the Wild using pytorch-lightning",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Download `nerf_synthetic.zip` from [here](https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Download `nerf_llff_data.zip` from [here](https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1)\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kwea123/nerf_pl/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 137,
      "date": "Tue, 28 Dec 2021 23:18:44 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kwea123/nerf_pl/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kwea123/nerf_pl",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/kwea123/nerf_pl/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/kwea123/nerf_pl/master/test.ipynb",
      "https://raw.githubusercontent.com/kwea123/nerf_pl/master/extract_mesh.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9525812011662587,
        0.9791151766027405,
        0.9938279456168801,
        0.8331681177237508,
        0.9985095379288673,
        0.9998577334674871
      ],
      "excerpt": "NVIDIA GPU with CUDA>=10.1 (tested with 1 RTX2080Ti) \nClone this repo by git clone --recursive https://github.com/kwea123/nerf_pl \nPython>=3.6 (installation via anaconda is recommended, use conda create -n nerf_pl python=3.6 to create a conda environment and activate it by conda activate nerf_pl) \nPython libraries \nInstall core requirements by pip install -r requirements.txt \nInstall torchsearchsorted by cd torchsearchsorted then pip install . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289637016648501
      ],
      "excerpt": "NOTE: the above configuration doesn't work for some scenes like drums, ship. In that case, consider increasing the batch_size or change the optimizer to radam. I managed to train on all scenes with these modifications. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.938462292827418
      ],
      "excerpt": "1. Install [COLMAP](https://github.com/colmap/colmap) following [installation guide](https://colmap.github.io/install.html) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8734264513150277
      ],
      "excerpt": "3. Clone [LLFF](https://github.com/Fyusion/LLFF) and run `python img2poses.py $your-images-folder` \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9322609392449874
      ],
      "excerpt": "| Ref pytorch  |  6.0 | 0.147s | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8452302782036695
      ],
      "excerpt": "I also prepared colab notebooks that allow you to run the algorithm on any machine without GPU requirement. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9065932471709682,
        0.9503189345333785
      ],
      "excerpt": "Run (example) \npython train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9065932471709682,
        0.9503189345333785
      ],
      "excerpt": "Run (example) \npython train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8266860879780539
      ],
      "excerpt": "Download the pretrained models and training logs in release. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8943096850060217
      ],
      "excerpt": "python eval.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8279412852037151
      ],
      "excerpt": "It will create folder results/{dataset_name}/{scene_name} and run inference on all test data, finally create a gif out of them. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kwea123/nerf_pl/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Quei-An Chen\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "nerf_pl",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "nerf_pl",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kwea123",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kwea123/nerf_pl/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "kwea123",
        "body": "Used for nerfw branch.\r\n\r\nTrain command (trained on 8 time downscaled images, just for proof of implementation):\r\n```\r\npython prepare_phototourism.py --root_dir /home/ubuntu/data/IMC-PT/brandenburg_gate/ --img_downscale 8\r\n\r\npython train.py \\\r\n  --root_dir /home/ubuntu/data/IMC-PT/brandenburg_gate/ --dataset_name phototourism \\\r\n  --img_downscale 8 --use_cache \\\r\n  --N_importance 64 --N_samples 64 --encode_a --encode_t --beta_min 0.03 --N_vocab 1500 --N_emb_xyz 15 \\\r\n  --num_epochs 20 --batch_size 1024 \\\r\n  --optimizer adam --lr 5e-4 --lr_scheduler cosine \\\r\n  --exp_name brandenburg_scale8_nerfw\r\n```\r\n\r\nProfiler Report\r\n```\r\nAction                      \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\r\n-----------------------------------------------------------------------------------------------------------------------------\r\nTotal                       \t|  -              \t|_              \t|  2.5398e+04     \t|  100 %          \t|\r\n-----------------------------------------------------------------------------------------------------------------------------\r\nrun_training_epoch          \t|  1269.8         \t|20             \t|  2.5396e+04     \t|  99.991         \t|\r\nrun_training_batch          \t|  0.14633        \t|170760         \t|  2.4988e+04     \t|  98.384         \t|\r\noptimizer_step_and_closure_0\t|  0.12823        \t|170760         \t|  2.1896e+04     \t|  86.212         \t|\r\ntraining_step_and_backward  \t|  0.1241         \t|170760         \t|  2.1192e+04     \t|  83.438         \t|\r\nmodel_backward              \t|  0.099837       \t|170760         \t|  1.7048e+04     \t|  67.124         \t|\r\nmodel_forward               \t|  0.024055       \t|170760         \t|  4107.6         \t|  16.173         \t|\r\non_train_batch_end          \t|  0.00052083     \t|170760         \t|  88.938         \t|  0.35018        \t|\r\nget_train_batch             \t|  0.00023393     \t|170760         \t|  39.946         \t|  0.15728        \t|\r\nevaluation_step_and_end     \t|  0.52576        \t|21             \t|  11.041         \t|  0.043472       \t|\r\ncache_result                \t|  1.2894e-05     \t|854050         \t|  11.012         \t|  0.043357       \t|\r\non_after_backward           \t|  1.0743e-05     \t|170760         \t|  1.8345         \t|  0.007223       \t|\r\non_batch_start              \t|  1.0535e-05     \t|170760         \t|  1.799          \t|  0.0070832      \t|\r\non_batch_end                \t|  9.6894e-06     \t|170760         \t|  1.6546         \t|  0.0065145      \t|\r\non_before_zero_grad         \t|  8.5198e-06     \t|170760         \t|  1.4548         \t|  0.0057282      \t|\r\ntraining_step_end           \t|  6.6891e-06     \t|170760         \t|  1.1422         \t|  0.0044974      \t|\r\non_train_batch_start        \t|  5.9285e-06     \t|170760         \t|  1.0124         \t|  0.003986       \t|\r\non_validation_end           \t|  0.027978       \t|21             \t|  0.58754        \t|  0.0023133      \t|\r\non_validation_batch_end     \t|  0.00055518     \t|21             \t|  0.011659       \t|  4.5904e-05     \t|\r\non_epoch_start              \t|  0.00054319     \t|20             \t|  0.010864       \t|  4.2774e-05     \t|\r\non_validation_start         \t|  0.00024484     \t|21             \t|  0.0051417      \t|  2.0244e-05     \t|\r\non_validation_batch_start   \t|  5.3095e-05     \t|21             \t|  0.001115       \t|  4.3901e-06     \t|\r\nvalidation_step_end         \t|  2.1799e-05     \t|21             \t|  0.00045779     \t|  1.8024e-06     \t|\r\non_train_epoch_start        \t|  1.7319e-05     \t|20             \t|  0.00034637     \t|  1.3638e-06     \t|\r\non_epoch_end                \t|  1.5776e-05     \t|20             \t|  0.00031551     \t|  1.2423e-06     \t|\r\non_train_end                \t|  0.0002874      \t|1              \t|  0.0002874      \t|  1.1316e-06     \t|\r\non_validation_epoch_end     \t|  1.1708e-05     \t|21             \t|  0.00024586     \t|  9.6803e-07     \t|\r\non_validation_epoch_start   \t|  8.0324e-06     \t|21             \t|  0.00016868     \t|  6.6415e-07     \t|\r\non_train_start              \t|  0.00015864     \t|1              \t|  0.00015864     \t|  6.2463e-07     \t|\r\non_train_epoch_end          \t|  7.2367e-06     \t|20             \t|  0.00014473     \t|  5.6986e-07     \t|\r\non_fit_start                \t|  1.4059e-05     \t|1              \t|  1.4059e-05     \t|  5.5355e-08     \t|\r\n```\r\n\r\nEval command (used for **scale2_epoch29** model):\r\n```\r\npython eval.py \\\r\n  --root_dir /home/ubuntu/data/IMC-PT/brandenburg_gate/ \\\r\n  --dataset_name phototourism --scene_name brandenburg_test \\\r\n  --split test --N_samples 256 --N_importance 256 \\\r\n  --N_vocab 1500 --encode_a --encode_t \\\r\n  --ckpt_path ckpts/brandenburg/scale2/epoch\\=29.ckpt \\\r\n  --chunk 16384 --img_wh 320 240\r\n```\r\nYou can change the test camera path in `eval.py`.",
        "dateCreated": "2021-01-26T15:32:55Z",
        "datePublished": "2021-01-27T00:13:36Z",
        "html_url": "https://github.com/kwea123/nerf_pl/releases/tag/nerfw_branden",
        "name": "NeRF-W on brandenburg gate",
        "tag_name": "nerfw_branden",
        "tarball_url": "https://api.github.com/repos/kwea123/nerf_pl/tarball/nerfw_branden",
        "url": "https://api.github.com/repos/kwea123/nerf_pl/releases/36943400",
        "zipball_url": "https://api.github.com/repos/kwea123/nerf_pl/zipball/nerfw_branden"
      },
      {
        "authorType": "User",
        "author_name": "kwea123",
        "body": "Used for nerfw branch.\r\n\r\nTrain command:\r\n```\r\npython train.py \\\r\n  --root_dir /home/ubuntu/data/nerf_example_data/nerf_synthetic/lego \\\r\n  --dataset_name blender --img_wh 200 200 --data_perturb color occ \\\r\n  --N_importance 64 --N_samples 64 --noise_std 0 --encode_a --encode_t --beta_min 0.1 \\\r\n  --num_epochs 20 --batch_size 1024 \\\r\n  --optimizer adam --lr 5e-4 --lr_scheduler cosine \\\r\n  --exp_name lego_nerfw_all\r\n```\r\n\r\nEval command:\r\n```\r\npython eval.py \\\r\n  --root_dir /home/ubuntu/data/nerf_example_data/nerf_synthetic/lego \\\r\n  --dataset_name blender --split test --img_wh 200 200 \\\r\n  --N_importance 64 --encode_a --encode_t --beta_min 0.1 \\\r\n  --ckpt_path ckpts/lego_nerfw_all/epoch\\=19.ckpt \\\r\n  --scene_name nerfw_all\r\n```\r\n\r\nEval output: `Mean PSNR : 24.86`\r\n\r\nProfiler Report\r\n```\r\nAction                      \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\r\n-----------------------------------------------------------------------------------------------------------------------------\r\nTotal                       \t|  -              \t|_              \t|  1.1659e+04     \t|  100 %          \t|\r\n-----------------------------------------------------------------------------------------------------------------------------\r\nrun_training_epoch          \t|  582.57         \t|20             \t|  1.1651e+04     \t|  99.931         \t|\r\nrun_training_batch          \t|  0.14307        \t|78140          \t|  1.1179e+04     \t|  95.882         \t|\r\noptimizer_step_and_closure_0\t|  0.12437        \t|78140          \t|  9718.4         \t|  83.352         \t|\r\ntraining_step_and_backward  \t|  0.12006        \t|78140          \t|  9381.8         \t|  80.465         \t|\r\nmodel_backward              \t|  0.095661       \t|78140          \t|  7475.0         \t|  64.111         \t|\r\nmodel_forward               \t|  0.024116       \t|78140          \t|  1884.5         \t|  16.162         \t|\r\nevaluation_step_and_end     \t|  1.8998         \t|161            \t|  305.86         \t|  2.6233         \t|\r\non_train_batch_end          \t|  0.00053565     \t|78140          \t|  41.856         \t|  0.35898        \t|\r\nget_train_batch             \t|  0.00026832     \t|78140          \t|  20.966         \t|  0.17982        \t|\r\ncache_result                \t|  1.6708e-05     \t|391370         \t|  6.5391         \t|  0.056084       \t|\r\non_after_backward           \t|  1.3945e-05     \t|78140          \t|  1.0897         \t|  0.0093458      \t|\r\non_batch_start              \t|  1.1257e-05     \t|78140          \t|  0.87959        \t|  0.007544       \t|\r\non_batch_end                \t|  1.0574e-05     \t|78140          \t|  0.82626        \t|  0.0070866      \t|\r\non_before_zero_grad         \t|  9.9755e-06     \t|78140          \t|  0.77948        \t|  0.0066854      \t|\r\ntraining_step_end           \t|  7.3524e-06     \t|78140          \t|  0.57452        \t|  0.0049275      \t|\r\non_train_batch_start        \t|  7.0481e-06     \t|78140          \t|  0.55074        \t|  0.0047235      \t|\r\non_validation_end           \t|  0.025579       \t|21             \t|  0.53715        \t|  0.004607       \t|\r\non_validation_batch_end     \t|  0.00039767     \t|161            \t|  0.064025       \t|  0.00054912     \t|\r\non_epoch_start              \t|  0.00074399     \t|20             \t|  0.01488        \t|  0.00012762     \t|\r\non_validation_start         \t|  0.00024646     \t|21             \t|  0.0051757      \t|  4.439e-05      \t|\r\non_train_end                \t|  0.0033677      \t|1              \t|  0.0033677      \t|  2.8884e-05     \t|\r\non_validation_batch_start   \t|  1.301e-05      \t|161            \t|  0.0020947      \t|  1.7965e-05     \t|\r\nvalidation_step_end         \t|  9.2702e-06     \t|161            \t|  0.0014925      \t|  1.2801e-05     \t|\r\non_epoch_end                \t|  1.6658e-05     \t|20             \t|  0.00033316     \t|  2.8575e-06     \t|\r\non_validation_epoch_end     \t|  1.4696e-05     \t|21             \t|  0.00030862     \t|  2.6469e-06     \t|\r\non_train_start              \t|  0.00020975     \t|1              \t|  0.00020975     \t|  1.799e-06      \t|\r\non_validation_epoch_start   \t|  9.7831e-06     \t|21             \t|  0.00020545     \t|  1.7621e-06     \t|\r\non_train_epoch_start        \t|  9.096e-06      \t|20             \t|  0.00018192     \t|  1.5603e-06     \t|\r\non_train_epoch_end          \t|  8.8208e-06     \t|20             \t|  0.00017642     \t|  1.5131e-06     \t|\r\non_fit_start                \t|  1.3749e-05     \t|1              \t|  1.3749e-05     \t|  1.1792e-07     \t|\r\n```",
        "dateCreated": "2021-01-24T10:13:56Z",
        "datePublished": "2021-01-24T12:51:04Z",
        "html_url": "https://github.com/kwea123/nerf_pl/releases/tag/nerfw_all",
        "name": "NeRF-W with color perturbation and occluder",
        "tag_name": "nerfw_all",
        "tarball_url": "https://api.github.com/repos/kwea123/nerf_pl/tarball/nerfw_all",
        "url": "https://api.github.com/repos/kwea123/nerf_pl/releases/36822289",
        "zipball_url": "https://api.github.com/repos/kwea123/nerf_pl/zipball/nerfw_all"
      },
      {
        "authorType": "User",
        "author_name": "kwea123",
        "body": "Used for nerfw branch.\r\n\r\nTrain command:\r\n```\r\npython train.py \\\r\n  --root_dir /home/ubuntu/data/nerf_example_data/nerf_synthetic/lego \\\r\n  --dataset_name blender --img_wh 200 200 --data_perturb color \\\r\n  --N_importance 64 --N_samples 64 --noise_std 0 --encode_a \\\r\n  --num_epochs 20 --batch_size 1024 \\\r\n  --optimizer adam --lr 5e-4 --lr_scheduler cosine \\\r\n  --exp_name lego_nerfa_color\r\n```\r\n\r\nEval command:\r\n```\r\npython eval.py \\\r\n  --root_dir /home/ubuntu/data/nerf_example_data/nerf_synthetic/lego \\\r\n  --dataset_name blender --split test --img_wh 200 200 \\\r\n  --N_importance 64 --encode_a \\\r\n  --ckpt_path ckpts/lego_nerfa_color/epoch\\=19.ckpt \\\r\n  --scene_name nerfa_color\r\n```\r\n\r\nEval output: `Mean PSNR : 28.20`\r\n\r\nProfiler Report\r\n```\r\nAction                      \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\r\n-----------------------------------------------------------------------------------------------------------------------------\r\nTotal                       \t|  -              \t|_              \t|  1.0174e+04     \t|  100 %          \t|\r\n-----------------------------------------------------------------------------------------------------------------------------\r\nrun_training_epoch          \t|  508.31         \t|20             \t|  1.0166e+04     \t|  99.922         \t|\r\nrun_training_batch          \t|  0.12504        \t|78140          \t|  9770.7         \t|  96.036         \t|\r\noptimizer_step_and_closure_0\t|  0.10593        \t|78140          \t|  8277.7         \t|  81.362         \t|\r\ntraining_step_and_backward  \t|  0.10272        \t|78140          \t|  8026.6         \t|  78.893         \t|\r\nmodel_backward              \t|  0.081418       \t|78140          \t|  6362.0         \t|  62.532         \t|\r\nmodel_forward               \t|  0.021105       \t|78140          \t|  1649.1         \t|  16.209         \t|\r\nevaluation_step_and_end     \t|  1.6237         \t|161            \t|  261.41         \t|  2.5694         \t|\r\non_train_batch_end          \t|  0.00040171     \t|78140          \t|  31.39          \t|  0.30853        \t|\r\nget_train_batch             \t|  0.0002557      \t|78140          \t|  19.981         \t|  0.19639        \t|\r\ncache_result                \t|  1.4961e-05     \t|391370         \t|  5.8553         \t|  0.057551       \t|\r\non_after_backward           \t|  1.136e-05      \t|78140          \t|  0.88768        \t|  0.008725       \t|\r\non_batch_start              \t|  1.0067e-05     \t|78140          \t|  0.78663        \t|  0.0077318      \t|\r\non_batch_end                \t|  9.6172e-06     \t|78140          \t|  0.75149        \t|  0.0073863      \t|\r\non_before_zero_grad         \t|  9.0155e-06     \t|78140          \t|  0.70447        \t|  0.0069242      \t|\r\non_validation_end           \t|  0.026961       \t|21             \t|  0.56618        \t|  0.005565       \t|\r\ntraining_step_end           \t|  6.6222e-06     \t|78140          \t|  0.51746        \t|  0.0050861      \t|\r\non_train_batch_start        \t|  6.3198e-06     \t|78140          \t|  0.49383        \t|  0.0048539      \t|\r\non_validation_batch_end     \t|  0.00036434     \t|161            \t|  0.058659       \t|  0.00057656     \t|\r\non_epoch_start              \t|  0.00047801     \t|20             \t|  0.0095601      \t|  9.3966e-05     \t|\r\non_validation_start         \t|  0.00024532     \t|21             \t|  0.0051518      \t|  5.0637e-05     \t|\r\non_validation_batch_start   \t|  1.2674e-05     \t|161            \t|  0.0020406      \t|  2.0057e-05     \t|\r\nvalidation_step_end         \t|  8.6672e-06     \t|161            \t|  0.0013954      \t|  1.3716e-05     \t|\r\non_epoch_end                \t|  1.7733e-05     \t|20             \t|  0.00035466     \t|  3.4859e-06     \t|\r\non_train_end                \t|  0.00025723     \t|1              \t|  0.00025723     \t|  2.5283e-06     \t|\r\non_validation_epoch_end     \t|  1.1715e-05     \t|21             \t|  0.00024602     \t|  2.4181e-06     \t|\r\non_train_epoch_start        \t|  1.1723e-05     \t|20             \t|  0.00023446     \t|  2.3045e-06     \t|\r\non_train_start              \t|  0.00021311     \t|1              \t|  0.00021311     \t|  2.0946e-06     \t|\r\non_validation_epoch_start   \t|  8.2239e-06     \t|21             \t|  0.0001727      \t|  1.6975e-06     \t|\r\non_train_epoch_end          \t|  8.1054e-06     \t|20             \t|  0.00016211     \t|  1.5934e-06     \t|\r\non_fit_start                \t|  1.3379e-05     \t|1              \t|  1.3379e-05     \t|  1.315e-07      \t|\r\n```",
        "dateCreated": "2021-01-24T04:02:35Z",
        "datePublished": "2021-01-24T04:26:42Z",
        "html_url": "https://github.com/kwea123/nerf_pl/releases/tag/nerfa_color",
        "name": "NeRF-A with color perturbation",
        "tag_name": "nerfa_color",
        "tarball_url": "https://api.github.com/repos/kwea123/nerf_pl/tarball/nerfa_color",
        "url": "https://api.github.com/repos/kwea123/nerf_pl/releases/36816106",
        "zipball_url": "https://api.github.com/repos/kwea123/nerf_pl/zipball/nerfa_color"
      },
      {
        "authorType": "User",
        "author_name": "kwea123",
        "body": "Used for nerfw branch.\r\n\r\nTrain command:\r\n```\r\npython train.py \\\r\n  --dataset_name blender --img_wh 200 200 \\\r\n  --root_dir /home/ubuntu/data/nerf_example_data/nerf_synthetic/lego \\\r\n  --N_importance 64 --N_samples 64 --noise_std 0 \\\r\n  --num_epochs 20 --batch_size 1024 \\\r\n  --optimizer adam --lr 5e-4 --lr_scheduler cosine \\\r\n  --exp_name lego_nerfu_occ --beta_min 0.1 --data_perturb occ --encode_t\r\n```\r\n\r\nEval command:\r\n```\r\npython eval.py \\\r\n  --root_dir /home/ubuntu/data/nerf_example_data/nerf_synthetic/lego \\\r\n  --dataset_name blender --img_wh 200 200 --split test \\\r\n  --N_importance 64 \\\r\n  --ckpt_path ckpts/lego_nerfw_occ/epoch\\=19.ckpt \\\r\n  --encode_t --beta_min 0.1 \\\r\n  --scene_name nerfu_occ\r\n```\r\n\r\nEval output: `Mean PSNR : 28.60`\r\n\r\nNote I use a very small image size (200x200) to speed up my experiments.\r\n\r\nProfiler Report\r\n```\r\nAction                      \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\r\n-----------------------------------------------------------------------------------------------------------------------------\r\nTotal                       \t|  -              \t|_              \t|  1.0901e+04     \t|  100 %          \t|\r\n-----------------------------------------------------------------------------------------------------------------------------\r\nrun_training_epoch          \t|  544.74         \t|20             \t|  1.0895e+04     \t|  99.947         \t|\r\nrun_training_batch          \t|  0.13381        \t|78140          \t|  1.0456e+04     \t|  95.921         \t|\r\noptimizer_step_and_closure_0\t|  0.11587        \t|78140          \t|  9053.8         \t|  83.057         \t|\r\ntraining_step_and_backward  \t|  0.11173        \t|78140          \t|  8730.4         \t|  80.09          \t|\r\nmodel_backward              \t|  0.088715       \t|78140          \t|  6932.2         \t|  63.595         \t|\r\nmodel_forward               \t|  0.02281        \t|78140          \t|  1782.4         \t|  16.351         \t|\r\nevaluation_step_and_end     \t|  1.7842         \t|161            \t|  287.25         \t|  2.6352         \t|\r\non_train_batch_end          \t|  0.00042159     \t|78140          \t|  32.943         \t|  0.30221        \t|\r\nget_train_batch             \t|  0.00025085     \t|78140          \t|  19.602         \t|  0.17982        \t|\r\ncache_result                \t|  1.4803e-05     \t|391370         \t|  5.7934         \t|  0.053147       \t|\r\non_batch_start              \t|  1.0333e-05     \t|78140          \t|  0.80743        \t|  0.0074072      \t|\r\non_after_backward           \t|  9.5508e-06     \t|78140          \t|  0.7463         \t|  0.0068464      \t|\r\non_batch_end                \t|  9.4638e-06     \t|78140          \t|  0.7395         \t|  0.006784       \t|\r\non_before_zero_grad         \t|  8.3572e-06     \t|78140          \t|  0.65303        \t|  0.0059908      \t|\r\non_validation_end           \t|  0.025442       \t|21             \t|  0.53429        \t|  0.0049014      \t|\r\ntraining_step_end           \t|  6.2163e-06     \t|78140          \t|  0.48574        \t|  0.0044561      \t|\r\non_train_batch_start        \t|  5.99e-06       \t|78140          \t|  0.46806        \t|  0.0042939      \t|\r\non_validation_batch_end     \t|  0.00042104     \t|161            \t|  0.067788       \t|  0.00062187     \t|\r\non_epoch_start              \t|  0.00079988     \t|20             \t|  0.015998       \t|  0.00014676     \t|\r\non_validation_start         \t|  0.00024023     \t|21             \t|  0.0050449      \t|  4.6281e-05     \t|\r\non_validation_batch_start   \t|  1.391e-05      \t|161            \t|  0.0022395      \t|  2.0544e-05     \t|\r\nvalidation_step_end         \t|  8.4167e-06     \t|161            \t|  0.0013551      \t|  1.2431e-05     \t|\r\non_train_end                \t|  0.0003507      \t|1              \t|  0.0003507      \t|  3.2172e-06     \t|\r\non_epoch_end                \t|  1.611e-05      \t|20             \t|  0.0003222      \t|  2.9558e-06     \t|\r\non_train_epoch_start        \t|  1.5704e-05     \t|20             \t|  0.00031408     \t|  2.8813e-06     \t|\r\non_validation_epoch_end     \t|  1.4037e-05     \t|21             \t|  0.00029477     \t|  2.7041e-06     \t|\r\non_validation_epoch_start   \t|  8.7303e-06     \t|21             \t|  0.00018334     \t|  1.6819e-06     \t|\r\non_train_start              \t|  0.00016846     \t|1              \t|  0.00016846     \t|  1.5454e-06     \t|\r\non_train_epoch_end          \t|  7.923e-06      \t|20             \t|  0.00015846     \t|  1.4537e-06     \t|\r\non_fit_start                \t|  1.3867e-05     \t|1              \t|  1.3867e-05     \t|  1.2721e-07     \t|\r\n```",
        "dateCreated": "2021-01-23T12:27:52Z",
        "datePublished": "2021-01-23T12:34:30Z",
        "html_url": "https://github.com/kwea123/nerf_pl/releases/tag/nerfu_occ",
        "name": "NeRF-U with occluder",
        "tag_name": "nerfu_occ",
        "tarball_url": "https://api.github.com/repos/kwea123/nerf_pl/tarball/nerfu_occ",
        "url": "https://api.github.com/repos/kwea123/nerf_pl/releases/36804239",
        "zipball_url": "https://api.github.com/repos/kwea123/nerf_pl/zipball/nerfu_occ"
      },
      {
        "authorType": "User",
        "author_name": "kwea123",
        "body": "succulent plant model by @SpongeGirl\r\nimage size 496x372, spheric pose\r\n![1](https://user-images.githubusercontent.com/11364490/81401242-de932400-9169-11ea-8e4f-a5a9b83ef19d.png)\r\n",
        "dateCreated": "2020-05-08T07:31:54Z",
        "datePublished": "2020-05-08T11:24:23Z",
        "html_url": "https://github.com/kwea123/nerf_pl/releases/tag/v2.0.2",
        "name": "release succulent plant model",
        "tag_name": "v2.0.2",
        "tarball_url": "https://api.github.com/repos/kwea123/nerf_pl/tarball/v2.0.2",
        "url": "https://api.github.com/repos/kwea123/nerf_pl/releases/26308125",
        "zipball_url": "https://api.github.com/repos/kwea123/nerf_pl/zipball/v2.0.2"
      },
      {
        "authorType": "User",
        "author_name": "kwea123",
        "body": "release silica nerf model and reconstructed mesh (spheric poses)\r\n[link](https://drive.google.com/drive/folders/1qdZD7UgUzQIc52-xQ83QIJbFsvnXEkX2?usp=sharing) to the data\r\nimage size is 504x378 (original size 4032x3024)\r\n\r\nUsage: place the `poses_bounds.npy` under a folder `$DIR` (anywhere you want), then you can run \r\n```\r\npython eval.py \\\r\n   --root_dir $DIR \\\r\n   --dataset_name llff --scene_name silica \\\r\n   --img_wh 504 378 --N_importance 64 --spheric_poses --ckpt_path $CKPT_PATH\r\n```\r\nas usual. To extract the mesh, follow README_mesh.",
        "dateCreated": "2020-05-07T07:01:54Z",
        "datePublished": "2020-05-07T07:06:05Z",
        "html_url": "https://github.com/kwea123/nerf_pl/releases/tag/v2.0.1",
        "name": "release silica model",
        "tag_name": "v2.0.1",
        "tarball_url": "https://api.github.com/repos/kwea123/nerf_pl/tarball/v2.0.1",
        "url": "https://api.github.com/repos/kwea123/nerf_pl/releases/26258562",
        "zipball_url": "https://api.github.com/repos/kwea123/nerf_pl/zipball/v2.0.1"
      },
      {
        "authorType": "User",
        "author_name": "kwea123",
        "body": "Command:\r\n```\r\npython train.py \\\r\n  --dataset_name llff \\\r\n  --root_dir /home/ubuntu/data/nerf_example_data/nerf_llff_data/fern/ \\\r\n  --N_importance 64 --img_wh 504 378 \\\r\n  --batch_size 1024 --num_epochs 30 \\\r\n  --optimizer adam --lr 5e-4 \\\r\n  --lr_scheduler steplr --decay_step 10 20 --decay_gamma 0.5 \\\r\n  --exp_name fern\r\n```\r\n\r\nProfile\r\n```\r\nProfiler Report\r\n\r\nAction              \t|  Mean duration (s)\t|  Total time (s) \r\n-----------------------------------------------------------------\r\non_train_start      \t|  0.00023312     \t|  0.00023312     \r\non_epoch_start      \t|  0.00029521     \t|  0.0088563      \r\nget_train_batch     \t|  0.00023997     \t|  25.456         \r\non_batch_start      \t|  6.1591e-06     \t|  0.65317        \r\nmodel_forward       \t|  0.019652       \t|  2084.1         \r\nmodel_backward      \t|  0.069537       \t|  7374.4         \r\non_after_backward   \t|  1.5543e-06     \t|  0.16483        \r\noptimizer_step      \t|  0.0037302      \t|  395.59         \r\non_batch_end        \t|  0.00030407     \t|  32.247         \r\non_epoch_end        \t|  9.9102e-06     \t|  0.00029731     \r\non_train_end        \t|  0.00036468     \t|  0.00036468     \r\n```",
        "dateCreated": "2020-05-01T14:51:20Z",
        "datePublished": "2020-05-02T01:03:46Z",
        "html_url": "https://github.com/kwea123/nerf_pl/releases/tag/v2.0",
        "name": "release fern model",
        "tag_name": "v2.0",
        "tarball_url": "https://api.github.com/repos/kwea123/nerf_pl/tarball/v2.0",
        "url": "https://api.github.com/repos/kwea123/nerf_pl/releases/26099225",
        "zipball_url": "https://api.github.com/repos/kwea123/nerf_pl/zipball/v2.0"
      },
      {
        "authorType": "User",
        "author_name": "kwea123",
        "body": "lego model and reconstructed mesh.\r\nCommand:\r\n```\r\npython train.py \\\r\n   --dataset_name blender \\\r\n   --root_dir /home/ubuntu/data/nerf_example_data/nerf_synthetic/lego/ \\\r\n   --N_importance 64 --img_wh 400 400 --noise_std 0 \\\r\n   --batch_size 1024 --num_epochs 16 \\\r\n   --optimizer adam --lr 5e-4 \\\r\n   --lr_scheduler steplr --decay_step 2 4 8 --decay_gamma 0.5 \\\r\n   --exp_name exp3\r\n```\r\n\r\nDetailed profile:\r\n```\r\nAction              \t|  Mean duration (s)\t|  Total time (s) \r\n-----------------------------------------------------------------\r\non_train_start      \t|  1.2281e-05     \t|  1.2281e-05     \r\non_epoch_start      \t|  6.1691e-06     \t|  9.8706e-05     \r\nget_train_batch     \t|  0.00023678     \t|  59.198         \r\non_batch_start      \t|  4.4245e-06     \t|  1.1061         \r\nmodel_forward       \t|  0.041729       \t|  1.0432e+04     \r\nmodel_backward      \t|  0.046964       \t|  1.1741e+04     \r\non_after_backward   \t|  1.5339e-06     \t|  0.38347        \r\noptimizer_step      \t|  0.0035952      \t|  898.81         \r\non_batch_end        \t|  4.1799e-06     \t|  1.045          \r\non_epoch_end        \t|  5.4906e-06     \t|  8.785e-05      \r\non_train_end        \t|  9.583e-06      \t|  9.583e-06 \r\n```\r\n",
        "dateCreated": "2020-04-20T02:43:54Z",
        "datePublished": "2020-04-20T02:47:06Z",
        "html_url": "https://github.com/kwea123/nerf_pl/releases/tag/v1.0",
        "name": "release lego model",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/kwea123/nerf_pl/tarball/v1.0",
        "url": "https://api.github.com/repos/kwea123/nerf_pl/releases/25663917",
        "zipball_url": "https://api.github.com/repos/kwea123/nerf_pl/zipball/v1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 759,
      "date": "Tue, 28 Dec 2021 23:18:44 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "nerf",
      "neural-radiance-fields",
      "view-synthesis",
      "pytorch",
      "pytorch-lightning",
      "mesh",
      "3d-reconstruction",
      "volume-rendering",
      "mixed-reality",
      "unity3d",
      "colab",
      "ray-tracing",
      "nerf-in-the-wild",
      "nerf-w"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Unofficial implementation of [NeRF](https://arxiv.org/pdf/2003.08934.pdf) (Neural Radiance Fields) using pytorch ([pytorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning)). This repo doesn't aim at reproducibility, but aim at providing a simpler and faster training procedure (also simpler code with detailed comments to help to understand the work). Moreover, I try to extend much more opportunities by integrating this algorithm into game engine like Unity.\n\nOfficial implementation: [nerf](https://github.com/bmild/nerf) .. Reference pytorch implementation: [nerf-pytorch](https://github.com/yenchenlin/nerf-pytorch)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "<a href=\"https://www.youtube.com/playlist?list=PLDV2CyUo4q-K02pNEyDr7DYpTQuka3mbV\">\n<img src=\"https://user-images.githubusercontent.com/11364490/80913471-d5781080-8d7f-11ea-9f72-9d68402b8271.png\">\n</a>\n   \n",
      "technique": "Header extraction"
    }
  ]
}