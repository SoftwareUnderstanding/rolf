{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.03581"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite the following paper if you use the code:\n\n<pre><code>@InProceedings{Zhang:18,\n  author    = {Zhang, Jiacheng and Luan, Huanbo and Sun, Maosong and Zhai, Feifei and Xu, Jingfang and Zhang, Min and Liu, Yang},\n  title     = {Improving the Transformer Translation Model with Document-Level Context},\n  booktitle = {Proceedings of EMNLP},\n  year      = {2018},\n}\n</code></pre>\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{Zhang:18,\n  author    = {Zhang, Jiacheng and Luan, Huanbo and Sun, Maosong and Zhai, Feifei and Xu, Jingfang and Zhang, Min and Liu, Yang},\n  title     = {Improving the Transformer Translation Model with Document-Level Context},\n  booktitle = {Proceedings of EMNLP},\n  year      = {2018},\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/THUNLP-MT/Document-Transformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-03-13T07:57:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-01T08:29:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is the implementation of our work, which extends Transformer to integrate document-level context \\[[paper](https://arxiv.org/abs/1810.03581)\\]. The implementation is on top of [THUMT](https://github.com/thumt/THUMT)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Improving the Transformer translation model with document-level context",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Glaceon31/Document-Transformer/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. What is the context corpus?\n\nThe context corpus file contains one context sentence each line. Normally, context sentence is the several preceding source sentences within a document. For example, if the origin document-level corpus is:\n\n<pre><code>==== source ====\n&lt;document id=XXX>\n&lt;seg id=1>source sentence #1&lt;/seg>\n&lt;seg id=2>source sentence #2&lt;/seg>\n&lt;seg id=3>source sentence #3&lt;/seg>\n&lt;seg id=4>source sentence #4&lt;/seg>\n&lt;/document>\n\n==== target ====\n&lt;document id=XXX>\n&lt;seg id=1>target sentence #1&lt;/seg>\n&lt;seg id=2>target sentence #2&lt;/seg>\n&lt;seg id=3>target sentence #3&lt;/seg>\n&lt;seg id=4>target sentence #4&lt;/seg>\n&lt;/document></code></pre>\n\nThe inputs to our system should be processed as (suppose that 2 preceding source sentences are used as context):\n\n<pre><code>==== train.src ==== (source corpus)\nsource sentence #1\nsource sentence #2\nsource sentence #3\nsource sentence #4\n\n==== train.ctx ==== (context corpus)\n(the first line is empty)\nsource sentence #1\nsource sentence #1 source sentence #2 (there is only a space between the two sentence)\nsource sentence #2 source sentence #3\n\n==== train.trg ==== (target corpus)\ntarget sentence #1\ntarget sentence #2\ntarget sentence #3\ntarget sentence #4</code></pre>\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 22,
      "date": "Tue, 28 Dec 2021 00:47:53 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/THUNLP-MT/Document-Transformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "THUNLP-MT/Document-Transformer",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/Glaceon31/Document-Transformer/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/THUNLP-MT/Document-Transformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2018, Natural Language Processing Lab at Tsinghua University\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification,\\nare permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice, this\\n  list of conditions and the following disclaimer in the documentation and/or\\n  other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from this\\n  software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Improving the Transformer Translation Model with Document-Level Context",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Document-Transformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "THUNLP-MT",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/THUNLP-MT/Document-Transformer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 162,
      "date": "Tue, 28 Dec 2021 00:47:53 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "neural-machine-translation",
      "document-level-translation"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Note: The usage is not user-friendly. May improve later.\n\n1. Train a standard Transformer model, please refer to the user manual of [THUMT](https://github.com/thumt/THUMT). Suppose that model_baseline/model.ckpt-30000 performs best on validation set.\n\n2. Generate a dummy improved Transformer model with the following command:\n\n<pre><code>python THUMT/thumt/bin/trainer_ctx.py --inputs [source corpus] [target corpus] \\\n                                      --context [context corpus] \\\n                                      --vocabulary [source vocabulary] [target vocabulary] \\\n                                      --output model_dummy --model contextual_transformer \\\n                                      --parameters train_steps=1\n</code></pre>\n\n3. Generate the initial model by merging the standard Transformer model into the dummy model, then create a checkpoint file:\n\n<pre><code>python THUMT/thumt/scripts/combine_add.py --model model_dummy/model.ckpt-0 \\\n                                         --part model_baseline/model.ckpt-30000 --output train\nprintf 'model_checkpoint_path: \"new-0\"\\nall_model_checkpoint_paths: \"new-0\"' > train/checkpoint\n</code></pre>\n\n\n4. Train the improved Transformer model with the following command:\n\n<pre><code>python THUMT/thumt/bin/trainer_ctx.py --inputs [source corpus] [target corpus] \\\n                                      --context [context corpus] \\\n                                      --vocabulary [source vocabulary] [target vocabulary] \\\n                                      --output train --model contextual_transformer \\\n                                      --parameters start_steps=30000,num_context_layers=1\n</code></pre>\n\n5. Translate with the improved Transformer model:\n\n<pre><code>python THUMT/thumt/bin/translator_ctx.py --inputs [source corpus] --context [context corpus] \\\n                                         --output [translation result] \\\n                                         --vocabulary [source vocabulary] [target vocabulary] \\\n                                         --model contextual_transformer --checkpoints [model path] \\\n                                         --parameters num_context_layers=1\n</code></pre>\n\n",
      "technique": "Header extraction"
    }
  ]
}