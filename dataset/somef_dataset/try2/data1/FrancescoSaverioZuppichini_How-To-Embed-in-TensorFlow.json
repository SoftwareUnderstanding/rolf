{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1301.3781] and (Glove"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": " 'iste': 10, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": " 'nostra': 12, \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/FrancescoSaverioZuppichini/How-To-Embed-in-TensorFlow",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-18T17:41:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-27T13:47:51Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9593366746898317
      ],
      "excerpt": "Today we are going to see how to create embeddings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762,
        0.9376441286986215
      ],
      "excerpt": "Updated to tf 1.9 \n(Words embedding)[https://en.wikipedia.org/wiki/Word_embedding] is a way to represent words by creating high dimensional vector space in which similar words are close to each other. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9124396101298464
      ],
      "excerpt": "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9615168991866059
      ],
      "excerpt": "Now we need to define the Embedding size, so the dimension of each vector, in our case 50, and the vocabulary length \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9470039054170997
      ],
      "excerpt": "We know need to define the ids of the words we want to embed. Just for example, we are going to take abutere and patientia \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9390566273835748,
        0.9287082228087126,
        0.957056407345758,
        0.9323972174513264
      ],
      "excerpt": "Just to be sure you are following me. words_ids represent the ids of some words in a vocabolary. A vocabolary is a map from words (tokens) to ids.  \nWHY we have to do that? Neural Networks work with number, so we have to pass a number to the embedding layer \nTo embed we can use the low-level API. We first need to define a matrix of size [VOCAL_LEN, EMBED_SIZE] (20, 50) and then we have to tell TensorFlow where to look for our words ids using tf.nn.embedding_lookup. \ntf.nn.embedding_lookup creates an operation that retrieves the rows of the first parameters based on the index of the second. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9319708861235405
      ],
      "excerpt": "with tf.Session() as sess: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9319708861235405
      ],
      "excerpt": "with tf.Session() as sess: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9405507835407924
      ],
      "excerpt": "Since Tensorflow is a mess, there are always several ways to do the same things and it is not clear which one you should use, so no surprise there is also an other function to create embeddings.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9319708861235405
      ],
      "excerpt": "with tf.Session() as sess: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9319708861235405
      ],
      "excerpt": "with tf.Session() as sess: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9129550109489473
      ],
      "excerpt": "You can benefit by using pre-trained words embedding since they can improve performance of your model. They are usually trained with enormous datasets, e.g. Wikipedia, with bag-of-words of skip-grammar models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8960821142609172
      ],
      "excerpt": "By the way, TensorFlow Hub is buggy and does not work well on Jupiter. Or at least on my machine, you can try to run it and see if it works for you. On PyCharm I have not any problem so I will copy and paste the output. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9319708861235405
      ],
      "excerpt": "with tf.Session() as sess: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for my medium article",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/FrancescoSaverioZuppichini/How-To-Embed-in-TensorFlow/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 29 Dec 2021 12:08:52 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/FrancescoSaverioZuppichini/How-To-Embed-in-TensorFlow/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "FrancescoSaverioZuppichini/How-To-Embed-in-TensorFlow",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/FrancescoSaverioZuppichini/How-To-Embed-in-TensorFlow/master/notebook.ipynb",
      "https://raw.githubusercontent.com/FrancescoSaverioZuppichini/How-To-Embed-in-TensorFlow/master/.ipynb_checkpoints/notebook-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9446925806243314
      ],
      "excerpt": "You can create your custom class with the classic paradigm __init__ + __call__ to build it.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8273076402724087
      ],
      "excerpt": "embeddings = embed(['abutere','patientia']) #: you don't even need your ids, you can just pass the tokens \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.925671696398174,
        0.9457175861910134,
        0.9133368656218674
      ],
      "excerpt": "import tensorflow as tf \nimport numpy as np \nimport pprint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8113794357562127,
        0.8796325398080971
      ],
      "excerpt": "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. \n  from ._conv import register_converters as _register_converters \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359299706379749
      ],
      "excerpt": "text = \"Quo usque tandem abutere, Catilina, patientia nostra? Quamdiu etiam furor iste tuus nos eludet? Quem ad finem sese effrenata iactabit audacia?\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8227708205952854
      ],
      "excerpt": "vocab = {k:v for v,k in enumerate(np.unique(tokenized))} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8223086892567654
      ],
      "excerpt": "Let's print the vocabulary \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811854372964597
      ],
      "excerpt": " 'quem': 15, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8187803073681467
      ],
      "excerpt": "VOCAB_LEN = len(vocab.keys()) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135654125968134,
        0.8670539095623045,
        0.8650842222341302
      ],
      "excerpt": "with tf.Session() as sess: \n    sess.run(tf.global_variables_initializer()) \n    print(sess.run(embed)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135654125968134,
        0.8670539095623045,
        0.8650842222341302
      ],
      "excerpt": "with tf.Session() as sess: \n    sess.run(tf.global_variables_initializer()) \n    print(sess.run(embed)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135654125968134,
        0.8670539095623045,
        0.8650842222341302
      ],
      "excerpt": "with tf.Session() as sess: \n    sess.run(tf.global_variables_initializer()) \n    print(sess.run(embed)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135654125968134,
        0.8670539095623045,
        0.8650842222341302
      ],
      "excerpt": "with tf.Session() as sess: \n    sess.run(tf.global_variables_initializer()) \n    print(sess.run(embed)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8107034140524422
      ],
      "excerpt": "import tensorflow_hub as hub \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135654125968134,
        0.8670539095623045,
        0.8670539095623045
      ],
      "excerpt": "with tf.Session() as sess: \n    sess.run(tf.global_variables_initializer()) \n    sess.run(tf.tables_initializer()) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/FrancescoSaverioZuppichini/How-To-Embed-in-TensorFlow/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 FrancescoSaverioZuppichini\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "How to Embed words in Tensorflow",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "How-To-Embed-in-TensorFlow",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "FrancescoSaverioZuppichini",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/FrancescoSaverioZuppichini/How-To-Embed-in-TensorFlow/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 29 Dec 2021 12:08:52 GMT"
    },
    "technique": "GitHub API"
  }
}