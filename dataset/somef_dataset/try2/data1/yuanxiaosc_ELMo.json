{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1802.05365v2",
      "https://arxiv.org/abs/1802.05365v2",
      "https://arxiv.org/abs/1802.05365v2",
      "https://arxiv.org/abs/1802.05365 [cs.CL]\n \t(or https://arxiv.org/abs/1802.05365v2 [cs.CL] for this version)\n\n# Visualizing ELMo\n![](https://cdn-images-1.medium.com/max/800/1*RHsRbSospewPepQWliTf1g.png)\n![](https://cdn-images-1.medium.com/max/800/1*-xTo2Y-1VyvjYywhddyTOg.png)\n\n# \u66f4\u591a\u8d44\u6e90\n+ [\u671b\u6c5f\u4eba\u5de5\u667a\u5e93](https://yuanxiaosc.github.io/tags/ELMO/)\n+ [Deep contextualized word representations](https://arxiv.org/abs/1802.05365v2)\n+ [TensorFlow Hub \u5b9e\u73b0](https://tfhub.dev/google/elmo/2)\n+ [allennlp.org - elmo](https://allennlp.org/elmo)\n+ [bilm-tf](https://github.com/allenai/bilm-tf)\n+ [NAACL 2018\u6700\u4f73\u8bba\u6587\uff1a\u827e\u4f26\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u6240\u63d0\u51fa\u65b0\u578b\u6df1\u5ea6\u8bed\u5883\u5316\u8bcd\u8868\u5f81](https://www.jiqizhixin.com/articles/060704)\n+ [\u628a ELMo \u4f5c\u4e3a keras \u7684\u4e00\u4e2a\u5d4c\u5165\u5c42\u4f7f\u7528](https://github.com/strongio/keras-elmo)\n+ [Visualizing ELMo Contextual Vectors](https://towardsdatascience.com/visualizing-elmo-contextual-vectors-94168768fdaa)\n",
      "https://arxiv.org/abs/1802.05365v2 [cs.CL] for this version)\n\n# Visualizing ELMo\n![](https://cdn-images-1.medium.com/max/800/1*RHsRbSospewPepQWliTf1g.png)\n![](https://cdn-images-1.medium.com/max/800/1*-xTo2Y-1VyvjYywhddyTOg.png)\n\n# \u66f4\u591a\u8d44\u6e90\n+ [\u671b\u6c5f\u4eba\u5de5\u667a\u5e93](https://yuanxiaosc.github.io/tags/ELMO/)\n+ [Deep contextualized word representations](https://arxiv.org/abs/1802.05365v2)\n+ [TensorFlow Hub \u5b9e\u73b0](https://tfhub.dev/google/elmo/2)\n+ [allennlp.org - elmo](https://allennlp.org/elmo)\n+ [bilm-tf](https://github.com/allenai/bilm-tf)\n+ [NAACL 2018\u6700\u4f73\u8bba\u6587\uff1a\u827e\u4f26\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u6240\u63d0\u51fa\u65b0\u578b\u6df1\u5ea6\u8bed\u5883\u5316\u8bcd\u8868\u5f81](https://www.jiqizhixin.com/articles/060704)\n+ [\u628a ELMo \u4f5c\u4e3a keras \u7684\u4e00\u4e2a\u5d4c\u5165\u5c42\u4f7f\u7528](https://github.com/strongio/keras-elmo)\n+ [Visualizing ELMo Contextual Vectors](https://towardsdatascience.com/visualizing-elmo-contextual-vectors-94168768fdaa)\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9363339420639257
      ],
      "excerpt": "Comments:   NAACL 2018. Originally posted to openreview 27 Oct 2017. v2 updated for NAACL camera ready \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9824493046884036,
        0.829202906156098
      ],
      "excerpt": "Cite as:    arXiv:1802.05365 [cs.CL] \n    (or arXiv:1802.05365v2 [cs.CL] for this version) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "allennlp.org - elmo \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yuanxiaosc/ELMo",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-25T11:49:48Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-28T20:19:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "|name|description|\n|-|-|\n|[elmo_tfhub_use_methods.ipynb](https://nbviewer.jupyter.org/github/yuanxiaosc/ELMo/blob/master/elmo_tfhub_use_methods.ipynb)|Summarize four usage methods of Elmo embedding.|\n|[IMDB_ELMo_As_Embedding_Layer.ipynb](https://github.com/yuanxiaosc/ELMo/blob/master/tfhub_elmo_use_examples/IMDB_ELMo_As_Embedding_Layer.ipynb)|IMDB movie review sentiment analysis example|\n|[elmo_sentence_level_embedding.ipynb](https://github.com/yuanxiaosc/ELMo/blob/master/tfhub_elmo_use_examples/elmo_sentence_level_embedding.ipynb)|Kaggle's movie review sentiment analysis example|\n|[elmo_word_level_embedding.ipynb](https://github.com/yuanxiaosc/ELMo/blob/master/tfhub_elmo_use_examples/elmo_word_level_embedding.ipynb)|Kaggle's movie review sentiment analysis example|\n|[IMDB_ELMo_Preprocessing_Data.ipynb](https://github.com/yuanxiaosc/ELMo/blob/master/allennlp_elmo_use_examples/IMDB_ELMo_Preprocessing_Data.ipynb)|Preprocessing data with Elmo|\n|Visualizing ELMo Contextual Vectors|[Visualizing...](https://nbviewer.jupyter.org/github/yuanxiaosc/ELMo/blob/master/Visualizing%20ELMo%20Contextual%20Vectors/Visualizing%20ELMo%20Contextual%20Vectors.ipynb)|\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8770325313546249,
        0.9648179025550403,
        0.8471311961564294
      ],
      "excerpt": "ELMo: Embeddings from Language Models, which comes from the paper \"Deep contextualized word representations\". \nThis resource includes various methods of using ELMo, visual analysis of ELMo, and paper interpretation. \nNAACL 2018\u6700\u4f73\u8bba\u6587 Deep contextualized word representations\uff1a\u827e\u4f26\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u6240\u63d0\u51fa\u65b0\u578b\u6df1\u5ea6\u8bed\u5883\u5316\u8bcd\u8868\u5f81\uff08\u7814\u7a76\u8005\u4f7f\u7528\u4ece\u53cc\u5411 LSTM \u4e2d\u5f97\u5230\u7684\u5411\u91cf\uff0c\u8be5 LSTM \u662f\u4f7f\u7528\u6210\u5bf9\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u76ee\u6807\u5728\u5927\u578b\u6587\u672c\u8bed\u6599\u5e93\u4e0a\u8bad\u7ec3\u5f97\u5230\u7684\u3002\u56e0\u6b64\uff0c\u8be5\u8868\u5f81\u53eb\u4f5c ELMo\uff08Embeddings from Language Models\uff09\u8868\u5f81\u3002\uff09\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9986651257019825
      ],
      "excerpt": "Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8572628261573855
      ],
      "excerpt": "Subjects:   Computation and Language (cs.CL) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "ELMo: Embeddings from Language Models. Using, visualizing and understanding EMLo by examples!",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yuanxiaosc/ELMo/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10,
      "date": "Tue, 28 Dec 2021 02:07:28 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yuanxiaosc/ELMo/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "yuanxiaosc/ELMo",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/yuanxiaosc/ELMo/master/elmo_tfhub_use_methods.ipynb",
      "https://raw.githubusercontent.com/yuanxiaosc/ELMo/master/allennlp_elmo_use_examples/IMDB_ELMo_Preprocessing_Data.ipynb",
      "https://raw.githubusercontent.com/yuanxiaosc/ELMo/master/Visualizing%20ELMo%20Contextual%20Vectors/dimensionality_reduction_by_PCA.ipynb",
      "https://raw.githubusercontent.com/yuanxiaosc/ELMo/master/Visualizing%20ELMo%20Contextual%20Vectors/Visualizing%20ELMo%20Contextual%20Vectors.ipynb",
      "https://raw.githubusercontent.com/yuanxiaosc/ELMo/master/tfhub_elmo_use_examples/IMDB_ELMo_As_Embedding_Layer.ipynb",
      "https://raw.githubusercontent.com/yuanxiaosc/ELMo/master/tfhub_elmo_use_examples/elmo_sentence_level_embedding.ipynb",
      "https://raw.githubusercontent.com/yuanxiaosc/ELMo/master/tfhub_elmo_use_examples/elmo_word_level_embedding.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yuanxiaosc/ELMo/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ELMo",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ELMo",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "yuanxiaosc",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yuanxiaosc/ELMo/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 27,
      "date": "Tue, 28 Dec 2021 02:07:28 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "elmo-tutorial",
      "word-embeddings",
      "visualization"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nimport tensorflow as tf\nimport tensorflow_hub as hub\n#: elmo_url=\"https://tfhub.dev/google/elmo/2\"\n#: hub.Module(elmo_url, trainable=True)\n#: You can either use the URL directly or download the file locally and then use it.\n#: hub.Module(path_to_elmo_model, trainable=True)\n\ntokens_input = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"],\n                [\"dogs\", \"are\", \"in\", \"the\", \"fog\", \"\"]]\ntokens_length = [6, 5]\nmax_length = max(tokens_length)\n\ndef tokens_elmo(path_to_hub_elmo_model=\"https://tfhub.dev/google/elmo/2\"):\n    elmo_tokens_input = tf.placeholder(dtype=tf.string, shape=[None, max_length], name=\"tokens_input\")\n    elmo_sequence_length_input = tf.placeholder(dtype=tf.int32, shape=[None,], name=\"tokens_length\")\n\n    module = hub.Module(path_to_hub_elmo_model, trainable=True)\n    module_features = module(inputs={\"tokens\":elmo_tokens_input, \"sequence_len\":elmo_sequence_length_input},\n                             signature='tokens', as_dict=True)\n    elmo_embedding = module_features[\"elmo\"]  #:[batch_size, max_length, 1024], the weighted sum of the 3 layers, where the weights are trainable.\n    return elmo_tokens_input, elmo_sequence_length_input, elmo_embedding\n\nelmo_tokens_input, elmo_sequence_length_input, elmo_embedding = tokens_elmo(path_to_hub_elmo_model=\"/home/b418/jupyter_workspace/B418_common/\u8881\u5bb5/tfhub_modules/elmo\")\n\nwith tf.Session() as sess:\n    sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n    out_elmo_embedding = sess.run(elmo_embedding,feed_dict={elmo_tokens_input:tokens_input,\n                                                                elmo_sequence_length_input:tokens_length})\n    print(\"out_elmo_shape:\\t\", out_elmo_embedding.shape)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}