{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1805.09692\">Ritter et al. (2018",
      "https://arxiv.org/abs/1410.5401\">NTM</a>, <a href=\"https://www.nature.com/articles/nature20101/\">DNC</a>"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Ritter, S., Wang, J. X., Kurth-Nelson, Z., Jayakumar, S. M., Blundell, C., Pascanu, R., & Botvinick, M. (2018). Been There, Done That: Meta-Learning with Episodic Recall. arXiv [stat.ML]. Retrieved from http://arxiv.org/abs/1805.09692\n\n    - also see Blundell et al. 2016, Pritzel et al. 2017 and Kaiser et al 2017... \n\n- Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., \u2026 Kavukcuoglu, K. (2016). Asynchronous Methods for Deep Reinforcement Learning. Retrieved from http://arxiv.org/abs/1602.01783\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "<a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">PCA</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8715509475085587
      ],
      "excerpt": "<a href=\"https://princetonuniversity.github.io/PsyNeuLink/\">psyneulink</a>  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/qihongl/dnd-lstm",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-10T21:54:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-04T16:19:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "`src/contextual-choice.ipynb` tests the model on an \n<a href=\"https://en.wikipedia.org/wiki/Two-alternative_forced_choice#Behavioural_experiments\">evidence accumulation task </a>\nwith \"context\". \n\nMore concretely, in the i-th trial ... \n\n- At time t, the model receives noisy observation, x_t (e.g. random dots moving around, slightly drifting to left/right)\nand a \"context\" for this trial, call it context_i (e.g. an image of an apple)\n- The task is to press button 0 if x_t is, on average, negative and press 1 otherwise (like press left/right button according to the average direction of the moving dots). Let's denote the response target by y_i, so y_i \\in {0, 1}.  \n- If the model never saw trial i before, it has to base its decision in x_t. However, if it this is the 2nd encounter of trial i, assuming the model cached the association between context_i and y_i in its episodic memory, then the model can just output y_i. \n\n\nSince context is always presented within a trial, making decisions based on recalling the context-target association allows the model to respond faster, which leads to greater cumulative reward. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9652186323733124,
        0.964569792038799
      ],
      "excerpt": "Here're the stimuli for two example trials. The horizontal axis represents time, before the grey dotted line, I turned on very high level of noise so that making better-than-chance deicisons is impossible without episodic memory. The top half of the input represent the observation time series, and the bottom half represent the context (over time).  \nThe left/right figure shows a trial where the model needs to respond 0/1 since observation is negative/positive on average.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9588668909159251
      ],
      "excerpt": "Behaviorally, when the model encounters a previously-seen trial, the choice accuracy is better than chance at t == 0. By task design, this is only possible if the model can retrieve the correct episodic memory.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9591784457618875
      ],
      "excerpt": "analysis of the memory content shows that the choice is encoded in the memory:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    \u251c\u2500\u2500 model    \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8833488668110163
      ],
      "excerpt": "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 ContextualChoice.py         #: the definition of the contextual choice task \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9948770908887401
      ],
      "excerpt": "A variant of the DND part is implemented in  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A Python(PyTorch) implementation of memory augmented neural network based on Ritter et al. (2018). Been There, Done That: Meta-Learning with Episodic Recall. ICML.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/qihongl/dlstm-demo/releases",
    "technique": "GitHub API"
  },
  "executable_example": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://mybinder.org/v2/gh/qihongl/dnd-lstm/master",
      "technique": "Regular expression"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Sun, 26 Dec 2021 08:10:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/qihongl/dnd-lstm/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "qihongl/dnd-lstm",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/qihongl/dlstm-demo/master/src/contextual-choice.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9033987252512259
      ],
      "excerpt": "\u251c\u2500\u2500 requirements.txt \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9151270764930737
      ],
      "excerpt": "\u2514\u2500\u2500 src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8422086606397299
      ],
      "excerpt": "    \u251c\u2500\u2500 contextual-choice.py            #: train the model on a contextual choice task, in .py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8680843994418516
      ],
      "excerpt": "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 A2C.py                      #: an advantage actor critic agent \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9586232994076559,
        0.9336801098518991
      ],
      "excerpt": "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 utils.py \n    \u2514\u2500\u2500 \u2514\u2500\u2500 __init__.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.9586232994076559
      ],
      "excerpt": "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py \n    \u2514\u2500\u2500 utils.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/qihongl/dnd-lstm/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Qihong Lu\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "dnd-lstm",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "dnd-lstm",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "qihongl",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/qihongl/dnd-lstm/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is a LSTM cell with a differentiable neural dictionary described in <a href=\"https://arxiv.org/abs/1805.09692\">Ritter et al. (2018)</a>. You can run `dnd-lstm/src/contextual-choice.py` with jupyter binder or google colab via links above. \n\n\n<img src=\"https://github.com/qihongl/dnd-lstm/blob/master/figs/dnd-lstm-cell.png\" width=500>\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 24,
      "date": "Sun, 26 Dec 2021 08:10:03 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "episodic-memory",
      "lstm",
      "pytorch",
      "reinforcement-learning"
    ],
    "technique": "GitHub API"
  }
}