{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2103.03230",
      "https://arxiv.org/abs/2103.03230"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\r\n@article{zbontar2021barlow,\r\n  title={Barlow Twins: Self-Supervised Learning via Redundancy Reduction},\r\n  author={Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\\'e}phane},\r\n  journal={arXiv preprint arXiv:2103.03230},\r\n  year={2021}\r\n}\r\n```\r\n```bibtex\r\n@misc{grill2020bootstrap,\r\n    title = {Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning},\r\n    author = {Jean-Bastien Grill and Florian Strub and Florent Altch\u00e9 and Corentin Tallec and Pierre H. Richemond and Elena Buchatskaya and Carl Doersch and Bernardo Avila Pires and Zhaohan Daniel Guo and Mohammad Gheshlaghi Azar and Bilal Piot and Koray Kavukcuoglu and R\u00e9mi Munos and Michal Valko},\r\n    year = {2020},\r\n    eprint = {2006.07733},\r\n    archivePrefix = {arXiv},\r\n    primaryClass = {cs.LG}\r\n}\r\n```\r\n\r\n```bibtex\r\n@misc{chen2020exploring,\r\n    title={Exploring Simple Siamese Representation Learning}, \r\n    author={Xinlei Chen and Kaiming He},\r\n    year={2020},\r\n    eprint={2011.10566},\r\n    archivePrefix={arXiv},\r\n    primaryClass={cs.CV}\r\n}\r\n```\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{chen2020exploring,\n    title={Exploring Simple Siamese Representation Learning}, \n    author={Xinlei Chen and Kaiming He},\n    year={2020},\n    eprint={2011.10566},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{grill2020bootstrap,\n    title = {Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning},\n    author = {Jean-Bastien Grill and Florian Strub and Florent Altch\u00e9 and Corentin Tallec and Pierre H. Richemond and Elena Buchatskaya and Carl Doersch and Bernardo Avila Pires and Zhaohan Daniel Guo and Mohammad Gheshlaghi Azar and Bilal Piot and Koray Kavukcuoglu and R\u00e9mi Munos and Michal Valko},\n    year = {2020},\n    eprint = {2006.07733},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zbontar2021barlow,\n  title={Barlow Twins: Self-Supervised Learning via Redundancy Reduction},\n  author={Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\\'e}phane},\n  journal={arXiv preprint arXiv:2103.03230},\n  year={2021}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MaxLikesMath/Barlow-Twins-Pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-16T15:00:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-25T08:00:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8505065044739585,
        0.9062559092598952
      ],
      "excerpt": "An easy-to-use modular implementation of Barlow Twins: Self-Supervised Learning via Redundancy Reduction. \nThe code is adapted from the original github repo, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "An easy-to-use implementation of Barlow Twins for Pytorch.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MaxLikesMath/Barlow-Twins-Pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 00:56:47 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MaxLikesMath/Barlow-Twins-Pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "MaxLikesMath/Barlow-Twins-Pytorch",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MaxLikesMath/Barlow-Twins-Pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 MaxH\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Simple PyTorch Implementation of Barlow Twins",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Barlow-Twins-Pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "MaxLikesMath",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MaxLikesMath/Barlow-Twins-Pytorch/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Wed, 29 Dec 2021 00:56:47 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "pytorch-implementation",
      "self-supervised-learning",
      "deep-learning",
      "unsupervised-learning",
      "barlow-twins",
      "cnn",
      "python",
      "computer-vision",
      "pytorch-cnn",
      "self-super"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Using the package is relatively straightforward:\r\n\r\n```python\r\nfrom Twins.barlow import *\r\nfrom Twins.transform_utils import *\r\nimport torch\r\nfrom torchvision import models\r\nimport torchvision.transforms as transforms\r\nimport torchvision.datasets as dsets\r\n\r\n#:This is just any generic model\r\nmodel = torchvison.some_model\r\n\r\n#:Optional: define transformations for your specific dataset.\r\n#:Generally, it is best to use the original augmentations in the\r\n#:paper, replacing the Imagenet normalization with the normalization\r\n#:for your dataset.\r\n\r\ntransform = transforms.Compose([\r\n                transforms.RandomResizedCrop(image_size,\r\n                                            interpolation=Image.BICUBIC),\r\n                transforms.RandomHorizontalFlip(p=0.5),\r\n                transforms.RandomApply(\r\n                    [transforms.ColorJitter(brightness=0.4, contrast=0.4,\r\n                                            saturation=0.2, hue=0.1)],\r\n                    p=0.8\r\n                ),\r\n                transforms.RandomGrayscale(p=0.2),\r\n                GaussianBlur(p=1.0),\r\n                Solarization(p=0.0),\r\n                transforms.ToTensor(),\r\n                transforms.Normalize(mean, std)\r\n            ])\r\n\r\n#:For the transform argument for the dataset, pass in \r\n#: Twins.transform_utils.Transform(transform_1, transform_2)\r\n#:If transforms are None, the Imagenet default is used.\r\ndataset = dsets.some_dataset(**kwargs, \r\n                             transform=Transform(transform, transform))\r\n\r\nloader = torch.utils.data.DataLoader(dataset,\r\n                                        batch_size=batch_size,\r\n                                        shuffle=True)\r\n#:Make the BT instance, passing the model, the latent rep layer id,\r\n#: hidden units for the projection MLP, the tradeoff factor,\r\n#: and the loss scale.\r\nlearner = BarlowTwins(model, 'avg_pool', [512,1024, 1024, 1024],\r\n                      3.9e-3, 1)\r\n\r\noptimizer = torch.optim.Adam(learner.parameters(), lr=0.001)\r\n\r\n#:Single training epoch\r\nfor batch_idx, ((x1,x2), _) in enumerate(loader):\r\n    loss = learner(x1, x2)\r\n    optimizer.zero_grad()\r\n    loss.backward()\r\n    optimizer.step()\r\n```\r\nThat is basically it. Hopefully this is helpful!\r\n\r\n",
      "technique": "Header extraction"
    }
  ]
}