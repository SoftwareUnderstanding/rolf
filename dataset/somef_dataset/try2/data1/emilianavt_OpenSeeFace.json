{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1704.04861",
      "https://arxiv.org/abs/1905.02244",
      "https://arxiv.org/abs/1904.07399"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{deng2019retinaface,\n  title={RetinaFace: Single-stage Dense Face Localisation in the Wild},\n  author={Deng, Jiankang and Guo, Jia and Yuxiang, Zhou and Jinke Yu and Irene Kotsia and Zafeiriou, Stefanos},\n  booktitle={arxiv},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{yang2016wider,\n  Author = {Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},\n  Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  Title = {WIDER FACE: A Face Detection Benchmark},\n  Year = {2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{wayne2018lab,\n  author = {Wu, Wayne and Qian, Chen and Yang, Shuo and Wang, Quan and Cai, Yici and Zhou, Qiang},\n  title = {Look at Boundary: A Boundary-Aware Face Alignment Algorithm},\n  booktitle = {CVPR},\n  month = June,\n  year = {2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{bulat2017far,\n  title={How far are we from solving the 2D \\&amp; 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)},\n  author={Bulat, Adrian and Tzimiropoulos, Georgios},\n  booktitle={International Conference on Computer Vision},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9890131201201999,
        0.9999955246615172,
        0.9960965048981569
      ],
      "excerpt": "  author={Bulat, Adrian and Tzimiropoulos, Georgios}, \n  booktitle={International Conference on Computer Vision}, \n  year={2017} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999981916393006,
        0.9995605194554658,
        0.9953582834232122,
        0.8955886365383559,
        0.9959021299438506
      ],
      "excerpt": "  author = {Wu, Wayne and Qian, Chen and Yang, Shuo and Wang, Quan and Cai, Yici and Zhou, Qiang}, \n  title = {Look at Boundary: A Boundary-Aware Face Alignment Algorithm}, \n  booktitle = {CVPR}, \n  month = June, \n  year = {2018} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999709205711148,
        0.9999999995130295,
        0.9815649454650704,
        0.9954488832581693
      ],
      "excerpt": "  Author = {Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou}, \n  Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, \n  Title = {WIDER FACE: A Face Detection Benchmark}, \n  Year = {2016} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.99874514568038,
        0.9997621506499001
      ],
      "excerpt": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam \nSearching for MobileNetV3 by Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.994315882944322
      ],
      "excerpt": "Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression by Xinyao Wang, Liefeng Bo, Li Fuxin \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9699511706017162,
        0.9999746530919912,
        0.9978847734019899,
        0.9960965048981569
      ],
      "excerpt": "  title={RetinaFace: Single-stage Dense Face Localisation in the Wild}, \n  author={Deng, Jiankang and Guo, Jia and Yuxiang, Zhou and Jinke Yu and Irene Kotsia and Zafeiriou, Stefanos}, \n  booktitle={arxiv}, \n  year={2019} \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/emilianavt/OpenSeeFace",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-01T12:49:35Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T02:36:31Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9696445264611642,
        0.9666977796963176,
        0.9035847850291968
      ],
      "excerpt": "Note: This is a tracking library, not a stand-alone avatar puppeteering program. I'm also working on VSeeFace, which allows animating VRM and VSFAvatar 3D models by using OpenSeeFace tracking. VTube Studio uses OpenSeeFace for webcam based tracking to animate Live2D models. A renderer for the Godot engine can be found here. \nThis project implements a facial landmark detection model based on MobileNetV3. \nAs Pytorch 1.3 CPU inference speed on Windows is very low, the model was converted to ONNX format. Using onnxruntime it can run at 30 - 60 fps tracking a single face. There are four models, with different speed to tracking quality trade-offs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051623351837301
      ],
      "excerpt": "An up to date sample video can be found here, showing the default tracking model's performance under different noise and light levels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8173187868735649
      ],
      "excerpt": "The general process is as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421168264605666
      ],
      "excerpt": "After doing this for a while, untick the recording box and work on capturing another expression. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9193764395334899,
        0.8552974092143937
      ],
      "excerpt": "You should also get some statistics in the lower part of the component. \nIf there are issues with any expression being detected, keep adding data to it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9476671001215417,
        0.9666007361048093,
        0.9633438607180973,
        0.9693140309511282,
        0.8482320468903559
      ],
      "excerpt": "Before starting to capture expressions, make some faces and wiggle your eyebrows around, to warm up the feature detection part of the tracker. \nOnce you have a detection model that works decently, when using it take a moment to check all the expressions work as intended and add a little data if not. \nThe tracking seems to be quite robust even with partial occlusion of the face, glasses or bad lighting conditions. \nThe highest quality model is selected with --model 3, the fastest model with the lowest tracking quality is --model 0. \nLower tracking quality mainly means more rigid tracking, making it harder to detect blinking and eyebrow motion. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8319907277443793,
        0.9231851830708556,
        0.9539998453958575,
        0.9226119656712969,
        0.9812083169798291,
        0.9814887733383514,
        0.9704327986298419
      ],
      "excerpt": "When setting the number of faces to track to a higher number than the number of faces actually in view, the face detection model will run every --scan-every frames. This can slow things down, so try to set --faces no higher than the actual number of faces you are tracking. \nFour pretrained face landmark models are included. Using the --model switch, it is possible to select them for tracking. The given fps values are for running the model on a single face video on a single CPU core. Lowering the frame rate would reduce CPU usage by a corresponding degree. \nModel -1: This model is for running on toasters, so it's a very very fast and very low accuracy model. (213fps without gaze tracking) \nModel 0: This is a very fast, low accuracy model. (68fps) \nModel 1: This is a slightly slower model with better accuracy. (59fps) \nModel 2: This is a slower model with good accuracy. (50fps) \nModel 3 (default): This is the slowest and highest accuracy model. (44fps) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.996936545770946,
        0.9618800395160916
      ],
      "excerpt": "The landmark model is quite robust with respect to the size and orientation of the faces, so the custom face detection model gets away with rougher bounding boxes than other approaches. It has a favorable speed to accuracy ratio for the purposes of this project. \nThe builds in the release section of this repository contain a facetracker.exe inside a Binary folder that was built using pyinstaller and contains all required dependencies. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8081458574225184
      ],
      "excerpt": "  title={How far are we from solving the 2D \\&amp; 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9691824500841739
      ],
      "excerpt": "Additional training has been done on the WFLW dataset after reducing it to 66 points and replacing the contour points and tip of the nose with points predicted by the model trained up to this point. This additional training is done to improve fitting to eyes and eyebrows. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8004570961184779,
        0.979580204409224
      ],
      "excerpt": "For the training the gaze and blink detection model, the MPIIGaze dataset was used. Additionally, around 125000 synthetic eyes generated with UnityEyes were used during training. \nIt should be noted that additional custom data was also used during the training process and that the reference landmarks from the original datasets have been modified in certain ways to address various issues. It is likely not possible to reproduce these models with just the original LS3D-W and WFLW datasets, however the additional data is not redistributable. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8134513379520899,
        0.9213850718017956,
        0.9310075393493455
      ],
      "excerpt": "The algorithm is inspired by: \nDesigning Neural Network Architectures for Different Applications: From Facial Landmark Tracking to Lane Departure Warning System by YiTa Wu, Vice President of Engineering, ULSee \nReal-time Human Pose Estimation in the Browser with TensorFlow.js \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714232225564905,
        0.8614785767750397
      ],
      "excerpt": "For expression detection, LIBSVM is used. \nFace detection is done using a custom heatmap regression based face detection model or RetinaFace. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9861964275233991
      ],
      "excerpt": "RetinaFace detection is based on this implementation. The pretrained model was modified to remove unnecessary landmark detection and converted to ONNX format for a resolution of 640x640. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "@ENiwatori and family. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Robust realtime face and facial landmark tracking on CPU with Unity integration",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/emilianavt/OpenSeeFace/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 75,
      "date": "Fri, 24 Dec 2021 02:18:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/emilianavt/OpenSeeFace/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "emilianavt/OpenSeeFace",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9426769356630551
      ],
      "excerpt": "When distributing it, you should also distribute the Licenses folder along with it to make sure you conform to requirements set forth by some of the third party libraries. Unused models can be removed from redistributed packages without issue. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8295671551432842
      ],
      "excerpt": "To save both the trained model and the captured training data, type in a filename including its full path in the \"Filename\" field and tick the \"Save\" box. To load it, enter the filename and tick the \"Load\" box. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/emilianavt/OpenSeeFace/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C#",
      "C++",
      "C",
      "Batchfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 2-Clause \"Simplified\" License",
      "url": "https://api.github.com/licenses/bsd-2-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 2-Clause License\\n\\nCopyright (c) 2019, Emiliana (https://twitter.com/Emiliana_vt / https://github.com/emilianavt/)\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice,\\n   this list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright\\n   notice, this list of conditions and the following disclaimer in the\\n   documentation and/or other materials provided with the distribution.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\\nPOSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Overview",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "OpenSeeFace",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "emilianavt",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/emilianavt/OpenSeeFace/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "Changed the gaze tracking model to always run single threaded.",
        "dateCreated": "2021-09-11T13:57:18Z",
        "datePublished": "2021-09-17T20:20:39Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.20.4",
        "name": "OpenSeeFace v1.20.4",
        "tag_name": "v1.20.4",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.20.4",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/49774883",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.20.4"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "Fixed various issues, made other small improvements and added a new, experimental tracking model with improved wink support.",
        "dateCreated": "2021-08-07T15:47:14Z",
        "datePublished": "2021-08-07T16:45:28Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.20.3",
        "name": "OpenSeeFace v1.20.3",
        "tag_name": "v1.20.3",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.20.3",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/47468876",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.20.3"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "Fixed some bugs.",
        "dateCreated": "2020-12-13T11:46:51Z",
        "datePublished": "2020-12-13T12:33:02Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.20.2",
        "name": "OpenSeeFace v1.20.2",
        "tag_name": "v1.20.2",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.20.2",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/35203193",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.20.2"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "Reduced the impact of eye blinks and jaw movement on head pose estimation and fixed various things.",
        "dateCreated": "2020-12-07T17:11:44Z",
        "datePublished": "2020-12-07T17:20:27Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.20.1",
        "name": "OpenSeeFace v1.20.1",
        "tag_name": "v1.20.1",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.20.1",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/34940159",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.20.1"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "This release contains various fixes and adjustments as well as two new tracking models with different quality and speed tradeoffs.",
        "dateCreated": "2020-11-28T14:54:06Z",
        "datePublished": "2020-11-28T15:11:33Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.20.0",
        "name": "OpenSeeFace v1.20.0",
        "tag_name": "v1.20.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.20.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/34532832",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.20.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "This release adds support for jaw bone animation to `OpenSeeVRMDriver`, dynamic port selection support to `OpenSeeLauncher` and updates the binary build of the face tracker to use onnxruntime 1.5.1, fixing some performance issues caused by the tracker not respecting thread limits.",
        "dateCreated": "2020-10-15T15:58:35Z",
        "datePublished": "2020-10-16T13:01:54Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.19.0",
        "name": "OpenSeeFace v1.19.0",
        "tag_name": "v1.19.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.19.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/32671219",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.19.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "This release decodes landmarks from the python side again, as somehow using the decoded landmarks from setting `inference=True` on the models caused some issues",
        "dateCreated": "2020-10-08T11:07:51Z",
        "datePublished": "2020-10-08T11:51:50Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.18.3",
        "name": "OpenSeeFace v1.18.3",
        "tag_name": "v1.18.3",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.18.3",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/32316901",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.18.3"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "With this release, the landmark models decode landmarks within the ONNX models again. The included binary should run with lower CPU utilization at the same speed when enabling multithreading.",
        "dateCreated": "2020-10-05T17:57:57Z",
        "datePublished": "2020-10-05T18:04:47Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.18.2",
        "name": "OpenSeeFace v1.18.2",
        "tag_name": "v1.18.2",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.18.2",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/32180177",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.18.2"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "This release fixes a bug with the `OpenSeeLauncher` and adds support for selecting device capability lines for direct show cameras and a `--benchmark` option.",
        "dateCreated": "2020-09-28T23:50:13Z",
        "datePublished": "2020-09-29T00:03:02Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.17.0",
        "name": "OpenSeeFace v1.17.0",
        "tag_name": "v1.17.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.17.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/31910691",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.17.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "The main new feature with this release is a very fast, less accurate thirty point tracking model that can be activated with `--model -1`.",
        "dateCreated": "2020-09-05T19:03:54Z",
        "datePublished": "2020-09-05T19:23:11Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.16.0",
        "name": "OpenSeeFace v1.16.0",
        "tag_name": "v1.16.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.16.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/30784900",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.16.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "This release contains bug fixes and mostly small improvements. It also contains an optional, simplified expression detection method.",
        "dateCreated": "2020-08-21T20:47:02Z",
        "datePublished": "2020-08-21T21:25:29Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.15.3",
        "name": "OpenSeeFace v1.15.3",
        "tag_name": "v1.15.3",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.15.3",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/30000193",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.15.3"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "Further reliability improvements. Landmark decoding has been moved from the ONNX model back into `tracker.py` in one of the recent releases, but `model.py` still contains suitable decoding code for exporting your own ONNX model containing the landmark decoding.",
        "dateCreated": "2020-08-15T17:14:38Z",
        "datePublished": "2020-08-17T09:37:10Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.15.2",
        "name": "OpenSeeFace v1.15.2",
        "tag_name": "v1.15.2",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.15.2",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/29763218",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.15.2"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "Improved reliability and logging.",
        "dateCreated": "2020-08-11T11:42:01Z",
        "datePublished": "2020-08-11T11:44:57Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.15.1",
        "name": "OpenSeeFace v1.15.1",
        "tag_name": "v1.15.1",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.15.1",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/29555041",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.15.1"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "This release includes various small optimizations that can lead to a 20-25% performance gain under certain conditions.",
        "dateCreated": "2020-08-10T16:05:09Z",
        "datePublished": "2020-08-10T16:36:47Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.15.0",
        "name": "OpenSeeFace v1.15.0",
        "tag_name": "v1.15.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.15.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/29523773",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.15.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "This release fixes further issues with the new camera code.",
        "dateCreated": "2020-08-08T15:13:48Z",
        "datePublished": "2020-08-08T15:24:04Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.14.2",
        "name": "OpenSeeFace v1.14.2",
        "tag_name": "v1.14.2",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.14.2",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/29476709",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.14.2"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "This release accumulates a lot of random changes, mainly to the Unity parts. One big change is that escapi is no longer used for webcam reading on Windows and has been replaced with libdshowcamera, which is also used in OBS.",
        "dateCreated": "2020-08-06T15:59:34Z",
        "datePublished": "2020-08-06T18:23:53Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.14.0",
        "name": "OpenSeeFace v1.14.0",
        "tag_name": "v1.14.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.14.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/29420749",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.14.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "This release introduces a new model for `--model 2`. Unity components relying on 3D points will now skip frames with very bad 3D fits.",
        "dateCreated": "2020-03-30T14:44:33Z",
        "datePublished": "2020-03-30T15:04:28Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.13.0",
        "name": "OpenSeeFace v1.13.0",
        "tag_name": "v1.13.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.13.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/25008773",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.13.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "This release contains various improvements, such as improved models for `--model 0` and `--model 1`.",
        "dateCreated": "2020-03-28T13:57:56Z",
        "datePublished": "2020-03-28T14:21:47Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.12.0",
        "name": "OpenSeeFace v1.12.0",
        "tag_name": "v1.12.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.12.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/24959612",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.12.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "The main new feature of this release is a new default face detection model that is used to lock onto yet untracked faces for tracking. There are also some other small changes and fixes. The chin tracking point is used for fitting the 3D points once more to prevent the `solvePnP` function from choosing an upside down fit.",
        "dateCreated": "2020-03-22T13:20:17Z",
        "datePublished": "2020-03-22T13:27:03Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.11.0",
        "name": "OpenSeeFace v1.11.0",
        "tag_name": "v1.11.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.11.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/24748199",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.11.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "New in this version:\r\n\r\n* 3D points are now scaled according to the reference model along X and Y axis to make them more consistent. The tip of the nose should always be at (0, 0) and the nose should point straight up.\r\n* `OpenSeeVRMDriver` can now animate mouth and eyes using the face tracking information.\r\n* `OpenSeeVRMDriver` supports global hotkeys to override expressions.\r\n* Small fixes and changes.",
        "dateCreated": "2020-03-08T17:15:43Z",
        "datePublished": "2020-03-08T22:28:39Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.10.0",
        "name": "OpenSeeFace v1.10.0",
        "tag_name": "v1.10.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.10.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/24330328",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.10.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "This version introduces a number of manually designed features that can be read out from the `OpenSeeData` structure under the `features` field.",
        "dateCreated": "2020-03-01T13:40:30Z",
        "datePublished": "2020-03-01T13:44:47Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.9.0",
        "name": "OpenSeeFace v1.9.0",
        "tag_name": "v1.9.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.9.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/24109642",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.9.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "This version introduces a RetinaFace model for face detection, making it more reliable.",
        "dateCreated": "2020-02-25T23:54:32Z",
        "datePublished": "2020-02-26T00:01:09Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.8.0",
        "name": "OpenSeeFace v1.8.0",
        "tag_name": "v1.8.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.8.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/23983274",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.8.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "This version adds eye tracking. The `OpenSeeVRMDriver` example component also is updated to apply eye tracking data to VRM models.",
        "dateCreated": "2020-02-24T22:31:35Z",
        "datePublished": "2020-02-24T22:39:17Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.7.0",
        "name": "OpenSeeFace v1.7.0",
        "tag_name": "v1.7.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.7.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/23946378",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.7.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "New in this version:\r\n\r\n* The 3D model used internally for head pose and depth estimation will now adapt to the tracked landmarks over time, increasing accuracy and reducing 3D point movement due to head rotation.\r\n* `OpenSeeLauncher` will now work in projects built using IL2CPP when enabling the `usePinvoke` flag.\r\n* An option was added to the face tracker to offset face ids by a given value to allow receiving face tracking data from multiple sources.\r\n* The `OpenSee` component now actually closes its UDP socket. Previously, it could linger around, leading to issues when restarting a program using the component.\r\n* The `OpenSeeVRMDriver` component has been added to the examples directory. It will apply expressions detected by `OpenSeeExpression` to a VRM model and, when enabled for a given expression, let the model's eyes blink. In addition to this, it will also use OVRLipSync to apply visemes according to audio input. Other than the main `OVRLipSync` component, no additional components from that library are required in the scene for this.",
        "dateCreated": "2020-02-20T20:33:00Z",
        "datePublished": "2020-02-20T20:43:58Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.6.0",
        "name": "OpenSeeFace v1.6.0",
        "tag_name": "v1.6.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.6.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/23857350",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.6.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "New in this version:\r\n\r\n* A bug in expression training was fixed that reduced the training accuracy.\r\n* It is now possible to train expression detection models in such a way that they output probabilities, but this is much slower.\r\n* The `OpenSeeIKTarget` component now can mirror movement and rotation.",
        "dateCreated": "2020-02-13T21:24:41Z",
        "datePublished": "2020-02-13T21:34:47Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.5.0",
        "name": "OpenSeeFace v1.5.0",
        "tag_name": "v1.5.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.5.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/23660971",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.5.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "This release fixes a number of small bugs and includes an example for how to apply detected expressions to VRM models. In addition to this, the `OpenSeeExpression` component was changed to have less harsh requirements for training models, making an iterative approach much easier, where you collect a bit of data, test, collect more, test, and so on.",
        "dateCreated": "2020-02-11T08:55:31Z",
        "datePublished": "2020-02-11T22:48:09Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.4.0",
        "name": "OpenSeeFace v1.4.0",
        "tag_name": "v1.4.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.4.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/23586452",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.4.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "New things:\r\n\r\n* MobileNetV3 based models with better accuracy\r\n* SVM based expression detection for Unity\r\n* SVM training and inference for Unity through the `SVMModel` class using LIBSVM\r\n* `--models` was reordered to be more easily understood\r\n* Tracking multiple faces keeps track of which face is which\r\n* `--visualize` was changed. `2` now shows face ids, while `3` shows point numbers\r\n\r\nThis release was updated to include fixes for rotations and coordinates in Unity.",
        "dateCreated": "2020-02-09T00:25:55Z",
        "datePublished": "2020-02-08T20:39:30Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.3.1",
        "name": "OpenSeeFace v1.3.1",
        "tag_name": "v1.3.1",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.3.1",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/23506997",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.3.1"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": " All models were retrained to fix bugs in the training process. There are also other bug fixes and improvements.",
        "dateCreated": "2020-01-26T10:47:07Z",
        "datePublished": "2020-01-26T11:00:53Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.2.0",
        "name": "OpenSeeFace v1.2.0",
        "tag_name": "v1.2.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.2.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/23131979",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.2.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "Blink detection and stabilized 3D pose estimation",
        "dateCreated": "2020-01-03T15:41:07Z",
        "datePublished": "2020-01-03T15:45:20Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.1.0",
        "name": "OpenSeeFace v1.1.0",
        "tag_name": "v1.1.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.1.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/22579450",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.1.0"
      },
      {
        "authorType": "User",
        "author_name": "emilianavt",
        "body": "",
        "dateCreated": "2020-01-02T11:24:25Z",
        "datePublished": "2020-01-02T11:30:52Z",
        "html_url": "https://github.com/emilianavt/OpenSeeFace/releases/tag/v1.0.0",
        "name": "OpenSeeFace v1.0.0",
        "tag_name": "v1.0.0",
        "tarball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/tarball/v1.0.0",
        "url": "https://api.github.com/repos/emilianavt/OpenSeeFace/releases/22551032",
        "zipball_url": "https://api.github.com/repos/emilianavt/OpenSeeFace/zipball/v1.0.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python 3.7\n* ONNX Runtime\n* OpenCV\n* Pillow\n* Numpy\n\nThe required libraries can be installed using pip:\n\n     pip install onnxruntime opencv-python pillow numpy\n\nAlternatively poetry can be used to \ninstall all dependencies for this project in a separate virtual env:\n\n     poetry install\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 542,
      "date": "Fri, 24 Dec 2021 02:18:04 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "face-tracking",
      "face-landmarks",
      "depth-estimation",
      "unity",
      "unity3d",
      "python",
      "csharp",
      "udp",
      "onnx",
      "onnxruntime",
      "virtual-youtuber",
      "vtuber",
      "mobilenetv3",
      "pytorch",
      "openseeface",
      "face-detection",
      "detection-model",
      "landmark-model",
      "tracker",
      "cpu"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "A sample Unity project for VRM based avatar animation can be found [here](https://github.com/emilianavt/OpenSeeFaceSample).\n\nThe face tracking itself is done by the `facetracker.py` Python 3.7 script. It is a commandline program, so you should start it manually from cmd or write a batch file to start it. If you downloaded a release and are on Windows, you can run the `facetracker.exe` inside the `Binary` folder without having Python installed. You can also use the `run.bat` inside the `Binary` folder for a basic demonstration of the tracker.\n\nThe script will perform the tracking on webcam input or video file and send the tracking data over UDP. This design also allows tracking to be done on a separate PC from the one who uses the tracking information. This can be useful to enhance performance and to avoid accidentially revealing camera footage.\n\nThe provided `OpenSee` Unity component can receive these UDP packets and provides the received information through a public field called `trackingData`. The `OpenSeeShowPoints` component can visualize the landmark points of a detected face. It also serves as an example. Please look at it to see how to properly make use of the `OpenSee` component. Further examples are included in the `Examples` folder. The UDP packets are received in a separate thread, so any components using the `trackingData` field of the `OpenSee` component should first copy the field and access this copy, because otherwise the information may get overwritten during processing. This design also means that the field will keep updating, even if the `OpenSee` component is disabled.\n\nRun the python script with `--help` to learn about the possible options you can set.\n\n    python facetracker.py --help\n\nA simple demonstration can be achieved by creating a new scene in Unity, adding an empty game object and both the `OpenSee` and `OpenSeeShowPoints` components to it. While the scene is playing, run the face tracker on a video file:\n\n    python facetracker.py --visualize 3 --pnp-points 1 --max-threads 4 -c video.mp4\n\n__Note__: If dependencies were installed using [poetry](https://python-poetry.org/), the commands have to be executed from a `poetry shell` or have to be prefixed with `poetry run`.\n\nThis way the tracking script will output its own tracking visualization while also demonstrating the transmission of tracking data to Unity.\n\nThe included `OpenSeeLauncher` component allows starting the face tracker program from Unity. It is designed to work with the pyinstaller created executable distributed in the binary release bundles. It provides three public API functions:\n\n* `public string[] ListCameras()` returns the names of available cameras. The index of the camera in the array corresponds to its ID for the `cameraIndex` field. Setting the `cameraIndex` to `-1` will disable webcam capturing.\n* `public bool StartTracker()` will start the tracker. If it is already running, it will shut down the running instance and start a new one with the current settings.\n* `public void StopTracker()` will stop the tracker. The tracker is stopped automatically when the application is terminated or the `OpenSeeLauncher` object is destroyed.\n\nThe `OpenSeeLauncher` component uses WinAPI job objects to ensure that the tracker child process is terminated if the application crashes or closes without terminating the tracker process first.\n\nAdditional custom commandline arguments should be added one by one into elements of `commandlineArguments` array. For example `-v 1` should be added as two elements, one element containing `-v` and one containing `1`, not a single one containing both parts.\n\nThe included `OpenSeeIKTarget` component can be used in conjunction with FinalIK or other IK solutions to animate head motion.\n\n",
      "technique": "Header extraction"
    }
  ]
}