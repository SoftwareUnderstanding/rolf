{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository contains all code and data for the\n\"Towards Robust Named Entity Recognition for Historic German\" paper, that was\naccepted at [4th Workshop on Representation Learning for NLP](https://sites.google.com/view/repl4nlp2019/accepted-papers)\n(RepL4NLP) at ACL 2019. \n\nPaper is available [here](https://www.aclweb.org/anthology/W19-4312/).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We would like to thank the anonymous reviewers for their helpful and valuable\ncomments.\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1810.04805"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use our trained language models or results in your research, please use\nthe following *BibTeX* entry:\n\n```bibtex\n@inproceedings{schweter-baiter-2019-towards,\n    title = \"Towards Robust Named Entity Recognition for Historic {G}erman\",\n    author = \"Schweter, Stefan  and\n      Baiter, Johannes\",\n    booktitle = \"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)\",\n    month = aug,\n    year = \"2019\",\n    address = \"Florence, Italy\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/W19-4312\",\n    pages = \"96--103\",\n    abstract = \"In this paper we study the influence of using language model pre-training for named entity recognition for Historic German. We achieve new state-of-the-art results using carefully chosen training data for language models. For a low-resource domain like named entity recognition for Historic German, language model pre-training can be a strong competitor to CRF-only methods. We show that language model pre-training can be more effective than using transfer-learning with labeled datasets. Furthermore, we introduce a new language model pre-training objective, synthetic masked language model pre-training (SMLM), that allows a transfer from one domain (contemporary texts) to another domain (historical texts) by using only the same (character) vocabulary. Results show that using SMLM can achieve comparable results for Historic named entity recognition, even when they are only trained on contemporary texts. Our pre-trained character-based language models improve upon classical CRF-based methods and previous work on Bi-LSTMs by boosting F1 score performance by up to 6{\\%}.\",\n}\n```",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{schweter-baiter-2019-towards,\n    title = \"Towards Robust Named Entity Recognition for Historic {G}erman\",\n    author = \"Schweter, Stefan  and\n      Baiter, Johannes\",\n    booktitle = \"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)\",\n    month = aug,\n    year = \"2019\",\n    address = \"Florence, Italy\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/W19-4312\",\n    pages = \"96--103\",\n    abstract = \"In this paper we study the influence of using language model pre-training for named entity recognition for Historic German. We achieve new state-of-the-art results using carefully chosen training data for language models. For a low-resource domain like named entity recognition for Historic German, language model pre-training can be a strong competitor to CRF-only methods. We show that language model pre-training can be more effective than using transfer-learning with labeled datasets. Furthermore, we introduce a new language model pre-training objective, synthetic masked language model pre-training (SMLM), that allows a transfer from one domain (contemporary texts) to another domain (historical texts) by using only the same (character) vocabulary. Results show that using SMLM can achieve comparable results for Historic named entity recognition, even when they are only trained on contemporary texts. Our pre-trained character-based language models improve upon classical CRF-based methods and previous work on Bi-LSTMs by boosting F1 score performance by up to 6{\\%}.\",\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8636272261581446
      ],
      "excerpt": "23.07.2019: Initial version. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.937614004678116
      ],
      "excerpt": "Akbik et al., (2018), as they have \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9911959320294913
      ],
      "excerpt": "Flair (Akbik et al., 2018) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9302785107180478
      ],
      "excerpt": "Lample et al. (2016b) combine word \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8223200383550148
      ],
      "excerpt": "Bi-LSTM with CRF on top as proposed by Huang et al. (2015). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.922287838462994
      ],
      "excerpt": "Riedl and Pad\u00f3 (2018) (74.33%), who \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9493765143687258
      ],
      "excerpt": "Riedl and Pad\u00f3 (2018) (78.56%). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.941253838603129,
        0.8292136633006085
      ],
      "excerpt": "| LFT     | Riedl and Pad\u00f3 (2018) (no transfer-learning)   | 69.62% \n| LFT     | Riedl and Pad\u00f3 (2018) (with transfer-learning) | 74.33% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.941253838603129,
        0.8292136633006085
      ],
      "excerpt": "| ONB     | Riedl and Pad\u00f3 (2018) (no transfer-learning)   | 73.31% \n| ONB     | Riedl and Pad\u00f3 (2018) (with transfer-learning) | 78.56% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9497333350088942
      ],
      "excerpt": "Akbik et al., (2018). This model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9911959320294913
      ],
      "excerpt": "German Wikipedia (Akbik et al., 2018) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9517606609040192
      ],
      "excerpt": "We also consider the masked language modeling (SMLM) objective of Devlin et al. (2018). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8842858533388339
      ],
      "excerpt": "corpus consisting of contemporary texts from Leipzig Corpora Collection (Goldhahn et al., 2012) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "| LFT (1926)        | Multi-lingual BERT                                             | \u2713              | Wikipedia               | 74.39% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9493765143687258,
        0.9493765143687258
      ],
      "excerpt": "| LFT (1926)        | Riedl and Pad\u00f3 (2018)  | -              | -                       | 69.62% \n| LFT (1926)        | Riedl and Pad\u00f3 (2018)\u2020 | -              | -                       | 74.33% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "| ONB (1710 - 1873) | Multi-lingual BERT                                             | \u2713              | Wikipedia               | 77.19% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9493765143687258,
        0.9493765143687258
      ],
      "excerpt": "| ONB (1710 - 1873) | Riedl and Pad\u00f3 (2018)  | -              | -                       | 73.31% \n| ONB (1710 - 1873) | Riedl and Pad\u00f3 (2018)\u2020 | -              | -                       | 78.56% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| LFT     | German    |         8.30 |          8.70 | 76.07% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8088876880207017
      ],
      "excerpt": "For a low-resource domain like named entity recognition for Historic German, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8255290041477188
      ],
      "excerpt": "proposed by Cotterell and Duh (2017). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8984478102791149
      ],
      "excerpt": "achieve comparable results for Historic named entity recognition, even when they \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "  --number INTEGER   Define experiment number \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8582350767192024
      ],
      "excerpt": "With the --number argument you should define a unique id for your experiment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "de - German Flair Embeddings \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dbmdz/historic-ner",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-11T14:29:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-27T12:02:22Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Named entity recognition (NER) is a central component in natural language\nprocessing tasks. Identifying named entities is a key part in systems e.g.\nfor question answering or entity linking. Traditionally, NER systems are built\nusing conditional random fields (CRFs). Recent systems are using neural network\narchitectures like bidirectional LSTM with a CRF-layer ontop and pre-trained\nword embeddings ([Ma and Hovy, 2016](http://aclweb.org/anthology/P16-1101);\n[Lample et al., 2016a](http://aclweb.org/anthology/N16-1030);\n[Reimers and Gurevych, 2017](http://aclweb.org/anthology/D17-1035);\n[Lin et al., 2017](http://aclweb.org/anthology/W17-4421)).\n\nPre-trained word embeddings have been shown to be of great use for downstream\nNLP tasks ([Mikolov et al., 2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf);\n[Pennington et al., 2014](https://www.aclweb.org/anthology/D14-1162)). Many\nrecently proposed approaches go beyond these pre-trained embeddings. Recent\nworks have proposed methods that produce different representations for the same\nword depending on its contextual usage\n([Peters et al., 2017](http://aclweb.org/anthology/P17-1161),[2018a](https://aclweb.org/anthology/N18-1202);\n[Akbik et al., 2018](https://www.aclweb.org/anthology/C18-1139);\n[Devlin et al., 2018](https://arxiv.org/abs/1810.04805)).\nThese methods have shown to be very powerful in the fields of named entity\nrecognition, coreference resolution, part-of-speech tagging and question\nanswering, especially in combination with classic word embeddings.\n\nOur paper is based on the work of [Riedl and Pad\u00f3 (2018)](http://aclweb.org/anthology/P18-2020).\nThey showed how to build a\nmodel for German named entity recognition (NER) that performs at the state of\nthe art for both contemporary and historical texts. Labeled historical texts\nfor German named entity recognition are a low-resource domain. In order to\nachieve robust state-of-the-art results for historical texts they used\ntransfer-learning with labeled data from other high-resource domains like\nCoNLL-2003 ([Tjong Kim Sang and De Meulder, 2003](http://aclweb.org/anthology/W03-0419))\nor GermEval ([Benikova et al., 2014](http://www.lrec-conf.org/proceedings/lrec2014/pdf/276_Paper.pdf)).\nThey showed that using Bi-LSTM with a CRF as the top layer and word embeddings\noutperforms CRFs with hand-coded features in a big-data situation.\n\nWe build up upon their work and use the same low-resource datasets for Historic\nGerman. Furthermore, we show how to achieve new state-of-the-art results for\nHistoric German named entity recognition by using only unlabeled data via\npre-trained language models and word embeddings. We also introduce a novel\nlanguage model pre-training objective, that uses only contemporary texts for\ntraining to achieve comparable state-of-the-art results on historical texts.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "              and LFT. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8987210785452335,
        0.9346096638224726
      ],
      "excerpt": "Recent advances in language modeling using deep neural networks have shown that \nthese models learn representations, that vary with the network depth from \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8677524929144351,
        0.9225149732478745
      ],
      "excerpt": "language models to low-resource named entity recognition for Historic German. \nWe show on a series of experiments that character-based pre-trained language \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882535852856055,
        0.944956435026778,
        0.8064293440299604
      ],
      "excerpt": "Our pre-trained character-based language models improve upon classical \nCRF-based methods and previous work on Bi-LSTMs by boosting F1 score \nperformance by up to 6%. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925515752913936
      ],
      "excerpt": "(Wikipedia and Crawl) as word embeddings. Flair allows us to easily combine \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9481031319159988
      ],
      "excerpt": "embeddings with character features. In our experiments we combined several \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9608321257031891
      ],
      "excerpt": "trained with a forward and backward character-based language model (LSTM) on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8370435596843506,
        0.9793470207041581
      ],
      "excerpt": "Bi-LSTM with CRF on top as proposed by Huang et al. (2015). \nThe next figure shows a high level overview of our used model. A sentence is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826008931010028,
        0.8977281594462174
      ],
      "excerpt": "language model. From this LM, we retrieve for each word a contextual embedding \nthat we pass into a vanilla Bi-LSTM-CRF: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9370970708313245
      ],
      "excerpt": "These datasets are based on historical texts that were extracted \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164381391154454,
        0.9142340187261346,
        0.8973451494133812,
        0.9449560151700072,
        0.8586455796204543
      ],
      "excerpt": "Europeana collection of historical newspapers. \nThe first corpus is the collection of Tyrolean periodicals and newspapers from \nthe Dr Friedrich Temann Library (LFT). The LFT corpus consists of approximately \n87,000 tokens from 1926. The second corpus is a collection of Austrian newspaper \ntexts from the Austrian National Library (ONB). The ONB corpus consists of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8139901597490311,
        0.9713701783112586,
        0.8789778800090734
      ],
      "excerpt": "remaining entities as miscellaneous (MISC). The next tables contain an overview \nof the number of named entities of the two datasets. No miscellaneous entities \n(MISC) are found in the ONB dataset and only a few are annotated in the LFT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9230885194717384,
        0.8874203984698331
      ],
      "excerpt": "small compared to contemporary corpora like CoNLL-2003 or GermEval. They also \nhave a different language variety (German and Austrian) and they include a high \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9591912509335743
      ],
      "excerpt": "errors or misrecognition of characters (e.g. Bifmarck instead of Bi\u222bmarck) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9727407472424425
      ],
      "excerpt": "performance (0.8 to 1.5%) of our system in some cases. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9919146011785672
      ],
      "excerpt": "FastText for Wikipedia and Common Crawl leads to a F1 score of 72.50% on the LFT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9653465331085698
      ],
      "excerpt": "used transfer-learning with more labeled data. The following table also shows \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066543916147254
      ],
      "excerpt": "| LFT     | Riedl and Pad\u00f3 (2018) (with transfer-learning) | 74.33% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066543916147254
      ],
      "excerpt": "| ONB     | Riedl and Pad\u00f3 (2018) (with transfer-learning) | 78.56% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9636447517347788
      ],
      "excerpt": "language models on two datasets from the Europeana collection of historical \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8476475653012915,
        0.8033570640287986,
        0.9582205233708377
      ],
      "excerpt": "consists of articles from the Wiener Zeitung newspaper (WZ) covering \n801,543,845 tokens from 1703 - 1875. We choose the two corpora, because they \nhave a temporal overlap with the LFT corpus (1926) and the ONB corpus \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.988427688989042,
        0.9219917379707644,
        0.8772604006972776
      ],
      "excerpt": "for the language model corpora and the datasets used in the downstream task. \nThere is a huge temporal overlap between the ONB dataset and the WZ corpus, \nwhereas the overlap between the LFT dataset and the HHA corpus is relatively \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222988899291924,
        0.9279507813619658,
        0.8686471980422104
      ],
      "excerpt": "for comparison. We perform a per-layer analysis of the multi-lingual BERT model \non the development set to find the best layer for our task. For the German \nlanguage model, we use the same pre-trained language model for German as used in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8595726370992645,
        0.9823785143205712,
        0.9632288215509389,
        0.9434596099415511,
        0.8553692583439421
      ],
      "excerpt": "of half a billion tokens. \nThe next table shows that the temporal aspect of training data for the language \nmodels has deep impact on the performance. On LFT (1926) the language model \ntrained on the HHA corpus (1888 - 1945) leads to a F1 score of 77.51%, which \nis a new state-of-the art result on this dataset. The result is 3.18% better \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9873066106174373,
        0.8056615893074229,
        0.9470854471177044,
        0.9528308133405556,
        0.917485291434611,
        0.8720312776921939
      ],
      "excerpt": "which uses transfer-learning with more labeled training data. The language model \ntrained on the WZ corpus (1703-1875) only achieves a F1 score of 75.60%, likely \nbecause the time period of the data used for pre-training (19th century) is too \nfar removed from that of the downstream task (mid-1920s). The next table also \nshows the results of pre-trained language models on the ONB (1710 - 1873) \ndataset. The language models, that were trained on contemporary data like the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8469745814543349,
        0.9708319793768904,
        0.9406368327297828,
        0.891996032745915
      ],
      "excerpt": "texts from the 18-19th century. The language model trained on the HHA corpus \nperforms better, since there is a substantially temporal overlap with the ONB \ncorpus. The language model trained on the WZ corpus (1703-1875) leads to the \nbest results with a F1 score of 85.31%. This result is 6.75% better than the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9083883859082502
      ],
      "excerpt": "which again uses transfer-learning with additionally labeled training data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9850751616551499
      ],
      "excerpt": "subword-based language model, in contrast to our character-based language model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8491758185035599
      ],
      "excerpt": "The main motivation for using SMLM is to transfer a corpus from one domain \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9013788647351386,
        0.8844614668699273
      ],
      "excerpt": "into the source domain. With this technique it is possible to create a synthetic \ncorpus, that \"emulates\" OCR errors or spelling mistakes without having any data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629569526848768,
        0.8415864598228501
      ],
      "excerpt": "datasets. We refer to these characters as target vocabulary. Then we obtained a \ncorpus consisting of contemporary texts from Leipzig Corpora Collection (Goldhahn et al., 2012) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9055260103782382,
        0.9557962133907774,
        0.9035184992806009,
        0.8527458991134749,
        0.9647180811442126
      ],
      "excerpt": "following SMLM objective is used: Iterate overall characters in the \ncontemporary corpus. Leave the character unchanged in 90% of the time. For the \nremaining 10% we employ the following strategy: in 20% of the time replace the \ncharacter with a masked character, that does not exist in the target vocabulary. \nIn 80% of the time we randomly replace the character by a symbol from the target \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.898047325353815,
        0.8676020759826799,
        0.8475741998538665
      ],
      "excerpt": "The following table shows that the language model trained with SMLM achieves \nthe second best result on LFT with 77.16%. The ONB corpus is more challenging \nfor SMLM, because it includes texts from a totally different time period \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9465130846745042,
        0.9612600213974403,
        0.9107073985060996,
        0.8014649172183792
      ],
      "excerpt": "with SMLM only. \nThe following table shows results on LFT and ONB with different language models. \nThe German language model refers to the model used in Akbik et al., (2018). \nWe perform a per-layer analysis for BERT on the development set and use the best \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8922918097612645,
        0.9397367577145438
      ],
      "excerpt": "Wikipedia and Common Crawl as well as character embeddings. \u2020 indicates the \nusage of additional training data (GermEval) for transfer learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9324952957363284,
        0.9198406999844635,
        0.9801799762168214,
        0.974947338938444,
        0.9479932224061716,
        0.9517876308094699,
        0.9501666271183205
      ],
      "excerpt": "The usage of pre-trained character-based language models boosts performance for \nboth LFT and ONB datasets. The results in the previous table show, that the \nselection of the language model corpus plays an important role: a corpus with a \nlarge degree of temporal overlap with the downstream task performs better than \ncorpus with little to no temporal overlap. In order to compare our trained \nlanguage models with each other, we measure both the perplexity of the forward \nlanguage model and the backward language model on the test dataset for LFT and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9588327619630147,
        0.938285015399184
      ],
      "excerpt": "averaged. The results for LFT and ONB are shown in the next table. \nFor all language models (except one) there is a clear correlation between \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9862470241253309,
        0.9168941786815822,
        0.8664157305060953,
        0.9623673535936276,
        0.9686368966911689,
        0.8886411784174432
      ],
      "excerpt": "forward and backward language model) yields better performance in terms of the \nF1 score on the downstream NER tasks. But this assumption does not hold for the \nlanguage model that was trained on synthetic data via SMLM objective: \nThe perplexity for this language model (both forward and backward) is relatively \nhigh compared to other language models, but the F1 score results are better than \nsome other language models with lower perplexity. This variation can be observed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8987804760713227,
        0.8537908049069978,
        0.880223858527217,
        0.9194074664229527,
        0.9407336087535061
      ],
      "excerpt": "Is perplexity a good measure for comparing language models and a useful \nindicator for their results on downstream tasks? \nThe previous experiments show, that language model pre-training does work very \nwell, even for domains with low data resources. Cotterell and Duh (2017) \nshowed that using CRF-based methods outperform traditional Bi-LSTM in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.915406801593408,
        0.8535952842439856,
        0.9042041843257236
      ],
      "excerpt": "using Bi-LSTMs in combination with pre-trained language models. Our experiments \nalso showed, that pre-trained language models can also help to improve \nperformance, even when no training data for the target domain is used \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8291055392510946,
        0.8015242507174344
      ],
      "excerpt": "The next table shows the averaged perplexity for all sentences in the test \ndataset for LFT for all pre-trained language models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9772137346405092,
        0.867300449144091,
        0.9828378180044904
      ],
      "excerpt": "In this paper we have studied the influence of using language model pre-training \nfor named entity recognition for Historic German. We achieve new \nstate-of-the-art results using carefully chosen training data for language \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8612483221136914
      ],
      "excerpt": "language model pre-training can be a strong competitor to CRF-only methods as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9021809892483094,
        0.9221956620903463,
        0.855199652504994,
        0.8415927189380078
      ],
      "excerpt": "We showed that language model pre-training can be more effective than using \ntransfer-learning with labeled datasets. \nFurthermore, we introduced a new language model pre-training objective, \nsynthetic masked language model pre-training (SMLM), that allows a transfer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8466318601288265
      ],
      "excerpt": "using only the same (character) vocabulary. Results showed that using SMLM can \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9619721610294456,
        0.9646498266531495
      ],
      "excerpt": "The next table shows the parameters that we used for training our language \nmodels. As our character-based language model relies on raw text, no \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9409270330703017
      ],
      "excerpt": "corpus for development data and another 1/500 for test data during the language \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.922619141400597,
        0.9795633690953134,
        0.8803052593097004
      ],
      "excerpt": "to train a language model with SMLM objective. We use different values of p \nin range of [80, 90, 95] for leaving the character unchanged in the SMLM \nobjective and found that p = 90 yields the best results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8211266844335577
      ],
      "excerpt": "recognition model with the Flair library: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9714803296783744,
        0.8836393513099227
      ],
      "excerpt": "We reduce the learning rate by a factor of 0.5 with a patience of 3. This \nfactor determines the number of epochs with no improvement after which learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8716831046523151,
        0.8595129626249621
      ],
      "excerpt": "for the feature-based approach does not work well. Thus, we perform a per-layer \nanalysis that trains a model with a specific layer from the multi-lingual BERT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9605490875019985
      ],
      "excerpt": "we visualize the performance for each layer of the BERT model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8214843124983549,
        0.909712228576169
      ],
      "excerpt": "We train all NER models with IOBES (Ratinov and Roth, 2009) \ntagging scheme. In the prediction step we convert IOBES tagging scheme to IOB, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8875232523621842,
        0.8067350164479065
      ],
      "excerpt": "For all NER models we train and evaluate 3 runs and report an averaged F1 score. \nWe briefly describe a few ideas we implemented that did not seem to be effective \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095118366781719
      ],
      "excerpt": "    with FastText embeddings trained on Wikipedia articles. On LFT this model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9871959572718345
      ],
      "excerpt": "    out-of-memory errors on our system with 32GB of RAM. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9818643278857634
      ],
      "excerpt": "    whole corpus) with a vocabulary size of 1,094,628 tokens both for the HH and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023121632836459,
        0.9912491586063067,
        0.9779996901137643,
        0.8427369925627878,
        0.9401737169500176,
        0.9189478040200496
      ],
      "excerpt": "    for training a NER model on both LFT and ONB. On LFT we achieved a F1 score \n    of 72.18%, which is 5.33% behind our new state-of-the-art result. On ONB we \n    achieved a F1 score of 75.72%, which is 9.59% behind our new \n    state-of-the-art result. We assume that training a ELMo Transformer model \n    for more epochs would lead to better results. \nMore information about obtaining and using the LFT and ONB datasets can be found \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9412355757926587
      ],
      "excerpt": "  --lms TEXT         Comma separated list of language models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8488774965944276
      ],
      "excerpt": "  --help             Show this message and exit. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897211425355702
      ],
      "excerpt": "crawl - Embeddings trained with FastText on Common Crawl data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8951853810323582
      ],
      "excerpt": "It is also possible to combine several embeddings as a comma separated list like \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8883453625946608
      ],
      "excerpt": "The following language models can be used with the --lms parameter: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8740297337834116
      ],
      "excerpt": "hamburger_anzeiger - Our trained Flair Embeddings on the Hamburger Anzeiger \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8740297337834116
      ],
      "excerpt": "wiener_zeitung - Our trained Flair Embeddings on the Wiener Zeitung corpus \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.914080541219296
      ],
      "excerpt": "tagging scheme into an IOB format that is supported by the CoNLL-2003 evaluation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8806052831554363
      ],
      "excerpt": "A previously trained NER model is passed to the --model option. The following \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8033735619428917
      ],
      "excerpt": "The following training command can be used to reproduce our results on the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9144739920296618,
        0.8399712398734608,
        0.9727763042424411,
        0.8526440646969138
      ],
      "excerpt": "We release models trained on the ONB (1710-1873) and the LFT (1926) datasets. \nAll models are trained with the latest version of Flair. \nNote: We use BPEmbeddings instead of the combination of Wikipedia, Common Crawl and character embeddings (as used in the paper), \nso save space and training/inferencing time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9662221319838792
      ],
      "excerpt": "Paper reported an averaged F1-score of 85.31. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9662221319838792,
        0.9708999525821798,
        0.9743019122942881,
        0.8633254049337296
      ],
      "excerpt": "Paper reported an averaged F1-score of 77.51. \n\u2020 denotes that this model is selected for upload. \nWe release our models on the Hugging Face model hub. The following models are available: \n| Model                                                  | Model hub page \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Repository for \"Towards Robust Named Entity Recognition for Historic German\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/stefan-it/historic-ner/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Tue, 21 Dec 2021 05:19:25 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dbmdz/historic-ner/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dbmdz/historic-ner",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8505815078469189
      ],
      "excerpt": "preprocessing steps like tokenization are needed. We use 1/500 of the complete \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8233742695517758
      ],
      "excerpt": "With the --number argument you should define a unique id for your experiment. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "| Training    | 1,605 |    0 | 182 | 2,674 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8383368907332598
      ],
      "excerpt": "| Test        |   221 |    0 |  16 |   355 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "| Training    | 3,998 |    2 | 2,293 | 4,009 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "| Test        |   441 |    1 |   324 |   506 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8001632351434107
      ],
      "excerpt": "| Dataset           | Configuration                                                  | Pre-trained LM | Pre-training data       | F-Score \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8483819940059046
      ],
      "excerpt": "model training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471632383263955
      ],
      "excerpt": "| Input sentence                                                                          | Output sentence \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9281909987746079,
        0.9459306480564178
      ],
      "excerpt": "$ python3 experiment_runner.py --help                                                                                                                                    [10:15:59] \nUsage: experiment_runner.py [OPTIONS] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8076322351023393
      ],
      "excerpt": "  --dataset TEXT     Define dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8358981660484872
      ],
      "excerpt": "CoNLL-2003 script. Our predict.py script loads a trained NER model and outputs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8185712620003271
      ],
      "excerpt": "The predict.py script comes with the following arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9185160430739052,
        0.9574426122765262
      ],
      "excerpt": "$ python3 predict.py --help \nUsage: predict.py [OPTIONS] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8076322351023393
      ],
      "excerpt": "  --dataset TEXT  Define dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9000565544500608
      ],
      "excerpt": "$ python3 predict.py --dataset onb --model resources/taggers/experiment_1_2/best-model.pt | perl conlleval.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826743382711377
      ],
      "excerpt": "$ python3 experiment_runner.py --number 1 --dataset onb --embeddings wikipedia,crawl,character --lms wiener_zeitung --runs 3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826743382711377
      ],
      "excerpt": "$ python3 experiment_runner.py --number 2 --dataset lft --embeddings wikipedia,crawl,character --lms hamburger_anzeiger --runs 3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826743382711377
      ],
      "excerpt": "$ python3 experiment_runner.py --number 1 --dataset onb --embeddings bpe --lms wiener_zeitung --runs 3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826743382711377
      ],
      "excerpt": "$ python3 experiment_runner.py --number 2 --dataset lft --embeddings bpe --lms hamburger_anzeiger --runs 3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8295251002028329
      ],
      "excerpt": "| Dataset \\ Run | Run 1 | Run 2 | Run 3\u2020    | Avg. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8295251002028329
      ],
      "excerpt": "| Dataset \\ Run | Run 1 | Run 2 | Run 3\u2020    | Avg. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8101031004495304
      ],
      "excerpt": "| Test          | 77.07 | 77.35 | 77.20     | 77.21 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dbmdz/historic-ner/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Stefan Schweter\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Towards Robust Named Entity Recognition for Historic German",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "historic-ner",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dbmdz",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dbmdz/historic-ner/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14,
      "date": "Tue, 21 Dec 2021 05:19:25 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "named-entity-recognition",
      "historic-german"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This sections shows how to use one of our trained models with Flair in order to\nperform NER on a sentence.\n\nThen you can use the following code to perform NER:\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n#: Noisy OCR :)\nsentence = Sentence(\"April Martin Ansclm , K. Gefan - gen-Auffehers Georg Sausgruber .\")\n\ntagger: SequenceTagger = SequenceTagger.load(\"dbmdz/flair-historic-ner-onb\")\ntagger.predict(sentence)\n\nsentence.to_tagged_string()\n```\n\nThis outputs:\n\n```python\n'April Martin <B-PER> Ansclm <E-PER> , K. Gefan - gen-Auffehers Georg <B-PER> Sausgruber <E-PER> .'\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}