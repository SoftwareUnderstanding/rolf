{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2105.03824",
      "https://arxiv.org/abs/1910.10683"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "* https://github.com/cccntu/transformers/compare/7736fad96135a7eff55932ce974b7be2a74fcb1d..54d12d5e88e5bc0da75f35c109a4424b1afcdf66 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cccntu/ft5-demo-space",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-17T11:14:13Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-11T17:15:48Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "FNet (https://arxiv.org/abs/2105.03824) is an attention-free model that is faster to train, and uses less memory. It does so by replaces self-attention with unparameterized Fourier Transform, but custom implementation is required to do causal (unidirectional) mask. In this project, we instead explore a hybrid approach. We use a encoder-decoder architecture, where the encoder is an attention-free, using Fourier Transform, and the decoder uses cross-attention and decoder self-attention.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9875473293649731,
        0.8974159157135868
      ],
      "excerpt": "This is a proof of concept for FT5, a novel hybrid model based on FNet and T5. The idea is first proposed on the huggingface forum. \nThe model architecture is based on T5 (https://arxiv.org/abs/1910.10683), except the encoder self-attention is replaced by fourier transform as in FNet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.8038175273654687
      ],
      "excerpt": "Data: OpenWebText (huggingface link) \nI chose this dataset because it's sufficiently large (pre-training iterates over the dataset ~4 times), but not too large (makes coding easier, so we could start training ealier), it's clean, and it's directly loadable on huggingface. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9655442953264163,
        0.9504374464054308,
        0.9905964078145204
      ],
      "excerpt": "Note: This is >2x faster than the original script on TPU. I found the preprocessing is the bottleneck because TPU is so fast, so I used PyTorch Dataloader to parallelize it. I also refactored training loop. \nIt is fine-tuned on CNN Dailymail Dataset (huggingface link). I chose this data because it's used in original T5 paper, so it can be directly compared. \nThe fine-tuning hyperparameter again follows T5, except it is manually ealry-stopped, since CNN/DM is a much smaller dataset and model overfits at a fraction of training time. The best checkpoint is selected by rouge2 on validation set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192078250339667,
        0.9944846479203655,
        0.8120694214893878,
        0.8938914495664974,
        0.8192078250339667,
        0.9944846479203655,
        0.9767526537684436,
        0.8871344143136075
      ],
      "excerpt": "The pre-trained model is here: flax-community/t5-base-openwebtext \nAnd the model fine-tuned on CNN/DM is here: flax-community/t5-base-cnn-dm \nPre-training takes 59 hours on TPUv3x8, with the same batch size, training steps, and optimizer as in T5 paper. \nIn T5 paper, fine-tuning uses half of the training step as pre-training, but the model already starts overfitting at 5 hours. \nThe pre-trained model is here: flax-community/ft5-base-openwebtext \nAnd the model fine-tuned on CNN/DM is here: flax-community/ft5-cnn-dm \nOur best (decided by vaidation rouge-2) checkpoints achieves rouge-2 of 18.61 (t5) and 16.5 (ft5) on the test set of CNN/DM. \nIt is lower than the numbers reported by T5 paper. We found 2 major difference that might be the cause. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Mirror of the huggingface demo space",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cccntu/ft5-demo-space/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 07:11:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cccntu/ft5-demo-space/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "cccntu/ft5-demo-space",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8918974083095406
      ],
      "excerpt": "* https://github.com/cccntu/transformers/compare/7736fad96135a7eff55932ce974b7be2a74fcb1d..54d12d5e88e5bc0da75f35c109a4424b1afcdf66 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cccntu/ft5-demo-space/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "FT5 News Summarizer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ft5-demo-space",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "cccntu",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cccntu/ft5-demo-space/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 07:11:19 GMT"
    },
    "technique": "GitHub API"
  }
}