{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2008.02217](https://arxiv.org/abs/2008.02217",
      "https://arxiv.org/abs/2008.02217",
      "https://arxiv.org/abs/1802.04712"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Hubert Ramsauer<sup>1</sup>, Bernhard Sch\u00e4fl<sup>1</sup>, Johannes Lehner<sup>1</sup>, Philipp Seidl<sup>1</sup>, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066,
        0.8356013927728488,
        0.9979639156924583,
        0.9354546553049569
      ],
      "excerpt": "Geir Kjetil Sandve<sup>4</sup>, Victor Greiff<sup>3</sup>, David Kreil<sup>2</sup>, Michael Kopp<sup>2</sup>, G\u00fcnter \nKlambauer<sup>1</sup>, Johannes Brandstetter<sup>1</sup>, Sepp Hochreiter<sup>1, 2</sup> \n<sup>1</sup> ELLIS Unit Linz and LIT AI Lab, Institute for Machine Learning, Johannes Kepler University Linz, Austria \n<sup>2</sup> Institute of Advanced Research in Artificial Intelligence (IARAI) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ml-jku/hopfield-layers",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-09T10:26:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T10:31:56Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9968029537584643,
        0.9968029537584643,
        0.9341404091370336,
        0.9778821681682958
      ],
      "excerpt": "<sup>3</sup> Department of Immunology, University of Oslo, Norway \n<sup>4</sup> Department of Informatics, University of Oslo, Norway \nThe transformer and BERT models pushed the performance on NLP tasks to new levels via their attention mechanism. We show \nthat this attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9674277877997751
      ],
      "excerpt": "The new Hopfield network has three types of energy minima (fixed points of the update): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9277256231359781
      ],
      "excerpt": "metastable states averaging over a subset of patterns, and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.930885295816764
      ],
      "excerpt": "Transformers learn an attention mechanism by constructing an embedding of patterns and queries into an associative \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9574461740744441,
        0.8480487352227886,
        0.988832771147746,
        0.9359423660499498
      ],
      "excerpt": "operate in higher layers in metastable states. The gradient in transformers is maximal in the regime of metastable \nstates, is uniformly distributed when averaging globally, and vanishes when a fixed point is near a stored pattern. \nBased on the Hopfield network interpretation, we analyzed learning of transformer and BERT architectures. Learning \nstarts with attention heads that average and then most of them switch to metastable states. However, the majority of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8936656824978653,
        0.8540891168232513,
        0.9191915171123828,
        0.9077337494346966,
        0.9937802560841261,
        0.9356780824567792
      ],
      "excerpt": "propose. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information \ncreated in lower layers. These heads seem a promising target for improving transformers. Neural networks that integrate \nHopfield networks that are equivalent to attention heads outperform other methods on immune repertoire classification, \nwhere the Hopfield net stores several hundreds of thousands of patterns. \nWith this repository, we provide a PyTorch implementation of a new layer called \u201cHopfield\u201d which allows to equip deep \nlearning architectures with Hopfield networks as new memory concepts. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.990406441367091,
        0.9473951303338858,
        0.8076624559891429,
        0.9911617005259707,
        0.9968029537584643
      ],
      "excerpt": "Some implementations of this repository are based on existing ones of the \nofficial PyTorch repository v1.6.0 and accordingly extended and \nmodified. In the following, the involved parts are listed: \nThe implementation of HopfieldCore is based on the implementation \n  of MultiheadAttention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9911617005259707,
        0.9968029537584643
      ],
      "excerpt": "The implementation of hopfield_core_forward is based on the implementation \n  of multi_head_attention_forward \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9911617005259707,
        0.9968029537584643
      ],
      "excerpt": "The implementation of HopfieldEncoderLayer is based on the implementation \n  of TransformerEncoderLayer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9911617005259707,
        0.9968029537584643
      ],
      "excerpt": "The implementation of HopfieldDecoderLayer is based on the implementation \n  of TransformerDecoderLayer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Hopfield Networks is All You Need",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ml-jku/hopfield-layers/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 106,
      "date": "Wed, 29 Dec 2021 20:14:08 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ml-jku/hopfield-layers/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ml-jku/hopfield-layers",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ml-jku/hopfield-layers/master/examples/latch_sequence/latch_sequence_demo.ipynb",
      "https://raw.githubusercontent.com/ml-jku/hopfield-layers/master/examples/bit_pattern/bit_pattern_demo.ipynb",
      "https://raw.githubusercontent.com/ml-jku/hopfield-layers/master/examples/mnist_bags/mnist_bags_demo.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The recommended way to install the software is to use `pip/pip3`:\n\n```bash\n$ pip3 install git+https://github.com/ml-jku/hopfield-layers\n```\n\nTo successfully run the [Jupyter notebooks](https://jupyter.org) contained in [examples](examples/), additional\nthird-party modules are needed:\n\n```bash\n$ pip3 install -r examples/requirements.txt\n```\n\nThe installation of the [Jupyter software](https://jupyter.org/install.html) itself is not covered. More details on how\nto install Jupyter are available at the [official installation page](https://jupyter.org/install.html).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8425245751769771
      ],
      "excerpt": "network can store exponentially (with the dimension) many patterns,converges with one update, and has exponentially \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ml-jku/hopfield-layers/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/ml-jku/hopfield-layers/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Maximilian Ilse and Jakub Tomczak\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Hopfield Networks is All You Need",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "hopfield-layers",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ml-jku",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ml-jku/hopfield-layers/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The software was developed and tested on the following 64-bit operating systems:\n\n- CentOS Linux release 8.1.1911 (Core)\n- macOS 10.15.5 (Catalina)\n\nAs the development environment, [Python](https://www.python.org) 3.8.3 in combination\nwith [PyTorch](https://pytorch.org) 1.6.0 was used (a version of at least 1.5.0 should be sufficient). More details on\nhow to install PyTorch are available on the [official project page](https://pytorch.org).\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1244,
      "date": "Wed, 29 Dec 2021 20:14:08 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To get up and running with Hopfield-based networks, only <i>one</i> argument needs to be set, the size (depth) of the\ninput.\n\n```python\nfrom hflayers import Hopfield\n\nhopfield = Hopfield(input_size=...)\n```\n\nIt is also possible to replace commonly used pooling functions with a Hopfield-based one. Internally, a <i>state\npattern</i> is trained, which in turn is used to compute pooling weights with respect to the input.\n\n```python\nfrom hflayers import HopfieldPooling\n\nhopfield_pooling = HopfieldPooling(input_size=...)\n```\n\nA second variant of our Hopfield-based modules is one which employs a trainable but fixed lookup mechanism. Internally,\none or multiple <i>stored patterns</i> and <i>pattern projections</i> are trained (optionally in a non-shared manner),\nwhich in turn are used as a lookup mechanism independent of the input data.\n\n```python\nfrom hflayers import HopfieldLayer\n\nhopfield_lookup = HopfieldLayer(input_size=...)\n```\n\nThe usage is as <i>simple</i> as with the main module, but equally <i>powerful</i>.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Generally, the Hopfield layer is designed to be used to implement or to substitute different layers like:\n\n- <b>Pooling layers:</b> We consider the Hopfield layer as a pooling layer if only one static state (query) pattern\n  exists. Then, it is de facto a pooling over the sequence, which results from the softmax values applied on the stored\n  patterns. Therefore, our Hopfield layer can act as a pooling layer.\n\n- <b>Permutation equivariant layers:</b> Our Hopfield layer can be used as a plug-in replacement for permutation\n  equivariant layers. Since the Hopfield layer is an associative memory it assumes no dependency between the input\n  patterns.\n\n- <b>GRU & LSTM layers:</b> Our Hopfield layer can be used as a plug-in replacement for GRU & LSTM layers. Optionally,\n  for substituting GRU & LSTM layers, positional encoding might be considered.\n\n- <b>Attention layers:</b>  Our Hopfield layer can act as an attention layer, where state (query) and stored (key)\n  patterns are different, and need to be associated.\n\nThe folder [examples](examples/) contains multiple demonstrations on how to use the <code>Hopfield</code>, <code>\nHopfieldPooling</code> as well as the <code>HopfieldLayer</code> modules. To successfully run the\ncontained [Jupyter notebooks](https://jupyter.org), additional third-party modules\nlike [pandas](https://pandas.pydata.org) and [seaborn](https://seaborn.pydata.org) are required.\n\n- [Bit Pattern Set](examples/bit_pattern/bit_pattern_demo.ipynb): The dataset of this demonstration falls into the\n  category of <i>binary classification</i> tasks in the domain of <i>Multiple Instance Learning (MIL)</i> problems. Each\n  bag comprises a collection of bit pattern instances, wheres each instance is a sequence of <b>0s</b> and <b>1s</b>.\n  The positive class has specific bit patterns injected, which are absent in the negative one. This demonstration shows,\n  that <code>Hopfield</code>, <code>HopfieldPooling</code> and <code>HopfieldLayer</code> are capable of learning and\n  filtering each bag with respect to the class-defining bit patterns.\n\n- [Latch Sequence Set](examples/latch_sequence/latch_sequence_demo.ipynb): We study an easy example of learning\n  long-term dependencies by using a simple <i>latch task</i>,\n  see [Hochreiter and Mozer](https://link.springer.com/chapter/10.1007/3-540-44668-0_92). The essence of this task is\n  that a sequence of inputs is presented, beginning with one of two symbols, <b>A</b> or <b>B</b>, and after a variable\n  number of time steps, the model has to output a corresponding symbol. Thus, the task requires memorizing the original\n  input over time. It has to be noted, that both class-defining symbols must only appear at the first position of a\n  sequence. This task was specifically designed to demonstrate the capability of recurrent neural networks to capture\n  long term dependencies. This demonstration shows, that <code>Hopfield</code>, <code>HopfieldPooling</code> and <code>\n  HopfieldLayer</code> adapt extremely fast to this specific task, concentrating only on the first entry of the\n  sequence.\n\n- [Attention-based Deep Multiple Instance Learning](examples/mnist_bags/mnist_bags_demo.ipynb): The dataset of this\n  demonstration falls into the category of <i>binary classification</i> tasks in the domain of <i>Multiple Instance\n  Learning (MIL)</i> problems, see [Ilse and Tomczak](https://arxiv.org/abs/1802.04712). Each bag comprises a collection\n  of <b>28x28</b> grayscale images/instances, whereas each instance is a sequence of pixel values in the range\n  of <b>[0; 255]</b>. The amount of instances per pag is drawn from a Gaussian with specified mean and variance. The\n  positive class is defined by the presence of the target number/digit, whereas the negative one by its absence.\n\n",
      "technique": "Header extraction"
    }
  ]
}