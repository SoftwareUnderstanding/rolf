{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1512.03385",
      "https://arxiv.org/abs/1512.03385"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Credits for the dataset slice go to [@mrdbourke](https://github.com/mrdbourke), as he nicely provided \nme the information via Twitter DM.\n\nCredits for the tips on how to serve a PyTorch transfer learning model using TorchServe go to \n[@prashantsail](https://github.com/prashantsail) as he properly explained in \n[this comment](https://github.com/pytorch/serve/issues/620#issuecomment-674971664).\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8894750347818047
      ],
      "excerpt": ":computer: Credits \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998369197675008
      ],
      "excerpt": "    `\"Deep Residual Learning for Image Recognition\" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9180029447729768
      ],
      "excerpt": "        super(ImageClassifier, self).init(BasicBlock, [2,2,2,2], num_classes=10) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alvarobartt/serving-pytorch-models",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-01T19:02:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-21T08:13:36Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9366809747477394,
        0.9731280670911897
      ],
      "excerpt": "TorchServe is the ML model serving framework developed by PyTorch. \nAlong this repository, the procedure so as to train and deploy a transfer learning CNN model using  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9175110961979623
      ],
      "excerpt": "WARNING: TorchServe is experimental and subject to change. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9428172625245376,
        0.9111530572479973,
        0.9279696514759228
      ],
      "excerpt": "The dataset that is going to be used to train the image classification model is  \nFood101, but not the complete version of it, \njust a slice of 10 classes, which is more or less the 10% of the dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9353930370242307
      ],
      "excerpt": "were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8528505516348692,
        0.9156517852813427,
        0.9182097936404883
      ],
      "excerpt": "of 512 pixels. \nWe will proceed with a transfer learning approach using ResNet as its backbone \nwith a pre-trained set of weights trained on ImageNet, as it is the SOTA when it  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8932016162687453,
        0.949184040247569,
        0.972004430169571,
        0.8352488708839071,
        0.9521372697411428
      ],
      "excerpt": "In this case, as we want to serve a PyTorch model, we will be using  \nPyTorch's implementation of ResNet \nand more concretely, ResNet18, where the 18 stands for the number of layers that it contains. \nAs we are going to use transfer learning from a pre-trained PyTorch model, we will load the ResNet18 model \nand freeze it's weights using the following piece of code: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9375518326921721
      ],
      "excerpt": "for param in model.parameters(): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9051023316457153,
        0.922599117390614,
        0.8860229508299814
      ],
      "excerpt": "Once loaded, we need to update the fc layer, which stands for fully connected and it's the last  \nlayer of the model, and over the one that the weights will be calculated to optimize the network  \nfor our dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8640898813428294
      ],
      "excerpt": "    nn.Linear(model.fc.in_features, 128), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model.fc = sequential_layer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693994133093632
      ],
      "excerpt": "Note: for more details regarding the model training process, feel free to check it at  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9202982417663031
      ],
      "excerpt": "the pre-trained set of weights, with the following piece of code: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8892624151618805,
        0.845093591802531
      ],
      "excerpt": "weights is being loaded into that architecture, which means that the keys should match between the model and the weights. \nAs we used transfer learning from a pre-trained model and we just modified the last fully connected layer (fc), we need to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8911708811541078,
        0.9381524790020943
      ],
      "excerpt": "and for the rest of the PyTorch pre-trained models at torchvision/models. \nThe code for the ResNet18 model looks like: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9047266689400563
      ],
      "excerpt": "Which translated to our model file it should look like: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9122237156631903,
        0.9156101615830372,
        0.9135703269232208,
        0.9428558218193438,
        0.9043197744282916,
        0.9075794610308022
      ],
      "excerpt": "that file. We then need to initialize that class with our architecture, which in this case is the same one as the ResNet18, \nincluding the BasicBlock, specifying the ResNet18 layers [2,2,2,2] and then we modify the number of classes, which for  \nour case is 10 as we previously mentioned. \nFinally, so as to make the state_dict match with the model class, we need to override the self.fc layer, which is the last \nlayer of the network. As we use that sequential layer while training the model, the final weights have been optimized for our \ndataset over that layer, so just overriding it we will get the model's architecture with our modifications. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8637506947983816
      ],
      "excerpt": "load the weights using the following piece of code: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = ImageClassifier() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8763275589224362
      ],
      "excerpt": "a GPU (or more) available or not and, if so, which is the name of that device depending on its ID if there's more than  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9093097848575896,
        0.8840648628884973
      ],
      "excerpt": "as described in the section above. \nFirst of all you will need to generate the MAR file, which is the \"ready to serve\" archive of the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8092942499987675
      ],
      "excerpt": "So torch-model-archiver's used flags stand for: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8519049298668383
      ],
      "excerpt": "--handler: the Python file which defines the data preprocessing, inference and postprocessing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8304527957777789,
        0.8273939985655104
      ],
      "excerpt": "the relationships between the IDs (model's target) and the labels/names and/or also additional files  \nrequired by the model-file to format the output data in a cleaner way. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8244933767691379,
        0.8650301293289415
      ],
      "excerpt": "More information regarding torch-model-archiver available at  \nTorch Model Archiver for TorchServe. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9796396884552856,
        0.8359859425827986
      ],
      "excerpt": "of a pre-trained PyTorch model as a MAR file, starts with the deployment of the TorchServe REST APIs, which are the \nInference API, Management API and Metrics API, deployed by default on localhost (of if you prefer 127.0.0.1) in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9244394662457398
      ],
      "excerpt": "So on, the command to deploy the current MAR model stored under deployment/model-store/ is the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.942227953646636,
        0.975822937918494
      ],
      "excerpt": "--models: is(are) the name(s) of the model(s) that will be served on the startup, including both an alias  \nwhich will be the API endpoint of that concrete model and the filename of that model, with format endpoint=model_name.mar. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8812300017670085
      ],
      "excerpt": "In order to check the availability of the deployed TorchServe API, you can just send a HTTP GET \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9343132774825443,
        0.8182181210705769
      ],
      "excerpt": "bit more of time depending on your machine specs. \nIn order to reproduce the TorchServe deployment in an Ubuntu Docker image, you should just use the following set of commands: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Serving PyTorch models with TorchServe :fire:",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alvarobartt/serving-pytorch-models/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Sun, 26 Dec 2021 01:10:57 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alvarobartt/serving-pytorch-models/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "alvarobartt/serving-pytorch-models",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/alvarobartt/serving-pytorch-models/master/deployment/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/alvarobartt/serving-pytorch-models/master/notebooks/transfer-learning.ipynb",
      "https://raw.githubusercontent.com/alvarobartt/serving-pytorch-models/master/notebooks/data-overview.ipynb",
      "https://raw.githubusercontent.com/alvarobartt/serving-pytorch-models/master/notebooks/create-sanity-data.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/alvarobartt/serving-pytorch-models/master/deployment/dockerd-entrypoint.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8686053048202554,
        0.9431210945663379
      ],
      "excerpt": "Note that this is the English version, for the Spanish version please read README-es.md. \n:hammer_and_wrench: Requirements \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8246843758119126
      ],
      "excerpt": "Once the state_dict has been generated from the pre-trained model, you need to make sure that it can be loaded properly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8611816570082682
      ],
      "excerpt": "You can find more Image Classification pre-trained PyTorch models at  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9844077467155781,
        0.9456236151648941,
        0.861492866282933
      ],
      "excerpt": "Note: the model has been trained on a NVIDIA GeForce GTX 1070 8GB GPU using CUDA 11. If you want to get you GPU specs, just \nuse the nvidia-smi command on your console, but make sure that you have your NVIDIA drivers properly installed. So as  \nto check whether PyTorch is using the GPU you can just use the following piece of code which will tell you whether there's \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906285630647157
      ],
      "excerpt": "one GPU. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9344812000444824
      ],
      "excerpt": "In order to deploy the model you will need to reproduce the following steps once you installed all the requirements \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9717106327039013
      ],
      "excerpt": "                     --version 1.0 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8893782113890794
      ],
      "excerpt": "--version: it's optional even though it's a nice practice to include the version of the models  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8202385041490451
      ],
      "excerpt": "Note: you can define custom handlers, but you don't need to as there are already some default handlers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8031328138588875
      ],
      "excerpt": "--ncs: means that you want to disable the snapshot feature (optional). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8581472309294128
      ],
      "excerpt": "If everything goes as expected, it should output the following response: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8381230100096396
      ],
      "excerpt": "you did run the TorchServe deployment or from the logs/ directory that is created automatically while deploying TorchServe from \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9470440940672663
      ],
      "excerpt": "following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8169821131114681
      ],
      "excerpt": "Then the next time you deploy TorchServe, it will take less time than the first one if the models to be server were already \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8587476985249702
      ],
      "excerpt": ":mage_man: Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8706954550897273
      ],
      "excerpt": "reviewed test images are provided as well as 750 training images. On purpose, the training images  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8900486270063179,
        0.913449763121086
      ],
      "excerpt": "from torchvision import models \nmodel = models.resnet18(pretrained=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8002845373788898
      ],
      "excerpt": "split as 80%-20% for training and validation, respectively. And tested over the TEST dataset  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8368764281806401,
        0.8754979896301536
      ],
      "excerpt": "        pretrained (bool): If True, returns a model pre-trained on ImageNet \n        progress (bool): If True, displays a progress bar of the download to stderr \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401851861682346
      ],
      "excerpt": "from torchvision.models.resnet import ResNet, BasicBlock \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8889363224962027,
        0.8977206042100564,
        0.91892912920148
      ],
      "excerpt": "                     --model-file model/model.py \\ \n                     --serialized-file model/foodnet_resnet18.pth \\ \n                     --handler model/handler.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8134144487476591
      ],
      "excerpt": "--model-name: name that the generated MAR \"ready to serve\" file will have. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8049646985025397
      ],
      "excerpt": "torchserve --start --ncs --ts-config deployment/config.properties --model-store deployment/model-store \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8291759405256358
      ],
      "excerpt": "registered/loaded, as TorchServe keeps them cached under a /tmp directory so it won't need to load them again if neither the name nor  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.881173410293818
      ],
      "excerpt": "docker run --rm --name torchserve_docker \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alvarobartt/serving-pytorch-models/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Dockerfile",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Alvaro Bartolome del Canto, @alvarobartt at GitHub\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Serving PyTorch models with TorchServe :fire:",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "serving-pytorch-models",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "alvarobartt",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alvarobartt/serving-pytorch-models/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "First of all you will need to make sure that you have Java JDK 11 installed, as it is\nrequired by `torchserve` while deploying the model since it is exposing the APIs using Java.\n\n```bash\nsudo apt install --no-install-recommends -y openjdk-11-jre-headless\n```\n\nThen you can proceed with the installation of the PyTorch Python packages required for \nboth training and serving the model. \n\n```bash\npip install torch==1.7.0 torchvision==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\npip install torchserve==0.2.0 torch-model-archiver==0.2.0\n```\n\nOr you can also install them from the `requirements.txt` file as it follows:\n\n```bash\npip install -r requirements.txt\n```\n\nIf you have any problems regarding the PyTorch installation, visit \n[PyTorch - Get Started Locally](https://pytorch.org/get-started/locally/)\n\n---\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 76,
      "date": "Sun, 26 Dec 2021 01:10:57 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "torchserve",
      "pytorch",
      "model-serving",
      "pytorch-cnn",
      "model-deployment",
      "machine-learning",
      "mlops",
      "image-classification",
      "serve-pytorch"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Once you completed all the steps above, you can send a sample request to the deployed model so as to see its performance\nand make the inference. In this case, as the problem we are facing is an image classification problem, we will use a sample\nimage as the one provided below and then send it as a file on the HTTP request's body as it follows:\n\n```bash\nwget https://raw.githubusercontent.com/alvarobartt/pytorch-model-serving/master/images/sample.jpg\ncurl -X POST http://localhost:8080/predictions/foodnet -T sample.jpg\n```\n\nWhich should output something similar to:\n\n```json\n{\n  \"hamburger\": 0.6911126375198364,\n  \"grilled_salmon\": 0.11039528995752335,\n  \"pizza\": 0.039219316095113754,\n  \"steak\": 0.03642556071281433,\n  \"chicken_curry\": 0.03306535258889198,\n  \"sushi\": 0.028345594182610512,\n  \"chicken_wings\": 0.027532529085874557,\n  \"fried_rice\": 0.01296720840036869,\n  \"ice_cream\": 0.012180349789559841,\n  \"ramen\": 0.008756187744438648\n}\n```\n\n__Remember__: that the original inference's output is the dict with the identifier of each class, not the class names,\nin this case as we included `index_to_name.json` as an extra-file while creating the MAR, TorchServe is automatically \nassigning the identifiers with the class names so that the prediction is clearer.\n\n  ---\n\nThe commands above translated into Python code looks like:\n\n```python\n#: Download a sample image from the available samples at alvarobartt/pytorch-model-serving/images\nimport urllib\nurl, filename = (\"https://raw.githubusercontent.com/alvarobartt/pytorch-model-serving/master/images/sample.jpg\", \"sample.jpg\")\ntry: urllib.URLopener().retrieve(url, filename)\nexcept: urllib.request.urlretrieve(url, filename)\n\n#: Transform the input image into a bytes object\nimport cv2\nfrom PIL import Image\nfrom io import BytesIO\n\nimage = Image.fromarray(cv2.imread(filename))\nimage2bytes = BytesIO()\nimage.save(image2bytes, format=\"PNG\")\nimage2bytes.seek(0)\nimage_as_bytes = image2bytes.read()\n\n#: Send the HTTP POST request to TorchServe\nimport requests\n\nreq = requests.post(\"http://localhost:8080/predictions/foodnet\", data=image_as_bytes)\nif req.status_code == 200: res = req.json()\n```\n\n__Note__: that to execute the sample piece of code above you will need more requirements than the ones specified in the\n[Requirements section](#hammer_and_wrench-requirements) so just run the following command so as to install them:\n\n  ```bash\n  pip install opencv-python pillow requests --upgrade\n  ```\n\n---\n\n",
      "technique": "Header extraction"
    }
  ]
}