{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.05150},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  archivePrefix = {arXiv},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  eprint    = {2004.05150},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  timestamp = {Tue, 14 Apr 2020 16:40:34 +0200},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  bibsource = {dblp computer science bibliography, https://dblp.org}<br/>\n}\n\n@article{DBLP:journals/corr/abs-1912-08777,<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  author    = {Jingqing Zhang and\n               Yao Zhao and\n               Mohammad Saleh and\n               Peter J. Liu},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  title     = {{PEGASUS:} Pre-training with Extracted Gap-sentences for Abstractive\n               Summarization},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  journal   = {CoRR},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  volume    = {abs/1912.08777},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  year      = {2019},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  url       = {http://arxiv.org/abs/1912.08777},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  archivePrefix = {arXiv},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  eprint    = {1912.08777},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  timestamp = {Fri, 03 Jan 2020 16:10:45 +0100},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-08777.bib},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  bibsource = {dblp computer science bibliography, https://dblp.org}<br/>\n}"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "@article{DBLP:journals/corr/abs-1910-03771,<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  author    = {Thomas Wolf and\n               Lysandre Debut and\n               Victor Sanh and\n               Julien Chaumond and\n               Clement Delangue and\n               Anthony Moi and\n               Pierric Cistac and\n               Tim Rault and\n               R\u00e9mi Louf and\n               Morgan Funtowicz and\n               Jamie Brew},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  title     = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  journal   = {CoRR},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  volume    = {abs/1910.03771},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  year      = {2019},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  url       = {http://arxiv.org/abs/1910.03771},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  archivePrefix = {arXiv},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  eprint    = {1910.03771},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  timestamp = {Tue, 02 Jun 2020 12:49:01 +0200},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-03771.bib},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  bibsource = {dblp computer science bibliography, https://dblp.org}<br/>\n}\n\n@article{DBLP:journals/corr/abs-2004-05150,<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  author    = {Iz Beltagy and\n               Matthew E. Peters and\n               Arman Cohan},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  title     = {Longformer: The Long-Document Transformer},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  journal   = {CoRR},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  volume    = {abs/2004.05150},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  year      = {2020},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  url       = {https://arxiv.org/abs/2004.05150},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  archivePrefix = {arXiv},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  eprint    = {2004.05150},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  timestamp = {Tue, 14 Apr 2020 16:40:34 +0200},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  bibsource = {dblp computer science bibliography, https://dblp.org}<br/>\n}\n\n@article{DBLP:journals/corr/abs-1912-08777,<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  author    = {Jingqing Zhang and\n               Yao Zhao and\n               Mohammad Saleh and\n               Peter J. Liu},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  title     = {{PEGASUS:} Pre-training with Extracted Gap-sentences for Abstractive\n               Summarization},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  journal   = {CoRR},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  volume    = {abs/1912.08777},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  year      = {2019},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  url       = {http://arxiv.org/abs/1912.08777},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  archivePrefix = {arXiv},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  eprint    = {1912.08777},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  timestamp = {Fri, 03 Jan 2020 16:10:45 +0100},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-08777.bib},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  bibsource = {dblp computer science bibliography, https://dblp.org}<br/>\n}\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DBLP:journals/corr/abs-1912-08777,<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  author    = {Jingqing Zhang and\n               Yao Zhao and\n               Mohammad Saleh and\n               Peter J. Liu},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  title     = {{PEGASUS:} Pre-training with Extracted Gap-sentences for Abstractive\n               Summarization},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  journal   = {CoRR},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  volume    = {abs/1912.08777},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  year      = {2019},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  url       = {http://arxiv.org/abs/1912.08777},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  archivePrefix = {arXiv},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  eprint    = {1912.08777},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  timestamp = {Fri, 03 Jan 2020 16:10:45 +0100},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-08777.bib},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  bibsource = {dblp computer science bibliography, https://dblp.org}<br/>\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DBLP:journals/corr/abs-2004-05150,<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  author    = {Iz Beltagy and\n               Matthew E. Peters and\n               Arman Cohan},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  title     = {Longformer: The Long-Document Transformer},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  journal   = {CoRR},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  volume    = {abs/2004.05150},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  year      = {2020},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  url       = {https://arxiv.org/abs/2004.05150},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  archivePrefix = {arXiv},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  eprint    = {2004.05150},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  timestamp = {Tue, 14 Apr 2020 16:40:34 +0200},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  bibsource = {dblp computer science bibliography, https://dblp.org}<br/>\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DBLP:journals/corr/abs-1910-03771,<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  author    = {Thomas Wolf and\n               Lysandre Debut and\n               Victor Sanh and\n               Julien Chaumond and\n               Clement Delangue and\n               Anthony Moi and\n               Pierric Cistac and\n               Tim Rault and\n               R\u00e9mi Louf and\n               Morgan Funtowicz and\n               Jamie Brew},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  title     = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  journal   = {CoRR},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  volume    = {abs/1910.03771},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  year      = {2019},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  url       = {http://arxiv.org/abs/1910.03771},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  archivePrefix = {arXiv},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  eprint    = {1910.03771},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  timestamp = {Tue, 02 Jun 2020 12:49:01 +0200},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-03771.bib},<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;  bibsource = {dblp computer science bibliography, https://dblp.org}<br/>\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amoramine/Pegasus_with_Longformer_summarization",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-08T09:55:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T04:11:41Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Pegasus is a large Transformer-based encoder-decoder model with a new pre-training objective which is adapted to abstractive summarization. More specifically, the pre-training objective, called \"Gap Sentence Generation (GSG)\", consists of masking important sentences from a document and generating these gap-sentences.\n\nOn the other hand, the Longformer is a Transformer which replaces the full-attention mechanism (quadratic dependency) with a novel attention mechanism which scale linearly with the input sequence length. Consequently, Longformer can process sequences up to 4,096 tokens long (8 times longer than BERT which is limited to 512 tokens).\n\nThis project plugs Longformer's attention mechanism to Pegasus in order to perform abstractive summarization on long documents. The conversion is done in loading_scripts/Pegasus_to_4k.py which enables Pegasus to process sequences up to 4,096 tokens long (rather than 512 tokens). Note that the `max_pos` parameter can be changed to accept even longer sequences (e.g `max_pos=16384`). The new Pegasus model is then fine-tuned on BigPatent dataset. To assess the model's performance on long documents, all training examples are filtered such that they have a minimum length of 4000 tokens.\n \nThis project was built using HuggingFace's Transformers library. The model is trained using model partitioning (with fairscale) and parallel batch processing on a cluster of 8 GPUs.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amoramine/Pegasus_with_Longformer_summarization/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 19:15:55 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/amoramine/Pegasus_with_Longformer_summarization/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "amoramine/Pegasus_with_Longformer_summarization",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/amoramine/Pegasus_with_Longformer_summarization/main/tune.sh"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/amoramine/Pegasus_with_Longformer_summarization/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Pegasus_with_Longformer_summarization",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Pegasus_with_Longformer_summarization",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "amoramine",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amoramine/Pegasus_with_Longformer_summarization/blob/main/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run this project, clone the repo and execute the following commands:\n\n1) `cd Pegasus_with_Longformer_summarization`\n2) `pip install -r requirements.txt`\n3) `pip install git+https://github.com/allenai/longformer.git`\n4) `pip install tokenizers==0.10.3`\n5) Comment out `import 'SAVE_STATE_WARNING' from torch.optim.lr_scheduler` in lib/python3.7/site-packages/transformers/trainer_pt_utils.py\n6) Add `with torch.no_grad():` above `out[:, 0 : dim // 2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))` in lib/python3.7/site-packages/transformers/modeling_bart.py\n7) `python loading_scripts/pegasus_to_4k.py`\n8) `git clone -b v4.5.1-release https://github.com/huggingface/transformers`\n9) `cd transformers`\n10) `pip install -e .` \n11) `cd .. ; python download_long_Big_Patent_data.py` \n12) `bash tune.sh`\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 13,
      "date": "Thu, 23 Dec 2021 19:15:55 GMT"
    },
    "technique": "GitHub API"
  }
}