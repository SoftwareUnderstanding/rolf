{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762 Attention Is All You Need\n\n[2]: https://arxiv.org/abs/1807.03819 Universal Transformers\n\n[3]: https://arxiv.org/abs/1810.04805 BERT: Pre-training of Deep Bidirectional Transformers for\n\n\n###  \n### Important Note: \n#### Note: Not everything in this repository was made by myself. Parts of the code here and there were found online. \n#### I tried to give credict whenever this happened but mistakes might still happen!. \n#### If by any chance some authors didn't get their credit and you know it: PM ME! I will fix it ASAP!\n#### Cheers ^^ \n",
      "https://arxiv.org/abs/1807.03819 Universal Transformers\n\n[3]: https://arxiv.org/abs/1810.04805 BERT: Pre-training of Deep Bidirectional Transformers for\n\n\n###  \n### Important Note: \n#### Note: Not everything in this repository was made by myself. Parts of the code here and there were found online. \n#### I tried to give credict whenever this happened but mistakes might still happen!. \n#### If by any chance some authors didn't get their credit and you know it: PM ME! I will fix it ASAP!\n#### Cheers ^^ \n",
      "https://arxiv.org/abs/1810.04805 BERT: Pre-training of Deep Bidirectional Transformers for\n\n\n###  \n### Important Note: \n#### Note: Not everything in this repository was made by myself. Parts of the code here and there were found online. \n#### I tried to give credict whenever this happened but mistakes might still happen!. \n#### If by any chance some authors didn't get their credit and you know it: PM ME! I will fix it ASAP!\n#### Cheers ^^ \n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1]: https://arxiv.org/abs/1706.03762 Attention Is All You Need\n\n[2]: https://arxiv.org/abs/1807.03819 Universal Transformers\n\n[3]: https://arxiv.org/abs/1810.04805 BERT: Pre-training of Deep Bidirectional Transformers for\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ypeleg/MinimalIsAllYouNeed",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-01T17:24:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-11T07:23:58Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    model = Bert() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.8962151653227748
      ],
      "excerpt": "    model.fit(X, Y) \nHave you ever wanted to work with transformers but just got drowned in ocean of models where none just did what you wanted? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8023945419833299,
        0.8720078544266223
      ],
      "excerpt": "Introducing \"Minimal Is All You Need\": Minimal Is All You Need is a Python library implementing nuts and bolts, for building Transformers models using Keras. \nThe library supports: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "* positional encoding and embeddings, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    model = Bert() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    model = XLNet() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    model = GPT_2() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    model = ELMo() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    model = TransformerXL() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    model = GPT() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    model = Transformer() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8260108963343304
      ],
      "excerpt": "It also allows you to piece together a multi-step Transformer model in a flexible way, for example: (Credit: Zhao HG) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9569745677719036,
        0.9109770294488916,
        0.8440175491925249
      ],
      "excerpt": "The (Universal) Transformer is a deep learning architecture described in arguably one of the most impressive DL papers of 2017 and 2018: \nthe \"[Attention is all you need][1]\" and the \"[Universal Transformers][2]\" \nby Google Research and Google Brain teams. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8466753518144876,
        0.923619930245938
      ],
      "excerpt": "which has inspired a big wave of new research models that keep coming ever since. \nThese models demonstrate new state-of-the-art results in various NLP tasks, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "<In Development> Transformers for Keras that support sklearn's .fit .predict .",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ypeleg/MinimalIsAllYouNeed/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Sat, 25 Dec 2021 18:29:22 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ypeleg/MinimalIsAllYouNeed/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ypeleg/MinimalIsAllYouNeed",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "add_coordinate_embedding = TransformerCoordinateEmbedding(transformer_depth, name='coordinate_embedding') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8886423005898035
      ],
      "excerpt": "you can build your version of Transformer, by re-arranging them differently or replacing some of them. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.912631828322746,
        0.8421074476017179,
        0.8789462919902599
      ],
      "excerpt": "transformer_block = TransformerBlock( name='transformer', num_heads=8, residual_dropout=0.1, attention_dropout=0.1, use_masking=True) \nadd_coordinate_embedding = TransformerCoordinateEmbedding(transformer_depth, name='coordinate_embedding') \noutput = transformer_input #: shape: (<batch size>, <sequence length>, <input size>) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ypeleg/MinimalIsAllYouNeed/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Minimal Is All You Need",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "MinimalIsAllYouNeed",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ypeleg",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ypeleg/MinimalIsAllYouNeed/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 25,
      "date": "Sat, 25 Dec 2021 18:29:22 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n    from minimal_is_all_you_need import Bert, GPT_2, XLNet, ELMo, GPT,  Transformer, TransformerXL, the_loss_of_bert\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}