{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The goals / steps of this project are the following:\n* Load the data set (see below for links to the project data set)\n* Explore, summarize and visualize the data set\n* Design, train and test a model architecture\n* Use the model to make predictions on new images\n* Analyze the softmax probabilities of the new images\n* Summarize the results with a written report\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1502.03167 and http://cs231n.github.io/neural-networks-2/#batchnorm\n\n### The Tensor Graph \n![Tensor Graph](graph_run.png"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "NUM_CHANNELS = 1 if GRAYSCALE else 3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266,
        0.9030859728368266
      ],
      "excerpt": "      <th>10</th> \n      <td>10</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.8356013927728488
      ],
      "excerpt": "      <th>12</th> \n      <td>12</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "      <td>No entry</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.8356013927728488
      ],
      "excerpt": "      <th>30</th> \n      <td>30</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.8356013927728488
      ],
      "excerpt": "      <th>37</th> \n      <td>37</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "num_images = 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9707925318074234
      ],
      "excerpt": "    plt.title('Label: %d' % labels[i]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693,
        0.8665716475375693,
        0.8665716475375693,
        0.8665716475375693,
        0.8665716475375693
      ],
      "excerpt": "    if a[0] == 1: img = translate(img) \n    if a[1] == 1: img = rotate(img) \n    if a[2] == 1: img = shear(img) \n    if a[3] == 1: img = blur(img) \n    if a[4] == 1: img = gamma(img) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559,
        0.9559715772848645
      ],
      "excerpt": "    translated_img.append(translate(sample_1)),rotated_img.append(rotate(sample_1)) \n    shear_img.append(shear(sample_1)), blur_img.append(blur(sample_1)), gamma_img.append(gamma(sample_1)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9969835261629555,
        0.9969835261629555,
        0.9546157635310808
      ],
      "excerpt": "plt.subplot(row, col,1), plt.title('Shift'), plt.subplot(row, col,2), plt.title('Rotation') \nplt.subplot(row, col,3), plt.title('Shear'), plt.subplot(row, col,4), plt.title('Blur') \nplt.subplot(row, col,5), plt.title('Gamma') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if len(new_images) > 0: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.868616796700031,
        0.9983360443725813,
        0.9072564759784449,
        0.8854398367006655,
        0.9252690500369315,
        0.9072564759784449
      ],
      "excerpt": "        net = slim.max_pool2d(net, [3, 3], 1, padding='SAME', scope='pool1')  #: output shape: (32, 32, 16) \n        net = slim.conv2d(net, 64, [5, 5], 3, padding='VALID', scope='conv2')  #: output shape: (10, 10, 64) \n        net = slim.max_pool2d(net, [3, 3], 1, scope='pool2')  #: output shape: (8, 8, 64) \n        net = slim.conv2d(net, 128, [3, 3], scope='conv3')  #: output shape: (8, 8, 128) \n        net = slim.conv2d(net, 64, [3, 3], scope='conv4')  #: output shape: (8, 8, 64) \n        net = slim.max_pool2d(net, [3, 3], 1, scope='pool3')  #: output shape: (6, 6, 64) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507374082549614,
        0.8592871015078071,
        0.9507374082549614,
        0.8592871015078071,
        0.9507374082549614
      ],
      "excerpt": "        net = slim.fully_connected(net, 1024, scope='fc1') \n        net = tf.nn.dropout(net, keep_prob) \n        net = slim.fully_connected(net, 1024, scope='fc2') \n        net = tf.nn.dropout(net, keep_prob) \n        net = slim.fully_connected(net, NUM_CLASSES, scope='fc3') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    logits = net \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8488312902669851
      ],
      "excerpt": "Describe how you trained your model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if RESTORE: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266,
        0.9263275875538001
      ],
      "excerpt": "        #: Print accuracy every 10 epochs \n        if (epoch+1) % 10 == 0 or epoch == 0 or (epoch+1) == NUM_EPOCH: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if SAVE_MODEL: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8357664768244879
      ],
      "excerpt": "        print('Accuracy history saved at accuracy_history.p') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8357664768244879
      ],
      "excerpt": "Accuracy history saved at accuracy_history.p \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9654846892514747
      ],
      "excerpt": "plt.title('Train (blue) and Validation (red) Accuracies over Epochs') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8829021559993814
      ],
      "excerpt": "    image = Image.open(image_file) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9707925318074234
      ],
      "excerpt": "    plt.title(image_files[i]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if first_line: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if y == pred: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9707925318074234
      ],
      "excerpt": "    plt.title(final_preds[i]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8829021559993814
      ],
      "excerpt": "    image = Image.open(image_file) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Speed limit (80km/h): 0.30% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8829021559993814
      ],
      "excerpt": "    image = Image.open(image_file) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ibabbar/Traffic-Sign-Classifier",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-12T07:11:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-12T07:14:27Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n#:#:#: To start off let's do a basic data summary.\n\n#: Number of training examples\nn_train = augmented_X_train.shape[0]\n\n#: Number of validation examples\nn_validation = X_valid.shape[0]\n\n#: Number of testing examples\nn_test = X_test.shape[0]\n\n#: What's the shape of an image?\nimage_shape = augmented_X_train[0].shape\n\n#: How many classes are in the dataset\nn_classes = np.unique(augmented_y_train).shape[0]\n\nprint(\"Number of training examples =\", n_train)\nprint(\"Number of validation examples =\", n_validation)\nprint(\"Number of testing examples =\", n_test)\nprint(\"Image data shape =\", image_shape)\nprint(\"Number of classes =\", n_classes)\n```\n\n    Number of training examples = 155275\n    Number of validation examples = 4410\n    Number of testing examples = 12630\n    Image data shape = (32, 32, 3)\n    Number of classes = 43\n\n\n\n```python\nX_train = augmented_X_train\ny_train = augmented_y_train \n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n#:#:#: To start off let's do a basic data summary.\n\n#: Number of training examples\nn_train = X_train.shape[0]\n\n#: Number of validation examples\nn_validation = X_valid.shape[0]\n\n#: Number of testing examples\nn_test = X_test.shape[0]\n\n#: What's the shape of an image?\nimage_shape = X_train[0].shape\n\n#: How many classes are in the dataset\nn_classes = np.unique(y_train).shape[0]\n\nprint(\"Number of training examples =\", n_train)\nprint(\"Number of validation examples =\", n_validation)\nprint(\"Number of testing examples =\", n_test)\nprint(\"Image data shape =\", image_shape)\nprint(\"Number of classes =\", n_classes)\n```\n\n    Number of training examples = 34799\n    Number of validation examples = 4410\n    Number of testing examples = 12630\n    Image data shape = (32, 32, 3)\n    Number of classes = 43\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9095989214785583,
        0.8819907518538044
      ],
      "excerpt": "IMG_SIZE = 32  #: square image of size IMG_SIZE x IMG_SIZE \nGRAYSCALE = False  #: convert image to grayscale? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.822302610293918
      ],
      "excerpt": "MODEL_SAVE_PATH = 'model.ckpt'  #: where to save trained model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8893056447647266
      ],
      "excerpt": "      <td>End of speed limit (80km/h)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9299521347009448
      ],
      "excerpt": "      <td>Right-of-way at the next intersection</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8244941069959402
      ],
      "excerpt": "      <td>Road narrows on the right</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003317140197043
      ],
      "excerpt": "      <td>Road work</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "      <td>Beware of ice/snow</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9603633294947393
      ],
      "excerpt": "      <td>End of all speed and passing limits</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9091066017513999
      ],
      "excerpt": "      <td>End of no passing</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9193901652940623
      ],
      "excerpt": "      <td>End of no passing by vehicles over 3.5 metric ...</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8126436185938742,
        0.9598758209873941,
        0.8877591227499633,
        0.8142119585676293
      ],
      "excerpt": "Defined a method random_transform() to randomly apply multiple transformation on one image. \nSince the traffic sign samples is not distrubted equally, this could affect the CNN's performance. The network would be biased toward classes with more samples. Data Augmentation generates additional data on each traffic sign. For example, if the average sample for each classes is 1000 images. Any class has less than 300 images need to generate more data.  \nData Augmentation includes but not limited to blurring, rotating, shearing, translating, changing brighness on original images. \nFor image transformation, I found this documentation from OpenCV is helpful:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for sign, sign_images in enumerate(separated_data): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "    for img in sign_images: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8237305880984547
      ],
      "excerpt": ": Create a barchart of frequencies \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9951869678433076,
        0.9592242978144666
      ],
      "excerpt": "is quite important that the data is shuffled to remove the influence of drifts within the data. \nI have managed to get quite a good validation accuracy without augmenting the data to create a much larger data set.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9604032414816791
      ],
      "excerpt": "The model was implemented using TensorFlow-Slim. TF-Slim is a lightweight library for defining, training and evaluating complex models in TensorFlow.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9678396000835945
      ],
      "excerpt": "* The overall dimensions of each layer are smaller, because our input image dimensions are smaller than that of the ImageNet challenge, and because we are trying to identify less classes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.981420724959032
      ],
      "excerpt": "It takes a significant amount of effort to twick the filter sizes, strides, padding to be able to get the model to perform well. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.886791789081861,
        0.8435974706715814,
        0.9641005302091283,
        0.9160008330706978,
        0.9277173258312303,
        0.9277173258312303,
        0.9160008330706978
      ],
      "excerpt": "Convolution with 3x3 kernel, stride of 1, depth of 16, same padding \nMax pooling with 3x3 kernel, stride of 1, same padding \nConvolution with 5x5 kernel, stride of 3, depth of 64, valid padding \nMax pooling with 3x3 kernel, stride of 1, valid padding \nConvolution with 3x3 kernel, stride of 1, depth of 128, same padding \nConvolution with 3x3 kernel, stride of 1, depth of 64, same padding \nMax pooling with 3x3 kernel, stride of 1, valid padding \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9523270553736408
      ],
      "excerpt": "Batch normalization was applied after the convolutional layers, to (a) make the model more robust to bad random weight initialization, (b) allow higher learning rates, and (c) help with regularization. More details about batch normalization are availabe at https://arxiv.org/abs/1502.03167 and http://cs231n.github.io/neural-networks-2/#batchnorm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "    with tf.variable_scope('Convolutional_Neural_Network'): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "    with slim.arg_scope([slim.conv2d], normalizer_fn=slim.batch_norm): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9803594448842159
      ],
      "excerpt": "To train the model, I used standard Stochastic Gradient Descent (SGD). I experimented with other optimizers like ADAM, but the performance was not good. I twicked the number of epochs ranging from 50 to 100 until I got a model validation accuracy of 96.16%. The learning rate remained unchanged throughout. I also adjusted the drop rate to adjust for overfitting from a value of 0.5 to 0.75 which resulted in very poor performance. I also tried to automate the process by using grid search hyperparameter tuning techniques on the learning rate, dropout rate, epochs and number of fully connected layers. This proved to be quite compute intensive, grid search ran for days and I had to abandon the idea until I had more computational resources. So I resorted to a random hyperparameter search method. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8326554878266264,
        0.9757679590230871,
        0.9820123173908909,
        0.9238467735943552,
        0.9538771266970699,
        0.9732518091657333
      ],
      "excerpt": "* Batch size too big: Either (a) my GPU will simply run out of memory, or (b) a single gradient update will take too long, to a point where it is not worth the extra variance reduction from going to a larger batch size (we are basically trying to \"optimize\" the gradient descent convergence speed). \nThus I chose a middle ground between the two extremes. I was not methodical in this process of batch size tuning, due to time constraints, and the resulting performance optimization derived from of tuning the batch size did not seem high. \nThe number of epochs was chosen such that it is large enough to see the training and validation accuracies saturating. This means the model cannot improve anymore, given more training time. \nThe hyperparameters were chosen based on a random hyperparameter combination search. The code to perform the search is available in search_params.py. In this process, I randomly chose 20 hyperparameter combinations, ran training on my model, and chose the hyperparameter combination that gave the highest test accuracy. ter combination was the best, since the test set should be \"hidden\" until the model has been finalized. \nLooking at the plot of train vs. validation accuracy, we can see the validation accuracy is only slighly below the training accuracy throughout the training process, which is a good indication of model performance. \nThe model's accuracy on the official test set is 96.41%, which is a good score. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.938585808039219
      ],
      "excerpt": "    with tf.Graph().as_default(), tf.Session() as sess: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.912299866932559
      ],
      "excerpt": "    #: Only dumping the graph, to visualize the architecture \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.895608950120991
      ],
      "excerpt": "        with open('accuracy_history.p', 'rb') as f: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.946152696452462
      ],
      "excerpt": "        #: For book-keeping, keep track of training and validation accuracy over epochs, like such: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8732826323461433
      ],
      "excerpt": "    #: Record time elapsed for performance check \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8055626172015554
      ],
      "excerpt": "        #: Instantiate generator for training data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9019298307814381,
        0.9872149632430804
      ],
      "excerpt": "        #: then we must calculate weighted average of the accuracy of the final (partial) batch, \n        #: w.r.t. the rest of the full batches \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.90288126412365
      ],
      "excerpt": "        #: Save model to disk \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.895608950120991
      ],
      "excerpt": "        with open('accuracy_history.p', 'wb') as f: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9096372089153341
      ],
      "excerpt": "To give yourself more insight into how your model is working, download at least five pictures of German traffic signs from the web and use your model to predict the traffic sign type. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "image_files  = ['sample_images/' + image_file for image_file in os.listdir('sample_images')] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for image_file in image_files: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9257471578833955
      ],
      "excerpt": "I could improve the predictions of the new image sample results by augmenting the validation iamge set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.938585808039219,
        0.9302287581602335
      ],
      "excerpt": "with tf.Graph().as_default(), tf.Session() as sess: \n    #: Instantiate the CONV NN model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9619809516325051
      ],
      "excerpt": "#: Run inference on CNN to make predictions, and remember the logits for later \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "final_preds = [label_map[pred] for pred in preds] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8917837975357025,
        0.8119025198788479,
        0.8846995999330939,
        0.8981101533134306,
        0.8220034846315138,
        0.944207648646226
      ],
      "excerpt": "sample_images/children.jpg --> Children crossing  This is the correct predicition \nsample_images/wild_animal_crossing.jpg --> Wild animals crossing This is the correct predicition \nsample_images/loose_gravel.jpg --> Yield This is not the correct predicition \nsample_images/maximum_height_allowed.jpg --> Go straight or right This is not the correct predicition \nsample_images/60_speedlimit.jpg --> Speed limit (60km/h) This is the correct predicition \nThe accuracy rate of the prediction is 60% (3/5) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.96235763043071
      ],
      "excerpt": ": For each of the 5 predictions, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076,
        0.9560187895509076
      ],
      "excerpt": "for pred in preds: \n    for i, y in enumerate(y_train): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.909611961296473
      ],
      "excerpt": "for i, image in enumerate(train_images): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.938585808039219
      ],
      "excerpt": "with tf.Graph().as_default(), tf.Session() as sess: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076,
        0.8795006277712293
      ],
      "excerpt": "    top_k_pred = [label_map[idx] for idx in top_k_idx] \n#: Show the image for reference \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "image_files  = ['sample_images/' + image_file for image_file in os.listdir('sample_images')] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for image_file in image_files: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8244941069959402
      ],
      "excerpt": "Road narrows on the right: 0.00% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003317140197043
      ],
      "excerpt": "Road work: 0.00% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471870699843013
      ],
      "excerpt": "Dangerous curve to the right: 0.00% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003317140197043
      ],
      "excerpt": "Road work: 0.00% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "Beware of ice/snow: 0.00% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9299521347009448
      ],
      "excerpt": "Right-of-way at the next intersection: 1.60% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.938585808039219
      ],
      "excerpt": "with tf.Graph().as_default(), tf.Session() as sess: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8465402595250658
      ],
      "excerpt": "sample_images_german/padestrian_crosswalk_ahead.jpg --> Children crossing This is the correct prediction  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8465402595250658,
        0.9180859598837611
      ],
      "excerpt": "sample_images_german/signal_lights_ahead.jpg --> Traffic signals This is the correct prediction  \nsample_images_german/road_work.jpg --> Road work This is the correct prediction  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9879751254804896
      ],
      "excerpt": "Our prediction accuracy is 80%. The model accuracy has improved by 20% which is quite significantly from the previous prediction. As earlier referenced the image quality has an effect on the model accuracy. It would be also intersting to visualize softmax because I could see what is causing some of the misclassification. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.938585808039219
      ],
      "excerpt": "with tf.Graph().as_default(), tf.Session() as sess: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "image_files  = ['sample_images_german/' + image_file for image_file in os.listdir('sample_images_german')] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for image_file in image_files: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "top_k_pred = [label_map[idx] for idx in top_k_idx] \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ibabbar/Traffic-Sign-Classifier/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 12:45:00 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ibabbar/Traffic-Sign-Classifier/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ibabbar/Traffic-Sign-Classifier",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ibabbar/Traffic-Sign-Classifier/master/Traffic_Sign_Classifier.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* sample_images/children.jpg --> Children crossing, Test Set --> Children crossing 100% match \n* sample_images/wild_animal_crossing.jpg --> Wild animals crossing, Test Set --> Wild animals crossing 100% match \n* sample_images/loose_gravel.jpg --> Yield, Test Set --> Yield no match, Test Set prediction correct but not a match on the new image\n* sample_images/maximum_height_allowed.jpg --> Go straight or right, Test Set --> , Test Set prediction correct but not a match on the new image\n* sample_images/60_speedlimit.jpg --> Speed limit (60km/h), Test Set --> Speed limit (60km/h)100% match\n\nThe test set has an accuracy of 100% compared to the new sample image prediction accuracy of 60%. This can be attributed to overfitting on the test set. The image quality is also a determining factor on these results.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": " \nI performed the following data preprocessing steps:\n\n* Initial test of the model performance with current dataset without twicking any parameters\n\n * Trying to get the model to perform with an accuracy rate over 93% is quite tricky because they are so many paremeters that can be tuned and twicked. There are multiple approaches my first step for the machine learning pipeline was to run the model first and then pre-process the data.\n \n \n* Image Augmentation details are explained the \"How I did Data Augmentation\" section\n\n\n* Standardize the pixel values: `new_value = (old_value - 128) / 128`\n  * This has the effect of zero-centering the data, and making the data fall within the range -1 to 1\n  * Standardizing the pixel values helps gradient descent converge faster\n  * Dividing the pixel values by 128 is not strictly necessary for this project, because all original pixel values are on the same scale, from 0 to 255. However, this division step is computationally cheap, and it's good practice in the long term to standardize the image preprocessing requirements, over many different projects with potentially different image representations. Over the course of many projects in the future, abiding by this practice can help reduce confusion and potential frustration.\n  \n* Convert the integer class labels into one-hot encoded labels\n  * This is important because different traffic signs do not have integer-like relationships with one another. For example, if a stop sign is labeled 0, a speed limit sign labeled 1, and a yield sign labeled 2, an integer label would imply a stop sign is more similar to a speed limit sign than a yield sign. A neural network classifier would mathematically assume this relationship as well. Practically, there is no such relationship. By converting all labels into one-hot encoded labels, we avoid this incorrect underlying assumption.\n  \n* Randomly shuffle the data \n * It is extremely important to shuffle the training data, so that we do not obtain entire minibatches of    highly correlated examples\n * If the data is given in some meaningful order, this can bias the gradient and lead to poor convergence. \n\n\n```python\ndef preprocess_data_shuffle(X, y):\n    \"\"\"\n    Preprocess image data, and convert labels into one-hot\n    and shuffle the data\n    Arguments:\n        * X: Image data\n        * y: Labels\n\n    Returns:\n        * Preprocessed X_shuffled, one-hot version of y_shuffled\n    \"\"\"\n    #: Convert from RGB to grayscale if applicable\n    if GRAYSCALE:\n        X = rgb_to_gray(X)\n\n    #: Make all image array values fall within the range -1 to 1\n    #: Note all values in original images are between 0 and 255, as uint8\n    X = X.astype('float32')\n    X = (X - 128.) / 128.\n\n    #: Convert the labels from numerical labels to one-hot encoded labels\n    y_onehot = np.zeros((y.shape[0], NUM_CLASSES))\n    for i, onehot_label in enumerate(y_onehot):\n        onehot_label[y[i]] = 1.\n    y = y_onehot\n    #:Shuffle the data\n    X_shuffled, y_shuffled = shuffle(X, y)\n    return X_shuffled, y_shuffled\n```\n\n\n```python\ndef preprocess_data(X, y):\n    \"\"\"\n    Preprocess image data, and convert labels into one-hot\n\n    Arguments:\n        * X: Image data\n        * y: Labels\n\n    Returns:\n        * Preprocessed X, one-hot version of y\n    \"\"\"\n    #: Convert from RGB to grayscale if applicable\n    if GRAYSCALE:\n        X = rgb_to_gray(X)\n\n    #: Make all image array values fall within the range -1 to 1\n    #: Note all values in original images are between 0 and 255, as uint8\n    X = X.astype('float32')\n    X = (X - 128.) / 128.\n\n    #: Convert the labels from numerical labels to one-hot encoded labels\n    y_onehot = np.zeros((y.shape[0], NUM_CLASSES))\n    for i, onehot_label in enumerate(y_onehot):\n        onehot_label[y[i]] = 1.\n    y = y_onehot\n    \n    return X, y\n```\n\n\n```python\ndef next_batch(X, y, batch_size, augment_data):\n    \"\"\"\n    Generator to generate data and labels\n    Each batch yielded is unique, until all data is exhausted\n    If all data is exhausted, the next call to this generator will throw a StopIteration\n\n    Arguments:\n        * X: image data, a tensor of shape (dataset_size, 32, 32, 3)\n        * y: labels, a tensor of shape (dataset_size,)  <-- i.e. a list\n        * batch_size: Size of the batch to yield\n        * augment_data: Boolean value, whether to augment the data (i.e. perform image transform)\n\n    Yields:\n        A tuple of (images, labels), where:\n            * images is a tensor of shape (batch_size, 32, 32, 3)\n            * labels is a tensor of shape (batch_size,)\n    \"\"\"\n    \n    #: We know X and y are randomized from the train/validation split already,\n    #: so just sequentially yield the batches\n    start_idx = 0\n    while start_idx < X.shape[0]:\n        images = X[start_idx : start_idx + batch_size]\n        labels = y[start_idx : start_idx + batch_size]\n\n        yield (np.array(images), np.array(labels))\n\n        start_idx += batch_size\n```\n\n\n```python\ndef calculate_accuracy(data_gen, data_size, batch_size, accuracy, x, y, keep_prob, sess):\n    \"\"\"\n    Helper function to calculate accuracy on a particular dataset\n\n    Arguments:\n        * data_gen: Generator to generate batches of data\n        * data_size: Total size of the data set, must be consistent with generator\n        * batch_size: Batch size, must be consistent with generator\n        * accuracy, x, y, keep_prob: Tensor objects in the neural network\n        * sess: TensorFlow session object containing the neural network graph\n\n    Returns:\n        * Float representing accuracy on the data set\n    \"\"\"\n    num_batches = math.ceil(data_size / batch_size)\n    last_batch_size = data_size % batch_size\n\n    accs = []  #: accuracy for each batch\n\n    for _ in range(num_batches):\n        images, labels = next(data_gen)\n\n        #: Perform forward pass and calculate accuracy\n        #: Note we set keep_prob to 1.0, since we are performing inference\n        acc = sess.run(accuracy, feed_dict={x: images, y: labels, keep_prob: 1.})\n        accs.append(acc)\n\n    #: Calculate average accuracy of all full batches (the last batch is the only partial batch)\n    acc_full = np.mean(accs[:-1])\n\n    #: Calculate weighted average of accuracy accross batches\n    acc = (acc_full * (data_size - last_batch_size) + accs[-1] * last_batch_size) / data_size\n\n    return acc\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n#: Count frequency of each label\nlabels, counts = np.unique(y_train, return_counts=True)\n\n#: Plot the histogram\nplt.rcParams[\"figure.figsize\"] = [15, 5]\naxes = plt.gca()\naxes.set_xlim([-1,43])\n\nplt.bar(labels, counts, tick_label=labels, width=0.8, align='center')\nplt.title('Class Distribution across Training Data')\nplt.show()\n```\n\n\n![png](output_19_0.png)\n\n\nThe training data's class distribution is highly skewed.\n\n\n```python\n#: Count frequency of each label\nlabels, counts = np.unique(y_valid, return_counts=True)\n\n#: Plot the histogram\nplt.rcParams[\"figure.figsize\"] = [15, 5]\naxes = plt.gca()\naxes.set_xlim([-1,43])\n\nplt.bar(labels, counts, tick_label=labels, width=0.8, align='center')\nplt.title('Class Distribution across Validation Data')\nplt.show()\n```\n\n\n![png](output_21_0.png)\n\n\nThe validation  data is also skewed, in the same way the training data is skewed. The only difference is the individual counts for each class are less.\n\n\n```python\n#: Count frequency of each label\nlabels, counts = np.unique(y_test, return_counts=True)\n\n#: Plot the histogram\nplt.rcParams[\"figure.figsize\"] = [15, 5]\naxes = plt.gca()\naxes.set_xlim([-1,43])\n\nplt.bar(labels, counts, tick_label=labels, width=0.8, align='center')\nplt.title('Class Distribution across Testing Data')\nplt.show()\n```\n\n\n![png](output_23_0.png)\n\n\nThe test data is also skewed, in the same way the training data is skewed. The only difference is the individual counts for each class are less.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9257175720597752
      ],
      "excerpt": "with the following differences: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.838644348121631
      ],
      "excerpt": "Go straight or left: 11.88% \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.925671696398174,
        0.9010195194286076,
        0.8628110089442617
      ],
      "excerpt": "import tensorflow as tf \nimport tensorflow.contrib.slim as slim  \nfrom tensorflow.contrib.layers import flatten \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9068127677393759
      ],
      "excerpt": "import matplotlib.pyplot as plt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044146895679722,
        0.9416522774131079
      ],
      "excerpt": "from PIL import Image \nfrom sklearn.utils import shuffle \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9457175861910134,
        0.9133368656218674,
        0.9133368656218674,
        0.8401558704798054,
        0.8396948529258378,
        0.8214055523315815
      ],
      "excerpt": "import numpy as np \nimport cv2 \nimport math \nimport os \nimport time \nimport pickle \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8772823479068823
      ],
      "excerpt": "RESUME = False  #: resume training from previously trained model? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8317719395492194
      ],
      "excerpt": "BATCH_SIZE = 128  #: batch size for training (relatively small) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8680059285815445
      ],
      "excerpt": "SAVE_MODEL = True  #: save trained model to disk? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "training_file = 'train.p' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "testing_file = 'test.p' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359299706379749
      ],
      "excerpt": "        text-align: left; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811854372964597
      ],
      "excerpt": "      <td>15</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359299706379749
      ],
      "excerpt": "        text-align: left; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174059093168353
      ],
      "excerpt": "indices = np.random.choice(list(range(n_train)), size=num_images, replace=False) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8864713366330325
      ],
      "excerpt": "plt.rcParams[\"figure.figsize\"] = [15, 5] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8310159060815993
      ],
      "excerpt": "    plt.subplot(1, num_images, i+1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737288687529231,
        0.8737288687529231,
        0.8997243352845468
      ],
      "excerpt": "    x_shift = np.random.uniform(-0.3 * x, 0.3 * x) \n    y_shift = np.random.uniform(-0.3 * y, 0.3 * y) \n    shift_matrix = np.float32([[1, 0, x_shift], [0, 1, y_shift]]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8399180631100392
      ],
      "excerpt": "    angle = np.random.uniform(-90, 90) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9096432342613207,
        0.8688606592923903,
        0.8737288687529231,
        0.8750720306779645,
        0.8997243352845468
      ],
      "excerpt": "    shear = np.random.randint(5,15) \n    pts1 = np.array([[5, 5], [20, 5], [5, 20]]).astype('float32') \n    pt1 = 5 + shear * np.random.uniform() - shear / 2 \n    pt2 = 20 + shear * np.random.uniform() - shear / 2 \n    pts2 = np.float32([[pt1, 5], [pt2, pt1], [5, pt2]]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737288687529231
      ],
      "excerpt": "    r_int = np.random.randint(0, 2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737288687529231
      ],
      "excerpt": "    gamma = np.random.uniform(0.2, 1.5) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079141805236937
      ],
      "excerpt": "    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8574031641756075
      ],
      "excerpt": "    a = np.random.randint(0, 2, [1, 5]).astype('bool')[0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8121038686553564,
        0.8121038686553564,
        0.8121038686553564,
        0.8121038686553564,
        0.8121038686553564
      ],
      "excerpt": "    plt.subplot(row, col,pos+1), plt.imshow(translated_img[i]),  plt.axis('off') \n    plt.subplot(row, col,pos+2), plt.imshow(rotated_img[i]), plt.axis('off')  \n    plt.subplot(row, col,pos+3), plt.imshow(shear_img[i]), plt.axis('off') \n    plt.subplot(row, col,pos+4), plt.imshow(blur_img[i]), plt.axis('off') \n    plt.subplot(row, col,pos+5), plt.imshow(gamma_img[i]), plt.axis('off') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664,
        0.8742512866547499
      ],
      "excerpt": "        test = random_transform(sample_1) \n        plt.subplot(5, 7, pos+i+1), plt.imshow(test), plt.axis('off') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8964514146200785,
        0.8986803158600359
      ],
      "excerpt": "ts, imgs_per_sign = np.unique(y_train, return_counts=True) \navg_per_sign = np.ceil(np.mean(imgs_per_sign)).astype('uint32') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.872705471657846,
        0.9241022411340848
      ],
      "excerpt": "expanded_data = np.array(np.zeros((1, 32,32,3))) \nexpanded_labels = np.array([0]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8973933083440926,
        0.9084820443788738
      ],
      "excerpt": "        sign_images = np.concatenate((sign_images, new_images),axis=0) \n    new_labels = np.full(len(sign_images),sign, dtype ='uint8') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8973933083440926,
        0.8973933083440926
      ],
      "excerpt": "    expanded_data = np.concatenate((expanded_data,sign_images), axis = 0) \n    expanded_labels = np.concatenate((expanded_labels, new_labels), axis=0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979680543287559,
        0.9127166971916394,
        0.8964514146200785,
        0.9241022411340848,
        0.8864713366330325
      ],
      "excerpt": "item, count = np.unique(y_train, return_counts=True) \nfreq = np.array((item, count)).T \nitem2, count2 = np.unique(augmented_y_train, return_counts=True) \nfreq2 = np.array((item2, count2)).T \nplt.figure(figsize=(15, 5)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8054656741958919,
        0.8054656741958919
      ],
      "excerpt": "        x = tf.placeholder('float', [None, IMG_SIZE, IMG_SIZE, NUM_CHANNELS]) \n        y = tf.placeholder('float', [None, NUM_CLASSES]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8128131768734507
      ],
      "excerpt": "    keep_prob = tf.placeholder(tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8124317103750185
      ],
      "excerpt": "        net = slim.conv2d(net, 64, [5, 5], 3, padding='VALID', scope='conv2')  #: output shape: (10, 10, 64) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.818691279070404
      ],
      "excerpt": "        net = slim.conv2d(net, 128, [3, 3], scope='conv3')  #: output shape: (8, 8, 128) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289669050403863
      ],
      "excerpt": "    #: Final output (logits) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8446434232000626
      ],
      "excerpt": "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learningrate).minimize(loss) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.823687576256224
      ],
      "excerpt": "* Batch size: 128 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.86270861701488,
        0.8936954105699045
      ],
      "excerpt": "    #: TF saver to save/restore trained model \n    saver = tf.train.Saver() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.899590627509464
      ],
      "excerpt": "        print('Restoring previously trained model at %s' % MODEL_SAVE_PATH) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8007582787222295
      ],
      "excerpt": "    #: Run NUM_EPOCH epochs of training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8584865702155933,
        0.8584865702155933
      ],
      "excerpt": "        train_gen = next_batch(X_train, y_train, BATCH_SIZE, True) \n        #:train_gen = next_batch(augmented_X_train, augmented_y_train, BATCH_SIZE, True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8035238572818386,
        0.80897680582524,
        0.80897680582524,
        0.827822980896969
      ],
      "excerpt": "        #: How many batches to run per epoch \n        num_batches_train = math.ceil(X_train.shape[0] / BATCH_SIZE) \n        #:num_batches_train = math.ceil(augmented_X_train.shape[0] / BATCH_SIZE) \n        #: Run training on each batch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8392029383816584
      ],
      "excerpt": "        #: If train/validation size % batch size != 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "        train_gen = next_batch(X_train, y_train, BATCH_SIZE_INF, True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "        valid_gen = next_batch(X_valid, y_valid, BATCH_SIZE_INF, True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621812072926057
      ],
      "excerpt": "        #: Record and report train/validation/test accuracies for this epoch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254422898194195
      ],
      "excerpt": "            print('Epoch %d -- Train acc.: %.4f, Validation acc.: %.4f, Elapsed time: %.2f sec' %\\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.910717752990623
      ],
      "excerpt": "    print('Total elapsed time: %.2f sec (%.2f min)' % (total_time, total_time/60)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8780530042385731
      ],
      "excerpt": "    print('Calculating test accuracy...') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.957306766029513
      ],
      "excerpt": "    print('Test acc.: %.4f' % (test_acc,)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8548423674100364
      ],
      "excerpt": "        print('Trained model saved at: %s' % save_path) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8463945641922367,
        0.8735810768805181,
        0.8952564401378761,
        0.8564451968778911,
        0.8829073824395168,
        0.8475705435170487,
        0.8035461917986191,
        0.809160883213795,
        0.8662071443893008,
        0.8584119808149383,
        0.8781211897620192
      ],
      "excerpt": "Epoch 1 -- Train acc.: 0.2485, Validation acc.: 0.2517, Elapsed time: 21.39 sec \nEpoch 10 -- Train acc.: 0.7888, Validation acc.: 0.8576, Elapsed time: 179.58 sec \nEpoch 20 -- Train acc.: 0.8876, Validation acc.: 0.9070, Elapsed time: 200.23 sec \nEpoch 30 -- Train acc.: 0.9048, Validation acc.: 0.9059, Elapsed time: 199.83 sec \nEpoch 40 -- Train acc.: 0.9146, Validation acc.: 0.9084, Elapsed time: 199.67 sec \nEpoch 50 -- Train acc.: 0.9179, Validation acc.: 0.9175, Elapsed time: 199.71 sec \nEpoch 60 -- Train acc.: 0.9439, Validation acc.: 0.9222, Elapsed time: 199.57 sec \nEpoch 70 -- Train acc.: 0.9633, Validation acc.: 0.9395, Elapsed time: 199.76 sec \nEpoch 80 -- Train acc.: 0.9639, Validation acc.: 0.9381, Elapsed time: 199.89 sec \nEpoch 90 -- Train acc.: 0.9668, Validation acc.: 0.9372, Elapsed time: 199.95 sec \nEpoch 100 -- Train acc.: 0.9734, Validation acc.: 0.9422, Elapsed time: 199.73 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "Test acc.: 0.9233 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9266130138527792,
        0.8862325372491721,
        0.8574581101680899
      ],
      "excerpt": "hist = np.transpose(np.array(accuracy_history)) \nplt.plot(hist[0], 'b')  #: training accuracy \nplt.plot(hist[1], 'r')  #: validation accuracy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.923665806087056
      ],
      "excerpt": ": Read sample image files, resize them, convert to numpy arrays w/ dtype=uint8 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8252969315276958
      ],
      "excerpt": "    image = np.array(list(image.getdata()), dtype='uint8') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9209358819219875
      ],
      "excerpt": "images = np.array(images, dtype='uint8') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8649850933035759
      ],
      "excerpt": ": Visually inspect sample images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8310159060815993
      ],
      "excerpt": "    plt.subplot(3, 3, i+1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.832500186770906,
        0.8594142235991984
      ],
      "excerpt": "with open('signnames.csv', 'r') as f: \n    first_line = True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8274376321473749
      ],
      "excerpt": "        #: Ignore first line \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8440293382790072
      ],
      "excerpt": "    label_int, label_string = line.split(',') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9016032142396481
      ],
      "excerpt": "images, _ = preprocess_data(images, np.array([0 for _ in range(images.shape[0])])) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8936954105699045
      ],
      "excerpt": "saver = tf.train.Saver() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8181516612633755
      ],
      "excerpt": "lgts, preds = sess.run([logits, predictions], feed_dict={x: images, keep_prob: 1.}) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.81751354429605
      ],
      "excerpt": "INFO:tensorflow:Restoring parameters from model.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8970888477663823,
        0.9061819790310054
      ],
      "excerpt": ": Print predictions on my sample images \nprint('Predictions on sample images\\n') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785
      ],
      "excerpt": "    print('%s --> %s' % (image_files[i], final_preds[i])) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8009219110167269
      ],
      "excerpt": "Predictions on sample images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8009219110167269
      ],
      "excerpt": "Predictions on sample images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8393772247095552,
        0.8589534893990137
      ],
      "excerpt": ": Load the German Traffic Sign training set again \ntraining_file = 'train.p' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359703228822304
      ],
      "excerpt": ": Display images from test set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8310159060815993
      ],
      "excerpt": "    plt.subplot(3, 3, i+1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8668258580770863
      ],
      "excerpt": "plt.show()   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8054656741958919,
        0.8123763140827432,
        0.8123763140827432,
        0.8008331685760428
      ],
      "excerpt": "    logits = tf.placeholder('float', [None, NUM_CLASSES]) \n    softmax = tf.nn.softmax(logits) \n    top_k_val, top_k_idx = tf.nn.top_k(softmax, k=5) \ntop_k_vals, top_k_idxs = sess.run([top_k_val, top_k_idx], feed_dict={logits: lgts}) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8593499603554345
      ],
      "excerpt": "    print('%s: %.2f%%' % (top_k_pred[i].replace('\\n', ''), top_k_val[i] * 100)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8101419916747523
      ],
      "excerpt": ": Re-read sample images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8252969315276958
      ],
      "excerpt": "    image = np.array(list(image.getdata()), dtype='uint8') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9209358819219875
      ],
      "excerpt": "images = np.array(images, dtype='uint8') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8301548116944105
      ],
      "excerpt": "No vehicles: 0.01% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9016032142396481
      ],
      "excerpt": "images, _ = preprocess_data(images, np.array([0 for _ in range(images.shape[0])])) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8936954105699045
      ],
      "excerpt": "saver = tf.train.Saver() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9061819790310054
      ],
      "excerpt": "print('Predictions on sample images\\n') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785
      ],
      "excerpt": "    print('%s --> %s' % (image_files[i], final_preds[i])) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.81751354429605,
        0.8009219110167269
      ],
      "excerpt": "INFO:tensorflow:Restoring parameters from model.ckpt \nPredictions on sample images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8054656741958919,
        0.8123763140827432,
        0.8123763140827432,
        0.8008331685760428
      ],
      "excerpt": "    logits = tf.placeholder('float', [None, NUM_CLASSES]) \n    softmax = tf.nn.softmax(logits) \n    top_k_val, top_k_idx = tf.nn.top_k(softmax, k=5) \ntop_k_vals, top_k_idxs = sess.run([top_k_val, top_k_idx], feed_dict={logits: lgts}) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8252969315276958
      ],
      "excerpt": "    image = np.array(list(image.getdata()), dtype='uint8') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9209358819219875
      ],
      "excerpt": "images = np.array(images, dtype='uint8') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8310159060815993
      ],
      "excerpt": "    plt.subplot(1, 5, i+1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8310159060815993
      ],
      "excerpt": "    plt.xlabel(pred_certainty_str(top_k_vals[i], top_k_idxs[i])) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9012248701992861
      ],
      "excerpt": "import seaborn as sns \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8414871024615834,
        0.8310159060815993
      ],
      "excerpt": "    fig = plt.figure(figsize=(12,4)) \n    plt.xlabel(pred_certainty_str(top_k_vals[i], top_k_idxs[i])) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ibabbar/Traffic-Sign-Classifier/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "HTML",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017-2018 Udacity, Inc.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Traffic Sign Classifier",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Traffic-Sign-Classifier",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ibabbar",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ibabbar/Traffic-Sign-Classifier/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    lgts, preds = sess.run([logits, predictions], feed_dict={x: images, keep_prob: 1.})\n\nfinal_preds = [label_map[pred] for pred in preds]\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 12:45:00 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I obtained the above 5 images from a dataset of German traffic signs [German Traffic Signs](https://www.adac.de/_mmm/pdf/fi_verkehrszeichen_engl_infobr_0915_30482.pdf)\n\n\n```python\n#: Read sample image files, resize them, convert to numpy arrays w/ dtype=uint8\nimage_files  = ['sample_images_german/' + image_file for image_file in os.listdir('sample_images_german')]\nimages = []\nfor image_file in image_files:\n    image = Image.open(image_file)\n    image = image.convert('RGB')\n    image = image.resize((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)\n    image = np.array(list(image.getdata()), dtype='uint8')\n    image = np.reshape(image, (32, 32, 3))\n\n    images.append(image)\nimages = np.array(images, dtype='uint8')\n\n#: Visually inspect sample images\nfor i, image in enumerate(images):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(image)\n    plt.title(image_files[i])\n\nplt.tight_layout()\nplt.show()\n```\n\n\n![png](output_81_0.png)\n\n\n\n```python\n#: Load signnames.csv to map label number to sign string\nlabel_map = {}\nwith open('signnames.csv', 'r') as f:\n    first_line = True\n    for line in f:\n        #: Ignore first line\n        if first_line:\n            first_line = False\n            continue\n\n        #: Populate label_map\n        label_int, label_string = line.split(',')\n        label_int = int(label_int)\n\n        label_map[label_int] = label_string\n        \n",
      "technique": "Header extraction"
    }
  ]
}