{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1612.03969"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Big shout-out to Jim Fleming for his initial Tensorflow Implementation - his Dynamic Memory Cell Implementation \nspecifically made things a lot easier.\n\nReference: [Jim Fleming's EntNet Memory Cell](https://github.com/jimfleming/recurrent-entity-networks/blob/master/entity_networks/dynamic_memory_cell.py)\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/siddk/entity-network",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-02-21T03:12:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-20T16:55:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.954653871803063,
        0.9865764747914536,
        0.9739929628868417,
        0.8904254776633872,
        0.8200346273615607,
        0.9418800219572494,
        0.9609997930441072
      ],
      "excerpt": "Tensorflow/TFLearn Implementation of \"Tracking the World State with Recurrent Entity Networks\" by Henaff et. al. \nBy building a set of disparate memory cells, each responsible for different concepts, entities, or other content, Recurrent Entity Networks (EntNets) are able to efficiently and robustly maintain a \u201cworld-state\u201d - one that can be updated easily and effectively with the influx of new information.  \nFurthermore, one can either let EntNet cell keys vary, or specifically seed them with specific embeddings, thereby forcing the model to track a given set of entities/objects/locations, allowing for the easy interpretation of the underlying decision-making process. \nImplementation results are as follows (graphs of training/validation loss will be added later). Some of the tasks  \nare fairly computationally intensive, so it might take a while to get benchmark results. \nNote that for efficiency, training stopped after validation accuracy passed a threshold of 95%. This is different than \nthe method used in the paper, which runs tasks for 200 epochs, and reports the best model across 10 different runs. The number of runs, epochs to converge, and final train/validation/test accuracies (best on validation over different runs) for this repository relative to the paper results are as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8909209973926638,
        0.9413564995009202
      ],
      "excerpt": "Note that the italics above indicate examples of overfitting. Note that the notes rows consist of single runs \nof the model - this is probably why multiple runs are necessary. If this continues to happen, I'll look into ways \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9165611899114254,
        0.9033387313613228
      ],
      "excerpt": "The bold above denotes failure to convergence. I'm not sure why this is happening, but I'll note that Jim \nFleming reports the same sort of issue in his implementation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9785300073676801
      ],
      "excerpt": "id is the id of the task at hand. As an example, here is the plot for the graph of Task 1 - Single Supporting Fact's  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8889225041851655
      ],
      "excerpt": "Entity Networks consist of three separate components: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8515854046697791
      ],
      "excerpt": "2) The Dynamic Memory (the core of the model), that keeps a disparate set of memory cells, each with a different vector key <img src=\"https://rawgit.com/siddk/entity-network/master/eval/svgs/40cca55dbe7b8452cf1ede03d21fe3ed.svg?invert_in_darkmode\" align=middle width=17.806305pt height=14.10255pt/> (the location), and a hidden state memory <img src=\"https://rawgit.com/siddk/entity-network/master/eval/svgs/6d22be1359e204374e6f0b45e318d561.svg?invert_in_darkmode\" align=middle width=15.517425pt height=22.74591pt/> (the content) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9086692833206821
      ],
      "excerpt": "A breakdown of the components are as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9169505666263966,
        0.9117564127275581
      ],
      "excerpt": "This repository (like the paper) utilizes a learned multiplicative mask, where each embedding of the sentence is multiplied element-wise with a mask vector <img src=\"https://rawgit.com/siddk/entity-network/master/eval/svgs/9b6dbadab1b122f6d297345e9d3b8dd7.svg?invert_in_darkmode\" align=middle width=12.65154pt height=22.74591pt/> and then summed together.  \nAlternatively, one could just as easily imagine an LSTM or CNN encoder to generate this initial input. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8233325629853377,
        0.9907865722249435,
        0.898511845986085,
        0.981319838599468
      ],
      "excerpt": "The keys and state vectors function similarly to how the program keys and program embeddings function in the NPI/NTM - the keys represent location, while the memories are content. \nOnly the content (memories) get updated at inference time, with the influx of new information.  \nFurthermore, one can seed and fix the key vectors such that they reflect certain words/entities => the paper does this by fixing key vectors to certain word embeddings, and using a simple BoW state encoding. \nThis repository currently only supports random key vector seeds. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.927125444495949
      ],
      "excerpt": "New state update - U, V, W are model parameters that are shared across all memory cells . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9509894827795039
      ],
      "excerpt": "Gated update, elementwise product of g with $\\tilde{h}$. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9159850700596901
      ],
      "excerpt": "Normalizes states based on cosine similarity. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "Weighted sum of hidden states \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8999608111099703
      ],
      "excerpt": "As long as you can build some sort of loss using y, then the entirety of the model is trainable via Backpropagation-Through-Time (BPTT). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.943832991697912,
        0.8269617053350495
      ],
      "excerpt": "model/ - Model definition code, including the definition of the Dynamic Memory Cell. \npreprocessor/ - Preprocessing code to load and vectorize the bAbI Tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Tensorflow implementation of \"Tracking the World State with Recurrent Entity Networks\" [https://arxiv.org/abs/1612.03969] by Henaff, Weston, Szlam, Bordes, and LeCun.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/siddk/entity-network/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 16,
      "date": "Fri, 24 Dec 2021 16:44:11 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/siddk/entity-network/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "siddk/entity-network",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/siddk/entity-network/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Recurrent Entity Networks",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "entity-network",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "siddk",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/siddk/entity-network/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 58,
      "date": "Fri, 24 Dec 2021 16:44:11 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "recurrent-entity-networks",
      "embeddings",
      "key-vectors",
      "tensorflow",
      "tensorflow-models",
      "recurrent-neural-networks"
    ],
    "technique": "GitHub API"
  }
}