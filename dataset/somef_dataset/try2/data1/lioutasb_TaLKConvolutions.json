{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2002.03184"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{lioutas2020timeaware,\n    author={Vasileios Lioutas and Yuhong Guo},\n    title={Time-aware Large Kernel Convolutions},\n    booktitle={Proceedings of the 37th International Conference on Machine Learning (ICML)},\n    year={2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{lioutas2020timeaware,\n    author={Vasileios Lioutas and Yuhong Guo},\n    title={Time-aware Large Kernel Convolutions},\n    booktitle={Proceedings of the 37th International Conference on Machine Learning (ICML)},\n    year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.911449585530318
      ],
      "excerpt": "    --source-lang de --target-lang en --max-tokens 4000 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8275419710363161
      ],
      "excerpt": "Training and evaluating TaLK Convolutions on WMT16 En-De using cosine scheduler on one machine with 8 NVIDIA GPUs: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lioutasb/TaLKConvolutions",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-06T15:27:02Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-15T02:16:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9765244580200637
      ],
      "excerpt": "This repository contains the source code, pre-trained models, as well as instructions to reproduce results for our paper Time-aware Large Kernel Convolutions (ICML 2020). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9959702197203038
      ],
      "excerpt": "TaLK Convolutions is a sequence modeling method that uses an adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized learnable kernel matrix. It utilizes a fast parallelized implementation of the summed-area table, also known as the integral image operation, to efficiently calculate the convolution output that uses the summation kernel. We generate relative offsets for each timestep of the input sequence, which are used to adaptively expand the size of the summation kernel conditioned on the input. This method yields a time complexity of O(n), effectively making the sequence encoding process linear to the number of tokens. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8076612349305027
      ],
      "excerpt": "In order to support the parallelization of TaLK Convolutions, we have developed our own CUDA primitives. To install the kernels, use the commands below. We tested compiling the kernels using CUDA 10.1 but if a future CUDA release does not work, please feel free to open an issue. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Official PyTorch implementation of Time-aware Large Kernel (TaLK) Convolutions (ICML 2020)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lioutasb/TaLKConvolutions/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Mon, 27 Dec 2021 17:46:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lioutasb/TaLKConvolutions/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lioutasb/TaLKConvolutions",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lioutasb/TaLKConvolutions/master/utils/compound_split_bleu.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9893272198983933,
        0.9906248903846466,
        0.8066825402298099,
        0.9906248903846466,
        0.9820226428242687,
        0.8269825609313461
      ],
      "excerpt": "git clone https://github.com/lioutasb/TaLKConvolutions.git \ncd TaLKConvolutions \nIn order to support the parallelization of TaLK Convolutions, we have developed our own CUDA primitives. To install the kernels, use the commands below. We tested compiling the kernels using CUDA 10.1 but if a future CUDA release does not work, please feel free to open an issue. \ncd talkconv/talkconv_module/ \npython setup.py install \nWe are welcoming contributions from experienced CUDA developers regarding making the CUDA kernels more efficient. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8678154254587249,
        0.8663771557349185,
        0.8663771557349185,
        0.8213727784375082
      ],
      "excerpt": "IWSLT14 German-English | download (.pt) | IWSLT14 test: download (.zip) \nWMT16 English-German | download (.pt) | newstest2014: download (.zip) \nWMT14 English-French | download (.pt) | newstest2014: download (.zip) \nPlease follow the instructions https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md to preprocess the data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9075491309075312
      ],
      "excerpt": "mkdir -p $SAVE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9075491309075312
      ],
      "excerpt": "mkdir -p $SAVE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8087594629564195
      ],
      "excerpt": "bash utils/compound_split_bleu.sh wmt14_gen_ende.txt  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9075491309075312
      ],
      "excerpt": "mkdir -p $SAVE \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8024778931870533
      ],
      "excerpt": "Dataset | Model | Prepared test set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9117614141733106,
        0.8651048762657209,
        0.8651048762657209
      ],
      "excerpt": "IWSLT14 German-English | download (.pt) | IWSLT14 test: download (.zip) \nWMT16 English-German | download (.pt) | newstest2014: download (.zip) \nWMT14 English-French | download (.pt) | newstest2014: download (.zip) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": ": Training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8960798090125345,
        0.9053200747745077
      ],
      "excerpt": "python utils/average_checkpoints.py --inputs $SAVE \\ \n    --num-epoch-checkpoints 10 --output \"${SAVE}/model.pt\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8520496344523806
      ],
      "excerpt": "    --batch-size 128 --beam 5 --remove-bpe --lenpen 1.6 --gen-subset test --quiet  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": ": Training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8166088653259512
      ],
      "excerpt": "    data-bin/wmt16_en_de_bpe32k --fp16 --log-interval 100 --no-progress-bar --distributed-no-spawn \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066887973318759,
        0.9053200747745077
      ],
      "excerpt": "python utilss/average_checkpoints.py --inputs $SAVE \\ \n    --num-epoch-checkpoints 10 --output \"${SAVE}/model.pt\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8001164886064323
      ],
      "excerpt": "fairseq-generate data-bin/wmt16_en_de_bpe32k --user-dir talkconv/talkconv_fairseq \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8615312803939889,
        0.8514432074199458
      ],
      "excerpt": "  --batch-size 128 --beam 4 --remove-bpe --lenpen 0.35 --gen-subset test > wmt14_gen_ende.txt  \nbash utils/compound_split_bleu.sh wmt14_gen_ende.txt  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": ": Training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8166088653259512
      ],
      "excerpt": "    data-bin/wmt14_en_fr --fp16 --log-interval 100 --no-progress-bar --distributed-no-spawn \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8960798090125345,
        0.9053200747745077
      ],
      "excerpt": "python utils/average_checkpoints.py --inputs $SAVE \\ \n    --num-epoch-checkpoints 10 --output \"${SAVE}/model.pt\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8001164886064323
      ],
      "excerpt": "fairseq-generate data-bin/wmt14_en_fr --user-dir talkconv/talkconv_fairseq \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8520496344523806
      ],
      "excerpt": "    --batch-size 128 --beam 6 --remove-bpe --lenpen 0.65 --gen-subset test --quiet  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lioutasb/TaLKConvolutions/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Vasileios Lioutas\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Time-aware Large Kernel (TaLK) Convolutions (Lioutas et al., 2020)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "TaLKConvolutions",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lioutasb",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lioutasb/TaLKConvolutions/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [PyTorch](http://pytorch.org/) version >= 1.3.1\n* [fairseq](https://github.com/pytorch/fairseq) version >= 0.10.1\n* Python version >= 3.6\n* CUDA >= 10.1\n* NVIDIA's [apex](https://github.com/NVIDIA/apex) library (for mixed-precision training)\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 28,
      "date": "Mon, 27 Dec 2021 17:46:30 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "icml",
      "icml-2020",
      "icml2020",
      "talk-convolutions",
      "seq2seq"
    ],
    "technique": "GitHub API"
  }
}