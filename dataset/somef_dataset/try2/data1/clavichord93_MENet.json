{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.01083",
      "https://arxiv.org/abs/1803.09127"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8356013927728488,
        0.8356013927728488,
        0.8356013927728488,
        0.9278824608274014
      ],
      "excerpt": "| 228-MENet-12$\\times$1 (g=3) | 144 | 66.43 | 86.72 | \n| 256-MENet-12$\\times$1 (g=4) | 140 | 66.59 | 86.74 | \n| 352-MENet-12$\\times$1 (g=8) | 144 | 66.69 | 86.92 | \n| 348-MENet-12$\\times$1 (g=3)  | 299 | 69.91 | 89.08 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/qinzheng93/MENet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-01-18T03:37:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-25T02:45:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository contains the code for **MENet** (*Merging-and-Evolution* networks), a new family of compact networks which alleviate the loss of inter-group information in [ShuffleNet](https://arxiv.org/abs/1707.01083).\n\nThe key idea of MENet is to utilize a **merging** operation and an **evolution** operation on the feature map generated from a group convolution for leveraging the inter-group information. The merging and evolution operations encode features across all channels into a narrow feature map, and combine it with the original network for better representation.\n\nMENet is composed of **ME modules**, whose structure is illustrated in Figure 1.\n\n![ME module](https://raw.githubusercontent.com/clavichord93/MENet/master/screenshots/ME_module.png)\nFigure 1. The structure of ME module. *(a)*: Standard ME module. *(b)*: Downsampling ME module. *GConv*: Group convolution. *DWConv*: Depthwise convolution.\n\nOur paper ([arXiv](https://arxiv.org/abs/1803.09127)) has been accepted as a conference paper by IJCNN 2018.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9887475653597955
      ],
      "excerpt": "The models are trained on 4 Tesla K80 GPUs using SGD for 120 epochs. We use a batch size of 256 and Nesterov momentum of 0.9. The weight decay is set to 4e-5. The learning rate starts from 0.1, and decreases by a factor of 10 every 30 epochs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This repo contains code for *Merging and Evolution: Improving Convolutional Neural Networks for Mobile Applications*.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/clavichord93/MENet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 12:47:22 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/qinzheng93/MENet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "qinzheng93/MENet",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/qinzheng93/MENet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Merging and Evolution: Improving Convolutional Neural Networks for Mobile Application",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "MENet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "qinzheng93",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/qinzheng93/MENet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. pytorch >= 0.2.0, torchvision >= 0.2.0\n2. graphviz >= 0.8.0\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Fri, 24 Dec 2021 12:47:22 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Before starting, modify the data directory in `config/imagenet/data-config/*.json` to your data path.\n\nTo train a model:\n\n```bash\npython -u main.py \\\n       --data /path/to/data/config \\\n       --model /path/to/model/config \\\n       --optim /path/to/optim/config \\\n       --sched /path/to/sched/config \\\n       --label model_label \\\n       [--print-freq N] \\\n       [--resume] \\\n       [--evaluate]\n```\n\nwhere `model_label` is the name of the checkpoint to be saved or resumed. For example:\n\n```bash\npython -u main.py \\\n       --data config/imagenet/data-config/imagenet-aggressive.json \\\n       --model config/imagenet/model-config/menet/228-MENet-12x1-group-3.json \\\n       --optim config/imagenet/optim-config/SGD-120-nesterov.json \\\n       --sched config/imagenet/sched-config/StepLR-30-0.1.json \\\n       --label 228-MENet-12x1-group-3\n```\n\nFor simplicity, we train models and save checkpoints in multi-GPU models (using `torch.nn.DataParallel`), which means the keys in the `state_dict` saved have the prefix `module.`. To convert a multi-GPU model to single-GPU model, run `convert_model.py`:\n\n```bash\npython -u convert_model.py \\\n       --data /path/to/data/config \\\n       --model /path/to/model/config \\\n       --label model_label \\\n       --input /path/to/checkpoint/file \\\n       --output /path/to/output/file\n```\n\nOur pre-trained models are single-GPU models (without prefix). To evaluate single-GPU models, run `evaluate.py`:\n\n```bash\npython -u evaluate.py \\\n       --data /path/to/data/config \\\n       --model /path/to/model/config \\\n       --checkpoint /path/to/checkpoint/file \\\n       [--print-freq N]\n```\n\n`main.py` is modified from [the pytorch example](https://github.com/pytorch/examples/blob/master/imagenet/main.py).\n\n",
      "technique": "Header extraction"
    }
  ]
}