{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1212.5701",
      "https://arxiv.org/abs/1412.6980"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9997911825949886,
        0.9903599131127802
      ],
      "excerpt": "Paper-2011 published on Journal of Machine Learning Research, cited by 3178 (It's August 5, 2018.). \nPaper-2012 on Arxiv, cited by 1960 (It's August 5, 2018.). \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/suzyi/optimization",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-08-05T02:57:38Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-15T11:17:07Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9599538610208448
      ],
      "excerpt": "Intro to numerical optimization method: papers recommendation and code implementation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9745650175024377
      ],
      "excerpt": "Deep learning models are typically trained by a stochastic gradient descent optimizer. There are many variations of stochastic gradient descent: Adam, RMSProp, Adagrad, etc. For an understanding intro to these algorithm, see here. All of them let you set the learning rate. This parameter tells the optimizer how far to move the weights in the direction of the gradient for a mini-batch. If the learning rate is low, then training is more reliable, but optimization will take a lot of time because steps towards the minimum of the loss function are tiny. If the learning rate is high, then training may not converge or even diverge. Weight changes can be so big that the optimizer overshoots the minimum and makes the loss worse. The training should start from a relatively large learning rate because, in the beginning, random weights are far from optimal, and then the learning rate can decrease during training to allow more fine-grained weight updates. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9583062284625773
      ],
      "excerpt": "Problems that can be trasferred as SOCP. For more details, see Boyd's paper \"Applications of second-Order cone \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8459633677820391,
        0.8490037945672047
      ],
      "excerpt": "paper Two-Player Games for Efficient Non-Convex Constrained Optimization \nThe convex-concave procedure (CCCP) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9827828596303774
      ],
      "excerpt": "CVXOPT is a free software package for convex optimization based on the Python programming language, developed by Martin Andersen, Joachim Dahl, and Lieven Vandenberghe. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9214976987898622
      ],
      "excerpt": "CVXPY is a Python-embedded modeling language for convex optimization problems, Steven Diamond, Eric Chu, Akshay Agrawal and Stephen Boyd. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8651168387289433
      ],
      "excerpt": "CVX is a Matlab-based modeling system for convex optimization, developed by Michael Grant and Stephen Boyd. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285057689970699,
        0.8542788241959781
      ],
      "excerpt": "Geometric programmings are special mathematical programs that can be converted to convex form using a change of variables. The convex form of GPs can be expressed as DCPs, but CVX also provides a special mode that allows a GP to be specified in its native form. CVX will automatically perform the necessary conversion, compute a numerical solution, and translate the results back to the original problem. \nOpenopt solves general (smooth and nonsmooth) nonlinear programs, including problems with integer constraints. Unlike CVXOPT, it has no software for solving semidefinite programs. The solvers were all written by Dmitrey Kroshko himself and don't have a long history, so testing was probably limited. OpenOpt itself does _not- interface to general third party solvers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060185361300068
      ],
      "excerpt": "Gurobi is an optimization software. According to wikipedia, it supports both python and matlab mainly for solving Linear Programming (LP), Quadratic Programming (QP), Quadratically constrained Programming (QCP), mixed integer linear programming (MILP), mixed-integer quadratic programming (MIQP), and mixed-integer quadratically constrained programming (MIQCP). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333031921219924
      ],
      "excerpt": "Distributed Optimization is also known as decentralized optimization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Intro to numerical optimization method: papers recommendation and code implementation.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/suzyi/optimization/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This problem [(notebook)](https://github.com/suzyi/optimization/blob/master/notebook/constrainedOpt.ipynb) appears at the inverse process of a relu DNN.\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 09:17:55 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/suzyi/optimization/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "suzyi/optimization",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/suzyi/optimization/master/notebook/nonneg_matrix_fact.ipynb",
      "https://raw.githubusercontent.com/suzyi/optimization/master/notebook/constrainedOpt.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "+ Step 1 - You'd better have cvxpy installed in a virtual environment.\n+ Step 2 - You need to have `numpy` and `scipy` in stalled in your virtual environment in advance.\n+ Step 3 - `pip install cvxpy`\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/suzyi/optimization/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "optimization",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "optimization",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "suzyi",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/suzyi/optimization/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Fri, 24 Dec 2021 09:17:55 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "| Date | codes |\n| ---- | ---- |\n| Oct 10, 2018 | [Non-negative matrix factorization](https://github.com/suzyi/optimization/blob/master/notebook/nonneg_matrix_fact.ipynb) |\n",
      "technique": "Header extraction"
    }
  ]
}