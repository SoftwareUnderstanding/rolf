{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Code borrows heavily from [DCGAN](https://github.com/soumith/dcgan.torch). The data loader is modified from [DCGAN](https://github.com/soumith/dcgan.torch) and  [Context-Encoder](https://github.com/pathak22/context-encoder).\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1611.07004"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code for your research, please cite our paper <a href=\"https://arxiv.org/pdf/1611.07004v1.pdf\">Image-to-Image Translation Using Conditional Adversarial Networks</a>:\n\n```\n@article{pix2pix2017,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  journal={CVPR},\n  year={2017}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{pix2pix2017,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  journal={CVPR},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9099845036220141
      ],
      "excerpt": "Project | Arxiv | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9878673543585983,
        0.9868512987915664
      ],
      "excerpt": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros  \n CVPR, 2017. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9615825383762427
      ],
      "excerpt": "Open this URL in your browser: http://localhost:8000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9952474852238322
      ],
      "excerpt": "If you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper Collection: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pokurin123/pix2pix_try",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-28T11:17:39Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-28T11:18:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8552078246538597
      ],
      "excerpt": "Image-to-Image Translation with Conditional Adversarial Networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9324057265461965,
        0.9693279310347549
      ],
      "excerpt": "On some tasks, decent results can be obtained fairly quickly and on small datasets. For example, to learn to generate facades (example shown above), we trained on just 400 images for about 2 hours (on a single Pascal Titan X GPU). However, for harder problems it may be important to train on far larger datasets, and for many hours or even days. \nNote: Please check out our PyTorch implementation for pix2pix and CycleGAN. The PyTorch version is under active development and can produce results comparable to or better than this Torch version. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9573305914890285
      ],
      "excerpt": "We provide a python script to generate training data in the form of pairs of images {A,B}, where A and B are two different depictions of the same underlying scene. For example, these might be pairs {label map, photo} or {bw image, color image}. Then we can learn to translate A to B or B to A: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807756918396975
      ],
      "excerpt": "Once the data is formatted this way, call: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.981594821815469
      ],
      "excerpt": "Further notes: Our pre-trained FCN model is not supposed to work on Cityscapes in the original resolution (1024x2048) as it was trained on 256x256 images that are then upsampled to 1024x2048 during training. The purpose of the resizing during training was to 1) keep the label maps in the original high resolution untouched and 2) avoid the need to change the standard FCN training code and the architecture for Cityscapes. During test time, you need to synthesize 256x256 results. Our test code will automatically upsample your results to 1024x2048 before feeding them to the pre-trained FCN model. The output is at 1024x2048 resolution and will be compared to 1024x2048 ground truth labels. You do not need to resize the ground truth labels. The best way to verify whether everything is correct is to reproduce the numbers for real images in the paper first. To achieve it, you need to resize the original/real Cityscapes images (not labels) to 256x256 and feed them to the evaluation code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9350665472344665
      ],
      "excerpt": "By default, the server listens on localhost. Pass 0.0.0.0 to allow external connections on any interface: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9558357241382083
      ],
      "excerpt": "L1 error is plotted to the display by default. Set the environment variable display_plot to a comma-separated list of values errL1, errG and errD to visualize the L1, generator, and discriminator error respectively. For example, to plot only the generator and discriminator errors to the display instead of the default L1 error, set display_plot=\"errG,errD\". \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pokurin123/pix2pix_try/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 05:18:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pokurin123/pix2pix_try/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pokurin123/pix2pix_try",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pokurin123/pix2pix_try/master/scripts/eval_cityscapes/download_fcn8s.sh",
      "https://raw.githubusercontent.com/pokurin123/pix2pix_try/master/datasets/download_dataset.sh",
      "https://raw.githubusercontent.com/pokurin123/pix2pix_try/master/models/download_model.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8917133644880719
      ],
      "excerpt": "bash ./datasets/download_dataset.sh dataset_name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8772628310207694
      ],
      "excerpt": "bash ./models/download_model.sh model_name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414119019616536,
        0.9663744104549803
      ],
      "excerpt": "bash ./scripts/eval_cityscapes/download_fcn8s.sh \nThen make sure ./scripts/eval_cityscapes/ is in your system's python path. If not, run the following command to add it \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.966050761734001,
        0.8339092128974351
      ],
      "excerpt": "Now you can run the following command to evaluate your predictions: \npython ./scripts/eval_cityscapes/evaluate.py --cityscapes_dir /path/to/original/cityscapes/dataset/ --result_dir /path/to/your/predictions/ --output_dir /path/to/output/directory/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.970409074371173
      ],
      "excerpt": "Install it with: luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9449444451460599
      ],
      "excerpt": "<img src=\"imgs/examples.jpg\" width=\"900px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9215401035499162
      ],
      "excerpt": "DATA_ROOT=/path/to/data/ name=expt_name which_direction=AtoB th train.lua \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8166317608570164
      ],
      "excerpt": "See opt in train.lua for additional training options. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9088023534976722,
        0.8033736798720692
      ],
      "excerpt": "DATA_ROOT=/path/to/data/ name=expt_name which_direction=AtoB phase=val th test.lua \nThis will run the model named expt_name in direction AtoB on all images in /path/to/data/val. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8087478602986792
      ],
      "excerpt": "See opt in test.lua for additional testing options. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802869318621687
      ],
      "excerpt": "- facades_label2image (label -> facade): trained on the CMP Facades dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8875489604619244,
        0.8967576975011664
      ],
      "excerpt": "Create folder /path/to/data with subfolders A and B. A and B should each have their own subfolders train, val, test, etc. In /path/to/data/A/train, put training images in style A. In /path/to/data/B/train, put the corresponding images in style B. Repeat same for other data splits (val, test, etc). \nCorresponding images in a pair {A,B} must be the same size and have the same filename, e.g., /path/to/data/A/train/1.jpg is considered to correspond to /path/to/data/B/train/1.jpg. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003972563035415
      ],
      "excerpt": "python scripts/combine_A_and_B.py --fold_A /path/to/data/A --fold_B /path/to/data/B --fold_AB /path/to/data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.857432110517551,
        0.9035851527100461
      ],
      "excerpt": "python ./scripts/eval_cityscapes/evaluate.py --cityscapes_dir /path/to/original/cityscapes/dataset/ --result_dir /path/to/your/predictions/ --output_dir /path/to/output/directory/ \nImages stored under --result_dir should contain your model predictions on the Cityscapes validation split, and have the original Cityscapes naming convention (e.g., frankfurt_000001_038418_leftImg8bit.png). The script will output a text file under --output_dir containing the metric. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pokurin123/pix2pix_try/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Lua",
      "Python",
      "MATLAB",
      "TeX",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/pokurin123/pix2pix_try/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2016, Phillip Isola and Jun-Yan Zhu\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\n\\n\\n----------------------------- LICENSE FOR DCGAN --------------------------------\\nBSD License\\n\\nFor dcgan.torch software\\n\\nCopyright (c) 2015, Facebook, Inc. All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\\n\\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n\\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n\\nNeither the name Facebook nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "pix2pix",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pix2pix_try",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pokurin123",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pokurin123/pix2pix_try/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Linux or OSX\n- NVIDIA GPU + CUDA CuDNN (CPU mode and CUDA without CuDNN may work with minimal modification, but untested)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 05:18:30 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Install torch and dependencies from https://github.com/torch/distro\n- Install torch packages `nngraph` and `display`\n```bash\nluarocks install nngraph\nluarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec\n```\n- Clone this repo:\n```bash\ngit clone git@github.com:phillipi/pix2pix.git\ncd pix2pix\n```\n- Download the dataset (e.g., [CMP Facades](http://cmp.felk.cvut.cz/~tylecr1/facade/)):\n```bash\nbash ./datasets/download_dataset.sh facades\n```\n- Train the model\n```bash\nDATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA th train.lua\n```\n- (CPU only) The same training command without using a GPU or CUDNN. Setting the environment variables ```gpu=0 cudnn=0``` forces CPU only\n```bash\nDATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA gpu=0 cudnn=0 batchSize=10 save_epoch_freq=5 th train.lua\n```\n- (Optionally) start the display server to view results as the model trains. ( See [Display UI](#display-ui) for more details):\n```bash\nth -ldisplay.start 8000 0.0.0.0\n```\n\n- Finally, test the model:\n```bash\nDATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA phase=val th test.lua\n```\nThe test results will be saved to an html file here: `./results/facades_generation/latest_net_G_val/index.html`.\n\n",
      "technique": "Header extraction"
    }
  ]
}