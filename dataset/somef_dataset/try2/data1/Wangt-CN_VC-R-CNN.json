{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I really appreciate [**Kaihua Tang**](https://kaihuatang.github.io/), **[Yulei Niu](https://yuleiniu.github.io/)**, **[Xu Yang](https://scholar.google.com.sg/citations?user=SqdxMH0AAAAJ&hl=zh-CN)**, **Jiaxin Qi**, **Xinting Hu** and [**Dong Zhang**](https://zhangdong-njust.github.io/) for their greatly helpful advice and lending me GPUs! \n\n\nIf you have any questions or concerns, please kindly email to [**Tan Wang**](https://wangt-cn.github.io/).\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2002.12204",
      "https://arxiv.org/abs/2002.12204",
      "https://arxiv.org/abs/1911.05722"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{wang2020visual,\n  title={Visual commonsense r-cnn},\n  author={Wang, Tan and Huang, Jianqiang and Zhang, Hanwang and Sun, Qianru},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={10760--10770},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9997401479214306,
        0.9999999876730642,
        0.9786332573762893,
        0.9947639938064143
      ],
      "excerpt": "Tan Wang, Jianqiang Huang, Hanwang Zhang, Qianru Sun <br /> \nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020 <br /> \nKey Words: &nbsp;Causal Intervention; &nbsp;Visual Common Sense; &nbsp;Representation Learning <br /> \n[Paper], [Zhihu Article], [15min Slides], [Video] <br /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8098992205766328
      ],
      "excerpt": "  <img src=\"https://github.com/Wangt-CN/Wangt-CN.github.io/blob/master/project/vc-rcnn/framework_github.png\" width=\"600px\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999955955071949
      ],
      "excerpt": "If you find our VC feature and code helpful, please kindly consider citing: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9202222245543838
      ],
      "excerpt": "Downstream Vision & Language Tasks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090016440670298
      ],
      "excerpt": "class MyDataset(object): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9783595426586629
      ],
      "excerpt": "    boxes = [[0, 0, 10, 10], [10, 20, 50, 50]] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "    labels = torch.tensor([10, 20]) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Wangt-CN/VC-R-CNN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-19T15:06:14Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-26T13:43:09Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8958613258025176
      ],
      "excerpt": "[NEW]: We have provided the training code of VC R-CNN and detailed readme file. :star2: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9908909204298771,
        0.8422835983243784
      ],
      "excerpt": "This repository contains the official PyTorch implementation and the proposed VC feature for CVPR 2020 Paper \"Visual Commonsense R-CNN\". For technical details, please refer to: \nVisual Commonsense R-CNN <br /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8285738980269748
      ],
      "excerpt": "VC R-CNN Framework \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8700851147740756
      ],
      "excerpt": "This project aims to build a visual commonsense representation learning framework based on the current object detection codebase with un-/self-supervised learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9492068218097306,
        0.9532926778549292,
        0.9728732951747976,
        0.8157401484328949,
        0.9634899959958502
      ],
      "excerpt": "Effective: Our visual commonsense representation encodes the``sense-making'' knowledge between object RoIs with causal intervention rather than just trivial correlation prediction. Compared to the previous widely used Up-Down Feature, our VC can be regarded as an effective supplementary knowledge that models the interaction between objects for the downstream tasks.  \nEasy to Use: As we introduced in our paper, the VC Feature is extracted by providing the RoI boxes coordinates. Then the VC Feature can be just concatenated on the previous visual object features (e.g., Up-Down Feature) and ready to roll.  (Ps: the concatenation maybe too simple for some cases or tasks, users can try something else and welcome feedback.) \nEasy to Expand: With a learned VC R-CNN framework, we can easily extract VC Features for any images and prepare them as an ``augmentation feature'' for the currently used representation conveniently. \nVC R-CNN \nFast, Memory-efficient, Multi-GPU: Our VC R-CNN framework is based on the well-known maskrcnn-benchmark from facebook. Therefore, our VC R-CNN just inherit all its advantages. (It's pity that the detectron2 had not been released when I am working on this project, however maskrcnn-benchmark can be a stable version.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9757446984250079
      ],
      "excerpt": "For easy-to-use, here we directly provide the pretrained VC Features on the entire MSCOCO dataset based on the Up-Down feature's boxes in the below links (The link is updated). The features are stored in tar.gz format. The previous features can be found in OLD_FEATURE. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9210232872891033,
        0.9214110076236798,
        0.9485460861952648
      ],
      "excerpt": "Please check Downstream Tasks for more details: \nSome tips for using in downstream tasks \nWe recommend users to add the dimension of the start multi-layers (embedding layer, fc and so on) in the downstream networks since the feature size add from 2048 to 3072 (for Up-Down Feature). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9578783415685158,
        0.8577942577817597
      ],
      "excerpt": "We find the self-attentive operation on feature (e.g., the refining encoder in AoANet) may hurt the effectiveness of our VC Feature. Details can be kindly found at the bottom of Page 7 in our paper. \nThe concatenation of Up-Down and VC Feature maybe too simple for some downstream tasks. It can be regarded as a baseline and I believe there would be more potential on VC Feature.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9450725125735105,
        0.8840053371608262,
        0.9788867955265352
      ],
      "excerpt": "Parameters about VC: They are in the end of default.py with annotations. Users can make changes according to their own situation. \n1. Using your own model \nSince the goal of our VC R-CNN is to train the visual commonsense representations by self-supervised learning, we have no metrics for evaluation and we treat it as the feature extraction process. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8880087446242839
      ],
      "excerpt": "2. Using our pretrained VC model on COCO \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8521402394218456
      ],
      "excerpt": "For learning VC Feature on your own dataset, the crux is to make your own dataset COCO-style (can refer to the data format in detection task) and design the dataloader file, for example coco.py and openimages.py. Here we provide an example for reference. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8774325842748769
      ],
      "excerpt": "    #: add the labels to the boxlist \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9331646658337033
      ],
      "excerpt": "    #: Here you can also add many other characters to the boxlist in addition to the labels, for example `image_id', `category_id' and so on. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8593993976119897
      ],
      "excerpt": "    #: return the image, the boxlist and the idx in your dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9165259544213658,
        0.9385084359224708
      ],
      "excerpt": "    #: we want to split the batches according to the aspect ratio \n    #: of the image, as it can be more efficient than loading the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9004816733994234
      ],
      "excerpt": "vc_rcnn/data/datasets/__init__.py: add it to __all__ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8597209804970991,
        0.9192864078081258
      ],
      "excerpt": "2. Extracting features of customized dataset \nRecall that with the trained VC R-CNN, we can directly extract VC Features given raw images and bounding box coordinates. Therefore, the method to design dataloader is similar to the above. The only difference is you may want to load box coordinates file for feature extraction and the labels, classes is unnecessary. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8483254458787244,
        0.9384875692130845,
        0.9273651572480587
      ],
      "excerpt": "3. Some Tips and Traps \nAs our experiment results shown in paper, training our VC R-CNN on a larger dataset cannot bring much gain to the downstream tasks on other datasets. The probable reason maybe the COCO is enough to learn the commonsense feature for its downstream tasks. Therefore we suggest users: if you want to perform downstream tasks on Dataset A, you can firstly train our VC on the Dataset A.  \nWhen you design the Dataloader file, the most important thing is to pay attention to the box format (XYXY or XYWH) and adopt the correct command to load them. I have made this mistakes at the beginning of my project. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9795239786102136,
        0.8352652489943436,
        0.9828903513314741,
        0.9521030508333191
      ],
      "excerpt": "Here we provide our experience (mainly the failure 2333) in training our VC R-CNN and using VC Feature. And we hope it can provide some help or possible ideas to the users to further develop this field :) \n1. For Training VC R-CNN \nAfter reading the paper MoCo: Momentum Contrast for Unsupervised Visual Representation Learning by Kaiming He, I have tried to construct a better dictionary learning scheme for our VC Feature self-supervised learning. In our current implementation, the dictionary keep constant during training and we want to borrow the idea from MoCo to UPDATE the confounder dictionary iteratively. Since the iterative step (How often is the dictionary updated) can be set arbitrarily, I have tried a few steps but the result is similar. We want to further explore this in our future work. \nWe have tried to add an 'Observation Window' into data sampling, which means for each image we just sample a window contains, for example 10 objects randomly each time. We want the model can learn the latent spatial relationship at the same time, however, the results can be worse.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9423131589182477,
        0.9711345255486334,
        0.9459443881307497
      ],
      "excerpt": "As we discussed in our paper, our VC Feature achieves a less significant gain on VQA task than that for image captioning. We thought the possible reason can be the limited ability of the current question understanding. We are also wondering if we can train the Vision & Language Commonsense Representations in the future. \nIn all the downstream tasks, we just concatenate the VC Feature on the previous Up-Down feature. This operation maybe too simple and I believe it does NOT reach VC 's full potential. My own bandwidth is limited but I know if more researchers try to use it and design more suitable downstream models, maybe we can create more better results :) \nThe evaluation for the feature (self-supervised learning) can be too trivial and hard. This is the problem for all the self-supervised learning problem. The model performance cannot be estimated in training procedure. And if the things you want to learn is the feature (just like us), you need to evaluate it on many downstream tasks. Therefore, how to find a more effective way to evaluate the learning process can be a good point for research. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "[CVPR 2020] The official pytorch implementation of ``Visual Commonsense R-CNN''",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Unzip the file with command:\n```bash\ntar -xzvf file_name\n```\n\n- The feature format (The shape of each numpy file is [n x 1024]):\n```\ncoco_trainval/test_year\n |---image_id1.npy\n |---image_id2.npy\n  ...\n |---image_idN.npy\n```\n\n- Concatenate on the previous feature in the downstream task training.\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Wangt-CN/VC-R-CNN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 54,
      "date": "Mon, 27 Dec 2021 00:49:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Wangt-CN/VC-R-CNN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Wangt-CN/VC-R-CNN",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Wangt-CN/VC-R-CNN/master/docker/Dockerfile",
      "https://raw.githubusercontent.com/Wangt-CN/VC-R-CNN/master/docker/docker-jupyter/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Wangt-CN/VC-R-CNN/master/install.sh",
      "https://raw.githubusercontent.com/Wangt-CN/VC-R-CNN/master/downstream/AoANet_VC/test-last.sh",
      "https://raw.githubusercontent.com/Wangt-CN/VC-R-CNN/master/downstream/AoANet_VC/train.sh",
      "https://raw.githubusercontent.com/Wangt-CN/VC-R-CNN/master/downstream/AoANet_VC/test-best.sh",
      "https://raw.githubusercontent.com/Wangt-CN/VC-R-CNN/master/downstream/AoANet_VC/scripts/copy_model.sh",
      "https://raw.githubusercontent.com/Wangt-CN/VC-R-CNN/master/downstream/AoANet_VC/vis/test-last.sh",
      "https://raw.githubusercontent.com/Wangt-CN/VC-R-CNN/master/downstream/AoANet_VC/vis/test-best.sh",
      "https://raw.githubusercontent.com/Wangt-CN/VC-R-CNN/master/downstream/AoANet_VC/vis/train-wo-refining.sh",
      "https://raw.githubusercontent.com/Wangt-CN/VC-R-CNN/master/downstream/MCAN_VC/setup.sh",
      "https://raw.githubusercontent.com/Wangt-CN/VC-R-CNN/master/downstream/Up-Down_VC/scripts/copy_model.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. First, you need to download the COCO dataset and annotations. We assume that you save them in `/path_to_COCO_dataset/`\n2. Then you need modify the path in `vc_rcnn/config/paths_catalog.py`, containing the `DATA_DIR` and `DATASETS path`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Please check [INSTALL.md](INSTALL.md) for installation instructions.\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8066315816677659
      ],
      "excerpt": "What can you get from this repo? [The Road Map] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8992916411128484
      ],
      "excerpt": "Pretrained on COCO \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.922389731690886
      ],
      "excerpt": "Specifically, you can just run the following command to achieve the features. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9592996655477675
      ],
      "excerpt": "Then, you need modify the following files: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8677234995849012
      ],
      "excerpt": "default.py: OUTPUT_DIR denotes the model output dir. TENSORBOARD_EXPERIMENT is the tensorboard loger output dir. Another parameter the user may need notice is the SOLVER.IMS_PER_BATCH which denotes the number of total images per batch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8940172369164078
      ],
      "excerpt": "python -m torch.distributed.launch --nproc_per_node=$NGPUS tools/test_net.py --config-file \"path/to/config/file.yaml\" TEST.IMS_PER_BATCH images_per_gpu x $GPUS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8940172369164078,
        0.8006903047238545
      ],
      "excerpt": "python -m torch.distributed.launch --nproc_per_node=$NGPUS tools/test_net.py --config-file \"path/to/config/file.yaml\" TEST.IMS_PER_BATCH images_per_gpu x $GPUS \n1. Training on customized dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8870758199028167
      ],
      "excerpt": "vc_rcnn/data/datasets/__init__.py: add it to __all__ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Wangt-CN/VC-R-CNN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Shell",
      "HTML",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Ruotian(RT) Luo\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Visual Commonsense R-CNN (VC R-CNN)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "VC-R-CNN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Wangt-CN",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Wangt-CN/VC-R-CNN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Most of the configuration files that we provide assume that we are running 2 images on each GPU with 8 GPUs. In order to be able to run it on fewer GPUs, there are a few possibilities: \n\n**1. Single GPU Training:** \nModify the cfg parameters. Here is an example:\n\n```bash\npython tools/train_net.py --config-file \"configs/e2e_mask_rcnn_R_101_FPN_1x.yaml\" --skip-test SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0025 SOLVER.MAX_ITER 720000 SOLVER.STEPS \"(480000, 640000)\" TEST.IMS_PER_BATCH 1 MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN 2000\n```\n\nPs: To running more images on one GPU, you can refer to the [maskrcnn-benchmark].\n\n**2. Multi-GPU training:**\nThe maskrcnn-benchmark directly support the multi-gpu training with `torch.distributed.launch`. You can run the command like (you need change $NGPUS to the num of GPU you use):\n\n```bash\npython -m torch.distributed.launch --nproc_per_node=$NGPUS tools/train_net.py --config-file \"path/to/config/file.yaml\" --skip-test MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN images_per_gpu x 1000\n```\n\n**Notes**: \n\n- In our experiments, we adopted `e2e_mask_rcnn_R_101_FPN_1x.yaml` **without the Mask Branch** (set False) as our config file.\n- When training VC, actually we need not test scripts, thus we set `--skip-test` to skip the test process after training. The test script is used to extract vc feature. Or if you design your own test, you can remove `--skip-test`.\n- The `MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN` denotes that the proposals are selected for per the batch rather than per image in the default training. The value is calculated by **1000 x images-per-gpu**. Here we have 2 images per GPU, therefore we set the number as 1000 x 2 = 2000. If we have 8 images per GPU, the value should be set as 8000. See [#672@maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark/issues/672) for more details.\n- Please note that the learning rate & iteration change rule follows the [scheduling rules from Detectron](https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14-L30), which means the lr need to be set 2x if the number of GPUs become 2x. In our methods, the learning rate is set for 4 GPUs and each GPU has 2 images.\n- In my practice, the learning rate can not be best customized since the VC training is not a supervised model and you cannot measure the goodness of the VC model from training procedure. We have provide a general suitable learning rate and you can make some slight modification. \n- You can turn on the **Tensorboard** logger by add `--use-tensorboard` into command (Need to install tensorflow and tensorboardx first).\n- The confounder dictionary `dic_coco.npy` and the prior `stat_prob.npy` are in the [tools](tools).\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 305,
      "date": "Mon, 27 Dec 2021 00:49:31 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- **I want to use your VC Feature pretrained on COCO:** \n  - Download the [VC Feature on COCO](#vc-feature) and concatenate it on Up-Down feature for usage.\n  - You can also try other methods to use the VC Feature rather than just concatenation.\n- **I want to retrain your VC R-CNN on COCO:**\n  - [Perform Training on COCO Dataset](#perform-training-on-coco-dataset)\n- **I want to train the VC R-CNN on my own dataset and extract VC Features:**\n  - [Add your Customized Dataset](#add-your-customized-dataset)\n  - [Perform Training on COCO Dataset](#perform-training-on-coco-dataset)\n  - [Evaluation (Feature Extraction)](#evaluation-feature-extraction)\n\n</br>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Unzip the file with command:\n```bash\ntar -xzvf file_name\n```\n\n- The feature format (The shape of each numpy file is [n x 1024]):\n```\ncoco_trainval/test_year\n |---image_id1.npy\n |---image_id2.npy\n  ...\n |---image_idN.npy\n```\n\n- Concatenate on the previous feature in the downstream task training.\n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}