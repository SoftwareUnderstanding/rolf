{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1506.02438",
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1509.02971",
      "https://arxiv.org/abs/1804.10332",
      "https://arxiv.org/abs/1707.06347.\n- [Approximately optimal approximate reinforcement learning](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwjPytbijb_eAhUZfnAKHfAtDzEQFjAAegQICRAC&url=https%3A%2F%2Fpeople.eecs.berkeley.edu%2F~pabbeel%2Fcs287-fa09%2Freadings%2FKakadeLangford-icml2002.pdf&usg=AOvVaw1lMj6AB90nJbKT-5pNcQ9P)\n- [Reinforcement Learning: An Introduction](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=2ahUKEwiT_9bN-77eAhXYdXAKHetgAiQQFjABegQIBRAC&url=http%3A%2F%2Fincompleteideas.net%2Fbook%2Fbookdraft2017nov5.pdf&usg=AOvVaw00kFmqVbFSdkU3PTkJMJrO)\n  - Chapter 13 \nPolicy Gradient Methods\n  - 13.2 The Policy Gradient Theorem\n  - 13.3 REINFORCE: Monte Carlo Policy Gradient\n  - 13.4 REINFORCE with Baseline\n  - 13.5 Actor\u2013Critic Methods\n- [Understanding RL: The Bellman Equations](https://joshgreaves.com/reinforcement-learning/understanding-rl-the-bellman-equations/)\n- Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., & Riedmiller, M. (2014, June). [Deterministic policy gradient algorithms.](http://www.jmlr.org/proceedings/papers/v32/silver14.pdf) In ICML, 2014.\n- Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., ... & Wierstra, D. (2015). [Continuous control with deep reinforcement learning.](https://arxiv.org/abs/1509.02971) arXiv preprint https://arxiv.org/abs/1509.02971.\n- [OpenAI Gym \u5165\u9580](https://qiita.com/ishizakiiii/items/75bc2176a1e0b65bdd16)\n- [[Python] Keras-RL\u3067\u7c21\u5358\u306b\u5f37\u5316\u5b66\u7fd2(DQN)\u3092\u8a66\u3059](https://qiita.com/inoory/items/e63ade6f21766c7c2393)\n- [OpenAI Gym\u3067FX\u306e\u30c8\u30ec\u30fc\u30c7\u30a3\u30f3\u30b0\u74b0\u5883\u3092\u69cb\u7bc9\u3059\u308b](https://qiita.com/hide-tono/items/bb9691477831e48f0989)\n- Tan, J., Zhang, T., Coumans, E., Iscen, A., Bai, Y., Hafner, D., & Vanhoucke, V. (2018).\n  [Sim-to-Real: Learning Agile Locomotion For Quadruped Robots.](https://arxiv.org/abs/1804.10332) arXiv \n  preprint https://arxiv.org/abs/1804.10332.\n\n## Progress\n- baselines\u306b\u3088\u308b\u52d5\u4f5c\u306f\u30d0\u30b0\u306e\u305f\u3081\u5931\u6557\u3002\n  `TypeError: learn() missing 1 required positional argument: 'network'`\n  \u3068\u3044\u3046\u30a8\u30e9\u30fc\u3002\n- [Tensorflow agents PPO](https://github.com/google-research/batch-ppo)\u306b\u3088\u308b\n\u52d5\u4f5c\u78ba\u8a8d\u306f\u3067\u304d\u305f\u3002\u305f\u3060\u3057\u8a13\u7df4\u306e\u307f\u3002\u8b66\u544a\u304c\u5927\u91cf\u306b\u8868\u793a\u3055\u308c\u308b\u306e\u3067\u6d88\u3057\u305f\u3044\u3002 `pendulum` \u3068\u3044\u3046\n\u540d\u524d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u4f5c\u6210\u3055\u308c\u308b\u3002Configuration\u306f `pybullet_envs/agents/configs.py`\n\u306e\u4e2d\u3067\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u3002",
      "https://arxiv.org/abs/1509.02971.\n- [OpenAI Gym \u5165\u9580](https://qiita.com/ishizakiiii/items/75bc2176a1e0b65bdd16)\n- [[Python] Keras-RL\u3067\u7c21\u5358\u306b\u5f37\u5316\u5b66\u7fd2(DQN)\u3092\u8a66\u3059](https://qiita.com/inoory/items/e63ade6f21766c7c2393)\n- [OpenAI Gym\u3067FX\u306e\u30c8\u30ec\u30fc\u30c7\u30a3\u30f3\u30b0\u74b0\u5883\u3092\u69cb\u7bc9\u3059\u308b](https://qiita.com/hide-tono/items/bb9691477831e48f0989)\n- Tan, J., Zhang, T., Coumans, E., Iscen, A., Bai, Y., Hafner, D., & Vanhoucke, V. (2018).\n  [Sim-to-Real: Learning Agile Locomotion For Quadruped Robots.](https://arxiv.org/abs/1804.10332) arXiv \n  preprint https://arxiv.org/abs/1804.10332.\n\n## Progress\n- baselines\u306b\u3088\u308b\u52d5\u4f5c\u306f\u30d0\u30b0\u306e\u305f\u3081\u5931\u6557\u3002\n  `TypeError: learn() missing 1 required positional argument: 'network'`\n  \u3068\u3044\u3046\u30a8\u30e9\u30fc\u3002\n- [Tensorflow agents PPO](https://github.com/google-research/batch-ppo)\u306b\u3088\u308b\n\u52d5\u4f5c\u78ba\u8a8d\u306f\u3067\u304d\u305f\u3002\u305f\u3060\u3057\u8a13\u7df4\u306e\u307f\u3002\u8b66\u544a\u304c\u5927\u91cf\u306b\u8868\u793a\u3055\u308c\u308b\u306e\u3067\u6d88\u3057\u305f\u3044\u3002 `pendulum` \u3068\u3044\u3046\n\u540d\u524d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u4f5c\u6210\u3055\u308c\u308b\u3002Configuration\u306f `pybullet_envs/agents/configs.py`\n\u306e\u4e2d\u3067\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u3002",
      "https://arxiv.org/abs/1804.10332.\n\n## Progress\n- baselines\u306b\u3088\u308b\u52d5\u4f5c\u306f\u30d0\u30b0\u306e\u305f\u3081\u5931\u6557\u3002\n  `TypeError: learn() missing 1 required positional argument: 'network'`\n  \u3068\u3044\u3046\u30a8\u30e9\u30fc\u3002\n- [Tensorflow agents PPO](https://github.com/google-research/batch-ppo)\u306b\u3088\u308b\n\u52d5\u4f5c\u78ba\u8a8d\u306f\u3067\u304d\u305f\u3002\u305f\u3060\u3057\u8a13\u7df4\u306e\u307f\u3002\u8b66\u544a\u304c\u5927\u91cf\u306b\u8868\u793a\u3055\u308c\u308b\u306e\u3067\u6d88\u3057\u305f\u3044\u3002 `pendulum` \u3068\u3044\u3046\n\u540d\u524d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u4f5c\u6210\u3055\u308c\u308b\u3002Configuration\u306f `pybullet_envs/agents/configs.py`\n\u306e\u4e2d\u3067\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u3002"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)\n- Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015, June). [Trust Region Policy Optimization.](http://proceedings.mlr.press/v37/schulman15.pdf) In ICML, 2015\n- Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). [Proximal policy optimization algorithms.](https://arxiv.org/abs/1707.06347) arXiv preprint arXiv:1707.06347.\n- [Approximately optimal approximate reinforcement learning](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwjPytbijb_eAhUZfnAKHfAtDzEQFjAAegQICRAC&url=https%3A%2F%2Fpeople.eecs.berkeley.edu%2F~pabbeel%2Fcs287-fa09%2Freadings%2FKakadeLangford-icml2002.pdf&usg=AOvVaw1lMj6AB90nJbKT-5pNcQ9P)\n- [Reinforcement Learning: An Introduction](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=2ahUKEwiT_9bN-77eAhXYdXAKHetgAiQQFjABegQIBRAC&url=http%3A%2F%2Fincompleteideas.net%2Fbook%2Fbookdraft2017nov5.pdf&usg=AOvVaw00kFmqVbFSdkU3PTkJMJrO)\n  - Chapter 13 \nPolicy Gradient Methods\n  - 13.2 The Policy Gradient Theorem\n  - 13.3 REINFORCE: Monte Carlo Policy Gradient\n  - 13.4 REINFORCE with Baseline\n  - 13.5 Actor\u2013Critic Methods\n- [Understanding RL: The Bellman Equations](https://joshgreaves.com/reinforcement-learning/understanding-rl-the-bellman-equations/)\n- Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., & Riedmiller, M. (2014, June). [Deterministic policy gradient algorithms.](http://www.jmlr.org/proceedings/papers/v32/silver14.pdf) In ICML, 2014.\n- Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., ... & Wierstra, D. (2015). [Continuous control with deep reinforcement learning.](https://arxiv.org/abs/1509.02971) arXiv preprint arXiv:1509.02971.\n- [OpenAI Gym \u5165\u9580](https://qiita.com/ishizakiiii/items/75bc2176a1e0b65bdd16)\n- [[Python] Keras-RL\u3067\u7c21\u5358\u306b\u5f37\u5316\u5b66\u7fd2(DQN)\u3092\u8a66\u3059](https://qiita.com/inoory/items/e63ade6f21766c7c2393)\n- [OpenAI Gym\u3067FX\u306e\u30c8\u30ec\u30fc\u30c7\u30a3\u30f3\u30b0\u74b0\u5883\u3092\u69cb\u7bc9\u3059\u308b](https://qiita.com/hide-tono/items/bb9691477831e48f0989)\n- Tan, J., Zhang, T., Coumans, E., Iscen, A., Bai, Y., Hafner, D., & Vanhoucke, V. (2018).\n  [Sim-to-Real: Learning Agile Locomotion For Quadruped Robots.](https://arxiv.org/abs/1804.10332) arXiv \n  preprint arXiv:1804.10332.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "$ source pybullet-env/bin/activate \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/taku-y/20181125-pybullet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-11T01:43:09Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-23T08:05:05Z",
    "technique": "GitHub API"
  },
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/taku-y/20181125-pybullet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 28 Dec 2021 03:55:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/taku-y/20181125-pybullet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "taku-y/20181125-pybullet",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Python 3.7 have a problem when installing tensorflow \n(https://github.com/tensorflow/tensorflow/issues/20444).\n\n```bash\n#: See https://apple.stackexchange.com/questions/329187\n$ brew install \\\n  https://raw.githubusercontent.com/Homebrew/homebrew-core/\\\n  f2a764ef944b1080be64bd88dca9a1d80130c558/Formula/python.rb\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "$ cd $WORKDIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8799912836851375,
        0.9967777177014457,
        0.999746712887969,
        0.9893272198983933,
        0.9906248903846466,
        0.999746712887969,
        0.9906248903846466,
        0.999746712887969,
        0.9853596540239681,
        0.9313313324161989
      ],
      "excerpt": "$ source pybullet-env/bin/activate \n$ pip install tensorflow \n$ pip install gym \n$ git clone https://github.com/openai/baselines.git \n$ cd baselines \n$ pip install -e . \n$ cd .. \n$ pip install pybullet \n$ pip install ruamel-yaml \n$ cd pybullet-env/lib/python3.6/site-packages/pybullet_envs/examples \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "$ python kukaGymEnvTest.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8284215435398932
      ],
      "excerpt": "\u540d\u524d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u4f5c\u6210\u3055\u308c\u308b\u3002Configuration\u306f pybullet_envs/agents/configs.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/taku-y/20181125-pybullet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "20181125-pybullet",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "20181125-pybullet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "taku-y",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/taku-y/20181125-pybullet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 28 Dec 2021 03:55:02 GMT"
    },
    "technique": "GitHub API"
  }
}