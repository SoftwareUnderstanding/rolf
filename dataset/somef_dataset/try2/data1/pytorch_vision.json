{
  "citation": [
    {
      "confidence": [
        0.895067659672329
      ],
      "excerpt": ".. image:: https://pepy.tech/badge/torchvision \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9853813194532899,
        0.9367093089033924
      ],
      "excerpt": ".. image:: https://img.shields.io/badge/dynamic/json.svg?label=docs&url=https%3A%2F%2Fpypi.org%2Fpypi%2Ftorchvision%2Fjson&query=%24.info.version&colorB=brightgreen&prefix=v \n    :target: https://pytorch.org/vision/stable/index.html \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": ".. _Pillow-SIMD : https://github.com/uploadcare/pillow-simd \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8283216015784888,
        0.9278824608274014
      ],
      "excerpt": ".. _libjpeg: http://ijg.org/ \n.. _libjpeg-turbo: https://libjpeg-turbo.org/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9747762585923935
      ],
      "excerpt": ".. _pyav : https://github.com/PyAV-Org/PyAV \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/pytorch/vision/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pytorch/vision",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to Torchvision\nWe want to make contributing to this project as easy and transparent as possible.\nTL;DR\nWe appreciate all contributions. If you are interested in contributing to Torchvision, there are many ways to help out. \nYour contributions may fall into the following categories:\n\n\nIt helps the project if you could \n\nReport issues you're facing\nGive a :+1: on issues that others reported and that are relevant to you \n\n\n\nAnswering queries on the issue tracker, investigating bugs are very valuable contributions to the project.\n\n\nYou would like to improve the documentation. This is no less important than improving the library itself! \nIf you find a typo in the documentation, do not hesitate to submit a GitHub pull request.\n\n\nIf you would like to fix a bug\n\nplease pick one from the list of open issues labelled as \"help wanted\"\ncomment on the issue that you want to work on this issue\nsend a PR with your fix, see below. \n\n\n\nIf you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us.\n\n\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nDevelopment installation\nInstall PyTorch Nightly\n```bash\nconda install pytorch -c pytorch-nightly\nor with pip (see https://pytorch.org/get-started/locally/)\npip install numpy\npip install --pre torch -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html\n```\nInstall Torchvision\n```bash\ngit clone https://github.com/pytorch/vision.git\ncd vision\npython setup.py develop\nor, for OSX\nMACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py develop\nfor C++ debugging, please use DEBUG=1\nDEBUG=1 python setup.py develop\npip install flake8 typing mypy pytest pytest-mock scipy\nYou may also have to install `libpng-dev` and `libjpeg-turbo8-dev` libraries:bash\nconda install libpng jpeg\n```\nDevelopment Process\nIf you plan to modify the code or documentation, please follow the steps below:\n\nFork the repository and create your branch from main.\nIf you have modified the code (new feature or bug-fix), please add unit tests.\nIf you have changed APIs, update the documentation. Make sure the documentation builds.\nEnsure the test suite passes.\nMake sure your code passes the formatting checks (see below).\n\nFor more details about pull requests, \nplease read GitHub's guides. \nIf you would like to contribute a new model, please see here.\nIf you would like to contribute a new dataset, please see here. \nCode formatting and typing\nFormatting\nThe torchvision code is formatted by black,\nand checked against pep8 compliance with flake8.\nInstead of relying directly on black however, we rely on\nufmt, for compatibility reasons with Facebook\ninternal infrastructure.\nTo format your code, install ufmt with pip install ufmt and use e.g.:\nbash\nufmt format torchvision\nFor the vast majority of cases, this is all you should need to run. For the\nformatting to be a bit faster, you can also choose to only apply ufmt to the\nfiles that were edited in your PR with e.g.:\nbash\nufmt format `git diff main --name-only`\nSimilarly, you can check for flake8 errors with flake8 torchvision, although\nthey should be fairly rare considering that most of the errors are automatically\ntaken care of by ufmt already.\nPre-commit hooks\nFor convenience and purely optionally, you can rely on pre-commit\nhooks which will run both ufmt and flake8 prior to\nevery commit.\nFirst install the pre-commit package with pip install pre-commit, and then\nrun pre-commit install at the root of the repo for the hooks to be set up -\nthat's it.\nFeel free to read the pre-commit docs to learn\nmore and improve your workflow. You'll see for example that pre-commit run\n--all-files will run both ufmt and flake8 without the need for you to\ncommit anything, and that the --no-verify flag can be added to git commit to\ntemporarily deactivate the hooks.\nType annotations\nThe codebase has type annotations, please make sure to add type hints if required. We use mypy tool for type checking:\nbash\nmypy --config-file mypy.ini\nUnit tests\nIf you have modified the code by adding a new feature or a bug-fix, please add unit tests for that. To run a specific \ntest: \n```bash\npytest test/<test-module.py> -vvv -k <test_myfunc>\ne.g. pytest test/test_transforms.py -vvv -k test_center_crop\n```\nIf you would like to run all tests:\nbash\npytest test -vvv \nTests that require internet access should be in\ntest/test_internet.py.\nDocumentation\nTorchvision uses Google style\nfor formatting docstrings. Length of line inside docstrings block must be limited to 120 characters.\nPlease, follow the instructions to build and deploy the documentation locally.\nInstall requirements\nbash\ncd docs\npip install -r requirements.txt\nBuild\nbash\ncd docs\nmake html\nThen open docs/build/html/index.html in your favorite browser.\nThe docs are also automatically built when you submit a PR. The job that\nbuilds the docs is named build_docs. You can access the rendered docs by\nclicking on that job and then going to the \"Artifacts\" tab.\nYou can clean the built docs and re-start the build from scratch by doing make\nclean.\nBuilding the example gallery - or not\nWhen you run make html for the first time, all the examples in the gallery\nwill be built. Subsequent builds should be faster, and will only build the\nexamples that have been modified.\nYou can run make html-noplot to not build the examples at all. This is\nuseful after a make clean to do some quick checks that are not related to\nthe examples.\nYou can also choose to only build a subset of the examples by using the\nEXAMPLES_PATTERN env variable, which accepts a regular expression. For\nexample EXAMPLES_PATTERN=\"transforms\" make html will only build the examples\nwith \"transforms\" in their name.\nNew model\nMore details on how to add a new model will be provided later. Please, do not send any PR with a new model without discussing \nit in an issue as, most likely, it will not be accepted.\nNew dataset\nMore details on how to add a new dataset will be provided later. Please, do not send any PR with a new dataset without discussing \nit in an issue as, most likely, it will not be accepted.\nPull Request\nIf all previous checks (flake8, mypy, unit tests) are passing, please send a PR. Submitted PR will pass other tests on \ndifferent operation systems, python versions and hardwares.\nFor more details about pull requests workflow, \nplease read GitHub's guides.\nLicense\nBy contributing to Torchvision, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-11-09T23:11:43Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T06:10:18Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9230741864283576,
        0.8277958961399113
      ],
      "excerpt": "The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision. \nTorchvision currently supports the following image backends: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421531721287399
      ],
      "excerpt": "Notes: libpng and libjpeg must be available at compilation time in order to be available. Make sure that it is available on the standard library locations, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8680604728172454
      ],
      "excerpt": "video_reader - This needs ffmpeg to be installed and torchvision to be built from source. There shouldn't be any conflicting version of ffmpeg installed. Currently, this is only supported on Linux. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8929822426895417
      ],
      "excerpt": "TorchVision provides an example project for how to use the models on C++ using JIT Script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8097242898440318,
        0.9074483316451477
      ],
      "excerpt": "The TorchVision package will also automatically look for the Torch package and add it as a dependency to my-target, \nso make sure that it is also available to cmake via the CMAKE_PREFIX_PATH. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9443088813195635
      ],
      "excerpt": "In order to get the torchvision operators registered with torch (eg. for the JIT), all you need to do is to ensure that you \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Datasets, Transforms and Models specific to Computer Vision",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can find the API documentation on the pytorch website: https://pytorch.org/vision/stable/index.html\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pytorch/vision/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5467,
      "date": "Fri, 24 Dec 2021 06:33:56 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pytorch/vision/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch/vision",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/smoke_test/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/pytorch/vision/tree/main/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pytorch/vision/main/test/sanity_checks.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pytorch/vision/main/scripts/fbcode_to_main_sync.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/packaging/build_conda.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/packaging/build_cmake.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/packaging/build_wheel.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/packaging/windows/internal/vc_install_helper.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/packaging/wheel/osx_wheel.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/packaging/wheel/linux_manywheel.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/ios/build_ios.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/ios/VisionTestApp/setup.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/ios/VisionTestApp/clean.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/build_docs/commit_docs.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/unittest/android/scripts/binary_android_upload.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/unittest/android/scripts/install_gradle.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/unittest/android/scripts/binary_android_build.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/unittest/ios/scripts/binary_ios_build.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/unittest/ios/scripts/binary_ios_upload.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/unittest/linux/scripts/post_process.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/unittest/linux/scripts/run_test.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/unittest/linux/scripts/setup_env.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/unittest/linux/scripts/install.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/unittest/windows/scripts/post_process.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/unittest/windows/scripts/run_test.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/unittest/windows/scripts/set_cuda_envs.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/unittest/windows/scripts/setup_env.sh",
      "https://raw.githubusercontent.com/pytorch/vision/main/.circleci/unittest/windows/scripts/install.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We recommend Anaconda as Python package management system. Please refer to `pytorch.org <https://pytorch.org/>`_\nfor the detail of PyTorch (``torch``) installation. The following is the corresponding ``torchvision`` versions and\nsupported Python versions.\n\n+--------------------------+--------------------------+---------------------------------+\n| ``torch``                | ``torchvision``          | ``python``                      |\n+==========================+==========================+=================================+\n| ``main`` / ``nightly``   | ``main`` / ``nightly``   | ``>=3.6``, ``<=3.9``            |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.10.0``               | ``0.11.1``               | ``>=3.6``, ``<=3.9``            |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.9.1``                | ``0.10.1``               | ``>=3.6``, ``<=3.9``            |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.9.0``                | ``0.10.0``               | ``>=3.6``, ``<=3.9``            |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.8.2``                | ``0.9.2``                | ``>=3.6``, ``<=3.9``            |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.8.1``                | ``0.9.1``                | ``>=3.6``, ``<=3.9``            |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.8.0``                | ``0.9.0``                | ``>=3.6``, ``<=3.9``            |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.7.1``                | ``0.8.2``                | ``>=3.6``, ``<=3.9``            |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.7.0``                | ``0.8.1``                | ``>=3.6``, ``<=3.8``            |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.7.0``                | ``0.8.0``                | ``>=3.6``, ``<=3.8``            |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.6.0``                | ``0.7.0``                | ``>=3.6``, ``<=3.8``            |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.5.1``                | ``0.6.1``                | ``>=3.5``, ``<=3.8``            |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.5.0``                | ``0.6.0``                | ``>=3.5``, ``<=3.8``            |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.4.0``                | ``0.5.0``                | ``==2.7``, ``>=3.5``, ``<=3.8`` |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.3.1``                | ``0.4.2``                | ``==2.7``, ``>=3.5``, ``<=3.7`` |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.3.0``                | ``0.4.1``                | ``==2.7``, ``>=3.5``, ``<=3.7`` |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.2.0``                | ``0.4.0``                | ``==2.7``, ``>=3.5``, ``<=3.7`` |\n+--------------------------+--------------------------+---------------------------------+\n| ``1.1.0``                | ``0.3.0``                | ``==2.7``, ``>=3.5``, ``<=3.7`` |\n+--------------------------+--------------------------+---------------------------------+\n| ``<=1.0.1``              | ``0.2.2``                | ``==2.7``, ``>=3.5``, ``<=3.7`` |\n+--------------------------+--------------------------+---------------------------------+\n\nAnaconda:\n\n.. code:: bash\n\n    conda install torchvision -c pytorch\n\npip:\n\n.. code:: bash\n\n    pip install torchvision\n\nFrom source:\n\n.. code:: bash\n\n    python setup.py install\n    # or, for OSX\n    # MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install\n\n\nIn case building TorchVision from source fails, install the nightly version of PyTorch following\nthe linked guide on the  `contributing page <https://github.com/pytorch/vision/blob/main/CONTRIBUTING.md#development-installation>`_ and retry the install.\n\nBy default, GPU support is built if CUDA is found and ``torch.cuda.is_available()`` is true.\nIt's possible to force building GPU support by setting ``FORCE_CUDA=1`` environment variable,\nwhich is useful when building a docker image.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9849069048986117,
        0.975308466056318
      ],
      "excerpt": "libpng_ - can be installed via conda :code:conda install libpng or any of the package managers for debian-based and RHEL-based Linux distributions. \nlibjpeg - can be installed via conda :code:conda install jpeg or any of the package managers for debian-based and RHEL-based Linux distributions. libjpeg-turbo can be used as well. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8877494219145105,
        0.8918974083095406,
        0.9667326416418838
      ],
      "excerpt": ".. _Pillow : https://python-pillow.org/ \n.. _Pillow-SIMD : https://github.com/uploadcare/pillow-simd \n.. _accimage: https://github.com/pytorch/accimage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8505988200227659
      ],
      "excerpt": ".. _pyav : https://github.com/PyAV-Org/PyAV \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9965316295201231,
        0.9820226428242687
      ],
      "excerpt": " conda install -c conda-forge ffmpeg \n python setup.py install \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9879885870492989
      ],
      "excerpt": "Installation From source: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9620406150340135,
        0.9944375700570437,
        0.8851009371765425
      ],
      "excerpt": "mkdir build \ncd build \n# Add -DWITH_CUDA=on support for the CUDA if needed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9967888697644471
      ],
      "excerpt": "make install \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8787488035373847
      ],
      "excerpt": "find_package(TorchVision REQUIRED) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8108630306827413
      ],
      "excerpt": "so make sure that it is also available to cmake via the CMAKE_PREFIX_PATH. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8341735847020567
      ],
      "excerpt": "In order to get the torchvision operators registered with torch (eg. for the JIT), all you need to do is to ensure that you \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pytorch/vision/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "Cuda",
      "Shell",
      "Batchfile",
      "Java",
      "CMake",
      "Objective-C++",
      "PowerShell",
      "Objective-C",
      "Dockerfile",
      "C",
      "Ruby"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) Soumith Chintala 2016, \\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "torchvision",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "vision",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pytorch/vision/blob/main/README.rst",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "datumbox",
        "body": "This minor release bumps the pinned PyTorch version to v1.10.1 and contains some minor bug fixes.\r\n\r\n# Highlights\r\n\r\n## Bug Fixes\r\n- [CI] Fix clang_format issue (#5061)\r\n- [CI, MOBILE] Fix binary_libtorchvision_ops_android job (#5062)\r\n- [CI] Add numpy as explicit dependency to build_cmake.sh (#5065)\r\n- [MODELS] Amend the weights only if quantize=True. (#5066)\r\n- [TRANSFORMS] Fix augmentation space to be uint8 compatible (#5067)\r\n- [DATASETS] Fix WIDERFace download links (#5068)\r\n- [BUILD, WINDOWS] Workaround for loading bundled DLLs (#5094)",
        "dateCreated": "2021-12-13T22:45:31Z",
        "datePublished": "2021-12-16T10:07:38Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.11.2",
        "name": "Minor bugfix release",
        "tag_name": "v0.11.2",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.11.2",
        "url": "https://api.github.com/repos/pytorch/vision/releases/55436753",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.11.2"
      },
      {
        "authorType": "User",
        "author_name": "seemethere",
        "body": "Users were reporting issues installing torchvision on PyPI, this release contains an update to the dependencies for wheels to point directly to torch==0.10.0",
        "dateCreated": "2021-10-21T16:16:22Z",
        "datePublished": "2021-10-21T16:58:04Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.11.1",
        "name": "Update dependency on wheels to match version in PyPI",
        "tag_name": "v0.11.1",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.11.1",
        "url": "https://api.github.com/repos/pytorch/vision/releases/51807199",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.11.1"
      },
      {
        "authorType": "User",
        "author_name": "datumbox",
        "body": "This release introduces the RegNet and EfficientNet architectures, a new FX-based utility to perform Feature Extraction, new data augmentation techniques such as RandAugment and TrivialAugment, updated training recipes that support EMA, Label Smoothing, Learning-Rate Warmup, Mixup and Cutmix, and many more.\r\n\r\n## Highlights\r\n\r\n### New Models\r\n\r\n[RegNet](https://arxiv.org/abs/2003.13678) and [EfficientNet](https://arxiv.org/abs/1905.11946) are two popular architectures that can be scaled to different computational budgets. In this release we include 22 pre-trained weights for their classification variants. The models were trained on ImageNet and can be used as follows:\r\n\r\n```python\r\nimport torch\r\nfrom torchvision import models\r\n\r\nx = torch.rand(1, 3, 224, 224)\r\n\r\nregnet = models.regnet_y_400mf(pretrained=True)\r\nregnet.eval()\r\npredictions = regnet(x)\r\n\r\nefficientnet = models.efficientnet_b0(pretrained=True)\r\nefficientnet.eval()\r\npredictions = efficientnet(x)\r\n```\r\n\r\nThe accuracies of the pre-trained models obtained on ImageNet val are seen below (see [#4403](https://github.com/pytorch/vision/pull/4403#issuecomment-930381524), [#4530](https://github.com/pytorch/vision/pull/4530#issuecomment-933213238) and [#4293](https://github.com/pytorch/vision/pull/4293) for more details)\r\n\r\n|Model\t|Acc@1\t|Acc@5\t|\r\n|---\t|---\t|---\t|\r\n|regnet_x_400mf\t|72.834\t|90.95\t|\r\n|regnet_x_800mf\t|75.212\t|92.348\t|\r\n|regnet_x_1_6gf\t|77.04\t|93.44\t|\r\n|regnet_x_3_2gf\t|78.364\t|93.992\t|\r\n|regnet_x_8gf\t|79.344\t|94.686\t|\r\n|regnet_x_16gf\t|80.058\t|94.944\t|\r\n|regnet_x_32gf\t|80.622\t|95.248\t|\r\n|regnet_y_400mf\t|74.046\t|91.716\t|\r\n|regnet_y_800mf\t|76.42\t|93.136\t|\r\n|regnet_y_1_6gf\t|77.95\t|93.966\t|\r\n|regnet_y_3_2gf\t|78.948\t|94.576\t|\r\n|regnet_y_8gf\t|80.032\t|95.048\t|\r\n|regnet_y_16gf\t|80.424\t|95.24\t|\r\n|regnet_y_32gf\t|80.878\t|95.34\t|\r\n|EfficientNet-B0\t|77.692\t|93.532\t|\r\n|EfficientNet-B1\t|78.642\t|94.186\t|\r\n|EfficientNet-B2\t|80.608\t|95.31\t|\r\n|EfficientNet-B3\t|82.008\t|96.054\t|\r\n|EfficientNet-B4\t|83.384\t|96.594\t|\r\n|EfficientNet-B5\t|83.444\t|96.628\t|\r\n|EfficientNet-B6\t|84.008\t|96.916\t|\r\n|EfficientNet-B7\t|84.122\t|96.908\t|\r\n\r\nWe would like to thank Ross Wightman and Luke Melas-Kyriazi for contributing the weights of the EfficientNet variants.\r\n\r\n### FX-based Feature Extraction\r\n\r\nA new Feature Extraction method has been added to our utilities. It uses PyTorch FX and enables us to retrieve the outputs of intermediate layers of a network which is useful for feature extraction and visualization. Here is an example of how to use the new utility:\r\n\r\n```python\r\nimport torch\r\nfrom torchvision.models import resnet50\r\nfrom torchvision.models.feature_extraction import create_feature_extractor\r\n\r\n\r\nx = torch.rand(1, 3, 224, 224)\r\n\r\nmodel = resnet50()\r\n\r\nreturn_nodes = {\r\n    \"layer4.2.relu_2\": \"layer4\"\r\n}\r\nmodel2 = create_feature_extractor(model, return_nodes=return_nodes)\r\nintermediate_outputs = model2(x)\r\n\r\nprint(intermediate_outputs['layer4'].shape)\r\n\r\n```\r\n\r\nWe would like to thank Alexander Soare for developing this utility.\r\n\r\n### New Data Augmentations\r\n\r\nTwo new Automatic Augmentation techniques were added: [Rand Augment](https://arxiv.org/abs/1909.13719) and [Trivial Augment](https://arxiv.org/abs/2103.10158). Both methods can be used as drop-in replacement of the AutoAugment technique as seen below:\r\n\r\n```python\r\nfrom torchvision import transforms\r\n\r\nt = transforms.RandAugment()\r\n# t = transforms.TrivialAugmentWide()\r\ntransformed = t(image)\r\n\r\ntransform = transforms.Compose([\r\n    transforms.Resize(256),\r\n    transforms.RandAugment(),  # transforms.TrivialAugmentWide()\r\n    transforms.ToTensor()])\r\n```\r\n\r\nWe would like to thank Samuel G. M\u00fcller for contributing Trivial Augment and for his help on refactoring the AA package.\r\n\r\n### Updated Training Recipes\r\n\r\nWe have updated our training reference scripts to add support of Exponential Moving Average, Label Smoothing, Learning-Rate Warmup, [Mixup](https://arxiv.org/abs/1710.09412), [Cutmix](https://arxiv.org/abs/1905.04899) and other [SOTA primitives](https://github.com/pytorch/vision/issues/3911). The above enabled us to improve the classification Acc@1 of some pre-trained models by [over 4 points](https://github.com/pytorch/vision/issues/3995). A major update of the existing pre-trained weights is expected on the next release.\r\n\r\n## Backward-incompatible changes\r\n\r\n[models] Use torch instead of scipy for random initialization of inception and googlenet weights (#4256)\r\n\r\n## Deprecations\r\n\r\n[models] Deprecate the C++ vision::models namespace (#4375)\r\n\r\n## New Features\r\n\r\n[datasets] Add iNaturalist dataset (#4123)\r\n[datasets] Download and Kinetics 400/600/700 Datasets (#3680)\r\n[datasets] Added LFW Dataset (#4255)\r\n[models] Add FX feature extraction as an alternative to intermediate_layer_getter (#4302) (#4418)\r\n[models] Add RegNet Architecture in TorchVision (#4403) (#4530) (#4550)\r\n[ops] Add new masks_to_boxes op (#4290) (#4469)\r\n[ops] Add StochasticDepth implementation (#4301)\r\n[reference scripts] Adding Mixup and Cutmix (#4379)\r\n[transforms] Integration of TrivialAugment with the current AutoAugment Code (#4221)\r\n[transforms] Adding RandAugment implementation (#4348)\r\n[models] Add EfficientNet Architecture in TorchVision (#4293)\r\n\r\n## Improvements\r\n\r\nVarious documentation improvements (#4239) (#4251) (#4275) (#4342) (#3894) (#4159) (#4133) (#4138) (#4089) (#3944) (#4349) (#3754) (#4308) (#4352) (#4318) (#4244) (#4362) (#3863) (#4382) (#4484) (#4503) (#4376) (#4457) (#4505) (#4363) (#4361) (#4337) (#4546) (#4553) (#4565) (#4567) (#4574) (#4575) (#4383) (#4390)  (#3409)  (#4451)  (#4340) (#3967)  (#4072)  (#4028) (#4132)\r\n[build] Add CUDA-11.3 builds to torchvision (#4248)\r\n[ci, tests] Skip some CPU-only tests on CircleCI machines with GPU (#4002) (#4025) (#4062)\r\n[ci] New issue templates (#4299)\r\n[ci] Various CI improvements, in particular putting back GPU testing on windows (#4421) (#4014) (#4053) (#4482) (#4475) (#3998) (#4388) (#4179) (#4394) (#4162) (#4065) (#3928) (#4081) (#4203) (#4011) (#4055) (#4074) (#4419) (#4067) (#4201) (#4200) (#4202) (#4496) (#3925)\r\n[ci] ping maintainers in case a PR was not properly labeled (#3993) (#4012) (#4021) (#4501)\r\n[datasets] Add bzip2 file compression support to datasets (#4097)\r\n[datasets] Faster dataset indexing (#3939)\r\n[datasets] Enable logging of internal dataset instanciations. (#4319)  (#4090) \r\n[datasets] Removed copy=False in torch.from_numpy in MNIST to avoid warning (#4184)\r\n[io] Add warning for files with corrupt containers (#3961)\r\n[models, tests] Add test to check that classification models are FX-compatible (#3662)\r\n[tests] Speedup various tests (#3929) (#3933)  (#3936)\r\n[models] Allow custom activation in SqueezeExcitation of EfficientNet (#4448)\r\n[models] Allow gradient backpropagation through GeneralizedRCNNTransform to inputs (#4327)\r\n[ops, tests] Add JIT tests (#4472)\r\n[ops] Make StochasticDepth FX-compatible (#4373)\r\n[ops] Added backward pass on CPU and CUDA for interpolation with anti-alias option (#4208) (#4211)\r\n[ops] Small refactoring to support opt mode for torchvision ops (fb internal specific) (#4080)  (#4095)\r\n[reference scripts] Added Exponential Moving Average support to classification reference script (#4381) (#4406) (#4407)\r\n[reference scripts] Adding label smoothing on classification reference (#4335)\r\n[reference scripts] Further enhance Classification Reference (#4444)\r\n[reference scripts] Replaced to_tensor() with pil_to_tensor() + convert_image_dtype() (#4452)\r\n[reference scripts] Update the metrics output on reference scripts (#4408)\r\n[reference scripts] Warmup schedulers in References (#4411)\r\n[tests] Add check for fx compatibility on segmentation and video models (#4131)\r\n[tests] Mock redirection logic for tests (#4197)\r\n[tests] Replace set_deterministic with non-deprecated spelling (#4212)\r\n[tests] Skip building torchvision with ffmpeg when python==3.9 (#4417)\r\n[tests] [jit] Make operation call accept Stack& instead Stack* (#63414) (#4380)\r\n[tests] make tests that involve GDrive more robust (#4454)\r\n[tests] remove dependency for dtype getters (#4291)\r\n[transforms] Replaced example usage of ToTensor() by PILToTensor() + ConvertImageDtype() (#4494)\r\n[transforms] Explicitly copying array in pil_to_tensor (#4566) (#4573)\r\n[transforms] Make get_image_size and get_image_num_channels public. (#4321)\r\n[transforms] adding gray images support for adjust_contrast and adjust_saturation (#4477)  (#4480)\r\n[utils] Support single color in utils.draw_bounding_boxes (#4075)\r\n[video, documentation] Port the video_api.ipynb notebook to the example gallery (#4241)\r\n[video, io, tests] Added check for invalid input file (#3932)\r\n[video, io] remove deprecated function call (#3861) (#3989)\r\n[video, tests] Removed test_audio_video_sync as it doesn't work as expected (#4050)\r\n[video] Build torchvision with ffmpeg only on Linux and ignore ffmpeg on other platforms (#4413, #4410, #4041)\r\n\r\n## Bug Fixes\r\n\r\n[build] Conda: Add numpy dependency (#4442)\r\n[build] Explicitly exclude PIL 8.3.0 from compatible dependencies (#4148)\r\n[build] More robust version check (#4285)\r\n[ci] Fix broken clang format test. (#4320)\r\n[ci] Remove mentions of conda-forge (#4082)\r\n[ci] fixup '*' -> '/.*/' for CI filter (#4059)\r\n[datasets] Fix download from google drive which was downloading empty files in some cases (#4109)\r\n[datasets] Fix splitting CelebA dataset (#4377)\r\n[datasets] Add support for files with periods in name (#4099)\r\n[io, tests] Don't check transparency channel for pil >= 8.3 in test_decode_png (#4167)\r\n[io] Fix size_t issues across JPEG versions and platforms (#4439)\r\n[io] Raise proper error when decoding 16-bits jpegs (#4101)\r\n[io] Unpinned the libjpeg version and fixed jpeg_mem_dest's size type Wind\u2026 (#4288)\r\n[io] deinterlacing PNG images with read_image (#4268)\r\n[io] More robust ffmpeg version query in setup.py (#4254)\r\n[io] Fixed read_image bug (#3948)\r\n[models] Don't download backbone weights if pretrained=True (#4283)\r\n[onnx, tests] Do not disable profiling executor in ONNX tests (#4324)\r\n[ops, tests] Fix DeformConvTester::test_backward_cuda by setting threads per block to 512 (#3942)\r\n[ops] Fix typing issue to make DeformConv2d scriptable (#4079)\r\n[ops] Fixes deform_conv issue with large input/output (#4351)\r\n[ops] Resolving tracing problem on StochasticDepth iterator. (#4372)\r\n[ops] Port quantize_val and dequantize_val into torchvision to avoid at::native and android xplat incompatibility (#4311)\r\n[reference scripts] Fix bug on EMA n_averaged estimation. (#4544) (#4545)\r\n[tests] Avoid cmyk in nvjpeg tests (#4246)\r\n[tests] Catch ValueError due to recent change to torch.testing.assert_close (#4165)\r\n[tests] Fix failing tests by catching the proper exception from torch.testing (#4121)\r\n[tests] Skip test if connection issues on fate (#4284)\r\n[transforms] Fix RandAugment and TrivialAugment bugs (#4370)\r\n[transforms] [FBcode->GH] [JIT] Add reference semantics to TorchScript classes (#44324) (#4166)\r\n[utils] Handle grayscale images on draw_bounding_boxes (#4043)  (#4049)\r\n[video, io] Fixed missing audio with video_reader and pyav backend (#3934, #4064)\r\n\r\n## Code Quality\r\n\r\nVarious typing improvements (#4369) (#4168) (#4169) (#4170) (#4171) (#4224) (#4227) (#4395) (#4409) (#4232) (#4234 (#4236) (#4226)  (#4416)\r\nRenamed the \u201cmaster\u201d branch into \u201cmain\u201d (#4306) (#4365)\r\n[ci] (fb-internal only) Allow all torchvision test rules to run with RE (#4073)\r\n[ci] add pre-commit hooks for convenient formatting checks (#4387)\r\n[ci] Import hipify_python only when needed (#4031)\r\n[io] Fixed a couple of typos and removed unnecessary bracket (#4345)\r\n[io] use from_blob to avoid memcpy (#4118)\r\n[models, ops] Moving common layers to ops (#4504)\r\n[models, ops] Replace MobileNetV3's SqueezeExcitation with EfficientNet's one (#4487)\r\n[models] Explicitely store a distance value that is reused (#4341)\r\n[models] Use torch instead of scipy for random initialization of inception and googlenet weights (#4256)\r\n[onnx, tests] Use test images from repo rather than internet for ONNX tests (#4176)\r\n[onnx] Import ONNX utils from symbolic_opset11 module (#4230)\r\n[ops] Fix clang formatting in deform_conv2d_kernel.cu (#3943)\r\n[ops] Update gpu atomics include path (#4478) (reverted)\r\n[reference scripts] Cleaned-up coco evaluation code (#4453)\r\n[reference scripts] remove unused package in coco_eval.py (#4404)\r\n[tests] Ported all tests to pytest (#3962) (#3996) (#3950) (#3964) (#3957) (#3959) (#3981) (#3952) (#3977) (#3974) (#3976) (#3983) (#3971) (#3988) (#3990) (#3985) (#3984) (#4030) (#3955)r (#4008) (#4010) (#4023) (#3954) (#4026) (#3953) (#4047) (#4185) (#3947) (#4045) (#4036) (#4034) (#3978) (#4046) (#3991) (#3930) (#4038) (#4037) (#4215) (#3972) (#3966) (#4114) (#4177) (#4280) (#3946) (#4233) (#4258) (#4035) (#4040) (#4000) (#4196) (#3922) (#4032)\r\n[tests] Prevent tests from leaking their respective RNG (#4497) (#3926) (#4250)\r\n[tests] Remove TestCase dependency for test_models_detection_anchor_utils.py (#4207)\r\n[tests] Removed tests executing deprecated F_t.center/five/ten_crop methods (#4479)\r\n[tests] Replace set_deterministic with non-deprecated spelling (#4212)\r\n[tests] Remove torchvision/test/fakedata_generation.py (#4130)\r\n[transforms, reference scripts] Added PILToTensor and ConvertImageDtype classes in reference scripts and used them to replace ToTensor(#4495, #4481)\r\n[transforms] Refactor AutoAugment to support more augmentations. (#4338)\r\n[transforms] Replace deprecated torch.lstsq with torch.linalg.lstsq (#3918)\r\n[video] Drop virtual from private member functions of Decoder class (#4027)\r\n[video] Fixed comparison warnings in audio_stream and video_stream (#4007)\r\n[video] Fixed some ffmpeg deprecation warnings in decoder (#4003)\r\n\r\n## Contributors\r\n\r\nWe're grateful for our community, which helps us improving torchvision by submitting issues and PRs, and providing feedback and suggestions. The following persons have contributed patches for this release:\r\n\r\nABD-01, Adam J. Stewart, Aditya Oke, Alex Lin, Alexander Grund, Alexander Soare, Allen Goodman, Amani Kiruga, Anirudh, Beat Buesser, beet, Bert Maher, Bruno Korbar, Camilo De La Torre, cyy, D. Khu\u00ea L\u00ea-Huu, David Fan, DevPranjal, dgenzel, dgenzel2, Dmitriy Genzel, Drishti Bhasin, Edward Z. Yang, Eli Uriegas, F-G Fernandez, Francisco Massa, Gary Miguel, Gaurav7888, IgorSusmelj, Ishan Kumar, Ivan Kobzarev, Jiawei Liu, Jithun Nair, Joao Gomes, Joe Early, Julien RIPOCHE, julienripoche, Kai Zhang, kingyiusuen, Loi Ly, Matti Picus, Meghan Lele, Muhammed Abdullah, Nicolas Hug, Nikita Shulga, ORippler, peterbell10, Philip Meier, Prabhat Roy, puhuk, Rajat Jaiswal, S Harish, Sahil Goyal, Samuel Gabriel, Santiago Castro, Saswat Das, Sepehr Sameni, Shengwei An, Shrill Shrestha, Shruti Pulstya, Sugato Ray, tanvimoharir, Vasilis Vryniotis, Vassilis C. Nicodemou, Vassilis Nicodemou, vfdev-5, Vincent Moens, Vivek Kumar, Yi Zhang, Yiwen Song, Yonghye Kwon, Yuchen Huang, Zhengxu Chen, Zhiqiang Wang, Zhongkai Zhu, zzk1st\r\n\r\n",
        "dateCreated": "2021-10-11T19:23:09Z",
        "datePublished": "2021-10-21T15:46:14Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.11.0",
        "name": "RegNet, EfficientNet, FX Feature Extraction and more",
        "tag_name": "v0.11.0",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.11.0",
        "url": "https://api.github.com/repos/pytorch/vision/releases/51792980",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.11.0"
      },
      {
        "authorType": "User",
        "author_name": "malfet",
        "body": "This release depends on pytorch 1.9.1\r\nNo functional changes other than minor updates to CI rules.",
        "dateCreated": "2021-09-14T02:19:45Z",
        "datePublished": "2021-09-27T04:40:36Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.10.1",
        "name": "Minor bugfix release",
        "tag_name": "v0.10.1",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.10.1",
        "url": "https://api.github.com/repos/pytorch/vision/releases/50291553",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.10.1"
      },
      {
        "authorType": "User",
        "author_name": "fmassa",
        "body": "This release improves support for mobile, with new mobile-friendly detection models based on SSD and SSDlite, CPU kernels for quantized NMS and quantized RoIAlign, pre-compiled binaries for iOS available in cocoapods and an iOS demo app. It also improves image IO by providing JPEG decoding on the GPU, and many more.\r\n\r\n# Highlights\r\n\r\n## [BETA] New models for detection\r\n\r\n[SSD](https://arxiv.org/abs/1512.02325) and [SSDlite](https://arxiv.org/abs/1801.04381) are two popular object detection architectures which are efficient in terms of speed and provide good results for low resolution pictures. In this release, we provide implementations for the original SSD model with VGG16 backbone and for its mobile-friendly variant SSDlite with MobileNetV3-Large backbone. The models were pre-trained on COCO train2017 and can be used as follows:\r\n\r\n```python\r\nimport torch\r\nimport torchvision\r\n\r\n# Original SSD variant\r\nx = [torch.rand(3, 300, 300), torch.rand(3, 500, 400)]\r\nm_detector = torchvision.models.detection.ssd300_vgg16(pretrained=True)\r\nm_detector.eval()\r\npredictions = m_detector(x)\r\n\r\n# Mobile-friendly SSDlite variant\r\nx = [torch.rand(3, 320, 320), torch.rand(3, 500, 400)]\r\nm_detector = torchvision.models.detection.ssdlite320_mobilenet_v3_large(pretrained=True)\r\nm_detector.eval()\r\npredictions = m_detector(x)\r\n```\r\n\r\nThe following accuracies can be obtained on COCO val2017 (full results available in #3403 and #3757):\r\n\r\nModel | mAP | mAP@50 | mAP@75\r\n-- | -- | -- | --\r\nSSD300 VGG16 | 25.1 | 41.5 | 26.2\r\nSSDlite320 MobileNetV3-Large | 21.3 | 34.3 | 22.1\r\n\r\n\r\n## [STABLE] Quantized kernels for object detection\r\n\r\nThe forward pass of the nms and roi_align operators now support tensors with a quantized dtype, which can help lowering the memory footprint of object detection models, particularly on mobile environments.\r\n\r\n## [BETA] JPEG decoding on the GPU\r\n\r\nDecoding jpegs is now possible on GPUs with the use of [nvjpeg](https://developer.nvidia.com/nvjpeg), which should be readily available in your CUDA setup. The decoding time of a single image should be about 2 to 3 times faster than with libjpeg on CPU. While the resulting tensor will be stored on the GPU device, the input raw tensor still needs to reside on the host (CPU), because the first stages of the decoding process take place on the host:\r\n\r\n```python\r\nfrom torchvision.io.image import read_file, decode_jpeg\r\n\r\ndata = read_file('path_to_image.jpg')  # raw data is on CPU\r\nimg = decode_jpeg(data, device='cuda')  # decoded image in on GPU\r\n```\r\n\r\n## [BETA] iOS support\r\n\r\nTorchVision 0.10 now provides pre-compiled iOS binaries for its C++ operators, which means you can run Faster R-CNN and Mask R-CNN on iOS. An example app on how to build a program leveraging those ops can be found in [here](https://github.com/pytorch/vision/tree/master/ios/VisionTestApp).\r\n\r\n## [STABLE] Speed optimizations for Tensor transforms\r\n\r\nThe resize and flip transforms have been optimized and its runtime improved by up to 5x on the CPU. The corresponding PRs were sent to PyTorch in https://github.com/pytorch/pytorch/pull/51653, https://github.com/pytorch/pytorch/pull/54500 and https://github.com/pytorch/pytorch/pull/56713\r\n\r\n## [STABLE] Documentation improvements\r\n\r\nSignificant improvements were made to the documentation. In particular, a new gallery of examples is available: see [here](https://pytorch.org/vision/master/auto_examples/index.html) for the latest version (the stable version is not released at the time of writing). These examples visually illustrate how each transform acts on an image, and also properly documents and illustrate the output of the segmentation models.\r\n\r\nThe example gallery will be extended in the future to provide more comprehensive examples and serve as a reference for common torchvision tasks.\r\n\r\n# Backwards Incompatible Changes\r\n\r\n* [transforms] Ensure input type of `normalize` is float. (#3621)\r\n* [models] Use PyTorch `smooth_l1_loss` and remove private custom implementation (#3539)\r\n\r\n# New Features\r\n\r\n* Added iOS binaries and test app (#3582)(#3629) (#3806)\r\n* [datasets] Added KITTI dataset (#3640)\r\n* [utils] Added utility to draw segmentation masks (#3330, #3824)\r\n* [models] Added the SSD & SSDlite object detection models (#3403, #3757, #3766, #3855, #3896, #3818, #3799)\r\n* [transforms] Added `antialias` option to `transforms.functional.resize` (#3761, #3810, #3842)\r\n* [transforms] Add new `max_size` parameter to `Resize` (#3494)\r\n* [io] Support for decoding jpegs on GPU with `nvjpeg` (#3792)\r\n* [ci, rocm] Add ROCm to builds (#3840) (#3604) (#3575)\r\n* [ops, models.quantization] Add quantized version of NMS (#3601)\r\n* [ops, models.quantization] Add quantized version of RoIAlign (#3624, #3904)\r\n\r\n# Improvement\r\n\r\n* [build] Various build improvements: (#3618) (#3622) (#3399) (#3794) (#3561)\r\n* [ci] Various CI improvements (#3647) (#3609) (#3635) (#3599) (#3778) (#3636) (#3809) (#3625) (#3764) (#3679) (#3869) (#3871) (#3444) (#3445) (#3480) (#3768) (#3919) (#3641)(#3900)\r\n* [datasets] Improve error handling in `make_dataset` (#3496)\r\n* [datasets] Remove caching from MNIST and variants (#3420)\r\n* [datasets] Make `DatasetFolder.find_classes` public (#3628)\r\n* [datasets] Separate extraction and decompression logic in `datasets.utils.extract_archive` (#3443)\r\n* [datasets, tests] Improve dataset test coverage and infrastructure (#3450) (#3457) (#3454) (#3447) (#3489) (#3661) (#3458 (#3705) (#3411) (#3461) (#3465) (#3543) (#3550) (#3665) (#3464) (#3595) (#3466) (#3468) (#3467) (#3486) (#3736) (#3730) (#3731) (#3477) (#3589) (#3503) (#3423) (#3492)(#3578) (#3605) (#3448) (#3864) (#3544)\r\n* [datasets, tests] Fix lazy importing for dataset tests (#3481)\r\n* [datasets, tests] Fix `test_extract(zip|tar|tar_xz|gzip)` on windows (#3542)\r\n* [datasets, tests] Fix `kwargs` forwarding in fake data utility functions (#3459)\r\n* [datasets, tests] Properly fix dataset test that passes by accident (#3434)\r\n* [documentation] Improve the documentation infrastructure (#3868) (#3724) (#3834) (#3689) (#3700) (#3513) (#3671) (#3490) (#3660) (#3594)\r\n* [documentation] Various documentation improvements (#3793) (#3715) (#3727) (#3838) (#3701) (#3923) (#3643) (#3537) (#3691) (#3453) (#3437) (#3732) (#3683) (#3853) (#3684) (#3576) (#3739) (#3530) (#3586) (#3744) (#3645) (#3694) (#3584) (#3615) (#3693) (#3706) (#3646) (#3780) (#3704) (#3774) (#3634)(#3591)(#3807)(#3663)\r\n* [documentation, ci] Improve the CI infrastructure for documentation (#3734) (#3837) (#3796) (#3711)\r\n* [io] remove deprecated function calls (#3859) (#3858)\r\n* [documentation, io] Improve IO docs and expose `ImageReadMode` in `torchvision.io` (#3812)\r\n* [onnx, models] Replace `reshape` with `flatten` in MobileNetV2 (#3462)\r\n* [ops, tests] Added test for `aligned=True` (#3540)\r\n* [ops, tests] Add onnx test for `batched_nms` (#3483)\r\n* [tests] Various test improvements (#3548) (#3422) (#3435) (#3860) (#3479) (#3721) (#3872) (#3908) (#2916) (#3917) (#3920) (#3579)\r\n* [transforms] add  `__repr__` for `transforms.RandomErasing` (#3491)\r\n* [transforms, documentation] Adds Documentation for AutoAugmentation (#3529)\r\n* [transforms, documentation] Add illustrations of transforms with sphinx-gallery (#3652)\r\n* [datasets] Remove pandas dependency for CelebA dataset (#3656, #3698)\r\n* [documentation] Add docs for missing datasets (#3536)\r\n* [referencescripts] Make reference scripts compatible with `submitit` (#3785)\r\n* [referencescripts] Updated `all_gather()` to make use of `all_gather_object()` from PyTorch (#3857)\r\n* [datasets] Added dataset download support in fbcode (#3823) (#3826)\r\n\r\n# Code quality\r\n\r\n* Remove inconsistent FB copyright headers (#3741)\r\n* Keep consistency in classes `ConvBNActivation` (#3750)\r\n* Removed unused imports (#3738, #3740, #3639)\r\n* Fixed `floor_divide` deprecation warnings seen in pytest output (#3672)\r\n* Unify onnx and JIT `resize` implementations (#3654)\r\n* Cleaned-up imports in test files related to datasets (#3720)\r\n* [documentation] Remove old css file (#3839)\r\n* [ci] Fix inconsistent version pinning across yaml files (#3790)\r\n* [datasets] Remove redundant `path.join` in `Places365` (#3545)\r\n* [datasets] Remove imprecise error handling in `PhotoTour` dataset (#3488)\r\n* [datasets, tests] Remove obsolete `test_datasets_transforms.py` (#3867)\r\n* [models] Making protected params of MobileNetV3 public (#3828)\r\n* [models] Make target argument in `transform.py` truly optional (#3866)\r\n* [models] Adding some references on MobileNetV3 implementation. (#3850)\r\n* [models] Refactored `set_cell_anchors()` in `AnchorGenerator` (#3755)\r\n* [ops] Minor cleanup of `roi_align_forward_kernel_impl` (#3619)\r\n* [ops] Replace deprecated `AutoNonVariableTypeMode` with `AutoDispatchBelowADInplaceOrView`. (#3786, #3897)\r\n* [tests] Port tests to use pytest (#3852, #3845, #3697, #3907, #3749)\r\n* [ops, tests] simplify `get_script_fn` (#3541)\r\n* [tests] Use torch.testing.assert_close in out test suite (#3886) (#3885) (#3883) (#3882) (#3881) (#3887) (#3880) (#3878) (#3877) (#3875) (#3888) (#3874) (#3884) (#3876) (#3879) (#3873)\r\n* [tests] Clean up test accept behaviour (#3759)\r\n* [tests] Remove unused `masks` variable in `test_image.py` (#3910)\r\n* [transforms] use ternary if in `resize` (#3533)\r\n* [transforms] replaced deprecated call to `ByteTensor` with `from_numpy` (#3813)\r\n* [transforms] Remove unnecessary casting in `adjust_gamma` (#3472)\r\n\r\n# Bugfixes\r\n\r\n* [ci] set empty cxx flags as default (#3474)\r\n* [android][test_app] Cleanup duplicate dependency (#3428)\r\n* Remove leftover exception (#3717)\r\n* Corrected spelling in a `TypeError` (#3659)\r\n* Add missing device info. (#3651)\r\n* Moving tensors to the right device (#3870)\r\n* Proper error message (#3725)\r\n* [ci, io] Pin JPEG version to resolve the size_t issue on windows (#3787)\r\n* [datasets] Make LSUN OS agnostic (#3455)\r\n* [datasets] Update `squeezenet` urls (#3581)\r\n* [datasets] Add `.item()` to the `target` variable in `fakedataset.py` (#3587)\r\n* [datasets] Fix VOC datasets for 2007 (#3572)\r\n* [datasets] Add custom user agent for download_url (#3498)\r\n* [datasets] Fix LSUN dataset tests flakyness (#3703)\r\n* [datasets] Fix (Fashion|K)MNIST download and MNIST download test (#3557)\r\n* [datasets] fix check for exceeded quota on Google Drive (#3710)\r\n* [datasets] Fix redirect behavior of datasets.utils.download_url (#3564)\r\n* [datasets] Update EMNIST url (#3567)\r\n* [datasets] Redirect datasets to correct urls (#3574)\r\n* [datasets] Prevent potential bug in `DatasetFolder.make_dataset` (#3733)\r\n* [datasets, tests] Fix redirection in download tests (#3568)\r\n* [documentation] Correct the size of returned tensor in comments of `ps_roi_pool.py` and `ps_roi_align.py` (#3849)\r\n* [io] Fix ternary operator to decide to store an image in Grayscale or RGB (#3553)\r\n* [io] Fixed audio-video synchronisation problem in `read_video()` when using `pts` as unit (#3791)\r\n* [models] Fix bug on detection backbones when `trainable_layers == 0` (#3906)\r\n* [models] Removed caching of anchors from `AnchorGenerator` (#3745)\r\n* [models] Update weights of classification models with new serialization format to allow proper unpickling (#3620, #3851)\r\n* [onnx, ops] Fix `roi_align` ONNX export (#3355)\r\n* [referencescripts] Only sync cuda ifn cuda available (#3674)\r\n* [referencescripts] Add checkpoints used for preemption. (#3789)\r\n* [transforms] Fix `to_tensor` for `accimage` backend (#3439)\r\n* [transforms] Make `crop` work the same for PIL and Tensor (#3770)\r\n* [transforms, models, tests] Fix some tests in fbcode (#3686)\r\n* [transforms, tests] Fix `test_random_autocontrast` flakyness (#3699)\r\n* [utils] Fix the spacing of labels on `draw_bounding_boxes` (#3895)\r\n* [utils, tests] Fix `test_draw_boxes` (#3631)\r\n\r\n# Deprecation\r\n\r\n* [transforms] Deprecate `_transforms_video` and `_functional_video` in favor of `transforms` (#3441)\r\n\r\n# Performance\r\n\r\n* [ops] Improve performance of `batched_nms` when number of boxes is large (#3426)\r\n* [transforms] Speed up `equalize` transform by using `bincount` instead of `histc` (#3493)\r\n\r\n# Contributors\r\n\r\nWe're grateful for our community, which helps us improving torchvision by submitting issues and PRs, and providing feedback and suggestions. The following persons have contributed patches for this release:\r\n\r\nAditya Oke, Akshay Kumar, Alessandro Melis, Avijit Dasgupta, Bruno Korbar, Caroline Chen, chengjuzhou, Edgar Andr\u00e9s Margffoy Tuay, Eli Uriegas, Francisco Massa, Guillem Orellana Trullols, harishsdev, Ivan Kobzarev, Jaesun Park, James Thewlis, Jeff Daily, Jeff Yang, Jithendra Paruchuri, Jon Janzen, KAI ZHAO, Ksenija Stanojevic, Lewis Patten, Matti Picus, moto, Mustafa Bal, Nicolas Hug, Nikhil Kumar, Nikita Shulga, Philip Meier, Prabhat Roy, Sanket Thakur, scott-vsi, Sofiane Abbar, t-rutten, urmi22, Vasilis Vryniotis, vfdev, Yuchen Huang, Zhengyang Feng, Zhiqiang Wang\r\n\r\nThank you!",
        "dateCreated": "2021-06-15T14:21:42Z",
        "datePublished": "2021-06-15T14:55:18Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.10.0",
        "name": "iOS support, GPU image decoding, SSDlite and more",
        "tag_name": "v0.10.0",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.10.0",
        "url": "https://api.github.com/repos/pytorch/vision/releases/44649543",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.10.0"
      },
      {
        "authorType": "User",
        "author_name": "fmassa",
        "body": "# Highlights\r\n\r\nThis minor release bumps the pinned PyTorch version to v1.8.1, and brings a few bugfixes for datasets, including MNIST download not being available.\r\n\r\n\r\n# Bugfixes\r\n- fix VOC datasets for 2007 (#3572)\r\n- Update EMNIST url (#3567)\r\n- Fix redirect behavior of datasets.utils.download_url (#3564)\r\n- Fix MNIST download for minor release (#3559) ",
        "dateCreated": "2021-03-25T03:52:04Z",
        "datePublished": "2021-03-25T17:51:25Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.9.1",
        "name": "Dataset bugfixes",
        "tag_name": "v0.9.1",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.9.1",
        "url": "https://api.github.com/repos/pytorch/vision/releases/40432431",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.9.1"
      },
      {
        "authorType": "User",
        "author_name": "fmassa",
        "body": "This release introduces improved support for mobile, with new mobile-friendly models, pre-compiled binaries for Android available in maven and an android demo app. It also improves image IO and provides new data augmentations including AutoAugment.\r\n\r\n# Highlights\r\n\r\n## Better mobile support\r\n\r\ntorchvision 0.9 adds support for the MobileNetV3 architecture with pre-trained weights for Classification, Object Detection and Segmentation tasks.\r\nIt also improves C++ operators so that they can be compiled and run on Android, and we are providing pre-compiled torchvision artifacts published to jcenter. An example application on how to use the torchvision ops on an Android app can be found in [here](https://github.com/pytorch/android-demo-app/tree/master/D2Go).\r\n\r\n### Classification\r\nWe provide MobileNetV3 variants (including a quantized version) pre-trained on ImageNet 2012.\r\n```python\r\nimport torch\r\nimport torchvision\r\n\r\n# Classification\r\nx = torch.rand(1, 3, 224, 224)\r\nm_classifier = torchvision.models.mobilenet_v3_large(pretrained=True)\r\n# m_classifier = torchvision.models.mobilenet_v3_small(pretrained=True)\r\nm_classifier.eval()\r\npredictions = m_classifier(x)\r\n\r\n# Quantized Classification\r\nx = torch.rand(1, 3, 224, 224)\r\nm_classifier = torchvision.models.quantization.mobilenet_v3_large(pretrained=True)\r\nm_classifier.eval()\r\npredictions = m_classifier(x)\r\n```\r\nThe pre-trained models have the following accuracies on ImageNet 2012 val:\r\n\r\n| Model | Top-1 Acc | Top-5 Acc\r\n| --- | --- | --- |\r\n| MobileNetV3 Large | 74.042 | 91.340 |\r\n| MobileNetV3 Large (Quantized) | 73.004 | 90.858 |\r\n| MobileNetV3 Small | 67.620 | 87.404 |\r\n\r\n\r\n### Object Detection\r\nWe provide two variants of Faster R-CNN with MobileNetV3 backbone pre-trained on COCO train2017. They can be obtained as follows\r\n```python\r\nimport torch\r\nimport torchvision\r\n\r\n# Fast Low Resolution Model\r\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\r\nm_detector = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\r\nm_detector.eval()\r\npredictions = m_detector(x)\r\n\r\n# Highly Accurate High Resolution Model\r\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\r\nm_detector = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\r\nm_detector.eval()\r\npredictions = m_detector(x)\r\n```\r\nAnd yield the following accuracies on COCO val 2017 (full results available in #3265):\r\n\r\n| Model | mAP | mAP@50 | mAP@75 |\r\n| --- | --- | --- | --- |\r\n| Faster R-CNN MobileNetV3-Large 320 FPN | 22.8 | 38.0 | 23.2 |\r\n| Faster R-CNN MobileNetV3-Large FPN | 32.8 | 52.5 | 34.3 |\r\n\r\n### Semantic Segmentation\r\nWe also provide pre-trained models for semantic segmentation. The models have been trained on a subset of COCO train2017, which contains the same 20 categories as those from Pascal VOC.\r\n```python\r\nimport torch\r\nimport torchvision\r\n\r\n# Fast Mobile Model\r\nx = torch.rand(1, 3, 520, 520)\r\nm_segmenter = torchvision.models.segmentation.lraspp_mobilenet_v3_large(pretrained=True)\r\nm_segmenter.eval()\r\npredictions = m_segmenter(x)\r\n\r\n# Highly Accurate Mobile Model\r\nx = torch.rand(1, 3, 520, 520)\r\nm_segmenter = torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True)\r\nm_segmenter.eval()\r\npredictions = m_segmenter(x)\r\n```\r\n\r\nThe pre-trained models give the following results on the subset of COCO val2017 which contain the same 20 categories as those present in Pascal VOC (full results in #3276):\r\n\r\n| Model | mean IoU | global pixelwise accuracy |\r\n| --- | --- | --- |\r\n| Lite R-ASPP with Dilated MobileNetV3 Large Backbone | 57.9 | 91.2 |\r\n| DeepLabV3 with Dilated MobileNetV3 Large Backbone | 60.3 | 91.2 |\r\n\r\n## Addition of the AutoAugment method\r\n\r\n[AutoAugment](https://arxiv.org/pdf/1805.09501.pdf) is a common Data Augmentation technique that can improve the accuracy of Scene Classification models. Though the data augmentation policies are directly linked to their trained dataset, empirical studies show that ImageNet policies provide significant improvements when applied to other datasets. \r\n\r\nIn TorchVision we implemented 3 policies learned on the following datasets: ImageNet, CIFA10 and SVHN. The new transform can be used standalone or mixed-and-matched with existing transforms:\r\n```python\r\nfrom torchvision import transforms\r\n\r\nt = transforms.AutoAugment()\r\ntransformed = t(image)\r\n\r\ntransform=transforms.Compose([\r\n    transforms.Resize(256),\r\n    transforms.AutoAugment(),\r\n    transforms.ToTensor()])\r\n```\r\n\r\n## Improved Image IO and on-the-fly image type conversions\r\n\r\nAll the read and decode methods of the `io.image` package have been updated to:\r\n\r\n* Add support for Palette, Grayscale Alpha and RBG Alpha image types during PNG decoding. \r\n* Allow the on-the-fly conversion of image from one type to the other during read.\r\n\r\n```python\r\nfrom torchvision.io.image import read_image, ImageReadMode\r\n\r\n# keeps original type, channels unchanged\r\nx1 = read_image(\"image.png\")\r\n\r\n# converts to grayscale, channels = 1\r\nx2 = read_image(\"image.png\", mode=ImageReadMode.GRAY)\r\n\r\n# converts to grayscale with alpha transparency, channels = 2\r\nx3 = read_image(\"image.png\", mode=ImageReadMode.GRAY_ALPHA)\r\n\r\n# coverts to RGB, channels = 3\r\nx4 = read_image(\"image.png\", mode=ImageReadMode.RGB)\r\n\r\n# converts to RGB with alpha transparency, channels = 4\r\nx5 = read_image(\"image.png\", mode=ImageReadMode.RGB_ALPHA)\r\n```\r\n\r\n## Python 3.9 and CUDA 11.1\r\nThis release adds official support for Python 3.9 and CUDA 11.1 (#3341, #3418)\r\n\r\n\r\n# Backwards Incompatible Changes\r\n\r\n* [Ops] Change default `eps` value of `FrozenBN` to better align with `nn.BatchNorm` (#2933)\r\n* [Ops] Remove deprecated _new_empty_tensor. (#3156)\r\n* [Transforms] `ColorJitter` gets its random params by calling `get_params()` (#3001)\r\n* [Transforms] Change rounding of transforms on integer tensors (#2964)\r\n* [Utils] Remove `normalize` from `save_image` (#3324)\r\n\r\n# New Features\r\n\r\n* [Datasets] Add WiderFace dataset (#2883)\r\n* [Models] Add MobileNetV3 architecture:\r\n    * Classification Models: (#3354, #3252, #3182, #3242, #3177)\r\n    * Object Detection Models: (#3265, #3253, #3223, #3243, #3244, #3248)\r\n    * Segmentation Models: (#3276)\r\n    * Quantized Models: (#3366, #3323)\r\n* [Models] Improve speed/accuracy of FasterRCNN by introducing a score threshold on RPN (#3205)\r\n* [Mobile] Add Android gradle project with demo test app (#2897)\r\n* [Transforms] Implemented AutoAugment, along with required new transforms + Policies (#3123)\r\n* [Ops] Added support of Autocast in all Operators: #2938, #2926, #2922, #2928, #2905, #2906, #2907, #2898\r\n* [Ops] Add modulation input for DeformConv2D (#2791)\r\n* [IO] Improved `io.image` with on-the-fly image type conversions: (#3193, #3069, #3024, #2988, #2984)\r\n* [IO] Add option to write audio to video file (#2304)\r\n* [Utils] Added a utility to draw bounding boxes (#2785, #3296,  #3075)\r\n\r\n# Improvements\r\n\r\n## Datasets\r\n\r\n* Concatenate small tensors in video datasets to reduce the use of shared file descriptor (#1795)\r\n* Improve testing for datasets (#3336, #3337, #3402, #3412, #3413, #3415, #3416, #3345, #3376, #3346, #3338)\r\n* Check if dataset file is located on Google Drive before downloading it (#3245)\r\n* Improve Coco implementation (#3417)\r\n* Make download_url follow redirects (#3236)\r\n* `make_dataset` as `staticmethod` of `DatasetFolder` (#3215)\r\n* Add a warning if any clip can't be obtained from a video in `VideoClips`. (#2513)\r\n\r\n## Models\r\n\r\n* Improve error message in `AnchorGenerator` (#2960)\r\n* Disable pretrained backbone downloading if pretrained is True in segmentation models (#3325)\r\n* Support for image with no annotations in RetinaNet (#3032)\r\n* Change RoIHeads reshape to support empty batches. (#3031)\r\n* Fixed typing exception throwing issues with JIT (#3029)\r\n* Replace deprecated `functional.sigmoid` with `torch.sigmoid` in RetinaNet (#3307)\r\n* Assert that inputs are floating point in Faster R-CNN normalize method (#3266)\r\n* Speedup RetinaNet's postprocessing (#2828)\r\n\r\n## Ops\r\n\r\n* Added eps in the `__repr__` of FrozenBN  (#2852)\r\n* Added `__repr__` to `MultiScaleRoIAlign` (#2840)\r\n* Exposing LevelMapper params in `MultiScaleRoIAlign` (#3151)\r\n* Enable autocast for all operators and let them use the dispatcher (#2926, #2922, #2928, #2898)\r\n\r\n## Transforms\r\n\r\n* `adjust_hue` now accepts tensors with one channel (#3222)\r\n* Add `fill` color support for tensor affine transforms (#2904)\r\n* Remove torchscript workaround for `center_crop` (#3118)\r\n* Improved error message for `RandomCrop` (#2816)\r\n\r\n## IO\r\n\r\n* Enabling to import `read_file` and the other methods from torchvision.io (#2918)\r\n* accept python bytes in `_read_video_from_memory()` (#3347)\r\n* Enable rtmp timeout in decoder (#3076)\r\n* Specify tls cert file to decoder through config (#3289, #3374)\r\n* Add UUID in LOG() in decoder (#3080)\r\n\r\n## References\r\n\r\n* Add weight averaging and storing methods in references utils (#3352)\r\n* Adding Preset Transforms in reference scripts (#3317)\r\n* Load variables when `--resume /path/to/checkpoint --test-only` (#3285)\r\n* Updated video classification ref example with new transforms (#2935)\r\n\r\n## Misc\r\n\r\n* Various documentation improvements (#3039, #3271, #2820, #2808, #3131, #3062, #3061, #3000, #3299, #3400, #2899, #2901, #2908, #2851, #2909, #3005, #2821, #2957, #3360, #3019, #3124, #3217, #2879, #3234, #3180, #3425, #2979, #2935, #3298, #3268, #3203, #3290, #3295, #3200, #2663, #3153, #3147, #3232)\r\n* The documentation infrastructure was improved, in particular the docs are now built on every PR and uploaded to CircleCI (#3259, #3378, #3408, #3373, #3290)\r\n* Avoid some deprecation warnings from PyTorch (#3348)\r\n* Ensure operators are added in C++ (#2798, #3091, #3391)\r\n* Fixed compilation warnings on C++ codebase (#3390)\r\n* CI Improvements (#3401, #3329, #2990, #2978, #3189, #3230, #3254, #2844, #2872, #2825, #3144, #3137, #2827, #2848, #2914, #3419, #2895, #2837)\r\n* Installation improvements (#3302, #2969, #3113, #3202)\r\n* CMake improvements (#2801, #2805, #3212, #3381)\r\n\r\n## Mobile\r\n\r\n* Add Torch Selective macros in all C++ Ops for better support on mobile (#3218)\r\n\r\n## Code Quality, testing\r\n\r\n* [BC-breaking] Modernized C++ codebase & made it mobile-friendly (25% faster to compile): #2885, #2891, #2892, #2893, #2905, #2906, #2907, #2938, #2944, #2945, #3011, #3020, #3097, #3105, #3134, #3135, #3143, #3146, #3154, #3156, #3163, #3218, #3308, #3311, #3312, #3326, #3350, #3390\r\n* Cleaned up Python codebase & made it more Pythonic: #3263, #3239, #3059, #3055, #3045, #3382, #3159, #3171\r\n* Improve type annotations (#3288, #3045, #2862, #2858, #2857, #2863, #2865, #2856, #2860, #2864, #2875, #2859, #2854, #2861, #3174, #3059)\r\n* Code refactoring and static analysis improvements (#3379, #3335, #3229, #3204, #3095)\r\n* Miscellaneous test improvements (#2966, #2965, #3018, #3035, #2961, #2806, #2812, #2815, #2834, #2874, #3099, #3092, #3160, #3103, #2971, #3023, #2803, #*3136*, #3319, #3310, #3287, #3033, #2983, #3386, #3369, #3116, #2985, #3320)\r\n\r\n# Bug Fixes\r\n\r\n* [DATASETS] Fixes EMNIST split and label issues (#2673)\r\n* [DATASETS] Fix overflow in STL10 fold reading (#3353)\r\n* [MODELS] Fix incorrectly frozen BN on ResNet FPN backbone (#3396)\r\n* [MODELS] Fix scriptability support in Inception V3 (#2976)\r\n* [MODELS] Changed default value of eps in FrozenBatchNorm to match BatchNorm: #2940 #2933\r\n* [MODELS] Fixed warning in `models.detection.transforms.resize_image_and_masks`. (#3237)\r\n* [MODELS] Fix trainable_layers on RetinaNet (#3234)\r\n* [MODELS] Fix ShuffleNetV2 ONNX model export issue. (#3158)\r\n* [UTILS] Fixes no grad and range bugs in utils. (#3269)\r\n* [UTILS] make_grid uses a more correct normalization (#2967)\r\n* [OPS] fix GET_THREADS() for ROCm with DeformConv (#2997)\r\n* [OPS] Fix NMS and IoU overflows for fp16 (#3383, #3382)\r\n* [OPS] Fix ops registration on windows (#3380)\r\n* [OPS] Fix initialisation bug on FeaturePyramidNetwork (#2954)\r\n* [IO] Replace hardcoded error code with ENODATA (#3277)\r\n* [REFERENCES] Fix repeated UserWarning and add more flexibility to reference code for segmentation tasks (#2886)\r\n* [TRANSFORMS] Fix default fill value in RandomRotation (#3303)\r\n* [TRANSFORMS] Correct aspect ratio sampling in transforms.RandomErasing (#3344)\r\n* [TRANSFORMS] Fix `CenterCrop` for Tensor size is greater than `imgsize` (#3333)\r\n* [TRANSFORMS] Functional to_tensor returns float tensor of default dtype (#3398)\r\n* [TRANSFORMS] Add explicit check for number of channels (#3013)\r\n* [TRANSFORMS] `pil_to_tensor` with accimage backend now return uint8 (#3109)\r\n* [TRANSFORMS] Fix potential overflow in `convert_image_dtype` (#3107)\r\n* [TRANSFORMS] Check num of channels on `adjust*_` transformations (#3069)\r\n\r\n# Deprecations\r\n\r\n*  [TRANSFORMS] Introduced InterpolationModes and deprecated arguments: `resample` and `fillcolor` (#2952, #3055)\r\n\r\n",
        "dateCreated": "2021-03-03T23:07:32Z",
        "datePublished": "2021-03-04T20:54:36Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.9.0",
        "name": "Mobile support, AutoAugment, improved IO and more",
        "tag_name": "v0.9.0",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.9.0",
        "url": "https://api.github.com/repos/pytorch/vision/releases/39183259",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.9.0"
      },
      {
        "authorType": "User",
        "author_name": "fmassa",
        "body": "This minor release bumps the pinned PyTorch version to v1.7.1, and contains some minor improvements.\r\n\r\n# Highlights\r\n\r\n## Python 3.9 support\r\n\r\nThis releases add native binaries for Python 3.9 #3063\r\n\r\n## Bugfixes\r\n\r\n- Make read_file and write_file accept unicode strings on Windows #2949\r\n- Replaced tuple creation by one acceptable by majority of compilers #2937 \r\n- Add docs for `focal_loss` #2979",
        "dateCreated": "2020-12-04T01:52:19Z",
        "datePublished": "2020-12-10T17:10:41Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.8.2",
        "name": "Python 3.9 support and bugfixes",
        "tag_name": "v0.8.2",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.8.2",
        "url": "https://api.github.com/repos/pytorch/vision/releases/35111076",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.8.2"
      },
      {
        "authorType": "User",
        "author_name": "seemethere",
        "body": "# Issues resolved:\r\n\r\n* *Cannot pip install torchvision==0.8.0+cu110* - https://github.com/pytorch/vision/issues/2912",
        "dateCreated": "2020-10-27T20:31:48Z",
        "datePublished": "2020-10-27T21:22:05Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.8.1",
        "name": "Added version suffix back to package",
        "tag_name": "v0.8.1",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.8.1",
        "url": "https://api.github.com/repos/pytorch/vision/releases/33133824",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.8.1"
      },
      {
        "authorType": "User",
        "author_name": "fmassa",
        "body": "This release brings new additions to torchvision that improves support for model deployment. Most notably, transforms in torchvision are now torchscript-compatible, and can thus be serialized together with your model for simpler deployment. Additionally, we provide native image IO with torchscript support, and a new video reading API (released as Beta) which is more flexible than `torchvision.io.read_video`.\r\n\r\n# Highlights\r\n\r\n## Transforms now support Tensor, batch computation, GPU and TorchScript\r\n\r\ntorchvision transforms are now inherited from nn.Module and can be torchscripted and applied on torch Tensor inputs as well as on PIL images. They also support Tensors with batch dimension and work seamlessly on CPU/GPU devices:\r\n```python\r\nimport torch\r\nimport torchvision.transforms as T\r\n\r\n# to fix random seed, use torch.manual_seed\r\n# instead of random.seed\r\ntorch.manual_seed(12)\r\n\r\ntransforms = torch.nn.Sequential(\r\n    T.RandomCrop(224),\r\n    T.RandomHorizontalFlip(p=0.3),\r\n    T.ConvertImageDtype(torch.float),\r\n    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\r\n)\r\nscripted_transforms = torch.jit.script(transforms)\r\n# Note: we can similarly use T.Compose to define transforms\r\n# transforms = T.Compose([...]) and \r\n# scripted_transforms = torch.jit.script(torch.nn.Sequential(*transforms.transforms))\r\n\r\ntensor_image = torch.randint(0, 256, size=(3, 256, 256), dtype=torch.uint8)\r\n# works directly on Tensors\r\nout_image1 = transforms(tensor_image)\r\n# on the GPU\r\nout_image1_cuda = transforms(tensor_image.cuda())\r\n# with batches\r\nbatched_image = torch.randint(0, 256, size=(4, 3, 256, 256), dtype=torch.uint8)\r\nout_image_batched = transforms(batched_image)\r\n# and has torchscript support\r\nout_image2 = scripted_transforms(tensor_image)\r\n```\r\nThese improvements enable the following new features:\r\n\r\n* support for GPU acceleration\r\n* batched transformations e.g. as needed for videos\r\n* transform multi-band torch tensor images (with more than 3-4 channels)\r\n* torchscript transforms together with your model for deployment\r\n\r\n**Note: Exceptions for TorchScript support includes `Compose`, `RandomChoice`, `RandomOrder`, `Lambda` and those applied on PIL images, such as `ToPILImage`.**\r\n\r\n## Native image IO for JPEG and PNG formats\r\n\r\ntorchvision 0.8.0 introduces native image reading and writing operations for JPEG and PNG formats. Those operators support TorchScript and return `CxHxW` tensors in `uint8` format, and can thus be now part of your model for deployment in C++ environments.\r\n\r\n```python\r\nfrom torchvision.io import read_image\r\n\r\n# tensor_image is a CxHxW uint8 Tensor\r\ntensor_image = read_image('path_to_image.jpeg')\r\n\r\n# or equivalently\r\nfrom torchvision.io.image import read_file, decode_image\r\n# raw_data is a 1d uint8 Tensor with the raw bytes\r\nraw_data = read_file('path_to_image.jpeg')\r\ntensor_image = decode_image(raw_data)\r\n\r\n# all operators are torchscriptable and can be\r\n# serialized together with your model torchscript code\r\nscripted_read_image = torch.jit.script(read_image)\r\n```\r\n\r\n## New detection model\r\n\r\nThis release adds a pretrained model for RetinaNet with a ResNet50 backbone from [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002), with the following accuracies on COCO val2017:\r\n```\r\nIoU metric: bbox\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.364\r\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.558\r\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.383\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.193\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.400\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.490\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.315\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.506\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.558\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.386\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.595\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.699\r\n```\r\n\r\n## [BETA] New Video Reader API\r\n\r\nThis release introduces a new video reading abstraction, which gives more fine-grained control on how to iterate over the videos. It supports image and audio, and implements an iterator interface so that it can be combined with the rest of the python ecosystem, such as `itertools`.\r\n\r\n```python\r\nfrom torchvision.io import VideoReader\r\n\r\n# stream indicates if reading from audio or video\r\nreader = VideoReader('path_to_video.mp4', stream='video')\r\n# can change the stream after construction\r\n# via reader.set_current_stream\r\n\r\n# to read all frames in a video starting at 2 seconds\r\nfor frame in reader.seek(2):\r\n    # frame is a dict with \"data\" and \"pts\" metadata\r\n    print(frame[\"data\"], frame[\"pts\"])\r\n\r\n# because reader is an iterator you can combine it with\r\n# itertools\r\nfrom itertools import takewhile, islice\r\n# read 10 frames starting from 2 seconds\r\nfor frame in islice(reader.seek(2), 10):\r\n    pass\r\n    \r\n# or to return all frames between 2 and 5 seconds\r\nfor frame in takewhile(lambda x: x[\"pts\"] < 5, reader.seek(2)):\r\n    pass\r\n```\r\n\r\n**Note: In order to use the Video Reader API, you need to compile torchvision from source and make sure that you have ffmpeg installed in your system.**\r\n**Note: the VideoReader API is currently released as beta and its API can change following user feedback.**\r\n\r\n# Backwards Incompatible Changes\r\n\r\n* [Transforms] Random seed now should be set with `torch.manual_seed` instead of `random.seed` (#2292)\r\n* [Transforms] `RandomErasing.get_params`  function\u2019s argument was previously `value=0` and is now `value=None`  which is interpreted as Gaussian random noise (#2386)\r\n* [Transforms] `RandomPerspective` and `F.perspective` changed the default value of interpolation to be `BILINEAR` instead of `BICUBIC` (#2558, #2561)\r\n* [Transforms] Fixes incoherence in `affine` transformation when center is defined as half image size + 0.5 (#2468)\r\n\r\n# New Features\r\n\r\n* [Ops] Added focal loss (#2784)\r\n* [Ops] Added bounding boxes conversion function (#2710, #2737)\r\n* [Ops] Added Generalized IOU (#2642)\r\n* [Models] Added RetinaNet object detection model (#2784)\r\n* [Datasets] Added Places365 dataset (#2610, #2625)\r\n* [Transforms] Added GaussianBlur transform (#2658)\r\n* [Transforms] Added torchscript, batch and GPU and tensor support for transforms (#2769, #2767, #2749, #2755, #2485, #2721, #2645, #2694, #2584, #2661, #2566, #2345, #2342, #2356, #2368, #2373, #2496, #2553, #2495, #2561, #2518, #2478, #2459, #2444, #2396, #2401, #2394, #2586, #2371, #2477, #2456, #2628, #2569, #2639, #2620, #2595, #2456, #2403, #2729)\r\n* [Transforms] Added example notebook for tensor transforms (#2730)\r\n* [IO] Added JPEG/PNG encoding / decoding ops\r\n    * JPEG (#2388, #2471, #2696, #2725)\r\n    * PNG (#2382, #2726, #2398, #2457, #2735)\r\n    * decode_image (#2680, #2695, #2718, #2764, #2766)\r\n* [IO] Added file reading / writing ops (#2728, #2765, #2768)\r\n* [IO] [BETA] Added new VideoReader API (#2683, #2781, #2778, #2802, #2596, #2612, #2734, #2770)\r\n\r\n# Improvements\r\n\r\n## Datasets\r\n\r\n* Added error message if Google Drive download quota is exceeded (#2321)\r\n* Optimized LSUN initialization time by only pulling keys from db (#2544)\r\n* Use more precise return type for gzip.open() (#2792)\r\n* Added UCF101 dataset tests (#2548)\r\n* Added download tests on a schedule (#2665, #2675, #2699, #2706, #2747, #2731)\r\n* Added typehints for datasets (#2487, #2521, #2522, #2523, #2524, #2526, #2528, #2529, #2525, #2527, #2530, #2533, #2534, #2535, #2536, #2532, #2538, #2537, #2539, #2531, #2540, #2667)\r\n\r\n## Models\r\n\r\n* Removed hard coded value in DeepLabV3 (#2793)\r\n* Changed the anchor generator default argument to an equivalent one (#2722)\r\n* Moved model construction location in `resnet_fpn_backbone` into after docstring (#2482)\r\n* Partially enabled type hints for models (#2668)\r\n\r\n## Ops\r\n\r\n* Moved RoIs shape check to C++ (#2794)\r\n* Use autocast built-in cast-helper functions (#2646)\r\n* Adde type annotations for `torchvision.ops` (#2331, #2462)\r\n\r\n## References\r\n\r\n* [References] Removed redundant target send to device in detection evaluation (#2503)\r\n* [References] Removed obsolete import in segmentation. (#2399)\r\n\r\n## Misc\r\n\r\n* [Transforms] Added support for negative padding in `pad` (#2744)\r\n* [IO] Added type hints for `torchvision.io` (#2543)\r\n* [ONNX] Export `ROIAlign` with `aligned=True` (#2613)\r\n\r\n## Internal\r\n\r\n* [Binaries] Added CUDA 11 binary builds (#2671)\r\n* [Binaries] Added DEBUG=1 option to build torchvision (#2603)\r\n* [Binaries] Unpin ninja version (#2358)\r\n* Warn if torchvision imported from repo root (#2759)\r\n* Added compatibility checks for C++ extensions (#2467)\r\n* Added probot (#2448)\r\n* Added ipynb to git attributes file (#2772)\r\n* CI improvements (#2328, #2346, #2374, #2437, #2465, #2579, #2577, #2633, #2640, #2727, #2754, #2674, #2678)\r\n* CMakeList improvements (#2739, #2684, #2626, #2585, #2587)\r\n* Documentation improvements (#2659, #2615, #2614, #2542, #2685, #2507, #2760, #2550, #2656, #2723, #2601, #2654, #2757, #2592, #2606)\r\n\r\n# Bug Fixes\r\n\r\n* [Ops] Fixed crash in deformable convolutions (#2604)\r\n* [Ops] Added empty batch support for `DeformConv2d` (#2782)\r\n* [Transforms] Enforced contiguous output in `to_tensor` (#2483)\r\n* [Transforms] Fixed fill parameter for PIL pad (#2515)\r\n* [Models] Fixed deprecation warning in `nonzero` for R-CNN models (#2705)\r\n* [IO] Explicitly cast to `size_t` in video decoder (#2389)\r\n* [ONNX] Fixed dynamic resize in Mask R-CNN (#2488)\r\n* [C++ API] Fixed function signatures for `torch::nn::Functional` (#2463)\r\n\r\n# Deprecations\r\n\r\n* [Transforms] Deprecated dedicated implementations `functional_tensor` of `F_t.center_crop`, `F_t.five_crop`, `F_t.ten_crop`, as they can be implemented as a function of `crop` (#2568)\r\n* [Transforms] Deprecated explicit usage of `F_pil` and `F_t` functions, users should instead use the general functional API (#2664)\r\n\r\n",
        "dateCreated": "2020-10-27T00:33:11Z",
        "datePublished": "2020-10-27T16:17:12Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.8.0",
        "name": "Improved transforms, native image IO, new video API and more",
        "tag_name": "v0.8.0",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.8.0",
        "url": "https://api.github.com/repos/pytorch/vision/releases/33106460",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.8.0"
      },
      {
        "authorType": "User",
        "author_name": "fmassa",
        "body": "# Highlights\r\n\r\n## Mixed precision support for all models\r\ntorchvision models now support mixed-precision training via the new `torch.cuda.amp` package. Using mixed precision support is easy: just wrap the model and the loss inside a `torch.cuda.amp.autocast` context manager. Here is an example with Faster R-CNN:\r\n\r\n```python\r\nimport torch, torchvision\r\n\r\ndevice = torch.device('cuda')\r\n\r\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn()\r\nmodel.to(device)\r\n\r\ninput = [torch.rand(3, 300, 400, device=device)]\r\nboxes = torch.rand((5, 4), dtype=torch.float32, device=device)\r\nboxes[:, 2:] += boxes[:, :2]\r\ntarget = [{\"boxes\": boxes,\r\n          \"labels\": torch.zeros(5, dtype=torch.int64, device=device),\r\n          \"image_id\": 4,\r\n          \"area\": torch.zeros(5, dtype=torch.float32, device=device),\r\n          \"iscrowd\": torch.zeros((5,), dtype=torch.int64, device=device)}]\r\n\r\n# use automatic mixed precision\r\nwith torch.cuda.amp.autocast():\r\n    loss_dict = model(input, target)\r\nlosses = sum(loss for loss in loss_dict.values())\r\n# perform backward outside of autocast context manager\r\nlosses.backward()\r\n```\r\n\r\n## New pre-trained segmentation models\r\n\r\nThis releases adds pre-trained weights for the ResNet50 variants of Fully-Convolutional Networks (FCN) and DeepLabV3.\r\nThey are available under `torchvision.models.segmentation`, and can be obtained as follows:\r\n```python\r\ntorchvision.models.segmentation.fcn_resnet50(pretrained=True)\r\ntorchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\r\n```\r\n\r\nThey obtain the following accuracies:\r\nNetwork | mean IoU | global pixelwise acc\r\n-- | -- | --\r\nFCN ResNet50 | 60.5 | 91.4\r\nDeepLabV3 ResNet50 | 66.4 | 92.4\r\n\r\n\r\n## Improved ONNX support for Faster / Mask / Keypoint R-CNN\r\n\r\nThis release restores ONNX support for the R-CNN family of models that had been temporarily dropped in the 0.6.0 release, and additionally fixes a number of corner cases in the ONNX export for these models.\r\nNotable improvements includes support for dynamic input shape exports, including images with no detections. \r\n\r\n# Backwards Incompatible Changes\r\n- [Transforms] Fix for integer fill value in constant padding (#2284)\r\n- [Models] Replace L1 loss with smooth L1 loss in Faster R-CNN for better performance (#2113)\r\n- [Transforms] Use `torch.rand` instead of `random.random()` for random transforms (#2520)\r\n\r\n# New Features\r\n- [Models] Add mixed-precision support (#2366, #2384)\r\n- [Models] Add `fcn_resnet50` and `deeplabv3_resnet50` pretrained models. (#2086, #2091)\r\n- [Ops] Added eps attribute to FrozenBatchNorm2d (#2190)\r\n- [Transforms] Add `convert_image_dtype` to functionals (#2078)\r\n- [Transforms] Add `pil_to_tensor` to functionals (#2092)\r\n\r\n# Bug Fixes\r\n- [JIT] Fix virtualenv and torchhub support by removing eager scripting calls (#2248)\r\n- [IO] Fix `write_video` when floating point FPS is passed (#2334)\r\n- [IO] Fix missing compilation files for video-reader (#2183)\r\n- [IO] Fix missing include for OSX in video decoder (#2224)\r\n- [IO] Fix overflow error for large buffers. (#2303)\r\n- [Ops] Fix wrong clamping in RoIAlign with `aligned=True` (#2438)\r\n- [Ops] Fix corner case in `interpolate` (#2146)\r\n- [Ops] Fix the use of `contiguous()` in C++ kernels (#2131)\r\n- [Ops] Restore support of tuple of Tensors for region pooling ops (#2199)\r\n- [Datasets] Fix bug related with trailing slash on UCF-101 dataset (#2186)\r\n- [Models] Make copy of targets in GeneralizedRCNNTransform (#2227)\r\n- [Models] Fix DenseNet issue with gradient checkpoints (#2236)\r\n- [ONNX] Fix  ONNX implementation of`heatmaps_to_keypoints` in KeypointRCNN (#2312)\r\n- [ONNX] Fix export of images with no detection for Faster / Mask / Keypoint R-CNN (#2126, #2215, #2272)\r\n\r\n# Deprecations\r\n- [Ops] Deprecate Conv2d, ConvTranspose2d and BatchNorm2d (#2244)\r\n- [Ops] Deprecate `interpolate` in favor of PyTorch's implementation (#2252)\r\n\r\n\r\n# Improvements\r\n\r\n## Datasets\r\n- Fix DatasetFolder error message (#2143)\r\n- Change `range(len)` to `enumerate` in `DatasetFolder` (#2153)\r\n- [DOC] Fix link URL to Flickr8k (#2178)\r\n- [DOC] Add CelebA to docs (#2107)\r\n- [DOC] Improve documentation of `DatasetFolder` and `ImageFolder` (#2112)\r\n\r\n## TorchHub\r\n- Fix torchhub tests due to numerical changes in torch.sum (#2361)\r\n- Add all the latest models to hubconf (#2189)\r\n\r\n## Transforms\r\n- Add `fill` argument to `__repr__` of `RandomRotation` (#2340)\r\n- Add tensor support for `adjust_hue` (#2300, #2355)\r\n- Make `ColorJitter` torchscriptable (#2298)\r\n- Make `RandomHorizontalFlip` and `RandomVerticalFlip` torchscriptable (#2282)\r\n- [DOC] Use consistent symbols in the doc of `Normalize` to avoid confusion (#2181)\r\n- [DOC] Fix typo in `hflip` in `functional.py` (#2177)\r\n- [DOC] Fix spelling errors in `functional.py` (#2333)\r\n\r\n## IO\r\n- Refactor `video.py` to improve clarity (#2335)\r\n- Save memory by not storing full frames in `read_video_timestamps` (#2202, #2268)\r\n- Improve warning when `video_reader` backend is not available (#2225)\r\n- Set `should_buffer` to True by default in `_read_from_stream` (#2201)\r\n- [Test] Temporarily disable one PyAV test (#2150)\r\n\r\n## Models\r\n- Improve target checks in GeneralizedRCNN (#2207, #2258)\r\n- Use Module objects instead of functions for some layers of Inception3 (#2287)\r\n- Add support for other normalizations in MobileNetV2 (#2267)\r\n- Expose layer freezing option to detection models (#2160, #2242)\r\n- Make ASPP-Layer in DeepLab more generic (#2174)\r\n- Faster initialization for Inception family of models (#2170, #2211)\r\n- Make `norm_layer` as parameters in `models/detection/backbone_utils.py` (#2081)\r\n- Updates integer division to use floor division operator (#2234, #2243)\r\n- [JIT] Clean up no longer needed workarounds for torchscript support (#2249, #2261, #2210)\r\n- [DOC] Add docs to clarify aspect ratio definition in RPN. (#2185)\r\n- [DOC] Fix roi_heads argument name in doctstring of GeneralizedRCNN (#2093)\r\n- [DOC] Fix type annotation in RPN docstring (#2149)\r\n- [DOC] add clarifications to Object detection reference documentation (#2241)\r\n- [Test] Add tests for negative samples for Mask R-CNN and Keypoint R-CNN (#2069)\r\n\r\n## Reference scripts\r\n- Add support for SyncBatchNorm in QAT reference script (#2230, #2280)\r\n- Fix training resuming in `references/segmentation` (#2142) \r\n- Rename `image` to `images` in `references/detection/engine.py` (#2187)\r\n\r\n## ONNX\r\n- Add support for dynamic input shape export in R-CNN models (#2087)\r\n\r\n## Ops\r\n- Added number of features in FrozenBatchNorm2d `__repr__` (#2168)\r\n- improve consistency among box IoU CPU / GPU calculations (#2072)\r\n- Avoid `using` in header files (#2257)\r\n- Make `ceil_div` `__host__ __device__` (#2217)\r\n- Don't include CUDAApplyUtils.cuh (#2127)\r\n- Add namespace to avoid conflict with ATen version of `channel_shuffle()` (#2206)\r\n- [DOC] Update the statement of supporting torchscript ops (#2343)\r\n- [DOC] Update torchvision ops in doc (#2341)\r\n- [DOC] Improve documentation for NMS (#2159)\r\n- [Test] Add more tests to NMS (#2279)\r\n\r\n## Misc\r\n\r\n- Add PyTorch version compatibility table to README (#2260)\r\n- Fix lint (#2182, #2226, #2070)\r\n- Update version to 0.6.0 in CMake (#2140)\r\n- Remove mock (#2096)\r\n- Remove warning about deprecated (#2064)\r\n- Cleanup unused import (#2067)\r\n- Type annotations for torchvision/utils.py (#2034)\r\n\r\n## CI\r\n- Add version suffix to build version\r\n- Add backslash to escape\r\n- Add workflows to run on tag\r\n- Bump version to 0.7.0, pin PyTorch to 1.6.0\r\n- Update link for cudnn 10.2 (#2277)\r\n- Fix binary builds with CUDA 9.2 on Windows (#2273)\r\n- Remove Python 3.5 from CI (#2158)\r\n- Improvements to CI infra (#2075, #2071, #2058,  #2073, #2099, #2137, #2204, #2264, #2274, #2319)\r\n- Master version bump 0.6 -> 0.7 (#2102)\r\n- Add test channels for pytorch version functions (#2208)\r\n- Add static type check with mypy (#2195, #1696, #2247)",
        "dateCreated": "2020-07-23T22:26:40Z",
        "datePublished": "2020-07-28T15:04:20Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.7.0",
        "name": "Mixed precision training, new models and improvements",
        "tag_name": "v0.7.0",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.7.0",
        "url": "https://api.github.com/repos/pytorch/vision/releases/28624217",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.7.0"
      },
      {
        "authorType": "User",
        "author_name": "seemethere",
        "body": "## Highlights\r\n\r\n* Bump pinned PyTorch version to [`v1.5.1`](https://github.com/pytorch/pytorch/releases/tag/v1.5.1)",
        "dateCreated": "2020-06-01T18:08:46Z",
        "datePublished": "2020-06-22T18:20:05Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.6.1",
        "name": "v0.6.1",
        "tag_name": "v0.6.1",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.6.1",
        "url": "https://api.github.com/repos/pytorch/vision/releases/27798257",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.6.1"
      },
      {
        "authorType": "User",
        "author_name": "fmassa",
        "body": "This release is the first one that officially drops support for Python 2.\r\nIt contains a number of improvements and bugfixes.\r\n\r\n## Highlights\r\n\r\n### Faster/Mask/Keypoint RCNN supports negative samples\r\n\r\nIt is now possible to feed training images to Faster / Mask / Keypoint R-CNN that do not contain any positive annotations.\r\nThis enables increasing the number of negative samples during training. For those images, the annotations expect a tensor with 0 in the number of objects dimension, as follows:\r\n```python\r\ntarget = {\"boxes\": torch.zeros((0, 4), dtype=torch.float32),\r\n          \"labels\": torch.zeros(0, dtype=torch.int64),\r\n          \"image_id\": 4,\r\n          \"area\": torch.zeros(0, dtype=torch.float32),\r\n          \"masks\": torch.zeros((0, image_height, image_width), dtype=torch.uint8),\r\n          \"keypoints\": torch.zeros((17, 0, 3), dtype=torch.float32),\r\n          \"iscrowd\": torch.zeros((0,), dtype=torch.int64)}\r\n```\r\n\r\n### Aligned flag for RoIAlign\r\n\r\n`RoIAlign` now supports the aligned flag, which aligns more precisely two neighboring pixel indices.\r\n\r\n### Refactored abstractions for C++ video decoder\r\n\r\nThis change is transparent to Python users, but the whole C++ backend for video reading (which needs torchvision to be compiled from source for it to be enabled for now) has been refactored into more modular abstractions.\r\nThe core abstractions are in https://github.com/pytorch/vision/tree/master/torchvision/csrc/cpu/decoder, and the video reader functions exposed to Python, by leveraging those abstractions, can be written in [a much more concise way](https://github.com/pytorch/vision/tree/master/torchvision/csrc/cpu/video_reader)\r\n\r\n## Backwards Incompatible Changes\r\n\r\n* Dropping Python2 support (#1761, #1792, #1984, #1976, #2037, #2033, #2017)\r\n* [Models] Fix inception quantized pre-trained model (#1954, #1969, #1975)\r\n* ONNX support for Mask R-CNN and Keypoint R-CNN has been temporarily dropped, but will be fixed in next releases\r\n\r\n## New Features\r\n\r\n* [Transforms] Add Perspective fill option (#1973)\r\n* [Ops]  `aligned` flag in ROIAlign (#1908)\r\n* [IO] Update video reader to use new decoder (#1978)\r\n* [IO] torchscriptable functions for video io (#1653, #1794)\r\n* [Models] Support negative samples in Faster R-CNN, Mask R-CNN and Keypoint R-CNN (#1911, #2069)\r\n\r\n## Improvements\r\n\r\n### Datasets\r\n\r\n* STL10: don't check integrity twice when download=True (#1787)\r\n* Improve code readability and docstring of video datasets(#2020)\r\n* [DOC] Fixed typo in Cityscapes docs (#1851)\r\n\r\n### Transforms\r\n\r\n* Allow passing list to the input argument 'scale' of RandomResizedCrop (#1997) (#2008)\r\n* F.normalize unsqueeze mean & std only for 1-d arrays (#2002)\r\n* Improved error messages for transforms.functional.normalize(). (#1915)\r\n* generalize number of bands calculation in to_tensor (#1781)\r\n* Replace 2 transpose ops with 1 permute in ToTensor(#2018)\r\n* Fixed Pillow version check for Pillow >= 10 (#2039)\r\n* [DOC]: Improve transforms.Normalize docs (#1784, #1858)\r\n* [DOC] Fixed missing new line in transforms.Crop docstring (#1922)\r\n\r\n### Ops\r\n\r\n* Check boxes shape in RoIPool / Align (#1968)\r\n* [ONNX] Export new_empty_tensor (#1733)\r\n* Fix Tensor::data<> deprecation. (#2028)\r\n* Fix deprecation warnings (#2055)\r\n\r\n### Models\r\n\r\n* Add warning and note docs for scipy (#1842) (#1966)\r\n* Added __repr__ attribute to GeneralizedRCNNTransform (#1834)\r\n* Replace mean on dimensions 2,3 by adaptive_avg_pooling2d in mobilenet (#1838)\r\n* Add init_weights keyword argument to Inception3 (#1832)\r\n* Add device to torch.tensor. (#1979)\r\n* ONNX export for variable input sizes in Faster R-CNN (#1840)\r\n* [JIT] Cleanup torchscript constant annotations (#1721, #1923, #1907, #1727)\r\n* [JIT] use // now that it is supported (#1658)\r\n* [JIT] add @torch.jit.script to ImageList (#1919)\r\n* [DOC] Improved docs for Faster R-CNN (#1886, #1868, #1768, #1763)\r\n* [DOC] add comments for the modified implementation of ResNet (#1983)\r\n* [DOC] Add comments to AnchorGenerator (#1941)\r\n* [DOC] Add comment in GoogleNet (#1932)\r\n\r\n### Documentation\r\n\r\n* Document int8 quantization model (#1951)\r\n* Update Doc with ONNX support (#1752)\r\n* Update README to reflect strict dependency on torch==1.4.0 (#1767)\r\n* Update sphinx theme (#2031)\r\n* Document origin of preprocessing mean / std (#1965)\r\n* Fix docstring formatting issues (#2049)\r\n\r\n### Reference scripts\r\n\r\n* Add return statement in evaluate function of detection reference script (#2029)\r\n* [DOC]Add default training parameters to classification reference README (#1998)\r\n* [DOC] Add README to references/segmentation (#1864)\r\n\r\n### Tests\r\n\r\n* Improve stability of test_nms_cuda (#2044)\r\n* [ONNX] Disable model tests since export of interpolate script module is broken (#1989)\r\n* Skip inception v3 in test/test_quantized_models (#1885)\r\n* [LINT] Small indentation fix (#1831)\r\n\r\n### Misc\r\n\r\n* Remove unintentional -O0 option in setup.py (#1770)\r\n* Create CODE_OF_CONDUCT.md\r\n* Update issue templates (#1913, #1914)\r\n* master version bump 0.5 \u2192 0.6\r\n* replace torch 1.5.0 items flagged with deprecation warnings (fix #1906) (#1918)\r\n* CUDA_SUFFIX \u2192 PYTORCH_VERSION_SUFFIX\r\n\r\n### CI\r\n\r\n* Remove av from the binary requirements (#2006)\r\n* ci: Add cu102 to CI and packaging, remove cu100 (#1980)\r\n* .circleci: Switch to use token for conda uploads (#1960)\r\n* Improvements to CI infra (#2051, #2032, #2046, #1735, #2048, #1789, #1731, #1961)\r\n* typing only needed for python 3.5 and previous (#1778)\r\n* Move C++ and Python linter to CircleCI (#2056, #2057)\r\n\r\n## Bug Fixes\r\n\r\n### Datasets\r\n\r\n* bug fix on downloading voc2007 test dataset (#1991)\r\n* fix lsun docstring example (#1935)\r\n* Fixes EMNIST classes attribute is wrong #1716 (#1736)\r\n* Force object annotation to be a list in VOC (#1790)\r\n\r\n### Models\r\n\r\n* Fix for AnchorGenerator when device switch happen (#1745)\r\n* [JIT] fix len error (#1981)\r\n* [JIT] fix googlenet no aux logits (#1949)\r\n* [JIT] Fix quantized googlenet (#1974)\r\n\r\n### Transforms\r\n\r\n* Fix for rotate fill with Images of type F (#1828)\r\n* Fix fill in rotate (#1760)\r\n\r\n### Ops\r\n\r\n* Fix bug in DeformConv2d for batch sizes > 32 (#2027, #2040)\r\n* Fix for roi_align ONNX export (#1988)\r\n* Fix torchscript issue in ConvTranspose2d (#1917)\r\n* Fix interpolate when no scale_factor is passed (#1785)\r\n* Fix Windows build by renaming Python init functions (#1779)\r\n* fix for loading models with num_batches_tracked in frozen bn (#1728)\r\n\r\n## Deprecations\r\n\r\n* the pts_unit of pts from read_video and read_video_timestamp is deprecated, and will be replaced in next releases with seconds.",
        "dateCreated": "2020-04-09T10:18:09Z",
        "datePublished": "2020-04-21T14:31:00Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.6.0",
        "name": "Drop Python 2 support, several improvements and bugfixes",
        "tag_name": "v0.6.0",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.6.0",
        "url": "https://api.github.com/repos/pytorch/vision/releases/24855330",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.6.0"
      },
      {
        "authorType": "User",
        "author_name": "fmassa",
        "body": "This release brings several new additions to torchvision that improves support for deployment. Most notably, all models in torchvision are torchscript-compatible, and can be exported to ONNX. Additionally, a few classification models have quantized weights.\r\n\r\n**Note: this is the last version of torchvision that officially supports Python 2.**\r\n\r\n# Breaking changes\r\n\r\n## Updated KeypointRCNN pre-trained weights\r\n\r\nThe pre-trained weights for keypointrcnn_resnet50_fpn have been updated and now correspond to the results reported in the documentation. The previous weights corresponded to an intermediate training checkpoint. (#1609)\r\n\r\n## Corrected the implementation for MNASNet\r\n\r\nThe previous implementation contained a bug which affects all MNASNet variants other than mnasnet1_0. The bug was that the first few layers needed to also be scaled in terms of width multiplier, along with all the rest. We now provide a new checkpoint for mnasnet0_5, which gives 32.17 top1 error. (#1224)\r\n\r\n# Highlights\r\n\r\n## TorchScript support for all models\r\n\r\nAll models in torchvision have native support for torchscript, for both training and testing. This includes complex models such as DeepLabV3, Mask R-CNN and Keypoint R-CNN.\r\nUsing torchscript with torchvision models is easy:\r\n```python\r\n# get a pre-trained model\r\nmodel = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\r\n\r\n# convert to torchscript\r\nmodel_script = torch.jit.script(model)\r\nmodel_script.eval()\r\n\r\n# compute predictions\r\npredictions = model_script([torch.rand(3, 300, 300)])\r\n```\r\n\r\n**Warning: the return type for the scripted version of Faster R-CNN, Mask R-CNN and Keypoint R-CNN is different from its eager counterpart, and it always returns a tuple of losses, detections. This discrepancy will be addressed in a future release.**\r\n\r\n## ONNX\r\n\r\nAll models in torchvision can now be exported to ONNX for deployment. This includes models such as Mask R-CNN.\r\n```python\r\n# get a pre-trained model\r\nmodel = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\r\nmodel.eval()\r\ninputs = [torch.rand(3, 300, 300)]\r\npredictions = model(inputs)\r\n\r\n# convert to ONNX\r\ntorch.onnx.export(model, inputs, \"model.onnx\",\r\n                  do_constant_folding=True,\r\n                  opset_version=11  # opset_version 11 required for Mask R-CNN\r\n                  )\r\n```\r\n**Warning: for Faster R-CNN / Mask R-CNN / Keypoint R-CNN, the current exported model is dependent on the input shape during export. As such, make sure that once the model has been exported to ONNX that all images that are fed to it have the same shape as the shape used to export the model to ONNX. This behavior will be made more general in a future release.**\r\n\r\n## Quantized models\r\n\r\ntorchvision now provides quantized models for ResNet, ResNext, MobileNetV2, GoogleNet, InceptionV3 and ShuffleNetV2, as well as reference scripts for quantizing your own model in references/classification/train_quantization.py (https://github.com/pytorch/vision/blob/master/references/classification/train_quantization.py). Obtaining a pre-trained quantized model can be obtained with a few lines of code:\r\n```python\r\nmodel = torchvision.models.quantization.mobilenet_v2(pretrained=True, quantize=True)\r\nmodel.eval()\r\n\r\n# run the model with quantized inputs and weights\r\nout = model(torch.rand(1, 3, 224, 224))\r\n```\r\n\r\nWe provide pre-trained quantized weights for the following models:\r\n\r\n| Model | Acc@1 | Acc@5 | \r\n| --- | --- | --- |\r\nMobileNet V2 | 71.658 | 90.150\r\nShuffleNet V2: | 68.360 | 87.582\r\nResNet 18 | 69.494 | 88.882\r\nResNet 50 | 75.920 | 92.814\r\nResNext 101 32x8d | 78.986 | 94.480\r\nInception V3 | 77.084 | 93.398\r\nGoogleNet | 69.826 | 89.404\r\n\r\n\r\n## Torchscript support for torchvision.ops\r\n\r\ntorchvision ops are now natively supported by torchscript. This includes operators such as nms, roi_align and roi_pool, and for the ops that support backpropagation, both eager and torchscript modes are supported in autograd.\r\n\r\n## New operators\r\n\r\n### Deformable Convolution (#1586) (#1660) (#1637)\r\n\r\nAs described in Deformable Convolutional Networks (https://arxiv.org/abs/1703.06211), torchvision now supports deformable convolutions. The model expects as input both the input as well as the offsets, and can be used as follows:\r\n```python\r\nfrom torchvision import ops\r\n\r\nmodule = ops.DeformConv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)\r\nx = torch.rand(1, 1, 10, 10)\r\n\r\n# number of channels for offset should be a multiple\r\n# of 2 * module.weight.size[2] * module.weight.size[3], which correspond\r\n# to the kernel_size\r\noffset = torch.rand(1, 2 * 3 * 3, 10, 10)\r\n\r\n# the output requires both the input and the offsets\r\nout = module(x, offset)\r\n```\r\n\r\nIf needed, the user can create their own wrapper module that imposes constraints on the offset. Here is an example, using a single convolution layer to compute the offset:\r\n\r\n```python\r\nclass BasicDeformConv2d(nn.Module):\r\n    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\r\n                 dilation=1, groups=1, offset_groups=1):\r\n        super().__init__()\r\n        offset_channels = 2 * kernel_size * kernel_size\r\n        self.conv2d_offset = nn.Conv2d(\r\n            in_channels,\r\n            offset_channels * offset_groups,\r\n            kernel_size=3,\r\n            stride=stride,\r\n            padding=dilation,\r\n            dilation=dilation,\r\n        )\r\n        self.conv2d = ops.DeformConv2d(\r\n            in_channels,\r\n            out_channels,\r\n            kernel_size=kernel_size,\r\n            stride=stride,\r\n            padding=dilation,\r\n            dilation=dilation,\r\n            groups=groups,\r\n            bias=False\r\n        )\r\n    \r\n    def forward(self, x):\r\n        offset = self.conv2d_offset(x)\r\n        return self.conv2d(x, offset)\r\n```\r\n\r\n### Position-sensitive RoI Pool / Align (#1410)\r\n\r\nPosition-Sensitive Region of Interest (RoI) Align operator mentioned in Light-Head R-CNN (https://arxiv.org/abs/1711.07264). These are available under ops.ps_roi_align, ps_roi_pool and the module equivalents ops.PSRoIAlign and ops.PSRoIPool, and have the same interface as RoIAlign / RoIPool.\r\n\r\n# New Features\r\n\r\n## TorchScript support\r\n\r\n* Bugfix in BalancedPositiveNegativeSampler introduced during torchscript support (#1670)\r\n* Make R-CNN models less verbose in script mode (#1671)\r\n* Minor torchscript fixes for Mask R-CNN (#1639)\r\n* remove BC-breaking changes (#1560)\r\n* Make maskrcnn scriptable (#1407)\r\n* Add Script Support for Video Resnet Models (#1393)\r\n* fix ASPPPooling (#1575)\r\n* Test that torchhub models are scriptable (#1242)\r\n* Make Googlnet & InceptionNet scriptable (#1349)\r\n* Make fcn_resnet Scriptable (#1352)\r\n* Make Densenet Scriptable (#1342)\r\n* make resnext scriptable (#1343)\r\n* make shufflenet and resnet scriptable (#1270)\r\n\r\n## ONNX\r\n\r\n* Enable KeypointRCNN test (#1673)\r\n* enable mask rcnn test (#1613)\r\n* Changes to Enable KeypointRCNN ONNX Export (#1593)\r\n* Disable Profiling in Failing Test (#1585)\r\n* Enable ONNX Test for FasterRcnn (#1555)\r\n* Support Exporting Mask Rcnn to ONNX (#1461)\r\n* Lahaidar/export faster rcnn (#1401)\r\n* Support Exporting RPN to ONNX (#1329)\r\n* Support Exporting MultiScaleRoiAlign to ONNX (#1324)\r\n* Support Exporting GeneralizedRCNNTransform to ONNX (#1325)\r\n\r\n## Quantization\r\n\r\n* Update quantized shufflenet weights (#1715)\r\n* Add commands to run quantized model with pretrained weights (#1547)\r\n* Quantizable googlenet, inceptionv3 and shufflenetv2 models (#1503)\r\n* Quantizable resnet and mobilenet models (#1471)\r\n* Remove model download from test_quantized_models (#1526)\r\n\r\n# Improvements\r\n\r\n## Bugfixes\r\n\r\n* Bugfix on GroupedBatchSampler for corner case where there are not enough examples in a category to form a batch (#1677)\r\n* Fix rpn memory leak and dataType errors. (#1657)\r\n* Fix torchvision install due to zippeg egg (#1536)\r\n\r\n## Transforms\r\n\r\n* Make shear operation area preserving (#1529)\r\n* PILLOW_VERSION deprecation updates (#1501)\r\n* Adds optional fill colour to rotate (#1280)\r\n\r\n## Ops\r\n\r\n* Add Deformable Convolution operation. (#1586) (#1660) (#1637)\r\n* Fix inconsistent NMS implementation between CPU and CUDA (#1556)\r\n* Speed up nms_cuda (#1704)\r\n* Implementation for Position-sensitive ROI Pool/Align (#1410)\r\n* Remove cpp extensions in favor of torch ops (#1348)\r\n* Make custom ops differentiable (#1314)\r\n* Fix Windows build in Torchvision Custom op Registration (#1320)\r\n* Revert \"Register Torchvision Ops as Cutom Ops (#1267)\" (#1316)\r\n* Register Torchvision Ops as Cutom Ops (#1267)\r\n* Use Tensor.data_ptr instead of .data (#1262)\r\n* Fix header includes for cpu (#1644)\r\n\r\n## Datasets\r\n\r\n* fixed test for windows by closing the created temporary files (#1662)\r\n* VideoClips windows fixes (#1661)\r\n* Fix VOC on Windows (#1641)\r\n* update dead LSUN link (#1626)\r\n* DatasetFolder should follow links when searching for data (#1580)\r\n* add .tgz support to extract_archive (#1650)\r\n* expose audio_channels as a parameter to kinetics dataset (#1559)\r\n* Implemented integrity check (md5 hash) after dataset download (#1456)\r\n* Move VideoClips dummy dataset to top level for pickling (#1649)\r\n* Remove download for ImageNet (#1457)\r\n* add tar.xz archive handler (#1361)\r\n* Fix DeprecationWarning for collections.Iterable import in LSUN (#1417)\r\n* Support empty target_type for CelebA dataset (#1351)\r\n* VOC2007 support test set (#1340)\r\n* Fix EMNSIT download URL (#1297) (#1318)\r\n* Refactored clip_sampler (#1562)\r\n\r\n## Documentation\r\n\r\n* Fix documentation for NMS (#1614)\r\n* More examples of functional transforms (#1402)\r\n* Fixed doc of crop functionals (#1388)\r\n* Added Training Sample code for fasterrcnn_resnet50_fpn (#1695)\r\n* Fix rpn.py typo (#1276)\r\n* Update README with minimum required version of PyTorch (#1272)\r\n* fix alignment of README (#1396)\r\n* fixed typo in DatasetFolder and ImageFolder (#1284)\r\n\r\n## Models\r\n\r\n* Bugfix for MNASNet (#1224)\r\n* Fix anchor dtype in AnchorGenerator (#1341)\r\n\r\n## Utils\r\n\r\n* Adding File object option to utils.save_image (#1301)\r\n* Fix make_grid: support any number of channels in tensor (#1300)\r\n* Fix bug of changing input tensor in utils.save_image (#1244)\r\n\r\n## Reference scripts\r\n\r\n* add a README for training object detection models (#1612)\r\n* Adding args for names of train and val directories (#1544)\r\n* Fix broken bitwise operation in Similarity Reference loss (#1604)\r\n* Fixing issue #1530 by starting ann_id to 1 in convert_to_coco_api (#1531)\r\n* Add commands for model training (#1203)\r\n* adding documentation for automatic mixed precision training (#1533)\r\n* Fix reference training script for Mask R-CNN for PyTorch 1.2 (during evaluation after epoch, mask datatype became bool, pycocotools expects uint8) (#1413)\r\n* fix a little bug about resume (#1628)\r\n* Better explain lr and batch size in references/detection/train.py (#1233)\r\n* update default parameters in references/detection (#1611)\r\n* Removed code redundancy/refactored inn video_classification (#1549)\r\n* Fix comment in default arguments in references/detection (#1243)\r\n\r\n## Tests\r\n\r\n* Correctness test implemented with old test architecture (#1511)\r\n* Simplify and organize test_ops. (#1551)\r\n* Replace asserts with assertEqual (#1488)(#1499)(#1497)(#1496)(#1498)(#1494)(#1487)(#1495)\r\n* Add expected result tests (#1377)\r\n* Add TorchHub tests to torchvision (#1319)\r\n* Scriptability checks for Tensor Transforms (#1690)\r\n* Add tests for results in script vs eager mode (#1430)\r\n* Test for checking non mutating behaviour of tensor transforms (#1656)\r\n* Disable download tests for Python2 (#1269)\r\n* Fix randomresized params flaky (#1282)\r\n\r\n## CI\r\n\r\n* Disable C++ models from being compiled without explicit request (#1535)\r\n* Fix discrepancy in regenerate.py (#1583)\r\n* soumith -> pytorch for docker images (#1577)\r\n* [wip] try vs2019 toolchain (#1509)\r\n* Make CI use PyTorch nightly (#1492)\r\n* Try enabling Windows CUDA CI (#1486)\r\n* Fix CUDA builds on Windows (#1485)\r\n* Try fix Windows CircleCI (#1433)\r\n* Fix CUDA CI (#1464)\r\n* Change approach for rebase to master (#1427)\r\n* Temporary fix for CI (#1411)\r\n* Use PyTorch 1.3 for CI (#1467)\r\n* Use links from S3 to install CUDA (#1472)\r\n* Enable CUDA 9.2 builds for Windows (#1381)\r\n* Fix nightly builds (#1374)\r\n* Fix Windows CI after #1301 (#1368)\r\n* Retry `anaconda login` for Windows builds (#1366)\r\n* Fix nightly wheels builds for Windows (#1358)\r\n* Fix CI for py2.7 cu100 wheels (#1354)\r\n* Fix Windows CI (#1347)\r\n* Windows build scripts (#1241)\r\n* Make CircleCI checkout merge commit (#1344)\r\n* use native python code generation logic (#1321)\r\n* Add CircleCI (v2) (#1298)",
        "dateCreated": "2020-01-13T14:30:34Z",
        "datePublished": "2020-01-15T22:11:49Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.5.0",
        "name": "Towards better research to production support",
        "tag_name": "v0.5.0",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.5.0",
        "url": "https://api.github.com/repos/pytorch/vision/releases/22829601",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.5.0"
      },
      {
        "authorType": "User",
        "author_name": "fmassa",
        "body": "This minor release introduces an optimized `video_reader` backend for torchvision. It is implemented in C++, and uses FFmpeg internally.\r\n\r\nThe new `video_reader` backend can be up to 6 times faster compared to the `pyav` backend.\r\n- When decoding all video/audio frames in the video, the new `video_reader` is 1.2x - 6x faster depending on the codec and video length.\r\n- When decoding a fixed number of video frames (e.g. [4, 8, 16, 32, 64, 128]), `video_reader` runs equally fast for small values (i.e. [4, 8, 16]) and runs up to 3x faster for large values (e.g. [32, 64, 128]).\r\n\r\n## Using the optimized video backend\r\n\r\n\r\nSwitching to the new backend can be done via `torchvision.set_video_backend('video_reader')` function. By default, we use a backend based on top of [PyAV](https://github.com/mikeboers/PyAV).\r\n\r\nDue to packaging issues with FFmpeg, in order to use the `video_reader` backend one need to first have `ffmpeg` available on the system, and then compile torchvision from source using the instructions from https://github.com/pytorch/vision#installation\r\n\r\n# Deprecations\r\nIn torchvision 0.4.0, the `read_video` and `read_video_timestamps` functions used `pts` relative to the video stream. This could lead to unaligned video-audio being returned in some cases.\r\n\r\ntorchvision now allow to specify a `pts_unit` argument in those functions. The default value is `'pts'` (with same behavior as before), and the user can now specify `pts_unit='sec'`, which produces consistently aligned results for both video and audio. The `'pts'` value is deprecated for now, and kept for backwards-compatibility.\r\n\r\nIn the next release, the default value of `pts_unit` will change to `'sec'`, so that calling `read_video` without specifying `pts_unit` returns consistently aligned audio-video results. This will require users to update their `VideoClips` checkpoints, which used to store the information in `pts` by default.\r\n\r\n# Changelog\r\n- [video reader] inception commit (#1303) 31fad34\r\n- Expose frame-rate and cache to video datasets (#1356) 85ffd93\r\n- Expose num_workers in VideoClips (#1359) 02a8c0a\r\n- Fix randomresized params flaky (#1282) 7c9bbf5\r\n- Video transforms (#1353) 64917bc\r\n- add _backend argument to init() of class VideoClips (#1363) 7874374\r\n- Video clips workers (#1369) 0982395\r\n- modified code of io.read_video and io.read_video_timestamps to intepret pts values in seconds (#1331) 17e355f\r\n- add metadata to video dataset classes. bug fix. more robustness (#1376) 49b01e3\r\n- move sampler into TV core. Update UniformClipSampler (#1408) f0d3daa\r\n- remove hardcoded video extension in kinetics400 dataset (#1418) 929c81d\r\n- Fix hmdb51 and ucf101 typo (#1420) b13931a\r\n- fix a bug related to audio_end_pts (#1431) 1258bb7\r\n- expose more io api (#1423) e48b958\r\n- Make video transforms private (#1429) 79daca1\r\n- extend video reader to support fast video probing (#1437) ed5b2dc\r\n- Better handle corrupted videos (#1463) da89dad\r\n- Temporary fix to remove ffmpeg from build time (#1475) ed04dee\r\n- fix a bug when video decoding fails and empty frames are returned (#1506) 2804c12\r\n- extend DistributedSampler to support group_size (#1512) 355e9d2\r\n- Unify video backend (#1514) 97b53f9\r\n- Unify video metadata in VideoClips (#1527) 7d509c5\r\n- Fixed compute_clips docstring (#1543) b438d32",
        "dateCreated": "2019-11-07T12:41:35Z",
        "datePublished": "2019-11-07T16:33:55Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.4.2",
        "name": "Optimized video reader backend",
        "tag_name": "v0.4.2",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.4.2",
        "url": "https://api.github.com/repos/pytorch/vision/releases/21283569",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.4.2"
      },
      {
        "authorType": "User",
        "author_name": "fmassa",
        "body": "This minor release provides binaries compatible with PyTorch 1.3.\r\n\r\nCompared to version 0.4.0, it contains a single bugfix for `HMDB51` and `UCF101` datasets, fixed in https://github.com/pytorch/vision/pull/1240",
        "dateCreated": "2019-10-11T17:22:38Z",
        "datePublished": "2019-10-30T15:18:42Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.4.1",
        "name": "Compat with PyTorch 1.3 and bugfix",
        "tag_name": "v0.4.1",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.4.1",
        "url": "https://api.github.com/repos/pytorch/vision/releases/21091027",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.4.1"
      },
      {
        "authorType": "User",
        "author_name": "fmassa",
        "body": "This release adds support for video models and datasets, and brings several improvements.\r\n\r\n\r\n**Note**: torchvision 0.4 requires PyTorch 1.2 or newer\r\n\r\n# Highlights\r\n\r\n## Video and IO\r\n\r\nVideo is now a first-class citizen in torchvision. The 0.4 release includes:\r\n\r\n* efficient IO primitives for reading and writing video files\r\n* Kinetics-400, HMDB51 and UCF101 datasets for action recognition, which are compatible with `torch.utils.data.DataLoader`\r\n* Pre-trained models for action recognition, trained on Kinetics-400\r\n*  Training and evaluation scripts for reproducing the training results.\r\n\r\nWriting your own video dataset is easy. We provide an utility class `VideoClips` that simplifies the task of enumerating all possible clips of fixed size in a list of video files by creating an index of all clips in a set of videos. It additionally allows to specify a fixed frame-rate for the videos.\r\n\r\n```python\r\nfrom torchvision.datasets.video_utils import VideoClips\r\n\r\nclass MyVideoDataset(object):\r\n    def __init__(self, video_paths):\r\n        self.video_clips = VideoClips(video_paths,\r\n                                      clip_length_in_frames=16,\r\n                                      frames_between_clips``=1,\r\n                                      frame_rate=15)\r\n\r\n    def __getitem__(self, idx):\r\n        video, audio, info, video_idx = self.video_clips.get_clip(idx)\r\n        return video, audio\r\n    \r\n    def __len__(self):\r\n        return self.video_clips.num_clips()\r\n```\r\n\r\nWe provide pre-trained models for action recognition, trained on Kinetics-400, which reproduce the results on the original papers where they have been first introduced, as well the corresponding training scripts.\r\n\r\n|model\t|clip @ 1\t|\r\n|---\t|---\t|\r\n|r3d_18\t|52.748\t|\r\n|mc3_18\t|53.898\t|\r\n|r2plus1d_18\t|57.498\t|\r\n\r\n# Bugfixes\r\n\r\n* change aspect ratio calculation formula in `references/detection` (#1194)\r\n* bug fixes in ImageNet (#1149)\r\n* fix save_image when height or width equals 1 (#1059)\r\n* Fix STL10 `__repr__` (#969)\r\n* Fix wrong behavior of `GeneralizedRCNNTransform` in Python2. (#960)\r\n\r\n# Datasets\r\n\r\n### New\r\n\r\n* Add USPS dataset (#961)(#1117)\r\n* Added support for the QMNIST dataset (#995)\r\n* Add HMDB51 and UCF101 datasets (#1156)\r\n* Add Kinetics400 dataset (#1077)\r\n\r\n### Improvements\r\n\r\n* Miscellaneous dataset fixes (#1174)\r\n* Standardize str argument verification in datasets (#1167)\r\n* Always pass `transform` and `target_transform` to abstract dataset (#1126)\r\n* Remove duplicate transform assignment in FakeDataset (#1125)\r\n* Automatic extraction for Cityscapes Dataset (#1066) (#1068)\r\n* Use joint transform in Cityscapes (#1024)(#1045)\r\n* CelebA: track attr names, support split=\"all\", code cleanup (#1008)\r\n* Add folds option to STL10 (#914)\r\n\r\n# Models\r\n\r\n### New\r\n\r\n* Add pretrained Wide ResNet (#912)\r\n* Memory efficient densenet (#1003) (#1090)\r\n* Implementation of the MNASNet family of models (#829)(#1043)(#1092)\r\n* Add VideoModelZoo models (#1130)\r\n\r\n### Improvements\r\n\r\n* Fix resnet fpn backbone for resnet18 and resnet34 (#1147)\r\n* Add checks to `roi_heads` in detection module (#1091)\r\n* Make shallow copy of input list in `GeneralizedRCNNTransform` (#1085)(#1111)(#1084)\r\n* Make MobileNetV2 number of channel divisible by 8 (#1005)\r\n* typo fix: ouput -> output in Inception and GoogleNet (#1034)\r\n* Remove empty proposals from the RPN (#1026)\r\n* Remove empty boxes before NMS (#1019)\r\n* Reduce code duplication in segmentation models (#1009)\r\n* allow user to define residual settings in MobileNetV2 (#965)\r\n* Use `flatten` instead of `view` (#1134)\r\n\r\n# Documentation\r\n\r\n* Consistency in detection box format (#1110)\r\n* Fix Mask R-CNN docs (#1089)\r\n* Add paper references to VGG and Resnet variants (#1088)\r\n* Doc, Test Fixes in `Normalize` (#1063)\r\n* Add transforms doc to more datasets (#1038)\r\n* Corrected typo: 5 to 0.5 (#1041)\r\n* Update doc for `torchvision.transforms.functional.perspective` (#1017)\r\n* Improve documentation for `fillcolor` option in `RandomAffine` (#994)\r\n* Fix `COCO_INSTANCE_CATEGORY_NAMES` (#991)\r\n* Added models information to documentation. (#985)\r\n* Add missing import in `faster_rcnn.py` documentation (#979)\r\n* Improve `make_grid` docs (#964)\r\n\r\n# Tests\r\n\r\n* Add test for SVHN (#1086)\r\n* Add tests for Cityscapes Dataset (#1079)\r\n* Update CI to Python 3.6 (#1044)\r\n* Make `test_save_image` more robust (#1037)\r\n* Add a generic test for the datasets (#1015)\r\n* moved fakedata generation to separate module (#1014)\r\n* Create imagenet fakedata on-the-fly (#1012)\r\n* Minor test refactorings (#1011)\r\n* Add test for CIFAR10(0) (#1010)\r\n* Mock MNIST download for less flaky tests (#1004)\r\n* Add test for ImageNet (#976)(#1006)\r\n* Add tests for datasets (#966)\r\n\r\n# Transforms\r\n\r\n### New\r\n\r\n* Add Random Erasing for image augmentation (#909) (#1060) (#1087) (#1095)\r\n\r\n### Improvements\r\n\r\n* Allowing 'F' mode for 1 channel FloatTensor in `ToPILImage` (#1100)\r\n* Add shear parallel to y-axis (#1070)\r\n* fix error message in `to_tensor` (#1000)\r\n* Fix TypeError in `RandomResizedCrop.get_params` (#1036)\r\n* Fix `normalize` for different `dtype` than `float32` (#1021)\r\n\r\n# Ops\r\n\r\n* Renamed `vision.h` files to `vision_cpu.h` and `vision_cuda.h` (#1051)(#1052)\r\n* Optimize `nms_cuda` by avoiding extra `torch.cat` call (#945)\r\n\r\n# Reference scripts\r\n\r\n* Expose data-path in the detection reference scripts (#1109)\r\n* Make `utils.py` work with pytorch-cpu (#1023)\r\n* Add mixed precision training with Apex (#972)(#1124)\r\n* Add reference code for similarity learning (#1101)\r\n\r\n# Build\r\n\r\n* Add windows build steps and wheel build scripts (#998)\r\n* add packaging scripts (#996)\r\n* Allow forcing GPU build with `FORCE_CUDA=1` (#927)\r\n\r\n# Misc\r\n\r\n* Misc lint fixes (#1020)\r\n* Reraise error on failed downloading (#1013)\r\n* add more hub models (#974)\r\n* make C extension lazy-import (#971)",
        "dateCreated": "2019-08-06T23:15:22Z",
        "datePublished": "2019-08-08T15:09:09Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.4.0",
        "name": "Video support, new datasets and models",
        "tag_name": "v0.4.0",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.4.0",
        "url": "https://api.github.com/repos/pytorch/vision/releases/19118646",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.4.0"
      },
      {
        "authorType": "User",
        "author_name": "fmassa",
        "body": "This release brings several new features to torchvision, including models for semantic segmentation, object detection, instance segmentation and person keypoint detection, and custom C++ / CUDA ops specific to computer vision.\r\n\r\n**Note: torchvision 0.3 requires PyTorch 1.1 or newer**\r\n\r\n# Highlights\r\n\r\n## Reference training / evaluation scripts\r\n\r\nWe now provide under the `references/` folder scripts for training and evaluation of the following tasks: classification, semantic segmentation, object detection, instance segmentation and person keypoint detection.\r\nTheir purpose is twofold:\r\n\r\n* serve as a log of how to train a specific model.\r\n* provide baseline training and evaluation scripts to bootstrap research\r\n\r\nThey all have an entry-point `train.py` which performs both training and evaluation for a particular task. Other helper files, specific to each training script, are also present in the folder, and they might get integrated into the torchvision library in the future.\r\n\r\nWe expect users should copy-paste and modify those reference scripts and use them for their own needs.\r\n\r\n## TorchVision Ops\r\n\r\nTorchVision now contains custom C++ / CUDA operators in `torchvision.ops`. Those operators are specific to computer vision, and make it easier to build object detection models.\r\nThose operators currently do not support PyTorch script mode, but support for it is planned for future releases.\r\n\r\nList of supported ops\r\n\r\n* `roi_pool` (and the module version `RoIPool`)\r\n* `roi_align` (and the module version `RoIAlign`)\r\n* `nms`, for non-maximum suppression of bounding boxes\r\n* `box_iou`, for computing the intersection over union metric between two sets of bounding boxes\r\n\r\nAll the other ops present in `torchvision.ops` and its subfolders are experimental, in particular:\r\n\r\n* `FeaturePyramidNetwork` is a module that adds a FPN on top of a module that returns a set of feature maps. \r\n* `MultiScaleRoIAlign` is a wrapper around `roi_align` that works with multiple feature map scales\r\n\r\nHere are a few examples on using torchvision ops:\r\n```python\r\nimport torch\r\nimport torchvision\r\n\r\n# create 10 random boxes\r\nboxes = torch.rand(10, 4) * 100\r\n# they need to be in [x0, y0, x1, y1] format\r\nboxes[:, 2:] += boxes[:, :2]\r\n# create a random image\r\nimage = torch.rand(1, 3, 200, 200)\r\n# extract regions in `image` defined in `boxes`, rescaling\r\n# them to have a size of 3x3\r\npooled_regions = torchvision.ops.roi_align(image, [boxes], output_size=(3, 3))\r\n# check the size\r\nprint(pooled_regions.shape)\r\n# torch.Size([10, 3, 3, 3])\r\n\r\n# or compute the intersection over union between\r\n# all pairs of boxes\r\nprint(torchvision.ops.box_iou(boxes, boxes).shape)\r\n# torch.Size([10, 10])\r\n```\r\n\r\n## Models for more tasks\r\n\r\nThe 0.3 release of torchvision includes pre-trained models for other tasks than image classification on ImageNet.\r\nWe include two new categories of models: region-based models, like Faster R-CNN, and dense pixelwise prediction models, like DeepLabV3.\r\n\r\n### Object Detection, Instance Segmentation and Person Keypoint Detection models\r\n\r\n**Warning: The API is currently experimental and might change in future versions of torchvision**\r\n\r\nThe 0.3 release contains pre-trained models for Faster R-CNN, Mask R-CNN and Keypoint R-CNN, all of them using ResNet-50 backbone with FPN.\r\nThey have been trained on COCO train2017 following the reference scripts in `references/`, and give the following results on COCO val2017\r\n\r\nNetwork | box AP | mask AP | keypoint AP\r\n-- | -- | -- | --\r\nFaster R-CNN ResNet-50 FPN | 37.0 | \u00a0 | \u00a0\r\nMask R-CNN ResNet-50 FPN | 37.9 | 34.6 | \u00a0\r\nKeypoint R-CNN ResNet-50 FPN | 54.6 | \u00a0 | 65.0\r\n\r\nThe implementations of the models for object detection, instance segmentation and keypoint detection are fast, specially during training.\r\n\r\nIn the following table, we use 8 V100 GPUs, with CUDA 10.0 and CUDNN 7.4 to report the results. During training, we use a batch size of 2 per GPU, and during testing a batch size of 1 is used.\r\n\r\nFor test time, we report the time for the model evaluation and post-processing (including mask pasting in image), but not the time for computing the precision-recall.\r\n\r\nNetwork | train time (s / it) | test time (s / it) | memory (GB)\r\n-- | -- | -- | --\r\nFaster R-CNN ResNet-50 FPN | 0.2288 | 0.0590 | 5.2\r\nMask R-CNN ResNet-50 FPN | 0.2728 | 0.0903 | 5.4\r\nKeypoint R-CNN ResNet-50 FPN | 0.3789 | 0.1242 | 6.8\r\n\r\n\r\nYou can load and use pre-trained detection and segmentation models with a few lines of code\r\n\r\n```python\r\nimport torchvision\r\n\r\nmodel = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\r\n# set it to evaluation mode, as the model behaves differently\r\n# during training and during evaluation\r\nmodel.eval()\r\n\r\nimage = PIL.Image.open('/path/to/an/image.jpg')\r\nimage_tensor = torchvision.transforms.functional.to_tensor(image)\r\n\r\n# pass a list of (potentially different sized) tensors\r\n# to the model, in 0-1 range. The model will take care of\r\n# batching them together and normalizing\r\noutput = model([image_tensor])\r\n# output is a list of dict, containing the postprocessed predictions\r\n```\r\n\r\n### Pixelwise Semantic Segmentation models\r\n\r\n**Warning: The API is currently experimental and might change in future versions of torchvision**\r\n\r\nThe 0.3 release also contains models for dense pixelwise prediction on images.\r\nIt adds FCN and DeepLabV3 segmentation models, using a ResNet50 and ResNet101 backbones.\r\nPre-trained weights for ResNet101 backbone are available, and have been trained on a subset of COCO train2017, which contains the same 20 categories as those from Pascal VOC.\r\n\r\nThe pre-trained models give the following results on the subset of COCO val2017 which contain the same 20 categories as those present in Pascal VOC:\r\n\r\nNetwork | mean IoU | global pixelwise acc\r\n-- | -- | --\r\nFCN ResNet101 | 63.7 | 91.9\r\nDeepLabV3 ResNet101 | 67.4 | 92.4\r\n\r\n\r\n# New Datasets\r\n\r\n* Add Caltech101, Caltech256, and CelebA (#775)\r\n* ImageNet dataset (#764) (#858) (#870)\r\n* Added Semantic Boundaries Dataset (#808) (#865)\r\n* Add VisionDataset as a base class for all datasets (#749) (#859) (#838) (#876) (#878)\r\n\r\n# New Models\r\n\r\n## Classification\r\n\r\n* Add GoogLeNet (Inception v1) (#678) (#821) (#828) (#816)\r\n* Add MobileNet V2 (#818) (#917)\r\n* Add ShuffleNet v2 (#849)  (#886) (#889) (#892) (#916)\r\n* Add ResNeXt-50 32x4d and ResNeXt-101 32x8d (#822) (#852) (#917)\r\n\r\n## Segmentation\r\n\r\n* Fully-Convolutional Network (FCN) with ResNet 101 backbone\r\n* DeepLabV3 with ResNet 101 backbone\r\n\r\n## Detection\r\n\r\n* Faster R-CNN R-50 FPN trained on COCO train2017 (#898) (#921)\r\n* Mask R-CNN R-50 FPN trained on COCO train2017 (#898) (#921)\r\n* Keypoint R-CNN R-50 FPN trained on COCO train2017 (#898) (#921) (#922)\r\n\r\n# Breaking changes\r\n\r\n* Make `CocoDataset` ids deterministically ordered (#868)\r\n\r\n# New Transforms\r\n\r\n* Add bias vector to `LinearTransformation` (#793) (#843) (#881)\r\n* Add Random Perspective transform  (#781) (#879)\r\n\r\n# Bugfixes\r\n\r\n* Fix user warning when applying `normalize` (#810)\r\n* Fix logic error in `check_integrity` (#871)\r\n\r\n# Improvements\r\n\r\n* Fixing mutation of 2d tensors in `to_pil_image` (#762)\r\n* Replace `tensor.view` with `tensor.unsqueeze(0)` in `make_grid` (#765)\r\n* Change usage of `view` to `reshape` in `resnet` to enable running with mkldnn (#890)\r\n* Improve `normalize` to work with tensors located on any device (#787)\r\n* Raise an `IndexError` for `FakeData.__getitem__()` if the index would be out of range (#780)\r\n* Aspect ratio is now sampled from a logarithmic distribution in `RandomResizedCrop`. (#799)\r\n* Modernize inception v3 weight initialization code (#824)\r\n* Remove duplicate code from densenet load_state_dict (#827)\r\n* Replace `endswith` calls in a loop with a single `endswith` call in `DatasetFolder` (#832)\r\n* Added missing dot in webp image extensions (#836)\r\n* fix inconsistent behavior for `~` expression (#850)\r\n* Minor Compressions in statements in `folder.py` (#874)\r\n* Minor fix to evaluation formula of `PILLOW_VERSION`  in `transforms.functional.affine` (#895)\r\n* added `is_valid_file` parameter to `DatasetFolder` (#867)\r\n* Add support for joint transformations in `VisionDataset` (#872)\r\n* Auto calculating return dimension of `squeezenet` forward method (#884)\r\n* Added `progress` flag to model getters (#875) (#910)\r\n* Add support for other normalizations (i.e., `GroupNorm`) in `ResNet` (#813)\r\n* Add dilation option to `ResNet` (#866)\r\n\r\n# Testing\r\n\r\n* Add basic model testing. (#811)\r\n* Add test for `num_class` in `test_model.py` (#815)\r\n* Added test for `normalize` functionality in `make_grid` function. (#840)\r\n* Added downloaded directory not empty check in `test_datasets_utils` (#844)\r\n* Added test for `save_image` in utils (#847)\r\n* Added tests for `check_md5` and `check_integrity` (#873)\r\n\r\n# Misc\r\n\r\n* Remove shebang in `setup.py` (#773)\r\n* configurable version and package names (#842)\r\n* More hub models (#851)\r\n* Update travis to use more recent GCC (#891)\r\n\r\n# Documentation\r\n\r\n* Add comments regarding downsampling layers of resnet (#794) \r\n* Remove unnecessary bullet point in InceptionV3 doc (#814)\r\n* Fix `crop` and `resized_crop` docs in `functional.py` (#817)\r\n* Added dimensions in the comments of googlenet (#788) \r\n* Update transform doc with random offset of padding due to `pad_if_needed` (#791)\r\n* Added the argument `transform_input` in docs of InceptionV3 (#789)\r\n* Update documentation for MNIST datasets (#778)\r\n* Fixed typo in `normalize()` function. (#823)\r\n* Fix typo in squeezenet (#841)\r\n* Fix typo in DenseNet comment (#857)\r\n* Typo and syntax fixes to transform docstrings (#887)",
        "dateCreated": "2019-05-22T19:15:46Z",
        "datePublished": "2019-05-22T19:45:45Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.3.0",
        "name": "Training scripts, detection/segmentation models and more",
        "tag_name": "v0.3.0",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.3.0",
        "url": "https://api.github.com/repos/pytorch/vision/releases/17521281",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "fmassa",
        "body": "\r\nThis version introduces several improvements and fixes.\r\n\r\n# Support for arbitrary input sizes for models\r\n\r\nIt is now possible to feed larger images than 224x224 into the models in torchvision.\r\nWe added an adaptive pooling just before the classifier, which adapts the size of the feature maps before the last layer, allowing for larger input images.\r\nRelevant PRs: #744 #747 #746 #672 #643\r\n\r\n# Bugfixes\r\n\r\n* Fix invalid argument error when using lsun method in windows (#508)\r\n* Fix FashionMNIST loading MNIST (#640)\r\n* Fix inception v3 input transform for trace & onnx (#621)\r\n\r\n# Datasets\r\n\r\n* Add support for webp and tiff images in ImageFolder #736 #724\r\n* Add K-MNIST dataset #687\r\n* Add Cityscapes dataset #695 #725 #739 #700\r\n* Add Flicker 8k and 30k datasets #674\r\n* Add VOCDetection and VOCSegmentation datasets #663\r\n* Add SBU Captioned Photo Dataset (#665)\r\n* Updated URLs for EMNIST #726\r\n* MNIST and FashionMNIST now have their own 'raw' and 'processed' folder #601\r\n* Add metadata to some datasets (#501)\r\n\r\n# Improvements\r\n\r\n* Allow RandomCrop to crop in the padded region #564\r\n* ColorJitter now supports min/max values #548\r\n* Generalize resnet to use block.extension #487\r\n* Move area calculation out of for loop in RandomResizedCrop #641\r\n* Add option to zero-init the residual branch in resnet (#498)\r\n* Improve error messages in to_pil_image #673\r\n* Added the option of converting to tensor for numpy arrays having only two dimensions in to_tensor (#686)\r\n* Optimize _find_classes in DatasetFolder via scandir in Python3 (#559)\r\n* Add padding_mode to RandomCrop (#489 #512)\r\n* Make DatasetFolder more generic (#527)\r\n* Add in-place option to normalize (#699)\r\n* Add Hamming and Box interpolations to transforms.py (#693)\r\n* Added the support of 2-channel Image modes such as 'LA' and adding a mode in 4 channel modes (#688)\r\n* Improve support for 'P' image mode in pad (#683)\r\n* Make torchvision depend on pillow-simd if already installed (#522)\r\n* Make tests run faster (#745) \r\n* Add support for non-square crops in RandomResizedCrop (#715)\r\n\r\n# Breaking changes\r\n\r\n* save_images now round to nearest integer #754\r\n\r\n# Misc\r\n\r\n* Added code coverage to travis #703\r\n* Add downloads and docs badge to README (#702)\r\n* Add progress to download_url #497 #524 #535\r\n* Replace 'residual' with 'identity' in resnet.py (#679)\r\n* Consistency changes in the models \r\n* Refactored MNIST and CIFAR to have data and target fields #578 #594\r\n* Update torchvision to newer versions of PyTorch\r\n* Relax assertion in `transforms.Lambda.__init__` (#637)\r\n* Cast MNIST target to int (#605)\r\n* Change default target type of FakedDataset to long (#581)\r\n* Improve docs of functional transforms (#602) \r\n* Docstring improvements\r\n* Add is_image_file to folder_dataset (#507)\r\n* Add deprecation warning in MNIST train[test]_labels[data] (#742)\r\n* Mention TORCH_MODEL_ZOO in models documentation. (#624)\r\n* Add scipy as a dependency to setup.py (#675)\r\n* Added size information for inception v3 (#719)",
        "dateCreated": "2019-03-01T21:40:22Z",
        "datePublished": "2019-02-26T17:13:20Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.2.2",
        "name": "More datasets, transforms and bugfixes ",
        "tag_name": "v0.2.2",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.2.2",
        "url": "https://api.github.com/repos/pytorch/vision/releases/15776822",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.2.2"
      },
      {
        "authorType": "User",
        "author_name": "soumith",
        "body": "This version introduces several fixes and improvements to the previous version.\r\n\r\n## Better printing of Datasets and Transforms\r\n\r\n* Add descriptions to Transform objects.\r\n```python\r\n# Now T.Compose([T.RandomHorizontalFlip(), T.RandomCrop(224), T.ToTensor()]) prints\r\nCompose(\r\n    RandomHorizontalFlip(p=0.5)\r\n    RandomCrop(size=(224, 224), padding=0)\r\n    ToTensor()\r\n)\r\n```\r\n* Add descriptions to Datasets\r\n```python\r\n# now torchvision.datasets.MNIST('~') prints\r\nDataset MNIST\r\n    Number of datapoints: 60000\r\n    Split: train\r\n    Root Location: /private/home/fmassa\r\n    Transforms (if any): None\r\n    Target Transforms (if any): None\r\n```\r\n## New transforms\r\n\r\n* Add RandomApply, RandomChoice, RandomOrder transformations #402 \r\n    * RandomApply: applies a list of transformation with a probability\r\n    * RandomChoice: choose randomly a single transformation from a list\r\n    * RandomOrder: apply transformations in a random order\r\n* Add random affine transformation #411 \r\n\r\n* Add reflect, symmetric and edge padding to `transforms.pad` #460 \r\n\r\n## Performance improvements\r\n\r\n* Speedup MNIST preprocessing by a factor of 1000x\r\n* make weight initialization optional to speed VGG construction. This makes loading pre-trained VGG models much faster\r\n* Accelerate `transforms.adjust_gamma` by using PIL's point function instead of custom numpy-based implementation\r\n\r\n## New Datasets\r\n\r\n* EMNIST - an extension of MNIST for hand-written letters\r\n* OMNIGLOT - a dataset for one-shot learning, with 1623 different handwritten characters from 50 different alphabets\r\n* Add a DatasetFolder class - generalization of ImageFolder\r\n\r\n## Miscellaneous improvements\r\n\r\n* FakeData accepts a seed argument, so having multiple different FakeData instances is now possible\r\n* Use consistent datatypes in Dataset targets. Now all datasets that returns labels will have them as int\r\n* Add probability parameter in `RandomHorizontalFlip` and `RandomHorizontalFlip`\r\n* Replace `np.random` by `random` in transforms - improves reproducibility in multi-threaded environments with default arguments\r\n* Detect tif images in ImageFolder\r\n* Add `pad_if_needed` to `RandomCrop`, so that if the crop size is larger than the image, the image is automatically padded\r\n* Add support in `transforms.ToTensor` for PIL Images with mode '1'\r\n\r\n## Bugfixes\r\n\r\n* Fix passing list of tensors to `utils.save_image`\r\n* single images passed to `make_grid` now are now also normalized\r\n* Fix PIL img close warnings\r\n* Added missing weight initializations to densenet\r\n* Avoid division by zero in `make_grid` when the image is constant\r\n* Fix `ToTensor` when PIL Image has mode F\r\n* Fix bug with `to_tensor` when the input is numpy array of type np.float32.",
        "dateCreated": "2018-04-24T18:16:07Z",
        "datePublished": "2018-04-24T18:16:46Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.2.1",
        "name": "New datasets, transforms and fixes",
        "tag_name": "v0.2.1",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.2.1",
        "url": "https://api.github.com/repos/pytorch/vision/releases/10699830",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.2.1"
      },
      {
        "authorType": "User",
        "author_name": "alykhantejani",
        "body": "This version introduced a functional interface to the transforms, allowing for joint random transformation of inputs and targets. We also introduced a few breaking changes to some datasets and transforms (see below for more details).\r\n\r\n## Transforms\r\nWe have introduced a functional interface for the torchvision transforms, available under `torchvision.transforms.functional`. This now makes it possible to do joint random transformations on inputs and targets, which is especially useful in tasks like object detection, segmentation and super resolution. For example, you can now do the following:\r\n\r\n```python\r\nfrom torchvision import transforms\r\nimport torchvision.transforms.functional as F\r\nimport random\r\n\r\ndef my_segmentation_transform(input, target):\r\n\ti, j, h, w = transforms.RandomCrop.get_params(input, (100, 100))\r\n\tinput = F.crop(input, i, j, h, w)\r\n\ttarget = F.crop(target, i, j, h, w)\r\n\tif random.random() > 0.5:\r\n\t\tinput = F.hflip(input)\r\n\t\ttarget = F.hflip(target)\r\n\tF.to_tensor(input), F.to_tensor(target)\r\n\treturn input, target\r\n```\r\nThe following transforms have also been added:\r\n- [`F.vflip` and `RandomVerticalFlip`](http://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.RandomVerticalFlip)\r\n- [FiveCrop](http://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.FiveCrop) and [TenCrop](http://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.TenCrop)\r\n- Various color transformations:\r\n  - [`ColorJitter`](http://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.ColorJitter)\r\n  - `F.adjust_brightness`\r\n  - `F.adjust_contrast`\r\n  - `F.adjust_saturation`\r\n  - `F.adjust_hue`\r\n- `LinearTransformation` for applications such as whitening\r\n- `Grayscale` and `RandomGrayscale`\r\n- `Rotate` and `RandomRotation`\r\n- `ToPILImage` now supports `RGBA` images\r\n- `ToPILImage` now accepts a `mode` argument so you can specify which colorspace the image should be\r\n- `RandomResizedCrop` now accepts `scale` and `ratio` ranges as input parameters\r\n\r\n## Documentation\r\nDocumentation is now auto generated and publishing to [pytorch.org](http://pytorch.org/docs/master/torchvision/index.html)\r\n\r\n## Datasets:\r\nSEMEION Dataset of handwritten digits added\r\nPhototour dataset patches computed  via multi-scale Harris corners now available by setting `name` equal to `notredame_harris`, `yosemite_harris` or `liberty_harris` in the `Phototour` dataset \r\n\r\n## Bug fixes:\r\n- Pre-trained densenet models is now CPU compatible #251\r\n\r\n## Breaking changes:\r\nThis version also introduced some breaking changes:\r\n- The `SVHN` dataset has now been made consistent with other datasets by making the label for the digit 0 be 0, instead of 10 (as it was previously) (see #194 for more details)\r\n- the `labels` for the unlabelled `STL10` dataset is now an array filled with `-1`\r\n- the order of the input args to the deprecated `Scale` transform has changed from `(width, height)` to `(height, width)` to be consistent with other transforms\r\n",
        "dateCreated": "2017-11-25T18:10:23Z",
        "datePublished": "2017-11-27T18:09:09Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.2.0",
        "name": "v0.2.0: New transforms + a new functional interface",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.2.0",
        "url": "https://api.github.com/repos/pytorch/vision/releases/8654656",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "soumith",
        "body": "- Ability to switch image backends between PIL and accimage\r\n- Added more tests\r\n- Various bug fixes and doc improvements\r\n\r\n## Models\r\n\r\n - Fix for inception v3 input transform bug https://github.com/pytorch/vision/pull/144\r\n - Added pretrained VGG models with batch norm\r\n\r\n## Datasets\r\n\r\n - Fix indexing bug in LSUN dataset (https://github.com/pytorch/vision/pull/177) \r\n - enable `~` to be used in dataset paths\r\n - `ImageFolder` now returns the same (sorted) file order on different machines (https://github.com/pytorch/vision/pull/193)\r\n\r\n## Transforms \r\n\r\n - transforms.Scale now accepts a tuple as new size or single integer\r\n\r\n## Utils\r\n\r\n - can now pass a pad value to make_grid and save_image ",
        "dateCreated": "2017-08-06T06:59:07Z",
        "datePublished": "2017-09-11T16:15:34Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.1.9",
        "name": "More models and some bug fixes",
        "tag_name": "v0.1.9",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.1.9",
        "url": "https://api.github.com/repos/pytorch/vision/releases/7710357",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.1.9"
      },
      {
        "authorType": "User",
        "author_name": "soumith",
        "body": "## New Features\r\n### Models\r\n- SqueezeNet 1.0 and 1.1 models added, along with pre-trained weights\r\n- Add pre-trained weights for VGG models\r\n  - Fix location of dropout in VGG\r\n- `torchvision.models` now expose `num_classes` as a constructor argument\r\n- Add InceptionV3 model and pre-trained weights\r\n- Add DenseNet models and pre-trained weights\r\n\r\n### Datasets\r\n\r\n- Add STL10 dataset\r\n- Add SVHN dataset\r\n- Add PhotoTour dataset\r\n\r\n### Transforms and Utilities\r\n- `transforms.Pad` now allows fill colors of either number tuples, or named colors like `\"white\"`\r\n- add normalization options to `make_grid` and `save_image`\r\n- `ToTensor` now supports more input types\r\n\r\n## Performance Improvements\r\n\r\n## Bug Fixes\r\n- ToPILImage now supports a single image\r\n- Python3 compatibility bug fixes\r\n- `ToTensor` now copes with all PIL Image types, not just RGB images\r\n- ImageFolder now only scans subdirectories.\r\n  - Having files like `.DS_Store` is now no longer a blocking hindrance\r\n  - Check for non-zero number of images in ImageFolder\r\n  - Subdirectories of classes have recursive scans for images\r\n- LSUN test set loads now",
        "dateCreated": "2017-03-29T14:24:48Z",
        "datePublished": "2017-04-03T04:53:18Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.1.8",
        "name": "More models and datasets. Some bugfixes",
        "tag_name": "v0.1.8",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.1.8",
        "url": "https://api.github.com/repos/pytorch/vision/releases/5952131",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.1.8"
      },
      {
        "authorType": "User",
        "author_name": "soumith",
        "body": "A small release, just needed a version bump because of PyPI.",
        "dateCreated": "2017-01-19T20:12:01Z",
        "datePublished": "2017-04-03T04:41:57Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.1.7",
        "name": "Just a version bump",
        "tag_name": "v0.1.7",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.1.7",
        "url": "https://api.github.com/repos/pytorch/vision/releases/5952080",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.1.7"
      },
      {
        "authorType": "User",
        "author_name": "soumith",
        "body": "## New Features\r\n- Add `torchvision.models`: Definitions and pre-trained models for common vision models\r\n  - ResNet, AlexNet, VGG models added with downloadable pre-trained weights \r\n- adding padding to RandomCrop. Also add `transforms.Pad`\r\n- Add MNIST dataset\r\n\r\n## Performance Fixes\r\n- Fixing performance of LSUN Dataset\r\n\r\n\r\n## Bug Fixes\r\n- Some Python3 fixes\r\n- Bug fixes in save_image, add single channel support\r\n",
        "dateCreated": "2017-01-19T00:54:56Z",
        "datePublished": "2017-04-03T04:40:39Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.1.6",
        "name": "Add models and modelzoo, some bugfixes",
        "tag_name": "v0.1.6",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.1.6",
        "url": "https://api.github.com/repos/pytorch/vision/releases/5952074",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.1.6"
      },
      {
        "authorType": "User",
        "author_name": "soumith",
        "body": "Introduced Datasets and Transforms.\r\n\r\nAdded common datasets\r\n\r\n- COCO (Captioning and Detection)\r\n- LSUN Classification\r\n- ImageFolder\r\n- Imagenet-12\r\n- CIFAR10 and CIFAR100\r\n\r\n- Added utilities for saving images from Tensors.\r\n",
        "dateCreated": "2016-11-18T06:44:22Z",
        "datePublished": "2017-04-03T04:35:26Z",
        "html_url": "https://github.com/pytorch/vision/releases/tag/v0.1.5",
        "name": "First release",
        "tag_name": "v0.1.5",
        "tarball_url": "https://api.github.com/repos/pytorch/vision/tarball/v0.1.5",
        "url": "https://api.github.com/repos/pytorch/vision/releases/5952057",
        "zipball_url": "https://api.github.com/repos/pytorch/vision/zipball/v0.1.5"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10585,
      "date": "Fri, 24 Dec 2021 06:33:56 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "computer-vision",
      "machine-learning"
    ],
    "technique": "GitHub API"
  }
}