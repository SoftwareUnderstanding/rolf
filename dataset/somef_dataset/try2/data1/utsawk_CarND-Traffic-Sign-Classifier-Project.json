{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1502.03167\n\n[5]. https://arxiv.org/abs/1805.11604\n\n[6]. http://cs231n.github.io/neural-networks-3/#anneal\n\n[7]. https://arxiv.org/pdf/1409.1556.pdf\n\n",
      "https://arxiv.org/abs/1805.11604\n\n[6]. http://cs231n.github.io/neural-networks-3/#anneal\n\n[7]. https://arxiv.org/pdf/1409.1556.pdf\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[image1]: ./writeup_images/histogram_training.png \"Histogram of training data\"\n[image2]: ./writeup_images/histogram_valid.png \"Histogram of validation data\"\n[image3]: ./writeup_images/mean_std.png \"Mean and standard deviation of data\"\n[image4]: ./writeup_images/Equalization.png \"Equalization techniques considered\"\n[image5]: ./writeup_images/Problem_1.png \"Children crossing\"\n[image6]: ./writeup_images/Problem_2.png \"Bumpy road\"\n[image7]: ./writeup_images/internet_images.png \"Internet images\"\n[image8]: ./writeup_images/softmax1.png \"Softmax 1\"\n[image9]: ./writeup_images/softmax2.png \"Softmax 2\"\n[image10]: ./writeup_images/softmax3.png \"Softmax 3\"\n[image11]: ./writeup_images/softmax4.png \"Softmax 4\"\n[image12]: ./writeup_images/softmax5.png \"Softmax 5\"\n[image13]: ./writeup_images/softmax6.png \"Softmax 6\"\n[image14]: ./writeup_images/augmentation.png \"Augmentation\"\n[image15]: ./writeup_images/arch.jpg \"Architecture\"\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8906174419333412,
        0.9660991539358709,
        0.9944484218006108,
        0.9944484218006108
      ],
      "excerpt": "[2]. https://docs.opencv.org/3.1.0/d5/daf/tutorial_py_histogram_equalization.html \n[3]. http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf \n[4]. https://arxiv.org/abs/1502.03167 \n[5]. https://arxiv.org/abs/1805.11604 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/utsawk/CarND-Traffic-Sign-Classifier-Project",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-07T03:27:17Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-08-14T18:13:31Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I used the shape() property to get the shapes of of training, validation and test datasets. Shape can also be used to find the shape of traffic sign images. Number of classes can be found out using signnames.csv or finding unique entries in the training set - I use the latter\n\n* The size of training set is 34799\n* The size of the validation set is 4410\n* The size of test set is 12630\n* The shape of a traffic sign image is (32, 32, 3)\n* The number of unique classes/labels in the data set is 43\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9748956060950598,
        0.9683383207528309,
        0.8412770035657463
      ],
      "excerpt": "The goals / steps of this project are the following: \n* Load the data set (see below for links to the project data set) \n* Explore, summarize and visualize the data set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8784454581602327,
        0.9363581332277736
      ],
      "excerpt": "* Analyze the softmax probabilities of the new images \n* Summarize the results with a written report \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8673821460156319,
        0.9362574425577712,
        0.9956438268838141,
        0.9723134222867741
      ],
      "excerpt": "I plot the normalized histogram of the both the training and validation dataset -  it can be seen that both of the datasets have similar distributions. It can also be seen that some image categories are under-represented like Class \n![Histogram of training data][image1] \n![Histogram of validation data][image2] \nI also plot the mean and standard deviation image. It can be seen from these images that the center of the image carries the traffic sign. The standard deviation is interesting because most of the image is dark - I would have expected the region close to the borders of the image to be varying in pixel intensity because of the varied background of traffic sign images. However, all the images are cropped with traffic sign occupying the majority of the image leading to low standard deviation throughout the 32*32 image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.841265887305762
      ],
      "excerpt": "My final model consisted of the following layers: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8993509578918676
      ],
      "excerpt": "The overall achitecture is presented in the figure below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8830941494505625,
        0.9291079812218755,
        0.9823830954062169
      ],
      "excerpt": "1. Xavier initialization [3]: I saw marked differences in early (in epochs) performance based on the starting weights. When using truncated normal, the training/validation performance was heavily dependent on the mean and standard deviation chosen. The same was true for normal distribution. I used Xavier initiation and immediately saw improvement in early epochs. \n2. Batch normalization [4]: I tried batch normalization and saw faster convergence. Even though running it on my computer was taking more time per epoch, batch norm lead to faster convergence (in number of epochs). The exact reasons for batch norm's effectiveness are still poorly understood and is an active area of research [5]. I applied batch normalization before the RELU activation in all the layers, though recently people have been using it post the RELU activation. \n3. Regularization: I experimented a lot with dropout probabilities for the different layers and ended up using 0.25 for convolutional layer and concat layer in addition to a dropout of 0.5 for fully connected layers. Without dropout, the network was overfitting easily, which is usually a good sign that the network is implemented correctly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8841046468695618,
        0.9600361085652824,
        0.8288559804026229,
        0.9942556999674067,
        0.9944729289125198
      ],
      "excerpt": "5. Batch size of 128.  \n6. Adam optimizer: I started with Adam optimizer and it worked well and I did not get a chance to experiment with other optimizers. \n7. 100 epochs was used for final submission even though the model seemed to have converged with very few epochs. \nThe code for making predictions on my final model is located in the 40th cell of the Ipython notebook. \nFor most of the images, the softmax probabilities of the correct labels are high, except for the road work sign, which has almost equal softmax probability as the right-of-way at next intersection. This is probably because of the tree being in the background of this particular road sign that confuses the Conv Net. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9299521347009448
      ],
      "excerpt": "| 1                     | Right-of-way at the next intersection                                     |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003317140197043
      ],
      "excerpt": "| 0.412                 | Road work                             | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Udacity CarND Traffic Sign Classifier Project",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/utsawk/CarND-Traffic-Sign-Classifier-Project/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "My final model results were:\n* training set accuracy of 100%\n* validation set accuracy of 98.8% \n* test set accuracy of 98.0%\n\n\nI tried the following architectures: \n1. LeNet (shown in lecture) and enhancements to it including adapting learning rate, dropout for different layers, etc. This is present as a function in my final submission.\n2. A VGGNet [7] like architecture (not that deep, but employing same padding level) with 3 convolution layers (convolution+batch norm+RELU+max pooling) and two fully connected layers with adaptive learning rate and dropouts. I excluded this in the final submission .\n2. Sermanet architecture shown in [1]. I tried two flavors of it and immediately saw improvement. The main idea here is to \"short-circuit\" the output of the first convolutional layer directly into the fully connected layer. I saw a marked improvment in the convergence time with this method. The validation accuracy in every run was ~0.97 in just 3 epochs. For the final submission, I let it run for 100 epochs. **The final architecture is based on this and described below. The implementation is SermaNet2() in my submission.**\n\nMy journey to submission was long and I spent a lot of time experimenting with hyperparameters:\n* I started with the LeNet architecture and tried to study the rest of the design components like data augmentation, weight initialization, learning rate, dropout, batch normalization as described in next few bullets. \n* I started with initial weight optimization study and quickly observed that covergence rate was heavily dependent on initialization hyperparameters of mean/standard deviation and also distribution (truncated gaussian v/s gaussian). I ended up using Xavier initialization after which I never had to worry about weight initialization.\n* The second hyperparameter I played with was learning rate. I saw marginal improvement on using learning rate adaptation of reducing it by 0.1 every 20 epochs and continued using it for the rest of the project. I kept a flag to turn off adaptation every now and then to test its effectiveness.\n* With the above steps, the model continued to overfit the training data with ~94% accuracy on validation data. I introduced dropout into the model and the validation accuracy improved to ~96%.\n* I added batch normalization and it improved convergence rate. I kept a flag and experimented with turning it off and on.\n* I wanted to further improve the accuracy and started looking at other architectures like GoogleNet, SermaNet, VGGNet, etc. I implemented SermaNet to the best of my understanding with much smaller feature sizes than in the paper. For example, the paper uses 108-108 filter depth and I used 12-32 filter depth in my submission. I implemented two different flavors of the concatenation layer - one concatenating the second layer with the output of a third convolutional layer and another concatenating output of first and second convolution layer. The latter has lesser parameters and gives better performance and was used for the final submission.\n* In the end I tried the VGGNet-like architecture mentioned above, though it gave me slightly lower accuracy than the final submission.\n \n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 02:13:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/utsawk/CarND-Traffic-Sign-Classifier-Project/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "utsawk/CarND-Traffic-Sign-Classifier-Project",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/utsawk/CarND-Traffic-Sign-Classifier-Project/master/Traffic_Sign_Classifier.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Here are the results of the prediction:\n\n| Image\t\t\t        |     Prediction\t        \t\t\t\t\t| \n|:---------------------:|:---------------------------------------------:| \n| Right-of-way at the next intersection      \t\t| Right-of-way at the next intersection   \t\t\t\t\t\t\t\t\t| \n| Bumpy road     \t\t\t| Bumpy road \t\t\t\t\t\t\t\t\t\t|\n| Slippery road\t\t\t\t\t| Slippery road\t\t\t\t\t\t\t\t\t\t|\n| Road work\t      \t\t| Road work\t\t\t\t\t \t\t\t\t|\n| Children crossing\t\t\t| Children crossing      \t\t\t\t\t\t\t|\n| Speed limit (60km/h) | Speed limit (60km/h)      |\n\n\nThe model was able to correctly guess 6 out of 6 traffic signs, which gives an accuracy of 100%. This compares favorably to the accuracy on the test set of 98%\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "My final model results were:\n* training set accuracy of 100%\n* validation set accuracy of 98.8% \n* test set accuracy of 98.0%\n\n\nI tried the following architectures: \n1. LeNet (shown in lecture) and enhancements to it including adapting learning rate, dropout for different layers, etc. This is present as a function in my final submission.\n2. A VGGNet [7] like architecture (not that deep, but employing same padding level) with 3 convolution layers (convolution+batch norm+RELU+max pooling) and two fully connected layers with adaptive learning rate and dropouts. I excluded this in the final submission .\n2. Sermanet architecture shown in [1]. I tried two flavors of it and immediately saw improvement. The main idea here is to \"short-circuit\" the output of the first convolutional layer directly into the fully connected layer. I saw a marked improvment in the convergence time with this method. The validation accuracy in every run was ~0.97 in just 3 epochs. For the final submission, I let it run for 100 epochs. **The final architecture is based on this and described below. The implementation is SermaNet2() in my submission.**\n\nMy journey to submission was long and I spent a lot of time experimenting with hyperparameters:\n* I started with the LeNet architecture and tried to study the rest of the design components like data augmentation, weight initialization, learning rate, dropout, batch normalization as described in next few bullets. \n* I started with initial weight optimization study and quickly observed that covergence rate was heavily dependent on initialization hyperparameters of mean/standard deviation and also distribution (truncated gaussian v/s gaussian). I ended up using Xavier initialization after which I never had to worry about weight initialization.\n* The second hyperparameter I played with was learning rate. I saw marginal improvement on using learning rate adaptation of reducing it by 0.1 every 20 epochs and continued using it for the rest of the project. I kept a flag to turn off adaptation every now and then to test its effectiveness.\n* With the above steps, the model continued to overfit the training data with ~94% accuracy on validation data. I introduced dropout into the model and the validation accuracy improved to ~96%.\n* I added batch normalization and it improved convergence rate. I kept a flag and experimented with turning it off and on.\n* I wanted to further improve the accuracy and started looking at other architectures like GoogleNet, SermaNet, VGGNet, etc. I implemented SermaNet to the best of my understanding with much smaller feature sizes than in the paper. For example, the paper uses 108-108 filter depth and I used 12-32 filter depth in my submission. I implemented two different flavors of the concatenation layer - one concatenating the second layer with the output of a third convolutional layer and another concatenating output of first and second convolution layer. The latter has lesser parameters and gives better performance and was used for the final submission.\n* In the end I tried the VGGNet-like architecture mentioned above, though it gave me slightly lower accuracy than the final submission.\n \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Inspired by [1], I tried two image equalization techniques - histogram equalization and CLAHE (Contrast Limited Adaptive Histogram Equalization) applied to grayscale images [2]. Both these techniques improve the contrast in the image as shown in figure below (figure shows original image, histogram equalized image and CLAHE filtered image from left to right). The 70 in the image is hardly visible in the first image, however, the equalization techniques enhance the image immensely.\n\n![Equalization techniques considered][image4]\n\nI decided to use CLAHE (on grayscale images) for data preprocessing here because histogram equalization does not work well when there are large intensity variations in an image. This is easier to demonstrate on larger images but a couple of examples where histogram equalization does not work well are shown below (as before, figure shows original image, histogram equalized image and CLAHE filtered image from left to right).\n\n![Children crossing][image5]\n\n![Bumpy road][image6]\n\nAdditionally, I tried a few data augmentation techniques and ended up using the following augmentations:\n* Image rotated randomly in the range +/-[5, 15] degrees and then scaled by 0.9 or 1.1\n* Randomly perturbed in both horizontal and vertical directions by [-2, 2] pixels\n* Motion blurred with a kernel of size 2\n\nThe figure below shows the original RGB image and four processed images used for training (CLAHE filtered grayscale image, scaled and roated, randomly perturbed, and motion blurred)\n\n![augmentation][image14]\n\nNote that the augmentation is applied to grayscaled and CLAHE filtered images. This gives a dataset that is four times the original dataset. Note that each copy of training set image is augmented to produce 4 images and I do not selectively choose certain image categories to augment. Such datasets may represent natural distributions and thus it may not be a good idea to augment unevenly. This is because Augmentation should increase the robustness of the model when seeing unseen images.\n\nI centred the image around the pixel mean and normalized with the standard deviation because I wanted to center the data around zero and have similar ranges for the pixels. Images under different light variations can have largely different pixel values and we desire the network to learn other features in the image than the light conditions, thus centering around the mean and normalization helps the learning process. Normalization also ensures similar values of gradient while doing backpropagation and helps prevent gradient saturation (not too relevant here because image data is already upper bounded).\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "I used the shape() property to get the shapes of of training, validation and test datasets. Shape can also be used to find the shape of traffic sign images. Number of classes can be found out using signnames.csv or finding unique entries in the training set - I use the latter\n\n* The size of training set is 34799\n* The size of the validation set is 4410\n* The size of test set is 12630\n* The shape of a traffic sign image is (32, 32, 3)\n* The number of unique classes/labels in the data set is 43\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You're reading it! and here is a link to my [project code](https://github.com/utsawk/CarND-Traffic-Sign-Classifier-Project/blob/master/Traffic_Sign_Classifier.ipynb)\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8084466106252886
      ],
      "excerpt": "![Histogram of training data][image1] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8001406589839741
      ],
      "excerpt": "5. Batch size of 128.  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/utsawk/CarND-Traffic-Sign-Classifier-Project/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "HTML",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2016-2018 Udacity, Inc.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "**Traffic Sign Recognition**",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CarND-Traffic-Sign-Classifier-Project",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "utsawk",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/utsawk/CarND-Traffic-Sign-Classifier-Project/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 02:13:13 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You're reading it! and here is a link to my [project code](https://github.com/utsawk/CarND-Traffic-Sign-Classifier-Project/blob/master/Traffic_Sign_Classifier.ipynb)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Here are six German traffic signs that I found on the web:\n\n![internet_images][image7]\n\nOther than intentionally picking images that cover the majority of the height and width of the image, I tried to be impartial in selecting the image. The reason I did this was the training data set has images in which traffic sign occupies the majority of the pixel space. I resized the image to 32x32 to fit the modelling. Most images have watermark on them and some of them have varied backgrounds.\n\n",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You're reading it! and here is a link to my [project code](https://github.com/utsawk/CarND-Traffic-Sign-Classifier-Project/blob/master/Traffic_Sign_Classifier.ipynb)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Inspired by [1], I tried two image equalization techniques - histogram equalization and CLAHE (Contrast Limited Adaptive Histogram Equalization) applied to grayscale images [2]. Both these techniques improve the contrast in the image as shown in figure below (figure shows original image, histogram equalized image and CLAHE filtered image from left to right). The 70 in the image is hardly visible in the first image, however, the equalization techniques enhance the image immensely.\n\n![Equalization techniques considered][image4]\n\nI decided to use CLAHE (on grayscale images) for data preprocessing here because histogram equalization does not work well when there are large intensity variations in an image. This is easier to demonstrate on larger images but a couple of examples where histogram equalization does not work well are shown below (as before, figure shows original image, histogram equalized image and CLAHE filtered image from left to right).\n\n![Children crossing][image5]\n\n![Bumpy road][image6]\n\nAdditionally, I tried a few data augmentation techniques and ended up using the following augmentations:\n* Image rotated randomly in the range +/-[5, 15] degrees and then scaled by 0.9 or 1.1\n* Randomly perturbed in both horizontal and vertical directions by [-2, 2] pixels\n* Motion blurred with a kernel of size 2\n\nThe figure below shows the original RGB image and four processed images used for training (CLAHE filtered grayscale image, scaled and roated, randomly perturbed, and motion blurred)\n\n![augmentation][image14]\n\nNote that the augmentation is applied to grayscaled and CLAHE filtered images. This gives a dataset that is four times the original dataset. Note that each copy of training set image is augmented to produce 4 images and I do not selectively choose certain image categories to augment. Such datasets may represent natural distributions and thus it may not be a good idea to augment unevenly. This is because Augmentation should increase the robustness of the model when seeing unseen images.\n\nI centred the image around the pixel mean and normalized with the standard deviation because I wanted to center the data around zero and have similar ranges for the pixels. Images under different light variations can have largely different pixel values and we desire the network to learn other features in the image than the light conditions, thus centering around the mean and normalization helps the learning process. Normalization also ensures similar values of gradient while doing backpropagation and helps prevent gradient saturation (not too relevant here because image data is already upper bounded).\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "My final model results were:\n* training set accuracy of 100%\n* validation set accuracy of 98.8% \n* test set accuracy of 98.0%\n\n\nI tried the following architectures: \n1. LeNet (shown in lecture) and enhancements to it including adapting learning rate, dropout for different layers, etc. This is present as a function in my final submission.\n2. A VGGNet [7] like architecture (not that deep, but employing same padding level) with 3 convolution layers (convolution+batch norm+RELU+max pooling) and two fully connected layers with adaptive learning rate and dropouts. I excluded this in the final submission .\n2. Sermanet architecture shown in [1]. I tried two flavors of it and immediately saw improvement. The main idea here is to \"short-circuit\" the output of the first convolutional layer directly into the fully connected layer. I saw a marked improvment in the convergence time with this method. The validation accuracy in every run was ~0.97 in just 3 epochs. For the final submission, I let it run for 100 epochs. **The final architecture is based on this and described below. The implementation is SermaNet2() in my submission.**\n\nMy journey to submission was long and I spent a lot of time experimenting with hyperparameters:\n* I started with the LeNet architecture and tried to study the rest of the design components like data augmentation, weight initialization, learning rate, dropout, batch normalization as described in next few bullets. \n* I started with initial weight optimization study and quickly observed that covergence rate was heavily dependent on initialization hyperparameters of mean/standard deviation and also distribution (truncated gaussian v/s gaussian). I ended up using Xavier initialization after which I never had to worry about weight initialization.\n* The second hyperparameter I played with was learning rate. I saw marginal improvement on using learning rate adaptation of reducing it by 0.1 every 20 epochs and continued using it for the rest of the project. I kept a flag to turn off adaptation every now and then to test its effectiveness.\n* With the above steps, the model continued to overfit the training data with ~94% accuracy on validation data. I introduced dropout into the model and the validation accuracy improved to ~96%.\n* I added batch normalization and it improved convergence rate. I kept a flag and experimented with turning it off and on.\n* I wanted to further improve the accuracy and started looking at other architectures like GoogleNet, SermaNet, VGGNet, etc. I implemented SermaNet to the best of my understanding with much smaller feature sizes than in the paper. For example, the paper uses 108-108 filter depth and I used 12-32 filter depth in my submission. I implemented two different flavors of the concatenation layer - one concatenating the second layer with the output of a third convolutional layer and another concatenating output of first and second convolution layer. The latter has lesser parameters and gives better performance and was used for the final submission.\n* In the end I tried the VGGNet-like architecture mentioned above, though it gave me slightly lower accuracy than the final submission.\n \n\n",
      "technique": "Header extraction"
    }
  ]
}