{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Many thanks to [CenterTrack](https://github.com/xingyizhou/CenterTrack) authors for their great framework!\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find it useful in your research, please consider citing our paper as follows:\n\n    @inproceedings{Wu2021TraDeS,\n    title={Track to Detect and Segment: An Online Multi-Object Tracker},\n    author={Wu, Jialian and Cao, Jiale and Song, Liangchen and Wang, Yu and Yang, Ming and Yuan, Junsong},\n    booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n    year={2021}}\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9999639447268318
      ],
      "excerpt": "Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, Junsong Yuan       \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JialianW/TraDeS",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-12T23:15:35Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T02:15:29Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8608104845705933
      ],
      "excerpt": "In CVPR, 2021. [Paper] [Project Page] [Demo (YouTube) (bilibili)] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8901588448408142
      ],
      "excerpt": "As reported in the MvMHAT paper, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8497469733840912
      ],
      "excerpt": "Please refer to Data.md for dataset preparation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9498942916563787
      ],
      "excerpt": "We follow CenterTrack which uses CrowdHuman to pretrain 2D object tracking model. Only the training set is used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Track to Detect and Segment: An Online Multi-Object Tracker (CVPR 2021)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JialianW/TraDeS/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 87,
      "date": "Sat, 25 Dec 2021 17:26:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JialianW/TraDeS/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "JialianW/TraDeS",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/JialianW/TraDeS/master/mot_switch_version.sh",
      "https://raw.githubusercontent.com/JialianW/TraDeS/master/nuscenes_switch_version.sh",
      "https://raw.githubusercontent.com/JialianW/TraDeS/master/experiments/mot17_full.sh",
      "https://raw.githubusercontent.com/JialianW/TraDeS/master/experiments/nuScenes_train.sh",
      "https://raw.githubusercontent.com/JialianW/TraDeS/master/experiments/youtube_vis.sh",
      "https://raw.githubusercontent.com/JialianW/TraDeS/master/experiments/mot17_train.sh",
      "https://raw.githubusercontent.com/JialianW/TraDeS/master/experiments/mot17_test.sh",
      "https://raw.githubusercontent.com/JialianW/TraDeS/master/experiments/crowdhuman.sh",
      "https://raw.githubusercontent.com/JialianW/TraDeS/master/experiments/nuScenes_test.sh",
      "https://raw.githubusercontent.com/JialianW/TraDeS/master/src/lib/model/networks/DCNv2/make.sh",
      "https://raw.githubusercontent.com/JialianW/TraDeS/master/src/tools/get_mot_17.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please refer to [INSTALL.md](readme/INSTALL.md) for installation instructions.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9164640256136145
      ],
      "excerpt": "sh experiments/mot17_test.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164640256136145
      ],
      "excerpt": "sh experiments/mot17_train.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023697225149864,
        0.9164640256136145,
        0.8089808825217336,
        0.9023697225149864
      ],
      "excerpt": "sh nuscenes_switch_version.sh \nsh experiments/nuScenes_test.sh \nTo switch back to the API versions for MOT experiments, you can run: \nsh mot_switch_version.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164640256136145
      ],
      "excerpt": "sh experiments/nuScenes_train.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164640256136145
      ],
      "excerpt": "sh experiments/crowdhuman.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8708182384332275
      ],
      "excerpt": "Test on MOT17 validation set: Place the MOT model in $TraDeS_ROOT/models/ and run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8851485980642128
      ],
      "excerpt": "Train on MOT17 halftrain set: Place the pretrained model in $TraDeS_ROOT/models/ and run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9045089647669573
      ],
      "excerpt": "Train on nuScenes train set: Place the pretrained model in $TraDeS_ROOT/models/ and run: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JialianW/TraDeS/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "Cuda",
      "C",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2019, Charles Shang\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\n3. Neither the name of the copyright holder nor the names of its\\n   contributors may be used to endorse or promote products derived from\\n   this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Track to Detect and Segment: An Online Multi-Object Tracker (CVPR 2021)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "TraDeS",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "JialianW",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JialianW/TraDeS/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Before run the demo, first download our trained models:\n[CrowdHuman model](https://drive.google.com/file/d/1pljgwSecg50OhCTc2yCEhEBY3AwvPFlp/view?usp=sharing) (2D tracking),\n[MOT model](https://drive.google.com/file/d/18DQi6LqFuO7_2QObvZSNK2y_F8yXT17p/view?usp=sharing) (2D tracking) or [nuScenes model](https://drive.google.com/file/d/1PHcDPIvb6owVuMZKR_YieyYN12IhbQLl/view?usp=sharing) (3D tracking). \nThen, put the models in `TraDeS_ROOT/models/` and `cd TraDeS_ROOT/src/`. **The demo result will be saved as a video in `TraDeS_ROOT/results/`.**\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 439,
      "date": "Sat, 25 Dec 2021 17:26:02 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Before run the demo, first download our trained models:\n[CrowdHuman model](https://drive.google.com/file/d/1pljgwSecg50OhCTc2yCEhEBY3AwvPFlp/view?usp=sharing) (2D tracking),\n[MOT model](https://drive.google.com/file/d/18DQi6LqFuO7_2QObvZSNK2y_F8yXT17p/view?usp=sharing) (2D tracking) or [nuScenes model](https://drive.google.com/file/d/1PHcDPIvb6owVuMZKR_YieyYN12IhbQLl/view?usp=sharing) (3D tracking). \nThen, put the models in `TraDeS_ROOT/models/` and `cd TraDeS_ROOT/src/`. **The demo result will be saved as a video in `TraDeS_ROOT/results/`.**\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "**Demo for a video clip from MOT dataset**: Run the demo (using the [MOT model](https://drive.google.com/file/d/18DQi6LqFuO7_2QObvZSNK2y_F8yXT17p/view?usp=sharing)):\n\n    python demo.py tracking --dataset mot --load_model ../models/mot_half.pth --demo ../videos/mot_mini.mp4 --pre_hm --ltrb_amodal --pre_thresh 0.5 --track_thresh 0.4 --inference --clip_len 3 --trades --save_video --resize_video --input_h 544 --input_w 960\n\n**Demo for a video clip which we randomly selected from YouTube**: Run the demo (using the [CrowdHuman model](https://drive.google.com/file/d/1pljgwSecg50OhCTc2yCEhEBY3AwvPFlp/view?usp=sharing)):\n\n    python demo.py tracking --load_model ../models/crowdhuman.pth --num_class 1 --demo ../videos/street_2d.mp4 --pre_hm --ltrb_amodal --pre_thresh 0.5 --track_thresh 0.5 --inference --clip_len 2 --trades --save_video --resize_video --input_h 480 --input_w 864\n\n**Demo for your own video or image folder**: Please specify the file path after `--demo` and run (using the [CrowdHuman model](https://drive.google.com/file/d/1pljgwSecg50OhCTc2yCEhEBY3AwvPFlp/view?usp=sharing)):\n\n    python demo.py tracking --load_model ../models/crowdhuman.pth --num_class 1 --demo $path to your video or image folder$ --pre_hm --ltrb_amodal --pre_thresh 0.5 --track_thresh 0.5 --inference --clip_len 2 --trades --save_video --resize_video --input_h $your_input_h$ --input_w $your_input_w$\n\n\n(Some Notes: (i) For 2D tracking, the models are only used for person tracking, since our method is only trained on CrowdHuman or MOT. You may train a model on COCO or your own dataset for multi-category 2D object tracking. \n(ii) `--clip_len` is set to 3 for MOT; otherwise, it should be 2. You may refer to our paper for this detail. (iii) The CrowdHuman model is more able to generalize to real world scenes than the MOT model. Note that both datasets are in non-commercial licenses.\n(iii) `input_h` and `input_w` shall be evenly divided by 32.)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "**Demo for a video clip from nuScenes dataset**: Run the demo (using the [nuScenes model](https://drive.google.com/file/d/1PHcDPIvb6owVuMZKR_YieyYN12IhbQLl/view?usp=sharing)):\n\n    python demo.py tracking,ddd --dataset nuscenes --load_model ../models/nuscenes.pth --demo ../videos/nuscenes_mini.mp4 --pre_hm --track_thresh 0.1 --inference --clip_len 2 --trades --save_video --resize_video --input_h 448 --input_w 800 --test_focal_length 633\n\n(You will need to specify test_focal_length for monocular 3D tracking demo to convert the image coordinate system back to 3D. The value 633 is half of a typical focal length (~1266) in nuScenes dataset in input resolution 1600x900. The mini demo video is in an input resolution of 800x448, so we need to use a half focal length. You don't need to set the test_focal_length when testing on the original nuScenes data.)\n\nYou can also refer to [CenterTrack](https://github.com/xingyizhou/CenterTrack) for the usage of webcam demo (code is available in this repo, but we have not tested yet).\n\n",
      "technique": "Header extraction"
    }
  ]
}