{
  "acknowledgments": {
    "confidence": [
      1.0
    ],
    "excerpt": "\\addcontentsline{toc}{chapter}{Acknowledgments}\n\\thispagestyle{empty}\n\\vspace*{20mm}\n\\begin{center}\n{\\usekomafont{section} Acknowledgments}\n\\end{center}\n\\vspace{10mm}\nI want to thank all the people that helped and contributed to this thesis. I further want to express my deepest gratitude to Gerhard Hagerer and Georg Groh. \n\\medskip\nThey gave me the opportunity to work on this fantastic topic and write my master's thesis. The weekly meetings gave me the motivation, feedback, and advice needed to complete this thesis. \n\\medskip\nIt is not common to have advisors who invest this much energy, time and passion for helping their students. I could always schedule a meeting with my advisor, and we would discuss how to proceed and talked about a lot of interesting topics together. Thanks a lot for this great time.\n\\medskip\nI would also like to thank my family, close friends and my girlfriend. They understood when I did not have time for them. However, for some reason, they still supported me regardless, with advice and freshly cooked meals when I did not have time to cook.\n\\cleardoublepage{}",
    "technique": "File Exploration"
  },
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/felixSchober/ABSA-Transformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-17T11:53:48Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-13T18:05:10Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Adds a short description for the log files.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9794968895496938,
        0.8589340703584322,
        0.8429634040554889,
        0.9890561780207763,
        0.9515524061584157,
        0.9951913051656124,
        0.9921753707767679,
        0.9755946036512252,
        0.975064371475579,
        0.8735108631787201
      ],
      "excerpt": "This is the repository for my NLP master thesis with the title Transfer and Multitask Learning for Aspect-Based Sentiment Analysis Using the Google Transformer Architecture. \nIt is based on the Google Transformer architecture, and the paper Attention is all you need (https://arxiv.org/abs/1706.03762) \nI recommend to check out the excellent Annotated Transformer guide from Harvard or the Illustrated Transformer by Jay Alammar. Both are excellent resources on the topic. \nThe full thesis document is located under /Thesis/050619_Final.pdf. For the presentation slides go to /Thesis/00_FinalPresentation.pptx. \nIn this master\u2019s thesis, we propose a novel neural network architecture for Aspect Based Sentiment Analysis (ABSA). This architecture is based on the Google transformer introduced in 2017 by Vaswani et al. [80] and inspired by the works of Schmitt et al. [69]. \nThe model we propose classifies multi-label aspect-based sentiment end-to-end. To the best of our knowledge, this is the first model which uses a transformer for aspect-based sentiment analysis. \nFurthermore, this thesis explores transfer- and multitask learning. We show that multitask learning is capable of augmenting data with auxiliary tasks, thereby boosting model performance. \nFor the evaluation of transfer learning, we reduce and categorize a collection of 18 million Amazon reviews. From this data, we form a new, large scale topic-based amazon review dataset with almost 1.2 million reviews. We use this dataset to trans- fer knowledge to a much smaller organic dataset. We demonstrate that we achieve significant performance increases using this technique. \nFinally, we evaluate and benchmark our proposed architecture on four datasets and report that the proposed ABSA-Transformer (ABSA-T) model achieves very competitive results on all datasets. \nThis application provides data loaders for the following datasets: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9786581233789755,
        0.9060114936558172,
        0.9033560916626494,
        0.9529874363113284,
        0.9844198619053981,
        0.9529874363113284
      ],
      "excerpt": "The data for GermEval-2017 can be downloaded directly from the project website. To be able to train on this data, make sure to correct a spelling mistake in one of the data splits. There is one label which is spelled incorrectly. Instead of postive it should be positive. \nPut the data into data/data/germeval-2017. \nThe data from the project website can be used directly without changes. \nPut the data into data/data/conll2003 \nTo get the data please send a mail to the socialROM team @ TUM. \nPut the data into data/data/organic2019 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8985364232701668
      ],
      "excerpt": "The Docker runtime always tries to run the local version of the image instead of pulling a new one from the Docker hub even when including the tag :latest. To upgrade a container, run the following commands: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9447869142648864
      ],
      "excerpt": "Connecting to a notebook running on a remote machine is easy. Just run this command on your local machine. This opens an SSH-tunnel to the remote machine and maps the jupyter-ports 8888 to your local machines 8888 port. Make sure a local notebook does not occupy this port. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8649862841093039
      ],
      "excerpt": "If you don't have an SSH key, this will ask for your password. After that don't expect any further output. It should still work. Just try to access the notebook on your local machine at http://localhost:8888  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955965215309175
      ],
      "excerpt": "This parameter is required. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8225761860608671
      ],
      "excerpt": "Specifies the maximum number of epochs to train. Since this package uses early stopping it is save to put a very high number here (default: 35) since the training will stop anyway if the model does not improve. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9597107519331427,
        0.8861795043384811
      ],
      "excerpt": "This parameter is important for some datasets since it specifies which task should be performed. \nFor the organic dataset this parameter specifies which data split should be used as well as the sentence combination selection. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8292004386304891
      ],
      "excerpt": "all: All entity-attribute combinations + sentiment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9603136800815485,
        0.818091946097303
      ],
      "excerpt": "For the sentence combination technique, add a _combine to the task (e.g. coarse_combine) \nIn case of the GermEval dataset the task parameter specifies whether or not to use multitask training. Use germeval for normal training and germeval_multitask for multitask training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8529176195960384,
        0.8845169393317587
      ],
      "excerpt": "Provide a path to a checkpoint-folder which contains checkpoints. The application will search for the checkpoint with the highest score and load the weights. Make sure that all hyperparameters are the same. \nDue to CUDA restrictions of pytorch it is currently not possible to load a GPU model in a CPU environment.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9672974478097504,
        0.9763566201087134
      ],
      "excerpt": "Currently, this parameter is only implemented for the transfer learning task. Using this parameter will load the source model which will be used for the knowledge transfer. For the other tasks the api can be used. The Trainer class provides the method load_model(self, file_name=None, custom_path=None). Use custom_path to specify the folder which contains the checkpoint file. \nThis parameter is a special parameter which produces the baseline model for the transfer learning task. This is a model with the same vocabulary size restriction as the normal transfer learning task. However, when this parameter is true, the knowledge transfer step will not be performed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This is the repository for my NLP master thesis with the title Transfer and Multitask Learning for Aspect-Based Sentiment Analysis Using the Google Transformer Architecture.  It is based on the Google Transformer architecture and the paper Attention is all you need (https://arxiv.org/abs/1706.03762)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/felixSchober/ABSA-Transformer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 22 Dec 2021 05:44:10 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/felixSchober/ABSA-Transformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "felixSchober/ABSA-Transformer",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/f1_issue.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/ClusteringTest.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/13_Amazon_Experiment_Single.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/10_PandasDF.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/14_AmazonReviewsPreprocess.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/00_f1.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/04_HyperParameterSearch.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/01_NER-Notebook.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/12_Organic19_Experiment_Single.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/13_GermEval17_Experiment_Single.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/03_AspectSentiment-Notebook.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/15_OrganicPreprocess.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/08_LoadModel.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/12_Organic19_Experiments.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/00_Graphs.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/09_ElmoEmbedder.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/02_GeneralSentiment-Notebook.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/19_HyperoptExecutor_NotbookForm.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/12_HyperOpt-Organic2019.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/03_AspectSentiment-Notebook%20-%20AdaBound.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/16_GermEval17-Evaluation.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/00_f1_Calculation.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/19_HyperoptExecutor_NotbookForm%20-%20Organic.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/18_XX_CoNLLExperiments.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/06_ModelEvaluator.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/05_Evaluator.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/03_HyperParameterSearch.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/18_XX_OrganicExperiments.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/19_HyperoptExecutor_NotbookForm%20-%20CoNLL-2003.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/HyperOpt%20Analysis.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/Playground.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/19_HyperoptExecutor_NotbookForm%20-%20GermEvalRandom.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/18_XX_GermEvalExperiments.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/17_XX_DockerExperiment.ipynb",
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/16_GermEvalPreprocess.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/felixSchober/ABSA-Transformer/master/Thesis/crop-logos.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run the image simply run this command in the terminal. (Replace `image:version` with your imageId. You can also get this id by running the command `docker images`)\n\nIt creates a container and start the jupyter notebook server which you can access at http://localhost:8888\n```\ndocker run -it --init --rm \\\n\t-p 8888:8888 \\\n\t--runtime=nvidia \\\n\t--name=absa \\\n\t--volume=$(pwd):/app \\\n\timage:version\n```\n\nFor the windows command line (not powershell), use `%cd%` instead to mount the current directory as a volume so that the run command looks like this:\n\n```\ndocker run -it --init --rm \\\n\t-p 8888:8888 \\\n\t--runtime=nvidia \\\n\t--name=absa \\\n\t--volume=%cd%:/app \\\n\timage:version\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To build the Docker image, navigate to the repository folder which contains the `Dockerfile`.\n\nNext, run the Docker build command:\n\n```\ndocker build .\n```\n\nMake sure you include the `.` at the end of the command to specify that Docker should search for the `Dockerfile` in the current directory.\n\nNote down the Docker image id which should have a format like `3624c152fb28`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8019071950994585
      ],
      "excerpt": "I recommend to check out the excellent Annotated Transformer guide from Harvard or the Illustrated Transformer by Jay Alammar. Both are excellent resources on the topic. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9367582411739294,
        0.878594523781767
      ],
      "excerpt": "The image is based on https://github.com/anibali/docker-pytorch. In order to run it with CUDA support you need to install the latest NVIDIA drivers and libraries as well as CUDA. \nYou also need to install the NVIDIA Docker runtime which you can find here: https://github.com/NVIDIA/nvidia-docker \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8237877259459261
      ],
      "excerpt": "There is one caveat regarding jupyter notebooks though. The browser tab on your local machine has to remain open the whole time. Once you lose connection or close the tab, the script does not stop but you will not get any further output. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8226771389320364
      ],
      "excerpt": "The command  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8113105696612579
      ],
      "excerpt": "Put the data into data/data/conll2003 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8113105696612579
      ],
      "excerpt": "Put the data into data/data/organic2019 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833663327056584
      ],
      "excerpt": "main.py germeval --cuda=False \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8630000982008428
      ],
      "excerpt": "main.py germeval --epochs=35 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9499983976897444
      ],
      "excerpt": "main.py germeval --name=GermEval-Glove \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.896489021386876,
        0.87929328309356
      ],
      "excerpt": "main.py germeval --task=germeval_multitask \nIf this parameter is set to true (main.py --random=True) a random classifier will be used for one epoch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9169794372862005
      ],
      "excerpt": "main.py germeval --task=germeval --restoreModel=/logs/test/0/checkpoints \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/felixSchober/ABSA-Transformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "TeX",
      "Python",
      "Dockerfile",
      "Batchfile",
      "Makefile",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ABSA-Transformer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ABSA-Transformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "felixSchober",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/felixSchober/ABSA-Transformer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The first step to run this code is to clone the repository using `git clone `\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To run this image you have to login to the DockerHub with `docker login`. To create a Docker account head over to https://cloud.docker.com\n\nThis repository can also be obtained prebuild from the DockerHub. For now, the repository is private. Once authenticated the image can be run via\n\n```\ndocker run -it --rm --init \\\n\t-p 8888:8888 \\\n\t--runtime=nvidia \\\n\t--volume=$(pwd):/app \\\n\t--name=absa\n\tjorba/absa-transformer:latest\n```\n\nUse this one-liner for easy copy and pasting: \n\n`docker run -it --rm --init -p 8888:8888 --runtime=nvidia --volume=$(pwd):/app --name=absa jorba/absa-transformer:latest`\n\nThe commands above starts the container in interactive mode which means that as soon as you close your session, the container stops. You probably don't want this behavior when running long experiments. To run the container in detached mode, use this command:\n\n`docker run -d -p 8888:8888 --rm --runtime=nvidia --volume=$(pwd):/app --name=absa jorba/absa-transformer:latest`\n\nTo get the token of the notebook, use `docker logs absa`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To run the image simply run this command in the terminal. (Replace `image:version` with your imageId. You can also get this id by running the command `docker images`)\n\nIt creates a container and start the jupyter notebook server which you can access at http://localhost:8888\n```\ndocker run -it --init --rm \\\n\t-p 8888:8888 \\\n\t--runtime=nvidia \\\n\t--name=absa \\\n\t--volume=$(pwd):/app \\\n\timage:version\n```\n\nFor the windows command line (not powershell), use `%cd%` instead to mount the current directory as a volume so that the run command looks like this:\n\n```\ndocker run -it --init --rm \\\n\t-p 8888:8888 \\\n\t--runtime=nvidia \\\n\t--name=absa \\\n\t--volume=%cd%:/app \\\n\timage:version\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "It is also possible to run the `main.py` file. This file can be used for model training and evaluation.\n\nThe following parameters can be used:\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This number specifies the number of times a experiment is repeated. The default parameter value is 1.\n\nThis parameter can be used like this\n\n`main.py germeval --runs=5`\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Wed, 22 Dec 2021 05:44:10 GMT"
    },
    "technique": "GitHub API"
  }
}