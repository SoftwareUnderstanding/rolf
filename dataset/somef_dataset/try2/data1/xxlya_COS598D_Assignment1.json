{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.02340.\n\n[2] Wang, C., Zhang, G. and Grosse, R., 2020. Picking winning tickets before training by preserving gradient flow. arXiv preprint https://arxiv.org/abs/2002.07376.\n\n[3] Tanaka, H., Kunin, D., Yamins, D.L. and Ganguli, S., 2020. Pruning neural networks without any data by iteratively conserving synaptic flow. arXiv preprint https://arxiv.org/abs/2006.05467.\n\n### Additional reading materials:\n\nA recent preprint [4] assessed [1-3].\n\n[4] Frankle, J., Dziugaite, G.K., Roy, D.M. and Carbin, M., 2020. Pruning Neural Networks at Initialization: Why are We Missing the Mark?. arXiv preprint https://arxiv.org/abs/2009.08576.\n\n## Getting Started\nFirst clone this repo, then install all dependencies\n```\npip install -r requirements.txt\n```\n\n## How to Run \nRun `python main.py --help` for a complete description of flags and hyperparameters. You can also go to `main.py` to check all the parameters. \n\nExample: Initialize a VGG16, prune with SynFlow and train it to the sparsity of 10^-0.5 . We have sparsity = 10**(-float(args.compression)).\n```\npython main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow --compression 0.5\n```\n\nTo save the experiment, please add `--expid {NAME",
      "https://arxiv.org/abs/2002.07376.\n\n[3] Tanaka, H., Kunin, D., Yamins, D.L. and Ganguli, S., 2020. Pruning neural networks without any data by iteratively conserving synaptic flow. arXiv preprint https://arxiv.org/abs/2006.05467.\n\n### Additional reading materials:\n\nA recent preprint [4] assessed [1-3].\n\n[4] Frankle, J., Dziugaite, G.K., Roy, D.M. and Carbin, M., 2020. Pruning Neural Networks at Initialization: Why are We Missing the Mark?. arXiv preprint https://arxiv.org/abs/2009.08576.\n\n## Getting Started\nFirst clone this repo, then install all dependencies\n```\npip install -r requirements.txt\n```\n\n## How to Run \nRun `python main.py --help` for a complete description of flags and hyperparameters. You can also go to `main.py` to check all the parameters. \n\nExample: Initialize a VGG16, prune with SynFlow and train it to the sparsity of 10^-0.5 . We have sparsity = 10**(-float(args.compression)).\n```\npython main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow --compression 0.5\n```\n\nTo save the experiment, please add `--expid {NAME",
      "https://arxiv.org/abs/2006.05467.\n\n### Additional reading materials:\n\nA recent preprint [4] assessed [1-3].\n\n[4] Frankle, J., Dziugaite, G.K., Roy, D.M. and Carbin, M., 2020. Pruning Neural Networks at Initialization: Why are We Missing the Mark?. arXiv preprint https://arxiv.org/abs/2009.08576.\n\n## Getting Started\nFirst clone this repo, then install all dependencies\n```\npip install -r requirements.txt\n```\n\n## How to Run \nRun `python main.py --help` for a complete description of flags and hyperparameters. You can also go to `main.py` to check all the parameters. \n\nExample: Initialize a VGG16, prune with SynFlow and train it to the sparsity of 10^-0.5 . We have sparsity = 10**(-float(args.compression)).\n```\npython main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow --compression 0.5\n```\n\nTo save the experiment, please add `--expid {NAME",
      "https://arxiv.org/abs/2009.08576.\n\n## Getting Started\nFirst clone this repo, then install all dependencies\n```\npip install -r requirements.txt\n```\n\n## How to Run \nRun `python main.py --help` for a complete description of flags and hyperparameters. You can also go to `main.py` to check all the parameters. \n\nExample: Initialize a VGG16, prune with SynFlow and train it to the sparsity of 10^-0.5 . We have sparsity = 10**(-float(args.compression)).\n```\npython main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow --compression 0.5\n```\n\nTo save the experiment, please add `--expid {NAME"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9999939165707461,
        0.9999721171962026,
        0.9898812104386919,
        0.8714162992508173,
        0.9732562554718903
      ],
      "excerpt": "[1] Lee, N., Ajanthan, T. and Torr, P.H., 2018. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340. \n[2] Wang, C., Zhang, G. and Grosse, R., 2020. Picking winning tickets before training by preserving gradient flow. arXiv preprint arXiv:2002.07376. \n[3] Tanaka, H., Kunin, D., Yamins, D.L. and Ganguli, S., 2020. Pruning neural networks without any data by iteratively conserving synaptic flow. arXiv preprint arXiv:2006.05467. \nA recent preprint [4] assessed [1-3]. \n[4] Frankle, J., Dziugaite, G.K., Roy, D.M. and Carbin, M., 2020. Pruning Neural Networks at Initialization: Why are We Missing the Mark?. arXiv preprint arXiv:2009.08576. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xxlya/COS598D_Assignment1",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-02-25T15:30:03Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-20T01:17:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8834144987467216
      ],
      "excerpt": "In this assignment, you are required to evaluate three advanced neural network pruning methods, including SNIP [1], GraSP [2] and SynFlow [3], and compare with two baseline pruning methods,including random pruning and magnitude-based pruning. In example/singleshot.py, we provide an example to do singleshot global pruning without iterative training. In example/multishot.py, we provde an example to do multi-shot iterative training. This assignment focuses on the pruning protocol in example/singleshot.py. Your are going to explore various pruning methods on differnt hyperparameters and network architectures. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "|   Data  |   Arch |   Rand |  Mag |  SNIP |  GraSP | SynFlow       |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924212690395703
      ],
      "excerpt": "|Cifar10 | VGG16 |    |      |        |     |         | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9728863087398651
      ],
      "excerpt": "Prune models on CIFAR10 with VGG16, please replace {} with sparsity 10^-a for a \\in {0.05,0.1,0.2,0.5,1,2}. Feel free to try other sparsity values. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285950358656266,
        0.9560453064179252,
        0.8611013529466875
      ],
      "excerpt": "For better visualization, you are encouraged to transfer the above three tables into curves and present them as three figrues. \nReport the sparsity and draw the weight histograms of each layer using pruner Rand |  Mag |  SNIP |  GraSP | SynFlow with the following settings \nmodel = vgg16, dataset=cifar10, compression = 0.5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Assignments for COS598D: System and Machine Learning",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xxlya/COS598D_Assignment1/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 07:05:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xxlya/COS598D_Assignment1/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "xxlya/COS598D_Assignment1",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/xxlya/COS598D_Assignment1/main/Results/figures.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9427901520288489
      ],
      "excerpt": "To track the runing time, you can use timeit. pip intall timeit if it has not been installed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8378080875653302
      ],
      "excerpt": "Bonus (optional) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.904138393951402,
        0.9252859157054978
      ],
      "excerpt": "python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow --compression 1 \npython main.py --model-class default --model fc --dataset cifar10 --experiment singleshot --pruner synflow --compression 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.904138393951402
      ],
      "excerpt": "python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow  --compression {} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674
      ],
      "excerpt": "import timeit \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807507535392148
      ],
      "excerpt": "print('Time: ', stop - start) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xxlya/COS598D_Assignment1/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Network Pruning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "COS598D_Assignment1",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "xxlya",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xxlya/COS598D_Assignment1/blob/main/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Run `python main.py --help` for a complete description of flags and hyperparameters. You can also go to `main.py` to check all the parameters. \n\nExample: Initialize a VGG16, prune with SynFlow and train it to the sparsity of 10^-0.5 . We have sparsity = 10**(-float(args.compression)).\n```\npython main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow --compression 0.5\n```\n\nTo save the experiment, please add `--expid {NAME}`. `--compression-list` and `--pruner-list` are not available for runing singleshot experiment. You can modify the souce code following `example/multishot.py` to run a list of parameters. `--prune-epochs` is also not available as it does not affect your pruning in singleshot setting. \n\nFor magnitude-based pruning, please set `--pre-epochs 200`. You can reduce the epochs for pretrain to save some time. The other methods do pruning before training, thus they can use the default setting `--pre-epochs 0`.\n\nPlease use the default batch size, learning rate, optimizer in the following experiment. Please use the default training and testing spliting. Please monitor training loss and testing loss, and set suitable training epochs. You may try `--post-epoch 100` for Cifar10 and `--post-epoch 10` for MNIST.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 07:05:03 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please describe the settings of your experiments. Please include the required results (described in Task 1 and 2). Please add captions to describe your figures and tables. It would be best to write brief discussions on your results, such as the patterns (what and why), conclusions, and any observations you want to discuss.  \n",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "First clone this repo, then install all dependencies\n```\npip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}