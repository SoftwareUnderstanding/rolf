{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1508.06576\n> * Code : https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/neural_style_transfer\n\n`DAY-59` : Today I wanted to study \"unit testing\" a bit more in detail. \n\nI have never used them before, mainly because I didn't work at big projects, so I've never felt the need. \nThanks to this challenge, during this period I have studied a lot of things that I have always postponed! \n\n> * Linkedin #59 : https://www.linkedin.com/posts/francescodisalvo-pa_python-tutorial-unit-testing-your-code-with-activity-6792174773486026752-bTGF\n> * YouTube : https://www.youtube.com/watch?v=6tNS--WetLI\n\n`DAY-60` : Today I spent most of my time cleaning data for my current Omdena's project and I also continued my past script for scraping tweets from twitter API's. \n\n> * Linkedin #60 : https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-activity-6792550411145359360-fjvu\n\n`DAY-61` : Today I continued working a bit with BeautifulSoup and I scraped news from https://news.yahoo.com/. \n\nIn the next days I will start working with NLP on these scraped data!\n\n<p align=\"center\">\n  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day61.jpg\"/>\n</p>\n\n> * Linkedin #61 :https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-scraping-beautifulsoup-activity-6792915608754257921-__bk\n\n`DAY-62` : Today I went through the implementation of LSTM in PyTorch for a sentiment analysis on the fairly well known IMDB dataset. The results are really promising! \n\n> * Linkedin #62 : https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-nlp-deeplearning-activity-6793273869177253888-eXPh\n\n`DAY-63` : Today I faced again one of my \"open problems\".\n\nHow can we label financial news for a Sentiment Analysis? Online there are just a few labelled dataset. I previously tried several techniques such as Vader, TextBlob et simila but they didn't achieve satisfactory results. \n\nSo, I have read a couple of papers and most of them used Supervised Classification without referring to any particular dataset.\n\nIn particular the authors of \"Efficacy of News Sentiment for Stock Market Prediction\" achieved an accuracy among 60% to 90%.\n\nIf you have some advices, I would love to hear them!\n\n> * Linkedin #63 : https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-machinelearning-finance-activity-6793627544219340800-PI8k\n> * Paper : https://ieeexplore.ieee.org/document/8862265\n\n`DAY-64` : Today I just worked for my current Omdena's project, so I spent a couple of hours scraping data from a website.\n\nIt is always satisfying when you're able to find the way for scraping autonomously 390 pages full of data with just a for-loop and some preliminary analysis!\n\n> * Linkedin #64 : https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-selenium-scraping-activity-6793995150340169729-qpaJ\n\n`DAY-65` : Today I went through a Ted Talk from Marshall Chang regarding the role of AI in Financial Markets. If you're curious, I highly recommend it! \n\n> * Linkedin #65 : https://www.linkedin.com/posts/francescodisalvo-pa_how-ai-traders-will-dominate-hedge-fund-activity-6794366851330650112-QiSe\n> * YouTube : https://www.youtube.com/watch?v=lzaBbQKUtAA\n\n`DAY-66` : It has been a great journey thanks to which I have had the chance to learn more, especially about Financial Machine Learning, Algorithmic Trading and Natural Language Processing. \n\nI can definitely say that I have learnt a lot and I have met a lot of amazing people along the way, that enriched, not only my technical skills, but also my desire to learn and share new things!\n\nThank you for your kind support, let's see to the next challenge! <br />\nFrancesco\n\n> * Linkedin #66 : https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-datascience-machinelearning-activity-6794696436198989824-9AW6"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9195926162616405
      ],
      "excerpt": "| Day 0 | Day 7 |  Financial Machine Learning| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066,
        0.8651642121654365,
        0.9156566588472104
      ],
      "excerpt": "| Day 21 | Day 33 |  Natural Language Processing | \n| Day 34 | Day 37 |  Web Scraping and Text Analysis | \n| Day 38 | Day 44 |  Natural Language Processing - Part 2 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9370497259427808
      ],
      "excerpt": "Book : Advances In Financial Machine Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9370497259427808
      ],
      "excerpt": "Book : Advances In Financial Machine Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9279628677675321
      ],
      "excerpt": "Youtube video: https://www.youtube.com/watch?v=-cLPasRzJeY&t \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9370497259427808
      ],
      "excerpt": "Book : Advances In Financial Machine Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9099469026122304
      ],
      "excerpt": "Another useful resource : https://ai.plainenglish.io/start-using-better-labels-for-financial-machine-learning-6eeac691e660 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.959312058154067
      ],
      "excerpt": "[1] article : https://ai.plainenglish.io/start-using-better-labels-for-financial-machine-learning-6eeac691e660 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.92272101626797
      ],
      "excerpt": "Long and Short positions : https://www.investor.gov/introduction-investing/investing-basics/how-stock-markets-work/stock-purchases-and-sales-long-and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9370497259427808
      ],
      "excerpt": "Book : Advances In Financial Machine Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9370497259427808,
        0.8422862053358849
      ],
      "excerpt": "Book : Advances In Financial Machine Learning \nPicture and slides : https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3257420 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9899638137356661
      ],
      "excerpt": "LinkedIn #10: https://www.linkedin.com/posts/francescodisalvo-pa_day-10-activity-6774030583296233472-4EE6 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374695145293155
      ],
      "excerpt": "LinkedIn #11 : https://www.linkedin.com/posts/francescodisalvo-pa_day11-activity-6774418286193987584-HbSl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8854398367006624
      ],
      "excerpt": "LinkedIn #12 : https://www.linkedin.com/posts/francescodisalvo-pa_day12-activity-6774766883167051776-Q-aj \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374695145293155
      ],
      "excerpt": "LinkedIn #18 : https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-finance-algotrading-activity-6776963630178025472-dGGL \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9204947163032924
      ],
      "excerpt": "Linkedin #27 : https://www.linkedin.com/posts/francescodisalvo-pa_mfml-019-how-to-avoid-machine-learning-activity-6780232245660659712-PXb5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374695145293155
      ],
      "excerpt": "Linkedin #29 : https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-finance-machinelearning-activity-6780960306752557056-jJKO \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8854398367006624
      ],
      "excerpt": "Linkedin #30 : https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-datascience-webscraping-activity-6781339744325386240-dgvs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374695145293155
      ],
      "excerpt": "Linkedin #32 : https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-datascience-machinelearning-activity-6782001756592209920-NHkv \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9818522549867529
      ],
      "excerpt": "Medium article : https://towardsdatascience.com/unsupervised-sentiment-analysis-a38bf1906483 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042530604903903
      ],
      "excerpt": "Article : https://towardsdatascience.com/sentiment-analysis-for-stock-price-prediction-in-python-bed40c65d178 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374695145293155,
        0.9042530604903903
      ],
      "excerpt": "Linkedin #35 : https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-datascience-machinelearning-activity-6783117279832887296-EPGh \nArticle : https://towardsdatascience.com/sentiment-analysis-for-stock-price-prediction-in-python-bed40c65d178 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042530604903903
      ],
      "excerpt": "Article : https://towardsdatascience.com/sentiment-analysis-for-stock-price-prediction-in-python-bed40c65d178 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8854398367006624,
        0.8111036989382164
      ],
      "excerpt": "Linkedin #37 : https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-datascience-machinelearning-activity-6783833851585171456-JcWa \nScraping inspiration : https://github.com/israel-dryer/Twitter-Scraper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.890734504593722
      ],
      "excerpt": "Sentiment Analysis : https://github.com/joosthub/PyTorchNLPBook/blob/master/chapters/chapter_3/3_5_Classifying_Yelp_Review_Sentiment.ipynb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9775166916262069
      ],
      "excerpt": "Blogpost : https://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977994744046882
      ],
      "excerpt": "Paper : https://arxiv.org/pdf/1409.3215.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9991295298092048
      ],
      "excerpt": "Paper : https://arxiv.org/pdf/1611.01576v1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838364455167615
      ],
      "excerpt": "Article : https://towardsdatascience.com/five-things-i-have-learned-after-solving-500-leetcode-questions-b794c152f7a1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9862994859103299,
        0.9968156676108885,
        0.986327158429457
      ],
      "excerpt": "Article : https://ai.googleblog.com/2017/04/federated-learning-collaborative.html \nPaper : https://ieeexplore.ieee.org/document/9084352 \nPaper : http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977994744046882
      ],
      "excerpt": "Paper : https://arxiv.org/pdf/1612.05424.pdf  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977994744046882
      ],
      "excerpt": "Paper : https://arxiv.org/pdf/1612.05424.pdf  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374695145293155
      ],
      "excerpt": "Paper : https://www.youtube.com/watch?v=NEaUSP4YerM \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8500811745160795
      ],
      "excerpt": "DAY-54 : Today I went through the iCaRL's paper, from Sylvestre-Alvise Rebuff et al.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977994744046882
      ],
      "excerpt": "Paper : https://arxiv.org/abs/1508.06576 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968156676108885
      ],
      "excerpt": "Paper : https://ieeexplore.ieee.org/document/8862265 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/francescodisalvo05/66DaysOfData",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-02-18T18:49:39Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-02T21:11:20Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8083050143190311
      ],
      "excerpt": "  \ud83d\udca1 The <a href=\"https://www.youtube.com/watch?v=qV_AlRwhI3I&t=12s\">#66DaysOfData</a> challenge consists on learning data science every day for 66 days and sharing the progress on a social media \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9891239558878078
      ],
      "excerpt": "\ud83c\udfaf It has been a great journey thanks to which I have had the chance to learn more, especially about Financial Machine Learning, Algorithmic Trading and Natural Language Processing. I can definitely say that I have learnt a lot and I have met a lot of amazing people along the way, that enriched, not only my technical skills, but also my desire to learn and share new things! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471801851184282
      ],
      "excerpt": "Thank you for your kind support, let's see to the next challenge! <br /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9530902406823537,
        0.9460496946645103
      ],
      "excerpt": "DAY-0 : Today I discovered two interesting libraries for extracting financial information. The first one is yFinance that extracts its information from Yahoo Finance. Thanks to the Ticker module we are able to select one or more tickers. Then, with the history method we are allowed to download the historical values in a specific period within a particular interval (daily,weekly..). It provides seven features: Open, High, Low, Close, Volume, Dividends, Stock Splits. Finally, we can gain some other information with the info method (123 in total). These information are related to the geographical location, economical status and so on.  \nThen, the second library is ta (Technical Analysis) that implements 32 indicators based mostly on the volume, volatility and trend. Understanding all these features would require a huge effort because it is \"trading oriented\", but I will surely study them later on. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9435380190628857
      ],
      "excerpt": "DAY-1 : Today I started reading the first pages of \"Advances in Financial Machine Learning\" by Marcos Lopez de Prado. It is divided into five parts:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8581807701448467,
        0.8939011223821338,
        0.854311416144838,
        0.8111575132674358,
        0.9268410711232731,
        0.89596862478316,
        0.892012791068616
      ],
      "excerpt": "* Fundamental data: information mostly obtained by business analytics, for example assets, sales and so on. Due to its variety, it may be hard to manipulate. \n* Market data: information regarding all trading activities. Since they are precise and with a very well known structure, they can be easily analyzed and manipulated. \n* Analytics: derivative data, based on multiple sources. \n* Alternative data: it can be produced by individuals (e.g. social media), business processes and sensors.  \nAll the information that we gain, must be processed and converted into a machine-friendly version. One of the most common representation is through bars. \n* Time bars: they are the most common ones and we explored them yesterday with yfinance. \n* Tick bars: they sample the time bars based on the trading activity. This allows to avoid taking samples from low trading activity intervals (that would be not so relevant). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8333622365953559
      ],
      "excerpt": "Tomorrow I will focus on some other advanced financial data structure and I will try to conclude the first chapter of this book! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9725914567570861,
        0.9857697146011658
      ],
      "excerpt": "DAY-2: Today I finished the first chapter of De Prado's book and I implemented some financial data structure thanks to the proposed exercises. I appreciated the importance of sampling, in fact the author pointed out that one of the most common errors regard the information used for training a model. The remaining data structures regards Information-Driven bars. Their purpose is to sample more frequently when we gain new information. Yesterday we saw Tick bars, Volume bars and Dollar bars. Today I discovered Tick imbalanced bars, Volume imbalanced bars and Dollar imbalanced bars. The main difference is that in this case we sample when we exceed a pre-defined expectation.  \nFinally, in order to check what I studied so far, I solved the first exercise of the chapter and I took the opportunity for learning the fundamentals of plotly, a fairly well known library that allows to create interactive charts! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9835146362803411,
        0.9726536320148326
      ],
      "excerpt": "DAY-3: Few days ago I was wondering how to deal with huge quantity of data (in the order of GBs). So, Today I saw an interesting YouTube video made by DecisionForest regarding the memory optimization in pandas. The first issue in Pandas is that it uses by default \"int64\" or \"float64\", therefore, a first improvement could be made by downcasting these features into \"int32\" and \"int32\" (or even 16). By using this approach he was able to save around 40% of memory on a huge dataset.  \nThe second step is to save the dataframe into some other optimized format. He proposed \"parquet\", an Apache Hadoop\u2019s columnar storage format, but I also discovered many others on the following blogpost that I would like to test much more in detail! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9830418683664583,
        0.9867894741142839
      ],
      "excerpt": "DAY-4: Today I studied the third chapter of \"Advances in Financial Machine Learning\" that focuses on how to label financial data.  \nOne of the biggest mistakes is to label observations with fixed thresholds. In fact, we should set the \"take profit\" and \"stop loss\" by using function of the risk! A possible way of labeling is with the so called \"triple-barrier method\", where the aim is to label an observation according to the first barrier touched. There are two horizontal bars (stop loss and take profit) and one vertical bar (expiration limit). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9362092445525196
      ],
      "excerpt": "The previous labeling alone is not so effective, in fact we need to know how much we should bet (bet size). This is called by the author \"meta labeling\". It helps increasing the f1-score, because we build a first model with an higher recall and then we correct the low precision by the meta labeling approach. In fact, it tries to filter out the false positive! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8580248202015309
      ],
      "excerpt": "3. Use a binary classifier (buy/sell)  on the meta-labels in order to improve the performances \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693948448251203
      ],
      "excerpt": "DAY-5 : Today I was approaching the implementation of triple barrier method and meta labeling but I found another interesting topic to focus on: the Bollinger Bands.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9340247686473819
      ],
      "excerpt": "I took the opportunity to work with the dollar bars (bars indexed by the traded dollar volume) thanks to a new implementation that I found in a very well done article [1]. Then, I implemented a simple script for defining these short or long positions and finally I plotted the result!  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877801342393466
      ],
      "excerpt": "Long and Short positions : https://www.investor.gov/introduction-investing/investing-basics/how-stock-markets-work/stock-purchases-and-sales-long-and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9706978906084539
      ],
      "excerpt": "DAY-6 : Today I jumped to the 6th chapter of \"Advances in Financial Machine Learning\" by Marcos Lopez de Prado that covers \"Ensemble methods\".  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9358659216131214,
        0.9500054828783248,
        0.8782422625354384
      ],
      "excerpt": "Both bagging and boosting are ensemble learning techniques, it means that they combine several base model in order to improve the overall performances. Here's the main features: \nLearners : they both get N learners by generating additional data in the training stage and the training sets are obtained by random sample with replacement. In bagging, any element has the same probability to be considered, whereas in boosting there are weights associated to the sampling process.  \nWeights : in bagging each model is indipendent, whereas in boosting each model takes into account the previous classifiers' success. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8721066085619805,
        0.8191567699660549
      ],
      "excerpt": "So, they both decrease the variance but boosting tries also to construct a stronger model.  \nThis was just a gentle introduction of the topic, but if you want to know more, I found a very detailed article on TowardsDataScience!  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9548351573334374,
        0.8904202375454398,
        0.9797922384152346,
        0.9147061604656105,
        0.9601706468604485,
        0.9336827366303746,
        0.90293610671482
      ],
      "excerpt": "DAY-7: Today I studied the 7th chapter of \"Advances in Financial Machine Learning\" by Marcos Lopez de Prado, devoted to \"Cross Validation in Finance\".  \nI was thinking about K-Fold with finance data for a while, and today De Prado confirmed my concerns. In fact he first pointed out \"why k-fold fails in finance\". The main reasons are basically two: \nin financial data we cannot leverage on the hypothesis that the observations are IID (Independent and Identically Distributed), \nthe testing sets are used multiple times in order to develop the model, so we may induct some bias \nOf course, since we'll have overlapping observations, we may observe the so called phenomenon of \"Data Leakage\" (when we use in training data some information that we do not expect during the prediction).  \nA first improvement can be made by \"purging the training set\", so we remove from the training set all observations whose labels are overlapped with the ones contained in the test set. If this approach is not enough for preventing the data leakage, De Prado proposes to impose an embargo on training observations after every test set. The idea is to define an embargo period ( typically 0.01T - where T is the number of bars) after every test set, in which we discard training samples! It is much more clear on the following picture. \nI tried to implement it on my own but I got stuck for a while, so I will try again tomorrow!  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737757903300163
      ],
      "excerpt": "See more : https://medium.com/@samuel.monnier/cross-validation-tools-for-time-series-ffa1a5a09bf9 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9359951513583308,
        0.9029262885353903,
        0.943405840635905
      ],
      "excerpt": "DAY-8: Today I played with Streamlit, a Python library that allows to easily create a web applications in a very simple way!  \nThink that this entire script took me just 52 lines. Streamlit also allows to host it on their servers for free, upon request. \nI also spent some time figuring out some cool features I can implement along the way. Any idea or feedback is always well received! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8764478462540741,
        0.9712608090752097
      ],
      "excerpt": "Here is the \"top 7\", according to Investopidia : https://www.investopedia.com/top-7-technical-analysis-tools-4773275 \nThen, I came up with some interesting features to implement in the next days and I'll surely post here all the future updates! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8793176948654243,
        0.9772352351864663,
        0.8785333968496641,
        0.9218105163948148,
        0.8437265035896896,
        0.9143371181115205
      ],
      "excerpt": "DAY-10: Today I implemented the Moving Average, a technical analysis tool that tries to cut out the noise from the trend, by updating the average price in a given period.  \nIn theory, if the price is above the MA the trend is up, and vice versa. The window of the moving average strongly depends on the trader's time horizon, but a common range goes from 10 to 200.   \nIt seems to be a very well known approach and I also discovered two different strategies that I will probably implement tomorrow! \nOf course it is not perfect, for two main reasons: \n1. the future is unpredictable by nature (Taleb docet), \n2. it does not work well with volatile stocks (e.g. cryptocurrencies). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9651092863534854,
        0.958613471031139,
        0.9655675760961882,
        0.8401196635516059,
        0.8841888150108769,
        0.8724434758852863
      ],
      "excerpt": "- SMA : a technical analysis tool that tries to cut out the noise from the trend, by updating the average price in a given period. In theory, if the price is above the MA the trend is up, and vice versa. \n- EMA : the idea is the similar to the previous one, but the exponential moving average gives more importance (in terms of weights) to the most recent observations. \nThe first strategy that I implemented is the \"Simple Crossover\", that uses just on single SMA. It tells us that when the price crosses above or below a moving average to signal a potential change in trend. \nThe second one uses two different SMAs, one for the long period and one for the short period. When the shorter-term MA crosses above the longer-term MA, it\u2019s a buy signal and vice versa, when the shorter-term MA crosses below the longer-term MA, it\u2019s a sell signal. \nFinally, the third one is the same as the previous one, but instead of using two SMAs, it uses two EMAs. \nI cannot quantify which is the best one (and actually I think that no one is the best one, at all), but I should make some more tests! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9538734985817143,
        0.8730137948982201
      ],
      "excerpt": "Then, yesterday I realized that in some points I had too many signals, sometimes also impossible (e.g. sell,sell,..). In order to fix that, I just used a flag \"pos\" that was equal to 1 if I got a position (so I would be able to sell but not to buy anymore) and 0 otherwise.  Then, I added these logical constraints to the main strategy and nothing more! \nI tested the Crossover strategy with two EMAs (15 and 100 days) on 4 stocks with a symbolic amount of 10k (each) from '2018-03-06'. I obtained the following results (net profts): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8974922941685352
      ],
      "excerpt": "Looking at the trends and signals, they seems promising, but of course I tested it in a very few stocks. There would be much other stocks where I would lose a lot of money!  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.89992228786309,
        0.9456178414621358,
        0.9787759668099789
      ],
      "excerpt": "I didn't take that long, but the result is much more clear now! I took the opportunity to test it on Twitter with an initial amount of $10,000.  \nThe result is quite interesting because from March 2019 to February 2019 I lost around $3,000. Then, the profit started increasing and it is still holding the position, with a current profit of $7,776.00. Of course we should see when it would have been sold!  \nSo, I noticed with my own eyes the limits of this strategy. When the stock is highly volatile, this strategy tends to not understand the various corrections, so the results are not so good. On the other hand, it performed well in the long term (also because the stock has experienced a remarkable rise). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9868361444855436,
        0.9601504196590416,
        0.9774943145039726
      ],
      "excerpt": "Today I started looking for some new technical analysis indicators and I went a bit deeper on the RSI (Relative Strength Index). It measures the magnitude of recent price changes to evaluate overbought or oversold conditions in the price of a stock or other asset. \nIts range of values goes from 0 to 100. Typically, when it goes below 30, it generally indicate that the stock is oversold, whereas it goes above 70, it means that it is overbought. \nI'm planning to implement a couple of other indicators and then I would like to move on some \"Machine Learning oriented\" tasks applied to the financial sector. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8074740627484217,
        0.8125301295337715
      ],
      "excerpt": "Today I implemented the RSI, that I mentioned yesterday.  \nThe last one that I'll implement will be the MACD (Moving Average Convergence/Divergence)! Then, I'll use streamlit for building a dashboard with all these technical analysis indicators that I coded so far. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8651038489786229
      ],
      "excerpt": "DAY-17:  Today I spent some time doing some sketches for my \"stock dashboard\". Then, I watched an amazing conference by Leda Braga, CEO of Systematica Investments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9368466282239252,
        0.9552508134546597
      ],
      "excerpt": "DAY-18: Today I started building the frontend of \"Stock Manager\", where I will try to show (and to sum up) everything I did in the past weeks. You can already see the skeleton! \nI still need to familiarize with Streamlit because I realized that there are a lot of hidden features. I would like to make it as versatile as possible, because I am planning to add two (or more) other sections, so I will need to manage a multi-page layout.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9731782006752259,
        0.925908631840995
      ],
      "excerpt": "DAY-19: Today I continued working on my stock manager. I added some general informations about the stock, as suggested me yesterday. Then I also organized and cleaned the code, in order to generalize a bit better all the features that I have in mind. I realized that I am not so organized as I thought, so I will try to improve also in this way! \nFinally, I started playing with the RSI indicator and I implemented the associated trading strategy. Unfortunately, I would have lost $ 129.24 on the Apple's stock! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9080944203417496,
        0.94536165842518,
        0.8900211741059089
      ],
      "excerpt": "DAY-20: Today I implemented the trading strategy based on the MACD indicator (Moving Average Convergence/Divergence). This time I was luckier and I would have earned $922.  \nThis was my 4th implemented indicator and I think I won't test other strategies or indicators later on. I have learned some technical indicators and I had a lot of fun,  but I think that I won't ever put my money in any automatic trading strategy (at least with my current knowledge). \nIn the following days I'll try to complete the current dashboard with some \"frontend tricks\" in order to conclude this first short milestone!  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8887397945289506,
        0.9858107734034901,
        0.9469167818692162
      ],
      "excerpt": "DAY-21: Today I decided to start focusing on NLP and I spent some time looking for some good resources! \nIn particular I checked the Stanford course on NLP with Deep Learning (thanks to Dave Emmanuel Magno) and also the fantastic repository of Thinam Tamang on his #66DaysOfData in NLP: https://lnkd.in/dQwUCSZ \nI decided to start with \"Natural Language Processing with Python\" by S. Bird, E. Klein, and E. Loper in order to learn the basis of NLP and I will decide the next one along the way! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9319589902569195,
        0.9714278341581623
      ],
      "excerpt": "DAY-22: Today I approached the Natural Language Processing from scratch. Even if I have already a bit of experience for some academic projects, I preferred to make a cleaned swap and to start from the bases. So, I started with tokenization and a text preprocessing. \nThe tokenization is the process by which the initial string is splitted into smaller units (called tokens). These tokens can be words, digits or punctuation. Then, in order to avoid  useless tokens they can be easily filtering with regular expressions or by iterating and checking the given conditions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.997620843686883,
        0.9956270434496709
      ],
      "excerpt": "Stemming is the process of producing morphological variants of a root/base word. For example [likes,liked,liking'] become all 'like'. It is extremely useful for reducing the number of distinct words and for retrieving more precise information. There are several algorithms, the most common ones are the porter and the snowball. The porter's stemmer is based on the idea that the suffixes in English are made up of a combination of simpler suffixes. Then, the snowball stemmer is considered the evolution of the porter's one, because it can be also used for non-english words! \nLemmatization is the process of grouping together different inflected words. It is similar to stemming, but here we obtain meaningful words. It is more precise, but in contrary, the computation is slower. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.994538382704789,
        0.9875099342955268
      ],
      "excerpt": "The term frequency inverse document frequency (tf-idf) gives an higher importance to all the tokens that occur frequently in a single document but rarely on the entire collection. So, it is suitable for heterogeneous documents. \nThen, the term frequency document frequency (tf-df) gives an higher importance to the tokens that are more frequent over the entire collection, so it is suitable for homogeneous documents. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9974712223647934,
        0.8946191692717226,
        0.9803916449035535
      ],
      "excerpt": "The Latent semantic analysis (LSA) is an algorithm that analyze the relationship of the words, in order to cluster them into topics. Since the number of topics is (obviously) much smaller than the number of topics, it is commonly used for reduce the dimension of your initial matrix. A slightly different algorithm is Linear Discriminant Analysis (LDA), which breaks down a document into a single topic. \nLDA is one of the fastest algorithms for dimension reduction, however, it is a supervised algorithm, so it requires some initial labels. \nIn the picture below there is the implementation of the algorithm, proposed in the book \"Natural Language Processing in Action\". \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8164141669714053
      ],
      "excerpt": "Book : Natural Language Processing in Action: Understanding, Analyzing, and Generating Text With Python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9879194566479983,
        0.9908228547778805
      ],
      "excerpt": "Latent Semantic Analysis (LSA), as I mentioned yesterday, is based on the well known Singular Value Decomposition (SVD). It is possible to truncate the tf-idf matrix in order to drastically reduce the dimension of the problem. This new representation highlights the \"latent sentiment\" of these topics. So, the LSA tells you which dimensions are relevant to the semantic of the documents. In fact the \"low variance\" topics may represents just noise.  \nThen, I tried to implement a simple pipeline with 'A Million News Headlines' dataset'. Firtsly, I cleaned the data by removing digits and hashtags (if any) I splitted each headline in tokens. Then, I decided to normalize the tokens with the Stemming technique because it is faster than the Lemmatization and finally, I used the TruncatedSVD for clustering these headlines in \"topics\" (7 in this case). Thanks to a snippet that I found on a Kaggle notebook I was able to get the top 10 elements for each cluster (topic). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8164141669714053
      ],
      "excerpt": "Book : Natural Language Processing in Action: Understanding, Analyzing, and Generating Text With Python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629980563463773,
        0.9098117225844163
      ],
      "excerpt": "DAY-27: Today I needed to recover some lectures for my university, so I couldn't go further with my NLP studies.  \nTherefore, I saw a youtube video by Cassie Kozyrkov. Her short videos are always extremely interesting! I like how she compares the \"learning theory\" as a teacher that tries to teach to his students. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9898517883070185
      ],
      "excerpt": "The Latent Dirichlet Allocation (LDiA) is a generative probabilistic model for collections of discrete data. It assumes that each document is a linear combination of a given number of topics, each one represented by a distribution of words. Once defined the ldia vector on a dataset of \"spam/not spam\" messages, I trained an LDA (Linear Discriminant Analysis) classifier. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8164141669714053,
        0.9914951354359959,
        0.9173287972875953
      ],
      "excerpt": "Book : Natural Language Processing in Action: Understanding, Analyzing, and Generating Text With Python \nDAY-29: Today I completed the first part of \"Natural Language Processing in Action\" and I spent some time looking for financial blog/websites to scrape, in order to analyze the impact of news on Stock Prices. \nIn particular I am planning to scrape from finviz.com that for each stock presents a table with its news from several different sources. At the beginning I will extract just the titles, but I might try to implement some \"scraping\" patterns for some rources (e.g. Yahoo Finance, Bloomberg and so on). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8164141669714053
      ],
      "excerpt": "Book : Natural Language Processing in Action: Understanding, Analyzing, and Generating Text With Python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9725423448388192,
        0.9297845870535579
      ],
      "excerpt": "In particular, for each stock there is a \"table\" with the headline of the latest news. So, thanks to BeautifulSoup I was able to scrape the main table with id=\"news-table\", then I iterated over each row and I extracted the title, the resource and the link of the full description. Then, I preprocessed the text by removing digits, punctuation and by normalizing with the stemming technique. \nIn the following days I will try to analyze the correlation among the \"sentiment\" over all headlines and the stock's trend.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9789381878568928,
        0.9213854823691691
      ],
      "excerpt": "DAY-31 : Today I started reading the first pages of the 6th chapter from \"Natural Language Processing in action\", regarding Word2Vec. It learns the meaning of the words by simply processing a large corpus of unlabeled data.  \nIt looks really promising, tomorrow I'll try to complete this chapter and to start implementing it into my small project. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8164141669714053,
        0.8223562483184929,
        0.9191909229822546,
        0.9590168559660055,
        0.8263161046537453
      ],
      "excerpt": "Book : Natural Language Processing in Action: Understanding, Analyzing, and Generating Text With Python \nDAY-32 : Today I studied the Word2Vec a bit more in detail, from \"Natural Language Processing in Action\". \nThen, I tried to apply the Word2Vec on the financial news scraped from finviz.com. Unfortunately, I realized that I didn't have enough data (because the headline proposed by finviz are just a small sample). So, I decided to start implement it on a \"more complete\" dataset. In particular I went through a dataset that I found on kaggle with 1.6 millions tweets. \nI spent some time trying to clean it and I realized once and for all that I need to improve with Regular Expressions! After that, I obtained interesting results. Now I need to figure it out how can I implement an unserpvised sentiment analysis. I think I should use KMeans with two clusters, but then I probably need to merge all the weights for each word (?). \nAs always, feedbacks, ideas and protips are always well received! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8164141669714053
      ],
      "excerpt": "Book : Natural Language Processing in Action: Understanding, Analyzing, and Generating Text With Python  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9506434270306025,
        0.8636755447930946,
        0.9056724880568993
      ],
      "excerpt": "DAY-33 : Today I tried to implement an Unsupervised Sentiment Analysis with Word2Vec, KMeans and further computations.  \nThanks to S\u00f6ren Grannemann I came across an interesting article that followed my initial idea. Unfortunately I wasn't able to replicate it into my dataset. I struggled for around one hour but then I decided to look for new articles and resources. \nThe most popular \"unsupervised tool\" for doing Sentiment Analysis seems to be Vader (Valence Aware Dictionary for sEntiment Reasoning). I was a bit skeptical at the beginning, but to be honest it surprised me. In the picture down below you can see some tests.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9646821274941794,
        0.855607689496303
      ],
      "excerpt": "DAY-34 : Today I spent a lot of time with Twitter API's in order to find out a way for overcoming the limitation imposed (100 tweets) but I got millions of errors.  \nIn particular I tried to iterate on a fixed time window as I saw in an article (1) but I face the same error any time:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9480195811337689
      ],
      "excerpt": "I also tried with tweepy but it doesn't work since the new API's update and twitterscraper doesn't return me anything.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8736539426012875,
        0.967905749751887,
        0.9806840906473981
      ],
      "excerpt": "DAY-35 : Today I finally managed Twitter APIs, so I was able to scrape one week of tweets with a given query.  \nSo, after cleaning the text (removing \"#\", tags, links and so on) I used VADER for an unsupervised sentiment analysis. In order to analyze the correlation among the sentiment and the stock price I used yfinance for extracting the prices in the same time window. \nThen, I grouped the tweets per day and I considered the percentage of positive tweets per day and I plot them with Plotly. This model is far from \"consistent\" but I guess it is a nice starting point. An important point to consider is that I extracted 4372 tweets, so it doesn't reflect the \"popular opinion\". In the following days I would like to improve the sentiment analysis, in order to get a slightly better result.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9052441192358103,
        0.8917718727875145,
        0.9704769761078691,
        0.9922155953772048
      ],
      "excerpt": "DAY-36 : Today I tried to improved the Sentiment Analysis \"score\", even if since it is unsupervised, it cannot easily judged. \nYesterday I used Vader and today I compared it with two other unservised algorithms: TextBlob and Flair. \nTextBlob provides the \"sentiment\" property, which returns the tuple (polarity,subjectiviy) where polarity is a float value between [-1.0,1]. Then, Flair is also able to predict the \"sentiment\" with a given confidence percentage.  \nAmong the three models, the most consistent (in this specific sample) seems to be Vader or TextBlob, but it is not so easy to evaluate. The bottlneck of this approach are the Twitter APIs because of its limit. In fact, there is a limit of 100 tweets for each request and it is not possible to send too many requests, in fact after a while I received the Error 429 (Too Many Requests). I'd try with a sleep between each request. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9976266415201069,
        0.98683942913622,
        0.8349760649368299,
        0.8579677165342047,
        0.8183309457941321,
        0.86338870971946
      ],
      "excerpt": "\"SpaceX is going to put a literal Dogecoin on the literal moon\".  \nIt is well known that Elon Musk \"pushed\" Dogecoing for a while thanks to his twitter account. So I decided to quantify how much influence he actually has on this cryptocurrency. So I decided to scrape Elon Musk's profile by using selenium in order to collect as much tweets as I could with the keyowrds ['doge','dogecoin','dog','shiba']. Furthermore, I was able to collect 15 tweets, from \"2021-02-04\" to \"2021-04-01\" (yesterday). \nFinally, I took the \"doge-usd\" prices from \"yfinance\", and the result is impressive! This is the reason why I love data science: for any question you have, if you find the right data, you'll be able to answer it (at least, most of the time)! \nThe curious thing is that he influenced both positively and negatively, in fact if you see three of the most significant falls, his tweet were: \n* Feb 11 =  Frodo was the underdoge, All thought he would fail, Himself most of all. \n* Feb 14 =  If major Dogecoin holders sell most of their coins, it will get my full support. Too much concentration is the only real issue imo. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8806945821804467
      ],
      "excerpt": "Even if it doesn't seem so complicate, I am always enthusiastic about these insights! I have also some ideas to implement in the following days from this base. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9548857214335952
      ],
      "excerpt": "DAY-38 : Today I decided that I will slow down a bit this weekend, so I did a wery well review of Gradient Descent thanks to the amazing StatQuest's video. I do not deny that most of the time I prefer to review some topics from this channel due to all the hypnotic intros! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9119245795548364,
        0.9967187238189558
      ],
      "excerpt": "DAY-39 : Today I briefly read the first chapter (introduction) of \"Natural Language Processing with PyTorch\". \nSince I wanted to practise on PyTorch due to a future project for my \"Machine Learning and Deep Learning\" course at univerisity, I decided to switch book! The book \"Natural Language Processing in action\" is very nice book but let's see how it will go with the new one! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9098066969719433,
        0.9500510271712388,
        0.9152227224266198
      ],
      "excerpt": "Book : Natural Language Processing with PyTorch \nDAY-40 : Today I went through the third chapter of \"Natural Language Processing with PyTorch\". \nFirst impressions of this book are good. This chapter was focused on the basics of Neural Networks. So, it covered concepts as Perceptron, Activation and Loss functions and finally it explained the neural network \"workload\", followed by a supervised sentiment analysis in PyTorch (that you can find below). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9098066969719433
      ],
      "excerpt": "Book : Natural Language Processing with PyTorch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9543799273267249
      ],
      "excerpt": "DAY-41 : Today I went ahead with the book \"Natural Language Processing with PyTorch\". \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9513208158741466
      ],
      "excerpt": "Vocabulary coordinates two Python dictionaries, that form a bijection between tokens and integers. Vectorizer, instead, converts individual tokens into integers. Finally SurnameDataset is responsible of the initial data management.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9098066969719433,
        0.9800834307914204
      ],
      "excerpt": "Book : Natural Language Processing with PyTorch \nDAY-42 : Today I completed the Surname Classification example from \"Natural Language Processing with PyTorch\". I also started looking at the introduction of Word Embeddings and I will continue it tomorrow! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9098066969719433,
        0.9522358403375513,
        0.9672829462868604,
        0.9269966033508169,
        0.992279024254211
      ],
      "excerpt": "Book : Natural Language Processing with PyTorch \nDAY-43 : Today I went deeper on Word Embedding with the book \"Natural Language Processing with PyTorch\". \nWord Embeddings methods map large representative vectors into lower dimensional space, maintaining any kind of semantic relationship. The main advantage are: faster computation avoid redundant representation avoid curse of dimensionality representations learned from task specific are optimal \nAll word embedding methods train with unlabeled data but use some auxiliary supervised tasks in order to extract some correlations among the words, such as \u201cpredict the next word\u201d, \u201cpredict the missing word in a sequence\u201d and so on. \nIn the following example I predicted the analogy of a sequence of terms in the order of \"word1 : word2 = word3 : X\". Here it is possible to see which are the consequences of biased data used to train the algorithms! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9098066969719433
      ],
      "excerpt": "Book : Natural Language Processing with PyTorch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9696140791940653
      ],
      "excerpt": "This representation was possible thanks to the t-SNE (t-distributed stochastic neighbor embedding), a manifold learning technique that constructs the probability distribution of the data in the initial dimensional space and then, it tries to maintain such a probability distribution over the smallest dimensional space.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8636885221731039,
        0.9214928064572351
      ],
      "excerpt": "DAY-45 : Today I went through the LSTM paper and a couple of additional resources. \nRNNs, due to its construction, suffer to short term memory. In order to mitigate this problem, the LSTM (Long Short Term Memory) architecture proposes to use an additional component, in order to learn long term dependencies. The computation will be heavier due to an increasing in complexity, so this architecture is suggested when you expect to learn long sequences to learn.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8463986902981886
      ],
      "excerpt": "On the other side, it is possible to controll the memory status in three possible ways: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9711284343950402,
        0.9159907325245974,
        0.9534216470133732,
        0.802520630395967
      ],
      "excerpt": "This paper proposes an end to end approach to sequence learning that makes minimal assumptions to the sequence structure. It uses a multilayered LSTM to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM decode the target sequence from the vector. \nDNNs can perform very well when inputs and targets can be sensibly encoded with vectors of fixed dimensionality but can perform dramatically worse with sequence of tokens. So, the idea is to use LSTM to read the input sequence, one timestamp at a time, to obtain large fixed dimensional vector representation, and then, use another LSTM to extract the output sequence from this former vector. \nThe traditional approach supposes to have input and output sequence vector with an equal length, and it might be a sort of \"bottleneck\". In this case the idea to use a double LSTM in order to encode the input sequence to a fixed vector, and then, to map this target vector into the actual prediction. The LSTM architecture has been proposed due to its \"larger\" tolerance to long term dependencies. \nAnother interesting trick regards the encoding, in fact the authors proposed to reverse the tokens. So, if \"a,b,c\" gives \"x,y,z\", now \"c,b,a\" gives \"x,y,a\". This trick allows to have a stronger communication between the \"a\" and \"x\".   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9842662678832915
      ],
      "excerpt": "RNNs are really valuable for modeling sequences, but the main problem is that they're slow, they work with one token at each timestamp. On the other hand, CNN are really valuable for extracting spatial features extremely fast (thanks to the parallelization). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9320050474270127,
        0.8435632527035233
      ],
      "excerpt": "The pooling component captures the most relevant features obtained so far. In this case, the authors  were inspired by the nature of the LSTM and the pooling layers simply mix the hidden states across timestamps, indipendently on each channel of the state vector. \nThis architecture is opened to severl improvements, such as regularizations, densely connected layers, encoder-decoder structures and so on.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9852929072314094,
        0.9728614182056847,
        0.9554501062359426
      ],
      "excerpt": "It is often argued the relevance of leetcode for technical interviews and I admit that sometimes I asked myself the usual question : why should I spend time doing DSA exercises instead of improving my Machine Learning skills or going ahead personal projects? \nI recognize the fundamental importance of Data Structure and Algorithms, but at the moment I can't say that I am pro or against leetcoding for technical roles. \nI'd be very happy to know some other opinions from different point of views! What do you think about this kind of training for technical roles?  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9270411580121444,
        0.937202388984023,
        0.986046203938775,
        0.9692006346922122,
        0.9966474696551649,
        0.908925214220865
      ],
      "excerpt": "DAY-49 : Today I studied the flavours of \"Federated Learning\" thanks to various papers and articles. \nFederating Learning was proposed for the first time by Google in 2017 and it was (and it still is) an innovating approach for training machine learning algorithms on clients' devices (smartphone et simila).  \nThe main idea is that the smartphone downloads the current model and it improves by learning the data from the smartphone itself, and just a small amount of data will be sent to the server for improving the general model. So in this way all the training data will be stored just on the client.  \nThe challenges of this interesting technique are related to: \n* computational resources of the devices \n* bandwith and latency  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9755161798624484
      ],
      "excerpt": "The training process is determined by the federated averaging algorithm, where the main server chooses a subset of data clients for a training step. So, these clients receive the model will send some information in order to improve the model. Finally the main server will optimize the model by averaging all these parameters. In order to preserve the privacy, there will be injected some noise before entering in the main system, so it would be hard to deanonimize them by aggregating them with other data.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8141514035741578,
        0.9405693988926868,
        0.9665412405099689,
        0.9239709029521225
      ],
      "excerpt": "DAY-50 : Today, for an academic assignement (in Computer Vision), I studied the paper \"Unsupervised Pixel\u2013Level Domain Adaptation\" with Generative Adversarial Networks. \nThe goal of domain adaptation is to train data on a source dataset and apply it on a target dataset generated from a different distribution. The term \"unsupervised\" is given by the fact that we have no labels in the target domain.  \nSo, since creating dataset such as ImageNet and COCO is quite expansive, so a feasible alternative is the use of synthetic data for modeling by using unsupervised domain adaptation. So we aim to transfer knowledge learned from a source domain, for which we have labeled data, to a target domain for which we have no labels.  \nThis was just the flavour of the paper, tomorrow I will go deeper into the model! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8337460858243507,
        0.9043481763492368,
        0.9191532109249188,
        0.8536755602011742,
        0.8733284656116759
      ],
      "excerpt": "DAY-51 : Today I went deeper on the paper \"Unsupervised Pixel\u2013Level Domain Adaptation\" with Generative Adversarial Networks. \nRecall that the goal of unsupervised domain adaptation is to train data on a source dataset and apply it on a target dataset generated from a different distribution where we have no labels.  \nAs you can see from the image, there are three main components on the model: the gnerator (G), the Discriminator (D) and the task specific classifier (T).  \nIn short, the generator G generates an image from a synthetic image and a noise vector. Then, the discriminator D discriminates among real and fake images whereas the task-specific classifier T, instead, assign a labels to the fake image.  \nThis model provides several benefits such as: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "- Data Augmentation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9465999178510657
      ],
      "excerpt": "But also, it outperforms previous state of the art unsupervised domain adaptation techniques on several different image datasets! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9842627567502845
      ],
      "excerpt": "DAY-52 : Today I spent most of my free time with Selenium, in order to accomplish a task for my current Omdena's project.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9663246371761064
      ],
      "excerpt": "DAY-52 : Today I went through t-SNE (t-distributed stochastic neighbor embedding), a manifold learning technique that constructs the probability distribution of the data in the initial dimensional space and then, it tries to maintain such a probability distribution on a smaller dimensional space. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.861841202976033
      ],
      "excerpt": "DAY-53 : Today I went through the PyTorch's implementation of the Pixel-level Domain Adaptation algorithm. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9153295542000468
      ],
      "excerpt": "iCaRL states for Incremental Classifier and Representation Learning, and it is devoted to a \"different\" training approach. As you can imagine, this training will be incremental. It means that we periodically fed the model with new class labels to train.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8869683836334066,
        0.9189063530902365,
        0.8635852036377578,
        0.9855922932982163
      ],
      "excerpt": "- It can continuously learn to improve when the system is running; \n- It can adapt to changes of the target concept \n- It doesn't need a priori informations about the number or distribution of the data \nThe main issue (that it is argued in the literature) of Incremental Learning is the cathastrophic forgetting, and this paper proposes a new approach for overcoming this issue, based on three main components: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8999645708701658,
        0.9508991134966328
      ],
      "excerpt": "- Set of exemplars: selection procedure and discard on the fly \n- Nearest mean of exemplars classifier : automatic adjustement to representation change. For each new sample, we'll assign it to the nearest approximate class mean in the feature space. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8000674450488139
      ],
      "excerpt": "DAY-55 : Today I went through a couple of papers related to Incremental Learning and Semantic Segmentation for an incoming academic projects in Computer Vision. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9061723011828153
      ],
      "excerpt": "Sometimes I like to recall some topics that I already know, in order to look them from other perspectives and I could say that it was worth it! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.943477297922509,
        0.988287604777745
      ],
      "excerpt": "DAY-58 : Today I went through the paper and the Pytorch implementation of the paper \"Neural Style Transfer Tutorial\". \nThe author proposed an approach for transfering the context of an image to another one, while holding the context fixed. In order to obtain the the style, they used a feature space built on top of the filter responsed in each layer of the network. It consists of the correlations among different filter responses over the spatial extent.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8590811441303946
      ],
      "excerpt": "Thanks to this challenge, during this period I have studied a lot of things that I have always postponed!  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9705744719367106
      ],
      "excerpt": "DAY-60 : Today I spent most of my time cleaning data for my current Omdena's project and I also continued my past script for scraping tweets from twitter API's.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9480550772376511
      ],
      "excerpt": "DAY-62 : Today I went through the implementation of LSTM in PyTorch for a sentiment analysis on the fairly well known IMDB dataset. The results are really promising!  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9834056498499288
      ],
      "excerpt": "In particular the authors of \"Efficacy of News Sentiment for Stock Market Prediction\" achieved an accuracy among 60% to 90%. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9524636649244566,
        0.9907190392862554
      ],
      "excerpt": "DAY-64 : Today I just worked for my current Omdena's project, so I spent a couple of hours scraping data from a website. \nIt is always satisfying when you're able to find the way for scraping autonomously 390 pages full of data with just a for-loop and some preliminary analysis! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9695236030928148,
        0.9611436422333391,
        0.8471801851184282
      ],
      "excerpt": "DAY-66 : It has been a great journey thanks to which I have had the chance to learn more, especially about Financial Machine Learning, Algorithmic Trading and Natural Language Processing.  \nI can definitely say that I have learnt a lot and I have met a lot of amazing people along the way, that enriched, not only my technical skills, but also my desire to learn and share new things! \nThank you for your kind support, let's see to the next challenge! <br /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "#66DaysOfData challenge in Financial Machine Learning ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/francescodisalvo05/66DaysOfData/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Thu, 30 Dec 2021 07:51:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/francescodisalvo05/66DaysOfData/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "francescodisalvo05/66DaysOfData",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_20.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_15.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_02.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day58.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_13.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_35.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_32.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_30.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_37.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_44.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_19.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_25.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_16.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_22.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_24.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_12.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_00.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_11.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_28.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_41.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_10.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_43.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_33.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_23.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_26.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_05.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_36.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/day_42.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/.ipynb_checkpoints/Day6-checkpoint.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/.ipynb_checkpoints/Day2-checkpoint.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/.ipynb_checkpoints/Day5-checkpoint.ipynb",
      "https://raw.githubusercontent.com/francescodisalvo05/66DaysOfData/main/notebooks-scripts/.ipynb_checkpoints/Day7-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9004235778166562
      ],
      "excerpt": "Jupyter notebook : https://github.com/francescodisalvo05/66DaysOfData/blob/main/notebooks/Day5.ipynb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8432615731897319
      ],
      "excerpt": "LinkedIn #11 : https://www.linkedin.com/posts/francescodisalvo-pa_day11-activity-6774418286193987584-HbSl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8638195576118045
      ],
      "excerpt": "LinkedIn #13 : https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-algotrading-datascience-activity-6775105830531407872-Gusq \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8224459001270336
      ],
      "excerpt": "LinkedIn #16 : https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-algotrading-datascience-activity-6776254545782677504-1jI-  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8480497526068278
      ],
      "excerpt": "LinkedIn #20 : https://www.linkedin.com/posts/francescodisalvo-pa_66daysofdata-datascience-machinelearning-activity-6777661385410142208-7aDc \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8555076313907575
      ],
      "excerpt": "Notebook : https://github.com/francescodisalvo05/66DaysOfData/blob/main/notebooks-scripts/day_22.ipynb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8555076313907575
      ],
      "excerpt": "Notebook : https://github.com/francescodisalvo05/66DaysOfData/blob/main/notebooks-scripts/day_23.ipynb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8555076313907575
      ],
      "excerpt": "Notebook : https://github.com/francescodisalvo05/66DaysOfData/blob/main/notebooks-scripts/day_24.ipynb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8555076313907575
      ],
      "excerpt": "Notebook : https://github.com/francescodisalvo05/66DaysOfData/blob/main/notebooks-scripts/day_25.ipynb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8555076313907575
      ],
      "excerpt": "Notebook : https://github.com/francescodisalvo05/66DaysOfData/blob/main/notebooks-scripts/day_26.ipynb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8185254928179275
      ],
      "excerpt": "Youtube : https://www.youtube.com/watch?v=ZQtuTqmr4WI \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8555076313907575
      ],
      "excerpt": "Notebook : https://github.com/francescodisalvo05/66DaysOfData/blob/main/notebooks-scripts/day_28.ipynb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8199431331947394
      ],
      "excerpt": "Sentiment Analysis : https://github.com/joosthub/PyTorchNLPBook/blob/master/chapters/chapter_3/3_5_Classifying_Yelp_Review_Sentiment.ipynb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9713899864370701
      ],
      "excerpt": "You can find the code here : https://github.com/eriklindernoren/PyTorch-GAN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8185254928179275
      ],
      "excerpt": "YouTube : https://www.youtube.com/watch?v=XMSjOatyH0k \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8732089505944296
      ],
      "excerpt": "YouTube : https://machinelearningcompass.com/model_optimization/bias_and_variance/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8434871230610268
      ],
      "excerpt": "Code : https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/neural_style_transfer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8185254928179275
      ],
      "excerpt": "YouTube : https://www.youtube.com/watch?v=6tNS--WetLI \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8185254928179275
      ],
      "excerpt": "YouTube : https://www.youtube.com/watch?v=lzaBbQKUtAA \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9115363560872731
      ],
      "excerpt": "  <img src=\"images/bg.png\" height=\"500px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day4.jpg\" height=\"400px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day7.jpg\" height=\"300px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day8.jpg\" height=\"500px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day12.jpg\" height=\"300px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811854372964597
      ],
      "excerpt": "DAY-15:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day19.jpg\" height=\"400px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day20.jpg\" height=\"400px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day23-1.jpg\" height=\"400px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day25.jpg\" height=\"400px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day26.jpg\" height=\"400px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day28.jpg\" height=\"400px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day29.jpg\" height=\"400px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day30-1.jpg\" height=\"400px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day32.jpg\" height=\"300px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day33.jpg\" height=\"300px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day33.jpg\" height=\"300px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day37.jpg\" height=\"300px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day41.jpg\" height=\"350px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day43.jpg\" height=\"450px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day45.jpg\" height=\"350px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138229211481706
      ],
      "excerpt": "  <img src=\"https://github.com/francescodisalvo05/66DaysOfData/blob/main/images/day46.jpg\" height=\"350px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "- Training Stability \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/francescodisalvo05/66DaysOfData/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "66DaysOfData </h1>",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "66DaysOfData",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "francescodisalvo05",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/francescodisalvo05/66DaysOfData/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 17,
      "date": "Thu, 30 Dec 2021 07:51:17 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "machine-learning",
      "data-science",
      "finance",
      "nlp"
    ],
    "technique": "GitHub API"
  }
}