{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use our code or method in your work, please consider citing the following:\n\n```\n@InProceedings{eccv_2018_yang_netadapt,\n\tauthor = {Yang, Tien-Ju and Howard, Andrew and Chen, Bo and Zhang, Xiao and Go, Alec and Sandler, Mark and Sze, Vivienne and Adam, Hartwig},\n\ttitle = {NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications},\n\tbooktitle = {The European Conference on Computer Vision (ECCV)},\n\tmonth = {September},\n\tyear = {2018}\n}\n```\n\nPlease direct any questions to the authors: Tien-Ju Yang (tjy@mit.edu) and Yi-Lun Liao (ylliao@mit.edu).\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{eccv_2018_yang_netadapt,\n    author = {Yang, Tien-Ju and Howard, Andrew and Chen, Bo and Zhang, Xiao and Go, Alec and Sandler, Mark and Sze, Vivienne and Adam, Hartwig},\n    title = {NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications},\n    booktitle = {The European Conference on Computer Vision (ECCV)},\n    month = {September},\n    year = {2018}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/denru01/netadapt",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-18T21:39:44Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T14:49:56Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "0. [Requirements](#requirements)\n0. [Usage](#usage)\n0. [Example](#example)\n0. [Customization](#customization)\n0. [Citation](#citation)\n\n<!---\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.859120002401202
      ],
      "excerpt": "worker.py : called by master.py to simplify a certain block of the network based on resource constraints and then run short-term finetune of simplified models. Several workers may execute at the same time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9365497926787555
      ],
      "excerpt": "To apply NetAdapt to differenct networks or different tasks, please follow the instructions: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.860059181823877
      ],
      "excerpt": "        def yourNetworkOrTask(model, input_data_shape, dataset_path, finetune_lr=1e-3): \n            return networkUtils_yourNetworkOrTask(model, input_data_shape, dataset_path, finetune_lr) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9690135801212472
      ],
      "excerpt": "Our current code base supports pruning Conv2d, ConvTranspose2d, and Linear with additive skip connection. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8508593638593988,
        0.9549639475463227
      ],
      "excerpt": "We can apply NetAdapt to different networks or tasks by using --arch yourNetworkOrTask in scripts/netadapt_alexnet-0.5mac.sh. \nAs for the values of other arguments, please see Usage.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This repo contains the official Pytorch reimplementation of the paper \"NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications\".",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/denru01/netadapt/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 41,
      "date": "Thu, 30 Dec 2021 01:25:09 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/denru01/netadapt/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "denru01/netadapt",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/denru01/netadapt/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/denru01/netadapt/master/scripts/unittest.sh",
      "https://raw.githubusercontent.com/denru01/netadapt/master/scripts/unittest_mobilenet.sh",
      "https://raw.githubusercontent.com/denru01/netadapt/master/scripts/netadapt_alexnet-0.5latency.sh",
      "https://raw.githubusercontent.com/denru01/netadapt/master/scripts/unittest_alexnet.sh",
      "https://raw.githubusercontent.com/denru01/netadapt/master/scripts/netadapt_mobilenet-0.5mac.sh",
      "https://raw.githubusercontent.com/denru01/netadapt/master/scripts/netadapt_mobilenet-0.5latency.sh",
      "https://raw.githubusercontent.com/denru01/netadapt/master/scripts/unittest_helloworld.sh",
      "https://raw.githubusercontent.com/denru01/netadapt/master/scripts/netadapt_alexnet-0.5mac.sh",
      "https://raw.githubusercontent.com/denru01/netadapt/master/scripts/netadapt_helloworld.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "    cd network_utils \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8193988374667249
      ],
      "excerpt": "network_utils_depth_mobilenetv1 : an example defining mobilenet, its correspoding loss function, data loaders. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8422345778862849
      ],
      "excerpt": "Create your own network_utils python file (said network_utils_yourNetworkOrTask.py) and place it under network_utils. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.893437310351934,
        0.8761234886440268,
        0.8381675083042451
      ],
      "excerpt": "    cp network_utils_alexnet.py ./network_utils_yourNetworkOrTask.py \nAdd from .network_utils_yourNetworkOrTask import * to __init__.py, which is under the same directory. \nModify class networkUtils_alexnet(...) in line 44 in network_utils_yourNetworkOrTask.py to class networkUtils_yourNetworkOrTask(...). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144758516211369
      ],
      "excerpt": "Modify data loader and loss functionsin function def __init__(...): in line 52. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/denru01/netadapt/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 denru01\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "netadapt",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "denru01",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/denru01/netadapt/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code base is tested with the following setting:\n\n1. Python 3.7.0\n2. CUDA 10.0\n3. Pytorch 1.2.0\n4. torchvision 0.4.0\n5. numpy 1.17.0\n6. scipy 1.3.1\n\nFirst clone the repo in the directory you want to work:\n\n        git clone https://github.com/denru01/netadapt.git  \n        cd netadapt\n\nIn the following context, we assume you are at the repo root.\n\nIf the versions of Python and CUDA are the same as yours, you can download the python packages using:\n    \n        pip install -r requirements.txt\n    \nTo verify the downloaded code base is correct, please run either\n\n        sh scripts/unittest.sh\n        \nor\n\n        sh scripts/unittest_helloworld.sh\n        sh scripts/unittest_alexnet.sh\n        sh scripts/unittest_mobilenet.sh\n        \nIf it is correct, you should not see any FAIL.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 147,
      "date": "Thu, 30 Dec 2021 01:25:09 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In order to apply NetAdapt, run:\n            \n        python master.py [-h] [-gp GPUS [GPUS ...]] [-re] [-im INIT_MODEL_PATH]\n                 [-mi MAX_ITERS] [-lr FINETUNE_LR] [-bu BUDGET]\n                 [-bur BUDGET_RATIO] [-rt RESOURCE_TYPE]\n                 [-ir INIT_RESOURCE_REDUCTION]\n                 [-irr INIT_RESOURCE_REDUCTION_RATIO]\n                 [-rd RESOURCE_REDUCTION_DECAY]\n                 [-st SHORT_TERM_FINE_TUNE_ITERATION] [-lt LOOKUP_TABLE_PATH]\n                 [-dp DATASET_PATH] [-a ARCH] [-si SAVE_INTERVAL]\n                 working_folder input_data_shape input_data_shape\n                 input_data_shape\n\n- `working_folder`: Root folder where models, related files and history information are saved. You can see how models are pruned progressively in `working_folder/master/history.txt`.\n\n- `input_data_shape`: Input data shape (C, H, W) (default: 3 224 224). If you want to apply NetAdapt to different tasks, you might need to change data shape.\n\n- `-h, --help`: Show this help message and exit.\n            \n- `-gp GPUS [GPUS ...],  --gpus GPUS [GPUS ...]`: Indices of available gpus (default: 0).\n            \n- `-re, --resume`: Resume from previous iteration. In order to resume, specify `--resume` and specify `working_folder` as the one you want to resume. \nThe resumed arguments will overwrite the arguments provided here.\nFor example, if you want to simplify a model by pruning and finetuning for 30 iterations (under `working_folder`), however, your program terminated after 20 iterations.\nThen you can use `--resume` to restore and continue for the last 10 iterations.\n            \n- `-im INIT_MODEL_PATH, --init_model_path INIT_MODEL_PATH`: Path to pretrained model.\n            \n- `-mi MAX_ITERS, --max_iters MAX_ITERS`: Maximum iteration of removing filters and short-term fine-tune (default: 10).\n            \n- `-lr FINETUNE_LR, --finetune_lr FINETUNE_LR`: Short-term fine-tune learning rate (default: 0.001).\n            \n- `-bu BUDGET, --budget BUDGET`: Resource constraint. If resource < `budget`, the process is terminated.\n            \n- `-bur BUDGET_RATIO, --budget_ratio BUDGET_RATIO`: If `--budget` is not specified, `buget` = `budget_ratio`\\*(pretrained model resource) (default: 0.25).\n            \n- `-rt RESOURCE_TYPE, --resource_type RESOURCE_TYPE`: Resource constraint type (default: FLOPS). We currently support `FLOPS`, `WEIGHTS`, and `LATENCY` (device `cuda:0`). If you want to add other resource\ntypes, please modify `def compute_resource(...)` in `network_util` python files (e.g. `network_utils/network_utils_alexnet`).\n            \n- `-ir INIT_RESOURCE_REDUCTION, --init_resource_reduction INIT_RESOURCE_REDUCTION`: For each iteration, target resource = current resource - `init_resource_reduction`\\*(`resource_reduction_decay`\\*\\*(iteration-1)).\n\n- `-irr INIT_RESOURCE_REDUCTION_RATIO, --init_resource_reduction_ratio INIT_RESOURCE_REDUCTION_RATIO`: If `--init_resource_reduction` is not specified,\n`init_resource_reduction` = `init_resource_reduction_ratio`\\*(pretrained model resource) (default: 0.025).\n\n- `-rd RESOURCE_REDUCTION_DECAY, --resource_reduction_decay RESOURCE_REDUCTION_DECAY`: For each iteration, target resource = current resource - `init_resource_reduction`\\*(`resource_reduction_decay`\\*\\*(iteration-1)) (default: 0.96).\n            \n- `-st SHORT_TERM_FINE_TUNE_ITERATION, --short_term_fine_tune_iteration SHORT_TERM_FINE_TUNE_ITERATION`: Short-term fine-tune iteration (default: 10).\n\n- `-lt LOOKUP_TABLE_PATH, --lookup_table_path LOOKUP_TABLE_PATH`: Path to lookup table.\n\n- `-dp DATASET_PATH, --dataset_path DATASET_PATH`: Path to dataset.\n\n- `-a ARCH, --arch ARCH  network_utils`: Defines how networks are pruned, fine-tuned, and evaluated. If you want to use\nyour own method, please see [**Customization**](#customization) and specify here. (default: alexnet)\n            \n- `-si SAVE_INTERVAL, --save_interval SAVE_INTERVAL`: Interval of iterations that all pruned models at the same iteration will be saved. \nUse `-1` to save only the best model at each iteration. \nUse `1` to save all models at each iteration. (default: -1).\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "<!---\n\nWe provide an example of applying **NetAdapt** to **AlexNet** on **CIFAR-10**. \nIf you want to apply the algorithm to different networks or even different tasks,\nplease see the following [**Customization**](#customization) section. \n\nThe example consists of the following four steps: 1. training AlexNet on CIFAR-10; 2. measuring latency; 3. applying NetAdapt; 4. evaluation using adapted models.\n\n1. **Training AlexNet on CIFAR-10.**\n\n    Train from scratch:\n    ```bash\n    python train.py data/ --dir models/alexnet/no-pretrain/checkpoint.pth.tar --arch alexnet\n    ```\n    Use Imagenet-pretrained model:\n    ```bash\n    python train.py data/ --dir models/alexnet/pretrain/checkpoint.pth.tar --pretrained --lr 0.01 --arch alexnet\n    ```\n    Evaluation:\n    ```bash\n    python eval.py data/ --dir models/alexnet/no-pretrain/checkpoint.pth.tar --arch alexnet\n    ```\n    \n    ```bash\n    python train.py data/ --dir models/mobilenet/model.pth.tar --arch mobilenet\n    ```\n2. **Measuring Latency**\n\n    Here we build the latency lookup table for `cuda:0` device:\n    ```bash\n    python build_lookup_table.py --dir latency_lut/lut_alexnet.pkl --arch alexnet\n    ```\n    It measures latency of different layers contained in the network (i.e. **AlexNet** here).\n    For conv layers, the sampled numbers of feature channels are multiples of `MIN_CONV_FEATURE_SIZE`.\n    For fc layers, the sampled numbers of features are multiples of `MIN_FC_FEATURE_SIZE`. \n        \n3. **Applying NetAdapt**\n\n    Modify which GPUs will be utilized (`-gp`) in `netadapt_alexnet.sh` and run the script to apply NetAdapt to a pretrained model:\n    ```bash\n    sh netadapt_alexnet.sh\n    ```\n    \n    After obtaining the adapted model, we need to finetune the model:\n    ```bash\n    python train.py data/ --arch alexnet --resume models/alexnet/no-pretrain/prune-by-mac/iter_100_best_model.pth.tar --dir models/alexnet/no-pretrain/checkpoint-adapt.pth.tar\n    ```\n    \n    <p align=\"center\">\n\t<img src=\"fig/netadapt_algo.png\" alt=\"photo not available\" width=\"90%\" height=\"90%\">\n    </p>\n\n4. **Evaluation Using Adapted Models**\n\n    After applying NetAdapt to a pretrained model, we can evaluate this adapted model using:\n    ```bash\n    python eval.py data/ --dir models/alexnet/no-pretrain/checkpoint-adapt.pth.tar --arch alexnet\n    ```\n    The adapted model can be restored **without modifying the orignal python file**.\n\n--->\n\nWe provide a simple example of applying **NetAdapt** to a very small [network](nets/helloworld.py):\n    \n        sh scripts/netadapt_helloworld.sh\n\nDetailed examples of applying **NetAdapt** to **AlexNet**/**MobileNet** on **CIFAR-10** are shown [**here (AlexNet)**](docs/alexnet/README.md) and [**here (MobileNet)**](docs/mobilenet/README.md).\n\n<p align=\"center\">\n<img src=\"fig/netadapt_algo.png\" alt=\"photo not available\" width=\"90%\" height=\"90%\">\n</p>\n\nIf you want to apply the algorithm to different networks or even different tasks,\nplease see the following [**Customization**](#customization) section.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}