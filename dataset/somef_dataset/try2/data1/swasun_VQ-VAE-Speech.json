{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1901.08810",
      "https://arxiv.org/abs/1609.03499",
      "https://arxiv.org/abs/1711.00937",
      "https://arxiv.org/abs/1811.02155",
      "https://arxiv.org/abs/1901.08810, 01 2019](https://arxiv.org/abs/1901.08810).\n\n* [van den Oord et al., 2016] [A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWaveNet: A generative model for raw audio,\u201d arXiv preprint https://arxiv.org/abs/1609.03499, 2016](https://arxiv.org/abs/1609.03499).\n\n* [van den Oord et al., 2017] [van den Oord A., and Oriol Vinyals. \"Neural discrete representation learning.\" Advances in Neural Information Processing Systems(NIPS). 2017](https://arxiv.org/abs/1711.00937).\n\n* [Ping et al., 2018] [Ping, Wei & Peng, Kainan & Chen, Jitong. (2018). ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech](https://github.com/ksw0306/ClariNet).\n\n* [ksw0306/ClariNet] https://github.com/ksw0306/ClariNet.\n\n* [Kim et al., 2018] [Kim, Sungwon & Lee, Sang-gil & Song, Jongyoon & Yoon, Sungroh. (2018). FloWaveNet : A Generative Flow for Raw Audio](https://arxiv.org/abs/1811.02155).\n\n* [ksw0306/FloWaveNet] https://github.com/ksw0306/FloWaveNet.\n\n* [r9y9/wavenet_vocoder] https://github.com/r9y9/wavenet_vocoder.\n\n* [zalandoresearch/pytorch-vq-vae] https://github.com/zalandoresearch/pytorch-vq-vae.\n\n* [deepmind/sonnet] https://github.com/deepmind/sonnet/blob/master/sonnet/examples/vqvae_example.ipynb.\n\n* [lmcinnes/umap] https://github.com/lmcinnes/umap",
      "https://arxiv.org/abs/1609.03499, 2016](https://arxiv.org/abs/1609.03499).\n\n* [van den Oord et al., 2017] [van den Oord A., and Oriol Vinyals. \"Neural discrete representation learning.\" Advances in Neural Information Processing Systems(NIPS). 2017](https://arxiv.org/abs/1711.00937).\n\n* [Ping et al., 2018] [Ping, Wei & Peng, Kainan & Chen, Jitong. (2018). ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech](https://github.com/ksw0306/ClariNet).\n\n* [ksw0306/ClariNet] https://github.com/ksw0306/ClariNet.\n\n* [Kim et al., 2018] [Kim, Sungwon & Lee, Sang-gil & Song, Jongyoon & Yoon, Sungroh. (2018). FloWaveNet : A Generative Flow for Raw Audio](https://arxiv.org/abs/1811.02155).\n\n* [ksw0306/FloWaveNet] https://github.com/ksw0306/FloWaveNet.\n\n* [r9y9/wavenet_vocoder] https://github.com/r9y9/wavenet_vocoder.\n\n* [zalandoresearch/pytorch-vq-vae] https://github.com/zalandoresearch/pytorch-vq-vae.\n\n* [deepmind/sonnet] https://github.com/deepmind/sonnet/blob/master/sonnet/examples/vqvae_example.ipynb.\n\n* [lmcinnes/umap] https://github.com/lmcinnes/umap"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [Chorowski et al., 2019] [Jan Chorowski, Ron J. Weiss, Samy Bengio, and Aaron van den Oord. Unsupervised speech representation learning using WaveNet autoencoders. arXiv e-prints, page arXiv:1901.08810, 01 2019](https://arxiv.org/abs/1901.08810).\n\n* [van den Oord et al., 2016] [A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWaveNet: A generative model for raw audio,\u201d arXiv preprint arXiv:1609.03499, 2016](https://arxiv.org/abs/1609.03499).\n\n* [van den Oord et al., 2017] [van den Oord A., and Oriol Vinyals. \"Neural discrete representation learning.\" Advances in Neural Information Processing Systems(NIPS). 2017](https://arxiv.org/abs/1711.00937).\n\n* [Ping et al., 2018] [Ping, Wei & Peng, Kainan & Chen, Jitong. (2018). ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech](https://github.com/ksw0306/ClariNet).\n\n* [ksw0306/ClariNet] https://github.com/ksw0306/ClariNet.\n\n* [Kim et al., 2018] [Kim, Sungwon & Lee, Sang-gil & Song, Jongyoon & Yoon, Sungroh. (2018). FloWaveNet : A Generative Flow for Raw Audio](https://arxiv.org/abs/1811.02155).\n\n* [ksw0306/FloWaveNet] https://github.com/ksw0306/FloWaveNet.\n\n* [r9y9/wavenet_vocoder] https://github.com/r9y9/wavenet_vocoder.\n\n* [zalandoresearch/pytorch-vq-vae] https://github.com/zalandoresearch/pytorch-vq-vae.\n\n* [deepmind/sonnet] https://github.com/deepmind/sonnet/blob/master/sonnet/examples/vqvae_example.ipynb.\n\n* [lmcinnes/umap] https://github.com/lmcinnes/umap\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9995318283478657,
        0.926081022393026
      ],
      "excerpt": "PyTorch implementation of VQ-VAE + WaveNet by [Chorowski et al., 2019] and VQ-VAE on speech signals by [van den Oord et al., 2017]. \nThe WaveNet [van den Oord et al., 2016] implementation is from [r9y9/wavenet_vocoder]. The VQ [van den Oord et al., 2016] implementation is inspired from [zalandoresearch/pytorch-vq-vae] and [deepmind/sonnet]. The ClariNet [Ping et al., 2018] and FloWaveNet [Kim et al., 2018] implementations are respectively from [ksw0306/ClariNet] and [ksw0306/FloWaveNet], but are not connected to the WaveNet decoder for now. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/swasun/VQ-VAE-Speech",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-19T08:29:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-15T06:57:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9607554250616209,
        0.8922589144321534,
        0.8749049485972785,
        0.8136229850959298,
        0.9045501223267947,
        0.8206687651661242
      ],
      "excerpt": "The code is actively changing; \nFor now instead of a Wavenet it's a deconvolutional NN to speed up my tests; \nFor now we are focusing on the evaluation of the VQ, and not on computing good audio samples. \nFor now only this architecture was used in our experiments to decrease the training time necessary to train a WaveNet. \nConvolutionalEncoder for the encoder and DeconvolutionalDecoder for the deconv decoder: \nThis figure describes the layers of the VQ-VAE model we have used. All convolution layers are in 1D dimension. The light orange color represents the convolutional part, whereas the dark orange represents the ReLU activation in the encoder. The two envelopes represent residual stacks. The purple arrows represents residual connections. The purple blocks are the embedding vectors. The pink layer represents the time-jitter regularization [Chorowski et al., 2019]. The light blue color represents the convolutional part, whereas the dark blue represents the ReLU activation in the decoder. The three pictures are view examples of respectively speech signal in waveform, MFCC features and log filterbank features. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9848579215219557
      ],
      "excerpt": "This figure shows the training evolution of the VQ-VAE model using two metrics: the loss values (the lower the better), and the perplexity, which is the average codebook usage. The model was trained during 15 epochs using the architecture described in Section VQ-VAE-Speech encoder + Deconv decoder. We used 44 vectors of dim 64 as the VQ space. All experiments have been setted with a seed of 1234 for reproducibility purpose. The jitter experiment used the jitter layer proposed in [Chorowski et al., 2019] during the training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9307445345431333,
        0.9400690436176488
      ],
      "excerpt": "In the following plot, each column is for the current epoch and each line of this epoch contains different time step of the training. Also, each box is a figure of the gradient flow on the encoder, vq and decoder layers. \nThe embedding are computed using [lmcinnes/umap] that is a dimension reduction technique that searches for a low dimensional projection of the data that has the closest possible equivalent \"fuzzy\" topological structure. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9739969912840747,
        0.9787239151506039,
        0.9276118108010235,
        0.9846152262577774,
        0.9431300375332988,
        0.9846152262577774,
        0.9431300375332988
      ],
      "excerpt": "The number of point is the time size of the MFCC features (divided by two because of the downsampling in the encoder) times the number of vectors times the batch size. The number of marks is the number of embedding vectors (i.e., the tokens). \nThe right embedding plot contains cluster of the same color (the points are normally superposed, here we used a small jitter for visualization purpose), with respective mark on top of them, meaning that the distances between the embedding vectors and the data frames are correctly reduced, as expected. \nWe have investigated further the inner representation by looking at the bigram matrices of the groundtruth alignments and the empirical alignments (computed by the encoding indices choice). \nBigram matrix of the groundtruth alignments with the diagonal: \nBigram matrix of the groundtruth alignments without the diagonal: \nBigram matrix of the empirical alignments with the diagonal: \nBigram matrix of the empirical alignments without the diagonal: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch implementation of VQ-VAE + WaveNet by [Chorowski et al., 2019] and VQ-VAE on speech signals by [van den Oord et al., 2017]",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/swasun/VQ-VAE-Speech/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 41,
      "date": "Tue, 28 Dec 2021 19:04:53 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/swasun/VQ-VAE-Speech/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "swasun/VQ-VAE-Speech",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "It requires python3, python3-pip, the packages listed in [requirements.txt](requirements.txt) and a recent version of git that supports [git-lfs](https://git-lfs.github.com/).\n\nTo install the required packages:\n```bash\npip3 install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8045001289534786
      ],
      "excerpt": "For now instead of a Wavenet it's a deconvolutional NN to speed up my tests; \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/swasun/VQ-VAE-Speech/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Charly Lamothe\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Overview",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "VQ-VAE-Speech",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "swasun",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/swasun/VQ-VAE-Speech/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 210,
      "date": "Tue, 28 Dec 2021 19:04:53 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "vq-vae",
      "vq-vae-wavenet",
      "wavenet",
      "speech",
      "speech-processing",
      "pytorch"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "First, move to the source directory:\n```bash\ncd src\n```\n\n```bash\npython3 main.py --help\n```\n\nOutput:\n```\nusage: main.py [-h] [--summary [SUMMARY]] [--export_to_features]\n               [--compute_dataset_stats]\n               [--experiments_configuration_path [EXPERIMENTS_CONFIGURATION_PATH]]\n               [--experiments_path [EXPERIMENTS_PATH]]\n               [--plot_experiments_losses] [--evaluate]\n               [--plot_comparaison_plot] [--plot_quantized_embedding_spaces]\n               [--compute_quantized_embedding_spaces_animation]\n               [--plot_distances_histogram] [--compute_many_to_one_mapping]\n               [--compute_alignments] [--compute_clustering_metrics]\n               [--compute_groundtruth_average_phonemes_number]\n               [--plot_clustering_metrics_evolution]\n               [--check_clustering_metrics_stability_over_seeds]\n               [--plot_gradient_stats]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --summary [SUMMARY]   The summary of the model based of a specified\n                        configuration file (default: None)\n  --export_to_features  Export the VCTK dataset files to features (default:\n                        False)\n  --compute_dataset_stats\n                        Compute the mean and the std of the VCTK dataset\n                        (default: False)\n  --experiments_configuration_path [EXPERIMENTS_CONFIGURATION_PATH]\n                        The path of the experiments configuration file\n                        (default:\n                        ../configurations/experiments_vq44-mfcc39.json)\n  --experiments_path [EXPERIMENTS_PATH]\n                        The path of the experiments ouput directory (default:\n                        ../experiments)\n  --plot_experiments_losses\n                        Plot the losses of the experiments based of the\n                        specified file in --experiments_configuration_path\n                        option (default: False)\n  --evaluate            Evaluate the model (default: False)\n  --plot_comparaison_plot\n                        Compute a comparaison plot for a single sample\n                        (default: False)\n  --plot_quantized_embedding_spaces\n                        Compute a 2D projection of the VQ codebook for a\n                        single sample (default: False)\n  --compute_quantized_embedding_spaces_animation\n                        Compute a 2D projection of the VQ codebook over\n                        training iterations (default: False)\n  --plot_distances_histogram\n                        Compute histograms of several distances to\n                        investiguate how close are the samples with the\n                        codebook (default: False)\n  --compute_many_to_one_mapping\n                        Compute the many to one mapping for all the samples\n                        (default: False)\n  --compute_alignments  Compute the groundtruth alignments and those of the\n                        specified experiments (default: False)\n  --compute_clustering_metrics\n                        Compute the clustering metrics between the groundtruth\n                        and the empirical alignments (default: False)\n  --compute_groundtruth_average_phonemes_number\n                        Compute the average number of phonemes per groundtruth\n                        alignment (default: False)\n  --plot_clustering_metrics_evolution\n                        Compute the evolution of the clustering metrics\n                        accross different number of embedding vectors\n                        (default: False)\n  --check_clustering_metrics_stability_over_seeds\n                        Check the evolution of the clustering metrics\n                        statbility over different seed values (default: False)\n  --plot_gradient_stats\n                        Plot the gradient stats of the training (default:\n                        False)\n```\n\nFirst, we need to download the dataset (only VCTK is supported for now) and compute the MFCC features:\n```bash\npython3 main.py --export_to_features\n```\n\nThe results are way better if the data are normalized. This can be done by computing the dataset stats with:\n```bash\npython3 main.py --compute_dataset_stats\n```\nand by setting `\"normalize\"` to true in the next part.\n\nThen, we have to create an experiments file (e.g., `../configurations/experiments_example.json`).\nExample of experiment file:\n```json\n{\n    \"experiments_path\": \"../experiments\",\n    \"results_path\": \"../results\",\n    \"configuration_path\": \"../configurations/vctk_features.yaml\",\n    \"seed\": 1234,\n    \"experiments\": {    \n        \"baseline\": {\n            \"num_epochs\": 15,\n            \"batch_size\": 2,\n            \"num_embeddings\": 44,\n            \"use_device\": \"cuda:1\",\n            \"normalize\": true\n        }\n    }\n}\n```\nThe parameters in the experiment will override the corresponding parameters from `vctk_features.yaml`. Other parameters can be add, such as `\"use_jitter\": true`, `\"jitter_probability\": 0.12` to enable the use of VQ jitter layer.\n\nThus, we can run the experiment(s) specified in the previous file:\n```bash\npython3 main.py --experiments_configuration_path ../configurations/experiments_example.json\n```\n\nEventually, we can plot the training evolution:\n```bash\npython3 main.py --experiments_configuration_path ../configurations/experiments_example.json --experiments_path ../experiments --plot_experiments_losses\n```\n\nAlso, we can evaluate our trained model with several ways, by using the main argument `--evaluate` followed with multiple sub evaluation arguments.\nFor example:\n```bash\npython3 main.py --experiments_configuration_path ../configurations/experiments_example.json --experiments_path ../experiments --evaluate --plot_comparaison_plot --plot_quantized_embedding_spaces --plot_distances_histogram --compute_alignments --compute_clustering_metrics\n```\n\nNote that `--plot_gradient_stats` argument will only work if `\"record_gradient_stats\": true` was added in the json exeperiment configuration file. Furthermore, `--plot_clustering_metrics_evolution` argument will only work for experiment [codebook_sizes](configuration/experiments_mfcc39-codebook_sizes.json) and `--check_clustering_metrics_stability_over_seeds` argument will only work for experiment [seeds](configuration/experiments_vq44-mfcc39-seeds.json).\nFor more examples, see the (configurations)[configurations] folder.\n\n",
      "technique": "Header extraction"
    }
  ]
}