{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.1556",
      "https://arxiv.org/abs/1409.1556.</br>\n[3] Chang et al., 2017, Active Bias: Training more accurate neural networks by emphasizing high variance samples. In NIPS.</br>\n[4] Han et al., 2018, Co-teaching: Robust training of deep neural networks with extremely noisy labels. In NIPS.</br>\n\n## 9. Contact\nHwanjun Song (songhwanjun@kaist.ac.kr); Minseok Kim (minseokkim@kaist.ac.kr); Jae-gil Lee (jaegil@kaist.ac.kr)"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] Huang et al., 2017, Densely connected convolutional networks. In CVPR.</br>\n[2] Simonyan et al., 2014, Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556.</br>\n[3] Chang et al., 2017, Active Bias: Training more accurate neural networks by emphasizing high variance samples. In NIPS.</br>\n[4] Han et al., 2018, Co-teaching: Robust training of deep neural networks with extremely noisy labels. In NIPS.</br>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.9999981719440794
      ],
      "excerpt": "Publication </br> \nSong, H., Kim, M., and Lee, J., \"SELFIE: Refurbishing Unclean Samples for Robust Deep Learning,\" In Proc. 2019 Int'l Conf. on Machine Learning (ICML), Long Beach, California, June 2019. [link] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9931434156627689
      ],
      "excerpt": "| CIFAR-10 (clean)       | 50,000            | 10,000            | 10        |    32x32   | link | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8832644726055581
      ],
      "excerpt": "| Tiny-ImageNet (clean) | 100,000           | 10,000            | 200       |    64x64   | link | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kaist-dmlab/SELFIE",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Hwanjun Song (songhwanjun@kaist.ac.kr); Minseok Kim (minseokkim@kaist.ac.kr); Jae-gil Lee (jaegil@kaist.ac.kr)\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-04T13:54:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-08T08:35:07Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8453335768352701,
        0.9954128954810247,
        0.9978564019877204
      ],
      "excerpt": "Song, H., Kim, M., and Lee, J., \"SELFIE: Refurbishing Unclean Samples for Robust Deep Learning,\" In Proc. 2019 Int'l Conf. on Machine Learning (ICML), Long Beach, California, June 2019. [link] \nOfficial tensorflow implementation of SELFIE. Specifically, in this implementation, we tested the performance of SELFIE using two popular convolutional neural networks, DenseNet [1] and VGGNet [2], on not only three simulated noisy datasets but also a real-world dataset. Active Bias [3] and Co-teaching [4], which are the two state-of-the-art robust training methods, were compared with SELFIE. \nOwing to the extremely high expressive power of deep neural networks, their side effect is to totally memorize training data even when the labels are extremely noisy. To overcome overfitting on the noisy labels, we propose a novel robust training method, which we call SELFIE, that trains the network on precisely calibrated samples together with clean samples. As in below Figure, it selectively corrects the losses of the training samples classified as refurbishable and combines them with the losses of clean samples to propagate backward. Taking advantage of this design, SELFIE effectively prevents the risk of noise accumulation from the false correction and fully exploits the training data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9224650061410433
      ],
      "excerpt": "SELFIE requires only a simple modification in the gradient descent step. As described below, the conventional update equation is replaced with the proposed one. If you are interested in further details, read our paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9953079525396271,
        0.8532268006581641
      ],
      "excerpt": "We compared SELFIE with default and two state-of-the-art robust training methods. We also provide the links of official/unofficial implementations for each method (The three algorithms are included in our implementation). \n- Default: Training method without any processing for noisy label. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8803476166982775,
        0.9652989067872917
      ],
      "excerpt": "- Co-teaching [4]: official (Pytorch) and unofficial (Tensorflow) \nWe evaluated the performance of SELIFE on four benchmark datasets. Here, ANIMAL-10N data set is our proprietary real-world noisy dataset of human-labled online images for 10 confusing animals. Please note that, in ANIMAL-10N, noisy labels were injected naturally by human mistakes, where its noise rate was estimated at 8%. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9371490345713617
      ],
      "excerpt": "For ease of experimentation, we provide download links for all datasets converted to the binary version.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8730178726031813
      ],
      "excerpt": "Each of these files is formatted as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9749440386756348
      ],
      "excerpt": "The reading procedure is similar to that of a popular CIFAR-10 tutorial. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9324819344939415
      ],
      "excerpt": "Except ANIMAL-10N dataset, since all datasets are clean, we artifically corrupted CIFAR-10, CIFAR-100 and Tiny-ImageNet datasets using two typical methods such that the true label i is flipped into the corrupted label j: i) Pair Noise and ii) Symmetry Noise. Below figures show the example of the noise transition matrix for each type. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9648234989365994
      ],
      "excerpt": "As for real-world noisy ANIMAL-10N dataset, the noise rate of training data is found at 8% by the corss-validation with grid search (See Appendix B). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9935429637692954,
        0.9811659754424515,
        0.9841446729229889
      ],
      "excerpt": "In our paper, for the evaluation, we used a momentum of 0.9, a batch size of 128, a dropout of 0.2, and batch normalization. For training schedule, we trained the network for 100 epochs and used an initial learning rate of 0.1, which was divided by 5 at 50% and 75% of the toral number of epochs.  \nAs for the algorithm hyperparameters, we fixed restart to 2 and used the best uncertainty threshold epsilon = 0.05, history length q = 15, which were obtained using the grid search (See Section 4.5 in our paper). \nWe trained DenseNet (L=25, k=12) and VGG-19 on the four benchmark datasets. The detailed anaysis on the evalutaion is discussed in our paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908827753523619
      ],
      "excerpt": "The noise rate of ANIMAL-10N is estimated at 8%. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kaist-dmlab/SELFIE/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Fri, 24 Dec 2021 17:59:15 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kaist-dmlab/SELFIE/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kaist-dmlab/SELFIE",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8019704149102619
      ],
      "excerpt": "For ease of experimentation, we provide download links for all datasets converted to the binary version.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365,
        0.9950268449552272,
        0.999746712887969
      ],
      "excerpt": "Python 3.6.4 \nTensorflow-gpu 1.8.0 (pip install tensorflow-gpu==1.8.0) \nTensorpack (pip install tensorpack) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8881996780636957
      ],
      "excerpt": "<img src=\"figures/key_idea.png \" width=\"400\">  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8741542412426502
      ],
      "excerpt": "<img src=\"figures/update_equation.png \" width=\"850\">  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8255157306916908
      ],
      "excerpt": "| Name (clean or noisy)    | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8699921201922342
      ],
      "excerpt": "RECORD_BYTES = ID_BYTES + LABEL_BYTES + width * height * depth \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8810441084645941,
        0.8955919760292188,
        0.8123763140827432,
        0.8123763140827432,
        0.8123763140827432,
        0.89819294525903
      ],
      "excerpt": "file_name, value = reader.read(filename_queue) \nbyte_record = tf.decode_raw(value, tf.uint8) \nimage_id = tf.strided_slice(byte_record, [0], [ID_BYTES]) \nimage_label = tf.strided_slice(byte_record, [ID_BYTES], [ID_BYTES + LABEL_BYTES]) \narray_image = tf.strided_slice(byte_record, [ID_BYTES + LABEL_BYTES], [RECORD_BYTES]) \ndepth_major_image = tf.reshape(array_image, [depth, height, width]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8741542412426502
      ],
      "excerpt": "<img src=\"figures/noise_type.png \" width=\"550\">  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8741542412426502
      ],
      "excerpt": "<img src=\"figures/synthetic_evaluation.png \" width=\"670\">  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8741542412426502
      ],
      "excerpt": "<img src=\"figures/realistic_evaluation.png \" width=\"420\">  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kaist-dmlab/SELFIE/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Hwanjun Song\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SELFIE: Refurbishing Unclean Samples for Robust Deep Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SELFIE",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kaist-dmlab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kaist-dmlab/SELFIE/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Dataset download:\n   ```\n   Download our datasets (binary format) and place them into *SELFIE/dataset/xxxxx*.\n   (e.g., SELFIE/dataset/CIFAR-10)\n   ```\n- Algorithm parameters\n   ```\n    -gpu_id: gpu number which you want to use (only support single gpu).\n    -data: dataset in {CIFAR-10, CIFAR-100, Tiny-ImageNet, ANIMAL-10N}.\n    -model_name: model in {VGG-19, DenseNet-10-12, DenseNet-25-12, DenseNet-40-12}.\n    -method_name: method in {Default, ActiveBias, Coteaching, SELFIE}.\n    -noise_type: synthetic noise type in {pair, symmetry, none}, none: do not inject synthetic noise.\n    -noise_rate: the rate which you want to corrupt (for CIFAR-10/100, Tiny-ImageNet) or the true noise rate of dataset (for ANIMAL-10N).\n    -log_dir: log directory to save the training/test error.\n   ```\n   \n- Algorithm configuration\n\n   Data augmentation and distortion are not applied, and training paramters are set to:\n   ```\n   Training epochs: 100\n   Batch size: 128\n   Learning rate: 0.1 (divided 5 at the approximately 50% and approximately 75% of the total number of epochs)\n   ```\n\n- Running commend\n   ```python\n   python main.py gpu_id data model_name method_name noise_type noise_rate log_dir\n   \n   #: e.g. 1., train DenseNet (L=25, k=12) on CIFAR-100 with pair noise of 40%.\n   #: python main.py 0 CIFAR-100 DenseNet-25-12 SELFIE pair 0.4 log/CIFAR-100/SELFIE\n   \n   #: e.g. 2., train DenseNet (L=25, k=12) on ANIMAL-10N with real-world noise of 8%\n   #: python main.py 0 ANIMAL-10N DenseNet-25-12 SELFIE none 0.08 log/ANIMAL-10N/SELFIE\n   ```\n\n- Detail of log file\n   ```\n   log.csv: generally, it saves training loss/error and test loss/error.\n    - format : epoch, training loss, training error, test loss, test error\n   However, Coteaching uses two network, so format is slightly different.\n    - format : epoch, training loss (network1), training error (notwork1), training loss (network2), training error (network2), test loss (notwork1), test error (network1), test loss (network2), test error (network2)\n   ```\n   \n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 40,
      "date": "Fri, 24 Dec 2021 17:59:15 GMT"
    },
    "technique": "GitHub API"
  }
}