{
  "citation": [
    {
      "confidence": [
        0.8990000916808355
      ],
      "excerpt": "Authors: Tianlin Liu and Friedemann Zenke. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222383658450612,
        0.9907077934707257,
        0.9982476551833407,
        0.9999931082405971,
        0.9664456561658856
      ],
      "excerpt": "  @article{Liu2020finding, \n    Title = {Finding trainable sparse networks through Neural Tangent Transfer}, \n    Author = {Tianlin Liu and Friedemann Zenke}, \n    journal={Proceedings of the 37th International Conference on Machine Learning (ICML)}, \n    Year = {2020}, \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fmi-basel/neural-tangent-transfer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-14T13:50:01Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-14T11:14:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "How to initialize a sparse deep neural network without compromising its performance? Our paper tackles this issue by instantiating sparse neural networks whose training dynamics in function space are as close as possible to the training dynamics of a dense one.\n\n![Schematic illustration of neural networks' output evolution during supervised training from time t = 0 (starting point) to t = T (endpoint) ](schematic.png)\n\nWe achieve this by minimizing the mismatch between the neural tangent kernels of the sparse and a dense teacher network, a method we refer to as *Neural Tangent Transfer* (NTT). NTT has two key advantages: (i) it only requires label-free data, and (ii) it can be used to find trainable layerwise sparse networks, e.g., CNNs with sparse convolutional filters, which are desirable for energy-efficient inference.\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.908925214220865,
        0.9731977657003618
      ],
      "excerpt": "Authors: Tianlin Liu and Friedemann Zenke. \nThis repository contains code to reproduce the experiments of our \"Finding trainable sparse networks through Neural Tangent Transfer\" published at ICML 2020. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code accompanying our paper \"Finding trainable sparse networks through Neural Tangent Transfer\" to be published at ICML-2020.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fmi-basel/neural-tangent-transfer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Fri, 24 Dec 2021 18:04:10 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/fmi-basel/neural-tangent-transfer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "fmi-basel/neural-tangent-transfer",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/fmi-basel/neural-tangent-transfer/master/notebooks/ntt_demo_mlp.ipynb",
      "https://raw.githubusercontent.com/fmi-basel/neural-tangent-transfer/master/notebooks/ntt_demo_cnn.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Code was tested on\n\n* Python 3.7\n* [jaxlib](https://github.com/google/jax) 0.1.37 \n* [jax](https://github.com/google/jax) 0.1.55\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/fmi-basel/neural-tangent-transfer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Finding trainable sparse networks through Neural Tangent Transfer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "neural-tangent-transfer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "fmi-basel",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fmi-basel/neural-tangent-transfer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 13,
      "date": "Fri, 24 Dec 2021 18:04:10 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We have prepared Jupyter notebooks in the ./notebooks/ folder to demonstrate NTT.\n\nThere are two main python scripts in the folder `nt_transfer`: `ntt.py` and `exploit.py`. The first `ntt.py` script takes care of the **NTT optimization phase**, where we use label-free data to learn the sparse neural network initialization; the second `exploit.py` takes care the **supervised learning phase**, where we perform supervised learning using labeled data.",
      "technique": "Header extraction"
    }
  ]
}