{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1509.02971\n- OPENAI Baselines post: https://blog.openai.com/better-exploration-with-parameter-noise/\n\nTasks:\n1. Change DDPG to Mountain car (May tune a bit the hyperparameters as constant time systems are different"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8130052599129608
      ],
      "excerpt": "Original paper: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.994799811898885
      ],
      "excerpt": "Original paper: https://arxiv.org/abs/1509.02971 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bitterbloom/Reinforcement-Learning",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-12T03:48:35Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-06T02:37:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8614783083816533,
        0.947281680350354,
        0.8832004955871877
      ],
      "excerpt": "1. Play with the hyperparameters and show their corresponding graphs. Which parameter caused the most change? Which one didn\u2019t affect that much? Discuss briefly your results \n2. Anneal the \ud835\udfae hyperparameter to decay linearly instead of being fixed? Did it help at all? Why? \n3. Try two different architectures and report any results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8005601872867385,
        0.8691520507133432
      ],
      "excerpt": "1. Change DDPG to Mountain car (May tune a bit the hyperparameters as constant time systems are different). Compare with DQN ( \n(Optional) As you see reward/cost penalize control law/actions change it so it penalize more control energy used and plot u(t) for different initial positions of the pendulum. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Reinforcement learning homework IBIO4615",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bitterbloom/Reinforcement-Learning/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 30 Dec 2021 00:00:05 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bitterbloom/Reinforcement-Learning/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "bitterbloom/Reinforcement-Learning",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9322609392449874
      ],
      "excerpt": "- Pytorch 1.0.1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9450440092643635,
        0.999746712887969,
        0.999746712887969,
        0.9967777177014457
      ],
      "excerpt": "- gym, matplotlib, numpy, tensorboardx \npip install gym \npip install tensorboardx  \npip install tensorflow=1.2 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bitterbloom/Reinforcement-Learning/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Reinforcement-Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Reinforcement-Learning",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "bitterbloom",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bitterbloom/Reinforcement-Learning/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 30 Dec 2021 00:00:05 GMT"
    },
    "technique": "GitHub API"
  }
}