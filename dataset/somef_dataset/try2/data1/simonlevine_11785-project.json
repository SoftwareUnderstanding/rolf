{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1708.02002.\n\n\n# Running the code\n\n## Data\n\nMIMIC-III/CXR data requires credentialed access. This pipeline requires:\n\n- D_ICD_DIAGNOSES.csv.gz (if descriptions of ICDs are wanted"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9185831580018877,
        0.808684694647591
      ],
      "excerpt": "Final course project for Fall 2020's 11-785: Deep Learning, Carnegie Mellon University. \nSkeleton code from https://github.com/ricardorei/lightning-text-classification \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9749000716163233
      ],
      "excerpt": "You can check this paper https://arxiv.org/abs/1708.02002. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simonlevine/clinical-longformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-14T22:22:14Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-11T09:43:28Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9470904269000665
      ],
      "excerpt": "Final course project for Fall 2020's 11-785: Deep Learning, Carnegie Mellon University. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9790032271316041,
        0.9708390079108357
      ],
      "excerpt": "In this project we attempt to circumvent the heavy pre-processing, truncation, etc. of health records instances seen in recent work. \nFor a full, writeup of the project, please see https://github.com/simonlevine/clinical-longformer/blob/master/Written%20Submission.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9290976997756738,
        0.9947556221339647
      ],
      "excerpt": "See the X-Transformer-ICD submodule for preprocessing scripts/integration with X-Transformer using MIMIC-III. \nUsing novel pre-training of Transformer encoders, this project tackles whole-document embedding for the clinical domain.  Additionally, we propose a fine-tuning process on electronic healthcare records for transformer models and a novel medical coding benchmark task. We release our best-performing encoder model and suggest future investigation with regard to the natural language tasks in this domain. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9387642598032766
      ],
      "excerpt": "We achieve SOTA results on our ICD prediction task \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8956340257241558,
        0.9647243713259668
      ],
      "excerpt": "get x-transformer trained on our best model. \nThe x-transformer pipeline uses the state-of-the-art extreme multilabel classification with label clustering and PIFA-TFIDF features to amke good use of label descriptions. ICD code labels not only have 'ICD9_CODE' labels, but also 'LONG_TITLE' labels with rich semantic information. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801918133239432,
        0.8010452491076809
      ],
      "excerpt": "Or sigmoid + hamming loss with {0, 1} values in labels like (1, 0, 0, 1). \nIn some cases, sigmoid + focal loss with {0, 1} values in labels like (1, 0, 0, 1) worked well. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9864977525465864
      ],
      "excerpt": "We also provide a General Equivalence Mapping if translating codes to ICD10 is desired (these data are relevant but quite old, MIMIC-IV is in the works). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8933321840520864
      ],
      "excerpt": "  - cleans discharge summaries of administrative language, etc. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9935288256005206,
        0.881805037635257,
        0.8620071983015458
      ],
      "excerpt": "This project is the course project for Fall 2020, 11-785: Deep Learning, at Carnegie Mellon University. \nHere, we benchmark various Transformer-based encoders (BERT, RoBERTa, Longformer) on electronic health records data from the MIMIC-III data lake. \nWe also benchmark our new pipeline against another novel pipeline developed in 2020 by Simon Levine and Jeremy Fisher, auto-icd-Transformers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9952293906733628
      ],
      "excerpt": "This latter model is simply allenAI's biomed-roberta with global attention, such that documents of up to 4096 token length are able to be used without truncation, a critical aspect of free-text EHR data). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9764478177879041,
        0.9792360254846821,
        0.9200630643734115
      ],
      "excerpt": "Global attention was not used in MLM because this is costly. We had out of memory issues on a 200gb machine with FP16, gradient checkpointing, and batch size of 1. \nNote that A corpus even of this size is likely not sufficient to have dramatic increases in performance, per AllenAI, but it couldn't hurt to try. \nSo, global attention is used in the end-to-end stage. We recommend you repeat MLM with more data and global attention if you have the resources. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simonlevine/11785-project/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Tue, 21 Dec 2021 05:23:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/simonlevine/clinical-longformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "simonlevine/clinical-longformer",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/simonlevine/11785-project/master/mimicxr2mimicivICD%20%281%29.ipynb",
      "https://raw.githubusercontent.com/simonlevine/11785-project/master/testing.ipynb",
      "https://raw.githubusercontent.com/simonlevine/11785-project/master/medNLI/testing_for_med_nli.ipynb",
      "https://raw.githubusercontent.com/simonlevine/11785-project/master/classifier_pipeline_medNLI/mnli_testing.ipynb",
      "https://raw.githubusercontent.com/simonlevine/11785-project/master/longformer_gen/convert_model_to_long.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/simonlevine/11785-project/master/format_mimic_for_ICD_classifier.sh",
      "https://raw.githubusercontent.com/simonlevine/11785-project/master/split_data_and_run_mlm_training.sh",
      "https://raw.githubusercontent.com/simonlevine/11785-project/master/elongate_roberta.sh",
      "https://raw.githubusercontent.com/simonlevine/11785-project/master/run_icd_classifier.sh",
      "https://raw.githubusercontent.com/simonlevine/11785-project/master/do_all_icd_tasks.sh",
      "https://raw.githubusercontent.com/simonlevine/11785-project/master/medNLI/run_medMLI.sh"
    ],
    "technique": "File Exploration"
  },
  "identifier": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://doi.org/10.5281/zenodo.4396595",
      "technique": "Regular expression"
    }
  ],
  "installation": [
    {
      "confidence": [
        0.8790823982019109
      ],
      "excerpt": "Skeleton code from https://github.com/ricardorei/lightning-text-classification \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8669334133716544
      ],
      "excerpt": "- Then, pull the model asRobertaLongForMaskedLM.from_pretrained('simonlevine/bioclinical-roberta-long')- Now, it can be used as usual. Note you may get untrained weights warnings. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8579449912014439
      ],
      "excerpt": "Data should be in the main working directory, in the \"data\" folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "- format_notes.py  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9141030630598617
      ],
      "excerpt": "format_data_for_training.py , OR, format_data_for_for_multilabel.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/simonlevine/clinical-longformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Rich Text Format",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Clinical-Longformer: Whole Document Embedding and Classification for the Clinical Domain",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "clinical-longformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "simonlevine",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simonlevine/clinical-longformer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "simonlevine",
        "body": "initial release (MIMIC-III + CXR MLM pre-training / MLP classifier on 50 most frequent ICD-9-CM codes from MIMIC III)",
        "dateCreated": "2020-12-16T19:47:24Z",
        "datePublished": "2020-12-28T23:42:57Z",
        "html_url": "https://github.com/simonlevine/clinical-longformer/releases/tag/v0.1-alpha",
        "name": "",
        "tag_name": "v0.1-alpha",
        "tarball_url": "https://api.github.com/repos/simonlevine/clinical-longformer/tarball/v0.1-alpha",
        "url": "https://api.github.com/repos/simonlevine/clinical-longformer/releases/35788921",
        "zipball_url": "https://api.github.com/repos/simonlevine/clinical-longformer/zipball/v0.1-alpha"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\npandas\ntensorboard\ntorch\ntransformers\npytorch-lightning\npytorch-nlp\nloguru\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- download the data.\n  \n- run ICD classification task:\n  - *Here, use a transformer as an encoder with a simple linear classifier with tanh activation.*\n  - install requirements from requirements.txt in a Python (~3.8+) virtual environment.\n  - run *format_mimic_for_ICD_classifier.sh* to clean notes and ready a dataframe. Note that this step can be varied using params.yaml to pull in other code sets (such as ICD procedures) as desired!\n  - run *run_icd_classifier.sh*. Note that you may want to alter the hyperparameters used by the training script (BERT vs RoBERTA vs Long-Clinical-RoBERTa, etc.). Theis is done via command line flags (see bottom of classifier_one_label.py)\n  - By default, we load in the 50 most frequent primary ICD9 diagnosis codes in MIMIC-III discharge summaries.\n    - ['41401', '0389', '41071', 'V3001', '4241', '51881', 'V3000', 'V3101', '431', '4240', '5070', '4280', '41041', '41011', '5789', '430', '486', '1983', '99859', '5849', '5770', '43491', '5712', '99662', '4271', '03842', '99811', 'V3401', '42731', '56212', '4373', '43411', '4321', '41519', '51884', '85221', '570', '03811', '53140', '03849', '4412', '42823', '44101', '42833', '0380', '85220', '4210', '4414', '51919', '5715']\n  - By default, Simon + Serena's bioclinical-Longformer is benchmarked with global attention.\n    - You should increase batch sizes and you may not need gradient checkpointing if just running roberta or bert.\n  - By default, **encoder weights are frozen during training** since we are benchmarking pre-trained encoders as-is, not fine-tuning.\n    - The classifier head is still used.\n  \n\n- Run masked-language modeling (generate your own clinical encoder!):\n  - run *format_mimic_for_ICD_classifier.sh*\n    - This will concatenate all of MIMIC-III and MIMIC-CXR, sans those encounters used in test datasets for becnhmarking.\n    - This will filter notes for administrative language, some punctuation, etcetera.\n  - run *split_data_and_run_mlm_training.sh*\n    - We pre-train AllenAI/BiomedRobertaBase as a default, as pre-training with global attention results in severe memory issues (OOM on 200gb VM) and extreme training time requirements (as it is, 300 hours were used to train our 5000-epoch RobertaForMaskedLM model).\n    - It's also unclear, given that we \"chunk\" our entire 2.2 million -encounter corpus, what benefit this would bring.\n    - Ideally, someone shoud get a corpus an order of magnitude larger and train document-by-document longformer from scratch...\n  - run *elongate_roberta.sh* to pull in a huggingface model and elongate it with global attention.\n    - Defaults to 4096 global attention tokens and 512 local.\n    - This will convert a roberta model to a longformer, essentially, but allows for larger document ingestion.\n    - Note that nothing is stopping you from attempting MLM pre-training using this model rather than roberta.\n      - In fact, we provide *pretrain_roberta_long.py* for just that task if you have the resources.\n\n-Run MedNLI:\n  -in Progress\n\n\n- Extension: run X-transformer for MIMIC-III (https://github.com/simonlevine/auto-icd-transformers) using your new encoder:\n  - We provide a forked repository using X-Transformer allowing for training your encoder on every ICD code in MIMIC-III, proc or diag, an extreme multilabel classification problem.\n  - Some code overlap, such as note preprocessing.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Run the pytorch-lightning trainer from ~~either training_multilabel.py or~~ training_onelabel.py.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10,
      "date": "Tue, 21 Dec 2021 05:23:59 GMT"
    },
    "technique": "GitHub API"
  }
}