{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1512.05287](",
      "https://arxiv.org/abs/1706.03762\n]("
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9855105081843352
      ],
      "excerpt": "Inspired by https://arxiv.org/abs/1706.03762 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/magahub/songrnn",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-25T14:55:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-25T14:56:06Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9609755766896099
      ],
      "excerpt": "Implement Chinese keyword spotting using RNN+CTC. This model is supposed to run on android phones or smaller devices, with low cpu and memory requirement. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254086637474882,
        0.9436469001719103,
        0.9578240788287785
      ],
      "excerpt": "in my experiment, we use fft_size=25ms and hop_size=10ms for stft, n_mel=40 for mel filter bank with RNN hidden_size=128  is enough. \nn_mel=60 almost the same performance with n_mel=40. (My guess is that input feature size and RNN hidden size should match, and hidden_size=128 is too small to model 60 feature from mel spectrogram. \nMaybe larger hidden size and deeper network can perform better. But in our case, there is no need to use that large model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8834951408626796,
        0.9466308812083878,
        0.9880783054850427,
        0.983577955941858
      ],
      "excerpt": "using CTC, the label is just text. \nWe use pinyin to represent words(using the marker), because some Chinese words have multiple phoneme, for example, one of our keyword \u4e50 has two pronounce: yue4 and le4, but we only want le4. \nOur label space consist of keywords and garbage word(all other words except keyword) \nWe insert space between words in order to force the model to learn stronger ability to cut the words. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9755089908049701,
        0.9394449182630016,
        0.9394449182630016,
        0.9394449182630016,
        0.9394449182630016,
        0.9394449182630016,
        0.9324686998172853
      ],
      "excerpt": "0 for space \n1 for ni3 \n2 for hao3 \n3 for le3 \n4 for garbage word \n5 for ctc blank \nAnd therefore we have a output space of 6. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9691635214207714
      ],
      "excerpt": "Actually we've tried frame-wise label with alignment for word(phoneme), but there is some difficulty and the outcome is not desirable. I will dicuss this in the loss function, later. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8348248811598148
      ],
      "excerpt": "The model consist of 2 layer GRU RNN(http://arxiv.org/abs/1406.1078),with hidden_size=128 ,no projection (have tried LSTM, no improvement, thus choosing GRU for less computation cost). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.89907478351613
      ],
      "excerpt": "Fully-connection layer map the 128 hidden size to output space of num_classes = 6 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9982064917228719,
        0.860059181823877,
        0.9692681217357941,
        0.9319093084039346,
        0.959961309707981,
        0.9074758702796846
      ],
      "excerpt": "I've tried cross-entropy-like loss function, and thus the data need to be labeled to frame-wise, i.e., the start and end of each word. The alignment and segment is done by our speech model, which, however, can only label the peak frame of each word instead of the boundry of the word (phoneme). And we can only give a rough approximation of the word boundry. We haven't found a perfect algorithm to do the alignment, thus the data quality is limited the to accuracy of the alignment. What's worse, it takes long time to do the alignment each time we want to add new data. With the cross-entropy loss, the model can only reach accurcary of about 85%. \nDeployment Model \nIn the deployment model, we just replace the CTCloss with CTCdecoder to process the rnn_output (softmax before CTCdecode) to get the output sequence. \nActually, I write a simple decode function to replace the ctc_beam_search_decode, because our case is very simple, so there is no need to use ctc decode. More importantly, by doing this, we enable streaming on decode stage. \nStreaming for Deployment Model \nIn the real production, we must enable streaming process, which mean we process the input audio in a almost real-time base, so as to reduce the latency. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9898935445137805,
        0.9325265247273341
      ],
      "excerpt": "The key of streaming is to keep the rnn state. \nFor example, we set a 300ms window size. Each time we feed 300ms audio into model, as well as the rnn state in the previous 300ms. And we fetch the softmax prob sequence as well as the rnn state. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9852697629961849,
        0.9920983113323396,
        0.8307444667213791,
        0.9647387157488228,
        0.9765150352444955,
        0.9387318606634193
      ],
      "excerpt": "A major drawback of this RNN model is that it will toally mess up when carrying the state of a long speech. (The reason, I guess, is that our training data are mostly short speech segment.) So we clear the rnn state after each trigger and each non-speech segment detected by the VAD. The VAD must be carefully tuned, otherwise it will cut off unfinished speech and clean the rnn state. \nThe data is about 80GB (linear spectrogram), which is too large to load into memory. So I save the data in tfrecords and feed into training model in streaming. \nI maintain two queues to manage the pipeline, filename queue and data queue. This part is tricky so be careful if you want to hack this part fo code, otherwise the pipeline might be stuck. Or if can also use tf's build-in input queue.(My queue is similar to tf's own queue, but add some customized features) \nYou can choose whatever data you want to save in tfrecords, raw wave or linear spectrogram or mel spectrogram. With respect to data volume, linear spec>raw wave>mel spec According to my experience, the computation of preprocessing is insignificant. \nMy advice is to save raw audio or linear spectrogram in tfrecords, because it's much easier to do some data enhancement (for example, adding noise, or other trick as you like) on the fly with linear domain, once it becomes mel spectrogram, things get much more complicated. \nOne more thing I would like to mention: the usage of tfrecord is also tricky, and the document seems missing from tf office website. So also be careful if you want to modify this part. A good practise can be found here: http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.961880204938328
      ],
      "excerpt": "In the customize branch, I implement a new feature which enable customized keyword by recording only 3 speech utterances. The new model is trained on original pre-trained model, with few data and fast training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9684632423553522,
        0.9073789698590183,
        0.9585160953977679,
        0.8598339675647749
      ],
      "excerpt": "The rnn part only learn to extract the speech features, so basically it has little to do with the output space projection. \nTherefore, we only want to modify the fully-connection matrix weights, and freezing all other variables. \nFor our expeirment keyword \u4f60\u597d\u4e50\u4e50, we have a [128,6] weights matrix, where hidden_size = 128 and num_classes = 6. To add new customized keyword, for example, \u82f9\u679c, we add a new [128,2] weight matrix and concat them. \n[128,4] original weights, for (space,ni3,hao3,le4) trainable=False \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.963028022029921,
        0.9281336394939227,
        0.9600785719610874
      ],
      "excerpt": "Theoritically, the new keyword is included in original garbage words, so if we want to modify the origin garbage words weights to add new mappings, we have to train the garbage words weights as well. However, the problem is that we want to train the new with only a few data, and the garbage words weight will totally mess up due to lack of adequate negtive data. \nThe ideal way is to train the garbage words weight but with as less change as possible. But I haven't figure out a way to do this, so I just freeze this weights and train the new weigihts with a few positive samples. The accurcay is acceptable. \nAnother problem is that logits scale of RNN outputs is not comparable between old weights and new weights. When doing softmax, this might cause problem, for example, the original keyword ni3 will be recognized as ping2. Still, I haven't figure out a way to fix this. The strategy I use now is to keep two fully-connection matrix, i.e., [128,6] and [128,8], and do softmax and decode respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9314572766327468,
        0.9590313849632973
      ],
      "excerpt": ", I've tried to use self-Attention to replace RNN in the model structure, with other parts unchange. \nA significant advantage of attention is fast training, thanks to parallel computation. The accurcay is almost the same as GRU RNN(slightly better). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8868845803163208
      ],
      "excerpt": "Some notes for attention: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8098850899791775
      ],
      "excerpt": "Given that keywords is short speech utterance and we process windowed streaming input, this might work in read production, potentially. Still need further experiment to verify this. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/magahub/songrnn/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 10:00:41 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/magahub/songrnn/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "magahub/songrnn",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8385665576033822
      ],
      "excerpt": "training data: 230,000 speech wav, with text label, about 100 hrs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216270093103228
      ],
      "excerpt": "For example, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8483819940059046
      ],
      "excerpt": "Training model \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/magahub/songrnn/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "keyword_spotting",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "songrnn",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "magahub",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/magahub/songrnn/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 10:00:41 GMT"
    },
    "technique": "GitHub API"
  }
}