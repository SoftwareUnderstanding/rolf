{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805\n\nWe will be using a pytorch based version as compiled by the awesome NLP researches from *hugging face*, which allows for easy model creation due to it's modularity as well as additionally giving us access to other NLP models if we would like to explore in the future.\n```\npip install pytorch-transformers\n```\n\nAs BERT was trained on the Wikipedia corpus, this may lead to inaccuracies for our task as the sentence structures that BERT was trained on defers from our more informal tweets. As such, we will be using transfer learning by fine-tuning BERT using the Sentiment140 dataset, which consists of 1.6 million evenly labels english tweets(postive/negative"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.959546310417776
      ],
      "excerpt": "4) Named-Entity Recognition  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9927059530876009
      ],
      "excerpt": "Research paper: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/danlim-wz/analysis_of_social_media_data_about_singapore",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-13T10:25:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-29T12:48:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9797285256638889,
        0.814223458307408
      ],
      "excerpt": "Singapore is a multi-racial and multi-cultural society with 4 main languages. This repository documents the journey to derive insights from textual social media data, from Twitter and Instagram, about Singapore with the following potential areas of exploration: \n1) Proportion of languages used \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8857087530286742
      ],
      "excerpt": "3) Sentiment Analysis on trending topics as well as internal vs external data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9659735392125489
      ],
      "excerpt": "As no datasets on social media data about Singapore are publicly available, data will be mined from the aforementioned social media platforms using AWS before being used for analysis. As such, this project will be segmented into 4 parts: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9669976005237984
      ],
      "excerpt": "2) Data cleaning & labelling using NLTK and spaCy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9364805038302909,
        0.9643974226421351,
        0.8381274101165207,
        0.931772871893737,
        0.966100054379479,
        0.891222434497508,
        0.8864553075437753
      ],
      "excerpt": "4) Extraction and visualization of results \nThe following graph showcases the overall approach for this project: \nTo collect data from Twitter and Instagram, we first need to create an EC2 instance on AWS to run the mining scripts indefinitely. \nAs the scripts are generally lightweight, we do not need heavy computing power. We will hence be using the the basic Amazon Linux instance which will be enough to fulfil our requirements. Although the instance is in the free-tier, please note there are hidden charges after a certain usage duration as well as for data transfer/storage options. Check out the pricing at this link: https://aws.amazon.com/ec2/pricing/ \nNext, head to AWS RDS to create a database where we will store the collected text data. PostgreSQL will be used in this guide. Note that there are also hidden charges and it is important to first check out their pricing plans before creation of any instances. \nIt will take a while for AWS to provision your resources. Once it is up and running, go into your database instance to copy its endpoint- we would need this to post data to our database from our ec2 instance. If you run into authentication problems, head to IAM and create a role that allows your AWS RDS instance to interface with your AWS EC2 instance: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html \nWe are now ready to run our mining scripts on our ec2 instance(almost)! Access to all tweets must first be authenticated by a user account's key and token. Follow this guide provided by Twitter to generate your access keys and tokens, which would be needed for our Twitter mining scipt: https://developer.twitter.com/en/docs/basics/authentication/oauth-1-0a/obtaining-user-access-tokens. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9771626247365901,
        0.9120785458531862
      ],
      "excerpt": "For Instagram, we will be mining the captions of posts related to Singapore. Scrapping data via the HTML way will allow us to mine more data as well as avoid having to use the Instagram API. However, they are very good at detecting mining bots and may excercise rate-limiting based on the IP address of the ec2 instance. \nThe Instaloader wrapper is an awesome, well documented library that like Tweepy, modularizes mining functions, making it really simple for us to mine data from instagram. Do check them out: https://github.com/instaloader/instaloader \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8993785911123005
      ],
      "excerpt": "Lastly, download the twitter_crawler.py and insta_crawler.py file from this repository and transfer them to your ec2 instance(using transfer protocols such as scp or putty). Do not forget to change the credentials of the database endpoint and Twitter access tokens. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9848628280713606,
        0.9869110316211372,
        0.9581129524816292,
        0.8218354723409177,
        0.9411818549494662,
        0.8917685586861968,
        0.8187186269902921,
        0.8565724067323461,
        0.8288286008647975,
        0.9388552495630158
      ],
      "excerpt": "Due to rate limiting, it may take some time to mine a sizable amount of text data. It took around 2 weeks to mine a million tweets. Additionally, a column indicating if the tweet is a retweet or not is saved. As duplicates are allowed, saving the retweet label as a binary count was chosen instead of the actual count. This is because the actual count may lead to unproportional emphasis on the retweet if numbers reflected are high. \nData cleaning is probablity the most important part of the whole process as the model only performs as well as the quality of the data you feed it. \nA brief glimpse into the dataset shows that it contains a lot of noise, we will hence clean it based on these conditions: \n1) Punctuation - These markers are mostly neutral and does not significantly influence any language tasks. \n2) Website links - Useless based on the objectives defined above. \n3) Mentioned usernames - Useless based on the objectives defined above. \n4) Emoticons - This is a special case: it is not useful when doing tasks like language recognition but may be useful in sentiment analysis. \n5) Stop words - Holds little meaning as it serves to structure the sentence. However, it depends on the model used: if the model contextualizes the sentence, it can prove to be useful. \nThe first three conditions will be done in all cases but conditions 4 and 5 may or may not be done based on the nature of the employed model. \nThe first three conditions can be easily done in python with an array and iteration through each tweet on the character level(refer to filter_tweets.py). Although emoticons can be removed the same way, new emoticons are being created everyday. It is best to use a library that constantly updates their list of reference emoticons to ensure high quality filtering of emoticons: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9914380010425541
      ],
      "excerpt": "We begin our analysis by determining the spread of languages used. As most models are only compatible with english, this can help us understand what kind of data we will need in order to build a multilingual model. To first build a language recognition model, we can train it in an unsupervised manner by concatenating different datasets of different languages and train it with labels given based on it's language. However, this will prove to be an arduous task as massive datasets would be needed to account for the languages supported and a large corresponding vocabulary. Fortunately, the research team at Facebook have open sourced their language detection model: fastText. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9730024257110842,
        0.9804804020153406,
        0.8894874565735715,
        0.9897744347995904,
        0.957464159949784
      ],
      "excerpt": "Subsequently, we use a hash table to count the occurrences of detected languages. We then sort the hash table and visualize the top 20 languages tweeted based on our mined dataset using matplot. Refer to language_visualization.py for the script used. The obtained bar chart is displayed below: \nFrom our results, it is surprising to see Japanese as the second most used language. \nSubsequently, we can use the earlier results to extract the english tweets related to Singapore. All english tweets which reflect a confidence of more than 0.5 will be used. \nThe model we will be using is BERT, which was developed by the Google AI research team. This is the current state-of-the-art model due to it's novel architecture that uses bi-directional LSTMs, allowing for contextualization of sentences in both directions as compared to it's predecessors, exceeding prior benchmarks in a series of NLP tasks. Read the paper here: https://arxiv.org/abs/1810.04805 \nWe will be using a pytorch based version as compiled by the awesome NLP researches from hugging face, which allows for easy model creation due to it's modularity as well as additionally giving us access to other NLP models if we would like to explore in the future. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9774571960492108
      ],
      "excerpt": "As BERT was trained on the Wikipedia corpus, this may lead to inaccuracies for our task as the sentence structures that BERT was trained on defers from our more informal tweets. As such, we will be using transfer learning by fine-tuning BERT using the Sentiment140 dataset, which consists of 1.6 million evenly labels english tweets(postive/negative), that was used in research.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.888089390219778,
        0.8191805574949367,
        0.9965508738231632,
        0.9896461171161668
      ],
      "excerpt": "Research paper: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12 \nAs the dataset has already been cleaned and labelled for us, we can start building/training the model straight away. Please refer to bert_training_script.py in this repository for the code used to train BERT. \nWhen working with very large datasets, it does not make sense to load the whole dataset into memory due to the lack/wastage of computing resources. It is thus good to understand the usage of the IterableDataset class used from Pytorch. This enables concurrent reading/pre-processing of data and training of the model, via the CPU and GPU working in tandem, ensuring that only a fraction of data is in memory at any given time, greatly saving computational resources. \nBert is a large model by itself which means that we probably will take a very long time with a mediocre GPU instance. As such, we will also be training BERT on multiple GPU instances on AWS SageMaker, which is a managed machine learning platform that makes it very easy for development/training and testing of AI models. The good thing about SageMaker it is very fast and easy to create jupyter notebooks to host your models and they only bill you for the exact time you take run the instance, unlike EC2 where there are hidden costs and it is harder to start/stop your resources. However, multi-GPU instances are very expensive and you should visit this link to be aware of the charges before proceeding: https://aws.amazon.com/sagemaker/pricing/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8558337121662609,
        0.9558733031147215
      ],
      "excerpt": "Head to SageMaker and create a new notebook instance. I created one in the North Virgina region as it was the cheapest. You can change your region in the top right corner: \nWe will be choosing the ml.p3.16xlarge instance which consists of 8xV100 GPUs, this will cut down training time from a day on a regular GPU to just 50 mins for our given dataset of 1.6 million samples. GPU types/quantities per instances are available at the same pricing link above. Please note that if you are saving the model straight into an AWS S3 bucket as per the approach map at the start of this repository, you would need give permissions to the S3 bucket when initializing the instance, or you can select and use a role which has access to S3.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8638819715630051,
        0.9664585525850112,
        0.8856820457079035,
        0.9334367561759115,
        0.9666733024435191
      ],
      "excerpt": "The code in the script has been edited to be compatible to train on mulitple GPUs using the DataParallel class from Pytorch. Head to this link if you wish to learn more about it: https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html \nAdjust the tunable parameters in the code to make the most use of the GPU resources. Using a hash table, we can determine the optimal length to use, in this case, we will be using a MAX_LEN of 32 as more than 95% of tweets are encompasses within 32 words: \nIt may also be helpful to query nvidia-smi on the terminal to achieve the highest possible batch size. If you are also using the ml.p3.16xlarge instance, a good batch size is 2624. This maximizes all GPUs almost evenly: \nAfter training for 4 epochs, we achieve an test accuracy of 84.8%. We then use this to predict the sentiment of the english tweets we mined, results are displayed in the chart below (0 means negative while 1 means positive): \nThe mean sentiment scores computed was 0.534, meaning the average sentiment of all english tweets were largely neutral but with a minute positive nature. Future updates will explore the validity of this score as well as topic-wise/multi-lingual sentiment analysis, to incorporate a more holistic approach towards drawing insights on tweets about Singapore. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Deep dive into the best end-to-end practices of text data analytics using AI. Implemented on AWS.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/danlim-wz/analysis_of_social_media_data_about_singapore/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 19:45:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/danlim-wz/analysis_of_social_media_data_about_singapore/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "danlim-wz/analysis_of_social_media_data_about_singapore",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8214449587956554,
        0.999746712887969
      ],
      "excerpt": "The Tweepy wrapper that modularizes the twitter API will be used to further simplify our mining script, install it in the ec2 instance using pip: \npip install tweepy  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999746712887969
      ],
      "excerpt": "pip install instaloader \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8954210667824842
      ],
      "excerpt": "Use the in-built terminal multiplexer to concurrently run both scripts: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.876432267914716
      ],
      "excerpt": "Ctrl+B D #:detaches from the terminal instance without terminating the script \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.813713412921092
      ],
      "excerpt": "2) Website links - Useless based on the objectives defined above. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999746712887969
      ],
      "excerpt": "pip install demoji \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9952263501819181,
        0.999746712887969
      ],
      "excerpt": "Install the python version of fastText: \npip install fasttext \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9995486579735738
      ],
      "excerpt": "pip install pytorch-transformers \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8763252429449695
      ],
      "excerpt": "--Repeat for insta_crawler.py-- \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/danlim-wz/analysis_of_social_media_data_about_singapore/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Analysis of Social Media Data about Singapore",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "analysis_of_social_media_data_about_singapore",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "danlim-wz",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/danlim-wz/analysis_of_social_media_data_about_singapore/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 19:45:33 GMT"
    },
    "technique": "GitHub API"
  }
}