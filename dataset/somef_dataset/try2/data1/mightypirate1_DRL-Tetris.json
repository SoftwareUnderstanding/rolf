{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1803.00933",
      "https://arxiv.org/abs/1509.06461 2: https://arxiv.org/abs/1511.06581",
      "https://arxiv.org/abs/1511.06581",
      "https://arxiv.org/abs/1506.02438]. This is the recommeded default for all new-comers.\n\nThe RL-framework and RL-algorithms may be separated into different repositories at some time in the future, but for now they are one.\n\n> NOTE: The master-branch contains some features that are experimental. If there are issues, revert to the stable branch (https://github.com/mightypirate1/DRL-Tetris/tree/stable"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mightypirate1/DRL-Tetris",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "yfflan at gmail dot com\n\n[SpeedBlocks]: <https://github.com/kroyee/SpeedBlocks>\n[tensorflow]: <https://www.tensorflow.org/install/>\n",
      "technique": "Header extraction"
    }
  ],
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Thank you for your interest in this project! Please refer to the following\nsections on how to contribute code and bug reports.\nReporting bugs\nAt the moment, this project is run in the spare time of a single person\n(Wenzel Jakob) with very limited resources\nfor issue tracker tickets. Thus, before submitting a question or bug report,\nplease take a moment of your time and ensure that your issue isn't already\ndiscussed in the project documentation provided at\nhttp://pybind11.readthedocs.org/en/latest.\nAssuming that you have identified a previously unknown problem or an important\nquestion, it's essential that you submit a self-contained and minimal piece of\ncode that reproduces the problem. In other words: no external dependencies,\nisolate the function(s) that cause breakage, submit matched and complete C++\nand Python snippets that can be easily compiled and run on my end.\nPull requests\nContributions are submitted, reviewed, and accepted using Github pull requests.\nPlease refer to this\narticle for details and\nadhere to the following rules to make the process as smooth as possible:\n\nMake a new branch for every feature you're working on.\nMake small and clean pull requests that are easy to review but make sure they\n  do add value by themselves.\nAdd tests for any new functionality and run the test suite (make pytest)\n  to ensure that no existing features break.\nThis project has a strong focus on providing general solutions using a\n  minimal amount of code, thus small pull requests are greatly preferred.\n\nLicensing of contributions\npybind11 is provided under a BSD-style license that can be found in the\nLICENSE file. By using, distributing, or contributing to this project, you\nagree to the terms and conditions of this license.\nYou are under no obligation whatsoever to provide any bug fixes, patches, or\nupgrades to the features, functionality or performance of the source code\n(\"Enhancements\") to anyone; however, if you choose to make your Enhancements\navailable either publicly, or directly to the author of this software, without\nimposing a separate written license agreement for such Enhancements, then you\nhereby grant the following license: a non-exclusive, royalty-free perpetual\nlicense to install, use, modify, prepare derivative works, incorporate into\nother computer software, distribute, and sublicense such enhancements or\nderivative works thereof, in binary and source code form.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-28T12:51:04Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-11T16:52:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9418170523642833,
        0.9785880435647136,
        0.9202712493389316,
        0.8883692171206413,
        0.9651059882569055,
        0.8103611508463802,
        0.9835429954565171,
        0.9079402146228198,
        0.9373402557890522,
        0.9057655909595551
      ],
      "excerpt": "This repository is three things: \nIt is the open-source multiplayer tetris game [SpeedBlocks] turned into a reinforcement learning (RL) environment with a complete python front-end. The Environment is highly customizable: game-field size, block types used, action type etc. are all easily changed. It is written to function well with RL at large scale by running arbitrary numbers of tetris-games in parallel, in a simple-to-use manner. \nA multi-processing framework for running multiple workers gathering trajectories for a trainer thread. The framework is flexible enough to facilitate many different RL algorithms. If you match the format of the template-agent provided, your algorithm should work right away with the framework. \nA small but growing family of RL-algorithms that learns to play two-player tetris through self-play: \nSIXten learns the value-function thru a k-step estimation scheme, utilizing the world-model of the environment and a prioritized distributed experience replay (modelled on Schaul et al.). Toghether with the multiprocessing framework described above it's similar to Ape-X (https://arxiv.org/abs/1803.00933) but the RL algorithm itself is different. \nSVENton is a double[1] dueling[2] k-step DQN-agent with a novel Convolutional Neuro-Keyboard interfacing it with the environment (1: https://arxiv.org/abs/1509.06461 2: https://arxiv.org/abs/1511.06581). It too utilizes the distributed prioritized experience replay, and the multi-processing framework. It is highly experimental, but it's included so SIXten doesn't get lonely. \nSVENton-PPO is similar to SVENton but trains by way of PPO. Trajectories are gathered by the workers, who also compute GAE-advantages [https://arxiv.org/abs/1506.02438]. This is the recommeded default for all new-comers. \nThe RL-framework and RL-algorithms may be separated into different repositories at some time in the future, but for now they are one. \nNOTE: The master-branch contains some features that are experimental. If there are issues, revert to the stable branch (https://github.com/mightypirate1/DRL-Tetris/tree/stable) or the kinda_stable branch (https://github.com/mightypirate1/DRL-Tetris/tree/kinda_stable). The latter \"should\" be stable, but testing all features is pretty time-consuming, and I try to make headway. \nThe quality of code has changed as I learned. Please be lenient when looking at the remnants of old code. There will come a day when it's fixed, let's hope! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456820580581601
      ],
      "excerpt": "experiments/sventon_dqn.py - Trains SVENton as above, but using a flavour of DQN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9882689613496931
      ],
      "excerpt": "The entire repository uses a settings-dictionary (the default values of which are found in tools/settings.py). To customize the environment, the agent, or the training procedure, create dictionary with settings that you pass to the relevant objects on creation. For examples of how to create such a dictionary, see the existing experiment-files in \"experiments/\". \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9948921558628141,
        0.9035956784990463,
        0.9669080215788
      ],
      "excerpt": "This is a design-choice I am - with the benefit of experience and hindsight - not too impressed with. My attention is finite and directed elsewhere in the project for now, as this works ok for RL-dev. If you hate it, feel free to contribute! ;) \nWhat pieces are being used is specified in the settngs-dictionary's field \"pieces\". It contains a list of any subset of {0,1,2,3,4,5,6}. [0,1,2,3,4,5,6] means the full set is used. The numbers correspond to the different pieces via the aliasing (L,J,S,Z,I,T,O) <~> (0,1,2,3,4,5,6). If those letters confuse you, you might want to check out https://tetris.fandom.com/wiki/Tetromino \nThe pre-defined settings on the master branch plays with only the O- and the L-piece to speed up training (pieces set to [0,6]). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8804624107168578
      ],
      "excerpt": "If you wish to customizations that are not obvious how to do, just contact me and I will produce the documentation needed asap. To write your own agent and/or customize the training procedure, you will have to write code. Probably the best way to get into the code is to look at the function \"thread_code\" in threads/worker_thread.py where the main training loop is located. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9293325557733555
      ],
      "excerpt": "If using different environments on different settings, the last one to be instantiated will impose it's settings on the others. This is generally only a problem when evaluating models trained on different settings. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9158207494256961,
        0.8829464930562054
      ],
      "excerpt": "So far no official client exists for playing against the agents you train. Coming soon is a closer integration of the environmnet backend and the game itself. This will allow for an evaluation mode where an agent plays versus a human player online in the same way that two human play against each other. Stay tuned! \nThe environment documentation is next on the todo-list. For now I will say that the functionality is similar conceptually to the OpenAI gym environments, and should be quite understandable from reading the code (check out the function \"thread_code\" in threads/worker_thread.py). Having said that, if you would like to see documentation happen faster, or if you have any question regarding this, contact me and I will happily answer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9667475280129004,
        0.8589835172786009
      ],
      "excerpt": "As I want to maintain full flexibility w.r.t what constitutes an action-space, there are no current plans on full gym-integration, but that - as all other things - might change with time. \nIf you want to get involved in this project and want to know what needs to be done, feel free to contact me and I will be happy to discuss! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Mastering the ancient art of competitive tetris, by way of RL and self-play!",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mightypirate1/DRL-Tetris/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 07:53:10 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mightypirate1/DRL-Tetris/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mightypirate1/DRL-Tetris",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/mightypirate1/DRL-Tetris/tree/master/environment/game_backend/pybind11/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mightypirate1/DRL-Tetris/master/environment/game_backend/pybind11/tools/check-style.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Pull the repository.\n* Install dependencies (see \"Dependencies\").\n* Build the backend module (see \"Build backend\").\n\n> NOTE: It has come to my attention that this code-base does not easily run on mac or windows. My apologies, I might fix that one day. Feel free to contribute :)\n\nOnce these steps are done, you should be able to use the environment, and to train your own agents (see \"Usage\").\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8441240296686662,
        0.9975717957383293
      ],
      "excerpt": "To build the package, we used CMake and make: \ncd path/to/DRL-tetris/environment/game_backend/source \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9689688202889957
      ],
      "excerpt": "env = environment.make(\"FullSize-v0\") \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8748707648425716
      ],
      "excerpt": "experiments/sixten_base.py  - Trains SIXten. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mightypirate1/DRL-Tetris/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Python",
      "CMake",
      "Shell",
      "Objective-C"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "GNU General Public License v3.0",
      "url": "https://api.github.com/licenses/gpl-3.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2016 Wenzel Jakob &#119;&#101;&#110;&#122;&#101;&#108;&#46;&#106;&#97;&#107;&#111;&#98;&#64;&#101;&#112;&#102;&#108;&#46;&#99;&#104;, All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\n3. Neither the name of the copyright holder nor the names of its contributors\\n   may be used to endorse or promote products derived from this software\\n   without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\nPlease also refer to the file CONTRIBUTING.md, which clarifies licensing of\\nexternal contributions to this project including patches, pull requests, etc.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DRL-Tetris",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DRL-Tetris",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mightypirate1",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mightypirate1/DRL-Tetris/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "mightypirate1",
        "body": "First release of DRL-Tetris. Most features are tested at least a little. Two scripts are provided that train and evaluate models respectively.",
        "dateCreated": "2020-04-26T09:29:07Z",
        "datePublished": "2020-04-26T09:42:57Z",
        "html_url": "https://github.com/mightypirate1/DRL-Tetris/releases/tag/v1.0",
        "name": "Solid",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/mightypirate1/DRL-Tetris/tarball/v1.0",
        "url": "https://api.github.com/repos/mightypirate1/DRL-Tetris/releases/25891552",
        "zipball_url": "https://api.github.com/repos/mightypirate1/DRL-Tetris/zipball/v1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The versions specified are the version used on the test system.\n\n- Python3 (3.6.3)\n- CMake (3.9.1)\n\nPython modules:\n- NumPy (1.16)\n- Tensorflow (1.12.0)\n- SciPy (1.2.0)\n- Docopt (0.6.2)\n- PyGame (1.9.4)\n\nOn Ubuntu, apt and pip3 solves the dependencies easily:\n```\napt install cmake python3-dev python3-pip\npip3 install docopt scipy numpy tensorflow\n```\n> Replace tensorflow with tensorflow-gpu for GPU support. This might require some work, but the official documentation should help: [tensorflow].\n\nIf the installation of any dependency fails, we refer to their documentation.\n\nIf you are not on Ubuntu, install the dependencies as you would on you system and proceed to the next step.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 07:53:10 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To start training, we recommend starting off from the example in experiments/sixten_base.py\n\nTo run the example project using 32 environments per worker thread, and 3 worker threads (+1 trainer thread), for 10M steps, run\n```\npython3 thread_train.py experiments/sventon_ppo.py --steps 10000000\n```\nperiodically during training, weights are saved to models/project_name/weightsNNN.w. Additionally, backups are made to models/project_name/weightsLATEST.w, and the final version is saved to models/project_name/weightsFINAL.w.\n\nTo test these weights out against themselves\n```\npython3 eval.py path/to/weightfile.w\n```\nor against other weights\n```\npython3 eval.py path/to/weightfile1.w path/to/weightfile2.w (...) --argmax\n```\nSettings are saved along with the weights so that it is normally possible to make bots made with different settings, neural-nets etc. play each other. As long as the game_size setting is the same across projects, they should be compatible! See \"Customization\" for more details.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The project ships with some pre-trained weights as a demo. When in the DRL-Tetris folder, try for instance\n```\npython3 eval.py models/demo_weights/SIXten/weightsDEMO1.w\n```\nto watch SIXten play.\n\nSimilarly,\n```\npython3 eval.py models/demo_weights/SVENton/weightsDEMO1.w\n```\nshows SVENton in action.\n\nSIXten was trained using a limited piece set, so it's na unfair comparison - but\n```\npython3 eval.py models/demo_weights/SVENton/weightsDEMO1.w models/demo_weights/SIXten/weightsDEMO1.w --all-pieces\n```\nshows the two agent types duking it out!\n\n",
      "technique": "Header extraction"
    }
  ]
}