{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1805.08318",
      "https://arxiv.org/abs/1706.08500"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.973830011697726,
        0.9957948171879981
      ],
      "excerpt": "Image (artistic) <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"center\"> | \nVideo <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"center\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9792090308263041
      ],
      "excerpt": "Image (stable) <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"center\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877118600674591
      ],
      "excerpt": "How to Achieve Stable Video \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9476660604754993
      ],
      "excerpt": "Your Own Machine \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8128224207520263
      ],
      "excerpt": "Video - it actually looks good!   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.973830011697726,
        0.9957948171879981
      ],
      "excerpt": "Image <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"center\"> \n| Video <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"center\">  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9614671091381286
      ],
      "excerpt": "Calling the API for video processing for a remote video \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8403442408180807
      ],
      "excerpt": "Calling the API for video processing for a local video \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jantic/DeOldify",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-31T23:32:34Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T20:12:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9296703818362919
      ],
      "excerpt": "Huggingface Web Demo (New): Integrated to Huggingface Spaces with Gradio. See demo:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9467756860788331
      ],
      "excerpt": "NEW Having trouble with the default image colorizer, aka \"artistic\"?  Try the \"stable\" one below.  It generally won't produce colors that are as interesting as \"artistic\", but the glitches are noticeably reduced.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8656499619292638
      ],
      "excerpt": "What is NoGAN? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9934971116483388,
        0.9201782076588597
      ],
      "excerpt": "Simply put, the mission of this project is to colorize and restore old images and film footage. \nWe'll get into the details in a bit, but first let's see some pretty pictures and videos! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9239303796693527,
        0.9917177536454845,
        0.9757450761186586,
        0.9809621684338286,
        0.996825417173868,
        0.9389121659977178,
        0.8516573875640945,
        0.9981067805966085
      ],
      "excerpt": "NoGAN - a new and weird but highly effective way to do GAN training for image to image. \nNoGAN training is crucial to getting the kind of stable and colorful images seen in this iteration of DeOldify. NoGAN training combines the benefits of GAN training (wonderful colorization) while eliminating the nasty side effects (like flickering objects in video). Believe it or not, video is rendered using isolated image generation without any sort of temporal modeling tacked on. The process performs 30-60 minutes of the GAN portion of \"NoGAN\" training, using 1% to 3% of imagenet data once.  Then, as with still image colorization, we \"DeOldify\" individual frames before rebuilding the video. \nIn addition to improved video stability, there is an interesting thing going on here worth mentioning. It turns out the models I run, even different ones and with different training structures, keep arriving at more or less the same solution.  That's even the case for the colorization of things you may think would be arbitrary and unknowable, like the color of clothing, cars, and even special effects (as seen in \"Metropolis\").   \nMy best guess is that the models are learning some interesting rules about how to colorize based on subtle cues present in the black and white images that I certainly wouldn't expect to exist.  This result leads to nicely deterministic and consistent results, and that means you don't have track model colorization decisions because they're not arbitrary.  Additionally, they seem remarkably robust so that even in moving scenes the renders are very consistent. \nOther ways to stabilize video add up as well. First, generally speaking rendering at a higher resolution (higher render_factor) will increase stability of colorization decisions.  This stands to reason because the model has higher fidelity image information to work with and will have a greater chance of making the \"right\" decision consistently.  Closely related to this is the use of resnet101 instead of resnet34 as the backbone of the generator- objects are detected more consistently and correctly with this. This is especially important for getting good, consistent skin rendering.  It can be particularly visually jarring if you wind up with \"zombie hands\", for example. \nAdditionally, gaussian noise augmentation during training appears to help but at this point the conclusions as to just how much are bit more tenuous (I just haven't formally measured this yet).  This is loosely based on work done in style transfer video, described here:  https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42.   \nSpecial thanks go to Rani Horev for his contributions in implementing this noise augmentation. \nThis is a new type of GAN training that I've developed to solve some key problems in the previous DeOldify model. It provides the benefits of GAN training while spending minimal time doing direct GAN training.  Instead, most of the training time is spent pretraining the generator and critic separately with more straight-forward, fast and reliable conventional methods.  A key insight here is that those more \"conventional\" methods generally get you most of the results you need, and that GANs can be used to close the gap on realism. During the very short amount of actual GAN training the generator not only gets the full realistic colorization capabilities that used to take days of progressively resized GAN training, but it also doesn't accrue nearly as much of the artifacts and other ugly baggage of GANs. In fact, you can pretty much eliminate glitches and artifacts almost entirely depending on your approach. As far as I know this is a new technique. And it's incredibly effective.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9274831846976022,
        0.9726406998893578,
        0.994968925037102,
        0.9814428919091838,
        0.9643931419844383,
        0.8822337943504819,
        0.905905052731582,
        0.9007716629187501,
        0.9978756665775226,
        0.9937798563947672,
        0.9845358532936082,
        0.9454050548035409,
        0.922739881073142,
        0.9831192346713913,
        0.9507993375117205
      ],
      "excerpt": "NoGAN-Based DeOldify Model \nThe steps are as follows: First train the generator in a conventional way by itself with just the feature loss. Next, generate images from that, and train the critic on distinguishing between those outputs and real images as a basic binary classifier. Finally, train the generator and critic together in a GAN setting (starting right at the target size of 192px in this case).  Now for the weird part:  All the useful GAN training here only takes place within a very small window of time.  There's an inflection point where it appears the critic has transferred everything it can that is useful to the generator. Past this point, image quality oscillates between the best that you can get at the inflection point, or bad in a predictable way (orangish skin, overly red lips, etc).  There appears to be no productive training after the inflection point.  And this point lies within training on just 1% to 3% of the Imagenet Data!  That amounts to about 30-60 minutes of training at 192px.   \nThe hard part is finding this inflection point.  So far, I've accomplished this by making a whole bunch of model save checkpoints (every 0.1% of data iterated on) and then just looking for the point where images look great before they go totally bonkers with orange skin (always the first thing to go). Additionally, generator rendering starts immediately getting glitchy and inconsistent at this point, which is no good particularly for video. What I'd really like to figure out is what the tell-tale sign of the inflection point is that can be easily automated as an early stopping point.  Unfortunately, nothing definitive is jumping out at me yet.  For one, it's happening in the middle of training loss decreasing- not when it flattens out, which would seem more reasonable on the surface.    \nAnother key thing about NoGAN training is you can repeat pretraining the critic on generated images after the initial GAN training, then repeat the GAN training itself in the same fashion.  This is how I was able to get extra colorful results with the \"artistic\" model.  But this does come at a cost currently- the output of the generator becomes increasingly inconsistent and you have to experiment with render resolution (render_factor) to get the best result.  But the renders are still glitch free and way more consistent than I was ever able to achieve with the original DeOldify model. You can do about five of these repeat cycles, give or take, before you get diminishing returns, as far as I can tell.   \nKeep in mind- I haven't been entirely rigorous in figuring out what all is going on in NoGAN- I'll save that for a paper. That means there's a good chance I'm wrong about something.  But I think it's definitely worth putting out there now because I'm finding it very useful- it's solving basically much of my remaining problems I had in DeOldify. \nThis builds upon a technique developed in collaboration with Jeremy Howard and Sylvain Gugger for Fast.AI's Lesson 7 in version 3 of Practical Deep Learning for Coders Part I. The particular lesson notebook can be found here: https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb \nThere are now three models to choose from in DeOldify. Each of these has key strengths and weaknesses, and so have different use cases.  Video is for video of course.  But stable and artistic are both for images, and sometimes one will do images better than the other.   \nMore details: \nArtistic - This model achieves the highest quality results in image coloration, in terms of interesting details and vibrance. The most notable drawback however is that it's a bit of a pain to fiddle around with to get the best results (you have to adjust the rendering resolution or render_factor to achieve this).  Additionally, the model does not do as well as stable in a few key common scenarios- nature scenes and portraits.  The model uses a resnet34 backbone on a UNet with an emphasis on depth of layers on the decoder side.  This model was trained with 5 critic pretrain/GAN cycle repeats via NoGAN, in addition to the initial generator/critic pretrain/GAN NoGAN training, at 192px.  This adds up to a total of 32% of Imagenet data trained once (12.5 hours of direct GAN training).   \nStable - This model achieves the best results with landscapes and portraits. Notably, it produces less \"zombies\"- where faces or limbs stay gray rather than being colored in properly.  It generally has less weird miscolorations than artistic, but it's also less colorful in general.  This model uses a resnet101 backbone on a UNet with an emphasis on width of layers on the decoder side.  This model was trained with 3 critic pretrain/GAN cycle repeats via NoGAN, in addition to the initial generator/critic pretrain/GAN NoGAN training, at 192px.  This adds up to a total of 7% of Imagenet data trained once (3 hours of direct GAN training). \nVideo - This model is optimized for smooth, consistent and flicker-free video.  This would definitely be the least colorful of the three models, but it's honestly not too far off from \"stable\". The model is the same as \"stable\" in terms of architecture, but differs in training.  It's trained for a mere 2.2% of Imagenet data once at 192px, using only the initial generator/critic pretrain/GAN NoGAN training (1 hour of direct GAN training). \nBecause the training of the artistic and stable models was done before the \"inflection point\" of NoGAN training described in \"What is NoGAN???\" was discovered,  I believe this amount of training on them can be knocked down considerably. As far as I can tell, the models were stopped at \"good points\" that were well beyond where productive training was taking place.  I'll be looking into this in the future. \nIdeally, eventually these three models will be consolidated into one that has all these good desirable unified.  I think there's a path there, but it's going to require more work!  So for now, the most practical solution appears to be to maintain multiple models. \nThis is a deep learning based model.  More specifically, what I've done is combined the following approaches: \nExcept the generator is a pretrained U-Net, and I've just modified it to have the spectral normalization and self-attention.  It's a pretty straightforward translation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702640013048217,
        0.9353216286117125,
        0.9670353020019752,
        0.8977110194185652,
        0.9849764764807868,
        0.9663204282656891,
        0.9205584071871264,
        0.8578921217036607,
        0.9618127053798078,
        0.9283987839320113
      ],
      "excerpt": "This is modified to incorporate a \"threshold\" critic loss that makes sure that the critic is \"caught up\" before moving on to generator training. \nThis is particularly useful for the \"NoGAN\" method described below. \nThere's no paper here! This is a new type of GAN training that I've developed to solve some key problems in the previous DeOldify model. \nThe gist is that you get the benefits of GAN training while spending minimal time doing direct GAN training. \nMore details are in the What is NoGAN? section (it's a doozy). \nLoss during NoGAN learning is two parts:  One is a basic Perceptual Loss (or Feature Loss) based on VGG16 \u2013 this just biases the generator model to replicate the input image. \nThe second is the loss score from the critic.  For the curious \u2013 Perceptual Loss isn't sufficient by itself to produce good results. \nIt tends to just encourage a bunch of brown/green/blue \u2013 you know, cheating to the test, basically, which neural networks are really good at doing! \nKey thing to realize here is that GANs essentially are learning the loss function for you \u2013 which is really one big step closer to toward the ideal that we're shooting for in machine learning. \nAnd of course you generally get much better results when you get the machine to learn something you were previously hand coding. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9705862093556655,
        0.9706793244648508,
        0.935819170331629,
        0.9708096491272699
      ],
      "excerpt": "Of note:  There's no longer any \"Progressive Growing of GANs\" type training going on here.  It's just not needed in lieu of the superior results obtained by the \"NoGAN\" technique described above. \nThe beauty of this model is that it should be generally useful for all sorts of image modification, and it should do it quite well. \nWhat you're seeing above are the results of the colorization model, but that's just one component in a pipeline that I'm developing with the exact same approach. \nThe easiest way to get started is to go straight to the Colab notebooks:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9588865708368924,
        0.8989750782663427
      ],
      "excerpt": "Special thanks to Matt Robinson and Mar\u00eda Benavente for their image Colab notebook contributions, and Robert Bell for the video Colab notebook work! \nThe images in the test_images folder have been removed because they were using Git LFS and that costs a lot of money when GitHub actually charges for bandwidth on a popular open source project (they had a billing bug for while that was recently fixed).  The notebooks that use them (the image test ones) still point to images in that directory that I (Jason) have personally and I'd like to keep it that way because, after all, I'm by far the primary and most active developer.  But they won't work for you.  Still, those notebooks are a convenient template for making your own tests if you're so inclined. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8151827266543753
      ],
      "excerpt": "Starting the notebook \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.914340918578702
      ],
      "excerpt": "Calling the API for image processing for a remote image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9341381352665218
      ],
      "excerpt": "Calling the API for image processing for a local image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9969521992654252
      ],
      "excerpt": "Adding the your model to the local subdirectory of the project \"data/models\" for instance : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8288865720373351
      ],
      "excerpt": "- Image Model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9317380340621232
      ],
      "excerpt": "We suspect some of you are going to want access to the original DeOldify model for various reasons.  We have that archived here:  https://github.com/dana-kelley/DeOldify \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A Deep Learning based project for colorizing and restoring old images (and video!)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jantic/DeOldify/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2058,
      "date": "Thu, 23 Dec 2021 05:49:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jantic/DeOldify/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jantic/DeOldify",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jantic/DeOldify/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jantic/DeOldify/master/ImageColorizerColab.ipynb",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/ColorizeTrainingStableLargeBatch.ipynb",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/VideoColorizer.ipynb",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/ColorizeTrainingWandb.ipynb",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/ColorizeTrainingVideo.ipynb",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/ImageColorizerStableTests.ipynb",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/ImageColorizerArtisticTests.ipynb",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/ImageColorizer.ipynb",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/ColorizeTrainingArtistic.ipynb",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/VideoColorizerColab.ipynb",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/ColorFIDBenchmarkArtistic.ipynb",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/ImageColorizerColabStable.ipynb",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/ColorizeTrainingStable.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jantic/DeOldify/master/run_notebook.sh",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/run_video_api.sh",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/quick_start.sh",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/run_image_api.sh",
      "https://raw.githubusercontent.com/jantic/DeOldify/master/api_cmd_example.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project is built around the wonderful Fast.AI library.  Prereqs, in summary:\n\n- **Fast.AI 1.0.51** (and its dependencies).  If you use any higher version you'll see grid artifacts in rendering and tensorboard will malfunction. So yeah...don't do that.\n- **PyTorch 1.0.1** Not the latest version of PyTorch- that will not play nicely with the version of FastAI above.  Note however that the conda install of FastAI 1.0.51 grabs the latest PyTorch, which doesn't work.  This is patched over by our own conda install but fyi.\n- **Jupyter Lab** `conda install -c conda-forge jupyterlab`\n- **Tensorboard** (i.e. install Tensorflow) and **TensorboardX** (https://github.com/lanpa/tensorboardX).  I guess you don't *have* to but man, life is so much better with it.  FastAI now comes with built in support for this- you just  need to install the prereqs: `conda install -c anaconda tensorflow-gpu` and `pip install tensorboardX`\n- **ImageNet** \u2013 Only if you're training, of course. It has proven to be a great dataset for my purposes.  http://www.image-net.org/download-images\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You should now be able to do a simple install with Anaconda. Here are the steps:\n\nOpen the command line and navigate to the root folder you wish to install.  Then type the following commands \n\n```console\ngit clone https://github.com/jantic/DeOldify.git DeOldify\ncd DeOldify\nconda env create -f environment.yml\n```\n\nThen start running with these commands:\n\n```console\nsource activate deoldify\njupyter lab\n```\n\nFrom there you can start running the notebooks in Jupyter Lab, via the url they provide you in the console.  \n\n> **Note:** You can also now do \"conda activate deoldify\" if you have the latest version of conda and in fact that's now recommended. But a lot of people don't have that yet so I'm not going to make it the default instruction here yet.\n\n**Alternative Install:** User daddyparodz has kindly created an installer script for Ubuntu, and in particular Ubuntu on WSL, that may make things easier:  https://github.com/daddyparodz/AutoDeOldifyLocal\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8899399546741313
      ],
      "excerpt": "Desktop (New): Want to run open source DeOldify for photos on Windows desktop? ColorfulSoft made such a thing here and it really works-  https://github.com/ColorfulSoft/DeOldify.NET . No GPU required! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8805254996503489
      ],
      "excerpt": "Get more updates on Twitter <img src=\"resource_images/Twitter_Social_Icon_Rounded_Square_Color.svg\" width=\"16\">. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8895399003663425
      ],
      "excerpt": "We have build for you a quickstart script for you in order to get up to speed in a minute. It's even compatible if you don't have GPU and will automatically adjust it's configuration according to your hardware (running on CPU will be slow with no surprise). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9893272198983933
      ],
      "excerpt": "git clone https://github.com/jantic/DeOldify.git DeOldify \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9912692656233638,
        0.8862914833382268
      ],
      "excerpt": "cd DeOldify &amp;&amp; ./quick_start.sh notebook my_super_password \nyour notebook will be accessible on port 8888 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9893272198983933
      ],
      "excerpt": "git clone https://github.com/jantic/DeOldify.git DeOldify \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9932079519378395
      ],
      "excerpt": "cd DeOldify &amp;&amp; ./quick_start.sh image_api \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9932079519378395
      ],
      "excerpt": "cd DeOldify &amp;&amp; ./quick_start.sh image_api \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9893272198983933
      ],
      "excerpt": "git clone https://github.com/jantic/DeOldify.git DeOldify \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8134285445367718
      ],
      "excerpt": "cd DeOldify &amp;&amp; docker build -t deoldify_jupyter -f Dockerfile . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9893272198983933
      ],
      "excerpt": "git clone https://github.com/jantic/DeOldify.git DeOldify \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8432512141086097
      ],
      "excerpt": "Note: If you don't have Nvidia Docker, here is the installation guide. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8355290538480111
      ],
      "excerpt": "both models are available here: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8271114044126654
      ],
      "excerpt": "Get more updates on Twitter <img src=\"resource_images/Twitter_Social_Icon_Rounded_Square_Color.svg\" width=\"16\">. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826690744422612
      ],
      "excerpt": "Example Images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8031599334984311
      ],
      "excerpt": "Running Docker \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8031599334984311
      ],
      "excerpt": "Running Docker \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jantic/DeOldify/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeOldify",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeOldify",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jantic",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jantic/DeOldify/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* **(Training Only) BEEFY Graphics card**.  I'd really like to have more memory than the 11 GB in my GeForce 1080TI (11GB).  You'll have a tough time with less.  The Generators and Critic are ridiculously large.  \n* **(Colorization Alone) A decent graphics card**. Approximately 4GB+ memory video cards should be sufficient.\n* **Linux**.  I'm using Ubuntu 18.04, and I know 16.04 works fine too.  **Windows is not supported and any issues brought up related to this will not be investigated.**\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14489,
      "date": "Thu, 23 Dec 2021 05:49:38 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We believe that open source has done a lot of good for the world.\u00a0 After all, DeOldify simply wouldn't exist without it. But we also believe that there needs to be boundaries on just how much is reasonable to be expected from an open source project maintained by just two developers.\n\nOur stance is that we're providing the code and documentation on research that we believe is beneficial to the world.\u00a0 What we have provided are novel takes on colorization, GANs, and video that are hopefully somewhat friendly for developers and researchers to learn from and adopt. This is the culmination of well over a year of continuous work, free for you. What wasn't free was shouldered by us, the developers.\u00a0 We left our jobs, bought expensive GPUs, and had huge electric bills as a result of dedicating ourselves to this.\n\nWhat we haven't provided here is a ready to use free \"product\" or \"app\", and we don't ever intend on providing that.\u00a0 It's going to remain a Linux based project without Windows support, coded in Python, and requiring people to have some extra technical background to be comfortable using it.\u00a0 Others have stepped in with their own apps made with DeOldify, some paid and some free, which is what we want! We're instead focusing on what we believe we can do best- making better commercial models that people will pay for.\u00a0\u00a0\nDoes that mean you're not getting the very best for free?\u00a0 Of course. We simply don't believe that we're obligated to provide that, nor is it feasible! We compete on research and sell that.\u00a0 Not a GUI or web service that wraps said research- that part isn't something we're going to be great at anyways. We're not about to shoot ourselves in the foot by giving away our actual competitive advantage for free, quite frankly.\n\nWe're also not willing to go down the rabbit hole of providing endless, open ended and personalized support on this open source project.\u00a0 Our position is this:\u00a0 If you have the proper background and resources, the project provides more than enough to get you started. We know this because we've seen plenty of people using it and making money off of their own projects with it.\u00a0\u00a0\n\nThus, if you have an issue come up and it happens to be an actual bug that having it be fixed will benefit users generally, then great- that's something we'll be happy to look into.\u00a0\n\nIn contrast, if you're asking about something that really amounts to asking for personalized and time consuming support that won't benefit anybody else, we're not going to help. It's simply not in our interest to do that. We have bills to pay, after all. And if you're asking for help on something that can already be derived from the documentation or code?\u00a0 That's simply annoying, and we're not going to pretend to be ok with that.\n\n",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**Note:**  Click images to watch\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "[![](http://img.youtube.com/vi/l3UXXid04Ys/0.jpg)](http://www.youtube.com/watch?v=l3UXXid04Ys)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "[![](http://img.youtube.com/vi/EXn-n2iqEjI/0.jpg)](http://www.youtube.com/watch?v=EXn-n2iqEjI)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\"Migrant Mother\" by Dorothea Lange (1936)\n\n![Migrant Mother](https://i.imgur.com/Bt0vnke.jpg)\n\nWoman relaxing in her livingroom in Sweden (1920)\n\n![Sweden Living Room](https://i.imgur.com/158d0oU.jpg)\n\n\"Toffs and Toughs\" by Jimmy Sime (1937)\n\n![Class Divide](https://i.imgur.com/VYuav4I.jpg)\n\nThanksgiving Maskers (1911)\n\n![Thanksgiving Maskers](https://i.imgur.com/n8qVJ5c.jpg)\n\nGlen Echo Madame Careta Gypsy Camp in Maryland (1925)\n\n![Gypsy Camp](https://i.imgur.com/1oYrJRI.jpg)\n\n\"Mr. and Mrs. Lemuel Smith and their younger children in their farm house, Carroll County, Georgia.\" (1941)\n\n![Georgia Farmhouse](https://i.imgur.com/I2j8ynm.jpg) \n\n\"Building the Golden Gate Bridge\" (est 1937)\n\n![Golden Gate Bridge](https://i.imgur.com/6SbFjfq.jpg)\n\n> **Note:**  What you might be wondering is while this render looks cool, are the colors accurate? The original photo certainly makes it look like the towers of the bridge could be white. We looked into this and it turns out the answer is no - the towers were already covered in red primer by this time. So that's something to keep in mind- historical accuracy remains a huge challenge!\n\n\"Terrasse de caf\u00e9, Paris\" (1925)\n\n![Cafe Paris](https://i.imgur.com/WprQwP5.jpg)\n\nNorwegian Bride (est late 1890s)\n\n![Norwegian Bride](https://i.imgur.com/MmtvrZm.jpg)\n\nZitk\u00e1la-\u0160\u00e1 (Lakota: Red Bird), also known as Gertrude Simmons Bonnin (1898)\n\n![Native Woman](https://i.imgur.com/zIGM043.jpg)\n\nChinese Opium Smokers (1880)\n\n![Opium Real](https://i.imgur.com/lVGq8Vq.jpg)\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "So that's the gist of this project \u2013 I'm looking to make old photos and film look reeeeaaally good with GANs, and more importantly, make the project *useful*.\nIn the meantime though this is going to be my baby and I'll be actively updating and improving the code over the foreseeable future.\nI'll try to make this as user-friendly as possible, but I'm sure there's going to be hiccups along the way.\n\nOh and I swear I'll document the code properly...eventually.  Admittedly I'm *one of those* people who believes in \"self documenting code\" (LOL).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```console\n./quick_start.sh\nmissing first argument\n\n\t  _____        ____  _     _ _  __\n\t |  __ \\      / __ \\| |   | (_)/ _|\n\t | |  | | ___| |  | | | __| |_| |_ _   _\n\t | |  | |/ _ \\ |  | | |/ _` | |  _| | | |\n\t | |__| |  __/ |__| | | (_| | | | | |_| |\n\t |_____/ \\___|\\____/|_|\\__,_|_|_|  \\__, |\n\t                                    __/ |\n\t                                   |___/\n\n\nusage : ./quick_start.sh notebook password -- to start the notebook with password\n             leave empty for no password (not recommended)\nusage : ./quick_start.sh image_api  -- to start image api\nusage : ./quick_start.sh video_api  -- to start video api\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}