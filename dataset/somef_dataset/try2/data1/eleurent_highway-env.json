{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1903.00220",
      "https://arxiv.org/abs/1904.04727",
      "https://arxiv.org/abs/1904.04700",
      "https://arxiv.org/abs/1909.11628",
      "https://arxiv.org/abs/1911.12250",
      "https://arxiv.org/abs/2004.14547",
      "https://arxiv.org/abs/2006.11441",
      "https://arxiv.org/abs/2007.09569",
      "https://arxiv.org/abs/2007.10401",
      "https://arxiv.org/abs/2007.13078",
      "https://arxiv.org/abs/2005.05441",
      "https://arxiv.org/abs/2011.03748",
      "https://arxiv.org/abs/2011.04950",
      "https://arxiv.org/abs/2002.10816",
      "https://arxiv.org/abs/2012.00724",
      "https://arxiv.org/abs/2101.02828",
      "https://arxiv.org/abs/2101.07140",
      "https://arxiv.org/abs/2102.03483",
      "https://arxiv.org/abs/2103.10245",
      "https://arxiv.org/abs/2104.08876",
      "https://arxiv.org/abs/2105.05701",
      "https://arxiv.org/abs/2106.10566",
      "https://arxiv.org/abs/2107.04538",
      "https://arxiv.org/abs/2109.03214"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use the project in your work, please consider citing it with:\n```bibtex\n@misc{highway-env,\n  author = {Leurent, Edouard},\n  title = {An Environment for Autonomous Driving Decision-Making},\n  year = {2018},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/eleurent/highway-env}},\n}\n```\n\nList of publications & preprints using `highway-env` (please open a pull request to add missing entries):\n*   [Approximate Robust Control of Uncertain Dynamical Systems](https://arxiv.org/abs/1903.00220) (Dec 2018)\n*   [Interval Prediction for Continuous-Time Systems with Parametric Uncertainties](https://arxiv.org/abs/1904.04727) (Apr 2019)\n*   [Practical Open-Loop Optimistic Planning](https://arxiv.org/abs/1904.04700) (Apr 2019)\n*   [\u03b1^\u03b1-Rank: Practically Scaling \u03b1-Rank through Stochastic Optimisation](https://arxiv.org/abs/1909.11628) (Sep 2019)\n*   [Social Attention for Autonomous Decision-Making in Dense Traffic](https://arxiv.org/abs/1911.12250) (Nov 2019)\n*   [Budgeted Reinforcement Learning in Continuous State Space](http://papers.nips.cc/paper/9128-budgeted-reinforcement-learning-in-continuous-state-space/) (Dec 2019)\n*   [Multi-View Reinforcement Learning](http://papers.nips.cc/paper/8422-multi-view-reinforcement-learning) (Dec 2019)\n*   [Reinforcement learning for Dialogue Systems optimization with user adaptation](https://tel.archives-ouvertes.fr/tel-02422691/) (Dec 2019)\n*   [Distributional Soft Actor Critic for Risk Sensitive Learning](https://arxiv.org/abs/2004.14547) (Apr 2020)\n*   [Bi-Level Actor-Critic for Multi-Agent Coordination](https://ojs.aaai.org/index.php/AAAI/article/view/6226) (Apr 2020)\n*   [Task-Agnostic Online Reinforcement Learning with an Infinite Mixture of Gaussian Processes](https://arxiv.org/abs/2006.11441) (Jun 2020)\n*   [Beyond Prioritized Replay: Sampling States in Model-Based RL via Simulated Priorities](https://arxiv.org/abs/2007.09569) (Jul 2020)\n*   [Robust-Adaptive Interval Predictive Control for Linear Uncertain Systems](https://arxiv.org/abs/2007.10401) (Jul 2020)\n*   [SMART: Simultaneous Multi-Agent Recurrent Trajectory Prediction](https://arxiv.org/abs/2007.13078) (Jul 2020)\n*   [Delay-Aware Multi-Agent Reinforcement Learning for Cooperative and Competitive Environments](https://arxiv.org/abs/2005.05441) (Aug 2020)\n*   [B-GAP: Behavior-Guided Action Prediction for Autonomous Navigation](https://arxiv.org/abs/2011.03748) (Nov 2020)\n*   [Model-based Reinforcement Learning from Signal Temporal Logic Specifications](https://arxiv.org/abs/2011.04950) (Nov 2020)\n*   [Robust-Adaptive Control of Linear Systems: beyond Quadratic Costs](https://arxiv.org/abs/2002.10816) (Dec 2020)\n*   [Assessing and Accelerating Coverage in Deep Reinforcement Learning](https://arxiv.org/abs/2012.00724) (Dec 2020)\n*   [Distributionally Consistent Simulation of Naturalistic Driving Environment for Autonomous Vehicle Testing](https://arxiv.org/abs/2101.02828) (Jan 2021)\n*   [Interpretable Policy Specification and Synthesis through Natural Language and RL](https://arxiv.org/abs/2101.07140) (Jan 2021)\n*   [Deep Reinforcement Learning Techniques in Diversified Domains: A Survey](https://link.springer.com/article/10.1007/s11831-021-09552-3) (Feb 2021)\n*   [Corner Case Generation and Analysis for Safety Assessment of Autonomous Vehicles](https://arxiv.org/abs/2102.03483) (Feb 2021)\n*   [Intelligent driving intelligence test for autonomous vehicles with naturalistic and adversarial environment](https://www.nature.com/articles/s41467-021-21007-8) (Feb 2021)\n*   [Building Safer Autonomous Agents by Leveraging Risky Driving Behavior Knowledge](https://arxiv.org/abs/2103.10245)\n*   [Quick Learner Automated Vehicle Adapting its Roadmanship to Varying Traffic Cultures with Meta Reinforcement Learning](https://arxiv.org/abs/2104.08876) (Apr 2021)\n*   [Deep Multi-agent Reinforcement Learning for Highway On-Ramp Merging in Mixed Traffic](https://arxiv.org/abs/2105.05701) (May 2021)\n*   [Accelerated Policy Evaluation: Learning Adversarial Environments with Adaptive Importance Sampling](https://arxiv.org/abs/2106.10566) (Jun 2021)\n*   [Learning Interaction-aware Guidance Policies for Motion Planning in Dense Traffic Scenarios](https://arxiv.org/abs/2107.04538) (Jul 2021)\n*   [Robust Predictable Control](https://arxiv.org/abs/2109.03214) (Sep 2021)\n\nPhD theses\n*   [Reinforcement learning for Dialogue Systems optimization with user adaptation](https://hal.inria.fr/tel-02422691/) (2019)\n*   [Safe and Efficient Reinforcement Learning for Behavioural Planning in Autonomous Driving](https://hal.inria.fr/tel-03035705/) (2020)\n*   [Many-agent Reinforcement Learning](https://discovery.ucl.ac.uk/id/eprint/10124273/) (2021)\n\nMaster theses\n*   [Multi-Agent Reinforcement Learning with Application on Traffic Flow Control](https://www.diva-portal.org/smash/get/diva2:1573441/FULLTEXT01.pdf) (Jun 2021)\n*   [Deep Reinforcement Learning for Automated Parking](https://repositorio-aberto.up.pt/bitstream/10216/136074/2/494682.pdf) (Aug 2021)\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "cff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n- family-names: \"Leurent\"\n  given-names: \"Edouard\"\ntitle: \"An Environment for Autonomous Driving Decision-Making\"\nversion: 1.4\ndate-released: 2018-05-01\nurl: \"https://github.com/eleurent/highway-env\"",
      "technique": "File Exploration"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{highway-env,\n  author = {Leurent, Edouard},\n  title = {An Environment for Autonomous Driving Decision-Making},\n  year = {2018},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/eleurent/highway-env}},\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eleurent/highway-env",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-11-15T15:38:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T07:54:33Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9453814530344764
      ],
      "excerpt": "A collection of environments for autonomous driving and tactical decision-making tasks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.968996266234854
      ],
      "excerpt": "    <em>An episode of one of the environments available in highway-env.</em> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978922383701159,
        0.993637524987716
      ],
      "excerpt": "In this task, the ego-vehicle is driving on a multilane highway populated with other vehicles. \nThe agent's objective is to reach a high speed while avoiding collisions with neighbouring vehicles. Driving on the right side of the road is also rewarded. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9352673610985645
      ],
      "excerpt": "A faster variant, highway-fast-v0 is also available, with a degraded simulation accuracy to improve speed for large-scale training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9843587955237065
      ],
      "excerpt": "In this task, the ego-vehicle starts on a main highway but soon approaches a road junction with incoming vehicles on the access ramp. The agent's objective is now to maintain a high speed while making room for the vehicles so that they can safely merge in the traffic. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8414286918095129
      ],
      "excerpt": "In this task, the ego-vehicle if approaching a roundabout with flowing traffic. It will follow its planned route automatically, but has to handle lane changes and longitudinal control to pass the roundabout as fast as possible while avoiding collisions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9388437641793105
      ],
      "excerpt": "A goal-conditioned continuous control task in which the ego-vehicle must park in a given space with the appropriate heading. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9492563173164467
      ],
      "excerpt": "An intersection negotiation task with dense traffic. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "    <em>The DQN agent solving highway-v0.</em> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8572351820453471
      ],
      "excerpt": "This model-free value-based reinforcement learning agent performs Q-learning with function approximation, using a neural network to represent the state-action value function Q. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "    <em>The DDPG agent solving parking-v0.</em> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9755340787882133
      ],
      "excerpt": "This model-free policy-based reinforcement learning agent is optimized directly by gradient ascent. It uses Hindsight Experience Replay to efficiently learn how to solve a goal-conditioned task. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9861379313743056
      ],
      "excerpt": "The Value Iteration is only compatible with finite discrete MDPs, so the environment is first approximated by a finite-mdp environment using env.to_finite_mdp(). This simplified state representation describes the nearby traffic in terms of predicted Time-To-Collision (TTC) on each lane of the road. The transition model is simplistic and assumes that each vehicle will keep driving at a constant speed without changing lanes. This model bias can be a source of mistakes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9790485348297584
      ],
      "excerpt": "This agent leverages a transition and reward models to perform a stochastic tree search (Coulom, 2006) of the optimal trajectory. No particular assumption is required on the state representation or transition model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "    <em>The MCTS agent solving highway-v0.</em> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A minimalist environment for decision-making in autonomous driving ",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Read the [documentation online](https://highway-env.readthedocs.io/).\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eleurent/highway-env/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 324,
      "date": "Wed, 29 Dec 2021 18:46:21 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/eleurent/highway-env/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "eleurent/highway-env",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/eleurent/highway-env/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/eleurent/highway-env/master/scripts/sb3_highway_dqn.ipynb",
      "https://raw.githubusercontent.com/eleurent/highway-env/master/scripts/highway_planning.ipynb",
      "https://raw.githubusercontent.com/eleurent/highway-env/master/scripts/parking_model_based.ipynb",
      "https://raw.githubusercontent.com/eleurent/highway-env/master/scripts/parking_her.ipynb",
      "https://raw.githubusercontent.com/eleurent/highway-env/master/scripts/intersection_social_dqn.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "`pip install highway-env`\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8707843808844385
      ],
      "excerpt": "env = gym.make(\"highway-v0\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9135863039469521
      ],
      "excerpt": "    <em>The highway-v0 environment.</em> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955904989154949
      ],
      "excerpt": "env = gym.make(\"merge-v0\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9491690200796026
      ],
      "excerpt": "    <em>The merge-v0 environment.</em> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955904989154949
      ],
      "excerpt": "env = gym.make(\"roundabout-v0\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9491690200796026
      ],
      "excerpt": "    <em>The roundabout-v0 environment.</em> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955904989154949
      ],
      "excerpt": "env = gym.make(\"parking-v0\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9491690200796026
      ],
      "excerpt": "    <em>The parking-v0 environment.</em> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955904989154949
      ],
      "excerpt": "env = gym.make(\"intersection-v0\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9491690200796026
      ],
      "excerpt": "    <em>The intersection-v0 environment.</em> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955904989154949
      ],
      "excerpt": "env = gym.make(\"racetrack-v0\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9491690200796026
      ],
      "excerpt": "    <em>The racetrack-v0 environment.</em> \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8205969958445952
      ],
      "excerpt": "The agent then performs a Value Iteration to compute the corresponding optimal state-value function. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/eleurent/highway-env/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Edouard Leurent\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "highway-env",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "highway-env",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "eleurent",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eleurent/highway-env/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "eleurent",
        "body": "This release introduces additional content:\r\n- a new continuous control environment, `racetrack-v0`, where the agent must learn to steer and follow the tracks, while avoiding other vehicles\r\n- a new `\"on_road\"` layer in the `OccupancyGrid` observation type, which enables the observer to see the drivable space\r\n- a new `\"align_to_vehicle_axes\"` option in the `OccupancyGrid` observation type, which renders the observation in the local vehicle frame\r\n- a new `DiscreteAction` action type, which discretizes the original `ContinuousAction` type. This allows to do low-level control, but with a small discrete action space (e.g. for DQN). Note that this is different from the `DiscreteMetaAction` type, which implements its own low-level sub-policies.\r\n- new example scripts and notebooks for training agents, such as a PPO continuous control policy for racetrack-v0.\r\n- updated documentation\r\n",
        "dateCreated": "2021-09-21T13:40:56Z",
        "datePublished": "2021-09-21T13:47:52Z",
        "html_url": "https://github.com/eleurent/highway-env/releases/tag/v1.4",
        "name": "New continuous control environment: racetrack-v0",
        "tag_name": "v1.4",
        "tarball_url": "https://api.github.com/repos/eleurent/highway-env/tarball/v1.4",
        "url": "https://api.github.com/repos/eleurent/highway-env/releases/49986157",
        "zipball_url": "https://api.github.com/repos/eleurent/highway-env/zipball/v1.4"
      },
      {
        "authorType": "User",
        "author_name": "eleurent",
        "body": "This release contains\r\n\r\n- A few fixes for compatibility with SB3\r\n- Some changes for video rendering and framerate\r\n- `highway-fast-v0`: a faster variant of highway-v0 to train/debug models more quickly",
        "dateCreated": "2021-08-30T08:58:52Z",
        "datePublished": "2021-08-30T09:10:57Z",
        "html_url": "https://github.com/eleurent/highway-env/releases/tag/v1.3",
        "name": "Faster variant of highway-v0, and bug fixes",
        "tag_name": "v1.3",
        "tarball_url": "https://api.github.com/repos/eleurent/highway-env/tarball/v1.3",
        "url": "https://api.github.com/repos/eleurent/highway-env/releases/48660272",
        "zipball_url": "https://api.github.com/repos/eleurent/highway-env/zipball/v1.3"
      },
      {
        "authorType": "User",
        "author_name": "eleurent",
        "body": "Minor update with\r\n* different handling of image observations + example script with stable baselines\r\n* small changes in the dynamical model",
        "dateCreated": "2021-04-29T09:48:20Z",
        "datePublished": "2021-04-29T09:48:48Z",
        "html_url": "https://github.com/eleurent/highway-env/releases/tag/v1.2",
        "name": "Compatibility with stable-baselines3",
        "tag_name": "v1.2",
        "tarball_url": "https://api.github.com/repos/eleurent/highway-env/tarball/v1.2",
        "url": "https://api.github.com/repos/eleurent/highway-env/releases/42190640",
        "zipball_url": "https://api.github.com/repos/eleurent/highway-env/zipball/v1.2"
      },
      {
        "authorType": "User",
        "author_name": "eleurent",
        "body": "Main changes:\r\n- vehicles appearance\r\n- exit environment\r\n- u-turn environment\r\n- lidar observation\r\n- big fix in video recording",
        "dateCreated": "2021-03-12T16:38:09Z",
        "datePublished": "2021-03-12T16:38:26Z",
        "html_url": "https://github.com/eleurent/highway-env/releases/tag/v1.1",
        "name": "First PyPI release",
        "tag_name": "v1.1",
        "tarball_url": "https://api.github.com/repos/eleurent/highway-env/tarball/v1.1",
        "url": "https://api.github.com/repos/eleurent/highway-env/releases/39718894",
        "zipball_url": "https://api.github.com/repos/eleurent/highway-env/zipball/v1.1"
      },
      {
        "authorType": "User",
        "author_name": "eleurent",
        "body": "",
        "dateCreated": "2020-09-04T08:42:19Z",
        "datePublished": "2020-09-07T16:02:20Z",
        "html_url": "https://github.com/eleurent/highway-env/releases/tag/v1.0",
        "name": "First stable release",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/eleurent/highway-env/tarball/v1.0",
        "url": "https://api.github.com/repos/eleurent/highway-env/releases/30863565",
        "zipball_url": "https://api.github.com/repos/eleurent/highway-env/zipball/v1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1062,
      "date": "Wed, 29 Dec 2021 18:46:21 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning",
      "autonomous-driving",
      "gym-environment"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Agents solving the `highway-env` environments are available in the [eleurent/rl-agents](https://github.com/eleurent/rl-agents) and [DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3) repositories.\n\nSee the [documentation](https://highway-env.readthedocs.io/en/latest/quickstart.html#training-an-agent) for some examples and notebooks.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nimport gym\nimport highway_env\n\nenv = gym.make(\"highway-v0\")\n\ndone = False\nwhile not done:\n    action = ... #: Your agent code here\n    obs, reward, done, info = env.step(action)\n    env.render()\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}