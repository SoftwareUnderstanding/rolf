{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.1556"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find Context-Transformer useful in your research, please consider citing:\n```BibTeX\n@inproceedings{yang2020context,\n  title={Context-Transformer: Tackling Object Confusion for Few-Shot Detection.},\n  author={Yang, Ze and Wang, Yali and Chen, Xianyu and Liu, Jianzhuang and Qiao, Yu},\n  booktitle={AAAI},\n  pages={12653--12660},\n  year={2020}\n}\n```\n&nbsp;\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{yang2020context,\n  title={Context-Transformer: Tackling Object Confusion for Few-Shot Detection.},\n  author={Yang, Ze and Wang, Yali and Chen, Xianyu and Liu, Jianzhuang and Qiao, Yu},\n  booktitle={AAAI},\n  pages={12653--12660},\n  year={2020}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Ze-Yang/Context-Transformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-23T08:19:10Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T04:10:50Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To tackle the object confusion problem in few-shot detection, we propose a novel Context-Transformer within a concise deep transfer framework. Specifically, Context-Transformer can effectively leverage source-domain object knowledge as guidance, and automatically formulate relational context clues to enhance the detector's generalization capcity to the target domain.\nIt can be flexibly embedded in the popular SSD-style detectors, which makes it a plug-and-play module for end-to-end few-shot learning. For more details, please refer to our [original paper](https://arxiv.org/pdf/2003.07304.pdf).\n\n<p align=center><img width=\"80%\" src=\"doc/Motivation.png\"/></p>\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9543780780584082
      ],
      "excerpt": "This repository contains the official implementation of the AAAI 2020 paper Context-Transformer: Tackling Object Confusion for Few-Shot Detection. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Context-Transformer: Tackling Object Confusion for Few-Shot Detection, AAAI 2020",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```Shell\n#: specify a directory for dataset to be downloaded into, else default is ~/data/\nsh data/scripts/VOC2007.sh #: <directory>\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```Shell\n#: specify a directory for dataset to be downloaded into, else default is ~/data/\nsh data/scripts/VOC2012.sh #: <directory>\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Download the MS COCO dataset from [official website](http://mscoco.org/) to `data/COCO/` (or make a symlink `ln -s /path/to/coco data/COCO`). All annotation files (.json) should be placed under the `COCO/annotations/` folder. It should have this basic structure\n```Shell\n$COCO/\n$COCO/cache/\n$COCO/annotations/\n$COCO/images/\n$COCO/images/train2014/\n$COCO/images/val2014/\n```\nNote: The current COCO dataset has released new *train2017* and *val2017* sets which are just new splits of the same image sets. \n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Ze-Yang/Context-Transformer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Wed, 22 Dec 2021 19:43:50 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Ze-Yang/Context-Transformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Ze-Yang/Context-Transformer",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Ze-Yang/Context-Transformer/master/make.sh",
      "https://raw.githubusercontent.com/Ze-Yang/Context-Transformer/master/data/scripts/VOC2007.sh",
      "https://raw.githubusercontent.com/Ze-Yang/Context-Transformer/master/data/scripts/VOC2012.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To evaluate the incremental model on VOC2007 test set (specify your target split via `--split`):\n```Shell\npython test.py -d VOC --split 1 --setting incre -p 2 --save-folder weights/fewshot/incre/VOC_split1_5shot --resume\n```\nNote:\n- --resume: load model from the last checkpoint in the folder `--save-folder`.\n\nIf you would like to manually specify the path to load model, use `--load-file path/to/model.pth` instead of `--resume`.\n\n&nbsp;\n\nShould you have any questions regarding this repo, feel free to email me at ze001@e.ntu.edu.sg.\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To evaluate the transferred model on VOC2007 test set:\n```Shell\npython test.py -d VOC -p 2 --save-folder weights/fewshot/transfer/VOC_5shot --resume\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To evaluate the pretrained model on VOC2007 test set (specify your target split via `--split`):\n```Shell\npython test.py -d VOC --split 1 --setting incre -p 1 --save-folder weights/VOC_split1_pretrain --resume\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To evaluate the pretrained model on COCO minival set:\n```Shell\npython test.py -d COCO -p 1 --save-folder weights/COCO60_pretrain --resume\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To finetune on VOC dataset for split1 setting *(1 shot)*:\n```Shell\npython train.py -d VOC --split 1 --setting incre -p 2 -m ours --shot 1 --save-folder weights/fewshot/incre/VOC_split1_1shot --load-file weights/VOC_split1_pretrain/model_final.pth -max 200 --steps 150 --checkpoint-period 50 --warmup-iter 0 --no-mixup-iter 100\n```\nTo finetune on VOC dataset for split1 setting *(5 shot)*:\n```Shell\npython train.py -d VOC --split 1 --setting incre -p 2 -m ours --shot 5 --save-folder weights/fewshot/incre/VOC_split1_5shot --load-file weights/VOC_split1_pretrain/model_final.pth -max 400 --steps 350 --checkpoint-period 50 --warmup-iter 0 --no-mixup-iter 100\n```\nNote:\n- Simply change `--split` for other split settings.\n- For other shot settings, feel free to adjust `--shot`, `-max`, `--steps` and `--no-mixup-iter` to obtain satisfactory results.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To finetune on VOC dataset *(1 shot)*:\n```Shell\npython train.py --load-file weights/COCO60_pretrain/model_final.pth --save-folder weights/fewshot/transfer/VOC_1shot -d VOC -p 2 --shot 1 --method ours -max 2000 --steps 1500 1750 --checkpoint-period 200 --warmup-iter 0 --no-mixup-iter 750 -b 20\n```\nTo finetune on VOC dataset *(5 shot)*:\n```Shell\npython train.py --load-file weights/COCO60_pretrain/model_final.pth --save-folder weights/fewshot/transfer/VOC_5shot -d VOC -p 2 --shot 5 --method ours -max 4000 --steps 3000 3500 --checkpoint-period 500 --warmup-iter 0 --no-mixup-iter 1500\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To pretrain RFBNet on VOC split1 (simply change `--split` for other splits):\n```Shell\npython train.py --save-folder weights/VOC_split1_pretrain -d VOC -p 1 -max 50000 --steps 30000 40000 --checkpoint-period 4000 --warmup-iter 1000 --setting incre --split 1\n```\nNote:\n- To ease your reproduce, feel free to download the above pretrained RFBNet models via [BaiduYun Driver](https://pan.baidu.com/s/1aW73KRm3anrX0ulcadQZMg) \nor [OneDrive](https://entuedu-my.sharepoint.com/:f:/g/personal/ze001_e_ntu_edu_sg/Ep1kRewPKKJCi0hIrAcyRKsBKm4q78TdZDh_O-cwfeQs-A?e=X9uk5r) directly.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To pretrain RFBNet on source domain dataset COCO60:\n```Shell\npython train.py --save-folder weights/COCO60_pretrain -d COCO -p 1\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Run the following command to obtain nonvoc/voc split annotation files (.json): \n```\npython data/split_coco_dataset_voc_nonvoc.py\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Move the Main2007.zip and Main2012.zip under `data/` folder to `data/VOCdevkit/VOC2007/ImageSets/` and `data/VOCdevkit/VOC2012/ImageSets/` respectively, and unzip them. Make sure that the .txt files contained in the zip file are under corresponding `path/to/Main/` folder.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Clone this repository. This repository is mainly based on [RFBNet](https://github.com/ruinmessi/RFBNet) and [Detectron2](https://github.com/facebookresearch/detectron2), many thanks to them.\n- Install [anaconda](https://www.anaconda.com/distribution/) and requirements:\n    - python 3.6\n    - PyTorch 1.4.0\n    - CUDA 10.0\n    - gcc 5.4\n    - cython\n    - opencv\n    - matplotlib\n    - tabulate\n    - termcolor\n    - tensorboard\n\n        You can setup the entire environment simply with following lines:\n\n        ```sh\n        conda create -n CT python=3.6 && conda activate CT\n        conda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n        conda install cython matplotlib tabulate termcolor tensorboard\n        pip install opencv-python\n        ```\n\n- Compile the nms and coco tools:\n```Shell\nsh make.sh\n```\n\nNote: \n- Check your GPU architecture support in utils/build.py, line 131. Default is:\n``` \n'nvcc': ['-arch=sm_61',\n``` \n- Ensure that the cuda environment is integrally installed, including compiler, tools and libraries. Plus, make sure the cudatoolkit version in the conda environment matches with the one you compile with. Check about that using `nvcc -V` and `conda list | grep cudatoolkit`, the output version should be the same.\n- We have test the code on PyTorch-1.4.0 and Python 3.6. It might be able to run on other versions but with no guarantee.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "| Method (*1-shot*) | *Split1* | *Split2* | *Split3* |\n|:-------|:-------:|:-----:|:-------:|\n| [Shmelkov2017](https://arxiv.org/pdf/1708.06977.pdf) | 23.9 | 19.2 | 21.4 |\n| [Kang2019](https://arxiv.org/pdf/1812.01866.pdf) | 14.8 | 15.7 | 19.2 |\n| Ours | **39.8** | **32.5** | **34.0** |\n\n| Method (*5-shot*) | *Split1* | *Split2* | *Split3* |\n|:-------|:-------:|:-----:|:-------:|\n| [Shmelkov2017](https://arxiv.org/pdf/1708.06977.pdf) | 38.8 | 32.5 | 31.8 |\n| [Kang2019](https://arxiv.org/pdf/1812.01866.pdf) | 33.9 | 30.1 | 40.6 |\n| Ours | **44.2** | **36.3** | **40.8** |\n\nNote:\n- The results here is higher than that reported in the paper due to training strategy adjustment.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "| Method |  *1shot* | *5shot* |\n|:-------|:-----:|:-------:|\n| [Prototype](https://github.com/ShaoqingRen/faster_rcnn) | 22.8 | 39.8 |\n| [Imprinted](http://pjreddie.com/darknet/yolo/) | 24.5 | 40.9 |\n| [Non-local](https://github.com/daijifeng001/R-FCN)| 25.2 | 41.0 |\n| Baseline | 21.5 | 39.4 |\n| Ours | **27.0** | **43.8** |\n\n*News*: We now support instance shot for COCO60 to VOC20 transfer setting, denoted by suffix `-IS` below.\n\n| Method |  *1shot* | *5shot* |\n|:-------|:-----:|:-------:|\n| Baseline-IS | 19.2 | 35.7 |\n| Ours-IS | **27.1** | **40.4** |\n\nNote:\n- The instance shots are kept the same as incremental setting, which is different from the image shots we originally used in transfer setting.\nTherefore, it's possible that the 1-shot result of Ours-IS (27.1) is comparable to Ours (27.0).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8835572673635612
      ],
      "excerpt": "First download the fc-reduced VGG-16 PyTorch base network weights at https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Ze-Yang/Context-Transformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C",
      "Cuda",
      "Shell",
      "C++"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Ze-Yang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Context-Transformer: Tackling Object Confusion for Few-Shot Detection",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Context-Transformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Ze-Yang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Ze-Yang/Context-Transformer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 73,
      "date": "Wed, 22 Dec 2021 19:43:50 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "few-shot",
      "pytorch",
      "rfbnet",
      "transfer-learning",
      "transformer",
      "object-detection"
    ],
    "technique": "GitHub API"
  }
}