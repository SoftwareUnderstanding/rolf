{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/luckynozomi/PPI_Bert",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-09T13:56:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-22T15:06:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.842892196928729
      ],
      "excerpt": "* guid: an unique ID for this entry. This is not used in training as a feature. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8158446014262077
      ],
      "excerpt": "* sentence: the sentnece of this entry. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9456577033283979
      ],
      "excerpt": "1. in get_labels() function: change the return value to the list of all the labels of your sentenfes; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.911454739067577,
        0.9897445407161569
      ],
      "excerpt": "    * guid: the unique ID of the sentence \n    * text_a: the text of the sentence \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8120300170917842,
        0.9445870604654238
      ],
      "excerpt": "    * label: the label of the sentence. \nFinally, in run_classifier.py, change all the instances of PPIProcessor to the name of your own data processor. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127,
        0.89597792934825,
        0.9117587231829651
      ],
      "excerpt": "[CLS] PROT1 interacts with PROT2 . [SEP]  \nSentence model takes the output of token [CLS] from the transformer as the input to the classification layers. \nSentence Model is implemented in sentence_model.py. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/luckynozomi/PPI_Bert/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 21:48:40 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/luckynozomi/PPI_Bert/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "luckynozomi/PPI_Bert",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/luckynozomi/PPI_Bert/master/fine_tune.sh",
      "https://raw.githubusercontent.com/luckynozomi/PPI_Bert/master/predict.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I tested this project with python 3.6.9\nwith the following packages and versions:\ntensorflow==1.14.0\ngast==0.2.2 (note that you have to install this specific version and overwrite the one installed by tensorflow, o/w there will be errors)\n\nYou can install all the packages I used (for running on CPU) using `pip install -r requirements-cpu.txt`, preferrablely under using a virtual environment. \n\nIf you want to use GPU, use `pip install -r requirements.txt`. Depending on your CUDA version, this may require additional setups.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8324117787794156
      ],
      "excerpt": "First, use chmod +x *.sh to make all shell files executable. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/luckynozomi/PPI_Bert/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Installing packages",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PPI_Bert",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "luckynozomi",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/luckynozomi/PPI_Bert/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 21:48:40 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Instance Model joins all the representations from tokens indexed with `entity_mask`, (aka token `[CLS]`, `PROT1`, and `PROT2`) and use this long output as input to the classification layers.\n\nThe `entity_mask` for the above sentence is `[0, 1, 4]`, where 1 and 4 are the positions of `PROT1` and `PROT2` in the sentence.\n\nNote that if you use instance model, you would implemented the way to calculate `entity_mask` yourself. The function is `get_entity_mask()`. It takes 2 input parameters:\n1. `tokens`: this is the list of tokens after tokenization.\n2. `tokenizer`: this is the tokenizer used for tokenizing the sentence.\n\nInstance model is implemented in [instance_model.py](instance_model.py).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Both the scripts mentioned below should be able to run after you changed `PROJECT_DIR`.\n\nRefer to the [BERT repo](https://github.com/google-research/bert) for additional pretrained models, as well as [BioBert repo](https://github.com/dmis-lab/biobert) for models pre-trained with bio text.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Run [predict.sh](predict.sh) to predict. Please refer to the file itself for documentations. \n`test_results.csv` will be generated under directory `TRAINED_CLASSIFIER` after prediction. It's a tsv file with 4 columns:\n1. guid\n2. predicted probabilities for each category (Note: multiple categories not tested)\n3. real label for the sentence\n4. the sentence itself\n\n",
      "technique": "Header extraction"
    }
  ]
}