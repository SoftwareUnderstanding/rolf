{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@misc{wang2020linformer,\n    title={Linformer: Self-Attention with Linear Complexity},\n    author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},\n    year={2020},\n    eprint={2006.04768},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n```\n\n```bibtex\n@inproceedings{vaswani2017attention,\n  title={Attention is all you need},\n  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia},\n  booktitle={Advances in neural information processing systems},\n  pages={5998--6008},\n  year={2017}\n}\n```\n[\"Listen with attention...\"](https://youtu.be/dRSOB-E0gPA?t=54)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{vaswani2017attention,\n  title={Attention is all you need},\n  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia},\n  booktitle={Advances in neural information processing systems},\n  pages={5998--6008},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{wang2020linformer,\n    title={Linformer: Self-Attention with Linear Complexity},\n    author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},\n    year={2020},\n    eprint={2006.04768},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.803637040433216
      ],
      "excerpt": "I am not the author of the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998085425843605
      ],
      "excerpt": "vis.plot_all_heads(title=\"All P_bar matrices\", #: Change the title if you'd like \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tatp22/linformer-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-11T15:09:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-17T04:37:52Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9948497421401638,
        0.9386102945588464,
        0.9853427840979532,
        0.9864401862464859
      ],
      "excerpt": "A practical implementation of the Linformer paper. This is attention with only linear complexity in n, allowing for very long sequence lengths (1mil+) to be attended to on modern hardware. \nThis repo is an Attention Is All You Need style transformer, complete with an encoder and decoder module. The novelty here is that now, one can make the attention heads linear. Check out how to use it below. \nThis is in the process of being validated on wikitext-2. Currently, it performs at the same level as other sparse attention mechanisms, like the Sinkhorn Transformer, but the best hyperparameters still have to be found. \nVisualization of the heads is also possible. To see more information, check out the Visualization section below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9304992908583495,
        0.8077724012940656,
        0.8189401064869494
      ],
      "excerpt": "With the methods flag, one can set the method that the linformer performs downsampling. Currently, three methods are supported: \nlearnable: This downsampling method creates a learnable n,k nn.Linear module. \nconvolution: This downsampling method creates a 1d convolution, with stride length and kernel size n/k. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8461777519189497,
        0.9838926126861685,
        0.917274401776368
      ],
      "excerpt": "In the future, I may include pooling or something else. But for now, these are the options that exist. \nAs an attempt to further introduce memory savings, the concept of checkpoint levels have been introduced. The current three checkpoint levels are C0, C1, and C2. When going up checkpoint levels, one sacrifices speed for memory savings. That is, checkpoint level C0 is the fastest, but takes up the most space on the GPU, while C2 is the slowest, but takes up the least space on the GPU. The details of each checkpoint level are as follows: \n* C0: No checkpointing. The models runs while keeping all of the attention heads and ff layers in the GPU memory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9221409271160117,
        0.9025530651702713,
        0.9507903615631863,
        0.9522020765571048
      ],
      "excerpt": "* C2: Along with the optimizations at the C1 level, checkpoint each head in each MultiHead Attention layer. With this, increasing nhead should have less of an impact on memory. However, concating the heads together with torch.cat still takes up a lot of memory, and this will hopefully be optimized out in the future. \nPerformance details are still unknown, but the option exists for users that want to try. \nAnother attempt to introduce memory savings in the paper was to introduce parameter sharing between projections. This is mentioned in section 4 of the paper; in particular, there were 4 different types of parameter sharing that the authors discussed, and all have been implemented in this repo. The first option takes up the most memory, and each further option reduces the necessary memory requirements. \n* none: This is no parameter sharing. For every head and for every layer, a new E and a new F matrix is calculated for every head at each layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8468360128983006,
        0.8684568840087262
      ],
      "excerpt": "* layerwise: There is one projection matrix P, and every head in every layer uses E = F = P. \nAs started in the paper, this means that for a 12 layer, 12 head network, there would be 288, 24, 12 and 1 different projection matrices, respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9537354365014655,
        0.9777327209896727
      ],
      "excerpt": "Also, note that according to the authors, in figure 3, this parameter sharing doesn't really affect the end result too much. So it may be best to just stick with layerwise sharing for everything, but the option exists for users to try it out. \nOne slight problem with the current implementation of the Linformer is that your sequence length has to match the input_size flag of the model. The Padder pads the input size such that the tensor can be fed into the network. An example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = Linformer( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = Padder(model) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "y = model(x) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8987887310932148
      ],
      "excerpt": "Starting with version 0.8.0, one can now visualize the attention heads of the linformer! To see this in action, simply import the Visualizer class, and run the plot_all_heads() function to see a picture of all the attention heads at each level, of size (n,k). Make sure that you specify visualize=True in the forward pass, as this saves the P_bar matrix so that the Visualizer class can properly visualize the head. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = Linformer( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "vis = Visualizer(model) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9524062115251373
      ],
      "excerpt": "Similar to the Reformer, I will be attempting to make a Encoder/Decoder Module, so that training can be simplified. This works like 2 LinformerLM classes. Params can be adjusted individually for each one, with the encoder having the enc_ prefix for all of the hyperparams, and the decoder having the dec_ prefix in a similar fashion. So far, what is implemented is: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9102138129949224
      ],
      "excerpt": "I am planning to have a way to generate text sequence for this. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9279583312511284
      ],
      "excerpt": "As opposed to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9645376902735145,
        0.99297791359993,
        0.8924150100105924,
        0.8888222246712,
        0.985454188124997,
        0.8246841338773249
      ],
      "excerpt": "Note that the Linformer has O(nk) time and space complexity. So, while it may be linear in n, make sure that your k is not too large as well. These are editable with input_size and dim_k, respectively. \nSpeaking about k, the authors found that empirical evidence supports the fact that \"the performance of Linformer model is mainly determined by the projected dimension k instead of the ratio n/k\". Therefore, even when increasing sequence lengths, it may be fine to keep a relatively low, constant k (the authors showed with k=256, that it still performed almost as good as a vanilla transformer). \nOne more tip for k: The authors recommend that k = O(d/eps^2), if self attention wants to be approximated by full attention, with eps error. \nThis code, so far, is pretty much only linear layers as well as matrix multiplications. So, libraries like apex should work with this, however, in practice, it has not been tested. \nIn practice, I found that the memory and time requirements are more on the order of O(nkd), with n=input_size, k=dim_k, and d=dim_d. \nRun some benchmark tests to see what the performance is (Doing that now) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "My take on a practical implementation of Linformer for Pytorch.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tatp22/linformer-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 28,
      "date": "Tue, 28 Dec 2021 20:34:25 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/tatp22/linformer-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "tatp22/linformer-pytorch",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\npip install linformer-pytorch\n```\n\nAlternatively,\n\n```\ngit clone https://github.com/tatp22/linformer-pytorch.git\ncd linformer-pytorch\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from linformer_pytorch import Linformer, Padder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth=3, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9027120252159228
      ],
      "excerpt": "print(y) #: (1, 500, 16) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from linformer_pytorch import Linformer, Visualizer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth=3, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8514036489524467
      ],
      "excerpt": "                   show=True, #: Show the picture \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from linformer_pytorch import LinformerEncDec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289669050403863
      ],
      "excerpt": "output = encdec(x,y) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/tatp22/linformer-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Peter Tatkowski\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Linformer Pytorch Implementation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "linformer-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "tatp22",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tatp22/linformer-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "Have not pushed up a release in a while, and this is a latest working version after 2 misc bugs have been fixed.",
        "dateCreated": "2020-10-10T13:17:21Z",
        "datePublished": "2020-10-10T13:21:21Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.19.3",
        "name": "Latest working version",
        "tag_name": "0.19.3",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.19.3",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/32403679",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.19.3"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": " Added intermediate ff dimension\r\n\r\nNow, the model dimension can be different in the intermediate layers.\r\nThis change applies to the ff module, and only in the encoder. Now, if\r\nthe flag `ff_intermediate` is not None, the layers will look like this:\r\n\r\n```\r\nchannels -> ff_dim -> ff_intermediate (For layer 1)\r\nff_intermediate -> ff_dim -> ff_intermediate (For layers 2 to depth-1)\r\nff_intermediate -> ff_dim -> channels (For layer depth)\r\n```\r\n\r\nAs opposed to\r\n\r\n```\r\nchannels -> ff_dim -> channels (For all layers)\r\n```",
        "dateCreated": "2020-08-04T15:58:25Z",
        "datePublished": "2020-08-04T16:03:28Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.16.0",
        "name": "Added intermediate dim change",
        "tag_name": "0.16.0",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.16.0",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/29279615",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.16.0"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "Now, the linformer supports convolution as a way to downsample the input, instead of relying on linear layers. This may reduce the amount of parameters necessary.",
        "dateCreated": "2020-07-31T09:22:18Z",
        "datePublished": "2020-07-31T09:28:54Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.15.0",
        "name": "Able to use convolutional nets instead of linear",
        "tag_name": "0.15.0",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.15.0",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/29156058",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.15.0"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "Finished an encoder and a decoder module. Also, causal attention works, when the `causal=True` flag is set. Will update the README shortly...",
        "dateCreated": "2020-07-25T18:13:50Z",
        "datePublished": "2020-07-28T16:13:06Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.14.0",
        "name": "Encoder Decoder finished, Causal attention",
        "tag_name": "0.14.0",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.14.0",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/29037024",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.14.0"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "Added masking to the Linformer. However, this is still a WIP, since masking cannot be done in the traditional sense, like what is done in the attention is all you need paper, because there is an overhead of adding another `(n,n)` matrix, which is infeasable.",
        "dateCreated": "2020-07-16T10:39:00Z",
        "datePublished": "2020-07-16T11:03:05Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.13.1",
        "name": "Added Masking",
        "tag_name": "0.13.1",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.13.1",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/28626979",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.13.1"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "The repo now supports an encoder and a decoder.\r\n\r\nTODO: Masking",
        "dateCreated": "2020-07-06T13:33:00Z",
        "datePublished": "2020-07-06T13:41:30Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.13.0",
        "name": "Started Encoder/Decoder work",
        "tag_name": "0.13.0",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.13.0",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/28257075",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.13.0"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "Fixed a bug with the sequencing of the Linformer. Now should train properly.",
        "dateCreated": "2020-07-04T17:00:16Z",
        "datePublished": "2020-07-06T13:13:17Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.12.4",
        "name": "Bug fixed",
        "tag_name": "0.12.4",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.12.4",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/28255776",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.12.4"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "A lm model is now available, for language modeling tasks",
        "dateCreated": "2020-07-02T05:52:23Z",
        "datePublished": "2020-07-02T14:30:23Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.12.1",
        "name": "LM model",
        "tag_name": "0.12.1",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.12.1",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/28161926",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.12.1"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "Rebased the code so it looks better, and added the option to plot the\r\nMHAttention module as well as the Linformer module\r\n\r\n",
        "dateCreated": "2020-06-29T01:42:27Z",
        "datePublished": "2020-06-29T05:49:58Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.11.0",
        "name": "Rebase, added option to plot MHAttention heads",
        "tag_name": "0.11.0",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.11.0",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/28011303",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.11.0"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "Check out pull request #7  to see the changes",
        "dateCreated": "2020-06-28T00:06:35Z",
        "datePublished": "2020-06-28T00:11:17Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.10.0",
        "name": "No weight matrices in `LinearAttentionHead`",
        "tag_name": "0.10.0",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.10.0",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/27993215",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.10.0"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "Added an option to the linformer to compare it with full attention. Watch out, this takes O(n^2) time and space complexity now, where n is the sequence length",
        "dateCreated": "2020-06-27T17:41:50Z",
        "datePublished": "2020-06-27T17:44:58Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.9.0",
        "name": "Full attention option",
        "tag_name": "0.9.0",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.9.0",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/27989523",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.9.0"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "Added the option to save the visualization to a file",
        "dateCreated": "2020-06-23T16:06:41Z",
        "datePublished": "2020-06-23T16:12:50Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.8.2",
        "name": "Added option to save visualization",
        "tag_name": "0.8.2",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.8.2",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/27836740",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.8.2"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "Added the visualizer class, which lets you see all of the attention heads.\r\n\r\nAlso fixed a bug where calculated the E and F matrices. They were calculated to be `(n,d)`, but instead, they should have been `(n,k)`. This has since been fixed.",
        "dateCreated": "2020-06-22T22:33:05Z",
        "datePublished": "2020-06-22T22:35:09Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.8.0",
        "name": "Added Visualizer, fixed bug",
        "tag_name": "0.8.0",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.8.0",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/27806136",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.8.0"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "As well as updating the README, I updated the default behavior of the calculation of the inner head dimension. Now, instead of the default value having to be given, it works just like in the \"attention is all you need\" paper, where it takes however many channels there are, and divides the channels by the number of heads, and then that dimension goes into each of the attention heads.",
        "dateCreated": "2020-06-21T14:50:40Z",
        "datePublished": "2020-06-21T14:52:33Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.7.0",
        "name": "0.7.0",
        "tag_name": "0.7.0",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.7.0",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/27762650",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.7.0"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "Added both the RELU and GELU activation function options to the multihead attention block",
        "dateCreated": "2020-06-20T16:08:23Z",
        "datePublished": "2020-06-20T16:14:02Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.6.0",
        "name": "Added activation to MHAttention",
        "tag_name": "0.6.0",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.6.0",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/27751718",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.6.0"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "Added the flag where one is able to reduce the value of `dim_k` by layer, with the `k_reduce_by_layer` flag. This was alluded to in Figure 1 of the paper, where the normalized cumulative eigenvalue index went up by layer, meaning that we can potentially get away with lower dimensions at higher depths. ",
        "dateCreated": "2020-06-17T21:46:41Z",
        "datePublished": "2020-06-17T21:53:37Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.5.0",
        "name": "Can decrease k by layer",
        "tag_name": "0.5.0",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.5.0",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/27660834",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.5.0"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "Added the `none`, `headwise`, `kv`, and `layerwise` parameter sharing options. Also, added positional encodings",
        "dateCreated": "2020-06-17T21:19:50Z",
        "datePublished": "2020-06-17T21:45:57Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.4.0",
        "name": "Added weight sharing options and pos enc",
        "tag_name": "0.4.0",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.4.0",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/27660604",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.4.0"
      },
      {
        "authorType": "User",
        "author_name": "tatp22",
        "body": "The way that the E and F matrices were calculated were changed. Before, they were an identity matrix, but with this release, they were changed to the way that the paper's authors recommended: As linear layers, with xavier init.",
        "dateCreated": "2020-06-15T21:42:43Z",
        "datePublished": "2020-06-17T21:44:32Z",
        "html_url": "https://github.com/tatp22/linformer-pytorch/releases/tag/0.3.1",
        "name": "E, F matrix calculation changed",
        "tag_name": "0.3.1",
        "tarball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/tarball/0.3.1",
        "url": "https://api.github.com/repos/tatp22/linformer-pytorch/releases/27660558",
        "zipball_url": "https://api.github.com/repos/tatp22/linformer-pytorch/zipball/0.3.1"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 295,
      "date": "Tue, 28 Dec 2021 20:34:25 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "artificial-intelligence",
      "deep-learning",
      "attention-mechanism",
      "pytorch",
      "machine-learning",
      "linformer",
      "paper"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Linformer Language Model\n\n```python\nfrom linformer_pytorch import LinformerLM\nimport torch\n\nmodel = LinformerLM(\n        num_tokens=10000, #: Number of tokens in the LM\n        input_size=512, #: Dimension 1 of the input\n        channels=64, #: Dimension 2 of the input\n        dim_d=None, #: Overwrites the inner dim of the attention heads. If None, sticks with the recommended channels // nhead, as in the \"Attention is all you need\" paper\n        dim_k=128, #: The second dimension of the P_bar matrix from the paper\n        dim_ff=128, #: Dimension in the feed forward network\n        dropout_ff=0.15, #: Dropout for feed forward network\n        nhead=4, #: Number of attention heads\n        depth=2, #: How many times to run the model\n        dropout=0.1, #: How much dropout to apply to P_bar after softmax\n        activation=\"gelu\", #: What activation to use. Currently, only gelu and relu supported, and only on ff network.\n        use_pos_emb=True, #: Whether or not to use positional embeddings\n        checkpoint_level=\"C0\", #: What checkpoint level to use. For more information, see below.\n        parameter_sharing=\"layerwise\", #: What level of parameter sharing to use. For more information, see below.\n        k_reduce_by_layer=0, #: Going down `depth`, how much to reduce `dim_k` by, for the `E` and `F` matrices. Will have a minimum value of 1.\n        full_attention=False, #: Use full attention instead, for O(n^2) time and space complexity. Included here just for comparison\n        include_ff=True, #: Whether or not to include the Feed Forward layer\n        w_o_intermediate_dim=None, #: If not None, have 2 w_o matrices, such that instead of `dim*nead,channels`, you have `dim*nhead,w_o_int`, and `w_o_int,channels`\n        emb_dim=128, #: If you want the embedding dimension to be different than the channels for the Linformer\n        causal=False, #: If you want this to be a causal Linformer, where the upper right of the P_bar matrix is masked out.\n        method=\"learnable\", #: The method of how to perform the projection. Supported methods are 'convolution', 'learnable', and 'no_params'\n        ff_intermediate=None, #: See the section below for more information\n        ).cuda()\nx = torch.randint(1,10000,(1,512)).cuda()\ny = model(x)\nprint(y) #: (1, 512, 10000)\n\n```\n\nLinformer self attention, stacks of `MHAttention` and `FeedForward`s\n\n```python\nfrom linformer_pytorch import Linformer\nimport torch\n\nmodel = Linformer(\n        input_size=262144, #: Dimension 1 of the input\n        channels=64, #: Dimension 2 of the input\n        dim_d=None, #: Overwrites the inner dim of the attention heads. If None, sticks with the recommended channels // nhead, as in the \"Attention is all you need\" paper\n        dim_k=128, #: The second dimension of the P_bar matrix from the paper\n        dim_ff=128, #: Dimension in the feed forward network\n        dropout_ff=0.15, #: Dropout for feed forward network\n        nhead=4, #: Number of attention heads\n        depth=2, #: How many times to run the model\n        dropout=0.1, #: How much dropout to apply to P_bar after softmax\n        activation=\"gelu\", #: What activation to use. Currently, only gelu and relu supported, and only on ff network.\n        checkpoint_level=\"C0\", #: What checkpoint level to use. For more information, see below.\n        parameter_sharing=\"layerwise\", #: What level of parameter sharing to use. For more information, see below.\n        k_reduce_by_layer=0, #: Going down `depth`, how much to reduce `dim_k` by, for the `E` and `F` matrices. Will have a minimum value of 1.\n        full_attention=False, #: Use full attention instead, for O(n^2) time and space complexity. Included here just for comparison\n        include_ff=True, #: Whether or not to include the Feed Forward layer\n        w_o_intermediate_dim=None, #: If not None, have 2 w_o matrices, such that instead of `dim*nead,channels`, you have `dim*nhead,w_o_int`, and `w_o_int,channels`\n        ).cuda()\nx = torch.randn(1, 262144, 64).cuda()\ny = model(x)\nprint(y) #: (1, 262144, 64)\n```\n\nLinformer Multihead attention\n\n```python\nfrom linformer_pytorch import MHAttention\nimport torch\n\nmodel = MHAttention(\n        input_size=512, #: Dimension 1 of the input\n        channels=64, #: Dimension 2 of the input\n        dim=8, #: Dim of each attn head\n        dim_k=128, #: What to sample the input length down to\n        nhead=8, #: Number of heads\n        dropout=0, #: Dropout for each of the heads\n        activation=\"gelu\", #: Activation after attention has been concat'd\n        checkpoint_level=\"C2\", #: If C2, checkpoint each of the heads\n        parameter_sharing=\"layerwise\", #: What level of parameter sharing to do\n        E_proj, F_proj, #: The E and F projection matrices\n        full_attention=False, #: Use full attention instead\n        w_o_intermediate_dim=None, #: If not None, have 2 w_o matrices, such that instead of `dim*nead,channels`, you have `dim*nhead,w_o_int`, and `w_o_int,channels`\n        )\nx = torch.randn(1, 512, 64)\ny = model(x)\nprint(y) #: (1, 512, 64)\n```\n\nThe Linear attention head, the novelty of the paper\n\n```python\nfrom linformer_pytorch import LinearAttentionHead\nimport torch\n\nmodel = LinearAttentionHead(\n        dim=64, #: Dim 2 of the input\n        dropout=0.1, #: Dropout of the P matrix\n        E_proj, F_proj, #: The E and F layers\n        full_attention=False, #: Use Full Attention instead\n        )\nx = torch.randn(1, 512, 64)\ny = model(x, x, x)\nprint(y) #: (1, 512, 64)\n```\n\nAn encoder/decoder module.\n\nNote: For causal sequences, one can set the `causal=True` flag on in the `LinformerLM` to mask out the top right in the `(n,k)` attention matrix.\n\n```python\nimport torch\nfrom linformer_pytorch import LinformerLM\n\nencoder = LinformerLM(\n    num_tokens=10000,\n    input_size=512,\n    channels=16,\n    dim_k=16,\n    dim_ff=32,\n    nhead=4,\n    depth=3,\n    activation=\"relu\",\n    k_reduce_by_layer=1,\n    return_emb=True,\n    )\ndecoder = LinformerLM(\n    num_tokens=10000,\n    input_size=512,\n    channels=16,\n    dim_k=16,\n    dim_ff=32,\n    nhead=4,\n    depth=3,\n    activation=\"relu\",\n    decoder_mode=True,\n    )\n\nx = torch.randint(1,10000,(1,512))\ny = torch.randint(1,10000,(1,512))\n\nx_mask = torch.ones_like(x).bool()\ny_mask = torch.ones_like(y).bool()\n\nenc_output = encoder(x, input_mask=x_mask)\nprint(enc_output.shape) #: (1, 512, 128)\ndec_output = decoder(y, embeddings=enc_output, input_mask=y_mask, embeddings_mask=x_mask)\nprint(dec_output.shape) #: (1, 512, 10000)\n```\n\nAn easy way to get the `E` and `F` matrices can be done by calling the `get_EF` function. As an example, for an `n` of `1000` and a `k` of `100`:\n\n```python\nfrom linfromer_pytorch import get_EF\nimport torch\n\nE = get_EF(1000, 100)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}