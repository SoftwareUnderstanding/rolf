{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This work was supported by the EU Marie Curie Initial Training Network (ITN) \u201cREtinal VAscular Modelling, Measurement And Diagnosis\" (REVAMMAD), Project no. 316990.\n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        0.8772692606136239
      ],
      "excerpt": "- Experiment results: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9302785107180478,
        0.9302785107180478,
        0.9302785107180478,
        0.9302785107180478,
        0.9302785107180478,
        0.9302785107180478,
        0.9302785107180478,
        0.9302785107180478
      ],
      "excerpt": "| Soares et al [1]        | .9614            | \n| Azzopardi et al. [2]    | .9614            | \n| Osareh et al  [3]       | .9650            | \n| Roychowdhury et al. [4] | .9670            | \n| Fraz et al.  [5]        | .9747            | \n| Qiaoliang et al. [6]    | .9738            | \n| Melinscak et al. [7]    | .9749            | \n| Liskowski et al.^ [8]   | .9790            | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "- opencv >=2.4.10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9302785107180478,
        0.9302785107180478,
        0.9302785107180478,
        0.9302785107180478,
        0.9302785107180478,
        0.9302785107180478
      ],
      "excerpt": "| Soares et al [1]        | .9671           | \n| Azzopardi et al. [2]    | .9563            | \n| Roychowdhury et al. [4] | .9688            | \n| Fraz et al.  [5]        | .9768            | \n| Qiaoliang et al. [6]    | .9879            | \n| Liskowski et al.^ [8]   | .9930            | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977478207207006
      ],
      "excerpt": "[1] Soares et al., \u201cRetinal vessel segmentation using the 2-d Gabor wavelet and supervised classification,\u201d Medical Imaging, IEEE Transactions on, vol. 25, no. 9, pp. 1214\u20131222, 2006. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9688766505138257,
        0.9198216112506125
      ],
      "excerpt": "Medical image analysis, vol. 19, no. 1, pp. 46\u201357, 2015. \n[3] Osareh et al., \u201cAutomatic blood vessel segmentation in color images of retina,\u201d Iran. J. Sci. Technol. Trans. B: Engineering, vol. 33, no. B2, pp. 191\u2013206, 2009. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/orobix/retina-unet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-08-12T10:46:14Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T03:52:27Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9779003546747007,
        0.8751005240602939,
        0.9946060123376919,
        0.8259163761138316
      ],
      "excerpt": "This repository contains the implementation of a convolutional neural network used to segment blood vessels in retina fundus images. This is a binary classification task: the neural network predicts if each pixel in the fundus image is either a vessel or not. \nThe neural network structure is derived from the U-Net architecture, described in this paper. \nThe performance of this neural network is tested on the DRIVE database, and it achieves the best score in terms of area under the ROC curve in comparison to the other methods published so far. Also on the STARE datasets, this method reports one of the best performances. \nBefore training, the 20 images of the DRIVE training datasets are pre-processed with the following transformations: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.967699659729723,
        0.9574393848840915
      ],
      "excerpt": "The training of the neural network is performed on sub-images (patches) of the pre-processed full images. Each patch, of dimension 48x48, is obtained by randomly selecting its center inside the full image. Also the patches partially or completely outside the Field Of View (FOV) are selected, in this way the neural network learns how to discriminate the FOV border from blood vessels. \nA set of 190000 patches is obtained by randomly extracting 9500 patches in each of the 20 DRIVE training images. Although the patches overlap, i.e. different patches may contain same part of the original images, no further data augmentation is performed. The first 90% of the dataset is used for training (171000 patches), while the last 10% is used for validation (19000 patches). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925958909810231,
        0.9623937093918643,
        0.9823439173242781,
        0.9698574717538156
      ],
      "excerpt": "The loss function is the cross-entropy and the stochastic gradient descent is employed for optimization. The activation function after each convolutional layer is the Rectifier Linear Unit (ReLU), and a dropout of 0.2 is used between two consecutive convolutional layers. \nTraining is performed for 150 epochs, with a mini-batch size of 32 patches. Using a GeForce GTX TITAN GPU the training lasts for about 20 hours. \nTesting is performed with the 20 images of the DRIVE testing dataset, using the gold standard as ground truth. Only the pixels belonging to the FOV are considered. The FOV is identified with the masks included in the DRIVE database. \nIn order to improve the performance, the vessel probability of each pixel is obtained by averaging multiple predictions. With a stride of 5 pixels in both height and width, multiple consecutive overlapping patches are extracted in each testing image. Then, for each pixel, the vessel probability is obtained by averaging probabilities over all the predicted patches covering the pixel. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.8121605752180592,
        0.8679097016552559,
        0.8276799424525618
      ],
      "excerpt": "- Model: \n  - test_model.png schematic representation of the neural network \n  - test_architecture.json description of the model in json format \n  - test_best_weights.h5 weights of the model which reported the minimum validation loss, as HDF5 file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9317548247650187
      ],
      "excerpt": "  - test_configuration.txt configuration of the parameters of the experiment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327354858502831,
        0.8111580800223408,
        0.9123979614852458,
        0.9579605458702034,
        0.8145690208086916
      ],
      "excerpt": "  - all_*.png the 20 images of the pre-processed originals, ground truth and predictions relative to the DRIVE testing dataset \n  - sample_input_*.png sample of 40 patches of the pre-processed original training images and the corresponding ground truth \n  - test_Original_GroundTruth_Prediction*.png from top to bottom, the original pre-processed image, the ground truth and the prediction. In the predicted image, each pixel shows the vessel predicted probability, no threshold is applied. \nThe following table compares this method to other recent techniques, which have published their performance in terms of Area Under the ROC curve (AUC ROC) on the DRIVE dataset. \n| Method                  | AUC ROC on DRIVE | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8282767821452159,
        0.9661279695745366,
        0.9677519909075619,
        0.9552248295218454
      ],
      "excerpt": "| this method         | .9790        | \n^ different definition of FOV \nThe neural network is developed with the Keras library, we refer to the Keras repository for the installation. \nThis code has been tested with Keras 1.1.0, using either Theano or TensorFlow as backend. In order to avoid dimensions mismatch, it is important to set \"image_dim_ordering\": \"th\" in the ~/.keras/keras.json configuration file. If this file isn't there, you can create it. See the Keras documentation for more details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9183448900717724
      ],
      "excerpt": "- scikit-learn >= 0.17.1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9928968497587627,
        0.9371194997634996
      ],
      "excerpt": "We refer to the DRIVE website for the description of the data. \nIt is convenient to create HDF5 datasets of the ground truth, masks and images for both training and testing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837012003776836
      ],
      "excerpt": "The network is trained on sub-images (patches) of the original full images, specify here the dimension of the patches. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9012854805205904,
        0.8530294728174842
      ],
      "excerpt": "- picture of the model structure (png) \n- a copy of the configuration file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9276680184090577,
        0.8734364367502059
      ],
      "excerpt": "The performance of the trained model is evaluated against the DRIVE testing dataset, consisting of 20 images (as many as in the training set). \nThe parameters for the testing can be tuned again in the configuration.txt file, specifically in the [testing settings] section, as described below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9418600640587428
      ],
      "excerpt": "- best_last: choose the model for prediction on the testing dataset: best = the model with the lowest validation loss obtained during the training; last = the model at the last epoch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8531985074903585,
        0.8051854298084412,
        0.8436904848235897,
        0.850644867211241,
        0.9686720997015398,
        0.9550873053339453
      ],
      "excerpt": "- Picture of all the corresponding segmentation ground truth (png) \n- Picture of all the corresponding segmentation predictions (png) \n- One or more pictures including (top to bottom): original pre-processed image, ground truth, prediction \n- Report on the performance \nAll the results are referred only to the pixels belonging to the FOV, selected by the masks included in the DRIVE database \nThis neural network has been tested also on another common database, the STARE. The neural network is identical as in the experiment with the DRIVE dataset, however some modifications in the code and in the methodology were necessary due to the differences between the two datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8958502126729635,
        0.978068839423418,
        0.955337542227939
      ],
      "excerpt": "The pre-processing is the same applied for the DRIVE dataset, and 9500 random patches of 48x48 pixels each are extracted from each of the 19 images forming the training set. Also the area outside the FOV has been considered for the patch extraction. From these patches, 90% (162450 patches) are used for training and 10% (18050 patches) are used for validation.  The training parameters (epochs, batch size...) are the same as in the DRIVE experiment. \nThe test is performed each time on the single image left out from the training dataset. Similarly to the DRIVE dataset, the vessel probability of each pixel is obtained by averaging over multiple overlapping patches, obtained with a stride of 5 pixels in both width and height. Only the pixels belonging to the FOV are considered. This time the FOV is identified by applying a color threshold in the original images, since no masks are available in the STARE dataset.   \nThe following table shows the results (in terms of AUC ROC) obtained over the 20 different trainings, with the stated image used for test. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9352522891451762,
        0.9570253348308247,
        0.8231171135882059
      ],
      "excerpt": "The folder ./STARE_results contains all the predictions. Each image shows (from top to bottom) the pre-processed original image of the STARE dataset, the ground truth and the corresponding prediction. In the predicted image, each pixel shows the vessel predicted probability, no threshold is applied. \nThe following table compares this method to other recent techniques, which have published their performance in terms of Area Under the ROC curve (AUC ROC) on the STARE dataset. \n| Method                  | AUC ROC on STARE | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8282767821452159,
        0.9661279695745366
      ],
      "excerpt": "| this method         | .9805        | \n^ different definition of FOV \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8435356119486009
      ],
      "excerpt": "[2] Azzopardi et al., \u201cTrainable cosfire filters for vessel delineation with application to retinal images,\u201d \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Retina blood vessel segmentation with a convolutional neural network",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/orobix/retina-unet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 459,
      "date": "Wed, 22 Dec 2021 00:29:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/orobix/retina-unet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "orobix/retina-unet",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.959746900901819,
        0.8708439851414099
      ],
      "excerpt": "The following dependencies are needed: \n- numpy >= 1.11.1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8176894630277319
      ],
      "excerpt": "- h5py >=2.6.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567493161535548
      ],
      "excerpt": "Here you can specify: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8440109965961252
      ],
      "excerpt": "If available, a GPU will be used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8440109965961252
      ],
      "excerpt": "If available, a GPU will be used. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.824064374237381
      ],
      "excerpt": "The results reported in the ./test folder are referred to the trained model which reported the minimum validation loss. The ./test folder includes: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8083079557978765
      ],
      "excerpt": "  - test_architecture.json description of the model in json format \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8287207434500591
      ],
      "excerpt": "python prepare_datasets_DRIVE.py \nThe HDF5 datasets for training and testing will be created in the folder ./DRIVE_datasets_training_testing/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8491564574724201
      ],
      "excerpt": "[data attributes] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8286572695472
      ],
      "excerpt": "[training settings] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123563035354394
      ],
      "excerpt": "- nohup: the standard output during the training is redirected and saved in a log file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python run_training.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8188751348065165,
        0.8073988495461991
      ],
      "excerpt": "The parameters for the testing can be tuned again in the configuration.txt file, specifically in the [testing settings] section, as described below: \n[testing settings] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8074891074557926
      ],
      "excerpt": "- full_images_to_test: number of full images for testing, max 20. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908360911285758
      ],
      "excerpt": "The section [experiment name] must be the name of the experiment you want to test, while [data paths] contains the paths to the testing datasets. Now the section [training settings] will be ignored. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python run_testing.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/orobix/retina-unet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Retina blood vessel segmentation with a convolution neural network (U-net)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "retina-unet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "orobix",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/orobix/retina-unet/blob/master/Readme.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code is written in Python, it is possible to replicate the experiment on the DRIVE database by following the guidelines below.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1097,
      "date": "Wed, 22 Dec 2021 00:29:23 GMT"
    },
    "technique": "GitHub API"
  }
}